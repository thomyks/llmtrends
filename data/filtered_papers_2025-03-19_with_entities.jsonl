{"id": "2012.09766", "submitter": "Sofian Chaybouti", "authors": "Sofian Chaybouti, Achraf Saghe, Aymen Shabou", "title": "MIX : a Multi-task Learning Approach to Solve Open-Domain Question\n  Answering", "comments": "8 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces MIX, a multi-task deep learning approach to solve\nopen-ended question-answering. First, we design our system as a multi-stage\npipeline of 3 building blocks: a BM25-based Retriever to reduce the search\nspace, a RoBERTa-based Scorer, and an Extractor to rank retrieved paragraphs\nand extract relevant text spans, respectively. Eventually, we further improve\nthe computational efficiency of our system to deal with the scalability\nchallenge: thanks to multi-task learning, we parallelize the close tasks solved\nby the Scorer and the Extractor. Our system is on par with state-of-the-art\nperformances on the squad-open benchmark while being simpler conceptually.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 17:22:30 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 20:06:03 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 13:56:45 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chaybouti", "Sofian", ""], ["Saghe", "Achraf", ""], ["Shabou", "Aymen", ""]], "extracted_entities": [{"text": "MIX", "label": "Few-shot Learning"}, {"text": "RoBERTa-based", "label": "RoBERTa"}, {"text": "multi-task learning", "label": "Few-shot Learning"}]}
{"id": "2106.09974", "submitter": "Bahram Sadeghi Bigham", "authors": "Bahram Sadeghi Bigham", "title": "Separating Geometric Data with Minimum Cost: Two Disjoint Convex Hulls", "comments": "This article has undergone many changes and should be reviewed and\n  rewritten in a different format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this study, a geometric version of an NP-hard problem (\"Almost $2-SAT$\"\nproblem) is introduced which has potential applications in clustering,\nseparation axis, binary sensor networks, shape separation, image processing,\netc. Furthermore, it has been illustrated that the new problem known as \"Two\nDisjoint Convex Hulls\" can be solved in polynomial time due to some\ncombinatorial aspects and geometric properties. For this purpose, an $O(n^2)$\nalgorithm has also been presented which employs the Separating Axis Theorem\n(SAT) and the duality of points/lines.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 07:55:08 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 13:01:08 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Bigham", "Bahram Sadeghi", ""]], "extracted_entities": [{"text": "SAT", "label": "BERT"}]}
{"id": "2107.01697", "submitter": "Andrei Zotov", "authors": "A. Zabrodin, A. Zotov", "title": "Field analogue of the Ruijsenaars-Schneider model", "comments": "46 pages, typos corrected", "journal-ref": "J. High Energ. Phys. 2022, 23 (2022)", "doi": "10.1007/JHEP07(2022)023", "report-no": null, "categories": "math-ph hep-th math.MP nlin.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest a field extension of the classical elliptic Ruijsenaars-Schneider\nmodel. The model is defined in two different ways which lead to the same\nresult. The first one is via the trace of a chain product of $L$-matrices which\nallows one to introduce the Hamiltonian of the model and to show that the model\nis gauge equivalent to a classical elliptic spin chain. In this way, one\nobtains a lattice field analogue of the Ruijsenaars-Schneider model with\ncontinuous time. The second method is based on investigation of general\nelliptic families of solutions to the 2D Toda equation. We derive equations of\nmotion for their poles, which turn out to be difference equations in space with\na lattice spacing $\\eta$, together with a zero curvature representation for\nthem. We also show that the equations of motion are Hamiltonian. The obtained\nsystem of equations can be naturally regarded as a field generalization of the\nRuijsenaars-Schneider system. Its lattice version coincides with the model\nintroduced via the first method. The limit $\\eta \\to 0$ is shown to give the\nfield extension of the Calogero-Moser model known in the literature. The fully\ndiscrete version of this construction is also discussed.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 17:58:05 GMT"}, {"version": "v2", "created": "Fri, 7 Jan 2022 11:33:07 GMT"}, {"version": "v3", "created": "Sun, 27 Mar 2022 15:51:21 GMT"}, {"version": "v4", "created": "Tue, 10 May 2022 15:44:10 GMT"}, {"version": "v5", "created": "Thu, 13 Mar 2025 14:08:02 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zabrodin", "A.", ""], ["Zotov", "A.", ""]], "extracted_entities": [{"text": "Ruijsenaars-Schneider\nmodel", "label": "AI model"}, {"text": "Ruijsenaars-Schneider model", "label": "AI model"}, {"text": "Calogero-Moser model", "label": "AI model"}]}
{"id": "2111.02019", "submitter": "Juho Timonen", "authors": "Juho Timonen and Harri L\\\"ahdesm\\\"aki", "title": "Scalable mixed-domain Gaussian process modeling and model reduction for\n  longitudinal data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian process (GP) models that combine both categorical and continuous\ninput variables have found use in analysis of longitudinal data and computer\nexperiments. However, standard inference for these models has the typical cubic\nscaling, and common scalable approximation schemes for GPs cannot be applied\nsince the covariance function is non-continuous. In this work, we derive a\nbasis function approximation scheme for mixed-domain covariance functions,\nwhich scales linearly with respect to the number of observations and total\nnumber of basis functions. The proposed approach is naturally applicable to\nalso Bayesian GP regression with discrete observation models. We demonstrate\nthe scalability of the approach and compare model reduction techniques for\nadditive GP models in a longitudinal data context. We confirm that we can\napproximate the exact GP model accurately in a fraction of the runtime compared\nto fitting the corresponding exact model. In addition, we demonstrate a\nscalable model reduction workflow for obtaining smaller and more interpretable\nmodels when dealing with a large number of candidate predictors.\n", "versions": [{"version": "v1", "created": "Wed, 3 Nov 2021 04:47:37 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2024 09:06:25 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 00:52:01 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Timonen", "Juho", ""], ["L\u00e4hdesm\u00e4ki", "Harri", ""]], "extracted_entities": [{"text": "cubic\nscaling", "label": "Scaling law"}]}
{"id": "2205.10856", "submitter": "Hideki Arahari", "authors": "Hideki Arahari, Sota Konishi, Kodai Takaoka, Seiji Akita and Hajime\n  Ishihara", "title": "Proposed optomechanical systems based on luminescence-induced optical\n  forces", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an optomechanical system utilizing luminescence-induced optical\nforces (LIOFs). Anisotropic dielectric structures enhance the recoil force from\nthe luminescence. The optomechanical resonator consists of a composite film\nwith a dielectric membrane, luminescent nanofilm, and a metallic substrate. The\nLIOF causes a mechanical frequency shift in the oscillator known as the optical\nspring effect. These results link the quantum properties of luminescent\nnanomaterials with those of other quantum-mechanical systems with vastly\ndifferent frequency regimes via induced vibrational modes.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2022 15:45:47 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 14:39:42 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Arahari", "Hideki", ""], ["Konishi", "Sota", ""], ["Takaoka", "Kodai", ""], ["Akita", "Seiji", ""], ["Ishihara", "Hajime", ""]], "extracted_entities": [{"text": "LIOFs", "label": "LLMs"}]}
{"id": "2209.14447", "submitter": "Sadman Sakib Enan", "authors": "Sadman Sakib Enan and Junaed Sattar", "title": "A Diver Attention Estimation Framework for Effective Underwater\n  Human-Robot Interaction", "comments": "9 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many underwater tasks, such as cable-and-wreckage inspection and\nsearch-and-rescue, can benefit from robust Human-Robot Interaction (HRI)\ncapabilities. With the recent advancements in vision-based underwater HRI\nmethods, Autonomous Underwater Vehicles (AUVs) have the capability to interact\nwith their human partners without requiring assistance from a topside operator.\nHowever, in these methods, the AUV assumes that the diver is ready for\ninteraction, while in reality, the diver may be distracted. In this paper, we\nattempt to address this problem by presenting a diver attention estimation\nframework for AUVs to autonomously determine the attentiveness of a diver, and\ndeveloping a robot controller to allow the AUV to navigate and reorient itself\nwith respect to the diver before initiating interaction. The core element of\nthe framework is a deep convolutional neural network called DATT-Net. It is\nbased on a pyramid structure that can exploit the geometric relations among 10\nfacial keypoints of a diver to estimate their head orientation, which we use as\nan indicator of attentiveness. Our on-the-bench experimental evaluations and\nreal-world experiments during both closed- and open-water robot trials confirm\nthe efficacy of the proposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2022 22:08:41 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 03:06:56 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Enan", "Sadman Sakib", ""], ["Sattar", "Junaed", ""]], "extracted_entities": [{"text": "attentiveness", "label": "Attention mechanism"}]}
{"id": "2210.12890", "submitter": "Chul Min Kim Ph.D.", "authors": "Chul Min Kim and Sang Pyo Kim", "title": "3+1 formulation of light modes in nonlinear electrodynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "gr-qc", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a 3+1 formulation of the light modes in nonlinear electrodynamics\ndescribed by Plebanski-type Lagrangians, which include Post-Maxwellian,\nBorn-Infeld, ModMax, and Heisenberg-Euler-Schwinger QED Lagrangians. In\nnonlinear electrodynamics, strong electromagnetic fields modify the vacuum to\nacquire optical properties. Such a field-modified vacuum can possess electric\npermittivity, magnetic permeability, and magneto-electric response, inducing\nnovel phenomena like vacuum birefringence. By exploiting the mathematical\nstructures of Plebanski-type Lagrangians, we obtain a streamlined procedure and\nexplicit formulas to determine light modes, i.e., refractive indices and\npolarization vectors for a given propagation direction. We also work out the\nlight modes of the mentioned Lagrangians for an arbitrarily strong magnetic\nfield. The 3+1 formulation advanced in this paper has direct applications to\nthe current vacuum birefringence research: terrestrial experiments using\npermanent magnets/ultra-intense lasers for the subcritical regime and\nastrophysical observation of the x-rays from highly magnetized neutron stars\nfor the near-critical and supercritical regimes.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2022 00:13:03 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2024 05:55:04 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 04:57:32 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Kim", "Chul Min", ""], ["Kim", "Sang Pyo", ""]], "extracted_entities": [{"text": "Plebanski-type Lagrangians", "label": "LLMs"}, {"text": "Post-Maxwellian", "label": "LLMs"}, {"text": "Born-Infeld", "label": "LLMs"}, {"text": "ModMax", "label": "LLMs"}, {"text": "Heisenberg-Euler-Schwinger QED Lagrangians", "label": "LLMs"}, {"text": "Plebanski-type Lagrangians", "label": "LLMs"}]}
{"id": "2302.01856", "submitter": "Anda Skeja", "authors": "Anda Skeja and Sofia C. Olhede", "title": "Entropy of Exchangeable Random Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.CO math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying the complexity of large graphs requires measures that extend\nbeyond predefined structural features and scale efficiently with graph size.\nThis work adopts a generative perspective, modeling large networks as\nexchangeable graphs to quantify the information content of their generating\nmechanisms via graphon entropy. As a graph property, graphon entropy is\ninvariant under isomorphisms, making it an effective measure of complexity;\nhowever, it is not directly computable. To address this, we introduce a suite\nof graphon entropy estimators, including a nonparametric estimator for broad\napplicability and specialized versions for structured graphons arising from\nwell-studied random graph models such as Erd\\H{o}s-R\\'enyi, Chung-Lu, and\nstochastic block models. We establish their large-sample properties, deriving\nconvergence rates and Central Limit Theorems. Simulations illustrate how the\nnonparametric graphon entropy estimator captures structural variations in\ngraphs, while real-world applications demonstrate its role in characterizing\nevolving network dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2023 16:59:32 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2024 00:43:36 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 16:10:50 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Skeja", "Anda", ""], ["Olhede", "Sofia C.", ""]], "extracted_entities": [{"text": "Central Limit Theorems", "label": "Scaling law"}]}
{"id": "2303.17117", "submitter": "Chengliang Liu", "authors": "Chengliang Liu, Jie Wen, Yong Xu, Bob Zhang, Liqiang Nie, Min Zhang", "title": "Reliable Representation Learning for Incomplete Multi-View Missing\n  Multi-Label Classification", "comments": "Accepted by TPAMI. Please contact me if you have any questions:\n  liucl1996@163.com", "journal-ref": null, "doi": "10.1109/TPAMI.2025.3546356", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a cross-topic of multi-view learning and multi-label classification,\nmulti-view multi-label classification has gradually gained traction in recent\nyears. The application of multi-view contrastive learning has further\nfacilitated this process, however, the existing multi-view contrastive learning\nmethods crudely separate the so-called negative pair, which largely results in\nthe separation of samples belonging to the same category or similar ones.\nBesides, plenty of multi-view multi-label learning methods ignore the possible\nabsence of views and labels. To address these issues, in this paper, we propose\nan incomplete multi-view missing multi-label classification network named RANK.\nIn this network, a label-driven multi-view contrastive learning strategy is\nproposed to leverage supervised information to preserve the intra-view\nstructure and perform the cross-view consistency alignment. Furthermore, we\nbreak through the view-level weights inherent in existing methods and propose a\nquality-aware sub-network to dynamically assign quality scores to each view of\neach sample. The label correlation information is fully utilized in the final\nmulti-label cross-entropy classification loss, effectively improving the\ndiscriminative power. Last but not least, our model is not only able to handle\ncomplete multi-view multi-label data, but also works on datasets with missing\ninstances and labels. Extensive experiments confirm that our RANK outperforms\nexisting state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2023 03:09:25 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2024 03:22:08 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 09:20:24 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Liu", "Chengliang", ""], ["Wen", "Jie", ""], ["Xu", "Yong", ""], ["Zhang", "Bob", ""], ["Nie", "Liqiang", ""], ["Zhang", "Min", ""]], "extracted_entities": [{"text": "multi-view contrastive learning", "label": "Few-shot Learning"}, {"text": "multi-view contrastive learning", "label": "Few-shot Learning"}, {"text": "label-driven multi-view contrastive learning strategy", "label": "Few-shot Learning"}]}
{"id": "2304.08845", "submitter": "Yujie Yang", "authors": "Yujie Yang, Zhilong Zheng, Shengbo Eben Li, Wei Xu, Jingjing Liu,\n  Xianyuan Zhan, Ya-Qin Zhang", "title": "Feasible Policy Iteration for Safe Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safety is the priority concern when applying reinforcement learning (RL)\nalgorithms to real-world control problems. While policy iteration provides a\nfundamental algorithm for standard RL, an analogous theoretical algorithm for\nsafe RL remains absent. In this paper, we propose feasible policy iteration\n(FPI), the first foundational dynamic programming algorithm for safe RL. FPI\nalternates between policy evaluation, region identification and policy\nimprovement. This follows actor-critic-scenery (ACS) framework where scenery\nrefers to a feasibility function that represents a feasible region. A\nregion-wise update rule is developed for the policy improvement step, which\nmaximizes state-value function inside the feasible region and minimizes\nfeasibility function outside it. With this update rule, FPI guarantees\nmonotonic expansion of feasible region, monotonic improvement of state-value\nfunction, and geometric convergence to the optimal safe policy. Experimental\nresults demonstrate that FPI achieves strictly zero constraint violation on\nlow-dimensional tasks and outperforms existing methods in constraint adherence\nand reward performance on high-dimensional tasks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2023 09:18:37 GMT"}, {"version": "v2", "created": "Sun, 28 Jan 2024 10:13:02 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 09:53:02 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Yang", "Yujie", ""], ["Zheng", "Zhilong", ""], ["Li", "Shengbo Eben", ""], ["Xu", "Wei", ""], ["Liu", "Jingjing", ""], ["Zhan", "Xianyuan", ""], ["Zhang", "Ya-Qin", ""]], "extracted_entities": [{"text": "scenery", "label": "contextual Embedding"}]}
{"id": "2306.08210", "submitter": "Shuyi Chen", "authors": "Shuyi Chen, Kaize Ding, Shixiang Zhu", "title": "Uncertainty-Aware Robust Learning on Noisy Graphs", "comments": "ICASSP 2025 camera ready", "journal-ref": "ICASSP 2025 - IEEE International Conference on Acoustics, Speech,\n  and Signal Processing", "doi": "10.1109/ICASSP49660.2025.10888672", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) have excelled in various graph learning tasks,\nparticularly node classification. However, their performance is often hampered\nby noisy measurements in real-world graphs, which can corrupt critical patterns\nin the data. To address this, we propose a novel uncertainty-aware graph\nlearning framework inspired by distributionally robust optimization.\nSpecifically, we use a graph neural network-based encoder to embed the node\nfeatures and find the optimal node embeddings by minimizing the worst-case risk\nthrough a minimax formulation. Such an uncertainty-aware learning process leads\nto improved node representations and a more robust graph predictive model that\neffectively mitigates the impact of uncertainty arising from data noise. Our\nexperimental results demonstrate superior predictive performance over baselines\nacross noisy scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2023 02:45:14 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 14:30:06 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chen", "Shuyi", ""], ["Ding", "Kaize", ""], ["Zhu", "Shixiang", ""]], "extracted_entities": [{"text": "distributionally robust optimization", "label": "Fine-tuning"}, {"text": "node embeddings", "label": "Embedding"}, {"text": "minimax formulation", "label": "Fine-tuning"}]}
{"id": "2307.03363", "submitter": "Yuyuan Li", "authors": "Yuyuan Li, Jiaming Zhang, Yixiu Liu, Chaochao Chen", "title": "Class-wise Federated Unlearning: Harnessing Active Forgetting with\n  Teacher-Student Memory Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy concerns associated with machine learning models have driven research\ninto machine unlearning, which aims to erase the memory of specific target\ntraining data from already trained models. This issue also arises in federated\nlearning, creating the need to address the federated unlearning problem.\nHowever, federated unlearning remains a challenging task. On the one hand,\ncurrent research primarily focuses on unlearning all data from a client,\noverlooking more fine-grained unlearning targets, e.g., class-wise and\nsample-wise removal. On the other hand, existing methods suffer from imprecise\nestimation of data influence and impose significant computational or storage\nburden. To address these issues, we propose a neuro-inspired federated\nunlearning framework based on active forgetting, which is independent of model\narchitectures and suitable for fine-grained unlearning targets. Our framework\ndistinguishes itself from existing methods by utilizing new memories to\noverwrite old ones. These new memories are generated through teacher-student\nlearning. We further utilize refined elastic weight consolidation to mitigate\ncatastrophic forgetting of non-target data. Extensive experiments on benchmark\ndatasets demonstrate the efficiency and effectiveness of our method, achieving\nsatisfactory unlearning completeness against backdoor attacks.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2023 03:07:26 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 15:10:10 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Yuyuan", ""], ["Zhang", "Jiaming", ""], ["Liu", "Yixiu", ""], ["Chen", "Chaochao", ""]], "extracted_entities": [{"text": "teacher-student\nlearning", "label": "Few-shot Learning"}]}
{"id": "2308.00137", "submitter": "Hemn Abdalla", "authors": "Hemn Barzan Abdalla, Awder Ahmed, Bahtiyar Mehmed, Mehdi Gheisari,\n  Maryam Cheraghy, Yang Liu", "title": "An Efficient Recommendation System in E-commerce using Passer learning\n  optimization based on Bi-LSTM", "comments": "22 pages, 5 figuers, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reviews play a crucial role in shaping consumer decisions, especially\nin the context of e-commerce. However, the quality and reliability of these\nreviews can vary significantly. Some reviews contain misleading or unhelpful\ninformation, such as advertisements, fake content, or irrelevant details. These\nissues pose significant challenges for recommendation systems, which rely on\nuser-generated reviews to provide personalized suggestions. This article\nintroduces a recommendation system based on Passer Learning\nOptimization-enhanced Bi-LSTM classifier applicable to e-commerce\nrecommendation systems with improved accuracy and efficiency compared to\nstate-of-the-art models. It achieves as low as 1.24% MSE on the baby dataset.\nThis lifts it as high as 88.58%. Besides, there is also robust performance of\nthe system on digital music and patio lawn garden datasets at F1 of 88.46% and\n92.51%, correspondingly. These results, made possible by advanced graph\nembedding for effective knowledge extraction and fine-tuning of classifier\nparameters, establish the suitability of the proposed model in various\ne-commerce environments.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2023 20:09:25 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2023 07:34:05 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 14:43:36 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Abdalla", "Hemn Barzan", ""], ["Ahmed", "Awder", ""], ["Mehmed", "Bahtiyar", ""], ["Gheisari", "Mehdi", ""], ["Cheraghy", "Maryam", ""], ["Liu", "Yang", ""]], "extracted_entities": [{"text": "advanced graph\nembedding", "label": "Embedding"}, {"text": "fine-tuning of classifier\nparameters", "label": "Fine-tuning"}]}
{"id": "2310.02883", "submitter": "Adel Magra", "authors": "Adel Magra, Aad van der Vaart, Harry van Zanten", "title": "Semi-parametric Bernstein-von Mises in Linear Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Bayesian approach for the recovery of scalar parameters arising\nin inverse problems. We consider a general signal-in white noise model where we\nhave access to two independent noisy observations of a function, and of a\nlinear transformation of the function. The linear operator is unknown up to a\nscalar parameter. We present a Bernstein-von Mises theorem for the marginal\nposterior of the scalar under regularity assumptions of the operator. We\nfurther derive Bernstein-von Mises results for different priors and apply them\nto two concrete examples: the recovery of the thermal diffusivity in a heat\nequation problem, and the recovery of a location parameter in a semi-blind\ndeconvolution problem.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2023 15:18:17 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2024 12:08:56 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 12:05:47 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Magra", "Adel", ""], ["van der Vaart", "Aad", ""], ["van Zanten", "Harry", ""]], "extracted_entities": [{"text": "linear operator", "label": "Scaling law"}]}
{"id": "2311.04830", "submitter": "Julian Lemmel", "authors": "Julian Lemmel, Radu Grosu", "title": "Real-Time Recurrent Reinforcement Learning", "comments": "14 pages, 9 figures, includes Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a biologically plausible RL framework for solving tasks in\npartially observable Markov decision processes (POMDPs). The proposed algorithm\ncombines three integral parts: (1) A Meta-RL architecture, resembling the\nmammalian basal ganglia; (2) A biologically plausible reinforcement learning\nalgorithm, exploiting temporal difference learning and eligibility traces to\ntrain the policy and the value-function; (3) An online automatic\ndifferentiation algorithm for computing the gradients with respect to\nparameters of a shared recurrent network backbone. Our experimental results\nshow that the method is capable of solving a diverse set of partially\nobservable reinforcement learning tasks. The algorithm we call real-time\nrecurrent reinforcement learning (RTRRL) serves as a model of learning in\nbiological neural networks, mimicking reward pathways in the basal ganglia.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2023 16:56:16 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2024 10:30:57 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 10:19:32 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Lemmel", "Julian", ""], ["Grosu", "Radu", ""]], "extracted_entities": [{"text": "temporal difference learning", "label": "Few-shot Learning"}, {"text": "eligibility traces", "label": "Few-shot Learning"}]}
{"id": "2311.05535", "submitter": "Nicholas Rivera", "authors": "Shiekh Zia Uddin, Nicholas Rivera, Devin Seyler, Jamison Sloan,\n  Yannick Salamin, Charles Roques-Carmes, Shutao Xu, Michelle Sander, Ido\n  Kaminer, and Marin Soljacic", "title": "Noise-immune quantum correlations of intense light", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lasers with high intensity generally exhibit strong intensity fluctuations\nfar above the shot-noise level. Taming this noise is pivotal to a wide range of\napplications, both classical and quantum. Here, we demonstrate the creation of\nintense light with quantum levels of noise even when starting from inputs with\nlarge amounts of excess noise. In particular, we demonstrate how intense\nsqueezed light with intensities approaching 0.1 TW/cm^2, but noise at or below\nthe shot noise level, can be produced from noisy inputs associated with\nhigh-power amplified laser sources (an overall noise-reduction of 30-fold).\nBased on a new theory of quantum noise in multimode systems, we show that the\nability to generate quantum light from noisy inputs results from multimode\nquantum correlations, which maximally decouple the output light from the\ndominant noise channels in the input light. As an example, we demonstrate this\neffect for femtosecond pulses in nonlinear fibers, but the noise-immune\ncorrelations that enable our results are generic to many other nonlinear\nsystems in optics and beyond.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2023 17:26:56 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2023 14:29:12 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2024 10:08:18 GMT"}, {"version": "v4", "created": "Thu, 13 Mar 2025 02:11:43 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Uddin", "Shiekh Zia", ""], ["Rivera", "Nicholas", ""], ["Seyler", "Devin", ""], ["Sloan", "Jamison", ""], ["Salamin", "Yannick", ""], ["Roques-Carmes", "Charles", ""], ["Xu", "Shutao", ""], ["Sander", "Michelle", ""], ["Kaminer", "Ido", ""], ["Soljacic", "Marin", ""]], "extracted_entities": [{"text": "multimode\nquantum correlations", "label": "quantisation"}]}
{"id": "2312.09672", "submitter": "Zhongyi Zhou", "authors": "Zhongyi Zhou, Jing Jin, Vrushank Phadnis, Xiuxiu Yuan, Jun Jiang, Xun\n  Qian, Kristen Wright, Mark Sherwood, Jason Mayes, Jingtao Zhou, Yiyi Huang,\n  Zheng Xu, Yinda Zhang, Johnny Lee, Alex Olwal, David Kim, Ram Iyengar, Na Li,\n  Ruofei Du", "title": "InstructPipe: Generating Visual Blocks Pipelines with Human Instructions\n  and LLMs", "comments": "CHI 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual programming has the potential of providing novice programmers with a\nlow-code experience to build customized processing pipelines. Existing systems\ntypically require users to build pipelines from scratch, implying that novice\nusers are expected to set up and link appropriate nodes from a blank workspace.\nIn this paper, we introduce InstructPipe, an AI assistant for prototyping\nmachine learning (ML) pipelines with text instructions. We contribute two large\nlanguage model (LLM) modules and a code interpreter as part of our framework.\nThe LLM modules generate pseudocode for a target pipeline, and the interpreter\nrenders the pipeline in the node-graph editor for further human-AI\ncollaboration. Both technical and user evaluation (N=16) shows that\nInstructPipe empowers users to streamline their ML pipeline workflow, reduce\ntheir learning curve, and leverage open-ended commands to spark innovative\nideas.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2023 10:34:53 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2024 06:04:47 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 11:47:05 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhou", "Zhongyi", ""], ["Jin", "Jing", ""], ["Phadnis", "Vrushank", ""], ["Yuan", "Xiuxiu", ""], ["Jiang", "Jun", ""], ["Qian", "Xun", ""], ["Wright", "Kristen", ""], ["Sherwood", "Mark", ""], ["Mayes", "Jason", ""], ["Zhou", "Jingtao", ""], ["Huang", "Yiyi", ""], ["Xu", "Zheng", ""], ["Zhang", "Yinda", ""], ["Lee", "Johnny", ""], ["Olwal", "Alex", ""], ["Kim", "David", ""], ["Iyengar", "Ram", ""], ["Li", "Na", ""], ["Du", "Ruofei", ""]], "extracted_entities": [{"text": "InstructPipe", "label": "Open-source LLMs"}, {"text": "InstructPipe", "label": "Open-source LLMs"}]}
{"id": "2312.10052", "submitter": "Zhongliang Zeng", "authors": "Dongdong Li, Zhongliang Zeng, Zhe Wang, Hai Yang", "title": "ESTformer: Transformer Utilizing Spatiotemporal Dependencies for\n  Electroencaphalogram Super-resolution", "comments": "Accepted by Knowledge-Based Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Towards practical applications of Electroencephalography (EEG), lightweight\nacquisition devices garner significant attention. However, EEG channel\nselection methods are commonly data-sensitive and cannot establish a unified\nsound paradigm for EEG acquisition devices. Through reverse conceptualisation,\nwe formulated EEG applications in an EEG super-resolution (SR) manner, but\nsuffered from high computation costs, extra interpolation bias, and few\ninsights into spatiotemporal dependency modelling. To this end, we propose\nESTformer, an EEG SR framework that utilises spatiotemporal dependencies based\non the transformer. ESTformer applies positional encoding methods and a\nmultihead self-attention mechanism to the space and time dimensions, which can\nlearn spatial structural correlations and temporal functional variations.\nESTformer, with the fixed mask strategy, adopts a mask token to upsample\nlow-resolution (LR) EEG data in the case of disturbance from mathematical\ninterpolation methods. On this basis, we designed various transformer blocks to\nconstruct a spatial interpolation module (SIM) and a temporal reconstruction\nmodule (TRM). Finally, ESTformer cascades the SIM and TRM to capture and model\nthe spatiotemporal dependencies for EEG SR with fidelity. Extensive\nexperimental results on two EEG datasets show the effectiveness of ESTformer\nagainst previous state-of-the-art methods, demonstrating the versatility of the\nTransformer for EEG SR tasks. The superiority of the SR data was verified in an\nEEG-based person identification and emotion recognition task, achieving a 2% to\n38% improvement compared with the LR data at different sampling scales.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2023 12:26:32 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 07:17:58 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Dongdong", ""], ["Zeng", "Zhongliang", ""], ["Wang", "Zhe", ""], ["Yang", "Hai", ""]], "extracted_entities": [{"text": "multihead self-attention mechanism", "label": "Attention mechanism"}]}
{"id": "2312.15960", "submitter": "Jingyao Li", "authors": "Jingyao Li, Pengguang Chen, Bin Xia, Hong Xu, Jiaya Jia", "title": "MoTCoder: Elevating Large Language Models with Modular of Thought for\n  Challenging Programming Tasks", "comments": "Model: https://huggingface.co/JingyaoLi/MoTCoder-15B-v1.0. Code:\n  https://github.com/dvlab-research/MoTCoder", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large Language Models (LLMs) have showcased impressive capabilities in\nhandling straightforward programming tasks. However, their performance tends to\nfalter when confronted with more challenging programming problems. We observe\nthat conventional models often generate solutions as monolithic code blocks,\nrestricting their effectiveness in tackling intricate questions. To overcome\nthis limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a\npioneering framework for MoT instruction tuning, designed to promote the\ndecomposition of tasks into logical sub-tasks and sub-modules. Our\ninvestigations reveal that, through the cultivation and utilization of\nsub-modules, MoTCoder significantly improves both the modularity and\ncorrectness of the generated solutions, leading to substantial relative pass@1\nimprovements of 12.9% on APPS and 9.43% on CodeContests. Our codes are\navailable at https://github.com/dvlab-research/MoTCoder.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2023 08:49:57 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2024 10:33:32 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2024 06:24:12 GMT"}, {"version": "v4", "created": "Thu, 13 Mar 2025 05:36:12 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Jingyao", ""], ["Chen", "Pengguang", ""], ["Xia", "Bin", ""], ["Xu", "Hong", ""], ["Jia", "Jiaya", ""]], "extracted_entities": [{"text": "Large Language Models", "label": "Large Language Model"}]}
{"id": "2401.04295", "submitter": "Yohei Nishino", "authors": "Yohei Nishino, Stefan Danilishin, Yutaro Enomoto, Teng Zhang", "title": "Frequency-dependent squeezing for gravitational-wave detection through\n  quantum teleportation", "comments": "14 pages, 10 figures, 2 table", "journal-ref": "Phys. Rev. A (2024)", "doi": "10.1103/PhysRevA.110.022601", "report-no": null, "categories": "quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ground-based interferometric gravitational wave detectors are highly precise\nsensors for weak forces, limited in sensitivity across their detection band by\nquantum fluctuations of light. Current and future instruments address this\nlimitation by injecting frequency-dependent squeezed vacuum into the detection\nport, utilizing narrow-band, low-loss optical cavities for optimal rotation of\nthe squeezing ellipse at each signal frequency. This study introduces a novel\nscheme employing the principles of quantum teleportation and entangled states\nof light. It allows achieving broadband suppression of quantum noise in detuned\nsignal recycled-Fabry-Perot--Michelson interferometers, which is the baseline\ndesign of the low-frequency detector within the Einstein Telescope xylophone\ndetector, without requiring additional filter cavities or modifications to the\ncore optics of the main interferometer.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2024 00:26:25 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2024 15:10:53 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 06:58:08 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Nishino", "Yohei", ""], ["Danilishin", "Stefan", ""], ["Enomoto", "Yutaro", ""], ["Zhang", "Teng", ""]], "extracted_entities": [{"text": "quantum teleportation", "label": "quantisation"}]}
{"id": "2401.08265", "submitter": "Pedro Alc\\'azar Guerrero", "authors": "Pedro Alc\\'azar Guerrero, Viet-Hung Nguyen, Jorge Mart\\'inez Romeral,\n  Aron W. Cummings, Jos\\'e-Hugo Garcia, Jean-Christophe Charlier and Stephan\n  Roche", "title": "Disorder-Induced Delocalization in Magic-Angle Twisted Bilayer Graphene", "comments": "Main text: 7 pages, 4 figures. Sup .Mat.: 4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mes-hall", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flat bands in moir\\'e systems are exciting new playgrounds for the generation\nand study of exotic many-body physics phenomena in low-dimensional materials.\nSuch physics is attributed to the vanishing kinetic energy and strong spatial\nlocalization of the flat-band states. Here we use numerical simulations to\nexamine the electronic transport properties of such flat bands in magic-angle\ntwisted bilayer graphene in the presence of disorder. We find that while a\nconventional downscaling of the mean free path with increasing disorder\nstrength occurs at higher energies, in the flat bands the mean free path can\nactually increase with increasing disorder strength.This phenomenon is also\ncaptured by the disorder-dependent quantum metric, which is directly linked to\nthe ground state localization.This disorder-induced delocalization suggests\nthat weak disorder may have a strong impact on the exotic physics of\nmagic-angle bilayer graphene and other related moir\\'e systems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2024 10:39:04 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2024 14:16:11 GMT"}, {"version": "v3", "created": "Tue, 17 Dec 2024 14:01:55 GMT"}, {"version": "v4", "created": "Thu, 13 Mar 2025 10:30:47 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Guerrero", "Pedro Alc\u00e1zar", ""], ["Nguyen", "Viet-Hung", ""], ["Romeral", "Jorge Mart\u00ednez", ""], ["Cummings", "Aron W.", ""], ["Garcia", "Jos\u00e9-Hugo", ""], ["Charlier", "Jean-Christophe", ""], ["Roche", "Stephan", ""]], "extracted_entities": [{"text": "moir\\'e systems", "label": "Mistral"}, {"text": "moir\\'e systems", "label": "Mistral"}]}
{"id": "2401.12736", "submitter": "Dachong Li", "authors": "Dachong Li, Li Li, Zhuangzhuang Chen, Jianqiang Li", "title": "$ShiftwiseConv:$ Small Convolutional Kernel with Large Kernel Effect", "comments": "CVPR 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Large kernels make standard convolutional neural networks (CNNs) great again\nover transformer architectures in various vision tasks. Nonetheless, recent\nstudies meticulously designed around increasing kernel size have shown\ndiminishing returns or stagnation in performance. Thus, the hidden factors of\nlarge kernel convolution that affect model performance remain unexplored. In\nthis paper, we reveal that the key hidden factors of large kernels can be\nsummarized as two separate components: extracting features at a certain\ngranularity and fusing features by multiple pathways. To this end, we leverage\nthe multi-path long-distance sparse dependency relationship to enhance feature\nutilization via the proposed Shiftwise (SW) convolution operator with a pure\nCNN architecture. In a wide range of vision tasks such as classification,\nsegmentation, and detection, SW surpasses state-of-the-art transformers and CNN\narchitectures, including SLaK and UniRepLKNet. More importantly, our\nexperiments demonstrate that $3 \\times 3$ convolutions can replace large\nconvolutions in existing large kernel CNNs to achieve comparable effects, which\nmay inspire follow-up works. Code and all the models at\nhttps://github.com/lidc54/shift-wiseConv.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2024 13:13:45 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 09:35:17 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Dachong", ""], ["Li", "Li", ""], ["Chen", "Zhuangzhuang", ""], ["Li", "Jianqiang", ""]], "extracted_entities": [{"text": "SLaK", "label": "Transformers"}]}
{"id": "2401.16796", "submitter": "Weibin Liao", "authors": "Weibin Liao, Yinghao Zhu, Zhongji Zhang, Yuhang Wang, Zixiang Wang, Xu\n  Chu, Yasha Wang, Liantao Ma", "title": "Learnable Prompt as Pseudo-Imputation: Rethinking the Necessity of\n  Traditional EHR Data Imputation in Downstream Clinical Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing the health status of patients based on Electronic Health Records\n(EHR) is a fundamental research problem in medical informatics. The presence of\nextensive missing values in EHR makes it challenging for deep neural networks\n(DNNs) to directly model the patient's health status. Existing DNNs training\nprotocols, including Impute-then-Regress Procedure and Jointly Optimizing of\nImpute-n-Regress Procedure, require the additional imputation models to\nreconstruction missing values. However, Impute-then-Regress Procedure\nintroduces the risk of injecting imputed, non-real data into downstream\nclinical prediction tasks, resulting in power loss, biased estimation, and\npoorly performing models, while Jointly Optimizing of Impute-n-Regress\nProcedure is also difficult to generalize due to the complex optimization space\nand demanding data requirements. Inspired by the recent advanced literature of\nlearnable prompt in the fields of NLP and CV, in this work, we rethought the\nnecessity of the imputation model in downstream clinical tasks, and proposed\nLearnable Prompt as Pseudo-Imputation (PAI) as a new training protocol to\nassist EHR analysis. PAI no longer introduces any imputed data but constructs a\nlearnable prompt to model the implicit preferences of the downstream model for\nmissing values, resulting in a significant performance improvement for all\nstate-of-the-arts EHR analysis models on four real-world datasets across two\nclinical prediction tasks. Further experimental analysis indicates that PAI\nexhibits higher robustness in situations of data insufficiency and high missing\nrates. More importantly, as a plug-and-play protocol, PAI can be easily\nintegrated into any existing or even imperceptible future EHR analysis models.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2024 07:19:36 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 06:17:29 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Liao", "Weibin", ""], ["Zhu", "Yinghao", ""], ["Zhang", "Zhongji", ""], ["Wang", "Yuhang", ""], ["Wang", "Zixiang", ""], ["Chu", "Xu", ""], ["Wang", "Yasha", ""], ["Ma", "Liantao", ""]], "extracted_entities": [{"text": "Jointly Optimizing of\nImpute-n-Regress Procedure", "label": "Prompting"}, {"text": "learnable prompt", "label": "Prompting"}, {"text": "Learnable Prompt", "label": "Prompting"}, {"text": "PAI", "label": "Prompting"}, {"text": "PAI", "label": "Prompting"}, {"text": "learnable prompt", "label": "Prompting"}, {"text": "PAI", "label": "Prompting"}, {"text": "PAI", "label": "Prompting"}]}
{"id": "2402.04863", "submitter": "Yingjie Mao", "authors": "Xiaoqi Li, Yingjie Mao, Zexin Lu, Wenkai Li, Zongwei Li", "title": "SCLA: Automated Smart Contract Summarization via LLMs and Control Flow\n  Prompt", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart contract code summarization is crucial for efficient maintenance and\nvulnerability mitigation. While many studies use Large Language Models (LLMs)\nfor summarization, their performance still falls short compared to fine-tuned\nmodels like CodeT5+ and CodeBERT. Some approaches combine LLMs with data flow\nanalysis but fail to fully capture the hierarchy and control structures of the\ncode, leading to information loss and degraded summarization quality. We\npropose SCLA, an LLM-based method that enhances summarization by integrating a\nControl Flow Graph (CFG) and semantic facts from the code's control flow into a\nsemantically enriched prompt. SCLA uses a control flow extraction algorithm to\nderive control flows from semantic nodes in the Abstract Syntax Tree (AST) and\nconstructs the corresponding CFG. Code semantic facts refer to both explicit\nand implicit information within the AST that is relevant to smart contracts.\nThis method enables LLMs to better capture the structural and contextual\ndependencies of the code. We validate the effectiveness of SCLA through\ncomprehensive experiments on a dataset of 40,000 real-world smart contracts.\nThe experiment shows that SCLA significantly improves summarization quality,\noutperforming the SOTA baselines with improvements of 26.7%, 23.2%, 16.7%, and\n14.7% in BLEU-4, METEOR, ROUGE-L, and BLEURT scores, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2024 13:58:26 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2024 06:09:16 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2024 14:18:32 GMT"}, {"version": "v4", "created": "Sat, 17 Aug 2024 03:41:42 GMT"}, {"version": "v5", "created": "Tue, 20 Aug 2024 02:34:56 GMT"}, {"version": "v6", "created": "Thu, 13 Mar 2025 07:05:15 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Xiaoqi", ""], ["Mao", "Yingjie", ""], ["Lu", "Zexin", ""], ["Li", "Wenkai", ""], ["Li", "Zongwei", ""]], "extracted_entities": [{"text": "Large Language Models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "CodeT5+", "label": "Transformer-based model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "SCLA", "label": "LLM-based"}, {"text": "semantically enriched prompt", "label": "Prompting"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "SCLA", "label": "LLM-based"}, {"text": "SCLA", "label": "LLM-based"}]}
{"id": "2402.08382", "submitter": "Junghyun Min", "authors": "Junghyun Min, Minho Lee, Woochul Lee, Yeonsoo Lee", "title": "Punctuation restoration improves structure understanding without\n  supervision", "comments": "11 pages, 1 figure, 6 tables. RepL4NLP 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning objectives like autoregressive and masked language\nmodeling constitute a significant part in producing pre-trained representations\nthat perform various downstream applications from natural language\nunderstanding to conversational tasks. However, despite impressive generative\ncapabilities of recent large language models, their abilities to capture\nsyntactic or semantic structure within text lag behind. We hypothesize that the\nmismatch between linguistic performance and competence in machines is\nattributable to insufficient learning of linguistic structure knowledge via\ncurrently popular pre-training objectives. Working with English, we show that\npunctuation restoration as a learning objective improves performance on\nstructure-related tasks like named entity recognition, open information\nextraction, chunking, and part-of-speech tagging. Punctuation restoration\nresults in $\\blacktriangle$$\\geq2\\%$p improvement in 16 out of 18 experiments,\nacross 6 out of 7 tasks. Our results show that punctuation restoration is an\neffective learning objective that can improve structure understanding and yield\na more robust structure-aware representations of natural language in base-sized\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2024 11:22:52 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2024 08:35:57 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 03:32:50 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Min", "Junghyun", ""], ["Lee", "Minho", ""], ["Lee", "Woochul", ""], ["Lee", "Yeonsoo", ""]], "extracted_entities": [{"text": "open information\nextraction", "label": "Embedding"}, {"text": "chunking", "label": "contextual Embedding"}, {"text": "part-of-speech tagging", "label": "Embedding"}]}
{"id": "2402.11057", "submitter": "Shijia Feng", "authors": "Shijia Feng, Michael Wray, Brian Sullivan, Youngkyoon Jang, Casimir\n  Ludwig, Iain Gilchrist, Walterio Mayol-Cuevas", "title": "Are you Struggling? Dataset and Baselines for Struggle Determination in\n  Assembly Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Determining when people are struggling from video enables a finer-grained\nunderstanding of actions and opens opportunities for building intelligent\nsupport visual interfaces. In this paper, we present a new dataset with three\nassembly activities and corresponding performance baselines for the\ndetermination of struggle from video. Three real-world problem-solving\nactivities including assembling plumbing pipes (Pipes-Struggle), pitching\ncamping tents (Tent-Struggle) and solving the Tower of Hanoi puzzle\n(Tower-Struggle) are introduced. Video segments were scored w.r.t. the level of\nstruggle as perceived by annotators using a forced choice 4-point scale. Each\nvideo segment was annotated by a single expert annotator in addition to\ncrowd-sourced annotations. The dataset is the first struggle annotation dataset\nand contains 5.1 hours of video and 725,100 frames from 73 participants in\ntotal. We evaluate three decision-making tasks: struggle classification,\nstruggle level regression, and struggle label distribution learning. We provide\nbaseline results for each of the tasks utilising several mainstream deep neural\nnetworks, along with an ablation study and visualisation of results. Our work\nis motivated toward assistive systems that analyze struggle, support users\nduring manual activities and encourage learning, as well as other video\nunderstanding competencies.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2024 20:12:33 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2024 16:42:12 GMT"}, {"version": "v3", "created": "Wed, 12 Mar 2025 03:46:20 GMT"}, {"version": "v4", "created": "Thu, 13 Mar 2025 14:08:10 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Feng", "Shijia", ""], ["Wray", "Michael", ""], ["Sullivan", "Brian", ""], ["Jang", "Youngkyoon", ""], ["Ludwig", "Casimir", ""], ["Gilchrist", "Iain", ""], ["Mayol-Cuevas", "Walterio", ""]], "extracted_entities": [{"text": "forced choice 4-point scale", "label": "Scaling law"}, {"text": "struggle label distribution learning", "label": "Few-shot Learning"}]}
{"id": "2403.00397", "submitter": "Felipe Garrido-Lucero", "authors": "R\\'emi Castera, Felipe Garrido-Lucero, Patrick Loiseau, Simon Mauras,\n  Mathieu Molina, Vianney Perchet", "title": "The Price of Opportunity Fairness in Matroid Allocation Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider matroid allocation problems under opportunity fairness\nconstraints: resources need to be allocated to a set of agents under matroid\nconstraints (which includes classical problems such as bipartite matching).\nAgents are divided into C groups according to a sensitive attribute, and an\nallocation is opportunity-fair if each group receives the same share\nproportional to the maximum feasible allocation it could achieve in isolation.\nWe study the Price of Fairness (PoF), i.e., the ratio between maximum size\nallocations and maximum size opportunity-fair allocations. We first provide a\ncharacterization of the PoF leveraging the underlying polymatroid structure of\nthe allocation problem. Based on this characterization, we prove bounds on the\nPoF in various settings from fully adversarial (wort-case) to fully random.\nNotably, one of our main results considers an arbitrary matroid structure with\nagents randomly divided into groups. In this setting, we prove a PoF bound as a\nfunction of the size of the largest group. Our result implies that, as long as\nthere is no dominant group (i.e., the largest group is not too large),\nopportunity fairness constraints do not induce any loss of social welfare\n(defined as the allocation size). Overall, our results give insights into which\naspects of the problem's structure affect the trade-off between opportunity\nfairness and social welfare.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2024 09:36:36 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 16:03:37 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Castera", "R\u00e9mi", ""], ["Garrido-Lucero", "Felipe", ""], ["Loiseau", "Patrick", ""], ["Mauras", "Simon", ""], ["Molina", "Mathieu", ""], ["Perchet", "Vianney", ""]], "extracted_entities": [{"text": "PoF", "label": "Model Bias and Fairness"}, {"text": "PoF", "label": "Model Bias and Fairness"}, {"text": "PoF", "label": "Model Bias and Fairness"}]}
{"id": "2403.03185", "submitter": "Cassidy Laidlaw", "authors": "Cassidy Laidlaw, Shivam Singhal, Anca Dragan", "title": "Correlated Proxies: A New Definition and Improved Mitigation for Reward\n  Hacking", "comments": "Spotlight at ICLR 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because it is difficult to precisely specify complex objectives,\nreinforcement learning policies are often optimized using proxy reward\nfunctions that only approximate the true goal. However, optimizing proxy\nrewards frequently leads to reward hacking: the optimized reward function\nceases to be a good proxy and the resulting policy performs poorly with respect\nto the unspecified true reward. Principled solutions to reward hacking have\nbeen impeded by the lack of a good definition for the problem. To address this\ngap, we introduce a definition of reward hacking based on the correlation\nbetween proxy and true rewards for states and actions seen by a \"reference\npolicy\" that breaks down under optimization. We show that this definition\ncaptures reward hacking behavior across several realistic settings, including\nin reinforcement learning from human feedback (RLHF). Using our formulation, we\nshow theoretically that regularization to the reference policy can effectively\nprevent reward hacking. While the current practice in RLHF applies a KL penalty\nbetween action distributions for this purpose, our theory suggests regularizing\nthe $\\chi^2$ divergence between the policies' occupancy measures can be more\neffective. We intuitively show the benefits of this type of regularization and\ndemonstrate that it better mitigates reward hacking in practice across four\nrealistic settings, including RLHF. Our code is available at\nhttps://github.com/cassidylaidlaw/orpo.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2024 18:22:15 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2024 17:52:57 GMT"}, {"version": "v3", "created": "Wed, 18 Dec 2024 05:51:35 GMT"}, {"version": "v4", "created": "Thu, 13 Mar 2025 17:35:13 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Laidlaw", "Cassidy", ""], ["Singhal", "Shivam", ""], ["Dragan", "Anca", ""]], "extracted_entities": [{"text": "reinforcement learning from human feedback", "label": "Few-shot Learning"}, {"text": "RLHF", "label": "Few-shot Learning"}, {"text": "RLHF", "label": "Few-shot Learning"}]}
{"id": "2403.05158", "submitter": "Zuguang Li", "authors": "Zuguang Li, Wen Wu, Shaohua Wu, and Wei Wang", "title": "Adaptive Split Learning over Energy-Constrained Wireless Edge Networks", "comments": "6 pages, 5 figures, 20 conferences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Split learning (SL) is a promising approach for training artificial\nintelligence (AI) models, in which devices collaborate with a server to train\nan AI model in a distributed manner, based on a same fixed split point.\nHowever, due to the device heterogeneity and variation of channel conditions,\nthis way is not optimal in training delay and energy consumption. In this\npaper, we design an adaptive split learning (ASL) scheme which can dynamically\nselect split points for devices and allocate computing resource for the server\nin wireless edge networks. We formulate an optimization problem to minimize the\naverage training latency subject to long-term energy consumption constraint.\nThe difficulties in solving this problem are the lack of future information and\nmixed integer programming (MIP). To solve it, we propose an online algorithm\nleveraging the Lyapunov theory, named OPEN, which decomposes it into a new MIP\nproblem only with the current information. Then, a two-layer optimization\nmethod is proposed to solve the MIP problem. Extensive simulation results\ndemonstrate that the ASL scheme can reduce the average training delay and\nenergy consumption by 53.7% and 22.1%, respectively, as compared to the\nexisting SL schemes.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2024 08:51:37 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 13:27:47 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Zuguang", ""], ["Wu", "Wen", ""], ["Wu", "Shaohua", ""], ["Wang", "Wei", ""]], "extracted_entities": [{"text": "Split learning", "label": "Few-shot Learning"}]}
{"id": "2403.08277", "submitter": "MinSoo Kim", "authors": "Minsoo Kim, Min-Cheol Sagong, Gi Pyo Nam, Junghyun Cho, and Ig-Jae Kim", "title": "VIGFace: Virtual Identity Generation for Privacy-Free Face Recognition", "comments": "Please refer to version 3 if you are citing this paper. Major\n  updates: (1)Test utilities updated: use AdaFace code. (2)Training method\n  updated: AdaFace+IR-SE50", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning-based face recognition continues to face challenges due to its\nreliance on huge datasets obtained from web crawling, which can be costly to\ngather and raise significant real-world privacy concerns. To address this\nissue, we propose VIGFace, a novel framework capable of generating synthetic\nfacial images. Our idea originates from pre-assigning virtual identities in the\nfeature space. Initially, we train the face recognition model using a real face\ndataset and create a feature space for both real and virtual identities, where\nvirtual prototypes are orthogonal to other prototypes. Subsequently, we train\nthe diffusion model based on the established feature space, enabling it to\ngenerate authentic human face images from real prototypes and synthesize\nvirtual face images from virtual prototypes. Our proposed framework provides\ntwo significant benefits. Firstly, it shows clear separability between existing\nindividuals and virtual face images, allowing one to create synthetic images\nwith confidence and without concerns about privacy and portrait rights.\nSecondly, it ensures improved performance through data augmentation by\nincorporating real existing images. Extensive experiments demonstrate the\nsuperiority of our virtual face dataset and framework, outperforming the\nprevious state-of-the-art on various face recognition benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2024 06:11:41 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2024 02:15:40 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 08:06:24 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Kim", "Minsoo", ""], ["Sagong", "Min-Cheol", ""], ["Nam", "Gi Pyo", ""], ["Cho", "Junghyun", ""], ["Kim", "Ig-Jae", ""]], "extracted_entities": [{"text": "privacy concerns", "label": "AI Ethics"}, {"text": "portrait rights", "label": "AI Ethics"}]}
{"id": "2403.11848", "submitter": "Lin Liu", "authors": "Ziying Song, Lei Yang, Shaoqing Xu, Lin Liu, Dongyang Xu, Caiyan Jia,\n  Feiyang Jia, Li Wang", "title": "GraphBEV: Towards Robust BEV Feature Alignment for Multi-Modal 3D Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating LiDAR and camera information into Bird's-Eye-View (BEV)\nrepresentation has emerged as a crucial aspect of 3D object detection in\nautonomous driving. However, existing methods are susceptible to the inaccurate\ncalibration relationship between LiDAR and the camera sensor. Such inaccuracies\nresult in errors in depth estimation for the camera branch, ultimately causing\nmisalignment between LiDAR and camera BEV features. In this work, we propose a\nrobust fusion framework called Graph BEV. Addressing errors caused by\ninaccurate point cloud projection, we introduce a Local Align module that\nemploys neighbor-aware depth features via Graph matching. Additionally, we\npropose a Global Align module to rectify the misalignment between LiDAR and\ncamera BEV features. Our Graph BEV framework achieves state-of-the-art\nperformance, with an mAP of 70.1\\%, surpassing BEV Fusion by 1.6\\% on the\nnuscenes validation set. Importantly, our Graph BEV outperforms BEV Fusion by\n8.3\\% under conditions with misalignment noise.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2024 15:00:38 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2024 04:05:24 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2024 12:16:31 GMT"}, {"version": "v4", "created": "Thu, 13 Mar 2025 06:23:17 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Song", "Ziying", ""], ["Yang", "Lei", ""], ["Xu", "Shaoqing", ""], ["Liu", "Lin", ""], ["Xu", "Dongyang", ""], ["Jia", "Caiyan", ""], ["Jia", "Feiyang", ""], ["Wang", "Li", ""]], "extracted_entities": [{"text": "Graph matching", "label": "Embedding"}]}
{"id": "2403.14404", "submitter": "Jan-Hendrik Bastek", "authors": "Jan-Hendrik Bastek, WaiChing Sun, Dennis M. Kochmann", "title": "Physics-Informed Diffusion Models", "comments": "26 pages, 9 figures, 3 tables; ICLR 2025 camera ready contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models such as denoising diffusion models are quickly advancing\ntheir ability to approximate highly complex data distributions. They are also\nincreasingly leveraged in scientific machine learning, where samples from the\nimplied data distribution are expected to adhere to specific governing\nequations. We present a framework that unifies generative modeling and partial\ndifferential equation fulfillment by introducing a first-principle-based loss\nterm that enforces generated samples to fulfill the underlying physical\nconstraints. Our approach reduces the residual error by up to two orders of\nmagnitude compared to previous work in a fluid flow case study and outperforms\ntask-specific frameworks in relevant metrics for structural topology\noptimization. We also present numerical evidence that our extended training\nobjective acts as a natural regularization mechanism against overfitting. Our\nframework is simple to implement and versatile in its applicability for\nimposing equality and inequality constraints as well as auxiliary optimization\nobjectives.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2024 13:52:55 GMT"}, {"version": "v2", "created": "Thu, 23 May 2024 09:34:29 GMT"}, {"version": "v3", "created": "Sun, 16 Feb 2025 14:11:01 GMT"}, {"version": "v4", "created": "Thu, 13 Mar 2025 08:07:40 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Bastek", "Jan-Hendrik", ""], ["Sun", "WaiChing", ""], ["Kochmann", "Dennis M.", ""]], "extracted_entities": [{"text": "scientific machine learning", "label": "Few-shot Learning"}]}
{"id": "2404.02175", "submitter": "Javier Mar\\'in", "authors": "Javier Marin", "title": "Symmetries, Scaling Laws and Phase Transitions in Consumer Advertising\n  Response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.LG q-fin.GN", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding how consumers respond to business advertising efforts is\nessential for optimizing marketing investment. This research introduces a new\nmodeling approach based on the concepts of symmetries and scaling laws in\nphysics to describe consumer response to advertising dynamics. Drawing from\nmathematical frameworks used in physics and social sciences, we propose a model\nthat accounts for a key aspect: the saturation effect. The model is validated\nagainst commonly used models, including the Michaelis-Menten and Hill\nequations, showing its ability to better capture nonlinearities in advertising\neffects. We introduce new key parameters like Marketing Sensitivity, Response\nSensitivity, and Behavioral Sensitivit, that offer additional insights into the\ndrivers of audience engagement and advertising performance. Our model provides\na rigorous yet practical tool for understanding audience behavior, contributing\nto the improvement of budget allocation strategies.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2024 11:23:31 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2024 06:33:19 GMT"}, {"version": "v3", "created": "Sun, 10 Nov 2024 10:10:44 GMT"}, {"version": "v4", "created": "Tue, 11 Mar 2025 18:32:02 GMT"}, {"version": "v5", "created": "Thu, 13 Mar 2025 08:48:26 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Marin", "Javier", ""]], "extracted_entities": [{"text": "scaling laws", "label": "Scaling law"}]}
{"id": "2404.04104", "submitter": "George Retsinas", "authors": "George Retsinas, Panagiotis P. Filntisis, Radek Danecek, Victoria F.\n  Abrevaya, Anastasios Roussos, Timo Bolkart, Petros Maragos", "title": "SMIRK: 3D Facial Expressions through Analysis-by-Neural-Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While existing methods for 3D face reconstruction from in-the-wild images\nexcel at recovering the overall face shape, they commonly miss subtle, extreme,\nasymmetric, or rarely observed expressions. We improve upon these methods with\nSMIRK (Spatial Modeling for Image-based Reconstruction of Kinesics), which\nfaithfully reconstructs expressive 3D faces from images. We identify two key\nlimitations in existing methods: shortcomings in their self-supervised training\nformulation, and a lack of expression diversity in the training images. For\ntraining, most methods employ differentiable rendering to compare a predicted\nface mesh with the input image, along with a plethora of additional loss\nfunctions. This differentiable rendering loss not only has to provide\nsupervision to optimize for 3D face geometry, camera, albedo, and lighting,\nwhich is an ill-posed optimization problem, but the domain gap between\nrendering and input image further hinders the learning process. Instead, SMIRK\nreplaces the differentiable rendering with a neural rendering module that,\ngiven the rendered predicted mesh geometry, and sparsely sampled pixels of the\ninput image, generates a face image. As the neural rendering gets color\ninformation from sampled image pixels, supervising with neural rendering-based\nreconstruction loss can focus solely on the geometry. Further, it enables us to\ngenerate images of the input identity with varying expressions while training.\nThese are then utilized as input to the reconstruction model and used as\nsupervision with ground truth geometry. This effectively augments the training\ndata and enhances the generalization for diverse expressions. Our qualitative,\nquantitative and particularly our perceptual evaluations demonstrate that SMIRK\nachieves the new state-of-the art performance on accurate expression\nreconstruction. Project webpage: https://georgeretsi.github.io/smirk/.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2024 14:00:07 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 12:56:37 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Retsinas", "George", ""], ["Filntisis", "Panagiotis P.", ""], ["Danecek", "Radek", ""], ["Abrevaya", "Victoria F.", ""], ["Roussos", "Anastasios", ""], ["Bolkart", "Timo", ""], ["Maragos", "Petros", ""]], "extracted_entities": [{"text": "SMIRK", "label": "Neural Language Model"}]}
{"id": "2404.10775", "submitter": "Hongxin Zhang", "authors": "Hongxin Zhang, Zeyuan Wang, Qiushi Lyu, Zheyuan Zhang, Sunli Chen,\n  Tianmin Shu, Behzad Dariush, Kwonjoon Lee, Yilun Du, Chuang Gan", "title": "COMBO: Compositional World Models for Embodied Multi-Agent Cooperation", "comments": "Published at ICLR 2025. 24 pages. The first three authors contributed\n  equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we investigate the problem of embodied multi-agent\ncooperation, where decentralized agents must cooperate given only egocentric\nviews of the world. To effectively plan in this setting, in contrast to\nlearning world dynamics in a single-agent scenario, we must simulate world\ndynamics conditioned on an arbitrary number of agents' actions given only\npartial egocentric visual observations of the world. To address this issue of\npartial observability, we first train generative models to estimate the overall\nworld state given partial egocentric observations. To enable accurate\nsimulation of multiple sets of actions on this world state, we then propose to\nlearn a compositional world model for multi-agent cooperation by factorizing\nthe naturally composable joint actions of multiple agents and compositionally\ngenerating the video conditioned on the world state. By leveraging this\ncompositional world model, in combination with Vision Language Models to infer\nthe actions of other agents, we can use a tree search procedure to integrate\nthese modules and facilitate online cooperative planning. We evaluate our\nmethods on three challenging benchmarks with 2-4 agents. The results show our\ncompositional world model is effective and the framework enables the embodied\nagents to cooperate efficiently with different agents across various tasks and\nan arbitrary number of agents, showing the promising future of our proposed\nmethods. More videos can be found at https://embodied-agi.cs.umass.edu/combo/.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2024 17:59:11 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 14:56:47 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhang", "Hongxin", ""], ["Wang", "Zeyuan", ""], ["Lyu", "Qiushi", ""], ["Zhang", "Zheyuan", ""], ["Chen", "Sunli", ""], ["Shu", "Tianmin", ""], ["Dariush", "Behzad", ""], ["Lee", "Kwonjoon", ""], ["Du", "Yilun", ""], ["Gan", "Chuang", ""]], "extracted_entities": [{"text": "compositional world model", "label": "AI model"}]}
{"id": "2404.14812", "submitter": "Yufeng Zhang", "authors": "Yufeng Zhang, Xuepeng Wang, Lingxiang Wu, Jinqiao Wang", "title": "Enhancing Chain of Thought Prompting in Large Language Models via\n  Reasoning Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chain of Thought (CoT) prompting can encourage language models to engage in\nmulti-step logical reasoning. The quality of the provided demonstrations\nsignificantly influences the success of downstream inference tasks. Current\nunsupervised CoT methods primarily select examples based on the semantics of\nthe questions, which can introduce noise and lack interpretability. In this\npaper, we propose leveraging reasoning patterns to enhance CoT prompting\neffectiveness. Reasoning patterns represent the process by which language\nmodels arrive at their final results. By utilizing prior knowledge and\nprompt-based methods from large models, we first construct task-specific\npattern sets. We then select diverse demonstrations based on different\nreasoning patterns. This approach not only mitigates the impact of noise but\nalso provides explicit interpretability to help us understand the mechanisms of\nCoT. Extensive experiments demonstrate that our method is more robust and\nconsistently leads to improvements across various reasoning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2024 07:50:00 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 03:03:57 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhang", "Yufeng", ""], ["Wang", "Xuepeng", ""], ["Wu", "Lingxiang", ""], ["Wang", "Jinqiao", ""]], "extracted_entities": [{"text": "Chain of Thought", "label": "Chain of thought"}, {"text": "CoT", "label": "Chain of thought"}, {"text": "CoT prompting", "label": "Prompting"}, {"text": "Reasoning patterns", "label": "Chain of thought"}, {"text": "reasoning patterns", "label": "Chain of thought"}]}
{"id": "2404.16487", "submitter": "Maximilian Warsinke", "authors": "Maximilian Warsinke and Tanja Koji\\'c and Maurizio Vergari and Robert\n  Spang and Jan-Niklas Voigt-Antons and Sebastian M\\\"oller", "title": "Comparing Continuous and Retrospective Emotion Ratings in Remote VR\n  Study", "comments": "The paper was presented at QoMEX 2024", "journal-ref": null, "doi": "10.1109/QoMEX61742.2024.10598301", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates the feasibility of remote virtual reality (VR)\nstudies conducted at home using VR headsets and video conferencing by deploying\nan experiment on emotion ratings. 20 participants used head-mounted displays to\nimmerse themselves in 360{\\deg} videos selected to evoke emotional responses.\nThe research compares continuous ratings using a graphical interface to\nretrospective questionnaires on a digitized Likert Scale for measuring arousal\nand valence, both based on the self-assessment manikin (SAM). It was\nhypothesized that the two different rating methods would lead to significantly\ndifferent values for both valence and arousal. The goal was to investigate\nwhether continuous ratings during the experience would better reflect users'\nemotions compared to the post-questionnaire by mitigating biases such as the\npeak-end rule. The results show significant differences with moderate to strong\neffect sizes for valence and no significant differences for arousal with low to\nmoderate effect sizes. This indicates the need for further investigation of the\nmethods used to assess emotion ratings in VR studies. Overall, this study is an\nexample of a remotely conducted VR experiment, offering insights into methods\nfor emotion elicitation in VR by varying the timing and interface of the\nrating.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2024 10:19:44 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 14:56:45 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Warsinke", "Maximilian", ""], ["Koji\u0107", "Tanja", ""], ["Vergari", "Maurizio", ""], ["Spang", "Robert", ""], ["Voigt-Antons", "Jan-Niklas", ""], ["M\u00f6ller", "Sebastian", ""]], "extracted_entities": [{"text": "peak-end rule", "label": "Model Bias and Fairness"}, {"text": "valence", "label": "Model Bias and Fairness"}]}
{"id": "2404.17365", "submitter": "Fleur Hendriks", "authors": "Fleur Hendriks (1), Vlado Menkovski (1), Martin Do\\v{s}k\\'a\\v{r} (2),\n  Marc G. D. Geers (1), Ond\\v{r}ej Roko\\v{s} (1) ((1) Eindhoven University of\n  Technology, (2) Czech Technical University in Prague)", "title": "Similarity Equivariant Graph Neural Networks for Homogenization of\n  Metamaterials", "comments": "60 pages, 22 figures. Published in CMAME (Computer Methods in Applied\n  Mechanics and Engineering)", "journal-ref": null, "doi": "10.1016/j.cma.2025.117867", "report-no": null, "categories": "cond-mat.soft cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Soft, porous mechanical metamaterials exhibit pattern transformations that\nmay have important applications in soft robotics, sound reduction and\nbiomedicine. To design these innovative materials, it is important to be able\nto simulate them accurately and quickly, in order to tune their mechanical\nproperties. Since conventional simulations using the finite element method\nentail a high computational cost, in this article we aim to develop a machine\nlearning-based approach that scales favorably to serve as a surrogate model. To\nensure that the model is also able to handle various microstructures, including\nthose not encountered during training, we include the microstructure as part of\nthe network input. Therefore, we introduce a graph neural network that predicts\nglobal quantities (energy, stress stiffness) as well as the pattern\ntransformations that occur (the kinematics). To make our model as accurate and\ndata-efficient as possible, various symmetries are incorporated into the model.\nThe starting point is an E(n)-equivariant graph neural network (which respects\ntranslation, rotation and reflection) that has periodic boundary conditions\n(i.e., it is in-/equivariant with respect to the choice of RVE), is scale\nin-/equivariant, can simulate large deformations, and can predict scalars,\nvectors as well as second and fourth order tensors (specifically energy, stress\nand stiffness). The incorporation of scale equivariance makes the model\nequivariant with respect to the similarities group, of which the Euclidean\ngroup E(n) is a subgroup. We show that this network is more accurate and\ndata-efficient than graph neural networks with fewer symmetries. To create an\nefficient graph representation of the finite element discretization, we use\nonly the internal geometrical hole boundaries from the finite element mesh to\nachieve a better speed-up and scaling with the mesh size.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2024 12:30:32 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2024 15:10:19 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 14:48:27 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Hendriks", "Fleur", ""], ["Menkovski", "Vlado", ""], ["Do\u0161k\u00e1\u0159", "Martin", ""], ["Geers", "Marc G. D.", ""], ["Roko\u0161", "Ond\u0159ej", ""]], "extracted_entities": [{"text": "graph neural network", "label": "Neural Language Model"}, {"text": "graph neural network", "label": "Neural Language Model"}, {"text": "scale equivariance", "label": "Scaling law"}]}
{"id": "2405.00965", "submitter": "Sajjad Ghiasvand", "authors": "Sajjad Ghiasvand, Amirhossein Reisizadeh, Mahnoosh Alizadeh, Ramtin\n  Pedarsani", "title": "Robust Decentralized Learning with Local Updates and Gradient Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As distributed learning applications such as Federated Learning, the Internet\nof Things (IoT), and Edge Computing grow, it is critical to address the\nshortcomings of such technologies from a theoretical perspective. As an\nabstraction, we consider decentralized learning over a network of communicating\nclients or nodes and tackle two major challenges: data heterogeneity and\nadversarial robustness. We propose a decentralized minimax optimization method\nthat employs two important modules: local updates and gradient tracking.\nMinimax optimization is the key tool to enable adversarial training for\nensuring robustness. Having local updates is essential in Federated Learning\n(FL) applications to mitigate the communication bottleneck, and utilizing\ngradient tracking is essential to proving convergence in the case of data\nheterogeneity. We analyze the performance of the proposed algorithm,\nDec-FedTrack, in the case of nonconvex-strongly concave minimax optimization,\nand prove that it converges a stationary point. We also conduct numerical\nexperiments to support our theoretical findings.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2024 03:03:34 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 06:37:34 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Ghiasvand", "Sajjad", ""], ["Reisizadeh", "Amirhossein", ""], ["Alizadeh", "Mahnoosh", ""], ["Pedarsani", "Ramtin", ""]], "extracted_entities": [{"text": "Federated Learning", "label": "Few-shot Learning"}, {"text": "decentralized learning", "label": "Zero-shot Learning"}, {"text": "Federated Learning", "label": "Few-shot Learning"}]}
{"id": "2405.03636", "submitter": "Joshua Zhao", "authors": "Joshua C. Zhao, Saurabh Bagchi, Salman Avestimehr, Kevin S. Chan,\n  Somali Chaterji, Dimitris Dimitriadis, Jiacheng Li, Ninghui Li, Arash\n  Nourian, Holger R. Roth", "title": "The Federation Strikes Back: A Survey of Federated Learning Privacy\n  Attacks, Defenses, Applications, and Policy Landscape", "comments": "Accepted to ACM Computing Surveys; 35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown incredible potential across a wide array of tasks,\nand accompanied by this growth has been an insatiable appetite for data.\nHowever, a large amount of data needed for enabling deep learning is stored on\npersonal devices, and recent concerns on privacy have further highlighted\nchallenges for accessing such data. As a result, federated learning (FL) has\nemerged as an important privacy-preserving technology that enables\ncollaborative training of machine learning models without the need to send the\nraw, potentially sensitive, data to a central server. However, the fundamental\npremise that sending model updates to a server is privacy-preserving only holds\nif the updates cannot be \"reverse engineered\" to infer information about the\nprivate training data. It has been shown under a wide variety of settings that\nthis privacy premise does not hold. In this survey paper, we provide a\ncomprehensive literature review of the different privacy attacks and defense\nmethods in FL. We identify the current limitations of these attacks and\nhighlight the settings in which the privacy of ann FL client can be broken. We\nfurther dissect some of the successful industry applications of FL and draw\nlessons for future successful adoption. We survey the emerging landscape of\nprivacy regulation for FL and conclude with future directions for taking FL\ntoward the cherished goal of generating accurate models while preserving the\nprivacy of the data from its participants.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2024 16:55:20 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 14:38:58 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhao", "Joshua C.", ""], ["Bagchi", "Saurabh", ""], ["Avestimehr", "Salman", ""], ["Chan", "Kevin S.", ""], ["Chaterji", "Somali", ""], ["Dimitriadis", "Dimitris", ""], ["Li", "Jiacheng", ""], ["Li", "Ninghui", ""], ["Nourian", "Arash", ""], ["Roth", "Holger R.", ""]], "extracted_entities": [{"text": "privacy regulation", "label": "AI Ethics"}]}
{"id": "2405.04579", "submitter": "Joseph Janssen", "authors": "Joseph Janssen, Ardalan Tootchi, Ali A. Ameli", "title": "Tackling water table depth modeling via machine learning: From proxy\n  observations to verifiability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spatial patterns of water table depth (WTD) play a crucial role in shaping\necological resilience, hydrological connectivity, and human-centric systems.\nGenerally, a large-scale (e.g., continental or global) continuous map of static\nWTD can be simulated using either physically-based (PB) or machine\nlearning-based (ML) models. We construct three fine-resolution (500 m) ML\nsimulations of WTD, using the XGBoost algorithm and more than 20 million real\nand proxy observations of WTD, across the United States and Canada. The three\nML models were constrained using known physical relations between WTD's drivers\nand WTD and were trained by sequentially adding real and proxy observations of\nWTD. Through an extensive (pixel-by-pixel) evaluation across the study region\nand within ten major ecoregions of North America, we demonstrate that our\nmodels (corr=0.6-0.75) can more accurately predict unseen real and proxy\nobservations of WTD compared to two available PB simulations of WTD\n(corr=0.21-0.40). However, we still argue that currently-available large-scale\nsimulations of static WTD could be uncertain within data-scarce regions such as\nsteep mountainous regions. We reason that biased observational data mainly\ncollected from low-elevation floodplains and the over-flexibility of available\nmodels can negatively affect the verifiability of large-scale simulations of\nWTD. Ultimately, we thoroughly discuss future directions that may help\nhydrogeologists decide how to improve machine learning-based WTD estimations.\nIn particular, we advocate for the use of proxy satellite data, the\nincorporation of physical laws, the implementation of better model verification\nstandards, the development of novel globally-available emergent indices, and\nthe collection of more reliable observations.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2024 09:05:17 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2024 06:44:06 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 12:12:17 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Janssen", "Joseph", ""], ["Tootchi", "Ardalan", ""], ["Ameli", "Ali A.", ""]], "extracted_entities": [{"text": "physical laws", "label": "Scaling law"}]}
{"id": "2405.10075", "submitter": "Kun Yuan", "authors": "Kun Yuan, Vinkle Srivastav, Nassir Navab, Nicolas Padoy", "title": "HecVL: Hierarchical Video-Language Pretraining for Zero-shot Surgical\n  Phase Recognition", "comments": "Accepted by MICCAI2024", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural language could play an important role in developing generalist\nsurgical models by providing a broad source of supervision from raw texts. This\nflexible form of supervision can enable the model's transferability across\ndatasets and tasks as natural language can be used to reference learned visual\nconcepts or describe new ones. In this work, we present HecVL, a novel\nhierarchical video-language pretraining approach for building a generalist\nsurgical model. Specifically, we construct a hierarchical video-text paired\ndataset by pairing the surgical lecture video with three hierarchical levels of\ntexts: at clip-level, atomic actions using transcribed audio texts; at\nphase-level, conceptual text summaries; and at video-level, overall abstract\ntext of the surgical procedure. Then, we propose a novel fine-to-coarse\ncontrastive learning framework that learns separate embedding spaces for the\nthree video-text hierarchies using a single model. By disentangling embedding\nspaces of different hierarchical levels, the learned multi-modal\nrepresentations encode short-term and long-term surgical concepts in the same\nmodel. Thanks to the injected textual semantics, we demonstrate that the HecVL\napproach can enable zero-shot surgical phase recognition without any human\nannotation. Furthermore, we show that the same HecVL model for surgical phase\nrecognition can be transferred across different surgical procedures and medical\ncenters. The code is available at https://github.com/CAMMA-public/SurgVLP\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2024 13:14:43 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 15:27:41 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Yuan", "Kun", ""], ["Srivastav", "Vinkle", ""], ["Navab", "Nassir", ""], ["Padoy", "Nicolas", ""]], "extracted_entities": [{"text": "embedding spaces", "label": "Embedding"}, {"text": "embedding\nspaces", "label": "Embedding"}]}
{"id": "2405.10653", "submitter": "David Mart\\'inez-Crespo", "authors": "David Mart\\'inez-Crespo and Cesare Tronci", "title": "Heisenberg dynamics of mixed quantum-classical systems", "comments": "22 pages, 1 figure. Submitted as a conribution to the Focus Point\n  \"Mathematics and Physics at the Quantum-Classical Interface\" in the European\n  Physical Journal Plus", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph math-ph math.MP math.SG quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the dynamics of interacting quantum and classical systems in the\nHeisenberg representation. Unlike the usual construction in standard quantum\nmechanics, mixed quantum-classical systems involve the interplay of unitary\noperators acting on the quantum observables and the Lagrangian trajectories\nsweeping the classical degrees of freedom. This interplay reflects an intricate\nstructure which is made particularly challenging by the backreaction excerpted\non the classical trajectories by the quantum degrees of freedom. While the\nbackreaction is underestimated in the common Ehrenfest model, more recent\nmethodologies succeed in capturing this important effect by resorting to\nKoopman wavefunctions in classical mechanics. Luckily, both Ehrenfest and\nKoopman models enjoy a variational framework which is exploited here to unfold\nthe geometric structure underlying quantum-classical coupling. A special role\nis played by the action of the diffeomorphic Lagrangian paths on a non-Abelian\npure-gauge potential which comprises statistical correlations. After presenting\nthe treatment in the simple case of Ehrenfest dynamics, we move on to the\nKoopman model and present the role of the backreaction terms therein. Finally,\nwe compare both models in the context of pure-dephasing systems.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2024 09:36:03 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 10:01:36 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Mart\u00ednez-Crespo", "David", ""], ["Tronci", "Cesare", ""]], "extracted_entities": [{"text": "Ehrenfest model", "label": "AI model"}, {"text": "Koopman model", "label": "AI model"}]}
{"id": "2405.14529", "submitter": "Simon Damm", "authors": "Simon Damm, Mike Laszkiewicz, Johannes Lederer, Asja Fischer", "title": "AnomalyDINO: Boosting Patch-based Few-shot Anomaly Detection with DINOv2", "comments": "Accepted at WACV 2025 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in multimodal foundation models have set new standards in\nfew-shot anomaly detection. This paper explores whether high-quality visual\nfeatures alone are sufficient to rival existing state-of-the-art\nvision-language models. We affirm this by adapting DINOv2 for one-shot and\nfew-shot anomaly detection, with a focus on industrial applications. We show\nthat this approach does not only rival existing techniques but can even\noutmatch them in many settings. Our proposed vision-only approach, AnomalyDINO,\nfollows the well-established patch-level deep nearest neighbor paradigm, and\nenables both image-level anomaly prediction and pixel-level anomaly\nsegmentation. The approach is methodologically simple and training-free and,\nthus, does not require any additional data for fine-tuning or meta-learning.\nThe approach is methodologically simple and training-free and, thus, does not\nrequire any additional data for fine-tuning or meta-learning. Despite its\nsimplicity, AnomalyDINO achieves state-of-the-art results in one- and few-shot\nanomaly detection (e.g., pushing the one-shot performance on MVTec-AD from an\nAUROC of 93.1% to 96.6%). The reduced overhead, coupled with its outstanding\nfew-shot performance, makes AnomalyDINO a strong candidate for fast deployment,\ne.g., in industrial contexts.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2024 13:15:13 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2024 09:23:32 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 09:32:39 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Damm", "Simon", ""], ["Laszkiewicz", "Mike", ""], ["Lederer", "Johannes", ""], ["Fischer", "Asja", ""]], "extracted_entities": [{"text": "multimodal foundation models", "label": "Foundation Model"}, {"text": "fine-tuning", "label": "Fine-tuning"}]}
{"id": "2406.04094", "submitter": "Zixi Chen", "authors": "Zixi Chen, Xuyang Ren, Yuya Hamamatsu, Gastone Ciuti, Cesare Stefanini", "title": "A Generalized Adaptive Jacobian Controller for Soft Robots", "comments": "10 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nonlinearity and hysteresis of soft robot motions have posed challenges\nin control. The Jacobian controller is transferred from rigid robot controllers\nand exhibits conciseness, but the improper assumption of soft robots induces\nthe feasibility only in a small local area. Accurate controllers like neural\nnetworks can deal with delayed and nonlinear motion, achieving high accuracy,\nbut they suffer from the high data amount requirement and black-box property.\nInspired by these approaches, we propose an adaptive generalized Jacobian\ncontroller for soft robots. This controller is constructed by the concise\nformat of the Jacobian controller but includes more states and independent\nmatrices, which is suitable for soft robotics. In addition, the initialization\nleverages the motor babbling strategy and batch optimization from neural\nnetwork controllers. In experiments, we first analyze the online controllers,\nincluding the Jacobian controller, the Gaussian process regression, and our\ncontroller. Real experiments have validated that our controller outperforms the\nRNN controller even with fewer data samples, and it is adaptive to various\nsituations without fine-tuning, like different control frequencies, softness,\nand even manufacturing errors. Future work may include online adjustment of the\ncontroller format and adaptability validation in more scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2024 14:11:09 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 13:04:28 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chen", "Zixi", ""], ["Ren", "Xuyang", ""], ["Hamamatsu", "Yuya", ""], ["Ciuti", "Gastone", ""], ["Stefanini", "Cesare", ""]], "extracted_entities": [{"text": "fine-tuning", "label": "Fine-tuning"}]}
{"id": "2406.04443", "submitter": "Eduard Gorbunov", "authors": "Savelii Chezhegov, Yaroslav Klyukin, Andrei Semenov, Aleksandr\n  Beznosikov, Alexander Gasnikov, Samuel Horv\\'ath, Martin Tak\\'a\\v{c}, Eduard\n  Gorbunov", "title": "Clipping Improves Adam-Norm and AdaGrad-Norm when the Noise Is\n  Heavy-Tailed", "comments": "63 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods with adaptive stepsizes, such as AdaGrad and Adam, are essential for\ntraining modern Deep Learning models, especially Large Language Models.\nTypically, the noise in the stochastic gradients is heavy-tailed for the later\nones. Gradient clipping provably helps to achieve good high-probability\nconvergence for such noises. However, despite the similarity between\nAdaGrad/Adam and Clip-SGD, the current understanding of the high-probability\nconvergence of AdaGrad/Adam-type methods is limited in this case. In this work,\nwe prove that AdaGrad/Adam (and their delayed version) can have provably bad\nhigh-probability convergence if the noise is heavy-tailed. We also show that\ngradient clipping fixes this issue, i.e., we derive new high-probability\nconvergence bounds with polylogarithmic dependence on the confidence level for\nAdaGrad-Norm and Adam-Norm with clipping and with/without delay for smooth\nconvex/non-convex stochastic optimization with heavy-tailed noise. Our\nempirical evaluations highlight the superiority of clipped versions of\nAdaGrad/Adam-Norm in handling the heavy-tailed noise.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2024 18:49:10 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 10:26:57 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chezhegov", "Savelii", ""], ["Klyukin", "Yaroslav", ""], ["Semenov", "Andrei", ""], ["Beznosikov", "Aleksandr", ""], ["Gasnikov", "Alexander", ""], ["Horv\u00e1th", "Samuel", ""], ["Tak\u00e1\u010d", "Martin", ""], ["Gorbunov", "Eduard", ""]], "extracted_entities": [{"text": "Adam", "label": "ALBERT"}, {"text": "Large Language Models", "label": "Large Language Model"}, {"text": "Adam", "label": "ALBERT"}, {"text": "Adam", "label": "ALBERT"}, {"text": "Adam-Norm", "label": "ALBERT"}]}
{"id": "2406.08426", "submitter": "Zijin Hong", "authors": "Zijin Hong, Zheng Yuan, Qinggang Zhang, Hao Chen, Junnan Dong, Feiran\n  Huang, Xiao Huang", "title": "Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating accurate SQL from users' natural language questions (text-to-SQL)\nremains a long-standing challenge due to the complexities involved in user\nquestion understanding, database schema comprehension, and SQL generation.\nTraditional text-to-SQL systems, which combine human engineering and deep\nneural networks, have made significant progress. Subsequently, pre-trained\nlanguage models (PLMs) have been developed for text-to-SQL tasks, achieving\npromising results. However, as modern databases and user questions grow more\ncomplex, PLMs with a limited parameter size often produce incorrect SQL. This\nnecessitates more sophisticated and tailored optimization methods, which\nrestricts the application of PLM-based systems. Recently, large language models\n(LLMs) have shown significant capabilities in natural language understanding as\nmodel scale increases. Thus, integrating LLM-based solutions can bring unique\nopportunities, improvements, and solutions to text-to-SQL research. In this\nsurvey, we provide a comprehensive review of existing LLM-based text-to-SQL\nstudies. Specifically, we offer a brief overview of the technical challenges\nand evolutionary process of text-to-SQL. Next, we introduce the datasets and\nmetrics designed to evaluate text-to-SQL systems. Subsequently, we present a\nsystematic analysis of recent advances in LLM-based text-to-SQL. Finally, we\nmake a summarization and discuss the remaining challenges in this field and\nsuggest expectations for future research directions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2024 17:13:17 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2024 13:51:30 GMT"}, {"version": "v3", "created": "Tue, 16 Jul 2024 08:06:57 GMT"}, {"version": "v4", "created": "Sun, 23 Feb 2025 22:22:20 GMT"}, {"version": "v5", "created": "Thu, 13 Mar 2025 08:45:35 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Hong", "Zijin", ""], ["Yuan", "Zheng", ""], ["Zhang", "Qinggang", ""], ["Chen", "Hao", ""], ["Dong", "Junnan", ""], ["Huang", "Feiran", ""], ["Huang", "Xiao", ""]], "extracted_entities": [{"text": "PLMs", "label": "Large Language Model"}, {"text": "PLMs", "label": "Large Language Model"}, {"text": "large language models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}]}
{"id": "2406.13035", "submitter": "Zhongwei Wan", "authors": "Zhongwei Wan, Xinjian Wu, Yu Zhang, Yi Xin, Chaofan Tao, Zhihong Zhu,\n  Xin Wang, Siqi Luo, Jing Xiong, Longyue Wang, Mi Zhang", "title": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models", "comments": "ICLR 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2024 20:01:51 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2024 08:27:48 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 03:16:43 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wan", "Zhongwei", ""], ["Wu", "Xinjian", ""], ["Zhang", "Yu", ""], ["Xin", "Yi", ""], ["Tao", "Chaofan", ""], ["Zhu", "Zhihong", ""], ["Wang", "Xin", ""], ["Luo", "Siqi", ""], ["Xiong", "Jing", ""], ["Wang", "Longyue", ""], ["Zhang", "Mi", ""]], "extracted_entities": [{"text": "Large Language Models", "label": "Large Language Model"}, {"text": "fine-tuning", "label": "Fine-tuning"}]}
{"id": "2406.13151", "submitter": "Ari Pakman", "authors": "Yarden Cohen, Alexandre Khae Wu Navarro, Jes Frellsen, Richard E.\n  Turner, Raziel Riemer, Ari Pakman", "title": "Bayesian Circular Regression with von Mises Quasi-Processes", "comments": null, "journal-ref": "Proceedings of the 28th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2025", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The need for regression models to predict circular values arises in many\nscientific fields. In this work we explore a family of expressive and\ninterpretable distributions over circle-valued random functions related to\nGaussian processes targeting two Euclidean dimensions conditioned on the unit\ncircle. The probability model has connections with continuous spin models in\nstatistical physics. Moreover, its density is very simple and has\nmaximum-entropy, unlike previous Gaussian process-based approaches, which use\nwrapping or radial marginalization. For posterior inference, we introduce a new\nStratonovich-like augmentation that lends itself to fast Gibbs sampling. We\nargue that transductive learning in these models favors a Bayesian approach to\nthe parameters and apply our sampling scheme to the Double Metropolis-Hastings\nalgorithm. We present experiments applying this model to the prediction of (i)\nwind directions and (ii) the percentage of the running gait cycle as a function\nof joint angles.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2024 01:57:21 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 05:50:20 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Cohen", "Yarden", ""], ["Navarro", "Alexandre Khae Wu", ""], ["Frellsen", "Jes", ""], ["Turner", "Richard E.", ""], ["Riemer", "Raziel", ""], ["Pakman", "Ari", ""]], "extracted_entities": [{"text": "transductive learning", "label": "Few-shot Learning"}]}
{"id": "2407.01131", "submitter": "Xuyang Liu", "authors": "Xuyang Liu, Ting Liu, Siteng Huang, Yi Xin, Yue Hu, Quanjun Yin,\n  Donglin Wang, Yuanyuan Wu, Honggang Chen", "title": "M2IST: Multi-Modal Interactive Side-Tuning for Efficient Referring\n  Expression Comprehension", "comments": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Referring expression comprehension (REC) is a vision-language task to locate\na target object in an image based on a language expression. Fully fine-tuning\ngeneral-purpose pre-trained vision-language foundation models for REC yields\nimpressive performance but becomes increasingly costly. Parameter-efficient\ntransfer learning (PETL) methods have shown strong performance with fewer\ntunable parameters. However, directly applying PETL to REC faces two\nchallenges: (1) insufficient multi-modal interaction between pre-trained\nvision-language foundation models, and (2) high GPU memory usage due to\ngradients passing through the heavy vision-language foundation models. To this\nend, we present M2IST: Multi-Modal Interactive Side-Tuning with M3ISAs: Mixture\nof Multi-Modal Interactive Side-Adapters. During fine-tuning, we fix the\npre-trained uni-modal encoders and update M3ISAs to enable efficient\nvision-language alignment for REC. Empirical results reveal that M2IST achieves\nbetter performance-efficiency trade-off than full fine-tuning and other PETL\nmethods, requiring only 2.11\\% tunable parameters, 39.61\\% GPU memory, and\n63.46\\% training time while maintaining competitive performance. Our code is\nreleased at https://github.com/xuyang-liu16/M2IST.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2024 09:53:53 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2024 12:57:42 GMT"}, {"version": "v3", "created": "Sun, 16 Feb 2025 18:44:39 GMT"}, {"version": "v4", "created": "Thu, 13 Mar 2025 08:48:16 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Liu", "Xuyang", ""], ["Liu", "Ting", ""], ["Huang", "Siteng", ""], ["Xin", "Yi", ""], ["Hu", "Yue", ""], ["Yin", "Quanjun", ""], ["Wang", "Donglin", ""], ["Wu", "Yuanyuan", ""], ["Chen", "Honggang", ""]], "extracted_entities": [{"text": "PETL", "label": "Few-shot Learning"}, {"text": "M3ISAs", "label": "Foundation Model"}, {"text": "fine-tuning", "label": "Fine-tuning"}, {"text": "M3ISAs", "label": "Foundation Model"}, {"text": "full fine-tuning", "label": "Fine-tuning"}]}
{"id": "2407.03715", "submitter": "Flavio Tonioni", "authors": "C\\'edric Debusschere, Flavio Tonioni, Thomas Van Riet", "title": "A distance conjecture beyond moduli?", "comments": "8+1 pages and references, comments welcome!; v2: 9+2 pages and\n  references, with typos fixed, refs. added, and an extra appendix comparing\n  with hep-th/2407.02705; v3, JHEP version: 11+2 pages and references, with\n  improved tests of the proposal in sec. 4, including 3 figs. and refs. added", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-th", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The distance conjecture states that for theories with moduli coupled to\ngravity a tower of states becomes light exponentially in the geodesic distance\nin moduli space. This specifies how effective field theories break down for\nlarge field values. However, phenomenological field theories have no moduli,\nbut a scalar potential that deforms dynamical trajectories away from geodesic\ncurves. In this note we speculate on how one should generalise the distance\nconjecture, in asymptotic field regimes, to include a scalar potential. We test\nthe generalised distance conjecture in a few cases, demonstrate a link with\npseudo-/fake supersymmetry and apply it to the ekpyrotic scenario in cosmology.\nFor the latter we observe that the pre-uplift KKLT potential could provide a\nstringy embedding of ekpyrosis away from the asymptotic regimes in field space.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2024 08:02:44 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2024 13:49:06 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 12:10:12 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Debusschere", "C\u00e9dric", ""], ["Tonioni", "Flavio", ""], ["Van Riet", "Thomas", ""]], "extracted_entities": [{"text": "stringy embedding", "label": "Embedding"}]}
{"id": "2407.09301", "submitter": "Joseph Lehec", "authors": "Joseph Lehec", "title": "Convergence in total variation for the kinetic Langevin algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CC math.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We prove non asymptotic total variation estimates for the kinetic Langevin\nalgorithm in high dimension when the target measure satisfies a Poincar\\'e\ninequality and has gradient Lipschitz potential. The main point is that the\nestimate improves significantly upon the corresponding bound for the non\nkinetic version of the algorithm, due to Dalalyan. In particular the dimension\ndependence drops from $O(n)$ to $O(\\sqrt n)$.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2024 14:33:34 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2024 12:45:06 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 11:20:53 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Lehec", "Joseph", ""]], "extracted_entities": [{"text": "Poincar\\'e\ninequality", "label": "Scaling law"}]}
{"id": "2407.16945", "submitter": "Chen Liu", "authors": "Chen Liu, Wei Zhang, Feng Qiu, Lincheng Li, Xin Yu", "title": "Affective Behaviour Analysis via Progressive Learning", "comments": "Accepted by ECCVWorkshop as the report of the first place in 7th ABAW\n  Track1 Competition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Affective Behavior Analysis aims to develop emotionally intelligent\ntechnology that can recognize and respond to human emotions. To advance this\nfield, the 7th Affective Behavior Analysis in-the-wild (ABAW) competition holds\nthe Multi-Task Learning Challenge based on the s-Aff-Wild2 database. The\nparticipants are required to develop a framework that achieves Valence-Arousal\nEstimation, Expression Recognition, and AU detection simultaneously. To achieve\nthis goal, we propose a progressive multi-task learning framework that fully\nleverages the distinct focuses of each task on facial emotion features.\nSpecifically, our method design can be summarized into three main aspects: 1)\nSeparate Training and Joint Training: We first train each task model separately\nand then perform joint training based on the pre-trained models, fully\nutilizing the feature focus aspects of each task to improve the overall\nframework performance. 2) Feature Fusion and Temporal Modeling:} We investigate\neffective strategies for fusing features extracted from each task-specific\nmodel and incorporate temporal feature modeling during the joint training\nphase, which further refines the performance of each task. 3) Joint Training\nStrategy Optimization: To identify the optimal joint training approach, we\nconduct a comprehensive strategy search, experimenting with various task\ncombinations and training methodologies to further elevate the overall\nperformance of each task. According to the official results, our team achieves\nfirst place in the MTL challenge with a total score of 1.5286 (i.e., AU F-score\n0.5580, Expression F-score 0.4286, CCC VA score 0.5420). Our code is publicly\navailable at https://github.com/YenanLiu/ABAW7th.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2024 02:24:21 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2024 02:24:11 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 03:12:34 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Liu", "Chen", ""], ["Zhang", "Wei", ""], ["Qiu", "Feng", ""], ["Li", "Lincheng", ""], ["Yu", "Xin", ""]], "extracted_entities": [{"text": "Joint Training", "label": "Few-shot Learning"}, {"text": "joint training", "label": "Few-shot Learning"}]}
{"id": "2407.20657", "submitter": "Hunmin Yang", "authors": "Hunmin Yang, Jongoh Jeong, Kuk-Jin Yoon", "title": "Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks", "comments": "Accepted to ECCV 2024 (Oral), Project Page:\n  https://PDCL-Attack.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent vision-language foundation models, such as CLIP, have demonstrated\nsuperior capabilities in learning representations that can be transferable\nacross diverse range of downstream tasks and domains. With the emergence of\nsuch powerful models, it has become crucial to effectively leverage their\ncapabilities in tackling challenging vision tasks. On the other hand, only a\nfew works have focused on devising adversarial examples that transfer well to\nboth unknown domains and model architectures. In this paper, we propose a novel\ntransfer attack method called PDCL-Attack, which leverages the CLIP model to\nenhance the transferability of adversarial perturbations generated by a\ngenerative model-based attack framework. Specifically, we formulate an\neffective prompt-driven feature guidance by harnessing the semantic\nrepresentation power of text, particularly from the ground-truth class labels\nof input images. To the best of our knowledge, we are the first to introduce\nprompt learning to enhance the transferable generative attacks. Extensive\nexperiments conducted across various cross-domain and cross-model settings\nempirically validate our approach, demonstrating its superiority over\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2024 08:52:16 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 06:16:16 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Yang", "Hunmin", ""], ["Jeong", "Jongoh", ""], ["Yoon", "Kuk-Jin", ""]], "extracted_entities": [{"text": "CLIP", "label": "Foundation Model"}, {"text": "CLIP", "label": "Foundation Model"}, {"text": "prompt learning", "label": "Few-shot Learning"}]}
{"id": "2408.04408", "submitter": "BaoYu Tan", "authors": "Baoyu Tan", "title": "Hawking radiation of magnetized particles via tunneling of Bardeen black\n  hole", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "gr-qc hep-th", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  So far, no one has studied regular black holes using the Parikh-Wilczek\nmethod. In this paper, we calculated the emission rate of magnetized particles\npassing through the event horizon of the Bardeen black hole by using the\nParikh-Wilczek method. The emission spectrum deviates from the pure thermal\nspectrum, but conforms to the unitary principle of quantum mechanics. Our\nresults support the conservation of information.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2024 12:17:04 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 03:07:27 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Tan", "Baoyu", ""]], "extracted_entities": [{"text": "unitary principle of quantum mechanics", "label": "quantisation"}]}
{"id": "2408.04991", "submitter": "Jessie De Kruijf", "authors": "Jessie de Kruijf, Eleonora Vanzan, Kimberly K. Boddy, Alvise\n  Raccanelli, Nicola Bartolo", "title": "Searching for blue in the dark", "comments": "15 pages, 8 figures. Matches version published in PRD", "journal-ref": "Phys. Rev. D 111, 063507 (2025)", "doi": "10.1103/PhysRevD.111.063507", "report-no": "UTWI-26-2024", "categories": "astro-ph.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primordial power spectrum of curvature perturbations has been\nwell-measured on large scales but remains fairly unconstrained at smaller\nscales, where significant deviations from $\\Lambda$CDM may occur. Measurements\nof 21-cm intensity mapping in the dark ages promise to access very small scales\nthat have yet to be probed, extending beyond the reach of cosmic microwave\nbackground and galaxy surveys. In this paper, we investigate how small-scale\npower-law enhancements -- or blue tilts -- of the primordial power spectrum\naffect the 21-cm power spectrum. We consider generic enhancements due to\ncurvature modes, isocurvature modes, and runnings of the spectral tilt. We\npresent forecasts for Earth- and lunar-based instruments to detect a\nblue-tilted primordial spectrum. We find that an Earth-based instrument capable\nof reaching the dark ages could detect any enhancements of power on nearly all\nthe scales it can observe, which depends on the baseline of the interferometer.\nThe smallest scales observed by such an instrument can only detect a very\nstrong enhancement. However, an instrument on the far side of the Moon of the\nsame size would be able to probe shallower slopes with higher precision. We\nforecast results for instruments with $100 \\, {\\rm km} \\, (3000 \\, {\\rm km})$\nbaselines and find that they can probe up to scales of order $k_{\\rm max} \\sim\n8 \\, {\\rm Mpc}^{-1} \\, (k_{\\rm max} \\sim 250 \\, {\\rm Mpc}^{-1})$, thereby\nproviding invaluable information on exotic physics and testing inflationary\nmodels on scales not otherwise accessible.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2024 10:50:18 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 10:55:55 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["de Kruijf", "Jessie", ""], ["Vanzan", "Eleonora", ""], ["Boddy", "Kimberly K.", ""], ["Raccanelli", "Alvise", ""], ["Bartolo", "Nicola", ""]], "extracted_entities": [{"text": "small-scale\npower-law enhancements", "label": "Scaling law"}]}
{"id": "2409.00425", "submitter": "Ning Liu", "authors": "Lu Chen, Baopi Liu, and Ning Liu", "title": "Phase behaviors and dynamics of active particle systems in double-well\n  potential", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.soft cond-mat.stat-mech physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we investigate the behaviors and dynamics of self-propelled\nparticles with active reorientation (AR) in a double-well potential. We explore\nthe competition between AR and external potentials, revealing that\nself-propelled particles exhibit flocking and clustering behaviors in an\nasymmetric potential trap. Through molecular dynamics simulations, we obtain a\nphase diagram that illustrates flocking behavior as a function of active\nreorientation and potential asymmetry. We compare the responses of inactive and\nactive particles to the potential, finding that active reorientation\nsignificantly increases aggregation on one side of the asymmetric potential\nwell. Additionally, by calculating the mean squared displacement and scaling\nexponent, we identify distinct diffusion regimes. Our findings demonstrate that\nactive particles with active reorientation are more sensitive to variations in\ndouble-well potentials.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2024 11:53:02 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 09:17:41 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chen", "Lu", ""], ["Liu", "Baopi", ""], ["Liu", "Ning", ""]], "extracted_entities": [{"text": "scaling\nexponent", "label": "Scaling law"}]}
{"id": "2409.06214", "submitter": "Kim Jaewoo", "authors": "Jaewoo Kim, Uehwan Kim", "title": "Towards Generalizable Scene Change Detection", "comments": "Camera-ready version. Accepted to CVPR 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While current state-of-the-art Scene Change Detection (SCD) approaches\nachieve impressive results in well-trained research data, they become\nunreliable under unseen environments and different temporal conditions;\nin-domain performance drops from 77.6% to 8.0% in a previously unseen\nenvironment and to 4.6% under a different temporal condition -- calling for\ngeneralizable SCD and benchmark. In this work, we propose the Generalizable\nScene Change Detection Framework (GeSCF), which addresses unseen domain\nperformance and temporal consistency -- to meet the growing demand for anything\nSCD. Our method leverages the pre-trained Segment Anything Model (SAM) in a\nzero-shot manner. For this, we design Initial Pseudo-mask Generation and\nGeometric-Semantic Mask Matching -- seamlessly turning user-guided prompt and\nsingle-image based segmentation into scene change detection for a pair of\ninputs without guidance. Furthermore, we define the Generalizable Scene Change\nDetection (GeSCD) benchmark along with novel metrics and an evaluation protocol\nto facilitate SCD research in generalizability. In the process, we introduce\nthe ChangeVPR dataset, a collection of challenging image pairs with diverse\nenvironmental scenarios -- including urban, suburban, and rural settings.\nExtensive experiments across various datasets demonstrate that GeSCF achieves\nan average performance gain of 19.2% on existing SCD datasets and 30.0% on the\nChangeVPR dataset, nearly doubling the prior art performance. We believe our\nwork can lay a solid foundation for robust and generalizable SCD research.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2024 04:45:25 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2025 05:28:05 GMT"}, {"version": "v3", "created": "Mon, 3 Mar 2025 01:46:42 GMT"}, {"version": "v4", "created": "Thu, 13 Mar 2025 13:55:30 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Kim", "Jaewoo", ""], ["Kim", "Uehwan", ""]], "extracted_entities": [{"text": "user-guided prompt", "label": "Prompting"}]}
{"id": "2409.07486", "submitter": "Weiqing Liu", "authors": "Junjie Li, Yang Liu, Weiqing Liu, Shikai Fang, Lewen Wang, Chang Xu\n  and Jiang Bian", "title": "MarS: a Financial Market Simulation Engine Powered by Generative\n  Foundation Model", "comments": "35 pages, 26 figures, ICLR 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.AI cs.CE cs.LG q-fin.TR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generative models aim to simulate realistic effects of various actions across\ndifferent contexts, from text generation to visual effects. Despite significant\nefforts to build real-world simulators, the application of generative models to\nvirtual worlds, like financial markets, remains under-explored. In financial\nmarkets, generative models can simulate complex market effects of participants\nwith various behaviors, enabling interaction under different market conditions,\nand training strategies without financial risk. This simulation relies on the\nfinest structured data in financial market like orders thus building the finest\nrealistic simulation. We propose Large Market Model (LMM), an order-level\ngenerative foundation model, for financial market simulation, akin to language\nmodeling in the digital world. Our financial Market Simulation engine (MarS),\npowered by LMM, addresses the domain-specific need for realistic, interactive\nand controllable order generation. Key observations include LMM's strong\nscalability across data size and model complexity, and MarS's robust and\npracticable realism in controlled generation with market impact. We showcase\nMarS as a forecast tool, detection system, analysis platform, and agent\ntraining environment, thus demonstrating MarS's \"paradigm shift\" potential for\na variety of financial applications. We release the code of MarS at\nhttps://github.com/microsoft/MarS/.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2024 08:16:22 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 09:26:41 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Junjie", ""], ["Liu", "Yang", ""], ["Liu", "Weiqing", ""], ["Fang", "Shikai", ""], ["Wang", "Lewen", ""], ["Xu", "Chang", ""], ["Bian", "Jiang", ""]], "extracted_entities": [{"text": "Large Market Model", "label": "Large Language Model"}, {"text": "LMM", "label": "Large Language Model"}, {"text": "LMM", "label": "Foundation Model"}, {"text": "LMM", "label": "Large Language Model"}]}
{"id": "2409.07655", "submitter": "Mohammad Mehrabi", "authors": "Mohammad Mehrabi, Omer Karaduman, and Stefan Wager", "title": "Optimal Mechanisms for Demand Response: An Indifference Set Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The time at which renewable (e.g., solar or wind) energy resources produce\nelectricity cannot generally be controlled. In many settings, however,\nconsumers have some flexibility in their energy consumption needs, and there is\ngrowing interest in demand-response programs that leverage this flexibility to\nshift energy consumption to better match renewable production -- thus enabling\nmore efficient utilization of these resources. We study optimal demand response\nin a setting where consumers use home energy management systems (HEMS) to\nautonomously adjust their electricity consumption. Our core assumption is that\nHEMS operationalize flexibility by querying the consumer for their preferences\nand computing the ``indifference set'' of all energy consumption profiles that\ncan be used to satisfy these preferences. Then, given an indifference set, HEMS\ncan respond to grid signals while guaranteeing user-defined comfort and\nfunctionality; e.g., if a consumer sets a temperature range, a HEMS can precool\nand preheat to align with peak renewable production, thus improving efficiency\nwithout sacrificing comfort. We show that while price-based mechanisms are not\ngenerally optimal for demand response, they become asymptotically optimal in\nlarge markets under a mean-field limit. Furthermore, we show that optimal\ndynamic prices can be efficiently computed in large markets by only querying\nHEMS about their planned consumption under different price signals. Using an\nOpenDSS-powered grid simulation for Phoenix, Arizona, we demonstrate that our\napproach enables meaningful demand response without creating grid instability.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2024 22:55:06 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 00:17:07 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Mehrabi", "Mohammad", ""], ["Karaduman", "Omer", ""], ["Wager", "Stefan", ""]], "extracted_entities": [{"text": "HEMS", "label": "LLMs"}, {"text": "HEMS", "label": "LLMs"}, {"text": "HEMS", "label": "LLMs"}, {"text": "HEMS", "label": "LLMs"}]}
{"id": "2409.11366", "submitter": "Lubomir Banas", "authors": "\\v{L}ubom\\'ir Ba\\v{n}as, Sebastian Herr", "title": "Numerical approximation of bi-harmonic wave maps into spheres", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a structure preserving non-conforming finite element\napproximation scheme for the bi-harmonic wave maps into spheres equation. It\nsatisfies a discrete energy law and preserves the non-convex sphere constraint\nof the continuous problem. The discrete sphere constraint is enforced at the\nmesh-points via a discrete Lagrange multiplier. This approach restricts the\nspatial approximation to the (non-conforming) linear finite elements. We show\nthat the numerical approximation converges to the weak solution of the\ncontinuous problem in spatial dimension $d=1$. The convergence analysis in\ndimensions $d>1$ is complicated by the lack of a discrete product rule as well\nas the low regularity of the numerical approximation in the non-conforming\nsetting. Hence, we show convergence of the numerical approximation in\nhigher-dimensions by introducing additional stabilization terms in the\nnumerical approximation. We present numerical experiments to demonstrate the\nperformance of the proposed numerical approximation and to illustrate the\nregularizing effect of the bi-Laplacian which prevents the formation of\nsingularities.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2024 17:15:58 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 12:23:44 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Ba\u0148as", "\u013dubom\u00edr", ""], ["Herr", "Sebastian", ""]], "extracted_entities": [{"text": "discrete energy law", "label": "Scaling law"}]}
{"id": "2409.11697", "submitter": "Thieu Vo", "authors": "Viet-Hoang Tran and Thieu N. Vo and Tho H. Tran and An T. Nguyen and\n  Tan M. Nguyen", "title": "Monomial Matrix Group Equivariant Neural Functional Networks", "comments": "10 pages in the main text. Published at NeurIPS 2024. The code is\n  available at https://github.com/MathematicalAI-NUS/Monomial-NFN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural functional networks (NFNs) have recently gained significant attention\ndue to their diverse applications, ranging from predicting network\ngeneralization and network editing to classifying implicit neural\nrepresentation. Previous NFN designs often depend on permutation symmetries in\nneural networks' weights, which traditionally arise from the unordered\narrangement of neurons in hidden layers. However, these designs do not take\ninto account the weight scaling symmetries of $\\ReLU$ networks, and the weight\nsign flipping symmetries of $\\sin$ or $\\Tanh$ networks. In this paper, we\nextend the study of the group action on the network weights from the group of\npermutation matrices to the group of monomial matrices by incorporating\nscaling/sign-flipping symmetries. Particularly, we encode these\nscaling/sign-flipping symmetries by designing our corresponding equivariant and\ninvariant layers. We name our new family of NFNs the Monomial Matrix Group\nEquivariant Neural Functional Networks (Monomial-NFN). Because of the expansion\nof the symmetries, Monomial-NFN has much fewer independent trainable parameters\ncompared to the baseline NFNs in the literature, thus enhancing the model's\nefficiency. Moreover, for fully connected and convolutional neural networks, we\ntheoretically prove that all groups that leave these networks invariant while\nacting on their weight spaces are some subgroups of the monomial matrix group.\nWe provide empirical evidence to demonstrate the advantages of our model over\nexisting baselines, achieving competitive performance and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2024 04:36:05 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2024 22:55:21 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 15:36:01 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Tran", "Viet-Hoang", ""], ["Vo", "Thieu N.", ""], ["Tran", "Tho H.", ""], ["Nguyen", "An T.", ""], ["Nguyen", "Tan M.", ""]], "extracted_entities": [{"text": "weight scaling symmetries", "label": "Scaling law"}, {"text": "scaling/sign-flipping symmetries", "label": "Scaling law"}]}
{"id": "2409.13191", "submitter": "Lai Wei", "authors": "Lai Wei, Zhen Ying, Muyang He, Yutong Chen, Qian Yang, Yanzhe Hong,\n  Jiaping Lu, Kaipeng Zheng, Shaoting Zhang, Xiaoying Li, Weiran Huang, Ying\n  Chen", "title": "Diabetica: Adapting Large Language Model to Enhance Multiple Medical\n  Tasks in Diabetes Care and Management", "comments": "Accepted by ICLR 2025 SCI-FM workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diabetes is a chronic disease with a significant global health burden,\nrequiring multi-stakeholder collaboration for optimal management. Large\nlanguage models (LLMs) have shown promise in various healthcare scenarios, but\ntheir effectiveness across diverse diabetes tasks remains unproven. Our study\nintroduced a framework to train and validate diabetes-specific LLMs. We first\ndeveloped a comprehensive data processing pipeline that includes data\ncollection, filtering, augmentation and refinement. This created a\nhigh-quality, diabetes-specific dataset and evaluation benchmarks from scratch.\nFine-tuned on the collected training dataset, our diabetes-specific LLM family\ndemonstrated state-of-the-art proficiency in processing various diabetes tasks\ncompared to other LLMs. Furthermore, clinical studies revealed the potential\napplications of our models in diabetes care, including providing personalized\nhealthcare, assisting medical education, and streamlining clinical tasks.\nGenerally, our introduced framework helps develop diabetes-specific LLMs and\nhighlights their potential to enhance clinical practice and provide\npersonalized, data-driven support for diabetes management across different end\nusers. Our codes, benchmarks and models are available at\nhttps://github.com/waltonfuture/Diabetica.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2024 03:47:54 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 13:20:17 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wei", "Lai", ""], ["Ying", "Zhen", ""], ["He", "Muyang", ""], ["Chen", "Yutong", ""], ["Yang", "Qian", ""], ["Hong", "Yanzhe", ""], ["Lu", "Jiaping", ""], ["Zheng", "Kaipeng", ""], ["Zhang", "Shaoting", ""], ["Li", "Xiaoying", ""], ["Huang", "Weiran", ""], ["Chen", "Ying", ""]], "extracted_entities": [{"text": "Diabetica", "label": "Open-source LLMs"}]}
{"id": "2409.15658", "submitter": "Siyuan Liu", "authors": "Siyuan Liu, Jiawei Du, Sicheng Xiang, Zibo Wang and Dingsheng Luo", "title": "Long-horizon Embodied Planning with Implicit Logical Inference and\n  Hallucination Mitigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-horizon embodied planning underpins embodied AI. To accomplish\nlong-horizon tasks, one of the most feasible ways is to decompose abstract\ninstructions into a sequence of actionable steps. Foundation models still face\nlogical errors and hallucinations in long-horizon planning, unless provided\nwith highly relevant examples to the tasks. However, providing highly relevant\nexamples for any random task is unpractical. Therefore, we present ReLEP, a\nnovel framework for Real-time Long-horizon Embodied Planning. ReLEP can\ncomplete a wide range of long-horizon tasks without in-context examples by\nlearning implicit logical inference through fine-tuning. The fine-tuned large\nvision-language model formulates plans as sequences of skill functions. These\nfunctions are selected from a carefully designed skill library. ReLEP is also\nequipped with a Memory module for plan and status recall, and a Robot\nConfiguration module for versatility across robot types. In addition, we\npropose a data generation pipeline to tackle dataset scarcity. When\nconstructing the dataset, we considered the implicit logical relationships,\nenabling the model to learn implicit logical relationships and dispel\nhallucinations. Through comprehensive evaluations across various long-horizon\ntasks, ReLEP demonstrates high success rates and compliance to execution even\non unseen tasks and outperforms state-of-the-art baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2024 01:47:23 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 10:15:59 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Liu", "Siyuan", ""], ["Du", "Jiawei", ""], ["Xiang", "Sicheng", ""], ["Wang", "Zibo", ""], ["Luo", "Dingsheng", ""]], "extracted_entities": [{"text": "Foundation models", "label": "Foundation Model"}, {"text": "fine-tuning", "label": "Fine-tuning"}]}
{"id": "2409.18042", "submitter": "Kai Chen", "authors": "Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu,\n  Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, Dingdong Wang, Kun Xiang, Haoyuan\n  Li, Haoli Bai, Jianhua Han, Xiaohui Li, Weike Jin, Nian Xie, Yu Zhang, James\n  T. Kwok, Hengshuang Zhao, Xiaodan Liang, Dit-Yan Yeung, Xiao Chen, Zhenguo\n  Li, Wei Zhang, Qun Liu, Jun Yao, Lanqing Hong, Lu Hou, Hang Xu", "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid\n  Emotions", "comments": "Accepted by CVPR 2025. Project Page: https://emova-ollm.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  GPT-4o, an omni-modal model that enables vocal conversations with diverse\nemotions and tones, marks a milestone for omni-modal foundation models.\nHowever, empowering Large Language Models to perceive and generate images,\ntexts, and speeches end-to-end with publicly available data remains challenging\nfor the open-source community. Existing vision-language models rely on external\ntools for speech processing, while speech-language models still suffer from\nlimited or totally without vision-understanding capabilities. To address this\ngap, we propose the EMOVA (EMotionally Omni-present Voice Assistant), to enable\nLarge Language Models with end-to-end speech abilities while maintaining the\nleading vision-language performance. With a semantic-acoustic disentangled\nspeech tokenizer, we surprisingly notice that omni-modal alignment can further\nenhance vision-language and speech abilities compared with the bi-modal aligned\ncounterparts. Moreover, a lightweight style module is introduced for the\nflexible speech style controls including emotions and pitches. For the first\ntime, EMOVA achieves state-of-the-art performance on both the vision-language\nand speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue\nwith vivid emotions.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2024 16:44:02 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2024 06:25:52 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 14:51:04 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chen", "Kai", ""], ["Gou", "Yunhao", ""], ["Huang", "Runhui", ""], ["Liu", "Zhili", ""], ["Tan", "Daxin", ""], ["Xu", "Jing", ""], ["Wang", "Chunwei", ""], ["Zhu", "Yi", ""], ["Zeng", "Yihan", ""], ["Yang", "Kuo", ""], ["Wang", "Dingdong", ""], ["Xiang", "Kun", ""], ["Li", "Haoyuan", ""], ["Bai", "Haoli", ""], ["Han", "Jianhua", ""], ["Li", "Xiaohui", ""], ["Jin", "Weike", ""], ["Xie", "Nian", ""], ["Zhang", "Yu", ""], ["Kwok", "James T.", ""], ["Zhao", "Hengshuang", ""], ["Liang", "Xiaodan", ""], ["Yeung", "Dit-Yan", ""], ["Chen", "Xiao", ""], ["Li", "Zhenguo", ""], ["Zhang", "Wei", ""], ["Liu", "Qun", ""], ["Yao", "Jun", ""], ["Hong", "Lanqing", ""], ["Hou", "Lu", ""], ["Xu", "Hang", ""]], "extracted_entities": [{"text": "GPT-4o", "label": "GPT"}, {"text": "Large Language Models", "label": "Large Language Model"}, {"text": "open-source community", "label": "Open-source LLMs"}, {"text": "vision-language models", "label": "Large Language Model"}, {"text": "speech-language models", "label": "Large Language Model"}, {"text": "Large Language Models", "label": "Large Language Model"}]}
{"id": "2409.20560", "submitter": "Jiachen Li", "authors": "Xiaopan Zhang and Hao Qin and Fuquan Wang and Yue Dong and Jiachen Li", "title": "LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and\n  Planning with LM-Driven PDDL Planner", "comments": "IEEE Conference on Robotics and Automation (ICRA 2025); Project\n  website: https://lamma-p.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language models (LMs) possess a strong capability to comprehend natural\nlanguage, making them effective in translating human instructions into detailed\nplans for simple robot tasks. Nevertheless, it remains a significant challenge\nto handle long-horizon tasks, especially in subtask identification and\nallocation for cooperative heterogeneous robot teams. To address this issue, we\npropose a Language Model-Driven Multi-Agent PDDL Planner (LaMMA-P), a novel\nmulti-agent task planning framework that achieves state-of-the-art performance\non long-horizon tasks. LaMMA-P integrates the strengths of the LMs' reasoning\ncapability and the traditional heuristic search planner to achieve a high\nsuccess rate and efficiency while demonstrating strong generalization across\ntasks. Additionally, we create MAT-THOR, a comprehensive benchmark that\nfeatures household tasks with two different levels of complexity based on the\nAI2-THOR environment. The experimental results demonstrate that LaMMA-P\nachieves a 105% higher success rate and 36% higher efficiency than existing\nLM-based multiagent planners. The experimental videos, code, datasets, and\ndetailed prompts used in each module can be found on the project website:\nhttps://lamma-p.github.io.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2024 17:58:18 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 06:17:58 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhang", "Xiaopan", ""], ["Qin", "Hao", ""], ["Wang", "Fuquan", ""], ["Dong", "Yue", ""], ["Li", "Jiachen", ""]], "extracted_entities": [{"text": "detailed prompts", "label": "Prompting"}]}
{"id": "2410.00263", "submitter": "Kun Yuan", "authors": "Kun Yuan, Vinkle Srivastav, Nassir Navab, Nicolas Padoy", "title": "Procedure-Aware Surgical Video-language Pretraining with Hierarchical\n  Knowledge Augmentation", "comments": "Accepted at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024 Spolight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Surgical video-language pretraining (VLP) faces unique challenges due to the\nknowledge domain gap and the scarcity of multi-modal data. This study aims to\nbridge the gap by addressing issues regarding textual information loss in\nsurgical lecture videos and the spatial-temporal challenges of surgical VLP. We\npropose a hierarchical knowledge augmentation approach and a novel\nProcedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining\n(PeskaVLP) framework to tackle these issues. The knowledge augmentation uses\nlarge language models (LLM) for refining and enriching surgical concepts, thus\nproviding comprehensive language supervision and reducing the risk of\noverfitting. PeskaVLP combines language supervision with visual\nself-supervision, constructing hard negative samples and employing a Dynamic\nTime Warping (DTW) based loss function to effectively comprehend the\ncross-modal procedural alignment. Extensive experiments on multiple public\nsurgical scene understanding and cross-modal retrieval datasets show that our\nproposed method significantly improves zero-shot transferring performance and\noffers a generalist visual representation for further advancements in surgical\nscene understanding.The code is available at\nhttps://github.com/CAMMA-public/SurgVLP\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2024 22:21:05 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 15:21:36 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Yuan", "Kun", ""], ["Srivastav", "Vinkle", ""], ["Navab", "Nassir", ""], ["Padoy", "Nicolas", ""]], "extracted_entities": [{"text": "large language models", "label": "Large Language Model"}, {"text": "LLM", "label": "Large Language Model"}]}
{"id": "2410.00658", "submitter": "Sebastiaan L. D. Ten Haaf", "authors": "Sebastiaan L. D. ten Haaf, Yining Zhang, Qingzhen Wang, Alberto\n  Bordin, Chun-Xiao Liu, Ivan Kulesh, Vincent P. M. Sietses, Christian G.\n  Prosko, Di Xiao, Candice Thomas, Michael J. Manfra, Michael Wimmer and Srijit\n  Goswami", "title": "Observation of edge and bulk states in a three-site Kitaev chain", "comments": "v2 (version after peer review) - 26 pages, 4 main figures, 11\n  supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mes-hall cond-mat.supr-con", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A chain of quantum dots (QDs) in semiconductor-superconductor hybrid systems\ncan form an artificial Kitaev chain hosting Majorana bound states (MBSs). These\nzero-energy states are expected to be localised on the edges of the chain, at\nthe outermost QDs. The remaining QDs, comprising the bulk, are predicted to\nhost an excitation gap that protects the MBSs at the edges from local on-site\nperturbations. Here we demonstrate this connection between the bulk and edges\nin a minimal system, by engineering a three-site Kitaev chain in a\ntwo-dimensional electron gas. Through direct tunneling spectroscopy on each\nsite, we show that the appearance of stable zero-bias conductance peaks at the\nouter QDs is correlated with the presence of an excitation gap in the middle\nQD. Furthermore, we show that this gap can be controlled by applying a\nsuperconducting phase difference between the two hybrid segments and that the\nMBSs are robust only when the excitation gap is present. We find a close\nagreement between experiments and the original Kitaev model, thus confirming\nkey predictions for MBSs in a three-site chain.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2024 13:14:17 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 16:20:34 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Haaf", "Sebastiaan L. D. ten", ""], ["Zhang", "Yining", ""], ["Wang", "Qingzhen", ""], ["Bordin", "Alberto", ""], ["Liu", "Chun-Xiao", ""], ["Kulesh", "Ivan", ""], ["Sietses", "Vincent P. M.", ""], ["Prosko", "Christian G.", ""], ["Xiao", "Di", ""], ["Thomas", "Candice", ""], ["Manfra", "Michael J.", ""], ["Wimmer", "Michael", ""], ["Goswami", "Srijit", ""]], "extracted_entities": [{"text": "MBSs", "label": "LLMs"}, {"text": "MBSs", "label": "LLMs"}]}
{"id": "2410.01727", "submitter": "Yilmazcan Ozyurt", "authors": "Yilmazcan Ozyurt, Stefan Feuerriegel, Mrinmaya Sachan", "title": "Automated Knowledge Concept Annotation and Question Representation\n  Learning for Knowledge Tracing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge tracing (KT) is a popular approach for modeling students' learning\nprogress over time, which can enable more personalized and adaptive learning.\nHowever, existing KT approaches face two major limitations: (1) they rely\nheavily on expert-defined knowledge concepts (KCs) in questions, which is\ntime-consuming and prone to errors; and (2) KT methods tend to overlook the\nsemantics of both questions and the given KCs. In this work, we address these\nchallenges and present KCQRL, a framework for automated knowledge concept\nannotation and question representation learning that can improve the\neffectiveness of any existing KT model. First, we propose an automated KC\nannotation process using large language models (LLMs), which generates question\nsolutions and then annotates KCs in each solution step of the questions.\nSecond, we introduce a contrastive learning approach to generate semantically\nrich embeddings for questions and solution steps, aligning them with their\nassociated KCs via a tailored false negative elimination approach. These\nembeddings can be readily integrated into existing KT models, replacing their\nrandomly initialized embeddings. We demonstrate the effectiveness of KCQRL\nacross 15 KT algorithms on two large real-world Math learning datasets, where\nwe achieve consistent performance improvements.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2024 16:37:19 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 13:09:14 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Ozyurt", "Yilmazcan", ""], ["Feuerriegel", "Stefan", ""], ["Sachan", "Mrinmaya", ""]], "extracted_entities": [{"text": "large language models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "embeddings", "label": "Embedding"}, {"text": "embeddings", "label": "Embedding"}, {"text": "embeddings", "label": "Embedding"}]}
{"id": "2410.04070", "submitter": "Ruizhe Chen", "authors": "Ruizhe Chen, Xiaotian Zhang, Meng Luo, Wenhao Chai, and Zuozhu Liu", "title": "PAD: Personalized Alignment of LLMs at Decoding-Time", "comments": "ICLR 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2024 08:00:55 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2024 13:27:36 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2024 06:15:35 GMT"}, {"version": "v4", "created": "Tue, 29 Oct 2024 12:51:33 GMT"}, {"version": "v5", "created": "Thu, 7 Nov 2024 06:21:14 GMT"}, {"version": "v6", "created": "Tue, 4 Mar 2025 13:51:14 GMT"}, {"version": "v7", "created": "Thu, 13 Mar 2025 13:37:57 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chen", "Ruizhe", ""], ["Zhang", "Xiaotian", ""], ["Luo", "Meng", ""], ["Chai", "Wenhao", ""], ["Liu", "Zuozhu", ""]], "extracted_entities": [{"text": "LLMs", "label": "LLM-based"}]}
{"id": "2410.04281", "submitter": "Yuqiao He", "authors": "Yuqiao He, Yuchao Chen, Jintao Wang, Jian Song", "title": "Age of Synchronization Minimization in Wireless Networks with Random\n  Updates and Time-Varying Timeliness Requirement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study considers a wireless network where multiple nodes transmit status\nupdates to a base station (BS) via a shared, error-free channel with limited\nbandwidth. The status updates arrive at each node randomly. We use the Age of\nSynchronization (AoS) as a metric to measure the information freshness of the\nupdates. The AoS of each node has a timely-varying importance which follows a\nMarkov chain. Our objective is to minimize the weighted sum AoS of the system.\nThe optimization problem is relaxed and formulated as a constrained Markov\ndecision process (CMDP). Solving the relaxed CMDP by a linear programming\nalgorithm yields a stationary policy, which helps us propose a near-stationary\npolicy for the original problem. Numerical simulations show that in most\nconfigurations, the AoS performance of our policy outperforms the policy\nchoosing the maximum AoS regardless of weight variations.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2024 20:30:59 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 09:28:58 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["He", "Yuqiao", ""], ["Chen", "Yuchao", ""], ["Wang", "Jintao", ""], ["Song", "Jian", ""]], "extracted_entities": [{"text": "Markov chain", "label": "Chain of thought"}]}
{"id": "2410.04392", "submitter": "Grigory Volovik", "authors": "G.E. Volovik", "title": "From Landau two-fluid model to de Sitter Universe", "comments": "27 pages, no figures. arXiv admin note: text overlap with\n  arXiv:2307.00860, draft of the paper for collection of papers dedicated to 60\n  years of Landau Institute", "journal-ref": null, "doi": null, "report-no": null, "categories": "gr-qc cond-mat.other hep-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The condensed matter analogs are useful for consideration of the phenomena\nrelated to the quantum vacuum. This is because in condensed matter we know\nphysics both in the infrared and in the ultraviolet limits, while in particle\nphysics and gravity the physics at trans-Planckian scale is unknown. One of the\ncorner stones of the connections between the non-relativistic condensed matter\nand the modern relativistic theories is the two-fluid hydrodynamics of\nsuperfluid helium. The dynamics and thermodynamics of the de Sitter state of\nthe expansion of the Universe bear some features of the multi-fluid system.\nThere are actually three components: the quantum vacuum, the gravitational\ncomponent and relativistic matter. The expanding de Sitter vacuum serves as the\nthermal bath with local temperature, which is twice the Gibbons-Hawking\ntemperature related to the cosmological horizon. This local temperature leads\nto the heating of matter component and the gravitational component. The latter\nbehaves as Zel'dovich stiff matter and represents the dark matter. In\nequilibrium and in the absence of the conventional matter the positive partial\npressure of dark matter compensates the negative partial pressure of quantum\nvacuum. That is why in the full equilibrium the total pressure is zero. This is\nsimilar to the superfluid and normal components in superfluids, which together\nproduce the zero pressure of the liquid in the absence of environment. If one\nassumes that in dynamics, the gravitational dark matter behaves as the real\nZel'dovich stiff matter, one obtains that both components experience the power\nlaw decay due to the energy exchange between these components. Then it follows\nthat their values at present time have the correct order of magnitude. We also\nconsider the other problems through the prism of condensed matter physics,\nincluding the black holes and Planck constant.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2024 08:12:39 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2024 13:08:32 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2025 12:52:58 GMT"}, {"version": "v4", "created": "Sat, 11 Jan 2025 10:59:28 GMT"}, {"version": "v5", "created": "Tue, 21 Jan 2025 14:54:40 GMT"}, {"version": "v6", "created": "Sun, 9 Feb 2025 18:43:42 GMT"}, {"version": "v7", "created": "Mon, 17 Feb 2025 11:53:03 GMT"}, {"version": "v8", "created": "Mon, 3 Mar 2025 08:36:46 GMT"}, {"version": "v9", "created": "Thu, 13 Mar 2025 14:38:25 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Volovik", "G. E.", ""]], "extracted_entities": [{"text": "trans-Planckian scale", "label": "Scaling law"}, {"text": "power\nlaw decay", "label": "Scaling law"}]}
{"id": "2410.04759", "submitter": "Yifan Liu", "authors": "Tianhui Cai, Yifan Liu, Zewei Zhou, Haoxuan Ma, Seth Z. Zhao, Zhiwen\n  Wu and Jiaqi Ma", "title": "Driving with Regulation: Interpretable Decision-Making for Autonomous\n  Vehicles with Retrieval-Augmented Reasoning via LLM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an interpretable decision-making framework for autonomous\nvehicles that integrates traffic regulations, norms, and safety guidelines\ncomprehensively and enables seamless adaptation to different regions. While\ntraditional rule-based methods struggle to incorporate the full scope of\ntraffic rules, we develop a Traffic Regulation Retrieval (TRR) Agent based on\nRetrieval-Augmented Generation (RAG) to automatically retrieve relevant traffic\nrules and guidelines from extensive regulation documents and relevant records\nbased on the ego vehicle's situation. Given the semantic complexity of the\nretrieved rules, we also design a reasoning module powered by a Large Language\nModel (LLM) to interpret these rules, differentiate between mandatory rules and\nsafety guidelines, and assess actions on legal compliance and safety.\nAdditionally, the reasoning is designed to be interpretable, enhancing both\ntransparency and reliability. The framework demonstrates robust performance on\nboth hypothesized and real-world cases across diverse scenarios, along with the\nability to adapt to different regions with ease.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2024 05:27:22 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 04:00:16 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Cai", "Tianhui", ""], ["Liu", "Yifan", ""], ["Zhou", "Zewei", ""], ["Ma", "Haoxuan", ""], ["Zhao", "Seth Z.", ""], ["Wu", "Zhiwen", ""], ["Ma", "Jiaqi", ""]], "extracted_entities": [{"text": "Large Language\nModel", "label": "Large Language Model"}]}
{"id": "2410.05116", "submitter": "Shang-Fu Chen", "authors": "Ayano Hiranaka, Shang-Fu Chen, Chieh-Hsin Lai, Dongjun Kim, Naoki\n  Murata, Takashi Shibuya, Wei-Hsiang Liao, Shao-Hua Sun, Yuki Mitsufuji", "title": "HERO: Human-Feedback Efficient Reinforcement Learning for Online\n  Diffusion Model Finetuning", "comments": "Published in International Conference on Learning Representations\n  (ICLR) 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Controllable generation through Stable Diffusion (SD) fine-tuning aims to\nimprove fidelity, safety, and alignment with human guidance. Existing\nreinforcement learning from human feedback methods usually rely on predefined\nheuristic reward functions or pretrained reward models built on large-scale\ndatasets, limiting their applicability to scenarios where collecting such data\nis costly or difficult. To effectively and efficiently utilize human feedback,\nwe develop a framework, HERO, which leverages online human feedback collected\non the fly during model learning. Specifically, HERO features two key\nmechanisms: (1) Feedback-Aligned Representation Learning, an online training\nmethod that captures human feedback and provides informative learning signals\nfor fine-tuning, and (2) Feedback-Guided Image Generation, which involves\ngenerating images from SD's refined initialization samples, enabling faster\nconvergence towards the evaluator's intent. We demonstrate that HERO is 4x more\nefficient in online feedback for body part anomaly correction compared to the\nbest existing method. Additionally, experiments show that HERO can effectively\nhandle tasks like reasoning, counting, personalization, and reducing NSFW\ncontent with only 0.5K online feedback. The code and project page are available\nat https://hero-dm.github.io/.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2024 15:12:01 GMT"}, {"version": "v2", "created": "Thu, 6 Mar 2025 17:11:55 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 08:12:07 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Hiranaka", "Ayano", ""], ["Chen", "Shang-Fu", ""], ["Lai", "Chieh-Hsin", ""], ["Kim", "Dongjun", ""], ["Murata", "Naoki", ""], ["Shibuya", "Takashi", ""], ["Liao", "Wei-Hsiang", ""], ["Sun", "Shao-Hua", ""], ["Mitsufuji", "Yuki", ""]], "extracted_entities": [{"text": "fine-tuning", "label": "Fine-tuning"}, {"text": "Feedback-Aligned Representation Learning", "label": "Few-shot Learning"}, {"text": "fine-tuning", "label": "Fine-tuning"}, {"text": "Feedback-Guided Image Generation", "label": "Few-shot Learning"}]}
{"id": "2410.05536", "submitter": "Hongjun Wang", "authors": "Hongjun Wang, Jiyuan Chen, Yinqiang Zheng, Xuan Song", "title": "Accelerating Flood Warnings by 10 Hours: The Power of River Network\n  Topology in AI-enhanced Flood Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Climate change-driven floods demand advanced forecasting models, yet Graph\nNeural Networks (GNNs) underutilize river network topology due to tree-like\nstructures causing over-squashing from high node resistance distances. This\nstudy identifies this limitation and introduces a reachability-based graph\ntransformation to densify topological connections, reducing resistance\ndistances. Empirical tests show transformed-GNNs outperform EA-LSTM in extreme\nflood prediction, achieving 24-h water level accuracy equivalent to EA-LSTM's\n14-h forecasts - a 71% improvement in long-term predictive horizon. The dense\ngraph retains flow dynamics across hierarchical river branches, enabling GNNs\nto capture distal node interactions critical for rare flood events. This\ntopological innovation bridges the gap between river network structure and GNN\nmodeling, offering a scalable framework for early warning systems.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2024 22:25:37 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2024 23:20:26 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 00:58:05 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Hongjun", ""], ["Chen", "Jiyuan", ""], ["Zheng", "Yinqiang", ""], ["Song", "Xuan", ""]], "extracted_entities": [{"text": "Graph\nNeural Networks", "label": "Neural Language Model"}, {"text": "GNNs", "label": "Neural Language Model"}, {"text": "GNNs", "label": "Neural Language Model"}]}
{"id": "2410.06215", "submitter": "Zaid Khan", "authors": "Zaid Khan, Elias Stengel-Eskin, Jaemin Cho, Mohit Bansal", "title": "DataEnvGym: Data Generation Agents in Teacher Environments with Student\n  Feedback", "comments": "ICLR 2025 Spotlight; Project Page: https://DataEnvGym.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of creating training data to teach models is currently driven by\nhumans, who manually analyze model weaknesses and plan how to create data that\nimproves a student model. Approaches using LLMs as annotators reduce human\neffort, but still require humans to interpret feedback from evaluations and\ncontrol the LLM to produce data the student needs. Automating this\nlabor-intensive process by creating autonomous data generation agents - or\nteachers - is desirable, but requires environments that can simulate the\nfeedback-driven, iterative, closed loop of data creation. To enable rapid,\nscalable testing for such agents and their modules, we introduce DataEnvGym, a\ntestbed of teacher environments for data generation agents. DataEnvGym frames\ndata generation as a sequential decision-making task, involving an agent\nconsisting of a data generation policy (which generates a plan for creating\ntraining data) and a data generation engine (which transforms the plan into\ndata), inside an environment that provides student feedback. The agent's goal\nis to improve student performance. Students are iteratively trained and\nevaluated on generated data, and their feedback (in the form of errors or weak\nskills) is reported to the agent after each iteration. DataEnvGym includes\nmultiple teacher environment instantiations across 3 levels of structure in the\nstate representation and action space. More structured environments are based\non inferred skills and offer more interpretability and curriculum control. We\nsupport 4 domains (math, code, VQA, and tool-use) and test multiple students\nand teachers. Example agents in our teaching environments can iteratively\nimprove students across tasks and settings. Moreover, we show that environments\nteach different skill levels and test variants of key modules, pointing to\nfuture work in improving data generation agents, engines, and feedback\nmechanisms.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2024 17:20:37 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2024 18:54:45 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 17:30:48 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Khan", "Zaid", ""], ["Stengel-Eskin", "Elias", ""], ["Cho", "Jaemin", ""], ["Bansal", "Mohit", ""]], "extracted_entities": [{"text": "LLMs", "label": "LLM"}]}
{"id": "2410.06846", "submitter": "Mutian He", "authors": "Mutian He, Philip N. Garner", "title": "Joint Fine-tuning and Conversion of Pretrained Speech and Language\n  Models towards Linear Complexity", "comments": "18 pages, 5 figures; ICLR 2025 camera ready. Code:\n  https://github.com/idiap/linearize-distill-pretrained-transformers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Architectures such as Linformer and Mamba have recently emerged as\ncompetitive linear time replacements for transformers. However, corresponding\nlarge pretrained models are often unavailable, especially in non-text domains.\nTo remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)\napproach that jointly converts a transformer model to a linear time substitute\nand fine-tunes it to a target task. We also compare several means to guide the\nfine-tuning to optimally retain the desired inference capability from the\noriginal model. The methods differ in their use of the target model and the\ntrajectory of the parameters. In a series of empirical studies on language\nprocessing, language modeling, and speech processing, we show that CALD can\neffectively recover the result of the original model, and that the guiding\nstrategy contributes to the result. Some reasons for the variation are\nsuggested.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2024 13:06:43 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2024 13:53:32 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2025 13:08:42 GMT"}, {"version": "v4", "created": "Thu, 13 Mar 2025 16:17:19 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["He", "Mutian", ""], ["Garner", "Philip N.", ""]], "extracted_entities": [{"text": "fine-tuning", "label": "Fine-tuning"}, {"text": "guiding\nstrategy", "label": "Prompting"}]}
{"id": "2410.08202", "submitter": "Zhaokai Wang", "authors": "Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jiawen Liu, Jifeng Dai,\n  Yu Qiao, Xizhou Zhu", "title": "Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large\n  Language Models with Endogenous Visual Pre-training", "comments": "Accepted by CVPR 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we focus on monolithic Multimodal Large Language Models\n(MLLMs) that integrate visual encoding and language decoding into a single LLM.\nIn particular, we identify that existing pre-training strategies for monolithic\nMLLMs often suffer from unstable optimization or catastrophic forgetting. To\naddress this issue, our core idea is to embed a new visual parameter space into\na pre-trained LLM, thereby stably learning visual knowledge from noisy data\nwhile freezing the LLM. Based on this principle, we present Mono-InternVL, a\nnovel monolithic MLLM that seamlessly integrates a set of visual experts via a\nmultimodal mixture-of-experts structure. Moreover, we propose an innovative\npre-training strategy to maximize the visual capability of Mono-InternVL,\nnamely Endogenous Visual Pre-training (EViP). In particular, EViP is designed\nas a progressive learning process for visual experts, which aims to fully\nexploit the visual knowledge from noisy data to high-quality data. To validate\nour approach, we conduct extensive experiments on 16 benchmarks. Experimental\nresults confirm the superior performance of Mono-InternVL than existing\nmonolithic MLLMs on 13 of 16 multimodal benchmarks, e.g., +80 points over Emu3\non OCRBench. Compared to the modular baseline, i.e., InternVL-1.5,\nMono-InternVL still retains comparable multimodal performance while reducing up\nto 67% first token latency. Code and model are released at\nhttps://github.com/OpenGVLab/Mono-InternVL.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2024 17:59:22 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2024 12:15:08 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 06:09:17 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Luo", "Gen", ""], ["Yang", "Xue", ""], ["Dou", "Wenhan", ""], ["Wang", "Zhaokai", ""], ["Liu", "Jiawen", ""], ["Dai", "Jifeng", ""], ["Qiao", "Yu", ""], ["Zhu", "Xizhou", ""]], "extracted_entities": [{"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}]}
{"id": "2410.11716", "submitter": "Lukas Pin", "authors": "Lukas Pin, Oleksandr Sverdlov, Frank Bretz, Bj\\\"orn Bornkamp", "title": "Randomization-based Inference for MCP-Mod", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dose selection is critical in pharmaceutical drug development, as it directly\nimpacts therapeutic efficacy and patient safety of a drug. The Generalized\nMultiple Comparison Procedures and Modeling (MCP-Mod) approach is commonly used\nin Phase II trials for testing and estimation of dose-response relationships.\nHowever, its effectiveness in small sample sizes, particularly with binary\nendpoints, is hindered by issues like complete separation in logistic\nregression, leading to non-existence of estimates. Motivated by an actual\nclinical trial using the MCP-Mod approach, this paper introduces penalized\nmaximum likelihood estimation (MLE) and randomization-based inference\ntechniques to address these challenges. Randomization-based inference allows\nfor exact finite sample inference, while population-based inference for MCP-Mod\ntypically relies on asymptotic approximations. Simulation studies demonstrate\nthat randomization-based tests can enhance statistical power in small to\nmedium-sized samples while maintaining control over type-I error rates, even in\nthe presence of time trends. Our results show that residual-based randomization\ntests using penalized MLEs not only improve computational efficiency but also\noutperform standard randomization-based methods, making them an adequate choice\nfor dose-finding analyses within the MCP-Mod framework. Additionally, we apply\nthese methods to pharmacometric settings, demonstrating their effectiveness in\nsuch scenarios. The results in this paper underscore the potential of\nrandomization-based inference for the analysis of dose-finding trials,\nparticularly in small sample contexts.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2024 15:48:27 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 13:05:23 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Pin", "Lukas", ""], ["Sverdlov", "Oleksandr", ""], ["Bretz", "Frank", ""], ["Bornkamp", "Bj\u00f6rn", ""]], "extracted_entities": [{"text": "Randomization-based inference", "label": "LLM-based"}, {"text": "statistical power", "label": "LLM-powered"}]}
{"id": "2410.11826", "submitter": "Jacopo Iollo", "authors": "Jacopo Iollo, Christophe Heinkel\\'e, Pierre Alliez, and Florence\n  Forbes", "title": "Bayesian Experimental Design via Contrastive Diffusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Bayesian Optimal Experimental Design (BOED) is a powerful tool to reduce the\ncost of running a sequence of experiments. When based on the Expected\nInformation Gain (EIG), design optimization corresponds to the maximization of\nsome intractable expected contrast between prior and posterior distributions.\nScaling this maximization to high dimensional and complex settings has been an\nissue due to BOED inherent computational complexity. In this work, we introduce\na pooled posterior distribution with cost-effective sampling properties and\nprovide a tractable access to the EIG contrast maximization via a new EIG\ngradient expression. Diffusion-based samplers are used to compute the dynamics\nof the pooled posterior and ideas from bi-level optimization are leveraged to\nderive an efficient joint sampling-optimization loop. The resulting efficiency\ngain allows to extend BOED to the well-tested generative capabilities of\ndiffusion models. By incorporating generative models into the BOED framework,\nwe expand its scope and its use in scenarios that were previously impractical.\nNumerical experiments and comparison with state-of-the-art methods show the\npotential of the approach.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2024 17:53:07 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 11:23:03 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Iollo", "Jacopo", ""], ["Heinkel\u00e9", "Christophe", ""], ["Alliez", "Pierre", ""], ["Forbes", "Florence", ""]], "extracted_entities": [{"text": "Diffusion-based samplers", "label": "LLM-based"}]}
{"id": "2410.12289", "submitter": "Nir Shlezinger", "authors": "Nir Shlezinger, Guy Revach, Anubhab Ghosh, Saikat Chatterjee, Shuo\n  Tang, Tales Imbiriba, Jindrich Dunik, Ondrej Straka, Pau Closas, and Yonina\n  C. Eldar", "title": "AI-Aided Kalman Filters", "comments": "Submitted to the IEEE Signal Processing Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SP eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kalman filter (KF) and its variants are among the most celebrated\nalgorithms in signal processing. These methods are used for state estimation of\ndynamic systems by relying on mathematical representations in the form of\nsimple state-space (SS) models, which may be crude and inaccurate descriptions\nof the underlying dynamics. Emerging data-centric artificial intelligence (AI)\ntechniques tackle these tasks using deep neural networks (DNNs), which are\nmodel-agnostic. Recent developments illustrate the possibility of fusing DNNs\nwith classic Kalman-type filtering, obtaining systems that learn to track in\npartially known dynamics. This article provides a tutorial-style overview of\ndesign approaches for incorporating AI in aiding KF-type algorithms. We review\nboth generic and dedicated DNN architectures suitable for state estimation, and\nprovide a systematic presentation of techniques for fusing AI tools with KFs\nand for leveraging partial SS modeling and data, categorizing design approaches\ninto task-oriented and SS model-oriented. The usefulness of each approach in\npreserving the individual strengths of model-based KFs and data-driven DNNs is\ninvestigated in a qualitative and quantitative study, whose code is publicly\navailable, illustrating the gains of hybrid model-based/data-driven designs. We\nalso discuss existing challenges and future research directions that arise from\nfusing AI and Kalman-type algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2024 06:47:53 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 12:40:48 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Shlezinger", "Nir", ""], ["Revach", "Guy", ""], ["Ghosh", "Anubhab", ""], ["Chatterjee", "Saikat", ""], ["Tang", "Shuo", ""], ["Imbiriba", "Tales", ""], ["Dunik", "Jindrich", ""], ["Straka", "Ondrej", ""], ["Closas", "Pau", ""], ["Eldar", "Yonina C.", ""]], "extracted_entities": [{"text": "publicly\navailable", "label": "Open-source LLMs"}]}
{"id": "2410.12854", "submitter": "Weibin Liao", "authors": "Weibin Liao, Xu Chu, Yasha Wang", "title": "TPO: Aligning Large Language Models with Multi-branch & Multi-step\n  Preference Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the domain of complex reasoning tasks, such as mathematical reasoning,\nrecent advancements have proposed the use of Direct Preference Optimization\n(DPO) to suppress output of dispreferred responses, thereby enhancing the\nlong-chain reasoning capabilities of large language models (LLMs). To this end,\nthese studies employed LLMs to generate preference trees via Tree-of-thoughts\n(ToT) and sample the paired preference responses required by the DPO algorithm.\nHowever, the DPO algorithm based on binary preference optimization is unable to\nlearn multiple responses with varying degrees of preference/dispreference that\nprovided by the preference trees, resulting in incomplete preference learning.\nIn this work, we introduce Tree Preference Optimization (TPO), that does not\nsample paired preference responses from the preference tree; instead, it\ndirectly learns from the entire preference tree during the fine-tuning.\nSpecifically, TPO formulates the language model alignment as a Preference List\nRanking problem, where the policy can potentially learn more effectively from a\nranked preference list of responses given the prompt. In addition, to further\nassist LLMs in identifying discriminative steps within long-chain reasoning and\nincrease the relative reward margin in the preference list, TPO utilizes\nAdaptive Step Reward to adjust the reward values of each step in trajectory for\nperforming fine-grained preference optimization. We carry out extensive\nexperiments on mathematical reasoning tasks to evaluate TPO. The experimental\nresults indicate that TPO consistently outperforms DPO across five public large\nlanguage models on four datasets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2024 22:22:05 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 06:40:44 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Liao", "Weibin", ""], ["Chu", "Xu", ""], ["Wang", "Yasha", ""]], "extracted_entities": [{"text": "large language models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "LLM"}, {"text": "Tree-of-thoughts", "label": "Chain of thought"}, {"text": "fine-tuning", "label": "Fine-tuning"}, {"text": "prompt", "label": "Prompting"}, {"text": "large\nlanguage models", "label": "Large Language Model"}]}
{"id": "2410.13640", "submitter": "Yiming Wang", "authors": "Yiming Wang, Pei Zhang, Baosong Yang, Derek F. Wong, Rui Wang", "title": "Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation", "comments": "Accepted by ICLR 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LLM self-evaluation relies on the LLM's own ability to estimate response\ncorrectness, which can greatly improve its deployment reliability. In this\nresearch track, we propose the Chain-of-Embedding (CoE) in the latent space to\nenable LLMs to perform output-free self-evaluation. CoE consists of all\nprogressive hidden states produced during the inference time, which can be\ntreated as the latent thinking path of LLMs. We find that when LLMs respond\ncorrectly and incorrectly, their CoE features differ, these discrepancies\nassist us in estimating LLM response correctness. Experiments in four diverse\ndomains and seven LLMs fully demonstrate the effectiveness of our method.\nMeanwhile, its label-free design intent without any training and\nmillisecond-level computational cost ensures real-time feedback in large-scale\nscenarios. More importantly, we provide interesting insights into LLM response\ncorrectness from the perspective of hidden state changes inside LLMs.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2024 15:09:24 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 16:16:12 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Yiming", ""], ["Zhang", "Pei", ""], ["Yang", "Baosong", ""], ["Wong", "Derek F.", ""], ["Wang", "Rui", ""]], "extracted_entities": [{"text": "LLM", "label": "LLM"}, {"text": "LLM", "label": "LLM"}, {"text": "Chain-of-Embedding", "label": "Embedding"}, {"text": "LLMs", "label": "LLM"}, {"text": "CoE", "label": "Embedding"}, {"text": "LLMs", "label": "LLM"}, {"text": "LLMs", "label": "LLM"}, {"text": "CoE", "label": "Embedding"}, {"text": "LLM", "label": "LLM"}, {"text": "LLMs", "label": "LLM"}, {"text": "LLM", "label": "LLM"}, {"text": "LLMs", "label": "LLM"}]}
{"id": "2410.14993", "submitter": "Hao Wu", "authors": "Hao Wu, Donglin Bai, Shiqi Jiang, Qianxi Zhang, Yifan Yang, Xin Ding,\n  Ting Cao, Yunxin Liu, Fengyuan Xu", "title": "Making Every Frame Matter: Continuous Activity Recognition in Streaming\n  Video via Adaptive Video Context Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video activity recognition has become increasingly important in robots and\nembodied AI. Recognizing continuous video activities poses considerable\nchallenges due to the fast expansion of streaming video, which contains\nmulti-scale and untrimmed activities. We introduce a novel system, CARS, to\novercome these issues through adaptive video context modeling. Adaptive video\ncontext modeling refers to selectively maintaining activity-related features in\ntemporal and spatial dimensions. CARS has two key designs. The first is an\nactivity spatial feature extraction by eliminating irrelevant visual features\nwhile maintaining recognition accuracy. The second is an activity-aware state\nupdate introducing dynamic adaptability to better preserve the video context\nfor multi-scale activity recognition. Our CARS runs at speeds $>$30 FPS on\ntypical edge devices and outperforms all baselines by 1.2\\% to 79.7\\% in\naccuracy. Moreover, we explore applying CARS to a large video model as a video\nencoder. Experimental results show that our CARS can result in a 0.46-point\nenhancement (on a 5-point scale) on the in-distribution video activity dataset,\nand an improvement ranging from 1.19\\% to 4\\% on zero-shot video activity\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2024 05:50:00 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 15:19:21 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wu", "Hao", ""], ["Bai", "Donglin", ""], ["Jiang", "Shiqi", ""], ["Zhang", "Qianxi", ""], ["Yang", "Yifan", ""], ["Ding", "Xin", ""], ["Cao", "Ting", ""], ["Liu", "Yunxin", ""], ["Xu", "Fengyuan", ""]], "extracted_entities": [{"text": "adaptive video context modeling", "label": "contextual Embedding"}, {"text": "Adaptive video\ncontext modeling", "label": "contextual Embedding"}]}
{"id": "2410.20643", "submitter": "Wilson Wongso", "authors": "Wilson Wongso, Hao Xue, Flora D. Salim", "title": "GenUP: Generative User Profilers as In-Context Learners for Next POI\n  Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Traditional Point-of-Interest (POI) recommendation systems often lack\ntransparency, interpretability, and scrutability due to their reliance on dense\nvector-based user embeddings. Furthermore, the cold-start problem -- where\nsystems have insufficient data for new users -- limits their ability to\ngenerate accurate recommendations. Existing methods often address this by\nleveraging similar trajectories from other users, but this approach can be\ncomputationally expensive and increases the context length for LLM-based\nmethods, making them difficult to scale. To address these limitations, we\npropose a method that generates natural language (NL) user profiles from\nlarge-scale, location-based social network (LBSN) check-ins, utilizing robust\npersonality assessments and behavioral theories. These NL profiles capture user\npreferences, routines, and behaviors, improving POI prediction accuracy while\noffering enhanced transparency. By incorporating NL profiles as system prompts\nto LLMs, our approach reduces reliance on extensive historical data, while\nremaining flexible, easily updated, and computationally efficient. Our method\nis not only competitive with other LLM-based and complex agentic frameworks but\nis also more scalable for real-world POI recommender systems. Results\ndemonstrate that our approach consistently outperforms baseline methods,\noffering a more interpretable and resource-efficient solution for POI\nrecommendation systems. Our source code is available at:\nhttps://github.com/w11wo/GenUP/.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2024 00:39:22 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 00:54:57 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wongso", "Wilson", ""], ["Xue", "Hao", ""], ["Salim", "Flora D.", ""]], "extracted_entities": [{"text": "dense\nvector-based user embeddings", "label": "Embedding"}, {"text": "system prompts", "label": "Prompting"}, {"text": "LLMs", "label": "LLM"}]}
{"id": "2411.00712", "submitter": "Shane Parker", "authors": "Ericka Roy Miller and Shane M. Parker", "title": "Numerically Stable Resonating Hartree-Fock", "comments": "12 pages, 4 figures", "journal-ref": "Journal of Chemical Physics, 162, 104115 (2025)", "doi": "10.1063/5.0246790", "report-no": null, "categories": "physics.chem-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The simulation of excited states at low computational cost remains an open\nchallenge for electronic structure (ES) methods. While much attention has been\ngiven to orthogonal ES methods, relatively little work has been done to develop\nnonorthogonal ES methods for excited states, particularly those involving\nnonorthogonal orbital optimization. We present here a numerically stable\nformulation of the Resonating Hartree-Fock (ResHF) method that uses the matrix\nadjugate to remove numerical instabilities in ResHF arising from nearly\northogonal orbitals, and we demonstrate improvements to ResHF wavefunction\noptimization as a result. We then benchmark the performance of ResHF against\nComplete Active Space Self-Consistent Field in the avoided crossing of LiF, the\ntorsional rotation of ethene, and the singlet-triplet energy gaps of a\nselection of small molecules. ResHF is a promising excited state method because\nit incorporates the orbital relaxation of state-specific methods, while\nretaining the correct state crossings of state-averaged approaches. Our\nopen-source ResHF implementation, yucca, is available on GitLab.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2024 16:26:18 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 01:07:21 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Miller", "Ericka Roy", ""], ["Parker", "Shane M.", ""]], "extracted_entities": [{"text": "yucca", "label": "Open-source LLMs"}]}
{"id": "2411.00915", "submitter": "Liang Mi", "authors": "Liang Mi, Weijun Wang, Wenming Tu, Qingfeng He, Rui Kong, Xinyu Fang,\n  Yazhu Dong, Yikang Zhang, Yunchun Li, Meng Li, Haipeng Dai, Guihai Chen,\n  Yunxin Liu", "title": "V-LoRA: An Efficient and Flexible System Boosts Vision Applications with\n  LoRA LMM", "comments": "EuroSys'2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large Multimodal Models (LMMs) have shown significant progress in various\ncomplex vision tasks with the solid linguistic and reasoning capacity inherited\nfrom large language models (LMMs). Low-rank adaptation (LoRA) offers a\npromising method to integrate external knowledge into LMMs, compensating for\ntheir limitations on domain-specific tasks. However, the existing LoRA model\nserving is excessively computationally expensive and causes extremely high\nlatency. In this paper, we present an end-to-end solution that empowers diverse\nvision tasks and enriches vision applications with LoRA LMMs. Our system,\nVaLoRA, enables accurate and efficient vision tasks by 1) an accuracy-aware\nLoRA adapter generation approach that generates LoRA adapters rich in\ndomain-specific knowledge to meet application-specific accuracy requirements,\n2) an adaptive-tiling LoRA adapters batching operator that efficiently computes\nconcurrent heterogeneous LoRA adapters, and 3) a flexible LoRA adapter\norchestration mechanism that manages application requests and LoRA adapters to\nachieve the lowest average response latency. We prototype VaLoRA on five\npopular vision tasks on three LMMs. Experiment results reveal that VaLoRA\nimproves 24-62% of the accuracy compared to the original LMMs and reduces\n20-89% of the latency compared to the state-of-the-art LoRA model serving\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2024 13:43:33 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2025 05:57:42 GMT"}, {"version": "v3", "created": "Tue, 11 Mar 2025 13:26:38 GMT"}, {"version": "v4", "created": "Thu, 13 Mar 2025 08:38:15 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Mi", "Liang", ""], ["Wang", "Weijun", ""], ["Tu", "Wenming", ""], ["He", "Qingfeng", ""], ["Kong", "Rui", ""], ["Fang", "Xinyu", ""], ["Dong", "Yazhu", ""], ["Zhang", "Yikang", ""], ["Li", "Yunchun", ""], ["Li", "Meng", ""], ["Dai", "Haipeng", ""], ["Chen", "Guihai", ""], ["Liu", "Yunxin", ""]], "extracted_entities": [{"text": "Large Multimodal Models", "label": "Large Language Model"}, {"text": "LMMs", "label": "Large Language Model"}, {"text": "LMMs", "label": "Large Language Model"}, {"text": "LMMs", "label": "Large Language Model"}, {"text": "LMMs", "label": "Large Language Model"}]}
{"id": "2411.01100", "submitter": "Xinran Miao", "authors": "Xinran Miao, Jiwei Zhao, Hyunseung Kang", "title": "Transfer Learning Between U.S. Presidential Elections: How Should We\n  Learn From A 2020 Ad Campaign To Inform 2024 Ad Campaigns?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For the 2024 U.S. presidential election, would negative, digital ads against\nDonald Trump impact voter turnout in Pennsylvania (PA), a key \"tipping point''\nstate? The gold standard to address this question, a randomized experiment\nwhere voters get randomized to different ads, yields unbiased estimates of the\nad effect, but is very expensive. Instead, we propose a less-than-ideal, but\nsignificantly cheaper and faster framework based on transfer learning, where we\ntransfer knowledge from a past ad experiment in 2020 to evaluate ads for 2024.\nA key component of our framework is a sensitivity analysis that quantifies the\nunobservable differences between 2020 and 2024 elections, where sensitivity\nparameters can be calibrated in a data-driven manner. We propose two estimators\nof the 2024 ad effect: a simple regression estimator with bootstrap, which we\nrecommend for practitioners in this field, and an estimator based on the\nefficient influence function for broader applications. Using our framework, we\nestimate the effect of running a negative, digital ad campaign against Trump on\nvoter turnout in PA for the 2024 election. Our findings indicate effect\nheterogeneity across counties of PA and among important subgroups stratified by\ngender, urbanicity, and education attainment.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2024 01:35:58 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 01:23:02 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Miao", "Xinran", ""], ["Zhao", "Jiwei", ""], ["Kang", "Hyunseung", ""]], "extracted_entities": [{"text": "transfer learning", "label": "Few-shot Learning"}]}
{"id": "2411.04954", "submitter": "Jingwei Xu", "authors": "Jingwei Xu, Zibo Zhao, Chenyu Wang, Wen Liu, Yi Ma, Shenghua Gao", "title": "CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM", "comments": "Project page: https://cad-mllm.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper aims to design a unified Computer-Aided Design (CAD) generation\nsystem that can easily generate CAD models based on the user's inputs in the\nform of textual description, images, point clouds, or even a combination of\nthem. Towards this goal, we introduce the CAD-MLLM, the first system capable of\ngenerating parametric CAD models conditioned on the multimodal input.\nSpecifically, within the CAD-MLLM framework, we leverage the command sequences\nof CAD models and then employ advanced large language models (LLMs) to align\nthe feature space across these diverse multi-modalities data and CAD models'\nvectorized representations. To facilitate the model training, we design a\ncomprehensive data construction and annotation pipeline that equips each CAD\nmodel with corresponding multimodal data. Our resulting dataset, named\nOmni-CAD, is the first multimodal CAD dataset that contains textual\ndescription, multi-view images, points, and command sequence for each CAD\nmodel. It contains approximately 450K instances and their CAD construction\nsequences. To thoroughly evaluate the quality of our generated CAD models, we\ngo beyond current evaluation metrics that focus on reconstruction quality by\nintroducing additional metrics that assess topology quality and surface\nenclosure extent. Extensive experimental results demonstrate that CAD-MLLM\nsignificantly outperforms existing conditional generative methods and remains\nhighly robust to noises and missing points. The project page and more\nvisualizations can be found at: https://cad-mllm.github.io/\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2024 18:31:08 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 06:11:16 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Xu", "Jingwei", ""], ["Zhao", "Zibo", ""], ["Wang", "Chenyu", ""], ["Liu", "Wen", ""], ["Ma", "Yi", ""], ["Gao", "Shenghua", ""]], "extracted_entities": [{"text": "CAD models", "label": "Large Language Model"}, {"text": "CAD models", "label": "Large Language Model"}, {"text": "CAD models", "label": "Large Language Model"}]}
{"id": "2411.05039", "submitter": "Subhankar Maity", "authors": "Aniket Deroy, Subhankar Maity", "title": "YouTube Comments Decoded: Leveraging LLMs for Low Resource Language\n  Classification", "comments": "Updated and Final Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sarcasm detection is a significant challenge in sentiment analysis,\nparticularly due to its nature of conveying opinions where the intended meaning\ndeviates from the literal expression. This challenge is heightened in social\nmedia contexts where code-mixing, especially in Dravidian languages, is\nprevalent. Code-mixing involves the blending of multiple languages within a\nsingle utterance, often with non-native scripts, complicating the task for\nsystems trained on monolingual data. This shared task introduces a novel gold\nstandard corpus designed for sarcasm and sentiment detection within code-mixed\ntexts, specifically in Tamil-English and Malayalam-English languages. The\nprimary objective of this task is to identify sarcasm and sentiment polarity\nwithin a code-mixed dataset of Tamil-English and Malayalam-English comments and\nposts collected from social media platforms. Each comment or post is annotated\nat the message level for sentiment polarity, with particular attention to the\nchallenges posed by class imbalance, reflecting real-world scenarios.In this\nwork, we experiment with state-of-the-art large language models like GPT-3.5\nTurbo via prompting to classify comments into sarcastic or non-sarcastic\ncategories. We obtained a macro-F1 score of 0.61 for Tamil language. We\nobtained a macro-F1 score of 0.50 for Malayalam language.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2024 17:58:01 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 16:17:21 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Deroy", "Aniket", ""], ["Maity", "Subhankar", ""]], "extracted_entities": [{"text": "prompting", "label": "Prompting"}]}
{"id": "2411.06518", "submitter": "Yuewen Sun", "authors": "Yuewen Sun, Lingjing Kong, Guangyi Chen, Loka Li, Gongxu Luo, Zijian\n  Li, Yixuan Zhang, Yujia Zheng, Mengyue Yang, Petar Stojanov, Eran Segal, Eric\n  P. Xing, Kun Zhang", "title": "Causal Representation Learning from Multimodal Biomedical Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prevalent in biomedical applications (e.g., human phenotype research),\nmultimodal datasets can provide valuable insights into the underlying\nphysiological mechanisms. However, current machine learning (ML) models\ndesigned to analyze these datasets often lack interpretability and\nidentifiability guarantees, which are essential for biomedical research. Recent\nadvances in causal representation learning have shown promise in identifying\ninterpretable latent causal variables with formal theoretical guarantees.\nUnfortunately, most current work on multimodal distributions either relies on\nrestrictive parametric assumptions or yields only coarse identification\nresults, limiting their applicability to biomedical research that favors a\ndetailed understanding of the mechanisms.\n  In this work, we aim to develop flexible identification conditions for\nmultimodal data and principled methods to facilitate the understanding of\nbiomedical datasets. Theoretically, we consider a nonparametric latent\ndistribution (c.f., parametric assumptions in previous work) that allows for\ncausal relationships across potentially different modalities. We establish\nidentifiability guarantees for each latent component, extending the subspace\nidentification results from previous work. Our key theoretical contribution is\nthe structural sparsity of causal connections between modalities, which, as we\nwill discuss, is natural for a large collection of biomedical systems.\nEmpirically, we present a practical framework to instantiate our theoretical\ninsights. We demonstrate the effectiveness of our approach through extensive\nexperiments on both numerical and synthetic datasets. Results on a real-world\nhuman phenotype dataset are consistent with established biomedical research,\nvalidating our theoretical and methodological framework.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2024 16:40:27 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 08:56:49 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Sun", "Yuewen", ""], ["Kong", "Lingjing", ""], ["Chen", "Guangyi", ""], ["Li", "Loka", ""], ["Luo", "Gongxu", ""], ["Li", "Zijian", ""], ["Zhang", "Yixuan", ""], ["Zheng", "Yujia", ""], ["Yang", "Mengyue", ""], ["Stojanov", "Petar", ""], ["Segal", "Eran", ""], ["Xing", "Eric P.", ""], ["Zhang", "Kun", ""]], "extracted_entities": [{"text": "causal representation learning", "label": "Few-shot Learning"}]}
{"id": "2411.06635", "submitter": "Aixa X. Andrade", "authors": "Aixa X. Andrade, Son Nguyen and Albert Montillo", "title": "scMEDAL for the interpretable analysis of single-cell transcriptomics\n  data with batch effect visualization using a deep mixed effects autoencoder", "comments": "Main manuscript: 28 pages, including 8 figures and 1 table.\n  Supplemental material: 19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  scRNA-seq data has the potential to provide new insights into cellular\nheterogeneity and data acquisition; however, a major challenge is unraveling\nconfounding from technical and biological batch effects. Existing batch\ncorrection algorithms suppress and discard these effects, rather than\nquantifying and modeling them. Here, we present scMEDAL, a framework for\nsingle-cell Mixed Effects Deep Autoencoder Learning, which separately models\nbatch-invariant and batch-specific effects using two complementary autoencoder\nnetworks. One network is trained through adversarial learning to capture a\nbatch-invariant representation, while a Bayesian autoencoder learns a\nbatch-specific representation. Comprehensive evaluations spanning conditions\n(e.g., autism, leukemia, and cardiovascular), cell types, and technical and\nbiological effects demonstrate that scMEDAL suppresses batch effects while\nmodeling batch-specific variation, enhancing accuracy and interpretability.\nUnlike prior approaches, the framework's fixed- and random-effects autoencoders\nenable retrospective analyses, including predicting a cell's expression as if\nit had been acquired in a different batch via genomap projections at the\ncellular level, revealing the impact of biological (e.g., diagnosis) and\ntechnical (e.g., acquisition) effects. By combining scMEDAL's batch-agnostic\nand batch-specific latent spaces, it enables more accurate predictions of\ndisease status, donor group, and cell type, making scMEDAL a valuable framework\nfor gaining deeper insight into data acquisition and cellular heterogeneity.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2024 00:10:48 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2024 21:20:06 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 16:15:37 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Andrade", "Aixa X.", ""], ["Nguyen", "Son", ""], ["Montillo", "Albert", ""]], "extracted_entities": [{"text": "adversarial learning", "label": "Few-shot Learning"}]}
{"id": "2411.13022", "submitter": "Ya\\c{s}ar Utku Al\\c{c}alar", "authors": "Ya\\c{s}ar Utku Al\\c{c}alar, Merve G\\\"ulle, Mehmet Ak\\c{c}akaya", "title": "Fast MRI for All: Bridging Equity Gaps via Training without Raw Data\n  Access", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Physics-driven deep learning (PD-DL) approaches have become popular for\nimproved reconstruction of fast magnetic resonance imaging (MRI) scans. Though\nPD-DL offers higher acceleration rates than existing clinical fast MRI\ntechniques, their use has been limited outside specialized MRI centers. A key\nchallenge is generalization to underrepresented pathologies or populations,\nnoted in multiple studies, with fine-tuning on target populations suggested for\nimprovement. However, current approaches for PD-DL training require access to\nraw k-space measurements, which is typically only available at specialized MRI\ncenters that have research agreements for such data access. This is especially\nan issue for rural and underserved areas, where commercial MRI scanners only\nprovide access to a final reconstructed image. To tackle these challenges, we\npropose Compressibility-inspired Unsupervised Learning via Parallel Imaging\nFidelity (CUPID) for high-quality PD-DL training using only routine clinical\nreconstructed images exported from an MRI scanner. CUPID evaluates output\nquality with a compressibility-based approach while ensuring that the output\nstays consistent with the clinical parallel imaging reconstruction through\nwell-designed perturbations. Our results show CUPID achieves similar quality to\nestablished PD-DL training that requires k-space data while outperforming\ncompressed sensing (CS) and diffusion-based generative methods. We further\ndemonstrate its effectiveness in a zero-shot training setup for retrospectively\nand prospectively sub-sampled acquisitions, attesting to its minimal training\nburden. As an approach that radically deviates from existing strategies, CUPID\npresents an opportunity to provide equitable access to fast MRI for underserved\npopulations in an attempt to reduce the inequalities associated with this\nexpensive imaging modality.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2024 03:53:41 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 15:54:28 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Al\u00e7alar", "Ya\u015far Utku", ""], ["G\u00fclle", "Merve", ""], ["Ak\u00e7akaya", "Mehmet", ""]], "extracted_entities": [{"text": "fine-tuning", "label": "Fine-tuning"}, {"text": "research agreements", "label": "AI Ethics"}]}
{"id": "2411.13163", "submitter": "Nabeel Seedat", "authors": "Nabeel Seedat, Caterina Tozzi, Andrea Hita Ardiaca, Mihaela van der\n  Schaar, James Weatherall, Adam Taylor", "title": "Unlocking Historical Clinical Trial Data with ALIGN: A Compositional\n  Large Language Model System for Medical Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The reuse of historical clinical trial data has significant potential to\naccelerate medical research and drug development. However, interoperability\nchallenges, particularly with missing medical codes, hinders effective data\nintegration across studies. While Large Language Models (LLMs) offer a\npromising solution for automated coding without labeled data, current\napproaches face challenges on complex coding tasks. We introduce ALIGN, a novel\ncompositional LLM-based system for automated, zero-shot medical coding. ALIGN\nfollows a three-step process: (1) diverse candidate code generation; (2)\nself-evaluation of codes and (3) confidence scoring and uncertainty estimation\nenabling human deferral to ensure reliability. We evaluate ALIGN on harmonizing\nmedication terms into Anatomical Therapeutic Chemical (ATC) and medical history\nterms into Medical Dictionary for Regulatory Activities (MedDRA) codes\nextracted from 22 immunology trials. ALIGN outperformed the LLM baselines,\nwhile also providing capabilities for trustworthy deployment. For MedDRA\ncoding, ALIGN achieved high accuracy across all levels, matching RAG and\nexcelling at the most specific levels (87-90% for HLGT). For ATC coding, ALIGN\ndemonstrated superior performance, particularly at lower hierarchy levels (ATC\nLevel 4), with 72-73% overall accuracy and 86-89% accuracy for common\nmedications, outperforming baselines by 7-22%. ALIGN's uncertainty-based\ndeferral improved accuracy by 17% to 90% accuracy with 30% deferral, notably\nenhancing performance on uncommon medications. ALIGN achieves this\ncost-efficiently at \\$0.0007 and \\$0.02 per code for GPT-4o-mini and GPT-4o,\nreducing barriers to clinical adoption. ALIGN advances automated medical coding\nfor clinical trial data, contributing to enhanced data interoperability and\nreusability, positioning it as a promising tool to improve clinical research\nand accelerate drug development.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2024 09:59:12 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 13:39:09 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Seedat", "Nabeel", ""], ["Tozzi", "Caterina", ""], ["Ardiaca", "Andrea Hita", ""], ["van der Schaar", "Mihaela", ""], ["Weatherall", "James", ""], ["Taylor", "Adam", ""]], "extracted_entities": [{"text": "RAG", "label": "RAG"}, {"text": "HLGT", "label": "GPT"}, {"text": "GPT-4o-mini", "label": "GPT"}, {"text": "GPT-4o", "label": "GPT"}]}
{"id": "2411.14871", "submitter": "Dingyuan Shi", "authors": "Dingyuan Shi, Yong Wang, Hangyu Li, Xiangxiang Chu", "title": "Preference Alignment for Diffusion Model via Explicit Denoised\n  Distribution Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion models have shown remarkable success in text-to-image generation,\nmaking preference alignment for these models increasingly important. The\npreference labels are typically available only at the terminal of denoising\ntrajectories, which poses challenges in optimizing the intermediate denoising\nsteps. In this paper, we propose to conduct Denoised Distribution Estimation\n(DDE) that explicitly connects intermediate steps to the terminal denoised\ndistribution. Therefore, preference labels can be used for the entire\ntrajectory optimization. To this end, we design two estimation strategies for\nour DDE. The first is stepwise estimation, which utilizes the conditional\ndenoised distribution to estimate the model denoised distribution. The second\nis single-shot estimation, which converts the model output into the terminal\ndenoised distribution via DDIM modeling. Analytically and empirically, we\nreveal that DDE equipped with two estimation strategies naturally derives a\nnovel credit assignment scheme that prioritizes optimizing the middle part of\nthe denoising trajectory. Extensive experiments demonstrate that our approach\nachieves superior performance, both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2024 11:45:33 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2024 14:55:08 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 02:36:28 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Shi", "Dingyuan", ""], ["Wang", "Yong", ""], ["Li", "Hangyu", ""], ["Chu", "Xiangxiang", ""]], "extracted_entities": [{"text": "single-shot estimation", "label": "Few-shot Learning"}]}
{"id": "2411.15995", "submitter": "Hui Zhou", "authors": "Hui Zhou, Xiaolan Liu, Sangarapillai Lambotharan", "title": "A General Sensing-assisted Channel Estimation Framework in Distributed\n  MIMO Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In 6G communications, it is envisioned to equip the traditional access point\n(AP) with sensing capability to fully benefit the existing wireless\ncommunication infrastructures. Thus, sensing-assisted communication has\nattracted significant attention from both industry and academia. However, most\nexisting works focused on sensing-assisted communication in line-of-sight (LoS)\nscenarios due to sensing limitations, where the sensing target (ST) and\ncommunication user equipment (UE) remain the same. In this paper, we propose a\ngeneral sensing-assisted channel estimation framework in the distributed\nmultiple-input and multiple-output (DMIMO) network and consider a scenario\nwhere the ST and UE are different entities. In addition, ST is a moving target\n(e.g. a robot) which causes channels between APs and UEs to vary due to changes\nin the reflection paths of the indoor environment. Therefore, we let multiple\nAPs to jointly sense the position of the ST, which will be incorporated in a\nRay tracing model to obtain a more accurate estimate of the channels from APs\nto UEs for both the LoS and non-line-of-sight (NLoS) scenarios. Simulation\nresults demonstrate that our proposed sensing-assisted communication framework\nachieves a much higher channel estimation accuracy and downlink throughput\ncompared to the traditional least-square (LS) channel estimation. More\nimportantly, the feasibility of the proposed framework has been validated to\nguarantee the stringent channel estimation accuracy requirement in the DMIMO\nnetwork.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2024 22:13:43 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 16:58:46 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhou", "Hui", ""], ["Liu", "Xiaolan", ""], ["Lambotharan", "Sangarapillai", ""]], "extracted_entities": [{"text": "Ray tracing model", "label": "AI model"}]}
{"id": "2411.16727", "submitter": "Yingwen Zhang", "authors": "Yingwen Zhang, Meng Wang, Xihua Sheng, Peilin Chen, Junru Li, Li Zhang\n  and Shiqi Wang", "title": "An Information-Theoretic Regularizer for Lossy Neural Image Compression", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy image compression networks aim to minimize the latent entropy of images\nwhile adhering to specific distortion constraints. However, optimizing the\nneural network can be challenging due to its nature of learning quantized\nlatent representations. In this paper, our key finding is that minimizing the\nlatent entropy is, to some extent, equivalent to maximizing the conditional\nsource entropy, an insight that is deeply rooted in information-theoretic\nequalities. Building on this insight, we propose a novel structural\nregularization method for the neural image compression task by incorporating\nthe negative conditional source entropy into the training objective, such that\nboth the optimization efficacy and the model's generalization ability can be\npromoted. The proposed information-theoretic regularizer is interpretable,\nplug-and-play, and imposes no inference overheads. Extensive experiments\ndemonstrate its superiority in regularizing the models and further squeezing\nbits from the latent representation across various compression structures and\nunseen domains.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2024 05:19:27 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2024 09:06:45 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 05:18:03 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhang", "Yingwen", ""], ["Wang", "Meng", ""], ["Sheng", "Xihua", ""], ["Chen", "Peilin", ""], ["Li", "Junru", ""], ["Zhang", "Li", ""], ["Wang", "Shiqi", ""]], "extracted_entities": [{"text": "latent entropy", "label": "quantisation"}, {"text": "information-theoretic\nequalities", "label": "Model Bias and Fairness"}]}
{"id": "2411.17274", "submitter": "Yikun Li", "authors": "Yikun Li, Ting Zhang, Ratnadira Widyasari, Yan Naing Tun, Huu Hung\n  Nguyen, Tan Bui, Ivana Clairine Irsan, Yiran Cheng, Xiang Lan, Han Wei Ang,\n  Frank Liauw, Martin Weyssow, Hong Jin Kang, Eng Lieh Ouh, Lwin Khin Shar,\n  David Lo", "title": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 8,203\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2024 09:51:55 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2024 03:52:23 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2025 04:08:15 GMT"}, {"version": "v4", "created": "Thu, 13 Mar 2025 10:41:04 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Yikun", ""], ["Zhang", "Ting", ""], ["Widyasari", "Ratnadira", ""], ["Tun", "Yan Naing", ""], ["Nguyen", "Huu Hung", ""], ["Bui", "Tan", ""], ["Irsan", "Ivana Clairine", ""], ["Cheng", "Yiran", ""], ["Lan", "Xiang", ""], ["Ang", "Han Wei", ""], ["Liauw", "Frank", ""], ["Weyssow", "Martin", ""], ["Kang", "Hong Jin", ""], ["Ouh", "Eng Lieh", ""], ["Shar", "Lwin Khin", ""], ["Lo", "David", ""]], "extracted_entities": [{"text": "LLM", "label": "Large Language Model"}, {"text": "LLM", "label": "Large Language Model"}, {"text": "LLM", "label": "Large Language Model"}, {"text": "fine-tuning", "label": "Fine-tuning"}]}
{"id": "2411.19553", "submitter": "Xiaosi Gu", "authors": "Xiaosi Gu and Tomoyuki Obuchi", "title": "Analysis of High-dimensional Gaussian Labeled-unlabeled Mixture Model\n  via Message-passing Algorithm", "comments": "48 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning (SSL) is a machine learning methodology that\nleverages unlabeled data in conjunction with a limited amount of labeled data.\nAlthough SSL has been applied in various applications and its effectiveness has\nbeen empirically demonstrated, it is still not fully understood when and why\nSSL performs well. Some existing theoretical studies have attempted to address\nthis issue by modeling classification problems using the so-called Gaussian\nMixture Model (GMM). These studies provide notable and insightful\ninterpretations. However, their analyses are focused on specific purposes, and\na thorough investigation of the properties of GMM in the context of SSL has\nbeen lacking. In this paper, we conduct such a detailed analysis of the\nproperties of the high-dimensional GMM for binary classification in the SSL\nsetting. To this end, we employ the approximate message passing and state\nevolution methods, which are widely used in high-dimensional settings and\noriginate from statistical mechanics. We deal with two estimation approaches:\nthe Bayesian one and the $\\ell_2$-regularized maximum likelihood estimation\n(RMLE). We conduct a comprehensive comparison between these two approaches,\nexamining aspects such as the global phase diagram, estimation error for the\nparameters, and prediction error for the labels. A specific comparison is made\nbetween the Bayes-optimal (BO) estimator and RMLE, as the BO setting provides\noptimal estimation performance and is ideal as a benchmark. Our analysis shows\nthat with appropriate regularizations, RMLE can achieve near-optimal\nperformance in terms of both the estimation error and prediction error,\nespecially when there is a large amount of unlabeled data. These results\ndemonstrate that the $\\ell_2$ regularization term plays an effective role in\nestimation and prediction in SSL approaches.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2024 08:57:07 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 00:22:52 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Gu", "Xiaosi", ""], ["Obuchi", "Tomoyuki", ""]], "extracted_entities": [{"text": "Semi-supervised learning", "label": "Zero-shot Learning"}]}
{"id": "2412.00196", "submitter": "Xiyuan Gao", "authors": "Xiyuan Gao", "title": "Spontaneous CP Violation and Flavor Changing Neutral Currents in Minimal\n  SO(10)", "comments": "24 pages, 4 figures; version published in PRD", "journal-ref": "Phys. Rev. D 111, 055013 (2025)", "doi": "10.1103/PhysRevD.111.055013", "report-no": null, "categories": "hep-ph hep-ex", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We explore spontaneous CP violation (SCPV) in the minimal non-supersymmetric\nSO(10) grand unified theory (GUT), with a scalar sector comprising a CP-even\n$45_H$, a $126_H$, and a complex $10_H$. All renormalizable couplings are real\ndue to CP symmetry, and the Kobayashi-Maskawa phase arises solely from complex\nelectroweak vacuum expectation values. The model requires an additional Higgs\ndoublet fine-tuned below 500 GeV and constrains new Yukawa couplings, linking\ncertain flavor-violating (FV) processes. Future proton decay observations may\nreveal correlated FV decay ratios, offering insights into minimal SO(10).\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2024 19:00:26 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 09:01:27 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Gao", "Xiyuan", ""]], "extracted_entities": [{"text": "fine-tuned below 500 GeV", "label": "Fine-tuning"}]}
{"id": "2412.01254", "submitter": "Liangwei Jiang", "authors": "Liangwei Jiang, Ruida Li, Zhifeng Zhang, Shuo Fang, Chenguang Ma", "title": "EmojiDiff: Advanced Facial Expression Control with High Identity\n  Preservation in Portrait Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to bring fine-grained expression control while maintaining\nhigh-fidelity identity in portrait generation. This is challenging due to the\nmutual interference between expression and identity: (i) fine expression\ncontrol signals inevitably introduce appearance-related semantics (e.g., facial\ncontours, and ratio), which impact the identity of the generated portrait; (ii)\neven coarse-grained expression control can cause facial changes that compromise\nidentity, since they all act on the face. These limitations remain unaddressed\nby previous generation methods, which primarily rely on coarse control signals\nor two-stage inference that integrates portrait animation. Here, we introduce\nEmojiDiff, the first end-to-end solution that enables simultaneous control of\nextremely detailed expression (RGB-level) and high-fidelity identity in\nportrait generation. To address the above challenges, EmojiDiff adopts a\ntwo-stage scheme involving decoupled training and fine-tuning. For decoupled\ntraining, we innovate ID-irrelevant Data Iteration (IDI) to synthesize\ncross-identity expression pairs by dividing and optimizing the processes of\nmaintaining expression and altering identity, thereby ensuring stable and\nhigh-quality data generation. Training the model with this data, we effectively\ndisentangle fine expression features in the expression template from other\nextraneous information (e.g., identity, skin). Subsequently, we present\nID-enhanced Contrast Alignment (ICA) for further fine-tuning. ICA achieves\nrapid reconstruction and joint supervision of identity and expression\ninformation, thus aligning identity representations of images with and without\nexpression control. Experimental results demonstrate that our method remarkably\noutperforms counterparts, achieves precise expression control with highly\nmaintained identity, and generalizes well to various diffusion models.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2024 08:24:11 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 08:32:46 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Jiang", "Liangwei", ""], ["Li", "Ruida", ""], ["Zhang", "Zhifeng", ""], ["Fang", "Shuo", ""], ["Ma", "Chenguang", ""]], "extracted_entities": [{"text": "fine-tuning", "label": "Fine-tuning"}, {"text": "fine-tuning", "label": "Fine-tuning"}]}
{"id": "2412.02171", "submitter": "Tianyi Wang", "authors": "Tianyi Wang, Zichen Wang, Cong Wang, Yuanchao Shu, Ruilong Deng, Peng\n  Cheng, Jiming Chen (Zhejiang University, Hangzhou, China)", "title": "Can't Slow me Down: Learning Robust and Hardware-Adaptive Object\n  Detectors against Latency Attacks for Edge Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a fundamental enabler for many real-time downstream\napplications such as autonomous driving, augmented reality and supply chain\nmanagement. However, the algorithmic backbone of neural networks is brittle to\nimperceptible perturbations in the system inputs, which were generally known as\nmisclassifying attacks. By targeting the real-time processing capability, a new\nclass of latency attacks are reported recently. They exploit new attack\nsurfaces in object detectors by creating a computational bottleneck in the\npost-processing module, that leads to cascading failure and puts the real-time\ndownstream tasks at risks. In this work, we take an initial attempt to defend\nagainst this attack via background-attentive adversarial training that is also\ncognizant of the underlying hardware capabilities. We first draw system-level\nconnections between latency attack and hardware capacity across heterogeneous\nGPU devices. Based on the particular adversarial behaviors, we utilize\nobjectness loss as a proxy and build background attention into the adversarial\ntraining pipeline, and achieve a reasonable balance between clean and robust\naccuracy. The extensive experiments demonstrate the defense effectiveness of\nrestoring real-time processing capability from $13$ FPS to $43$ FPS on Jetson\nOrin NX, with a better trade-off between the clean and robust accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2024 05:00:26 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 07:31:19 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Tianyi", "", "Zhejiang University, Hangzhou, China"], ["Wang", "Zichen", "", "Zhejiang University, Hangzhou, China"], ["Wang", "Cong", "", "Zhejiang University, Hangzhou, China"], ["Shu", "Yuanchao", "", "Zhejiang University, Hangzhou, China"], ["Deng", "Ruilong", "", "Zhejiang University, Hangzhou, China"], ["Cheng", "Peng", "", "Zhejiang University, Hangzhou, China"], ["Chen", "Jiming", "", "Zhejiang University, Hangzhou, China"]], "extracted_entities": [{"text": "background-attentive adversarial training", "label": "Few-shot Learning"}, {"text": "background attention", "label": "Attention mechanism"}]}
{"id": "2412.03021", "submitter": "Tianyu Chang", "authors": "Tianyu Chang, Xiaohao Chen, Zhichao Wei, Xuanpu Zhang, Qing-Guo Chen,\n  Weihua Luo, Peipei Song and Xun Yang", "title": "PEMF-VTO: Point-Enhanced Video Virtual Try-on via Mask-free Paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Virtual Try-on aims to seamlessly transfer a reference garment onto a\ntarget person in a video while preserving both visual fidelity and temporal\ncoherence. Existing methods typically rely on inpainting masks to define the\ntry-on area, enabling accurate garment transfer for simple scenes (e.g.,\nin-shop videos). However, these mask-based approaches struggle with complex\nreal-world scenarios, as overly large and inconsistent masks often destroy\nspatial-temporal information, leading to distorted results. Mask-free methods\nalleviate this issue but face challenges in accurately determining the try-on\narea, especially for videos with dynamic body movements. To address these\nlimitations, we propose PEMF-VTO, a novel Point-Enhanced Mask-Free Video\nVirtual Try-On framework that leverages sparse point alignments to explicitly\nguide garment transfer. Our key innovation is the introduction of\npoint-enhanced guidance, which provides flexible and reliable control over both\nspatial-level garment transfer and temporal-level video coherence.\nSpecifically, we design a Point-Enhanced Transformer (PET) with two core\ncomponents: Point-Enhanced Spatial Attention (PSA), which uses frame-cloth\npoint alignments to precisely guide garment transfer, and Point-Enhanced\nTemporal Attention (PTA), which leverages frame-frame point correspondences to\nenhance temporal coherence and ensure smooth transitions across frames.\nExtensive experiments demonstrate that our PEMF-VTO outperforms\nstate-of-the-art methods, generating more natural, coherent, and visually\nappealing try-on videos, particularly for challenging in-the-wild scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2024 04:24:15 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2024 02:57:24 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 14:22:12 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chang", "Tianyu", ""], ["Chen", "Xiaohao", ""], ["Wei", "Zhichao", ""], ["Zhang", "Xuanpu", ""], ["Chen", "Qing-Guo", ""], ["Luo", "Weihua", ""], ["Song", "Peipei", ""], ["Yang", "Xun", ""]], "extracted_entities": [{"text": "Point-Enhanced Spatial Attention", "label": "Attention mechanism"}, {"text": "Point-Enhanced\nTemporal Attention", "label": "Attention mechanism"}]}
{"id": "2412.06519", "submitter": "Simone Zoia", "authors": "Simon Badger, Heribertus Bayu Hartanto, Rene Poncelet, Zihao Wu, Yang\n  Zhang, Simone Zoia", "title": "Full-colour double-virtual amplitudes for associated production of a\n  Higgs boson with a bottom-quark pair at the LHC", "comments": "27 pages, 4 figures, 1 appendix, ancillary files available at\n  https://zenodo.org/records/14328503; v2: published version", "journal-ref": "J. High Energ. Phys. 2025, 66 (2025)", "doi": "10.1007/JHEP03(2025)066", "report-no": "IFJPAN-IV-2024-14, USTC-ICTS/PCFT-24-55, ZU-TH-63/24", "categories": "hep-ph hep-th", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the double-virtual amplitudes contributing to the production of a\nHiggs boson in association with a $b\\bar{b}$ pair at the Large Hadron Collider.\nWe perform the computation within the five-flavour scheme, which employs\nmassless bottom quarks and finite bottom-Yukawa coupling, taking into account\nall the colour structures. We derive the analytic form of the helicity\namplitudes through finite-field reconstruction techniques. The analytic\nexpressions have been implemented in a public C++ library, and we demonstrate\nthat evaluations are sufficiently stable and efficient for use in\nphenomenological studies.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2024 14:19:50 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 09:04:56 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Badger", "Simon", ""], ["Hartanto", "Heribertus Bayu", ""], ["Poncelet", "Rene", ""], ["Wu", "Zihao", ""], ["Zhang", "Yang", ""], ["Zoia", "Simone", ""]], "extracted_entities": [{"text": "public C++ library", "label": "Open-source LLMs"}]}
{"id": "2412.07377", "submitter": "Fuyi Yang", "authors": "Jiazuo Mu, Fuyi Yang, Yanshun Zhang, Mingqian Zhang, Junxiong Zhang,\n  Yongjian Luo, Lan Xu, Yujiao Shi and Yingliang Zhang", "title": "CADSpotting: Robust Panoptic Symbol Spotting on Large-Scale CAD Drawings", "comments": "18pages, 14 figures, Project web-page:\n  https://dgeneai.github.io/cadspotting-pages/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce CADSpotting, an effective method for panoptic symbol spotting in\nlarge-scale architectural CAD drawings. Existing approaches struggle with\nsymbol diversity, scale variations, and overlapping elements in CAD designs.\nCADSpotting overcomes these challenges by representing primitives through\ndensely sampled points with attributes like coordinates and colors, using a\nunified 3D point cloud model for robust feature learning. To enable accurate\nsegmentation in large, complex drawings, we further propose a novel Sliding\nWindow Aggregation (SWA) technique, combining weighted voting and Non-Maximum\nSuppression (NMS). Moreover, we introduce LS-CAD, a new large-scale CAD dataset\nto support our experiments, with each floorplan covering around 1,000 square\nmeters, significantly larger than previous benchmarks. Experiments on\nFloorPlanCAD and LS-CAD datasets show that CADSpotting significantly\noutperforms existing methods. We also demonstrate its practical value through\nautomating parametric 3D reconstruction, enabling interior modeling directly\nfrom raw CAD inputs.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2024 10:22:17 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2024 03:27:12 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 07:41:50 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Mu", "Jiazuo", ""], ["Yang", "Fuyi", ""], ["Zhang", "Yanshun", ""], ["Zhang", "Mingqian", ""], ["Zhang", "Junxiong", ""], ["Luo", "Yongjian", ""], ["Xu", "Lan", ""], ["Shi", "Yujiao", ""], ["Zhang", "Yingliang", ""]], "extracted_entities": [{"text": "robust feature learning", "label": "Few-shot Learning"}, {"text": "LS-CAD", "label": "Large Language Model"}, {"text": "FloorPlanCAD", "label": "Large Language Model"}, {"text": "LS-CAD", "label": "Large Language Model"}]}
{"id": "2412.07589", "submitter": "Jianzong Wu", "authors": "Jianzong Wu, Chao Tang, Jingbo Wang, Yanhong Zeng, Xiangtai Li, Yunhai\n  Tong", "title": "DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for\n  Customized Manga Generation", "comments": "[CVPR 2025] The project page is\n  https://jianzongwu.github.io/projects/diffsensei/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Story visualization, the task of creating visual narratives from textual\ndescriptions, has seen progress with text-to-image generation models. However,\nthese models often lack effective control over character appearances and\ninteractions, particularly in multi-character scenes. To address these\nlimitations, we propose a new task: \\textbf{customized manga generation} and\nintroduce \\textbf{DiffSensei}, an innovative framework specifically designed\nfor generating manga with dynamic multi-character control. DiffSensei\nintegrates a diffusion-based image generator with a multimodal large language\nmodel (MLLM) that acts as a text-compatible identity adapter. Our approach\nemploys masked cross-attention to seamlessly incorporate character features,\nenabling precise layout control without direct pixel transfer. Additionally,\nthe MLLM-based adapter adjusts character features to align with panel-specific\ntext cues, allowing flexible adjustments in character expressions, poses, and\nactions. We also introduce \\textbf{MangaZero}, a large-scale dataset tailored\nto this task, containing 43,264 manga pages and 427,147 annotated panels,\nsupporting the visualization of varied character interactions and movements\nacross sequential frames. Extensive experiments demonstrate that DiffSensei\noutperforms existing models, marking a significant advancement in manga\ngeneration by enabling text-adaptable character customization. The project page\nis https://jianzongwu.github.io/projects/diffsensei/.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2024 15:24:12 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 06:23:03 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wu", "Jianzong", ""], ["Tang", "Chao", ""], ["Wang", "Jingbo", ""], ["Zeng", "Yanhong", ""], ["Li", "Xiangtai", ""], ["Tong", "Yunhai", ""]], "extracted_entities": [{"text": "MLLM", "label": "Large Language Model"}, {"text": "masked cross-attention", "label": "Attention mechanism"}]}
{"id": "2412.07720", "submitter": "Jinyi Hu", "authors": "Jinyi Hu, Shengding Hu, Yuxuan Song, Yufei Huang, Mingxuan Wang, Hao\n  Zhou, Zhiyuan Liu, Wei-Ying Ma, Maosong Sun", "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2024 18:13:20 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 16:29:17 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Hu", "Jinyi", ""], ["Hu", "Shengding", ""], ["Song", "Yuxuan", ""], ["Huang", "Yufei", ""], ["Wang", "Mingxuan", ""], ["Zhou", "Hao", ""], ["Liu", "Zhiyuan", ""], ["Ma", "Wei-Ying", ""], ["Sun", "Maosong", ""]], "extracted_entities": [{"text": "ACDiT", "label": "Generative Pre-trained Transformer (GPT)"}, {"text": "ACDiT", "label": "Generative Pre-trained Transformer (GPT)"}, {"text": "ACDiT", "label": "Generative Pre-trained Transformer (GPT)"}, {"text": "ACDiT", "label": "Generative Pre-trained Transformer (GPT)"}, {"text": "ACDiT", "label": "Generative Pre-trained Transformer (GPT)"}, {"text": "ACDiT", "label": "Generative Pre-trained Transformer (GPT)"}, {"text": "ACDiT", "label": "Generative Pre-trained Transformer (GPT)"}]}
{"id": "2412.07752", "submitter": "Korbinian P\\\"oppel", "authors": "Korbinian P\\\"oppel, Maximilian Beck, Sepp Hochreiter", "title": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2024 18:50:37 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2025 17:34:22 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 11:14:49 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["P\u00f6ppel", "Korbinian", ""], ["Beck", "Maximilian", ""], ["Hochreiter", "Sepp", ""]], "extracted_entities": [{"text": "Transformers", "label": "Transformers"}, {"text": "sLSTM", "label": "Transformers"}, {"text": "RNNs", "label": "AI model"}, {"text": "Transformers", "label": "Transformers"}, {"text": "RNNs", "label": "AI model"}]}
{"id": "2412.09165", "submitter": "Zhijie Nie", "authors": "Zhijie Nie, Zhangchi Feng, Mingxin Li, Cunwang Zhang, Yanzhao Zhang,\n  Dingkun Long, Richong Zhang", "title": "When Text Embedding Meets Large Language Model: A Comprehensive Survey", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications-such as semantic matching, clustering,\nand information retrieval-continue to rely on text embeddings for their\nefficiency and effectiveness. Therefore, how to combine the LLMs and the text\nembeddings has become one of the hotspots of academic attention in recent\nyears. In this survey, we categorize the interplay between LLMs and text\nembeddings into three overarching themes: (1) LLM-augmented text embedding,\nenhancing traditional embedding methods with LLMs; (2) LLMs as text embedders,\nadapting their innate capabilities for high-quality embedding; and (3) Text\nembedding understanding with LLMs, leveraging LLMs to analyze and interpret\nembeddings. By organizing recent works based on interaction patterns rather\nthan specific downstream applications, we offer a novel and systematic overview\nof contributions from various research and application domains in the era of\nLLMs. Furthermore, we highlight the unresolved challenges that persisted in the\npre-LLM era with pre-trained language models (PLMs) and explore the emerging\nobstacles brought forth by LLMs. Building on this analysis, we outline\nprospective directions for the evolution of text embedding, addressing both\ntheoretical and practical opportunities in the rapidly advancing landscape of\nNLP.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2024 10:50:26 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 16:11:43 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Nie", "Zhijie", ""], ["Feng", "Zhangchi", ""], ["Li", "Mingxin", ""], ["Zhang", "Cunwang", ""], ["Zhang", "Yanzhao", ""], ["Long", "Dingkun", ""], ["Zhang", "Richong", ""]], "extracted_entities": [{"text": "Text embedding", "label": "Embedding"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "text embeddings", "label": "Embedding"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "text\nembeddings", "label": "Embedding"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "text\nembeddings", "label": "Embedding"}, {"text": "text embedding", "label": "Embedding"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "Text\nembedding", "label": "Embedding"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "text embedding", "label": "Embedding"}]}
{"id": "2412.13627", "submitter": "Rahul Sundar", "authors": "Rahul Sundar, Yucong Hu, Nishant Parashar, Antoine Blanchard, and\n  Boyko Dodov", "title": "TAUDiff: Highly efficient kilometer-scale downscaling using generative\n  diffusion models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deterministic regression-based downscaling models for climate variables often\nsuffer from spectral bias, which can be mitigated by generative models like\ndiffusion models. To enable efficient and reliable simulation of extreme\nweather events, it is crucial to achieve rapid turnaround, dynamical\nconsistency, and accurate spatio-temporal spectral recovery. We propose an\nefficient correction diffusion model, TAUDiff, that combines a deterministic\nspatio-temporal model for mean field downscaling with a smaller generative\ndiffusion model for recovering the fine-scale stochastic features. We\ndemonstrate the efficacy of this approach on downscaling atmospheric wind\nvelocity fields obtained from coarse GCM simulations. We then extend TAUDiff\nfor computationally efficient kilometer-scale downscaling of atmospheric wind\nvelocity fields. Owing to low inference times, our approach can ensure quicker\nsimulation of extreme events necessary for estimating associated risks and\neconomic losses.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2024 09:05:19 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 05:04:23 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Sundar", "Rahul", ""], ["Hu", "Yucong", ""], ["Parashar", "Nishant", ""], ["Blanchard", "Antoine", ""], ["Dodov", "Boyko", ""]], "extracted_entities": [{"text": "spectral bias", "label": "Model Bias and Fairness"}, {"text": "TAUDiff", "label": "AI model"}, {"text": "TAUDiff", "label": "AI model"}]}
{"id": "2412.14886", "submitter": "Nathan Goldman", "authors": "Anais Defossez, Laurens Vanderstraeten, Lucila Peralta Gavensky and\n  Nathan Goldman", "title": "Dynamic Realization of Majorana Zero Modes in a Particle-Conserving\n  Ladder", "comments": "Revised version; 13 pages, 9 figures, including Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.mes-hall cond-mat.quant-gas cond-mat.str-el", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a scheme to realize a topological superconducting system\nsupporting Majorana zero modes, within a number-conserving framework suitable\nfor optical-lattice experiments. Our approach builds on the engineering of\npair-hopping processes on a ladder geometry, using a sequence of pulses that\nactivate single-particle hopping in a time-periodic manner. We demonstrate that\nthis dynamic setting is well captured by an effective Hamiltonian that\npreserves the parity symmetry, a key requirement for the stabilization of\nMajorana zero modes. The phase diagram of our system is determined using a\nbosonization theory, which is then validated by a numerical study of the\ntopological bulk gap and entanglement spectrum using matrix product states. Our\nresults indicate that Majorana zero modes can be stabilized in a large\nparameter space, accessible in optical-lattice experiments.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2024 14:19:51 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 16:01:39 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Defossez", "Anais", ""], ["Vanderstraeten", "Laurens", ""], ["Gavensky", "Lucila Peralta", ""], ["Goldman", "Nathan", ""]], "extracted_entities": [{"text": "bosonization theory", "label": "quantisation"}]}
{"id": "2412.16780", "submitter": "Changchang Sun", "authors": "Changchang Sun and Ren Wang and Yihua Zhang and Jinghan Jia and\n  Jiancheng Liu and Gaowen Liu and Sijia Liu and Yan Yan", "title": "Forget Vectors at Play: Universal Input Perturbations Driving Machine\n  Unlearning in Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine unlearning (MU), which seeks to erase the influence of specific\nunwanted data from already-trained models, is becoming increasingly vital in\nmodel editing, particularly to comply with evolving data regulations like the\n``right to be forgotten''. Conventional approaches are predominantly\nmodel-based, typically requiring retraining or fine-tuning the model's weights\nto meet unlearning requirements. In this work, we approach the MU problem from\na novel input perturbation-based perspective, where the model weights remain\nintact throughout the unlearning process. We demonstrate the existence of a\nproactive input-based unlearning strategy, referred to forget vector, which can\nbe generated as an input-agnostic data perturbation and remains as effective as\nmodel-based approximate unlearning approaches. We also explore forget vector\narithmetic, whereby multiple class-specific forget vectors are combined through\nsimple operations (e.g., linear combinations) to generate new forget vectors\nfor unseen unlearning tasks, such as forgetting arbitrary subsets across\nclasses. Extensive experiments validate the effectiveness and adaptability of\nthe forget vector, showcasing its competitive performance relative to\nstate-of-the-art model-based methods. Codes are available at\nhttps://github.com/Changchangsun/Forget-Vector.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2024 21:27:22 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2025 17:00:18 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 01:25:27 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Sun", "Changchang", ""], ["Wang", "Ren", ""], ["Zhang", "Yihua", ""], ["Jia", "Jinghan", ""], ["Liu", "Jiancheng", ""], ["Liu", "Gaowen", ""], ["Liu", "Sijia", ""], ["Yan", "Yan", ""]], "extracted_entities": [{"text": "Machine unlearning", "label": "Zero-shot Learning"}, {"text": "evolving data regulations", "label": "AI Ethics"}, {"text": "fine-tuning", "label": "Fine-tuning"}]}
{"id": "2412.16833", "submitter": "Kaiwen Zuo", "authors": "Kaiwen Zuo, Yirui Jiang, Fan Mo, Pietro Lio", "title": "KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge\n  Graph Enhancement for Medical Diagnosis", "comments": "10 pages,5 figures,published to AAAI-25 Bridge Program", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating Large Language Models (LLMs) in healthcare diagnosis demands\nsystematic frameworks that can handle complex medical scenarios while\nmaintaining specialized expertise. We present KG4Diagnosis, a novel\nhierarchical multi-agent framework that combines LLMs with automated knowledge\ngraph construction, encompassing 362 common diseases across medical\nspecialties. Our framework mirrors real-world medical systems through a\ntwo-tier architecture: a general practitioner (GP) agent for initial assessment\nand triage, coordinating with specialized agents for in-depth diagnosis in\nspecific domains. The core innovation lies in our end-to-end knowledge graph\ngeneration methodology, incorporating: (1) semantic-driven entity and relation\nextraction optimized for medical terminology, (2) multi-dimensional decision\nrelationship reconstruction from unstructured medical texts, and (3)\nhuman-guided reasoning for knowledge expansion. KG4Diagnosis serves as an\nextensible foundation for specialized medical diagnosis systems, with\ncapabilities to incorporate new diseases and medical knowledge. The framework's\nmodular design enables seamless integration of domain-specific enhancements,\nmaking it valuable for developing targeted medical diagnosis systems. We\nprovide architectural guidelines and protocols to facilitate adoption across\nmedical contexts.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2024 02:40:59 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2025 00:07:09 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 03:05:30 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zuo", "Kaiwen", ""], ["Jiang", "Yirui", ""], ["Mo", "Fan", ""], ["Lio", "Pietro", ""]], "extracted_entities": [{"text": "Large Language Models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "KG4Diagnosis", "label": "Foundation Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "KG4Diagnosis", "label": "Foundation Model"}]}
{"id": "2412.17741", "submitter": "Rui Qian", "authors": "Rui Qian, Xin Yin, Dejing Dou", "title": "Reasoning to Attend: Try to Understand How <SEG> Token Works", "comments": "This work has been accepted to CVPR 2025, please refer to\n  https://github.com/rui-qian/READ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current Large Multimodal Models (LMMs) empowered visual grounding typically\nrely on $\\texttt{<SEG>}$ tokens as a text prompt to jointly optimize the\nvision-language model (e.g., LLaVA) and the downstream task-specific model\n(e.g., SAM). However, we observe that little research has looked into how it\nworks.In this work, we first visualize the similarity maps, which are obtained\nby computing the semantic similarity between the $\\texttt{<SEG>}$ token and the\nimage token embeddings derived from the last hidden layer in both the LLaVA\nencoder and SAM decoder. Intriguingly, we have found that a striking\nconsistency holds in terms of activation responses in the similarity map, which\nreveals that what the $\\texttt{<SEG>}$ token contributes to is semantic\nsimilarity within image-text pairs. Specifically, the $\\texttt{<SEG>}$ token, a\nplaceholder expanded in text vocabulary, extensively queries among individual\ntokenized image patches to match the semantics of an object from text to the\npaired image, while the Large Language Models (LLMs) are being fine-tuned. Upon\nthe above findings, we present READ, which facilitates LMMs' resilient\n$\\textbf{REA}$soning capability of where to atten$\\textbf{D}$ under the\nguidance of highly activated points borrowed from similarity maps. Remarkably,\nREAD features an intuitive design, Similarity as Points module (SasP), which\ncan be seamlessly applied to $\\texttt{<SEG>}$-like paradigms in a plug-and-play\nfashion. Also, extensive experiments have been conducted on ReasonSeg and\nRefCOCO(+/g) datasets. To validate whether READ suffers from catastrophic\nforgetting of previous skills after fine-tuning, we further assess its\ngeneration ability on an augmented FP-RefCOCO(+/g) dataset. All codes and\nmodels are publicly available at https://github.com/rui-qian/READ.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2024 17:44:05 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2024 10:19:44 GMT"}, {"version": "v3", "created": "Mon, 20 Jan 2025 07:57:50 GMT"}, {"version": "v4", "created": "Wed, 5 Mar 2025 15:55:51 GMT"}, {"version": "v5", "created": "Thu, 6 Mar 2025 04:11:30 GMT"}, {"version": "v6", "created": "Thu, 13 Mar 2025 14:04:12 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Qian", "Rui", ""], ["Yin", "Xin", ""], ["Dou", "Dejing", ""]], "extracted_entities": [{"text": "text prompt", "label": "Prompting"}, {"text": "SAM", "label": "Large Language Model"}, {"text": "image token embeddings", "label": "Embedding"}, {"text": "SAM", "label": "Large Language Model"}, {"text": "fine-tuning", "label": "Fine-tuning"}]}
{"id": "2412.18652", "submitter": "Christian Copetti", "authors": "Andrea Antinucci, Christian Copetti, Giovanni Galati and Giovanni Rizi", "title": "Topological Constraints on Defect Dynamics", "comments": "31 pages, 3 figures. v2 added references and improved nomenclature", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-th cond-mat.str-el", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extended objects (defects) in Quantum Field Theory exhibit rich, nontrivial\ndynamics describing a variety of physical phenomena. These systems often\ninvolve strong coupling at long distances, where the bulk and defects interact,\nmaking analytical studies challenging. By carefully analyzing the behavior of\nbulk symmetries in the presence of defects, we uncover robust topological\nconstraints on defect RG flows. Specifically, we introduce the notions of\n$\\textit{defect anomalies}$ and $\\textit{symmetry reflecting defects}$, both of\nwhich are RG-invariant. Several known notions, such as higher-form symmetries,\nfractionalization, and projective lines, are revealed to be manifestations of\ndefect anomalies, which also encompass novel phenomena and forbid trivial\ndefect dynamics in the IR. Meanwhile, symmetry reflecting defects are shown to\nremain coupled at low energies, imposing powerful dynamical constraints. We\nverify our findings through concrete examples: exactly solvable defect RG flows\nin (1+1)d Conformal Field Theories with symmetry reflecting lines and a surface\ndefect in (2+1)d scalar QED.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2024 19:00:01 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 11:15:45 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Antinucci", "Andrea", ""], ["Copetti", "Christian", ""], ["Galati", "Giovanni", ""], ["Rizi", "Giovanni", ""]], "extracted_entities": [{"text": "fractionalization", "label": "quantisation"}]}
{"id": "2412.18947", "submitter": "Kaiwen Zuo", "authors": "Kaiwen Zuo, Yirui Jiang", "title": "MedHallBench: A New Benchmark for Assessing Hallucination in Medical\n  Large Language Models", "comments": "Published to AAAI-25 Bridge Program", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical Large Language Models (MLLMs) have demonstrated potential in\nhealthcare applications, yet their propensity for hallucinations -- generating\nmedically implausible or inaccurate information -- presents substantial risks\nto patient care. This paper introduces MedHallBench, a comprehensive benchmark\nframework for evaluating and mitigating hallucinations in MLLMs. Our\nmethodology integrates expert-validated medical case scenarios with established\nmedical databases to create a robust evaluation dataset. The framework employs\na sophisticated measurement system that combines automated ACHMI (Automatic\nCaption Hallucination Measurement in Medical Imaging) scoring with rigorous\nclinical expert evaluations and utilizes reinforcement learning methods to\nachieve automatic annotation. Through an optimized reinforcement learning from\nhuman feedback (RLHF) training pipeline specifically designed for medical\napplications, MedHallBench enables thorough evaluation of MLLMs across diverse\nclinical contexts while maintaining stringent accuracy standards. We conducted\ncomparative experiments involving various models, utilizing the benchmark to\nestablish a baseline for widely adopted large language models (LLMs). Our\nfindings indicate that ACHMI provides a more nuanced understanding of the\neffects of hallucinations compared to traditional metrics, thereby highlighting\nits advantages in hallucination assessment. This research establishes a\nfoundational framework for enhancing MLLMs' reliability in healthcare settings\nand presents actionable strategies for addressing the critical challenge of AI\nhallucinations in medical applications.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2024 16:51:29 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2025 00:16:52 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 02:29:47 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zuo", "Kaiwen", ""], ["Jiang", "Yirui", ""]], "extracted_entities": [{"text": "Medical Large Language Models", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MedHallBench", "label": "Foundation Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MedHallBench", "label": "Foundation Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}]}
{"id": "2501.00012", "submitter": "Ian McCue", "authors": "Samuel Price, Zhaoxi Cao, Ian McCue", "title": "On-the-Fly Path Planning for the Design of Compositional Gradients in\n  High Dimensions", "comments": "28 pages, 9 figures, 3 supplementary figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cond-mat.mtrl-sci", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Functional gradients have recently experienced an explosion in activity due\nto advances in manufacturing, where compositions can now be spatially varied\non-the-fly during fabrication. In addition, modern computational thermodynamics\nhas reached sufficient maturity -- with respect to property databases and the\navailability of commercial software -- that gradients can be designed with\nspecific sets of properties. Despite these successes, there are practical\nlimitations on the calculation speeds of these thermodynamic tools that make it\nintractable to model every element in an alloy. As a result, most path planning\nis carried out via surrogate models on simplified systems (e.g., approximating\nInconel 718 as Ni$_{59}$Cr$_{23}$Fe$_{18}$ instead of\nNi$_{53}$Cr$_{23}$Fe$_{18}$Nb$_{3}$Mo$_{2}$Ti$_{1}$). In this work, we\ndemonstrate that this limitation can be overcome using a combination of\non-the-fly sampling and a conjectured corollary of the lever rule for\ntransformations of isothermal paths in arbitrary compositional dimensions. We\nquantitatively benchmark the effectiveness of this new method and find that it\ncan be as much as 106 times more efficient than surrogate modeling.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2024 18:54:40 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 17:20:56 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Price", "Samuel", ""], ["Cao", "Zhaoxi", ""], ["McCue", "Ian", ""]], "extracted_entities": [{"text": "lever rule", "label": "Scaling law"}]}
{"id": "2501.02509", "submitter": "Hui Li", "authors": "Hui Li, Xiaoyu Ren, Hongjiu Yu, Huiyu Duan, Kai Li, Ying Chen, Libo\n  Wang, Xiongkuo Min, Guangtao Zhai, Xu Liu", "title": "Facial Attractiveness Prediction in Live Streaming: A New Benchmark and\n  Multi-modal Method", "comments": "Section 3 in Images Collection has description errors about data\n  cleaning. The compared methods data of Table 3 lacks other metrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial attractiveness prediction (FAP) has long been an important computer\nvision task, which could be widely applied in live streaming for facial\nretouching, content recommendation, etc. However, previous FAP datasets are\neither small, closed-source, or lack diversity. Moreover, the corresponding FAP\nmodels exhibit limited generalization and adaptation ability. To overcome these\nlimitations, in this paper we present LiveBeauty, the first large-scale\nlive-specific FAP dataset, in a more challenging application scenario, i.e.,\nlive streaming. 10,000 face images are collected from a live streaming platform\ndirectly, with 200,000 corresponding attractiveness annotations obtained from a\nwell-devised subjective experiment, making LiveBeauty the largest open-access\nFAP dataset in the challenging live scenario. Furthermore, a multi-modal FAP\nmethod is proposed to measure the facial attractiveness in live streaming.\nSpecifically, we first extract holistic facial prior knowledge and multi-modal\naesthetic semantic features via a Personalized Attractiveness Prior Module\n(PAPM) and a Multi-modal Attractiveness Encoder Module (MAEM), respectively,\nthen integrate the extracted features through a Cross-Modal Fusion Module\n(CMFM). Extensive experiments conducted on both LiveBeauty and other\nopen-source FAP datasets demonstrate that our proposed method achieves\nstate-of-the-art performance. Dataset will be available soon.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2025 11:43:35 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 02:34:18 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Hui", ""], ["Ren", "Xiaoyu", ""], ["Yu", "Hongjiu", ""], ["Duan", "Huiyu", ""], ["Li", "Kai", ""], ["Chen", "Ying", ""], ["Wang", "Libo", ""], ["Min", "Xiongkuo", ""], ["Zhai", "Guangtao", ""], ["Liu", "Xu", ""]], "extracted_entities": [{"text": "LiveBeauty", "label": "Large Language Model"}, {"text": "LiveBeauty", "label": "Large Language Model"}, {"text": "LiveBeauty", "label": "Large Language Model"}]}
{"id": "2501.04467", "submitter": "Marc Aubreville", "authors": "Christof A. Bertram, Viktoria Weiss, Taryn A. Donovan, Sweta Banerjee,\n  Thomas Conrad, Jonas Ammeling, Robert Klopfleisch, Christopher Kaltenecker,\n  Marc Aubreville", "title": "Histologic Dataset of Normal and Atypical Mitotic Figures on Human\n  Breast Cancer (AMi-Br)", "comments": null, "journal-ref": "In: Palm, C., et al. Bildverarbeitung f\\\"ur die Medizin 2025. BVM\n  2025. Informatik aktuell. Springer Vieweg, Wiesbaden", "doi": "10.1007/978-3-658-47422-5_25", "report-no": null, "categories": "cs.CV cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Assessment of the density of mitotic figures (MFs) in histologic tumor\nsections is an important prognostic marker for many tumor types, including\nbreast cancer. Recently, it has been reported in multiple works that the\nquantity of MFs with an atypical morphology (atypical MFs, AMFs) might be an\nindependent prognostic criterion for breast cancer. AMFs are an indicator of\nmutations in the genes regulating the cell cycle and can lead to aberrant\nchromosome constitution (aneuploidy) of the tumor cells. To facilitate further\nresearch on this topic using pattern recognition, we present the first ever\npublicly available dataset of atypical and normal MFs (AMi-Br). For this, we\nutilized two of the most popular MF datasets (MIDOG 2021 and TUPAC) and\nsubclassified all MFs using a three expert majority vote. Our final dataset\nconsists of 3,720 MFs, split into 832 AMFs (22.4%) and 2,888 normal MFs (77.6%)\nacross all 223 tumor cases in the combined set. We provide baseline\nclassification experiments to investigate the consistency of the dataset, using\na Monte Carlo cross-validation and different strategies to combat class\nimbalance. We found an averaged balanced accuracy of up to 0.806 when using a\npatch-level data set split, and up to 0.713 when using a patient-level split.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2025 12:41:42 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 07:10:26 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Bertram", "Christof A.", ""], ["Weiss", "Viktoria", ""], ["Donovan", "Taryn A.", ""], ["Banerjee", "Sweta", ""], ["Conrad", "Thomas", ""], ["Ammeling", "Jonas", ""], ["Klopfleisch", "Robert", ""], ["Kaltenecker", "Christopher", ""], ["Aubreville", "Marc", ""]], "extracted_entities": [{"text": "AMFs", "label": "LLMs"}, {"text": "AMFs", "label": "LLMs"}, {"text": "MIDOG 2021", "label": "Open-source LLMs"}, {"text": "TUPAC", "label": "Open-source LLMs"}, {"text": "MFs", "label": "LLMs"}, {"text": "AMFs", "label": "LLMs"}, {"text": "MFs", "label": "LLMs"}]}
{"id": "2501.05031", "submitter": "Ronghao Dang", "authors": "Ronghao Dang, Yuqian Yuan, Wenqi Zhang, Yifei Xin, Boqiang Zhang, Long\n  Li, Liuyi Wang, Qinyang Zeng, Xin Li, Lidong Bing", "title": "ECBench: Can Multi-modal Foundation Models Understand the Egocentric\n  World? A Holistic Embodied Cognition Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The enhancement of generalization in robots by large vision-language models\n(LVLMs) is increasingly evident. Therefore, the embodied cognitive abilities of\nLVLMs based on egocentric videos are of great interest. However, current\ndatasets for embodied video question answering lack comprehensive and\nsystematic evaluation frameworks. Critical embodied cognitive issues, such as\nrobotic self-cognition, dynamic scene perception, and hallucination, are rarely\naddressed. To tackle these challenges, we propose ECBench, a high-quality\nbenchmark designed to systematically evaluate the embodied cognitive abilities\nof LVLMs. ECBench features a diverse range of scene video sources, open and\nvaried question formats, and 30 dimensions of embodied cognition. To ensure\nquality, balance, and high visual dependence, ECBench uses class-independent\nmeticulous human annotation and multi-round question screening strategies.\nAdditionally, we introduce ECEval, a comprehensive evaluation system that\nensures the fairness and rationality of the indicators. Utilizing ECBench, we\nconduct extensive evaluations of proprietary, open-source, and task-specific\nLVLMs. ECBench is pivotal in advancing the embodied cognitive capabilities of\nLVLMs, laying a solid foundation for developing reliable core models for\nembodied agents. All data and code are available at\nhttps://github.com/Rh-Dang/ECBench.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2025 07:43:49 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 07:45:55 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Dang", "Ronghao", ""], ["Yuan", "Yuqian", ""], ["Zhang", "Wenqi", ""], ["Xin", "Yifei", ""], ["Zhang", "Boqiang", ""], ["Li", "Long", ""], ["Wang", "Liuyi", ""], ["Zeng", "Qinyang", ""], ["Li", "Xin", ""], ["Bing", "Lidong", ""]], "extracted_entities": [{"text": "large vision-language models", "label": "Large Language Model"}, {"text": "LVLMs", "label": "Large Language Model"}, {"text": "LVLMs", "label": "Large Language Model"}, {"text": "LVLMs", "label": "Large Language Model"}, {"text": "LVLMs", "label": "Large Language Model"}, {"text": "LVLMs", "label": "Large Language Model"}]}
{"id": "2501.06828", "submitter": "Ruizhe Ou", "authors": "Ruizhe Ou, Yuan Hu, Fan Zhang, Jiaxin Chen, Yu Liu", "title": "GeoPix: Multi-Modal Large Language Model for Pixel-level Image\n  Understanding in Remote Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-modal large language models (MLLMs) have achieved remarkable success in\nimage- and region-level remote sensing (RS) image understanding tasks, such as\nimage captioning, visual question answering, and visual grounding. However,\nexisting RS MLLMs lack the pixel-level dialogue capability, which involves\nresponding to user instructions with segmentation masks for specific instances.\nIn this paper, we propose GeoPix, a RS MLLM that extends image understanding\ncapabilities to the pixel level. This is achieved by equipping the MLLM with a\nmask predictor, which transforms visual features from the vision encoder into\nmasks conditioned on the LLM's segmentation token embeddings. To facilitate the\nsegmentation of multi-scale objects in RS imagery, a class-wise learnable\nmemory module is integrated into the mask predictor to capture and store\nclass-wise geo-context at the instance level across the entire dataset. In\naddition, to address the absence of large-scale datasets for training\npixel-level RS MLLMs, we construct the GeoPixInstruct dataset, comprising\n65,463 images and 140,412 instances, with each instance annotated with text\ndescriptions, bounding boxes, and masks. Furthermore, we develop a two-stage\ntraining strategy to balance the distinct requirements of text generation and\nmasks prediction in multi-modal multi-task optimization. Extensive experiments\nverify the effectiveness and superiority of GeoPix in pixel-level segmentation\ntasks, while also maintaining competitive performance in image- and\nregion-level benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2025 14:45:27 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 08:16:01 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Ou", "Ruizhe", ""], ["Hu", "Yuan", ""], ["Zhang", "Fan", ""], ["Chen", "Jiaxin", ""], ["Liu", "Yu", ""]], "extracted_entities": [{"text": "Multi-modal large language models", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "segmentation token embeddings", "label": "Embedding"}, {"text": "MLLMs", "label": "Large Language Model"}]}
{"id": "2501.08137", "submitter": "Marcella Astrid", "authors": "Marcella Astrid, Enjie Ghorbel, Djamila Aouada", "title": "Audio-Visual Deepfake Detection With Local Temporal Inconsistencies", "comments": "Accepted in ICASSP 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes an audio-visual deepfake detection approach that aims to\ncapture fine-grained temporal inconsistencies between audio and visual\nmodalities. To achieve this, both architectural and data synthesis strategies\nare introduced. From an architectural perspective, a temporal distance map,\ncoupled with an attention mechanism, is designed to capture these\ninconsistencies while minimizing the impact of irrelevant temporal\nsubsequences. Moreover, we explore novel pseudo-fake generation techniques to\nsynthesize local inconsistencies. Our approach is evaluated against\nstate-of-the-art methods using the DFDC and FakeAVCeleb datasets, demonstrating\nits effectiveness in detecting audio-visual deepfakes.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2025 14:15:10 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2025 09:14:14 GMT"}, {"version": "v3", "created": "Wed, 12 Mar 2025 10:22:54 GMT"}, {"version": "v4", "created": "Thu, 13 Mar 2025 11:02:33 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Astrid", "Marcella", ""], ["Ghorbel", "Enjie", ""], ["Aouada", "Djamila", ""]], "extracted_entities": [{"text": "attention mechanism", "label": "Attention mechanism"}]}
{"id": "2501.10736", "submitter": "Shanwen Wang", "authors": "Shanwen Wang, Xin Sun, Changrui Chen, Danfeng Hong, Jungong Han", "title": "Semi-supervised Semantic Segmentation for Remote Sensing Images via\n  Multi-scale Uncertainty Consistency and Cross-Teacher-Student Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning offers an appealing solution for remote sensing (RS)\nimage segmentation to relieve the burden of labor-intensive pixel-level\nlabeling. However, RS images pose unique challenges, including rich multi-scale\nfeatures and high inter-class similarity. To address these problems, this paper\nproposes a novel semi-supervised Multi-Scale Uncertainty and\nCross-Teacher-Student Attention (MUCA) model for RS image semantic segmentation\ntasks. Specifically, MUCA constrains the consistency among feature maps at\ndifferent layers of the network by introducing a multi-scale uncertainty\nconsistency regularization. It improves the multi-scale learning capability of\nsemi-supervised algorithms on unlabeled data. Additionally, MUCA utilizes a\nCross-Teacher-Student attention mechanism to guide the student network, guiding\nthe student network to construct more discriminative feature representations\nthrough complementary features from the teacher network. This design\neffectively integrates weak and strong augmentations (WA and SA) to further\nboost segmentation performance. To verify the effectiveness of our model, we\nconduct extensive experiments on ISPRS-Potsdam and LoveDA datasets. The\nexperimental results show the superiority of our method over state-of-the-art\nsemi-supervised methods. Notably, our model excels in distinguishing highly\nsimilar objects, showcasing its potential for advancing semi-supervised RS\nimage segmentation tasks.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2025 11:57:20 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 14:18:36 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Shanwen", ""], ["Sun", "Xin", ""], ["Chen", "Changrui", ""], ["Hong", "Danfeng", ""], ["Han", "Jungong", ""]], "extracted_entities": [{"text": "Semi-supervised learning", "label": "Few-shot Learning"}, {"text": "Cross-Teacher-Student attention mechanism", "label": "Attention mechanism"}]}
{"id": "2501.10800", "submitter": "Emanuele La Malfa", "authors": "Oliver Goldstein, Emanuele La Malfa, Felix Drinkall, Samuele Marro,\n  Michael Wooldridge", "title": "Jailbreaking Large Language Models in Infinitely Many Ways", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the ``Infinitely Many Paraphrases'' attacks (IMP), a category of\njailbreaks that leverages the increasing capabilities of a model to handle\nparaphrases and encoded communications to bypass their defensive mechanisms.\nIMPs' viability pairs and grows with a model's capabilities to handle and bind\nthe semantics of simple mappings between tokens and work extremely well in\npractice, posing a concrete threat to the users of the most powerful LLMs in\ncommerce. We show how one can bypass the safeguards of the most powerful open-\nand closed-source LLMs and generate content that explicitly violates their\nsafety policies. One can protect against IMPs by improving the guardrails and\nmaking them scale with the LLMs' capabilities. For two categories of attacks\nthat are straightforward to implement, i.e., bijection and encoding, we discuss\ntwo defensive strategies, one in token and the other in embedding space. We\nconclude with some research questions we believe should be prioritised to\nenhance the defensive mechanisms of LLMs and our understanding of their safety.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2025 15:39:53 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 08:43:27 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Goldstein", "Oliver", ""], ["La Malfa", "Emanuele", ""], ["Drinkall", "Felix", ""], ["Marro", "Samuele", ""], ["Wooldridge", "Michael", ""]], "extracted_entities": [{"text": "LLMs", "label": "LLM"}, {"text": "safety policies", "label": "AI Ethics"}, {"text": "LLMs", "label": "LLM"}, {"text": "embedding space", "label": "Embedding"}, {"text": "LLMs", "label": "LLM"}]}
{"id": "2501.11069", "submitter": "Shibang Liu", "authors": "Shibang Liu, Xuemei Xie, and Guangming Shi", "title": "Refinement Module based on Parse Graph of Feature Map for Human Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parse graphs of the human body can be obtained in the human brain to help\nhumans complete the human Pose Estimation better (HPE). It contains a\nhierarchical structure, like a tree structure, and context relations among\nnodes. To equip models with such capabilities, many researchers predefine the\nparse graph of body structure to design HPE frameworks. However, these\nframeworks struggle to adapt to instances that deviate from the predefined\nparse graph and they are often parameter-heavy. Unlike them, we view the\nfeature map holistically, much like the human body. It can be optimized using\nparse graphs, where nodes' implicit feature representation boosts adaptability,\navoiding rigid structural limitations. In this paper, we design the Refinement\nModule based on the Parse Graph of feature map (RMPG), which includes two\nstages: top-down decomposition and bottom-up combination. In the first stage,\nthe feature map is constructed into a tree structure through recursive\ndecomposition, with each node representing a sub-feature map, thereby achieving\nhierarchical modeling of features. In the second stage, context information is\ncalculated and sub-feature maps with context are recursively connected to\ngradually build a refined feature map. Additionally, we design a hierarchical\nnetwork with fewer parameters using multiple RMPG modules to model the context\nrelations and hierarchies in the parse graph of body structure for HPE, some of\nwhich are supervised to obtain context relations among body parts. Our network\nachieves excellent results on multiple mainstream human pose datasets and the\neffectiveness of RMPG is proven on different methods. The code of RMPG will be\nopen.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2025 15:05:15 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2025 13:07:16 GMT"}, {"version": "v3", "created": "Sun, 2 Mar 2025 03:01:19 GMT"}, {"version": "v4", "created": "Thu, 13 Mar 2025 02:41:37 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Liu", "Shibang", ""], ["Xie", "Xuemei", ""], ["Shi", "Guangming", ""]], "extracted_entities": [{"text": "RMPG", "label": "contextual Embedding"}]}
{"id": "2501.12673", "submitter": "Daniel Ruberman", "authors": "Dave Auckly, Daniel Ruberman", "title": "Exotic families of embeddings", "comments": "25 page, 9 figures. Added acknowledgment to 2nd version", "journal-ref": "Frontiers in geometry and topology, Proc. Sympos. Pure Math., 109,\n  71--98, (2024) Amer. Math. Soc., Providence, RI", "doi": null, "report-no": null, "categories": "math.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a number of topologically trivial but smoothly non-trivial\nfamilies of embeddings of 3-manifolds in 4-manifolds. These include embeddings\nof homology spheres in $S^4$ that are not isotopic but have diffeomorphic\ncomplements, and families (parameterized by high-dimensional spheres) of\nembeddings of any 3-manifold that embeds in a blown-up K3 surface. In each\ncase, the families are constructed so as to be topologically trivial in an\nappropriate sense. We also illustrate a general technique for converting a\nnon-trivial family of embeddings into a non-trivial family of submanifolds.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2025 06:16:27 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 12:40:53 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Auckly", "Dave", ""], ["Ruberman", "Daniel", ""]], "extracted_entities": [{"text": "embeddings", "label": "Embedding"}, {"text": "embeddings", "label": "Embedding"}, {"text": "embeddings", "label": "Embedding"}]}
{"id": "2501.12973", "submitter": "Da-Sol Joo", "authors": "Da-Sol Joo", "title": "A global similarity correction for the RANS modeling of natural\n  convection in unstably stratified flows", "comments": "39 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study proposes a global similarity correction for Reynolds-averaged\nNavier--Stokes (RANS) modeling of buoyancy effects in unstably stratified\nflows. Conventional two-equation RANS models (e.g., the $k$-$\\varepsilon$\nmodel) lack a clear criterion for incorporating unstable buoyancy effects in\ntheir scale-determining equations (e.g., $\\varepsilon$-equation). To address\nthis gap, a global correction function is introduced, derived from a\ngeneralized algebraic formulation that incorporates available potential energy\nas an additional parameter. This function reproduces a global similarity law\ncommonly observed in natural convection flows--for instance, the correlation\namong the Nusselt, Rayleigh, and Prandtl numbers, which can be approximately\nexpressed as a single power law over a wide parameter range. A calibration\nmethod is proposed in which an approximate analytical solution for\nRayleigh--B\\'enard convection is obtained via equilibrium analysis, confirming\nthat the proposed model captures similarity relations not addressed by\nconventional one-point closures. Numerical results show significantly improved\nagreement with experimental data, accurately reproducing Nusselt number\ndependencies over broad ranges of Rayleigh and Prandtl numbers in unstably\nstratified flows, such as Rayleigh--B\\'enard convection and two types of\ninternally heated convection. The method remains fully compatible with standard\nRANS frameworks and reverts to traditional turbulence treatments in\nshear-driven flows where buoyant effects are negligible. By introducing only a\nsingle, simple, algebraic global function in the conventional\n$\\varepsilon$-equation, this approach significantly enhances the accuracy and\nrobustness of buoyancy-driven turbulence simulations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2025 15:57:40 GMT"}, {"version": "v2", "created": "Tue, 11 Mar 2025 16:27:13 GMT"}, {"version": "v3", "created": "Wed, 12 Mar 2025 03:28:37 GMT"}, {"version": "v4", "created": "Thu, 13 Mar 2025 02:47:37 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Joo", "Da-Sol", ""]], "extracted_entities": [{"text": "global similarity law", "label": "Scaling law"}, {"text": "Nusselt", "label": "BERT"}, {"text": "Prandtl", "label": "BERT"}, {"text": "single power law", "label": "Scaling law"}, {"text": "Nusselt", "label": "BERT"}, {"text": "Prandtl", "label": "BERT"}]}
{"id": "2501.13354", "submitter": "Weijie Li", "authors": "Yongxiang Liu and Weijie Li and Li Liu and Jie Zhou and Bowen Peng and\n  Yafei Song and Xuying Xiong and Wei Yang and Tianpeng Liu and Zhen Liu and\n  Xiang Li", "title": "ATRNet-STAR: A Large Dataset and Benchmark Towards Remote Sensing Object\n  Recognition in the Wild", "comments": "17 pages, 14 figures; ATRNet-STAR:\n  https://github.com/waterdisappear/ATRNet-STAR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The absence of publicly available, large-scale, high-quality datasets for\nSynthetic Aperture Radar Automatic Target Recognition (SAR ATR) has\nsignificantly hindered the application of rapidly advancing deep learning\ntechniques, which hold huge potential to unlock new capabilities in this field.\nThis is primarily because collecting large volumes of diverse target samples\nfrom SAR images is prohibitively expensive, largely due to privacy concerns,\nthe characteristics of microwave radar imagery perception, and the need for\nspecialized expertise in data annotation. Throughout the history of SAR ATR\nresearch, there have been only a number of small datasets, mainly including\ntargets like ships, airplanes, buildings, etc. There is only one vehicle\ndataset MSTAR collected in the 1990s, which has been a valuable source for SAR\nATR. To fill this gap, this paper introduces a large-scale, new dataset named\nATRNet-STAR with 40 different vehicle categories collected under various\nrealistic imaging conditions and scenes. It marks a substantial advancement in\ndataset scale and diversity, comprising over 190,000 well-annotated samples, 10\ntimes larger than its predecessor, the famous MSTAR. Building such a large\ndataset is a challenging task, and the data collection scheme will be detailed.\nSecondly, we illustrate the value of ATRNet-STAR via extensively evaluating the\nperformance of 15 representative methods with 7 different experimental settings\non challenging classification and detection benchmarks derived from the\ndataset. Finally, based on our extensive experiments, we identify valuable\ninsights for SAR ATR and discuss potential future research directions in this\nfield. We hope that the scale, diversity, and benchmark of ATRNet-STAR can\nsignificantly facilitate the advancement of SAR ATR.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2025 03:42:22 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2025 23:57:36 GMT"}, {"version": "v3", "created": "Fri, 7 Mar 2025 14:28:51 GMT"}, {"version": "v4", "created": "Thu, 13 Mar 2025 10:51:12 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Liu", "Yongxiang", ""], ["Li", "Weijie", ""], ["Liu", "Li", ""], ["Zhou", "Jie", ""], ["Peng", "Bowen", ""], ["Song", "Yafei", ""], ["Xiong", "Xuying", ""], ["Yang", "Wei", ""], ["Liu", "Tianpeng", ""], ["Liu", "Zhen", ""], ["Li", "Xiang", ""]], "extracted_entities": [{"text": "privacy concerns", "label": "AI Ethics"}]}
{"id": "2501.13859", "submitter": "Shiyu Zhang", "authors": "Shiyu Zhang, Cheng Yan, Yang Liu, Chenchen Jing, Lei Zhou, Wenjun Wang", "title": "Learning Visual Proxy for Compositional Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compositional Zero-Shot Learning (CZSL) aims to recognize novel\nattribute-object compositions by leveraging knowledge from seen compositions.\nExisting methods align textual prototypes with visual features through\nVision-Language Models (VLMs), but they face two key limitations: (1) modality\ngaps hinder the discrimination of semantically similar composition pairs, and\n(2) single-modal textual prototypes lack fine-grained visual cues, creating\nbottlenecks in VLM-based CZSL. In this paper, we introduce Visual Proxy\nLearning, a novel approach that facilitates the learning of distinct visual\ndistributions, effectively reducing the modality gap and improving\ncompositional generalization performance. Specifically, we initialize visual\nproxies for various attributes, objects, and their compositions using text\nrepresentations. By optimizing the visual space, we capture fine-grained visual\ncues and guide the learning of more discriminative visual representations for\nattributes, objects and compositions. Furthermore, we propose an effective\nCross-Modal Joint Learning (CMJL) strategy that imposes cross-modal constraints\nbetween the original text-image space and the fine-grained visual space. This\napproach not only boosts generalization for previously unseen composition pairs\nbut also sharpens the discrimination of similar pairs, fostering more robust\nand precise learning. Extensive experiments demonstrate state-of-the-art\nperformance in closed-world scenarios and competitive open-world results across\nfour established CZSL benchmarks, validating the effectiveness of our approach\nin advancing compositional generalization.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2025 17:30:27 GMT"}, {"version": "v2", "created": "Wed, 12 Mar 2025 05:46:59 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 04:04:32 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhang", "Shiyu", ""], ["Yan", "Cheng", ""], ["Liu", "Yang", ""], ["Jing", "Chenchen", ""], ["Zhou", "Lei", ""], ["Wang", "Wenjun", ""]], "extracted_entities": [{"text": "Compositional Zero-Shot Learning", "label": "Zero-shot Learning"}, {"text": "CZSL", "label": "Zero-shot Learning"}, {"text": "Visual Proxy\nLearning", "label": "Zero-shot Learning"}, {"text": "CZSL", "label": "Few-shot Learning"}]}
{"id": "2501.14225", "submitter": "Rong Ye", "authors": "Rong Ye, Yongxin Zhang, Yikai Zhang, Haoyu Kuang, Zhongyu Wei, Peng\n  Sun", "title": "Multi-agent KTO: Reinforcing Strategic Interactions of Large Language\n  Model in Language Game", "comments": "Preprint. Code and data will be available at\n  https://reneeye.github.io/MaKTO.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Achieving Artificial General Intelligence (AGI) requires AI agents that can\nnot only make stratigic decisions but also engage in flexible and meaningful\ncommunication. Inspired by Wittgenstein's language game theory in Philosophical\nInvestigations, we propose that language agents can learn through in-context\ninteraction rather than traditional multi-stage frameworks that separate\ndecision-making from language expression. Using Werewolf, a social deduction\ngame that tests language understanding, strategic interaction, and\nadaptability, we develop the Multi-agent Kahneman & Tversky's Optimization\n(MaKTO). MaKTO engages diverse models in extensive gameplay to generate\nunpaired desirable and unacceptable responses, then employs KTO to refine the\nmodel's decision-making process. In 9-player Werewolf games, MaKTO achieves a\n61% average win rate across various models, outperforming GPT-4o and two-stage\nRL agents by relative improvements of 23.0% and 10.9%, respectively. Notably,\nMaKTO also demonstrates human-like performance, winning 60% against expert\nplayers and showing only 49% detectability in Turing-style blind tests.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2025 04:09:03 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 03:55:17 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Ye", "Rong", ""], ["Zhang", "Yongxin", ""], ["Zhang", "Yikai", ""], ["Kuang", "Haoyu", ""], ["Wei", "Zhongyu", ""], ["Sun", "Peng", ""]], "extracted_entities": [{"text": "GPT-4o", "label": "GPT"}]}
{"id": "2501.15187", "submitter": "Zecheng Li", "authors": "Zecheng Li, Wengang Zhou, Weichao Zhao, Kepeng Wu, Hezhen Hu, Houqiang\n  Li", "title": "Uni-Sign: Toward Unified Sign Language Understanding at Scale", "comments": "Accepted by ICLR 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign language pre-training has gained increasing attention for its ability to\nenhance performance across various sign language understanding (SLU) tasks.\nHowever, existing methods often suffer from a gap between pre-training and\nfine-tuning, leading to suboptimal results. To address this, we propose\nUni-Sign, a unified pre-training framework that eliminates the gap between\npre-training and downstream SLU tasks through a large-scale generative\npre-training strategy and a novel fine-tuning paradigm. First, we introduce\nCSL-News, a large-scale Chinese Sign Language (CSL) dataset containing 1,985\nhours of video paired with textual annotations, which enables effective\nlarge-scale pre-training. Second, Uni-Sign unifies SLU tasks by treating\ndownstream tasks as a single sign language translation (SLT) task during\nfine-tuning, ensuring seamless knowledge transfer between pre-training and\nfine-tuning. Furthermore, we incorporate a prior-guided fusion (PGF) module and\na score-aware sampling strategy to efficiently fuse pose and RGB information,\naddressing keypoint inaccuracies and improving computational efficiency.\nExtensive experiments across multiple SLU benchmarks demonstrate that Uni-Sign\nachieves state-of-the-art performance across multiple downstream SLU tasks.\nDataset and code are available at github.com/ZechengLi19/Uni-Sign.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2025 11:51:23 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2025 09:44:28 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 12:51:29 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Zecheng", ""], ["Zhou", "Wengang", ""], ["Zhao", "Weichao", ""], ["Wu", "Kepeng", ""], ["Hu", "Hezhen", ""], ["Li", "Houqiang", ""]], "extracted_entities": [{"text": "fine-tuning", "label": "Fine-tuning"}, {"text": "fine-tuning", "label": "Fine-tuning"}, {"text": "fine-tuning", "label": "Fine-tuning"}]}
{"id": "2501.16879", "submitter": "Sergio Morell Ortega", "authors": "Jos\\'e V. Manj\\'on, Sergio Morell-Ortega, Marina Ruiz-Perez, Boris\n  Mansencal, Edern Le Bot, Marien Gadea, Enrique Lanuza, Gwenaelle Catheline,\n  Thomas Tourdias, Vincent Planche, R\\'emi Giraud, Denis Rivi\\`ere,\n  Jean-Fran\\c{c}ois Mangin, Nicole Labra-Avila, Roberto Vivo-Hernando, Gregorio\n  Rubio, Fernando Aparici, Maria de la Iglesia-Vaya, Pierrick Coup\\'e", "title": "Ultra-high resolution multimodal MRI dense labelled holistic brain atlas", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we introduce holiAtlas, a holistic, multimodal and\nhigh-resolution human brain atlas. This atlas covers different levels of\ndetails of the human brain anatomy, from the organ to the substructure level,\nusing a new dense labelled protocol generated from the fusion of multiple local\nprotocols at different scales. This atlas has been constructed averaging images\nand segmentations of 75 healthy subjects from the Human Connectome Project\ndatabase. Specifically, MR images of T1, T2 and WMn (White Matter nulled)\ncontrasts at 0.125 $mm^{3}$ resolution that were nonlinearly registered and\naveraged using symmetric group-wise normalisation to construct the atlas. At\nthe finest level, the holiAtlas protocol has 350 different labels derived from\n10 different delineation protocols. These labels were grouped at different\nscales to provide a holistic view of the brain at different levels in a\ncoherent and consistent manner. This multiscale and multimodal atlas can be\nused for the development of new ultra-high resolution segmentation methods that\ncan potentially leverage the early detection of neurological disorders.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2025 12:06:29 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 12:50:31 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Manj\u00f3n", "Jos\u00e9 V.", ""], ["Morell-Ortega", "Sergio", ""], ["Ruiz-Perez", "Marina", ""], ["Mansencal", "Boris", ""], ["Bot", "Edern Le", ""], ["Gadea", "Marien", ""], ["Lanuza", "Enrique", ""], ["Catheline", "Gwenaelle", ""], ["Tourdias", "Thomas", ""], ["Planche", "Vincent", ""], ["Giraud", "R\u00e9mi", ""], ["Rivi\u00e8re", "Denis", ""], ["Mangin", "Jean-Fran\u00e7ois", ""], ["Labra-Avila", "Nicole", ""], ["Vivo-Hernando", "Roberto", ""], ["Rubio", "Gregorio", ""], ["Aparici", "Fernando", ""], ["de la Iglesia-Vaya", "Maria", ""], ["Coup\u00e9", "Pierrick", ""]], "extracted_entities": [{"text": "symmetric group-wise normalisation", "label": "quantisation"}]}
{"id": "2501.17568", "submitter": "Ehsan Aminian", "authors": "Ehsan Aminian, Rita P. Ribeiro, Joao Gama", "title": "Histogram Approaches for Imbalanced Data Streams Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imbalanced domains pose a significant challenge in real-world predictive\nanalytics, particularly in the context of regression. While existing research\nhas primarily focused on batch learning from static datasets, limited attention\nhas been given to imbalanced regression in online learning scenarios. Intending\nto address this gap, in prior work, we proposed sampling strategies based on\nChebyshevs inequality as the first methodologies designed explicitly for data\nstreams. However, these approaches operated under the restrictive assumption\nthat rare instances exclusively reside at distribution extremes. This study\nintroduces histogram-based sampling strategies to overcome this constraint,\nproposing flexible solutions for imbalanced regression in evolving data\nstreams. The proposed techniques -- Histogram-based Undersampling (HistUS) and\nHistogram-based Oversampling (HistOS) -- employ incremental online histograms\nto dynamically detect and prioritize rare instances across arbitrary regions of\nthe target distribution to improve predictions in the rare cases. Comprehensive\nexperiments on synthetic and real-world benchmarks demonstrate that HistUS and\nHistOS substantially improve rare-case prediction accuracy, outperforming\nbaseline models while maintaining competitiveness with Chebyshev-based\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2025 11:03:02 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 11:38:47 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Aminian", "Ehsan", ""], ["Ribeiro", "Rita P.", ""], ["Gama", "Joao", ""]], "extracted_entities": [{"text": "batch learning", "label": "Few-shot Learning"}]}
{"id": "2501.18883", "submitter": "Jae Yong Lee", "authors": "Jae Yong Lee, Sungmin Kang, Shin Yoo", "title": "Predictive Prompt Analysis", "comments": "Accepted by FSE 2025, 5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large Language Models (LLMs) are machine learning models that have seen\nwidespread adoption due to their capability of handling previously difficult\ntasks. LLMs, due to their training, are sensitive to how exactly a question is\npresented, also known as prompting. However, prompting well is challenging, as\nit has been difficult to uncover principles behind prompting -- generally,\ntrial-and-error is the most common way of improving prompts, despite its\nsignificant computational cost. In this context, we argue it would be useful to\nperform `predictive prompt analysis', in which an automated technique would\nperform a quick analysis of a prompt and predict how the LLM would react to it,\nrelative to a goal provided by the user. As a demonstration of the concept, we\npresent Syntactic Prevalence Analyzer (SPA), a predictive prompt analysis\napproach based on sparse autoencoders (SAEs). SPA accurately predicted how\noften an LLM would generate target syntactic structures during code synthesis,\nwith up to 0.994 Pearson correlation between the predicted and actual\nprevalence of the target structure. At the same time, SPA requires only 0.4\\%\nof the time it takes to run the LLM on a benchmark. As LLMs are increasingly\nused during and integrated into modern software development, our proposed\npredictive prompt analysis concept has the potential to significantly ease the\nuse of LLMs for both practitioners and researchers.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2025 04:34:43 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 07:23:59 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Lee", "Jae Yong", ""], ["Kang", "Sungmin", ""], ["Yoo", "Shin", ""]], "extracted_entities": [{"text": "Large Language Models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "prompting", "label": "Prompting"}, {"text": "prompting", "label": "Prompting"}, {"text": "prompting", "label": "Prompting"}, {"text": "prompt", "label": "Prompting"}, {"text": "prompt", "label": "Prompting"}, {"text": "LLM", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}]}
{"id": "2502.02307", "submitter": "Jiawei Qin", "authors": "Jiawei Qin, Xucong Zhang, Yusuke Sugano", "title": "UniGaze: Towards Universal Gaze Estimation via Large-scale Pre-Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite decades of research on data collection and model architectures,\ncurrent gaze estimation models encounter significant challenges in generalizing\nacross diverse data domains. Recent advances in self-supervised pre-training\nhave shown remarkable performances in generalization across various vision\ntasks. However, their effectiveness in gaze estimation remains unexplored. We\npropose UniGaze, for the first time, leveraging large-scale in-the-wild facial\ndatasets for gaze estimation through self-supervised pre-training. Through\nsystematic investigation, we clarify critical factors that are essential for\neffective pretraining in gaze estimation. Our experiments reveal that\nself-supervised approaches designed for semantic tasks fail when applied to\ngaze estimation, while our carefully designed pre-training pipeline\nconsistently improves cross-domain performance. Through comprehensive\nexperiments of challenging cross-dataset evaluation and novel protocols\nincluding leave-one-dataset-out and joint-dataset settings, we demonstrate that\nUniGaze significantly improves generalization across multiple data domains\nwhile minimizing reliance on costly labeled data. source code and model are\navailable at https://github.com/ut-vision/UniGaze.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2025 13:24:23 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 15:59:03 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Qin", "Jiawei", ""], ["Zhang", "Xucong", ""], ["Sugano", "Yusuke", ""]], "extracted_entities": [{"text": "self-supervised pre-training", "label": "Few-shot Learning"}]}
{"id": "2502.06393", "submitter": "Dongheng Qian", "authors": "Dongheng Qian and Jing Wang", "title": "Quantum Non-Local Nonstabilizerness", "comments": "7+5 pages, 3+0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum entanglement and quantum nonstabilizerness are fundamental resources\nthat characterize distinct aspects of a quantum state: entanglement reflects\nnon-local correlations, while nonstabilizerness quantifies the deviation from\nstabilizer states. A quantum state becomes a valuable resource for applications\nlike universal quantum computation only when both quantities are present. Here,\nwe propose that quantum non-local nonstabilizerness (NN) serves as an effective\nmeasure of this combined resource, incorporating both entanglement and\nnonstabilizerness. We demonstrate that NN can be precisely computed for\ntwo-qubit pure states, where it is directly related to the entanglement\nspectrum. We then extend the definition of NN to mixed states and explore its\npresence in many-body quantum systems, revealing that the two-point NN decays\naccording to a power law in critical states. Furthermore, we explore\nmeasurement-induced NN and uncover an intriguing phenomenon termed\n\"nonstabilizerness swapping\", analogous to entanglement swapping, wherein\npost-measurement NN decays more slowly than any pre-measurement correlations.\nOur results thus represent a pivotal step towards accurately quantifying the\n\"quantumness\" of a state and reveal the potential for manipulating this\nresource through measurements.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2025 12:28:52 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2025 10:56:26 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 07:37:03 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Qian", "Dongheng", ""], ["Wang", "Jing", ""]], "extracted_entities": [{"text": "entanglement", "label": "quantisation"}, {"text": "entanglement", "label": "quantisation"}, {"text": "entanglement", "label": "quantisation"}, {"text": "entanglement", "label": "quantisation"}, {"text": "power law", "label": "Scaling law"}, {"text": "entanglement", "label": "quantisation"}]}
{"id": "2502.06432", "submitter": "Huaqiu Li", "authors": "Huaqiu Li, Wang Zhang, Xiaowan Hu, Tao Jiang, Zikang Chen, Haoqian\n  Wang", "title": "Prompt-SID: Learning Structural Representation Prompt via Latent\n  Diffusion for Single-Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many studies have concentrated on constructing supervised models utilizing\npaired datasets for image denoising, which proves to be expensive and\ntime-consuming. Current self-supervised and unsupervised approaches typically\nrely on blind-spot networks or sub-image pairs sampling, resulting in pixel\ninformation loss and destruction of detailed structural information, thereby\nsignificantly constraining the efficacy of such methods. In this paper, we\nintroduce Prompt-SID, a prompt-learning-based single image denoising framework\nthat emphasizes preserving of structural details. This approach is trained in a\nself-supervised manner using downsampled image pairs. It captures\noriginal-scale image information through structural encoding and integrates\nthis prompt into the denoiser. To achieve this, we propose a structural\nrepresentation generation model based on the latent diffusion process and\ndesign a structural attention module within the transformer-based denoiser\narchitecture to decode the prompt. Additionally, we introduce a scale replay\ntraining mechanism, which effectively mitigates the scale gap from images of\ndifferent resolutions. We conduct comprehensive experiments on synthetic,\nreal-world, and fluorescence imaging datasets, showcasing the remarkable\neffectiveness of Prompt-SID. Our code will be released at\nhttps://github.com/huaqlili/Prompt-SID.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2025 13:09:47 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 12:49:20 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Huaqiu", ""], ["Zhang", "Wang", ""], ["Hu", "Xiaowan", ""], ["Jiang", "Tao", ""], ["Chen", "Zikang", ""], ["Wang", "Haoqian", ""]], "extracted_entities": [{"text": "prompt", "label": "Prompting"}, {"text": "structural attention module", "label": "Attention mechanism"}, {"text": "prompt", "label": "Prompting"}, {"text": "scale replay\ntraining mechanism", "label": "Attention mechanism"}]}
{"id": "2502.07842", "submitter": "Jiyoon Kim", "authors": "Jiyoon Kim, Kang Eun Jeon, Yulhwa Kim, and Jong Hwan Ko", "title": "Column-wise Quantization of Weights and Partial Sums for Accurate and\n  Efficient Compute-In-Memory Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compute-in-memory (CIM) is an efficient method for implementing deep neural\nnetworks (DNNs) but suffers from substantial overhead from analog-to-digital\nconverters (ADCs), especially as ADC precision increases. Low-precision ADCs\ncan reduce this overhead but introduce partial-sum quantization errors\ndegrading accuracy. Additionally, low-bit weight constraints, imposed by cell\nlimitations and the need for multiple cells for higher-bit weights, present\nfurther challenges. While fine-grained partial-sum quantization has been\nstudied to lower ADC resolution effectively, weight granularity, which limits\noverall partial-sum quantized accuracy, remains underexplored. This work\naddresses these challenges by aligning weight and partial-sum quantization\ngranularities at the column-wise level. Our method improves accuracy while\nmaintaining dequantization overhead, simplifies training by removing two-stage\nprocesses, and ensures robustness to memory cell variations via independent\ncolumn-wise scale factors. We also propose an open-source CIM-oriented\nconvolution framework to handle fine-grained weights and partial-sums\nefficiently, incorporating a novel tiling method and group convolution.\nExperimental results on ResNet-20 (CIFAR-10, CIFAR-100) and ResNet-18\n(ImageNet) show accuracy improvements of 0.99%, 2.69%, and 1.01%, respectively,\ncompared to the best-performing related works. Additionally, variation analysis\nreveals the robustness of our method against memory cell variations. These\nfindings highlight the effectiveness of our quantization scheme in enhancing\naccuracy and robustness while maintaining hardware efficiency in CIM-based DNN\nimplementations. Our code is available at\nhttps://github.com/jiyoonkm/ColumnQuant.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2025 05:32:14 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 11:32:19 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Kim", "Jiyoon", ""], ["Jeon", "Kang Eun", ""], ["Kim", "Yulhwa", ""], ["Ko", "Jong Hwan", ""]], "extracted_entities": [{"text": "partial-sum quantization", "label": "quantisation"}, {"text": "partial-sum quantization", "label": "quantisation"}, {"text": "partial-sum quantization", "label": "quantisation"}]}
{"id": "2502.07938", "submitter": "Andrianos Michail", "authors": "Andrianos Michail, Corina Julia Racl\\'e, Juri Opitz, Simon Clematide", "title": "Adapting Multilingual Embedding Models to Historical Luxembourgish", "comments": "To appear in LaTeCH-CLfL 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The growing volume of digitized historical texts requires effective semantic\nsearch using text embeddings. However, pre-trained multilingual models face\nchallenges with historical content due to OCR noise and outdated spellings.\nThis study examines multilingual embeddings for cross-lingual semantic search\nin historical Luxembourgish (LB), a low-resource language. We collect\nhistorical Luxembourgish news articles from various periods and use GPT-4o for\nsentence segmentation and translation, generating 20,000 parallel training\nsentences per language pair. Additionally, we create a semantic search\n(Historical LB Bitext Mining) evaluation set and find that existing models\nperform poorly on cross-lingual search for historical Luxembourgish. Using our\nhistorical and additional modern parallel training data, we adapt several\nmultilingual embedding models through contrastive learning or knowledge\ndistillation and increase accuracy significantly for all models. We release our\nadapted models and historical Luxembourgish-German/French/English bitexts to\nsupport further research.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2025 20:35:29 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2025 10:38:40 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 13:19:30 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Michail", "Andrianos", ""], ["Racl\u00e9", "Corina Julia", ""], ["Opitz", "Juri", ""], ["Clematide", "Simon", ""]], "extracted_entities": [{"text": "text embeddings", "label": "Embedding"}, {"text": "multilingual embeddings", "label": "Embedding"}, {"text": "GPT-4o", "label": "GPT"}, {"text": "contrastive learning", "label": "Few-shot Learning"}, {"text": "knowledge\ndistillation", "label": "Knowledge distillation"}]}
{"id": "2502.08658", "submitter": "Hao Lyu", "authors": "Hao Lyu, Yanyong Guo, Pan Liu, Shuo Feng, Weilin Ren and Quansheng Yue", "title": "Knowledge-data fusion dominated vehicle platoon dynamics modeling and\n  analysis: A physics-encoded deep learning approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, artificial intelligence (AI)-enabled nonlinear vehicle platoon\ndynamics modeling plays a crucial role in predicting and optimizing the\ninteractions between vehicles. Existing efforts lack the extraction and capture\nof vehicle behavior interaction features at the platoon scale. More\nimportantly, maintaining high modeling accuracy without losing physical\nanalyzability remains to be solved. To this end, this paper proposes a novel\nphysics-encoded deep learning network, named PeMTFLN, to model the nonlinear\nvehicle platoon dynamics. Specifically, an analyzable parameters encoded\ncomputational graph (APeCG) is designed to guide the platoon to respond to the\ndriving behavior of the lead vehicle while ensuring local stability. Besides, a\nmulti-scale trajectory feature learning network (MTFLN) is constructed to\ncapture platoon following patterns and infer the physical parameters required\nfor APeCG from trajectory data. The human-driven vehicle trajectory datasets\n(HIGHSIM) were used to train the proposed PeMTFLN. The trajectories prediction\nexperiments show that PeMTFLN exhibits superior compared to the baseline models\nin terms of predictive accuracy in speed and gap. The stability analysis result\nshows that the physical parameters in APeCG is able to reproduce the platoon\nstability in real-world condition. In simulation experiments, PeMTFLN performs\nlow inference error in platoon trajectories generation. Moreover, PeMTFLN also\naccurately reproduces ground-truth safety statistics. The code of proposed\nPeMTFLN is open source.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2025 05:10:46 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 13:42:00 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Lyu", "Hao", ""], ["Guo", "Yanyong", ""], ["Liu", "Pan", ""], ["Feng", "Shuo", ""], ["Ren", "Weilin", ""], ["Yue", "Quansheng", ""]], "extracted_entities": [{"text": "PeMTFLN", "label": "AI model"}]}
{"id": "2502.12029", "submitter": "Qi Zhao", "authors": "Qi Zhao, Hongyu Yang, Qi Song, Xinwei Yao, Xiangyang Li", "title": "KnowPath: Knowledge-enhanced Reasoning via LLM-generated Inference Paths\n  over Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious complex tasks, yet they still suffer from hallucinations. Introducing\nexternal knowledge, such as knowledge graph, can enhance the LLMs' ability to\nprovide factual answers. LLMs have the ability to interactively explore\nknowledge graphs. However, most approaches have been affected by insufficient\ninternal knowledge excavation in LLMs, limited generation of trustworthy\nknowledge reasoning paths, and a vague integration between internal and\nexternal knowledge. Therefore, we propose KnowPath, a knowledge-enhanced large\nmodel framework driven by the collaboration of internal and external knowledge.\nIt relies on the internal knowledge of the LLM to guide the exploration of\ninterpretable directed subgraphs in external knowledge graphs, better\nintegrating the two knowledge sources for more accurate reasoning. Extensive\nexperiments on multiple real-world datasets confirm the superiority of\nKnowPath.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2025 17:02:01 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 13:22:46 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhao", "Qi", ""], ["Yang", "Hongyu", ""], ["Song", "Qi", ""], ["Yao", "Xinwei", ""], ["Li", "Xiangyang", ""]], "extracted_entities": [{"text": "Large language models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "KnowPath", "label": "LLM-based"}]}
{"id": "2502.12455", "submitter": "Minxuan Lv", "authors": "Minxuan Lv, Zhenpeng Su, Leiyu Pan, Yizhe Xiong, Zijia Lin, Hui Chen,\n  Wei Zhou, Jungong Han, Guiguang Ding, Cheng Luo, Di Zhang, Kun Gai, Songlin\n  Hu", "title": "DSMoE: Matrix-Partitioned Experts with Dynamic Routing for\n  Computation-Efficient Dense LLMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As large language models continue to scale, computational costs and resource\nconsumption have emerged as significant challenges. While existing\nsparsification methods like pruning reduce computational overhead, they risk\nlosing model knowledge through parameter removal. This paper proposes DSMoE\n(Dynamic Sparse Mixture-of-Experts), a novel approach that achieves\nsparsification by partitioning pre-trained FFN layers into computational\nblocks. We implement adaptive expert routing using sigmoid activation and\nstraight-through estimators, enabling tokens to flexibly access different\naspects of model knowledge based on input complexity. Additionally, we\nintroduce a sparsity loss term to balance performance and computational\nefficiency. Extensive experiments on LLaMA models demonstrate that under\nequivalent computational constraints, DSMoE achieves superior performance\ncompared to existing pruning and MoE approaches across language modeling and\ndownstream tasks, particularly excelling in generation tasks. Analysis reveals\nthat DSMoE learns distinctive layerwise activation patterns, providing new\ninsights for future MoE architecture design.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2025 02:37:26 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 10:40:09 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Lv", "Minxuan", ""], ["Su", "Zhenpeng", ""], ["Pan", "Leiyu", ""], ["Xiong", "Yizhe", ""], ["Lin", "Zijia", ""], ["Chen", "Hui", ""], ["Zhou", "Wei", ""], ["Han", "Jungong", ""], ["Ding", "Guiguang", ""], ["Luo", "Cheng", ""], ["Zhang", "Di", ""], ["Gai", "Kun", ""], ["Hu", "Songlin", ""]], "extracted_entities": [{"text": "large language models", "label": "Large Language Model"}]}
{"id": "2502.14614", "submitter": "Mingyi Jia", "authors": "Mingyi Jia and Junwen Duan and Yan Song and Jianxin Wang", "title": "FIND: Fine-grained Information Density Guided Adaptive\n  Retrieval-Augmented Generation for Disease Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieval-Augmented Large Language Models (LLMs), which integrate external\nknowledge into LLMs, have shown remarkable performance in various medical\ndomains, including clinical diagnosis. However, existing RAG methods struggle\nto effectively assess task difficulty to make retrieval decisions, thereby\nfailing to meet the clinical requirements for balancing efficiency and\naccuracy. So in this paper, we propose FIND (\\textbf{F}ine-grained\n\\textbf{In}formation \\textbf{D}ensity Guided Adaptive RAG), a novel framework\nthat improves the reliability of RAG in disease diagnosis scenarios. FIND\nincorporates a fine-grained adaptive control module to determine whether\nretrieval is necessary based on the information density of the input. By\noptimizing the retrieval process and implementing a knowledge filtering module,\nFIND ensures that the retrieval is better suited to clinical scenarios.\nExperiments on three Chinese electronic medical record datasets demonstrate\nthat FIND significantly outperforms various baseline methods, highlighting its\neffectiveness in clinical diagnosis tasks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2025 14:52:36 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 13:13:07 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Jia", "Mingyi", ""], ["Duan", "Junwen", ""], ["Song", "Yan", ""], ["Wang", "Jianxin", ""]], "extracted_entities": [{"text": "RAG", "label": "RAG"}, {"text": "RAG", "label": "RAG"}, {"text": "RAG", "label": "RAG"}]}
{"id": "2502.17599", "submitter": "Zhongwei Wan", "authors": "Zhongwei Wan, Hui Shen, Xin Wang, Che Liu, Zheda Mai, Mi Zhang", "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference", "comments": "NAACL 2025 Main", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2025 19:34:52 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 04:04:08 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wan", "Zhongwei", ""], ["Shen", "Hui", ""], ["Wang", "Xin", ""], ["Liu", "Che", ""], ["Mai", "Zheda", ""], ["Zhang", "Mi", ""]], "extracted_entities": [{"text": "Multimodal Large Language Models", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MEDA", "label": "LLM"}, {"text": "cross-modal attention entropy", "label": "Attention mechanism"}, {"text": "MLLMs", "label": "Large Language Model"}]}
{"id": "2502.17906", "submitter": "Yuki Sato", "authors": "Yuki Sato and Kiyoshi Kanazawa", "title": "Exactly solvable model of the square-root price impact dynamics under\n  the long-range market-order correlation", "comments": "6 pages, 3 figures. analytical calculus are updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cond-mat.stat-mech econ.GN q-fin.EC q-fin.MF q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In econophysics, there are several enigmatic empirical laws: (i)~the\nmarket-order flow has strong persistence (long-range order-sign correlation),\nwell formulated as the Lillo-Mike-Farmer model. This phenomenon seems\nparadoxical given the diffusive and unpredictable price dynamics; (ii)~the\nprice impact $I(Q)$ of a large metaorder $Q$ follows the square-root law,\n$I(Q)\\propto \\sqrt{Q}$. In this Letter, we propose an exactly solvable model of\nthe nonlinear price-impact dynamics that unifies these enigmas. We generalize\nthe Lillo-Mike-Farmer model to nonlinear price-impact dynamics, which is mapped\nto an exactly solvable L\\'evy-walk model. Our exact solution and numerical\nsimulations reveal three important points: First, the price dynamics remains\ndiffusive under the square-root law, even under the long-range correlation.\nSecond, price-movement statistics follows truncated power laws with typical\nexponent around three. Third, volatility has long memory. While this simple\nmodel lacks adjustable free parameters, it naturally aligns even with other\nenigmatic empirical laws, such as (iii)~the inverse-cubic law for price\nstatistics and (iv)~volatility clustering. This work illustrates the crucial\nrole of the square-root law in understanding rich and complex financial price\ndynamics from a single coherent viewpoint.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2025 07:12:03 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 11:49:37 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Sato", "Yuki", ""], ["Kanazawa", "Kiyoshi", ""]], "extracted_entities": [{"text": "square-root law", "label": "Scaling law"}, {"text": "square-root law", "label": "Scaling law"}, {"text": "inverse-cubic law", "label": "Scaling law"}, {"text": "square-root law", "label": "Scaling law"}]}
{"id": "2502.18008", "submitter": "Yashan Wang", "authors": "Yashan Wang, Shangda Wu, Jianhuai Hu, Xingjian Du, Yueqi Peng, Yongxin\n  Huang, Shuai Fan, Xiaobing Li, Feng Yu, Maosong Sun", "title": "NotaGen: Advancing Musicality in Symbolic Music Generation with Large\n  Language Model Training Paradigms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce NotaGen, a symbolic music generation model aiming to explore the\npotential of producing high-quality classical sheet music. Inspired by the\nsuccess of Large Language Models (LLMs), NotaGen adopts pre-training,\nfine-tuning, and reinforcement learning paradigms (henceforth referred to as\nthe LLM training paradigms). It is pre-trained on 1.6M pieces of music in ABC\nnotation, and then fine-tuned on approximately 9K high-quality classical\ncompositions conditioned on \"period-composer-instrumentation\" prompts. For\nreinforcement learning, we propose the CLaMP-DPO method, which further enhances\ngeneration quality and controllability without requiring human annotations or\npredefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in\nsymbolic music generation models with different architectures and encoding\nschemes. Furthermore, subjective A/B tests show that NotaGen outperforms\nbaseline models against human compositions, greatly advancing musical\naesthetics in symbolic music generation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2025 09:12:07 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2025 08:18:41 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2025 07:02:39 GMT"}, {"version": "v4", "created": "Thu, 13 Mar 2025 13:50:00 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Yashan", ""], ["Wu", "Shangda", ""], ["Hu", "Jianhuai", ""], ["Du", "Xingjian", ""], ["Peng", "Yueqi", ""], ["Huang", "Yongxin", ""], ["Fan", "Shuai", ""], ["Li", "Xiaobing", ""], ["Yu", "Feng", ""], ["Sun", "Maosong", ""]], "extracted_entities": [{"text": "Large Language Models", "label": "Large Language Model"}, {"text": "fine-tuning", "label": "Fine-tuning"}, {"text": "reinforcement learning", "label": "Few-shot Learning"}, {"text": "fine-tuned", "label": "Fine-tuning"}, {"text": "prompts", "label": "Prompting"}, {"text": "reinforcement learning", "label": "Few-shot Learning"}]}
{"id": "2502.18485", "submitter": "Jiaqi Xu", "authors": "Jiaqi Xu, Cuiling Lan, Xuejin Chen, Yan Lu", "title": "Deciphering Functions of Neurons in Vision-Language Models", "comments": "22 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The burgeoning growth of open-sourced vision-language models (VLMs) has\ncatalyzed a plethora of applications across diverse domains. Ensuring the\ntransparency and interpretability of these models is critical for fostering\ntrustworthy and responsible AI systems. In this study, our objective is to\ndelve into the internals of VLMs to interpret the functions of individual\nneurons. We observe the activations of neurons with respects to the input\nvisual tokens and text tokens, and reveal some interesting findings.\nParticularly, we found that there are neurons responsible for only visual or\ntext information, or both, respectively, which we refer to them as visual\nneurons, text neurons, and multi-modal neurons, respectively. We build a\nframework that automates the explanation of neurons with the assistant of\nGPT-4o. Meanwhile, for visual neurons, we propose an activation simulator to\nassess the reliability of the explanations for visual neurons. System\nstatistical analyses on top of one representative VLM of LLaVA, uncover the\nbehaviors/characteristics of different categories of neurons.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2025 10:00:06 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2025 06:32:05 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 07:13:38 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Xu", "Jiaqi", ""], ["Lan", "Cuiling", ""], ["Chen", "Xuejin", ""], ["Lu", "Yan", ""]], "extracted_entities": [{"text": "VLMs", "label": "Open-source LLMs"}, {"text": "GPT-4o", "label": "GPT"}]}
{"id": "2502.18548", "submitter": "Milan Vojnovic", "authors": "Milan Vojnovic and Se-Young Yun", "title": "What is the Alignment Objective of GRPO?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this note, we examine the aggregation of preferences achieved by the Group\nPolicy Optimisation (GRPO) algorithm, a reinforcement learning method used to\ntrain advanced artificial intelligence models such as DeepSeek-R1-Zero and\nDeepSeekMath. The GRPO algorithm trains a policy using a reward preference\nmodel, which is computed by sampling a set of outputs for a given context,\nobserving the corresponding rewards, and applying shift-and-scale normalisation\nto these reward values. Additionally, it incorporates a penalty function to\ndiscourage deviations from a reference policy.\n  We present a framework that enables us to characterise the stationary\npolicies of the GRPO algorithm. This analysis reveals that the aggregation of\npreferences differs fundamentally from standard logarithmic pooling, which is\nimplemented by other approaches such as RLHF. The precise form of preference\naggregation arises from the way the reward preference model is defined and from\nthe penalty function, which we show to essentially correspond to the reverse\nKullback-Leibler (KL) divergence between the aggregation policy and the\nreference policy.\n  Interestingly, we demonstrate that for groups of size two, the reward\npreference model corresponds to pairwise comparison preferences, similar to\nthose in other alignment methods based on pairwise comparison feedback. We\nprovide explicit characterisations of the aggregate preference for binary\nquestions, for groups of size two, and in the limit of large group size. This\nprovides insights into the dependence of the aggregate preference on parameters\nsuch as the regularisation constant and the confidence margin of question\nanswers.\n  Finally, we discuss the aggregation of preferences obtained by modifying the\nGRPO algorithm to use direct KL divergence as the penalty or to use rewards\nwithout scale normalisation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2025 15:56:56 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2025 10:18:29 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 16:48:34 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Vojnovic", "Milan", ""], ["Yun", "Se-Young", ""]], "extracted_entities": [{"text": "reward preference\nmodel", "label": "AI model"}, {"text": "shift-and-scale normalisation", "label": "quantisation"}, {"text": "reward preference model", "label": "AI model"}, {"text": "reward\npreference model", "label": "AI model"}]}
{"id": "2502.19339", "submitter": "Tohida Rehman Ms.", "authors": "Tohida Rehman, Soumabha Ghosh, Kuntal Das, Souvik Bhattacharjee,\n  Debarshi Kumar Sanyal, Samiran Chattopadhyay", "title": "Evaluating LLMs and Pre-trained Models for Text Summarization Across\n  Diverse Datasets", "comments": "5 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Text summarization plays a crucial role in natural language processing by\ncondensing large volumes of text into concise and coherent summaries. As\ndigital content continues to grow rapidly and the demand for effective\ninformation retrieval increases, text summarization has become a focal point of\nresearch in recent years. This study offers a thorough evaluation of four\nleading pre-trained and open-source large language models: BART, FLAN-T5,\nLLaMA-3-8B, and Gemma-7B, across five diverse datasets CNN/DM, Gigaword, News\nSummary, XSum, and BBC News. The evaluation employs widely recognized automatic\nmetrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and METEOR, to assess\nthe models' capabilities in generating coherent and informative summaries. The\nresults reveal the comparative strengths and limitations of these models in\nprocessing various text types.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2025 17:32:07 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 09:40:42 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Rehman", "Tohida", ""], ["Ghosh", "Soumabha", ""], ["Das", "Kuntal", ""], ["Bhattacharjee", "Souvik", ""], ["Sanyal", "Debarshi Kumar", ""], ["Chattopadhyay", "Samiran", ""]], "extracted_entities": [{"text": "Text summarization", "label": "Knowledge distillation"}, {"text": "text summarization", "label": "Knowledge distillation"}, {"text": "FLAN-T5", "label": "Large Language Model"}, {"text": "Gigaword", "label": "Large Language Model"}, {"text": "ROUGE-1", "label": "BERT"}, {"text": "BERTScore", "label": "BERT"}]}
{"id": "2502.19363", "submitter": "Ru Peng", "authors": "Ru Peng, Kexin Yang, Yawen Zeng, Junyang Lin, Dayiheng Liu, Junbo Zhao", "title": "DataMan: Data Manager for Pre-training Large Language Models", "comments": "ICLR2025 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance emergence of large language models (LLMs) driven by data\nscaling laws makes the selection of pre-training data increasingly important.\nHowever, existing methods rely on limited heuristics and human intuition,\nlacking comprehensive and clear guidelines. To address this, we are inspired by\n``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit\nits performance. As its pre-training capabilities are related to perplexity\n(PPL), we derive 14 quality criteria from the causes of text perplexity\nanomalies and introduce 15 common application domains to support domain mixing.\nIn this paper, we train a Data Manager (DataMan) to learn quality ratings and\ndomain recognition from pointwise rating, and use it to annotate a 447B token\npre-training corpus with 14 quality ratings and domain type. Our experiments\nvalidate our approach, using DataMan to select 30B tokens to train a\n1.3B-parameter language model, demonstrating significant improvements in\nin-context learning (ICL), perplexity, and instruction-following ability over\nthe state-of-the-art baseline. The best-performing model, based on the Overall\nScore l=5 surpasses a model trained with 50% more data using uniform sampling.\nWe continue pre-training with high-rated, domain-specific data annotated by\nDataMan to enhance domain-specific ICL performance and thus verify DataMan's\ndomain mixing ability. Our findings emphasize the importance of quality\nranking, the complementary nature of quality criteria, and their low\ncorrelation with perplexity, analyzing misalignment between PPL and ICL\nperformance. We also thoroughly analyzed our pre-training dataset, examining\nits composition, the distribution of quality ratings, and the original document\nsources.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2025 18:01:19 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 15:42:07 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Peng", "Ru", ""], ["Yang", "Kexin", ""], ["Zeng", "Yawen", ""], ["Lin", "Junyang", ""], ["Liu", "Dayiheng", ""], ["Zhao", "Junbo", ""]], "extracted_entities": [{"text": "large language models", "label": "Large Language Model"}, {"text": "data\nscaling laws", "label": "Scaling law"}, {"text": "prompting", "label": "Prompting"}, {"text": "in-context learning", "label": "contextual Embedding"}]}
{"id": "2502.19478", "submitter": "Jonas Matuszak", "authors": "Sowmiya Balan, Torsten Bringmann, Felix Kahlhoefer, Jonas Matuszak and\n  Carlo Tasillo", "title": "Sub-GeV dark matter and nano-Hertz gravitational waves from a\n  classically conformal dark sector", "comments": "v2: figure names corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ph astro-ph.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Strong first-order phase transitions in a dark sector offer a compelling\nexplanation for the stochastic gravitational wave background in the nano-Hertz\nrange recently detected by pulsar timing arrays (PTAs). We explore the\npossibility that such a phase transition at the same time gives mass to a\nstable fermion that accounts for the observed dark matter abundance and leads\nto testable effects in laboratory experiments. Concretely, we consider a\nclassically conformal dark sector with a hidden $U(1)^\\prime$ gauge symmetry\nthat couples to the Standard Model via kinetic mixing. Since the PTA signal\nrequires a phase transition in the MeV temperature range, spontaneous symmetry\nbreaking gives rise to a sub-GeV dark matter candidate that couples to the\nStandard Model via a dark photon mediator and obtains its relic abundance via\nannihilations into electrons and dark Higgs bosons. Such a scenario is tightly\nconstrained by laboratory searches for dark photons and cosmological\nconstraints on the decays of dark Higgs bosons after the phase transition. We\nshow that viable parameter regions can be found both for the case that the dark\nHiggs bosons remain in equilibrium with the Standard Model and that they\ndecouple and only decay much later. In the latter case, the parameter regions\npreferred by the PTA signal and the dark matter relic abundance can be fully\nexplored by future beam-dump experiments searching for missing energy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2025 19:00:01 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 17:59:14 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Balan", "Sowmiya", ""], ["Bringmann", "Torsten", ""], ["Kahlhoefer", "Felix", ""], ["Matuszak", "Jonas", ""], ["Tasillo", "Carlo", ""]], "extracted_entities": [{"text": "Standard Model", "label": "Foundation Model"}]}
{"id": "2502.19679", "submitter": "Linzhuo Li", "authors": "Linzhuo li", "title": "Old Experience Helps: Leveraging Survey Methodology to Improve AI Text\n  Annotation Reliability in Social Sciences", "comments": "8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a framework for assessing the reliability of Large\nLanguage Model (LLM) text annotations in social science research by adapting\nestablished survey methodology principles. Drawing parallels between survey\nrespondent behavior and LLM outputs, the study implements three key\ninterventions: option randomization, position randomization, and reverse\nvalidation. While traditional accuracy metrics may mask model instabilities,\nparticularly in edge cases, the framework provides a more comprehensive\nreliability assessment. Using the F1000 dataset in biomedical science and three\nsizes of Llama models (8B, 70B, and 405B parameters), the paper demonstrates\nthat these survey-inspired interventions can effectively identify unreliable\nannotations that might otherwise go undetected through accuracy metrics alone.\nThe results show that 5-25% of LLM annotations change under these\ninterventions, with larger models exhibiting greater stability. Notably, for\nrare categories approximately 50% of \"correct\" annotations demonstrate low\nreliability when subjected to this framework. The paper then introduce an\ninformation-theoretic reliability score (R-score) based on Kullback-Leibler\ndivergence that quantifies annotation confidence and distinguishes between\nrandom guessing and meaningful annotations at the case level. This approach\ncomplements existing expert validation methods by providing a scalable way to\nassess internal annotation reliability and offers practical guidance for prompt\ndesign and downstream analysis.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2025 01:42:10 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 03:06:47 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["li", "Linzhuo", ""]], "extracted_entities": [{"text": "Large\nLanguage Model", "label": "Large Language Model"}, {"text": "LLM", "label": "Large Language Model"}, {"text": "LLM", "label": "Large Language Model"}, {"text": "LLM", "label": "Large Language Model"}, {"text": "prompt\ndesign", "label": "Prompting"}]}
{"id": "2502.20524", "submitter": "Mirko Mizzoni", "authors": "Mirko Mizzoni, Pieter van Goor, Antonio Franchi", "title": "Unified Feedback Linearization for Nonlinear Systems with Dexterous and\n  Energy-Saving Modes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Systems with a high number of inputs compared to the degrees of freedom (e.g.\na mobile robot with Mecanum wheels) often have a minimal set of\nenergy-efficient inputs needed to achieve a main task (e.g. position tracking)\nand a set of energy-intense inputs needed to achieve an additional auxiliary\ntask (e.g. orientation tracking). This letter presents a unified control\nscheme, derived through feedback linearization, that can switch between two\nmodes: an energy-saving mode, which tracks the main task using only the\nenergy-efficient inputs while forcing the energy-intense inputs to zero, and a\ndexterous mode, which also uses the energy-intense inputs to track the\nauxiliary task as needed. The proposed control guarantees the exponential\ntracking of the main task and that the dynamics associated with the main task\nevolve independently of the a priori unknown switching signal. When the control\nis operating in dexterous mode, the exponential tracking of the auxiliary task\nis also guaranteed. Numerical simulations on an omnidirectional Mecanum wheel\nrobot validate the effectiveness of the proposed approach and demonstrate the\neffect of the switching signal on the exponential tracking behavior of the main\nand auxiliary tasks.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2025 21:19:09 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 12:23:35 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Mizzoni", "Mirko", ""], ["van Goor", "Pieter", ""], ["Franchi", "Antonio", ""]], "extracted_entities": [{"text": "feedback linearization", "label": "quantisation"}]}
{"id": "2503.00351", "submitter": "Weonjong Lee", "authors": "Seungyeob Jwa, Jeehun Kim, Sunghee Kim, Sunkyu Lee, Weonjong Lee,\n  Sungwoo Park", "title": "2025 update on $\\varepsilon_K$ in the Standard Model with lattice QCD\n  inputs", "comments": "21 pages, 15 figures, references updated. arXiv admin note: text\n  overlap with arXiv:1808.09657", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-lat hep-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present theoretical results for the indirect CP violation parameter,\n$|\\varepsilon_K|$ calculated directly from the standard model using lattice QCD\ninputs such as $\\hat{B}_K$, $|V_{cb}|$, $|V_{us}|$, $|V_{ud}|$, $\\xi_0$,\n$\\xi_2$, $F_K$, and $m_c$ (charm quark mass). We find a strong tension in\n$|\\varepsilon_K|$ at the $\\approx 5\\sigma$ ($5.2\\sigma \\sim 4.6\\sigma$) level\nbetween the experimental value and the theoretical value calculated directly\nfrom the standard model using lattice QCD inputs. The standard model with\nlattice QCD inputs describes only 65\\% of the experimental value of\n$|\\varepsilon_K|$, and does not explain its remaining 35\\%. We also find that\nthis tension disappears when we use inclusive $|V_{cb}|$ which comes from the\nheavy quark expansion and QCD sum rules. This tension is highly correlated with\nthe discrepancy between exclusive $|V_{cb}|$, and inclusive $|V_{cb}|$. We also\npresent results for $|\\varepsilon_K|$ obtained using the Brod-Gorbahn-Stamou\n(BGS) method of $u-t$ unitarity, which leads to even a stronger tension at the\n$5.5\\sigma \\sim 4.9\\sigma$ level with lattice QCD inputs.\n", "versions": [{"version": "v1", "created": "Sat, 1 Mar 2025 04:57:02 GMT"}, {"version": "v2", "created": "Fri, 7 Mar 2025 11:10:06 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 11:38:28 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Jwa", "Seungyeob", ""], ["Kim", "Jeehun", ""], ["Kim", "Sunghee", ""], ["Lee", "Sunkyu", ""], ["Lee", "Weonjong", ""], ["Park", "Sungwoo", ""]], "extracted_entities": [{"text": "standard model", "label": "AI model"}, {"text": "standard model", "label": "AI model"}]}
{"id": "2503.00758", "submitter": "Seyed Mohammad Sadegh Movahed", "authors": "Adeela Afzal, M. Alakhras, M. H. Jalali Kanafi and S. M. S. Movahed", "title": "Cosmic Strings-induced CMB anisotropies in light of Weighted Morphology", "comments": "20 pages, 10 figures and 3 tables, comments are welcome, V2: added\n  some references", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.CO astro-ph.IM physics.comp-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the morphological measures in assessing the geometrical and\ntopological properties of a generic cosmological stochastic field, we propose\nan extension of the weighted morphological measures, specifically the $n$th\nconditional moments of derivative (cmd-$n$). This criterion assigns a distinct\nweight to each excursion set point based on the associated field. We apply the\ncmd-$n$ on the Cosmic Microwave Background (CMB) to identify the cosmic string\nnetworks (CSs) through their unique Gott-Kaiser-Stebbins effect on the\ntemperature anisotropies. We also formulate the perturbative expansion of\ncmd-$n$ for the weak non-Gaussian regime up to $\\mathcal{O}(\\sigma_0^3)$. We\npropose a comprehensive pipeline designed to analyze the morphological\nproperties of string-induced CMB maps within the flat sky approximation. To\nevaluate the robustness of our proposed criteria, we employ string-induced\nhigh-resolution flat-sky CMB simulated patches of $7.2$ deg$^2$ size with a\nresolution of $0.42$ arc-minutes. Our results demonstrate that the minimum\ndetectable value of cosmic string tension is $G\\mu\\gtrsim 1.9\\times 10^{-7}$\nwhen a noise-free map is analyzed with normalized cmd-$n$. Whereas for the ACT,\nCMB-S4, and Planck-like experiments at 95.45\\% confidence level, the normalized\ncmd-$n$ can distinguish the CSs network for $G\\mu\\gtrsim2.9 \\times 10^{-7}$,\n$G\\mu\\gtrsim 2.4\\times 10^{-7}$ and $G\\mu\\gtrsim 5.8\\times 10^{-7}$,\nrespectively. The normalized cmd-$n$ exhibits a significantly enhanced\ncapability in detecting CSs relative to the Minkowski Functionals.\n", "versions": [{"version": "v1", "created": "Sun, 2 Mar 2025 06:41:26 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 10:25:13 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Afzal", "Adeela", ""], ["Alakhras", "M.", ""], ["Kanafi", "M. H. Jalali", ""], ["Movahed", "S. M. S.", ""]], "extracted_entities": [{"text": "cosmic string\nnetworks", "label": "LLMs"}, {"text": "CSs", "label": "LLMs"}, {"text": "CSs", "label": "LLMs"}]}
{"id": "2503.02191", "submitter": "Mia Mohammad Imran", "authors": "Mia Mohammad Imran, Robert Zita, Rebekah Copeland, Preetha Chatterjee,\n  Rahat Rizvi Rahman, and Kostadin Damevski", "title": "Understanding and Predicting Derailment in Toxic Conversations on GitHub", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Software projects thrive on the involvement and contributions of individuals\nfrom different backgrounds. However, toxic language and negative interactions\ncan hinder the participation and retention of contributors and alienate\nnewcomers. Proactive moderation strategies aim to prevent toxicity from\noccurring by addressing conversations that have derailed from their intended\npurpose. This study aims to understand and predict conversational derailment\nleading to toxicity on GitHub.\n  To facilitate this research, we curate a novel dataset comprising 202 toxic\nconversations from GitHub with annotated derailment points, along with 696\nnon-toxic conversations as a baseline. Based on this dataset, we identify\nunique characteristics of toxic conversations and derailment points, including\nlinguistic markers such as second-person pronouns, negation terms, and tones of\nBitter Frustration and Impatience, as well as patterns in conversational\ndynamics between project contributors and external participants.\n  Leveraging these empirical observations, we propose a proactive moderation\napproach to automatically detect and address potentially harmful conversations\nbefore escalation. By utilizing modern LLMs, we develop a conversation\ntrajectory summary technique that captures the evolution of discussions and\nidentifies early signs of derailment. Our experiments demonstrate that LLM\nprompts tailored to provide summaries of GitHub conversations achieve 70%\nF1-Score in predicting conversational derailment, strongly improving over a set\nof baseline approaches.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2025 02:01:37 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 03:25:44 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Imran", "Mia Mohammad", ""], ["Zita", "Robert", ""], ["Copeland", "Rebekah", ""], ["Chatterjee", "Preetha", ""], ["Rahman", "Rahat Rizvi", ""], ["Damevski", "Kostadin", ""]], "extracted_entities": [{"text": "GitHub", "label": "Open-source LLMs"}, {"text": "GitHub", "label": "Open-source LLMs"}, {"text": "modern LLMs", "label": "LLM"}, {"text": "GitHub", "label": "Open-source LLMs"}]}
{"id": "2503.02376", "submitter": "Guo Chen", "authors": "Guo Chen, Ling Lin, Chengfeng Zhang, Jie Zhang, Gilles Frapper, and\n  Xianlong Wang", "title": "Unsaturated Dinitrogen Difluoride under Pressure: toward high-Energy\n  Density Polymerized NF Chains", "comments": "High-energy-density material, First-principles calculation, Ab initio\n  molecular dynamics, NF compound", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on first-principles calculations and ab initio molecular dynamics\nsimulations, the polymerisation of the unsaturated cis dinitrogen-difluoride\n(cis-N2F2) molecular compound is investigated. The thermodynamic, dynamical and\nthermal stabilities of the nitrogen fluorine NF system are investigated at\nconditions of 0-3000 K and 0-200 GPa. The cis-N2F2 molecule is a suitable\nprecursor to obtain one-dimensional polymerized nitrogen-fluorine (poly-NF)\nchains at a pressure above 90 GPa and at a temperature around 1900 K.\nImportantly, these poly-NF chains can be quenched to room conditions, and\npotentially serve as a High-energy-density materials (HEDM). It has been\nestablished that when Al is utilised as a reducing agent, poly-NF chains\nexhibit a gravimetric energy density of 13.55 kJ/g, which exceeds that of cubic\ngauche nitrogen (cg-N, 9.70 kJ/g). This is attributable to the presence of both\npolymerised nitrogen and strong oxidising F atoms.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2025 08:05:17 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 05:40:22 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chen", "Guo", ""], ["Lin", "Ling", ""], ["Zhang", "Chengfeng", ""], ["Zhang", "Jie", ""], ["Frapper", "Gilles", ""], ["Wang", "Xianlong", ""]], "extracted_entities": [{"text": "Al", "label": "ALBERT"}]}
{"id": "2503.02597", "submitter": "Wei-Yao Wang", "authors": "Wei-Yao Wang, Zhao Wang, Helen Suzuki, Yoshiyuki Kobayashi", "title": "Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual\n  Attention for Multimodal LLMs", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in perceiving and reasoning over multimodal inquiries, ushering in a\nnew research era for foundation models. However, vision-language misalignment\nin MLLMs has emerged as a critical challenge, where the textual responses\ngenerated by these models are not factually aligned with the given text-image\ninputs. Existing efforts to address vision-language misalignment have focused\non developing specialized vision-language connectors or leveraging visual\ninstruction tuning from diverse domains. In this paper, we tackle this issue\nfrom a fundamental yet unexplored perspective by revisiting the core\narchitecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs\nconsisting of a causal attention mechanism, which limits the ability of the\nearlier modalities (e.g., images) to incorporate information from the latter\nmodalities (e.g., text). To address this problem, we propose \\MapleLeaf AKI, a\nnovel MLLM that unlocks causal attention into modality-mutual attention (MMA)\nto enable image tokens to attend to text tokens. This simple yet effective\ndesign allows AKI to achieve superior performance in 12 multimodal\nunderstanding benchmarks (+7.2% on average) without introducing additional\nparameters and increasing training time. Our MMA design is intended to be\ngeneric, allowing for application across various modalities, and scalable to\naccommodate diverse multimodal scenarios. The code and model are publicly\navailable at https://github.com/sony/aki to encourage further advancements in\nMLLMs across various directions.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2025 13:18:33 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 01:48:08 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Wei-Yao", ""], ["Wang", "Zhao", ""], ["Suzuki", "Helen", ""], ["Kobayashi", "Yoshiyuki", ""]], "extracted_entities": [{"text": "Multimodal Large Language Models", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "foundation models", "label": "Foundation Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "visual\ninstruction tuning", "label": "Fine-tuning"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "decoder-only LLMs", "label": "LLMs"}, {"text": "causal attention mechanism", "label": "Attention mechanism"}, {"text": "causal attention", "label": "Attention mechanism"}, {"text": "MLLMs", "label": "Large Language Model"}]}
{"id": "2503.02702", "submitter": "Chenyu Li", "authors": "Chenyu Li, Zhengjia Zhu, Jiyan He, Xiu Zhang", "title": "RedChronos: A Large Language Model-Based Log Analysis System for Insider\n  Threat Detection in Enterprises", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internal threat detection (IDT) aims to address security threats within\norganizations or enterprises by identifying potential or already occurring\nmalicious threats within vast amounts of logs. Although organizations or\nenterprises have dedicated personnel responsible for reviewing these logs, it\nis impossible to manually examine all logs entirely.In response to the vast\nnumber of logs, we propose a system called RedChronos, which is a Large\nLanguage Model-Based Log Analysis System. This system incorporates innovative\nimprovements over previous research by employing Query-Aware Weighted Voting\nand a Semantic Expansion-based Genetic Algorithm with LLM-driven Mutations. On\nthe public datasets CERT 4.2 and 5.2, RedChronos outperforms or matches\nexisting approaches in terms of accuracy, precision, and detection rate.\nMoreover, RedChronos reduces the need for manual intervention in security log\nreviews by approximately 90% in the Xiaohongshu Security Operation Center.\nTherefore, our RedChronos system demonstrates exceptional performance in\nhandling IDT tasks, providing innovative solutions for these challenges. We\nbelieve that future research can continue to enhance the system's performance\nin IDT tasks while also reducing the response time to internal risk events.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2025 15:18:40 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 11:47:44 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Chenyu", ""], ["Zhu", "Zhengjia", ""], ["He", "Jiyan", ""], ["Zhang", "Xiu", ""]], "extracted_entities": [{"text": "CERT 4.2 and 5.2", "label": "Open-source LLMs"}]}
{"id": "2503.02736", "submitter": "Fabrice Delbary", "authors": "Fabrice Delbary", "title": "Stable soap bubble clusters with multiple torus bubbles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two centuries and more particularly in the last decades, the\ngeometry of foams has become an important research domain, in mathematics,\nphysics, material sciences and biology. Most of the simplest geometrical\nobservations of bubble clusters have long resisted rigorous mathematical\nproofs. Geometries can even get more complicated if immiscible fluids are\nconsidered. Although they have to fulfill Plateau's laws like soap bubble\nclusters if the surface tensions are close to unity, this is not the case in\ngeneral. In 1996, Frederick J. Almgren asked whether there is \"any stable\ncluster of bubbles in $\\mathbb{R}^3$ with some bubble being topologically a\ntorus\". We propose to answer the latter numerically with simple numerical\nexamples. We build stable soap bubble clusters with a triple torus bubble, a\nfivefold torus bubble or an elevenfold torus bubble. The construction uses the\ngeometry of a simple immiscible fluids cluster with a torus bubble.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2025 15:56:18 GMT"}, {"version": "v2", "created": "Fri, 7 Mar 2025 14:32:18 GMT"}, {"version": "v3", "created": "Wed, 12 Mar 2025 16:48:00 GMT"}, {"version": "v4", "created": "Thu, 13 Mar 2025 12:18:48 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Delbary", "Fabrice", ""]], "extracted_entities": [{"text": "torus", "label": "Mistral"}, {"text": "torus", "label": "Mistral"}, {"text": "torus", "label": "Mistral"}, {"text": "torus", "label": "Mistral"}, {"text": "torus", "label": "Mistral"}]}
{"id": "2503.03265", "submitter": "Ping Chen", "authors": "Ping Chen, Xingpeng Zhang, Zhaoxiang Liu, Huan Hu, Xiang Liu, Kai\n  Wang, Min Wang, Yanlin Qian, Shiguo Lian", "title": "Optimizing for the Shortest Path in Denoising Diffusion Model", "comments": "Accepet by CVPR 2025 (10 pages, 6 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, we propose a novel denoising diffusion model based on\nshortest-path modeling that optimizes residual propagation to enhance both\ndenoising efficiency and quality. Drawing on Denoising Diffusion Implicit\nModels (DDIM) and insights from graph theory, our model, termed the Shortest\nPath Diffusion Model (ShortDF), treats the denoising process as a shortest-path\nproblem aimed at minimizing reconstruction error. By optimizing the initial\nresiduals, we improve the efficiency of the reverse diffusion process and the\nquality of the generated samples. Extensive experiments on multiple standard\nbenchmarks demonstrate that ShortDF significantly reduces diffusion time (or\nsteps) while enhancing the visual fidelity of generated samples compared to\nprior arts. This work, we suppose, paves the way for interactive\ndiffusion-based applications and establishes a foundation for rapid data\ngeneration. Code is available at https://github.com/UnicomAI/ShortDF.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2025 08:47:36 GMT"}, {"version": "v2", "created": "Thu, 6 Mar 2025 01:46:21 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 07:16:50 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chen", "Ping", ""], ["Zhang", "Xingpeng", ""], ["Liu", "Zhaoxiang", ""], ["Hu", "Huan", ""], ["Liu", "Xiang", ""], ["Wang", "Kai", ""], ["Wang", "Min", ""], ["Qian", "Yanlin", ""], ["Lian", "Shiguo", ""]], "extracted_entities": [{"text": "Shortest\nPath Diffusion Model", "label": "Foundation Model"}, {"text": "ShortDF", "label": "Foundation Model"}]}
{"id": "2503.03496", "submitter": "Konstantinos Kouroumpatzakis", "authors": "K. Kouroumpatzakis and J. Svoboda", "title": "Gas excitation in galaxies and active galactic nuclei with He\n  II{\\lambda}4686 and X-ray emission", "comments": "14 + 2 appendix pages, 7 figures, 1 table. Accepted for publication\n  in A&A on March 3, 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.GA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The origin of He II emission in galaxies remains a debated topic, requiring\nionizing photons with energies exceeding 54 eV. While massive stars, such as\nWolf-Rayet stars, have been considered potential sources, their UV flux often\nfails to fully explain the observed He II emission. Recent studies suggest that\nX-ray binaries (XRBs) might contribute significantly to this ionization. We\nexplore the relationship between X-ray and $\\rm He~II \\lambda4686$ emission in\na statistically significant sample of galaxies, investigating whether X-ray\nsources, including active galactic nuclei (AGNs) and XRBs, serve as the primary\nmechanism for He II ionization across different galactic environments. We\ncross-matched a sample of known well-detected He II galaxies with the Chandra\nSource Catalog, yielding 165 galaxies with X-ray and $\\rm He~II \\lambda4686$\ndetections. The sources were classified into star-forming galaxies (SFGs) and\nAGNs based on the BPT diagram and a classification scheme defined for He II\ngalaxies. We find a strong, linear correlation between X-ray and He II\nluminosity across AGNs and SFGs spanning over seven orders of magnitude. AGNs\ngenerally exhibit higher He II/H$\\beta$ flux ratios, stronger extinction, and\nharder X-ray spectra. The O32 ratio of SFGs is tightly correlated with the\nH$\\beta$ equivalent width ($\\rm EW_{H\\beta}$) but not with the He II/H$\\beta$\nratio, suggesting a different excitation mechanism. We derive an O32--$\\rm\nEW_{H\\beta}$ line above which only AGNs of our sample reside. The tight\ncorrelation between X-ray and He II luminosity supports X-rays as the primary\ndriver of He II excitation. While AGNs have one common ionization source, the\ncentral black hole, in SFGs low-energy species are mainly excited by UV\nemission related to star-forming activity, however, high-energy species like He\nII require the presence of XRBs.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2025 13:37:23 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 16:13:05 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Kouroumpatzakis", "K.", ""], ["Svoboda", "J.", ""]], "extracted_entities": [{"text": "XRBs", "label": "LLMs"}, {"text": "AGNs", "label": "LLMs"}, {"text": "SFGs", "label": "LLMs"}, {"text": "XRBs", "label": "LLMs"}]}
{"id": "2503.03971", "submitter": "Fanwen Wang Ms", "authors": "Fanwen Wang, Zi Wang, Yan Li, Jun Lyu, Chen Qin, Shuo Wang, Kunyuan\n  Guo, Mengting Sun, Mingkai Huang, Haoyu Zhang, Michael T\\\"anzer, Qirong Li,\n  Xinran Chen, Jiahao Huang, Yinzhe Wu, Kian Anvari Hamedani, Yuntong Lyu,\n  Longyu Sun, Qing Li, Ziqiang Xu, Bingyu Xin, Dimitris N. Metaxas, Narges\n  Razizadeh, Shahabedin Nabavi, George Yiasemis, Jonas Teuwen, Zhenxi Zhang,\n  Sha Wang, Chi Zhang, Daniel B. Ennis, Zhihao Xue, Chenxi Hu, Ruru Xu, Ilkay\n  Oksuz, Donghang Lyu, Yanxin Huang, Xinrui Guo, Ruqian Hao, Jaykumar H. Patel,\n  Guanke Cai, Binghua Chen, Yajing Zhang, Sha Hua, Zhensen Chen, Qi Dou, Xiahai\n  Zhuang, Qian Tao, Wenjia Bai, Jing Qin, He Wang, Claudia Prieto, Michael\n  Markl, Alistair Young, Hao Li, Xihong Hu, Lianmin Wu, Xiaobo Qu, Guang Yang,\n  Chengyan Wang", "title": "Towards Universal Learning-based Model for Cardiac Image Reconstruction:\n  Summary of the CMRxRecon2024 Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular magnetic resonance (CMR) imaging offers diverse contrasts for\nnon-invasive assessment of cardiac function and myocardial characterization.\nHowever, CMR often requires the acquisition of many contrasts, and each\ncontrast takes a considerable amount of time. The extended acquisition time\nwill further increase the susceptibility to motion artifacts. Existing deep\nlearning-based reconstruction methods have been proven to perform well in image\nreconstruction tasks, but most of them are designed for specific acquisition\nmodality or dedicated imaging parameter, which limits their ability to\ngeneralize across a variety of scan scenarios. To address this issue, the\nCMRxRecon2024 challenge consists of two specific tasks: Task 1 focuses on a\nmodality-universal setting, evaluating the out-of-distribution generalization\nof existing learning-based models, while Task 2 follows a k-space\nsampling-universal setting, assessing the all-in-one adaptability of universal\nmodels. Main contributions of this challenge include providing the largest\npublicly available multi-modality, multi-view cardiac k-space dataset; and\ndeveloping an open benchmarking platform for algorithm evaluation and shared\ncode library for data processing. In addition, through a detailed analysis of\nthe results submitted to the challenge, we have also made several findings,\nincluding: 1) adaptive prompt-learning embedding is an effective means for\nachieving strong generalization in reconstruction models; 2) enhanced data\nconsistency based on physics-informed networks is also an effective pathway\ntoward a universal model; 3) traditional evaluation metrics have limitations\nwhen assessing ground-truth references with moderate or lower image quality,\nhighlighting the need for subjective evaluation methods. This challenge\nattracted 200 participants from 18 countries, aimed at promoting their\ntranslation into clinical practice.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2025 23:45:00 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 17:01:37 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Fanwen", ""], ["Wang", "Zi", ""], ["Li", "Yan", ""], ["Lyu", "Jun", ""], ["Qin", "Chen", ""], ["Wang", "Shuo", ""], ["Guo", "Kunyuan", ""], ["Sun", "Mengting", ""], ["Huang", "Mingkai", ""], ["Zhang", "Haoyu", ""], ["T\u00e4nzer", "Michael", ""], ["Li", "Qirong", ""], ["Chen", "Xinran", ""], ["Huang", "Jiahao", ""], ["Wu", "Yinzhe", ""], ["Hamedani", "Kian Anvari", ""], ["Lyu", "Yuntong", ""], ["Sun", "Longyu", ""], ["Li", "Qing", ""], ["Xu", "Ziqiang", ""], ["Xin", "Bingyu", ""], ["Metaxas", "Dimitris N.", ""], ["Razizadeh", "Narges", ""], ["Nabavi", "Shahabedin", ""], ["Yiasemis", "George", ""], ["Teuwen", "Jonas", ""], ["Zhang", "Zhenxi", ""], ["Wang", "Sha", ""], ["Zhang", "Chi", ""], ["Ennis", "Daniel B.", ""], ["Xue", "Zhihao", ""], ["Hu", "Chenxi", ""], ["Xu", "Ruru", ""], ["Oksuz", "Ilkay", ""], ["Lyu", "Donghang", ""], ["Huang", "Yanxin", ""], ["Guo", "Xinrui", ""], ["Hao", "Ruqian", ""], ["Patel", "Jaykumar H.", ""], ["Cai", "Guanke", ""], ["Chen", "Binghua", ""], ["Zhang", "Yajing", ""], ["Hua", "Sha", ""], ["Chen", "Zhensen", ""], ["Dou", "Qi", ""], ["Zhuang", "Xiahai", ""], ["Tao", "Qian", ""], ["Bai", "Wenjia", ""], ["Qin", "Jing", ""], ["Wang", "He", ""], ["Prieto", "Claudia", ""], ["Markl", "Michael", ""], ["Young", "Alistair", ""], ["Li", "Hao", ""], ["Hu", "Xihong", ""], ["Wu", "Lianmin", ""], ["Qu", "Xiaobo", ""], ["Yang", "Guang", ""], ["Wang", "Chengyan", ""]], "extracted_entities": [{"text": "adaptive prompt-learning embedding", "label": "Embedding"}]}
{"id": "2503.04779", "submitter": "Thanh Le-Cong Le-Cong Thanh", "authors": "Thanh Le-Cong, Bach Le, Toby Murray", "title": "Can LLMs Reason About Program Semantics? A Comprehensive Evaluation of\n  LLMs on Formal Specification Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large Language Models (LLMs) are increasingly being used to automate\nprogramming tasks. Yet, LLMs' capabilities in reasoning about program semantics\nare still inadequately studied, leaving significant potential for further\nexploration. This paper introduces FormalBench, a comprehensive benchmark\ndesigned to evaluate LLMs' reasoning abilities on program semantics,\nparticularly via the task of synthesizing formal program specifications to\nassist verifying program correctness. This task requires both comprehensive\nreasoning over all possible program executions and the generation of precise,\nsyntactically correct expressions that adhere to formal syntax and semantics.\nUsing this benchmark, we evaluated the ability of LLMs in synthesizing\nconsistent and complete specifications. Our findings show that LLMs perform\nwell with simple control flows but struggle with more complex structures,\nespecially loops, even with advanced prompting. Additionally, LLMs exhibit\nlimited robustness against semantic-preserving transformations. We also\nhighlight common failure patterns and design self-repair prompts, improving\nsuccess rates by 25%.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2025 13:27:31 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 07:41:37 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Le-Cong", "Thanh", ""], ["Le", "Bach", ""], ["Murray", "Toby", ""]], "extracted_entities": [{"text": "Large Language Models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "advanced prompting", "label": "Prompting"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "design self-repair prompts", "label": "Prompting"}]}
{"id": "2503.04823", "submitter": "Yuheng Kuang", "authors": "Yuheng Kuang, Zhengning Wang, Jianping Zhang, Zhenyu Shi, Yuding Zhang", "title": "DA-STGCN: 4D Trajectory Prediction Based on Spatiotemporal Feature\n  Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of four-dimensional (4D) trajectory prediction within air\ntraffic management systems is on the rise. Key operations such as conflict\ndetection and resolution, aircraft anomaly monitoring, and the management of\ncongested flight paths are increasingly reliant on this foundational\ntechnology, underscoring the urgent demand for intelligent solutions. The\ndynamics in airport terminal zones and crowded airspaces are intricate and\never-changing; however, current methodologies do not sufficiently account for\nthe interactions among aircraft. To tackle these challenges, we propose\nDA-STGCN, an innovative spatiotemporal graph convolutional network that\nintegrates a dual attention mechanism. Our model reconstructs the adjacency\nmatrix through a self-attention approach, enhancing the capture of node\ncorrelations, and employs graph attention to distill spatiotemporal\ncharacteristics, thereby generating a probabilistic distribution of predicted\ntrajectories. This novel adjacency matrix, reconstructed with the\nself-attention mechanism, is dynamically optimized throughout the network's\ntraining process, offering a more nuanced reflection of the inter-node\nrelationships compared to traditional algorithms. The performance of the model\nis validated on two ADS-B datasets, one near the airport terminal area and the\nother in dense airspace. Experimental results demonstrate a notable improvement\nover current 4D trajectory prediction methods, achieving a 20% and 30%\nreduction in the Average Displacement Error (ADE) and Final Displacement Error\n(FDE), respectively. The incorporation of a Dual-Attention module has been\nshown to significantly enhance the extraction of node correlations, as verified\nby ablation experiments.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2025 03:42:49 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 03:39:44 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Kuang", "Yuheng", ""], ["Wang", "Zhengning", ""], ["Zhang", "Jianping", ""], ["Shi", "Zhenyu", ""], ["Zhang", "Yuding", ""]], "extracted_entities": [{"text": "dual attention mechanism", "label": "Attention mechanism"}, {"text": "self-attention mechanism", "label": "Attention mechanism"}]}
{"id": "2503.05136", "submitter": "Ronny Ko", "authors": "Ronny Ko", "title": "The Mathematical Construction of the BFV Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fully Homomorphic Encryption (FHE) is a cryptographic scheme that enables\ncomputations to be performed directly on encrypted data, as if the data were in\nplaintext. BFV is one of the most popular FHE schemes. The BFV scheme is\ndesigned for homomorphic addition and multiplication of integers. BFV's\nencoding scheme does not require such approximation issues, because BFV is\ndesigned to encode only integers. Therefore, BFV guarantees exact encryption\nand decryption. BFV is suitable for use cases where the encrypted and decrypted\nvalues should exactly match (e.g., voting, financial computation), whereas CKKS\nis suitable for the use cases that tolerate tiny errors (e.g., data analytics,\nmachine learning).\n  In BFV, each plaintext is encrypted as an RLWE ciphertext. Therefore, BFV's\nciphertext-to-ciphertext addition, ciphertext-to-plaintext addition, and\nciphertext-to-plaintext multiplication are implemented based on GLWE's\nhomomorphic addition and multiplication, with $k = 1$ to make GLWE an RLWE.\n  This tutorial article is designed to help the reader understand how BFV works\nfrom the mathematical level.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2025 04:29:11 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 15:18:50 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Ko", "Ronny", ""]], "extracted_entities": [{"text": "BFV", "label": "BERT"}, {"text": "BFV", "label": "BERT"}, {"text": "BFV", "label": "BERT"}, {"text": "BFV", "label": "BERT"}, {"text": "BFV", "label": "BERT"}, {"text": "BFV", "label": "BERT"}, {"text": "BFV", "label": "BERT"}, {"text": "BFV", "label": "BERT"}]}
{"id": "2503.05336", "submitter": "Laura Weidinger", "authors": "Laura Weidinger, Inioluwa Deborah Raji, Hanna Wallach, Margaret\n  Mitchell, Angelina Wang, Olawale Salaudeen, Rishi Bommasani, Deep Ganguli,\n  Sanmi Koyejo, William Isaac", "title": "Toward an Evaluation Science for Generative AI Systems", "comments": "First two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is an increasing imperative to anticipate and understand the\nperformance and safety of generative AI systems in real-world deployment\ncontexts. However, the current evaluation ecosystem is insufficient: Commonly\nused static benchmarks face validity challenges, and ad hoc case-by-case audits\nrarely scale. In this piece, we advocate for maturing an evaluation science for\ngenerative AI systems. While generative AI creates unique challenges for system\nsafety engineering and measurement science, the field can draw valuable\ninsights from the development of safety evaluation practices in other fields,\nincluding transportation, aerospace, and pharmaceutical engineering. In\nparticular, we present three key lessons: Evaluation metrics must be applicable\nto real-world performance, metrics must be iteratively refined, and evaluation\ninstitutions and norms must be established. Applying these insights, we outline\na concrete path toward a more rigorous approach for evaluating generative AI\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2025 11:23:48 GMT"}, {"version": "v2", "created": "Tue, 11 Mar 2025 12:31:22 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 00:09:08 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Weidinger", "Laura", ""], ["Raji", "Inioluwa Deborah", ""], ["Wallach", "Hanna", ""], ["Mitchell", "Margaret", ""], ["Wang", "Angelina", ""], ["Salaudeen", "Olawale", ""], ["Bommasani", "Rishi", ""], ["Ganguli", "Deep", ""], ["Koyejo", "Sanmi", ""], ["Isaac", "William", ""]], "extracted_entities": [{"text": "safety evaluation practices", "label": "AI Ethics"}, {"text": "evaluation\ninstitutions and norms", "label": "AI Ethics"}]}
{"id": "2503.05403", "submitter": "Verena H\\\"aberle", "authors": "Verena H\\\"aberle, Xiuqiang He, Linbin Huang, Florian D\\\"orfler, Steven\n  Low", "title": "Quantitative Decentralized Stability Certificates for Grid-Forming\n  Converter Control", "comments": "12 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a decentralized framework for guaranteeing the small-signal\nstability of future power systems with grid-forming converters. Our approach\nleverages dynamic loop-shifting techniques to compensate for the lack of\npassivity in the network dynamics and establishes decentralized parametric\nstability certificates, depending on the local device-level controls and\nincorporating the effects of the network dynamics. By following practical\ntuning rules, we are able to ensure plug-and-play operation without centralized\ncoordination. Unlike prior works, our approach accommodates coupled frequency\nand voltage dynamics, incorporates network dynamics, and does not rely on\nspecific network configurations or operating points, offering a general and\nscalable solution for the integration of power-electronics-based devices into\nfuture power systems. We validate our theoretical stability results through\nnumerical case studies in a high-fidelity simulation model.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2025 13:26:55 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 13:51:36 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["H\u00e4berle", "Verena", ""], ["He", "Xiuqiang", ""], ["Huang", "Linbin", ""], ["D\u00f6rfler", "Florian", ""], ["Low", "Steven", ""]], "extracted_entities": [{"text": "practical\ntuning rules", "label": "Fine-tuning"}]}
{"id": "2503.05473", "submitter": "Noah Mami\\'e", "authors": "Noah Mamie, Susie Xi Rao", "title": "The Society of HiveMind: Multi-Agent Optimization of Foundation Model\n  Swarms to Unlock the Potential of Collective Intelligence", "comments": "11 pages (excl. appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-agent systems address issues of accessibility and scalability of\nartificial intelligence (AI) foundation models, which are often represented by\nlarge language models. We develop a framework - the \"Society of HiveMind\"\n(SOHM) - that orchestrates the interaction between multiple AI foundation\nmodels, imitating the observed behavior of animal swarms in nature by following\nmodern evolutionary theories. On the one hand, we find that the SOHM provides a\nnegligible benefit on tasks that mainly require real-world knowledge. On the\nother hand, we remark a significant improvement on tasks that require intensive\nlogical reasoning, indicating that multi-agent systems are capable of\nincreasing the reasoning capabilities of the collective compared to the\nindividual agents. Our findings demonstrate the potential of combining a\nmultitude of diverse AI foundation models to form an artificial swarm\nintelligence capable of self-improvement through interactions with a given\nenvironment.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2025 14:45:03 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 14:20:53 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Mamie", "Noah", ""], ["Rao", "Susie Xi", ""]], "extracted_entities": [{"text": "Society of HiveMind", "label": "Foundation Model"}, {"text": "SOHM", "label": "Foundation Model"}]}
{"id": "2503.05491", "submitter": "Lo\\\"ic Fosse", "authors": "Lo\\\"ic Fosse and Fr\\'ed\\'eric B\\'echet and Beno\\^it Favre and\n  G\\'eraldine Damnati and Gw\\'enol\\'e Lecorv\\'e and Maxime Darrin and Philippe\n  Formont and Pablo Piantanida", "title": "Statistical Deficiency for Task Inclusion Estimation", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Tasks are central in machine learning, as they are the most natural objects\nto assess the capabilities of current models. The trend is to build general\nmodels able to address any task. Even though transfer learning and multitask\nlearning try to leverage the underlying task space, no well-founded tools are\navailable to study its structure. This study proposes a theoretically grounded\nsetup to define the notion of task and to compute the {\\bf inclusion} between\ntwo tasks from a statistical deficiency point of view. We propose a tractable\nproxy as information sufficiency to estimate the degree of inclusion between\ntasks, show its soundness on synthetic data, and use it to reconstruct\nempirically the classic NLP pipeline.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2025 15:00:28 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 08:41:29 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Fosse", "Lo\u00efc", ""], ["B\u00e9chet", "Fr\u00e9d\u00e9ric", ""], ["Favre", "Beno\u00eet", ""], ["Damnati", "G\u00e9raldine", ""], ["Lecorv\u00e9", "Gw\u00e9nol\u00e9", ""], ["Darrin", "Maxime", ""], ["Formont", "Philippe", ""], ["Piantanida", "Pablo", ""]], "extracted_entities": [{"text": "transfer learning", "label": "Few-shot Learning"}, {"text": "multitask\nlearning", "label": "Few-shot Learning"}]}
{"id": "2503.05710", "submitter": "Justin Bullock", "authors": "Justin B. Bullock, Samuel Hammond, Seb Krier", "title": "AGI, Governments, and Free Societies", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper examines how artificial general intelligence (AGI) could\nfundamentally reshape the delicate balance between state capacity and\nindividual liberty that sustains free societies. Building on Acemoglu and\nRobinson's 'narrow corridor' framework, we argue that AGI poses distinct risks\nof pushing societies toward either a 'despotic Leviathan' through enhanced\nstate surveillance and control, or an 'absent Leviathan' through the erosion of\nstate legitimacy relative to AGI-empowered non-state actors. Drawing on public\nadministration theory and recent advances in AI capabilities, we analyze how\nthese dynamics could unfold through three key channels: the automation of\ndiscretionary decision-making within agencies, the evolution of bureaucratic\nstructures toward system-level architectures, and the transformation of\ndemocratic feedback mechanisms. Our analysis reveals specific failure modes\nthat could destabilize liberal institutions. Enhanced state capacity through\nAGI could enable unprecedented surveillance and control, potentially\nentrenching authoritarian practices. Conversely, rapid diffusion of AGI\ncapabilities to non-state actors could undermine state legitimacy and\ngovernability. We examine how these risks manifest differently at the micro\nlevel of individual bureaucratic decisions, the meso level of organizational\nstructure, and the macro level of democratic processes. To preserve the narrow\ncorridor of liberty, we propose a governance framework emphasizing robust\ntechnical safeguards, hybrid institutional designs that maintain meaningful\nhuman oversight, and adaptive regulatory mechanisms.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2025 03:55:38 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 16:15:44 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Bullock", "Justin B.", ""], ["Hammond", "Samuel", ""], ["Krier", "Seb", ""]], "extracted_entities": [{"text": "robust\ntechnical safeguards", "label": "AI Ethics"}, {"text": "hybrid institutional designs", "label": "AI Ethics"}, {"text": "adaptive regulatory mechanisms", "label": "AI Ethics"}]}
{"id": "2503.05891", "submitter": "Jonas Golde", "authors": "Jonas Golde, Patrick Haller, Fabio Barth, Alan Akbik", "title": "MastermindEval: A Simple But Scalable Reasoning Benchmark", "comments": "9 pages, 2 figures, 4 tables. In: ICLR 2025 Workshop on Reasoning and\n  Planning for Large Language Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advancements in large language models (LLMs) have led to remarkable\nperformance across a wide range of language understanding and mathematical\ntasks. As a result, increasing attention has been given to assessing the true\nreasoning capabilities of LLMs, driving research into commonsense, numerical,\nlogical, and qualitative reasoning. However, with the rapid progress of\nreasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been\na growing demand for reasoning benchmarks that can keep pace with ongoing model\ndevelopments. In this paper, we introduce MastermindEval, a simple, scalable,\nand interpretable deductive reasoning benchmark inspired by the board game\nMastermind. Our benchmark supports two evaluation paradigms: (1) agentic\nevaluation, in which the model autonomously plays the game, and (2) deductive\nreasoning evaluation, in which the model is given a pre-played game state with\nonly one possible valid code to infer. In our experimental results we (1) find\nthat even easy Mastermind instances are difficult for current models and (2)\ndemonstrate that the benchmark is scalable to possibly more advanced models in\nthe future Furthermore, we investigate possible reasons why models cannot\ndeduce the final solution and find that current models are limited in deducing\nthe concealed code as the number of statement to combine information from is\nincreasing.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2025 19:24:59 GMT"}, {"version": "v2", "created": "Tue, 11 Mar 2025 17:33:51 GMT"}, {"version": "v3", "created": "Wed, 12 Mar 2025 15:02:43 GMT"}, {"version": "v4", "created": "Thu, 13 Mar 2025 14:59:54 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Golde", "Jonas", ""], ["Haller", "Patrick", ""], ["Barth", "Fabio", ""], ["Akbik", "Alan", ""]], "extracted_entities": [{"text": "LLMs", "label": "Large Language Model"}, {"text": "OpenAI", "label": "Open-source LLMs"}, {"text": "DeepSeek", "label": "Open-source LLMs"}]}
{"id": "2503.06369", "submitter": "Sahar Dastani", "authors": "Sahar Dastani, Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, David\n  Osowiechi, Gustavo Adolfo Vargas Hakim, Farzad Beizaee, Milad\n  Cheraghalikhani, Arnab Kumar Mondal, Herve Lombaert, Christian Desrosiers", "title": "Spectral State Space Model for Rotation-Invariant Visual Representation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  State Space Models (SSMs) have recently emerged as an alternative to Vision\nTransformers (ViTs) due to their unique ability of modeling global\nrelationships with linear complexity. SSMs are specifically designed to capture\nspatially proximate relationships of image patches. However, they fail to\nidentify relationships between conceptually related yet not adjacent patches.\nThis limitation arises from the non-causal nature of image data, which lacks\ninherent directional relationships. Additionally, current vision-based SSMs are\nhighly sensitive to transformations such as rotation. Their predefined scanning\ndirections depend on the original image orientation, which can cause the model\nto produce inconsistent patch-processing sequences after rotation. To address\nthese limitations, we introduce Spectral VMamba, a novel approach that\neffectively captures the global structure within an image by leveraging\nspectral information derived from the graph Laplacian of image patches. Through\nspectral decomposition, our approach encodes patch relationships independently\nof image orientation, achieving rotation invariance with the aid of our\nRotational Feature Normalizer (RFN) module. Our experiments on classification\ntasks show that Spectral VMamba outperforms the leading SSM models in vision,\nsuch as VMamba, while maintaining invariance to rotations and a providing a\nsimilar runtime efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 9 Mar 2025 00:37:43 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 02:10:35 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Dastani", "Sahar", ""], ["Bahri", "Ali", ""], ["Yazdanpanah", "Moslem", ""], ["Noori", "Mehrdad", ""], ["Osowiechi", "David", ""], ["Hakim", "Gustavo Adolfo Vargas", ""], ["Beizaee", "Farzad", ""], ["Cheraghalikhani", "Milad", ""], ["Mondal", "Arnab Kumar", ""], ["Lombaert", "Herve", ""], ["Desrosiers", "Christian", ""]], "extracted_entities": [{"text": "Vision\nTransformers", "label": "Transformers"}]}
{"id": "2503.06571", "submitter": "Xuan-May Le", "authors": "Xuan-May Le, Ling Luo, Uwe Aickelin, Minh-Tuan Tran, David Berlowitz,\n  Mark Howard", "title": "SHIP: A Shapelet-based Approach for Interpretable Patient-Ventilator\n  Asynchrony Detection", "comments": "Accepted at PAKDD 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Patient-ventilator asynchrony (PVA) is a common and critical issue during\nmechanical ventilation, affecting up to 85% of patients. PVA can result in\nclinical complications such as discomfort, sleep disruption, and potentially\nmore severe conditions like ventilator-induced lung injury and diaphragm\ndysfunction. Traditional PVA management, which relies on manual adjustments by\nhealthcare providers, is often inadequate due to delays and errors. While\nvarious computational methods, including rule-based, statistical, and deep\nlearning approaches, have been developed to detect PVA events, they face\nchallenges related to dataset imbalances and lack of interpretability. In this\nwork, we propose a shapelet-based approach SHIP for PVA detection, utilizing\nshapelets - discriminative subsequences in time-series data - to enhance\ndetection accuracy and interpretability. Our method addresses dataset\nimbalances through shapelet-based data augmentation and constructs a shapelet\npool to transform the dataset for more effective classification. The combined\nshapelet and statistical features are then used in a classifier to identify PVA\nevents. Experimental results on medical datasets show that SHIP significantly\nimproves PVA detection while providing interpretable insights into model\ndecisions.\n", "versions": [{"version": "v1", "created": "Sun, 9 Mar 2025 11:58:03 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 02:01:30 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Le", "Xuan-May", ""], ["Luo", "Ling", ""], ["Aickelin", "Uwe", ""], ["Tran", "Minh-Tuan", ""], ["Berlowitz", "David", ""], ["Howard", "Mark", ""]], "extracted_entities": [{"text": "SHIP", "label": "LLM-based"}, {"text": "SHIP", "label": "LLM-based"}]}
{"id": "2503.06669", "submitter": "Qingwen Bu", "authors": "AgiBot-World-Contributors, Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui,\n  Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xu Huang, Shu Jiang, Yuxin\n  Jiang, Cheng Jing, Hongyang Li, Jialu Li, Chiming Liu, Yi Liu, Yuxiang Lu,\n  Jianlan Luo, Ping Luo, Yao Mu, Yuehan Niu, Yixuan Pan, Jiangmiao Pang, Yu\n  Qiao, Guanghui Ren, Cheng Ruan, Jiaqi Shan, Yongjian Shen, Chengshi Shi,\n  Mingkang Shi, Modi Shi, Chonghao Sima, Jianheng Song, Huijie Wang, Wenhao\n  Wang, Dafeng Wei, Chengen Xie, Guo Xu, Junchi Yan, Cunbiao Yang, Lei Yang,\n  Shukai Yang, Maoqing Yao, Jia Zeng, Chi Zhang, Qinglin Zhang, Bin Zhao,\n  Chengyue Zhao, Jiaqi Zhao, Jianchao Zhu", "title": "AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable\n  and Intelligent Embodied Systems", "comments": "Project website: https://agibot-world.com/. Github repo:\n  https://github.com/OpenDriveLab/AgiBot-World. The author list is ordered\n  alphabetically by surname, with detailed contributions provided in the\n  appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We explore how scalable robot data can address real-world challenges for\ngeneralized robotic manipulation. Introducing AgiBot World, a large-scale\nplatform comprising over 1 million trajectories across 217 tasks in five\ndeployment scenarios, we achieve an order-of-magnitude increase in data scale\ncompared to existing datasets. Accelerated by a standardized collection\npipeline with human-in-the-loop verification, AgiBot World guarantees\nhigh-quality and diverse data distribution. It is extensible from grippers to\ndexterous hands and visuo-tactile sensors for fine-grained skill acquisition.\nBuilding on top of data, we introduce Genie Operator-1 (GO-1), a novel\ngeneralist policy that leverages latent action representations to maximize data\nutilization, demonstrating predictable performance scaling with increased data\nvolume. Policies pre-trained on our dataset achieve an average performance\nimprovement of 30% over those trained on Open X-Embodiment, both in in-domain\nand out-of-distribution scenarios. GO-1 exhibits exceptional capability in\nreal-world dexterous and long-horizon tasks, achieving over 60% success rate on\ncomplex tasks and outperforming prior RDT approach by 32%. By open-sourcing the\ndataset, tools, and models, we aim to democratize access to large-scale,\nhigh-quality robot data, advancing the pursuit of scalable and general-purpose\nintelligence.\n", "versions": [{"version": "v1", "created": "Sun, 9 Mar 2025 15:40:29 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 06:59:16 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["AgiBot-World-Contributors", "", ""], ["Bu", "Qingwen", ""], ["Cai", "Jisong", ""], ["Chen", "Li", ""], ["Cui", "Xiuqi", ""], ["Ding", "Yan", ""], ["Feng", "Siyuan", ""], ["Gao", "Shenyuan", ""], ["He", "Xindong", ""], ["Huang", "Xu", ""], ["Jiang", "Shu", ""], ["Jiang", "Yuxin", ""], ["Jing", "Cheng", ""], ["Li", "Hongyang", ""], ["Li", "Jialu", ""], ["Liu", "Chiming", ""], ["Liu", "Yi", ""], ["Lu", "Yuxiang", ""], ["Luo", "Jianlan", ""], ["Luo", "Ping", ""], ["Mu", "Yao", ""], ["Niu", "Yuehan", ""], ["Pan", "Yixuan", ""], ["Pang", "Jiangmiao", ""], ["Qiao", "Yu", ""], ["Ren", "Guanghui", ""], ["Ruan", "Cheng", ""], ["Shan", "Jiaqi", ""], ["Shen", "Yongjian", ""], ["Shi", "Chengshi", ""], ["Shi", "Mingkang", ""], ["Shi", "Modi", ""], ["Sima", "Chonghao", ""], ["Song", "Jianheng", ""], ["Wang", "Huijie", ""], ["Wang", "Wenhao", ""], ["Wei", "Dafeng", ""], ["Xie", "Chengen", ""], ["Xu", "Guo", ""], ["Yan", "Junchi", ""], ["Yang", "Cunbiao", ""], ["Yang", "Lei", ""], ["Yang", "Shukai", ""], ["Yao", "Maoqing", ""], ["Zeng", "Jia", ""], ["Zhang", "Chi", ""], ["Zhang", "Qinglin", ""], ["Zhao", "Bin", ""], ["Zhao", "Chengyue", ""], ["Zhao", "Jiaqi", ""], ["Zhu", "Jianchao", ""]], "extracted_entities": [{"text": "AgiBot World", "label": "Large Language Model"}]}
{"id": "2503.06692", "submitter": "Yuchen Yan", "authors": "Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Mengdi Zhang, Jian\n  Shao, Yueting Zhuang", "title": "InftyThink: Breaking the Length Limits of Long-Context Reasoning in\n  Large Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advanced reasoning in large language models has achieved remarkable\nperformance on challenging tasks, but the prevailing long-context reasoning\nparadigm faces critical limitations: quadratic computational scaling with\nsequence length, reasoning constrained by maximum context boundaries, and\nperformance degradation beyond pre-training context windows. Existing\napproaches primarily compress reasoning chains without addressing the\nfundamental scaling problem. To overcome these challenges, we introduce\nInftyThink, a paradigm that transforms monolithic reasoning into an iterative\nprocess with intermediate summarization. By interleaving short reasoning\nsegments with concise progress summaries, our approach enables unbounded\nreasoning depth while maintaining bounded computational costs. This creates a\ncharacteristic sawtooth memory pattern that significantly reduces computational\ncomplexity compared to traditional approaches. Furthermore, we develop a\nmethodology for reconstructing long-context reasoning datasets into our\niterative format, transforming OpenR1-Math into 333K training instances.\nExperiments across multiple model architectures demonstrate that our approach\nreduces computational costs while improving performance, with Qwen2.5-Math-7B\nshowing 3-13% improvements across MATH500, AIME24, and GPQA_diamond benchmarks.\nOur work challenges the assumed trade-off between reasoning depth and\ncomputational efficiency, providing a more scalable approach to complex\nreasoning without architectural modifications.\n", "versions": [{"version": "v1", "created": "Sun, 9 Mar 2025 16:59:14 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 16:00:47 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Yan", "Yuchen", ""], ["Shen", "Yongliang", ""], ["Liu", "Yang", ""], ["Jiang", "Jin", ""], ["Zhang", "Mengdi", ""], ["Shao", "Jian", ""], ["Zhuang", "Yueting", ""]], "extracted_entities": [{"text": "quadratic computational scaling", "label": "Scaling law"}, {"text": "reasoning chains", "label": "Chain of thought"}, {"text": "intermediate summarization", "label": "Knowledge distillation"}]}
{"id": "2503.06801", "submitter": "Takao Morinari", "authors": "Takao Morinari and Hibiki Takegami", "title": "Angular Dependence of Specific Heat and Magnetization Effects in the\n  Kitaev Model", "comments": "We have identified an error in the calculations presented in our\n  manuscript, which affects the discussion on periodicity in the section\n  concerning full magnetization effects. To ensure the accuracy and integrity\n  of our research, we have decided to withdraw the paper. We will carefully\n  address the issue and plan to submit a revised version once the necessary\n  corrections have been made", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.str-el", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the effect of a magnetic field on the Kitaev model using the\nequation of motion approach for the spin Green's function. Our study considers\nboth cases: suppressed magnetization and finite magnetization in the\nparamagnetic phase of the Kitaev model. When magnetization is suppressed, the\nspecific heat exhibits an angular dependence with a $60^\\circ$ periodicity,\nconsistent with recent experimental observations in $\\alpha$-RuCl$_3$. This\nbehavior can be interpreted as a signature of Majorana fermion gap formation.\nHowever, when magnetization is included, the periodicity shifts from $60^\\circ$\nto $180^\\circ$, suggesting that the Majorana fermion gap behavior is no longer\npresent in this regime. Our results indicate that the suppression of\nmagnetization is essential for observing features associated with Majorana\nfermions, suggesting that a nearest-neighbor antiferromagnetic interaction\ncould contribute to this suppression.\n", "versions": [{"version": "v1", "created": "Sun, 9 Mar 2025 22:58:47 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 05:10:58 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Morinari", "Takao", ""], ["Takegami", "Hibiki", ""]], "extracted_entities": [{"text": "Kitaev model", "label": "AI model"}]}
{"id": "2503.07378", "submitter": "Yusuke Hashimoto", "authors": "Yusuke Hashimoto, Xue Jia, Hao Li, Takaaki Tomai", "title": "Materials Map Integrating Experimental and Computational Data through\n  Graph-Based Machine Learning for Enhanced Materials Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Materials informatics (MI), which emerges from the integration of materials\nscience and data science, is expected to greatly streamline the material\ndiscovery and development. The data used for MI are obtained from both\ncomputational and experimental studies, while their integration remains\nchallenging. In our previous study, we reported the integration of these\ndatasets by applying a machine learning model that captures trends hidden in\nthe experimental datasets to compositional data stored in the computational\ndatabase. In this study, we use the obtained data to construct materials maps,\nwhich visualize the relation in the structural features of materials, aiming to\nsupport study by the experimental researchers. The map is constructed using the\nMatDeepLearn (MDL) framework, which implements the graph-based representation\nof material structures, deep learning, and dimensional reduction for the map\nconstruction. We evaluate the obtained materials maps through statistical\nanalysis and found that the MDL using message passing neural network (MPNN)\nenables efficient extraction of features that reflect the structural complexity\nof materials. Moreover, we found that this advantage does not necessarily\ntranslate into improved accuracy in predicting material properties. We\nattribute this unexpected outcome to the high learning performance inherent in\nMPNN, which can contribute to the structuring of data points within the\nmaterials map.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2025 14:31:34 GMT"}, {"version": "v2", "created": "Tue, 11 Mar 2025 06:31:52 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 10:04:14 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Hashimoto", "Yusuke", ""], ["Jia", "Xue", ""], ["Li", "Hao", ""], ["Tomai", "Takaaki", ""]], "extracted_entities": [{"text": "deep learning", "label": "Few-shot Learning"}, {"text": "dimensional reduction", "label": "Few-shot Learning"}]}
{"id": "2503.07384", "submitter": "Gonzalo Mancera", "authors": "Gonzalo Mancera, Daniel DeAlcala, Julian Fierrez, Ruben Tolosana,\n  Aythami Morales", "title": "Is My Text in Your AI Model? Gradient-based Membership Inference Test\n  applied to LLMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This work adapts and studies the gradient-based Membership Inference Test\n(gMINT) to the classification of text based on LLMs. MINT is a general approach\nintended to determine if given data was used for training machine learning\nmodels, and this work focuses on its application to the domain of Natural\nLanguage Processing. Using gradient-based analysis, the MINT model identifies\nwhether particular data samples were included during the language model\ntraining phase, addressing growing concerns about data privacy in machine\nlearning. The method was evaluated in seven Transformer-based models and six\ndatasets comprising over 2.5 million sentences, focusing on text classification\ntasks. Experimental results demonstrate MINTs robustness, achieving AUC scores\nbetween 85% and 99%, depending on data size and model architecture. These\nfindings highlight MINTs potential as a scalable and reliable tool for auditing\nmachine learning models, ensuring transparency, safeguarding sensitive data,\nand fostering ethical compliance in the deployment of AI/NLP technologies.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2025 14:32:56 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 12:37:37 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Mancera", "Gonzalo", ""], ["DeAlcala", "Daniel", ""], ["Fierrez", "Julian", ""], ["Tolosana", "Ruben", ""], ["Morales", "Aythami", ""]], "extracted_entities": [{"text": "LLMs", "label": "LLMs"}, {"text": "ethical compliance", "label": "AI Ethics"}]}
{"id": "2503.07496", "submitter": "Alejandro Tlaie", "authors": "Alejandro Tlaie, Jimmy Farrell", "title": "Securing External Deeper-than-black-box GPAI Evaluations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper examines the critical challenges and potential solutions for\nconducting secure and effective external evaluations of general-purpose AI\n(GPAI) models. With the exponential growth in size, capability, reach and\naccompanying risk of these models, ensuring accountability, safety, and public\ntrust requires frameworks that go beyond traditional black-box methods. The\ndiscussion begins with an analysis of the need for deeper-than-black-box\nevaluations (Section I), emphasizing the importance of understanding model\ninternals to uncover latent risks and ensure compliance with ethical and\nregulatory standards. Building on this foundation, Section II addresses the\nsecurity considerations of remote evaluations, outlining the threat landscape,\ntechnical solutions, and safeguards necessary to protect both evaluators and\nproprietary model data. Finally, Section III synthesizes these insights into\nactionable recommendations and future directions, aiming to establish a robust,\nscalable, and transparent framework for external assessments in GPAI\ngovernance.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2025 16:13:45 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 13:43:45 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Tlaie", "Alejandro", ""], ["Farrell", "Jimmy", ""]], "extracted_entities": [{"text": "ethical and\nregulatory standards", "label": "AI Ethics"}]}
{"id": "2503.07586", "submitter": "JaeWon Kim", "authors": "JaeWon Kim, Jiaying \"Lizzy\" Liu, Cassidy Pyle, Sowmya Somanath,\n  Lindsay Popowski, Hua Shen, Casey Fiesler, Gillian R. Hayes, Alexis Hiniker,\n  Wendy Ju, Florian \"Floyd\" Mueller, Ahmer Arif, Yasmine Kotturi", "title": "Design as Hope: Reimagining Futures for Seemingly Doomed Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Design has the power to cultivate hope, especially in the face of seemingly\nintractable societal challenges. This one-day workshop explores how design\nmethodologies -- ranging from problem reframing to participatory, speculative,\nand critical design -- can empower research communities to drive meaningful\nreal-world changes. By aligning design thinking with hope theory -- framework\nof viewing hope as \"goal-directed,\" \"pathways,\" and \"agentic\" thinking\nprocesses -- we aim to examine how researchers can move beyond focusing on harm\nmitigation and instead reimagine alternative futures. Through hands-on\nactivities, participants will engage in problem reframing, develop a taxonomy\nof design methods related to hope, and explore how community-driven design\napproaches can sustain efforts toward societal and individual hope. The\nworkshop also interrogates the ethical and practical boundaries of leveraging\nhope in design research. By the end of the session, participants will leave\nwith concrete strategies for integrating a hopeful design approach into their\nresearch, as well as a network for ongoing collaboration. Ultimately, we\nposition hopeful design not just as a practical tool for action and\nproblem-solving but as a catalyst for cultivating resilience and envisioning\ntransformative futures.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2025 17:49:10 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 11:27:00 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Kim", "JaeWon", ""], ["Liu", "Jiaying \"Lizzy\"", ""], ["Pyle", "Cassidy", ""], ["Somanath", "Sowmya", ""], ["Popowski", "Lindsay", ""], ["Shen", "Hua", ""], ["Fiesler", "Casey", ""], ["Hayes", "Gillian R.", ""], ["Hiniker", "Alexis", ""], ["Ju", "Wendy", ""], ["Mueller", "Florian \"Floyd\"", ""], ["Arif", "Ahmer", ""], ["Kotturi", "Yasmine", ""]], "extracted_entities": [{"text": "ethical and practical boundaries", "label": "AI Ethics"}]}
{"id": "2503.07933", "submitter": "Yirui Wang", "authors": "Qinji Yu, Yirui Wang, Ke Yan, Dandan Zheng, Dashan Ai, Dazhou Guo,\n  Zhanghexuan Ji, Yanzhou Su, Yun Bian, Na Shen, Xiaowei Ding, Le Lu, Xianghua\n  Ye, Dakai Jin", "title": "From Slices to Sequences: Autoregressive Tracking Transformer for\n  Cohesive and Consistent 3D Lymph Node Detection in CT Scans", "comments": "Technical report (11 pages plus supplementary)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lymph node (LN) assessment is an essential task in the routine radiology\nworkflow, providing valuable insights for cancer staging, treatment planning\nand beyond. Identifying scatteredly-distributed and low-contrast LNs in 3D CT\nscans is highly challenging, even for experienced clinicians. Previous lesion\nand LN detection methods demonstrate effectiveness of 2.5D approaches (i.e,\nusing 2D network with multi-slice inputs), leveraging pretrained 2D model\nweights and showing improved accuracy as compared to separate 2D or 3D\ndetectors. However, slice-based 2.5D detectors do not explicitly model\ninter-slice consistency for LN as a 3D object, requiring heuristic post-merging\nsteps to generate final 3D LN instances, which can involve tuning a set of\nparameters for each dataset. In this work, we formulate 3D LN detection as a\ntracking task and propose LN-Tracker, a novel LN tracking transformer, for\njoint end-to-end detection and 3D instance association. Built upon DETR-based\ndetector, LN-Tracker decouples transformer decoder's query into the track and\ndetection groups, where the track query autoregressively follows previously\ntracked LN instances along the z-axis of a CT scan. We design a new transformer\ndecoder with masked attention module to align track query's content to the\ncontext of current slice, meanwhile preserving detection query's high accuracy\nin current slice. An inter-slice similarity loss is introduced to encourage\ncohesive LN association between slices. Extensive evaluation on four lymph node\ndatasets shows LN-Tracker's superior performance, with at least 2.7% gain in\naverage sensitivity when compared to other top 3D/2.5D detectors. Further\nvalidation on public lung nodule and prostate tumor detection tasks confirms\nthe generalizability of LN-Tracker as it achieves top performance on both\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2025 00:22:05 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 00:01:12 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Yu", "Qinji", ""], ["Wang", "Yirui", ""], ["Yan", "Ke", ""], ["Zheng", "Dandan", ""], ["Ai", "Dashan", ""], ["Guo", "Dazhou", ""], ["Ji", "Zhanghexuan", ""], ["Su", "Yanzhou", ""], ["Bian", "Yun", ""], ["Shen", "Na", ""], ["Ding", "Xiaowei", ""], ["Lu", "Le", ""], ["Ye", "Xianghua", ""], ["Jin", "Dakai", ""]], "extracted_entities": [{"text": "masked attention module", "label": "Attention mechanism"}]}
{"id": "2503.08048", "submitter": "Sanghyuk Chun", "authors": "Sanghyuk Chun and Sangdoo Yun", "title": "LongProLIP: A Probabilistic Vision-Language Model with Long Context Text", "comments": "Accepted as a tiny paper at the 1st workshop of \"Quantify Uncertainty\n  and Hallucination in Foundation Models: The Next Frontier in Reliable AI\" at\n  ICLR 2025; code: https://github.com/naver-ai/prolip; models:\n  https://huggingface.co/collections/SanghyukChun/prolip-6712595dfc87fd8597350291", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Probabilistic Language-Image Pre-Training (ProLIP) has been\nproposed to tackle the multiplicity issue of vision-language (VL) tasks.\nDespite their success in probabilistic representation learning at a scale, the\nProLIP models cannot handle long context texts longer than 64 context length,\nwhich limits their ability to capture rich contextual information from longer\ntext sequences. To address this issue, this paper proposes a fine-tuning\nstrategy for ProLIP to accept longer texts, e.g., 256 text tokens. Experimental\nresults on Urban-1k and the DataComp evaluation suite show that the proposed\nLongProLIP recipe can improve understanding of long contexts while minimizing\nthe negative effect of fine-tuning.We also observe a trade-off between the long\ncontext understanding (measured by Urban-1k) and general zero-shot capability\n(measured by evaluation datasets by DataComp). Code is available at\nhttps://github.com/naver-ai/prolip\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2025 05:04:43 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 06:05:04 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chun", "Sanghyuk", ""], ["Yun", "Sangdoo", ""]], "extracted_entities": [{"text": "probabilistic representation learning", "label": "Few-shot Learning"}, {"text": "fine-tuning", "label": "Fine-tuning"}]}
{"id": "2503.08061", "submitter": "DongHeun Han", "authors": "DongHeun Han, Byungmin Kim, RoUn Lee, KyeongMin Kim, Hyoseok Hwang,\n  HyeongYeop Kang", "title": "ForceGrip: Data-Free Curriculum Learning for Realistic Grip Force\n  Control in VR Hand Manipulation", "comments": "19 pages, 10 figs (with appendix). Demo Video:\n  https://youtu.be/lR-YAfninJw", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.GR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic hand manipulation is a key component of immersive virtual reality\n(VR), yet existing methods often rely on a kinematic approach or motion-capture\ndatasets that omit crucial physical attributes such as contact forces and\nfinger torques. Consequently, these approaches prioritize tight,\none-size-fits-all grips rather than reflecting users' intended force levels. We\npresent ForceGrip, a deep learning agent that synthesizes realistic hand\nmanipulation motions, faithfully reflecting the user's grip force intention.\nInstead of mimicking predefined motion datasets, ForceGrip uses generated\ntraining scenarios-randomizing object shapes, wrist movements, and trigger\ninput flows-to challenge the agent with a broad spectrum of physical\ninteractions. To effectively learn from these complex tasks, we employ a\nthree-phase curriculum learning framework comprising Finger Positioning,\nIntention Adaptation, and Dynamic Stabilization. This progressive strategy\nensures stable hand-object contact, adaptive force control based on user\ninputs, and robust handling under dynamic conditions. Additionally, a proximity\nreward function enhances natural finger motions and accelerates training\nconvergence. Quantitative and qualitative evaluations reveal ForceGrip's\nsuperior force controllability and plausibility compared to state-of-the-art\nmethods. The video presentation of our paper is accessible at\nhttps://youtu.be/lR-YAfninJw.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2025 05:39:07 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 06:35:25 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Han", "DongHeun", ""], ["Kim", "Byungmin", ""], ["Lee", "RoUn", ""], ["Kim", "KyeongMin", ""], ["Hwang", "Hyoseok", ""], ["Kang", "HyeongYeop", ""]], "extracted_entities": [{"text": "ForceGrip", "label": "AI model"}, {"text": "ForceGrip", "label": "AI model"}, {"text": "ForceGrip", "label": "AI model"}]}
{"id": "2503.08179", "submitter": "Zicheng Ma", "authors": "Zicheng Ma and Chuanliu Fan and Zhicong Wang and Zhenyu Chen and\n  Xiaohan Lin and Yanheng Li and Shihao Feng and Jun Zhang and Ziqiang Cao and\n  Yi Qin Gao", "title": "ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with\n  Large Language Models", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large language models have made remarkable progress in the field of molecular\nscience, particularly in understanding and generating functional small\nmolecules. This success is largely attributed to the effectiveness of molecular\ntokenization strategies. In protein science, the amino acid sequence serves as\nthe sole tokenizer for LLMs. However, many fundamental challenges in protein\nscience are inherently structure-dependent. The absence of structure-aware\ntokens significantly limits the capabilities of LLMs for comprehensive\nbiomolecular comprehension and multimodal generation. To address these\nchallenges, we introduce a novel framework, ProtTeX, which tokenizes the\nprotein sequences, structures, and textual information into a unified discrete\nspace. This innovative approach enables joint training of the LLM exclusively\nthrough the Next-Token Prediction paradigm, facilitating multimodal protein\nreasoning and generation. ProtTeX enables general LLMs to perceive and process\nprotein structures through sequential text input, leverage structural\ninformation as intermediate reasoning components, and generate or manipulate\nstructures via sequential text output. Experiments demonstrate that our model\nachieves significant improvements in protein function prediction, outperforming\nthe state-of-the-art domain expert model with a twofold increase in accuracy.\nOur framework enables high-quality conformational generation and customizable\nprotein design. For the first time, we demonstrate that by adopting the\nstandard training and inference pipelines from the LLM domain, ProtTeX empowers\ndecoder-only LLMs to effectively address diverse spectrum of protein-related\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2025 08:43:05 GMT"}, {"version": "v2", "created": "Wed, 12 Mar 2025 08:46:33 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2025 13:54:27 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Ma", "Zicheng", ""], ["Fan", "Chuanliu", ""], ["Wang", "Zhicong", ""], ["Chen", "Zhenyu", ""], ["Lin", "Xiaohan", ""], ["Li", "Yanheng", ""], ["Feng", "Shihao", ""], ["Zhang", "Jun", ""], ["Cao", "Ziqiang", ""], ["Gao", "Yi Qin", ""]], "extracted_entities": [{"text": "LLMs", "label": "LLM"}, {"text": "LLMs", "label": "LLM"}, {"text": "LLMs", "label": "LLM"}, {"text": "LLM", "label": "LLM"}, {"text": "LLMs", "label": "LLM"}]}
{"id": "2503.08180", "submitter": "Haoyu Zhao", "authors": "Minyue Dai, Jingbo Wang, Ke Fan, Bin Ji, Haoyu Zhao, Junting Dong, Bo\n  Dai", "title": "Towards Synthesized and Editable Motion In-Betweening Through Part-Wise\n  Phase Representation", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Styled motion in-betweening is crucial for computer animation and gaming.\nHowever, existing methods typically encode motion styles by modeling whole-body\nmotions, often overlooking the representation of individual body parts. This\nlimitation reduces the flexibility of infilled motion, particularly in\nadjusting the motion styles of specific limbs independently. To overcome this\nchallenge, we propose a novel framework that models motion styles at the\nbody-part level, enhancing both the diversity and controllability of infilled\nmotions. Our approach enables more nuanced and expressive animations by\nallowing precise modifications to individual limb motions while maintaining\noverall motion coherence. Leveraging phase-related insights, our framework\nemploys periodic autoencoders to automatically extract the phase of each body\npart, capturing distinctive local style features. Additionally, we effectively\ndecouple the motion source from synthesis control by integrating motion\nmanifold learning and conditional generation techniques from both image and\nmotion domains. This allows the motion source to generate high-quality motions\nacross various styles, with extracted motion and style features readily\navailable for controlled synthesis in subsequent tasks. Comprehensive\nevaluations demonstrate that our method achieves superior speed, robust\ngeneralization, and effective generation of extended motion sequences.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2025 08:44:27 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 03:18:41 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Dai", "Minyue", ""], ["Wang", "Jingbo", ""], ["Fan", "Ke", ""], ["Ji", "Bin", ""], ["Zhao", "Haoyu", ""], ["Dong", "Junting", ""], ["Dai", "Bo", ""]], "extracted_entities": [{"text": "motion\nmanifold learning", "label": "Few-shot Learning"}]}
{"id": "2503.08421", "submitter": "Qiming Xia", "authors": "Qiming Xia, Wenkai Lin, Haoen Xiang, Xun Huang, Siheng Chen, Zhen\n  Dong, Cheng Wang, Chenglu Wen", "title": "Learning to Detect Objects from Multi-Agent LiDAR Scans without Manual\n  Labels", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised 3D object detection serves as an important solution for offline\n3D object annotation. However, due to the data sparsity and limited views, the\nclustering-based label fitting in unsupervised object detection often generates\nlow-quality pseudo-labels. Multi-agent collaborative dataset, which involves\nthe sharing of complementary observations among agents, holds the potential to\nbreak through this bottleneck. In this paper, we introduce a novel unsupervised\nmethod that learns to Detect Objects from Multi-Agent LiDAR scans, termed DOtA,\nwithout using labels from external. DOtA first uses the internally shared\nego-pose and ego-shape of collaborative agents to initialize the detector,\nleveraging the generalization performance of neural networks to infer\npreliminary labels. Subsequently,DOtA uses the complementary observations\nbetween agents to perform multi-scale encoding on preliminary labels, then\ndecodes high-quality and low-quality labels. These labels are further used as\nprompts to guide a correct feature learning process, thereby enhancing the\nperformance of the unsupervised object detection task. Extensive experiments on\nthe V2V4Real and OPV2V datasets show that our DOtA outperforms state-of-the-art\nunsupervised 3D object detection methods. Additionally, we also validate the\neffectiveness of the DOtA labels under various collaborative perception\nframeworks.The code is available at https://github.com/xmuqimingxia/DOtA.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2025 13:34:35 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 01:41:04 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Xia", "Qiming", ""], ["Lin", "Wenkai", ""], ["Xiang", "Haoen", ""], ["Huang", "Xun", ""], ["Chen", "Siheng", ""], ["Dong", "Zhen", ""], ["Wang", "Cheng", ""], ["Wen", "Chenglu", ""]], "extracted_entities": [{"text": "prompts", "label": "Prompting"}]}
{"id": "2503.08434", "submitter": "Armando Fortes", "authors": "Armando Fortes, Tianyi Wei, Shangchen Zhou, Xingang Pan", "title": "Bokeh Diffusion: Defocus Blur Control in Text-to-Image Diffusion Models", "comments": "Project page: https://atfortes.github.io/projects/bokeh-diffusion/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in large-scale text-to-image models have revolutionized\ncreative fields by generating visually captivating outputs from textual\nprompts; however, while traditional photography offers precise control over\ncamera settings to shape visual aesthetics -- such as depth-of-field -- current\ndiffusion models typically rely on prompt engineering to mimic such effects.\nThis approach often results in crude approximations and inadvertently altering\nthe scene content. In this work, we propose Bokeh Diffusion, a scene-consistent\nbokeh control framework that explicitly conditions a diffusion model on a\nphysical defocus blur parameter. By grounding depth-of-field adjustments, our\nmethod preserves the underlying scene structure as the level of blur is varied.\nTo overcome the scarcity of paired real-world images captured under different\ncamera settings, we introduce a hybrid training pipeline that aligns\nin-the-wild images with synthetic blur augmentations. Extensive experiments\ndemonstrate that our approach not only achieves flexible, lens-like blur\ncontrol but also supports applications such as real image editing via\ninversion.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2025 13:49:12 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 08:41:47 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Fortes", "Armando", ""], ["Wei", "Tianyi", ""], ["Zhou", "Shangchen", ""], ["Pan", "Xingang", ""]], "extracted_entities": [{"text": "textual\nprompts", "label": "Prompting"}]}
{"id": "2503.08481", "submitter": "Weijie Zhou", "authors": "Weijie Zhou, Manli Tao, Chaoyang Zhao, Haiyun Guo, Honghui Dong, Ming\n  Tang, Jinqiao Wang", "title": "PhysVLM: Enabling Visual Language Models to Understand Robotic Physical\n  Reachability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the environment and a robot's physical reachability is crucial\nfor task execution. While state-of-the-art vision-language models (VLMs) excel\nin environmental perception, they often generate inaccurate or impractical\nresponses in embodied visual reasoning tasks due to a lack of understanding of\nrobotic physical reachability. To address this issue, we propose a unified\nrepresentation of physical reachability across diverse robots, i.e.,\nSpace-Physical Reachability Map (S-P Map), and PhysVLM, a vision-language model\nthat integrates this reachability information into visual reasoning.\nSpecifically, the S-P Map abstracts a robot's physical reachability into a\ngeneralized spatial representation, independent of specific robot\nconfigurations, allowing the model to focus on reachability features rather\nthan robot-specific parameters. Subsequently, PhysVLM extends traditional VLM\narchitectures by incorporating an additional feature encoder to process the S-P\nMap, enabling the model to reason about physical reachability without\ncompromising its general vision-language capabilities. To train and evaluate\nPhysVLM, we constructed a large-scale multi-robot dataset, Phys100K, and a\nchallenging benchmark, EQA-phys, which includes tasks for six different robots\nin both simulated and real-world environments. Experimental results demonstrate\nthat PhysVLM outperforms existing models, achieving a 14\\% improvement over\nGPT-4o on EQA-phys and surpassing advanced embodied VLMs such as RoboMamba and\nSpatialVLM on the RoboVQA-val and OpenEQA benchmarks. Additionally, the S-P Map\nshows strong compatibility with various VLMs, and its integration into\nGPT-4o-mini yields a 7.1\\% performance improvement.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2025 14:34:41 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 11:19:12 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhou", "Weijie", ""], ["Tao", "Manli", ""], ["Zhao", "Chaoyang", ""], ["Guo", "Haiyun", ""], ["Dong", "Honghui", ""], ["Tang", "Ming", ""], ["Wang", "Jinqiao", ""]], "extracted_entities": [{"text": "GPT-4o", "label": "GPT"}, {"text": "GPT-4o-mini", "label": "GPT"}]}
{"id": "2503.08679", "submitter": "Iv\\'an Arcuschin", "authors": "Iv\\'an Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran\n  Rajamanoharan, Neel Nanda, Arthur Conmy", "title": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful", "comments": "Accepted to the Reasoning and Planning for Large Language Models\n  Workshop (ICLR 25), 10 main paper pages, 38 appendix pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art\nAI capabilities. However, recent studies have shown that CoT reasoning is not\nalways faithful, i.e. CoT reasoning does not always reflect how models arrive\nat conclusions. So far, most of these studies have focused on unfaithfulness in\nunnatural contexts where an explicit bias has been introduced. In contrast, we\nshow that unfaithful CoT can occur on realistic prompts with no artificial\nbias. Our results reveal non-negligible rates of several forms of unfaithful\nreasoning in frontier models: Sonnet 3.7 (16.3%), DeepSeek R1 (5.3%) and\nChatGPT-4o (7.0%) all answer a notable proportion of question pairs\nunfaithfully. Specifically, we find that models rationalize their implicit\nbiases in answers to binary questions (\"implicit post-hoc rationalization\").\nFor example, when separately presented with the questions \"Is X bigger than Y?\"\nand \"Is Y bigger than X?\", models sometimes produce superficially coherent\narguments to justify answering Yes to both questions or No to both questions,\ndespite such responses being logically contradictory. We also investigate\nrestoration errors (Dziri et al., 2023), where models make and then silently\ncorrect errors in their reasoning, and unfaithful shortcuts, where models use\nclearly illogical reasoning to simplify solving problems in Putnam questions (a\nhard benchmark). Our findings raise challenges for AI safety work that relies\non monitoring CoT to detect undesired behavior.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2025 17:56:30 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 17:49:58 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Arcuschin", "Iv\u00e1n", ""], ["Janiak", "Jett", ""], ["Krzyzanowski", "Robert", ""], ["Rajamanoharan", "Senthooran", ""], ["Nanda", "Neel", ""], ["Conmy", "Arthur", ""]], "extracted_entities": [{"text": "Chain-of-Thought", "label": "Chain of thought"}, {"text": "CoT reasoning", "label": "Chain of thought"}, {"text": "CoT reasoning", "label": "Chain of thought"}, {"text": "CoT", "label": "Chain of thought"}, {"text": "realistic prompts", "label": "Prompting"}, {"text": "ChatGPT-4o", "label": "ChatGPT"}, {"text": "models", "label": "AI model"}, {"text": "models", "label": "AI model"}, {"text": "models", "label": "AI model"}, {"text": "AI safety work", "label": "AI Ethics"}, {"text": "CoT", "label": "Chain of thought"}]}
{"id": "2503.08708", "submitter": "Jingyi Zheng", "authors": "Jingyi Zheng, Junfeng Wang, Zhen Sun, Wenhan Dong, Yule Liu, Xinlei He", "title": "TH-Bench: Evaluating Evading Attacks via Humanizing AI Text on\n  Machine-Generated Text Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Large Language Models (LLMs) advance, Machine-Generated Texts (MGTs) have\nbecome increasingly fluent, high-quality, and informative. Existing wide-range\nMGT detectors are designed to identify MGTs to prevent the spread of plagiarism\nand misinformation. However, adversaries attempt to humanize MGTs to evade\ndetection (named evading attacks), which requires only minor modifications to\nbypass MGT detectors. Unfortunately, existing attacks generally lack a unified\nand comprehensive evaluation framework, as they are assessed using different\nexperimental settings, model architectures, and datasets. To fill this gap, we\nintroduce the Text-Humanization Benchmark (TH-Bench), the first comprehensive\nbenchmark to evaluate evading attacks against MGT detectors. TH-Bench evaluates\nattacks across three key dimensions: evading effectiveness, text quality, and\ncomputational overhead. Our extensive experiments evaluate 6 state-of-the-art\nattacks against 13 MGT detectors across 6 datasets, spanning 19 domains and\ngenerated by 11 widely used LLMs. Our findings reveal that no single evading\nattack excels across all three dimensions. Through in-depth analysis, we\nhighlight the strengths and limitations of different attacks. More importantly,\nwe identify a trade-off among three dimensions and propose two optimization\ninsights. Through preliminary experiments, we validate their correctness and\neffectiveness, offering potential directions for future research.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2025 02:55:05 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 10:37:18 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zheng", "Jingyi", ""], ["Wang", "Junfeng", ""], ["Sun", "Zhen", ""], ["Dong", "Wenhan", ""], ["Liu", "Yule", ""], ["He", "Xinlei", ""]], "extracted_entities": [{"text": "Large Language Models", "label": "Large Language Model"}, {"text": "MGT", "label": "Large Language Model"}, {"text": "MGT", "label": "Large Language Model"}]}
{"id": "2503.08741", "submitter": "Letian Zhang", "authors": "Letian Zhang, Quan Cui, Bingchen Zhao, Cheng Yang", "title": "Oasis: One Image is All You Need for Multimodal Instruction Data\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The success of multi-modal large language models (MLLMs) has been largely\nattributed to the large-scale training data. However, the training data of many\nMLLMs is unavailable due to privacy concerns. The expensive and labor-intensive\nprocess of collecting multi-modal data further exacerbates the problem. Is it\npossible to synthesize multi-modal training data automatically without\ncompromising diversity and quality? In this paper, we propose a new method,\nOasis, to synthesize high-quality multi-modal data with only images. Oasis\nbreaks through traditional methods by prompting only images to the MLLMs, thus\nextending the data diversity by a large margin. Our method features a delicate\nquality control method which ensures the data quality. We collected over 500k\ndata and conducted incremental experiments on LLaVA-NeXT. Extensive experiments\ndemonstrate that our method can significantly improve the performance of MLLMs.\nThe image-based synthesis also allows us to focus on the specific-domain\nability of MLLMs. Code and data will be publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2025 08:25:40 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 06:15:32 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhang", "Letian", ""], ["Cui", "Quan", ""], ["Zhao", "Bingchen", ""], ["Yang", "Cheng", ""]], "extracted_entities": [{"text": "multi-modal large language models", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "privacy concerns", "label": "AI Ethics"}, {"text": "prompting", "label": "Prompting"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}]}
{"id": "2503.09010", "submitter": "Jingkai Sun", "authors": "Qiang Zhang, Zhang Zhang, Wei Cui, Jingkai Sun, Jiahang Cao, Yijie\n  Guo, Gang Han, Wen Zhao, Jiaxu Wang, Chenghao Sun, Lingfeng Zhang, Hao Cheng,\n  Yujie Chen, Lin Wang, Jian Tang, Renjing Xu", "title": "HumanoidPano: Hybrid Spherical Panoramic-LiDAR Cross-Modal Perception\n  for Humanoid Robots", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The perceptual system design for humanoid robots poses unique challenges due\nto inherent structural constraints that cause severe self-occlusion and limited\nfield-of-view (FOV). We present HumanoidPano, a novel hybrid cross-modal\nperception framework that synergistically integrates panoramic vision and LiDAR\nsensing to overcome these limitations. Unlike conventional robot perception\nsystems that rely on monocular cameras or standard multi-sensor configurations,\nour method establishes geometrically-aware modality alignment through a\nspherical vision transformer, enabling seamless fusion of 360 visual context\nwith LiDAR's precise depth measurements. First, Spherical Geometry-aware\nConstraints (SGC) leverage panoramic camera ray properties to guide\ndistortion-regularized sampling offsets for geometric alignment. Second,\nSpatial Deformable Attention (SDA) aggregates hierarchical 3D features via\nspherical offsets, enabling efficient 360{\\deg}-to-BEV fusion with\ngeometrically complete object representations. Third, Panoramic Augmentation\n(AUG) combines cross-view transformations and semantic alignment to enhance\nBEV-panoramic feature consistency during data augmentation. Extensive\nevaluations demonstrate state-of-the-art performance on the 360BEV-Matterport\nbenchmark. Real-world deployment on humanoid platforms validates the system's\ncapability to generate accurate BEV segmentation maps through panoramic-LiDAR\nco-perception, directly enabling downstream navigation tasks in complex\nenvironments. Our work establishes a new paradigm for embodied perception in\nhumanoid robotics.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2025 02:59:21 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 03:42:53 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhang", "Qiang", ""], ["Zhang", "Zhang", ""], ["Cui", "Wei", ""], ["Sun", "Jingkai", ""], ["Cao", "Jiahang", ""], ["Guo", "Yijie", ""], ["Han", "Gang", ""], ["Zhao", "Wen", ""], ["Wang", "Jiaxu", ""], ["Sun", "Chenghao", ""], ["Zhang", "Lingfeng", ""], ["Cheng", "Hao", ""], ["Chen", "Yujie", ""], ["Wang", "Lin", ""], ["Tang", "Jian", ""], ["Xu", "Renjing", ""]], "extracted_entities": [{"text": "Spatial Deformable Attention", "label": "Attention mechanism"}]}
{"id": "2503.09022", "submitter": "Wenjie Qu", "authors": "Wenjie Qu, Yuguang Zhou, Yongji Wu, Tingsong Xiao, Binhang Yuan,\n  Yiming Li, Jiaheng Zhang", "title": "Prompt Inversion Attack against Collaborative Inference of Large\n  Language Models", "comments": "To appear at IEEE Symposium on Security and Privacy 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Large language models (LLMs) have been widely applied for their remarkable\ncapability of content generation. However, the practical use of open-source\nLLMs is hindered by high resource requirements, making deployment expensive and\nlimiting widespread development. The collaborative inference is a promising\nsolution for this problem, in which users collaborate by each hosting a subset\nof layers and transmitting intermediate activation. Many companies are building\ncollaborative inference platforms to reduce LLM serving costs, leveraging\nusers' underutilized GPUs. Despite widespread interest in collaborative\ninference within academia and industry, the privacy risks associated with LLM\ncollaborative inference have not been well studied. This is largely because of\nthe challenge posed by inverting LLM activation due to its strong\nnon-linearity.\n  In this paper, to validate the severity of privacy threats in LLM\ncollaborative inference, we introduce the concept of prompt inversion attack\n(PIA), where a malicious participant intends to recover the input prompt\nthrough the activation transmitted by its previous participant. Extensive\nexperiments show that our PIA method substantially outperforms existing\nbaselines. For example, our method achieves an 88.4\\% token accuracy on the\nSkytrax dataset with the Llama-65B model when inverting the maximum number of\ntransformer layers, while the best baseline method only achieves 22.8\\%\naccuracy. The results verify the effectiveness of our PIA attack and highlights\nits practical threat to LLM collaborative inference systems.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2025 03:20:03 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 05:55:55 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Qu", "Wenjie", ""], ["Zhou", "Yuguang", ""], ["Wu", "Yongji", ""], ["Xiao", "Tingsong", ""], ["Yuan", "Binhang", ""], ["Li", "Yiming", ""], ["Zhang", "Jiaheng", ""]], "extracted_entities": [{"text": "Large language models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLM", "label": "Large Language Model"}, {"text": "LLM", "label": "Large Language Model"}, {"text": "input prompt", "label": "Prompting"}]}
{"id": "2503.09119", "submitter": "Jie Luo", "authors": "Jie Luo, Xueyin Chen", "title": "Training Hybrid Deep Quantum Neural Network for Reinforced Learning\n  Efficiently", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum computing offers a new paradigm for computation, exploiting an\nexponentially growing Hilbert space for data representation and operation.\nComputing results are obtained from measurement based stochastic sampling over\nnontrivial distributions produced by the quantum computing process with complex\ncorrelations from quantum entanglement. Quantum machine learning (QML) emerged\nas a promising method, potentially surpassing classical counterparts in\nefficiency and accuracy. While large scale fault tolerant quantum (LSFTQ)\nmachines are not yet available, recent works on hybrid quantum machine learning\nmodels, compatible with noisy intermediate scale quantum (NISQ) computers, have\nhinted at improved generalizability. Such hybrid deep quantum neural networks\n(hDQNNs) integrate GPU CPU based deep neural networks (DNNs) with forward\nparameterized quantum circuits (PQC) that can be straightforwardly executed on\nquantum processors. However, backpropagating through forward PQCs remains\ncomputationally inefficient if it is simulated on classical hardware, and\ncurrent quantum hardware constraints impede batched backpropagation even with\ndedicated PQCs for calculating gradients of forward PQCs, limiting scalability\nfor modern machine learning tasks. Here, we present a scalable quantum machine\nlearning architecture that overcomes these challenges with efficient\nbackpropagation, enabling scalable hDQNN training with PQCs on physical quantum\ncomputers or classical PQC simulators. Applied to reinforcement learning\nbenchmarks, our method highlights that hDQNNs could exhibit potentially\nimproved generalizability compared to purely classical models. These findings\noffer a pathway toward near term hybrid quantum classical computing systems for\nlarge scale machine learning and underscore the potential of hybrid quantum\nclassical architectures in advancing artificial intelligence.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2025 07:12:02 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 14:32:13 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Luo", "Jie", ""], ["Chen", "Xueyin", ""]], "extracted_entities": [{"text": "Quantum computing", "label": "quantisation"}, {"text": "quantum entanglement", "label": "quantisation"}, {"text": "Quantum machine learning", "label": "Few-shot Learning"}]}
{"id": "2503.09158", "submitter": "Fufangchen Zhao", "authors": "Fufangchen Zhao, Ming Li, Linrui Xu, Wenhao Jiang, Jian Gao, Danfeng\n  Yan", "title": "FaVChat: Unlocking Fine-Grained Facial Video Understanding with\n  Multimodal Large Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based multimodal large language models (VMLLMs) have demonstrated\nremarkable potential in cross-modal video understanding. However, their\nabilities in fine-grained face comprehension remain largely underexplored.\nGiven its pivotal role in human-centric intelligence, developing VMLLMs for\nfacial understanding holds a fundamental problem. To address this gap, we\npropose FaVChat, the first VMLLM specifically designed for fine-grained facial\nvideo understanding. To facilitate its training, we construct a large-scale\nfacial video dataset comprising over 60k videos, with the majority annotated\nwith 83 fine-grained facial attributes. These attributes are incorporated to\nenrich GPT-4o-generated captions, yielding 60k high-quality video-summary pairs\nand an additional 170k fine-grained question-answering (QA) pairs. To\neffectively capture rich facial clues, we propose a hybrid model architecture\ncomposed of a general visual encoder, a dedicated facial encoder, and a\nmixture-of-experts-enhanced adapter for adaptive fusion of multi-source visual\nfeatures. To mitigate information loss during feature transformation, we\nextract multi-granularity representations from the facial encoder and integrate\nthem into the subsequent LLM. This design enhances the model's ability to\ncomprehend and respond to questions involving diverse levels of visual details.\nWe employ a progressive training paradigm, transitioning from video\nsummarization to a high-quality subset of video QA, gradually increasing task\ncomplexity to enhance the model's fine-grained visual perception. We conduct\nextensive zero-shot evaluation on a couple of public benchmarks, demonstrating\nthat FaVChat consistently surpasses existing VMLLMs across multiple tasks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2025 08:33:46 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 10:45:03 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhao", "Fufangchen", ""], ["Li", "Ming", ""], ["Xu", "Linrui", ""], ["Jiang", "Wenhao", ""], ["Gao", "Jian", ""], ["Yan", "Danfeng", ""]], "extracted_entities": [{"text": "FaVChat", "label": "ChatGPT"}, {"text": "FaVChat", "label": "ChatGPT"}]}
{"id": "2503.09160", "submitter": "Hao Feng", "authors": "Hao Feng and Zhi Zuo and Jia-Hui Pan and Ka-Hei Hui and Yihua Shao and\n  Qi Dou and Wei Xie and Zhengzhe Liu", "title": "WonderVerse: Extendable 3D Scene Generation with Video Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce \\textit{WonderVerse}, a simple but effective framework for\ngenerating extendable 3D scenes. Unlike existing methods that rely on iterative\ndepth estimation and image inpainting, often leading to geometric distortions\nand inconsistencies, WonderVerse leverages the powerful world-level priors\nembedded within video generative foundation models to create highly immersive\nand geometrically coherent 3D environments. Furthermore, we propose a new\ntechnique for controllable 3D scene extension to substantially increase the\nscale of the generated environments. Besides, we introduce a novel abnormal\nsequence detection module that utilizes camera trajectory to address geometric\ninconsistency in the generated videos. Finally, WonderVerse is compatible with\nvarious 3D reconstruction methods, allowing both efficient and high-quality\ngeneration. Extensive experiments on 3D scene generation demonstrate that our\nWonderVerse, with an elegant and simple pipeline, delivers extendable and\nhighly-realistic 3D scenes, markedly outperforming existing works that rely on\nmore complex architectures.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2025 08:44:51 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 15:29:28 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Feng", "Hao", ""], ["Zuo", "Zhi", ""], ["Pan", "Jia-Hui", ""], ["Hui", "Ka-Hei", ""], ["Shao", "Yihua", ""], ["Dou", "Qi", ""], ["Xie", "Wei", ""], ["Liu", "Zhengzhe", ""]], "extracted_entities": [{"text": "video generative foundation models", "label": "Foundation Model"}]}
{"id": "2503.09216", "submitter": "Xu Chen", "authors": "Shuya Xing, Zhongxu Wei, Xu Chen, Junming Zhang, Zhenyu Yuan, Jiali\n  Zhao, Feng Jin, Tao Sun, Huifen Ren, Minjie Cui, Hong Chang, Tianping Ying,\n  Jiangang Guo, Xiaolong Chen, Shifeng Zhao, Wenping Zhou, Xinqi Li, Tian Qian,\n  Zhihai Cheng", "title": "Cooperation and competing of antipolar charge order and superconducting\n  states in a quasi-2D superatomic metallic crystal", "comments": "18 pages, 5 figures, to be sbmiited to Physical Review X", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.supr-con", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Exploring various unexpected new quantum states and their corresponding\nextraordinary physics in low dimensional quantum materials, and investing them\ninto application fields, is a primary concern of condensed matter physics.\nPreviously, we performed joint experiments and theoretical calculations to\ninvestigate a superatomic crystal of Au6Te12Se8 with low symmetry, which stacks\nthrough non-covalent inter-cube quasi-bonds. Au6Te12Se8 exhibits a triple-cube\ncharge density wave and spatially polarized metallic states interweaving at 9\nK. In addition, it undergoes a BKT phase transition at 2.8 K to form a\nquasi-two-dimensional superconducting state. The subsequent high-pressure\nexperimental results indicate that the two quantum states above compete with\nsuperconductivity. Here, we experimentally revealed competition and coexistence\namong superconductivity, triple-cube charge density waves, and polarized\nmetallic states during further temperature-decreasing process, as examined\nusing ultra-low temperature scanning tunneling microscopy/spectroscopy,\ntransport measurement, and Raman spectra. An extraordinary inversion symmetry\nbroken emerged inside the triple-cube-period of the original CDW at 300 mK,\nleading to the polarized metallic states transforming into parallel\npolarization states. The evidence of spontaneous polarization coexisting with\nsuperconductivity in real space is extremely rare and has been revealed by STM.\nIn addition, transport and Raman measurements indicate that there are multiple\nphase transition temperatures from 80K to that below the superconducting\ntransition temperature of Au6Te12Se8, which may also contain more unknown\nexotic quantum states, providing a platform for further exploration of\nintriguing physical properties.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2025 10:02:43 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 12:31:13 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Xing", "Shuya", ""], ["Wei", "Zhongxu", ""], ["Chen", "Xu", ""], ["Zhang", "Junming", ""], ["Yuan", "Zhenyu", ""], ["Zhao", "Jiali", ""], ["Jin", "Feng", ""], ["Sun", "Tao", ""], ["Ren", "Huifen", ""], ["Cui", "Minjie", ""], ["Chang", "Hong", ""], ["Ying", "Tianping", ""], ["Guo", "Jiangang", ""], ["Chen", "Xiaolong", ""], ["Zhao", "Shifeng", ""], ["Zhou", "Wenping", ""], ["Li", "Xinqi", ""], ["Qian", "Tian", ""], ["Cheng", "Zhihai", ""]], "extracted_entities": [{"text": "superconductivity", "label": "quantisation"}, {"text": "superconductivity", "label": "quantisation"}, {"text": "superconductivity", "label": "quantisation"}]}
{"id": "2503.09257", "submitter": "Haixing Gong", "authors": "Haixing Gong, Hui Zou, Xingzhou Liang, Shiyuan Meng, Pinlong Cai,\n  Xingcheng Xu, Jingjing Qu", "title": "DeepInnovation AI: A Global Dataset Mapping the AI innovation from\n  Academic Research to Industrial Patents", "comments": "32 pages and 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the rapidly evolving field of artificial intelligence (AI), mapping\ninnovation patterns and understanding effective technology transfer from\nresearch to applications are essential for economic growth. However, existing\ndata infrastructures suffer from fragmentation, incomplete coverage, and\ninsufficient evaluative capacity. Here, we present DeepInnovationAI, a\ncomprehensive global dataset containing three structured files.\nDeepPatentAI.csv: Contains 2,356,204 patent records with 8 field-specific\nattributes. DeepDiveAI.csv: Encompasses 3,511,929 academic publications with 13\nmetadata fields. These two datasets leverage large language models,\nmultilingual text analysis and dual-layer BERT classifiers to accurately\nidentify AI-related content, while utilizing hypergraph analysis to create\nrobust innovation metrics. Additionally, DeepCosineAI.csv: By applying semantic\nvector proximity analysis, this file presents approximately one hundred million\ncalculated paper-patent similarity pairs to enhance understanding of how\ntheoretical advancements translate into commercial technologies.\nDeepInnovationAI enables researchers, policymakers, and industry leaders to\nanticipate trends and identify collaboration opportunities. With extensive\ntemporal and geographical scope, it supports detailed analysis of technological\ndevelopment patterns and international competition dynamics, establishing a\nfoundation for modeling AI innovation and technology transfer processes.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2025 10:56:02 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 05:53:58 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Gong", "Haixing", ""], ["Zou", "Hui", ""], ["Liang", "Xingzhou", ""], ["Meng", "Shiyuan", ""], ["Cai", "Pinlong", ""], ["Xu", "Xingcheng", ""], ["Qu", "Jingjing", ""]], "extracted_entities": [{"text": "dual-layer BERT classifiers", "label": "BERT"}]}
{"id": "2503.09385", "submitter": "Masoud Jamshidiyan Tehrani", "authors": "Masoud Jamshidiyan Tehrani, Jinhan Kim, Paolo Tonella", "title": "PCLA: A Framework for Testing Autonomous Agents in the CARLA Simulator", "comments": "This work will be published at the FSE 2025 demonstration track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.RO cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent research on testing autonomous driving agents has grown significantly,\nespecially in simulation environments. The CARLA simulator is often the\npreferred choice, and the autonomous agents from the CARLA Leaderboard\nchallenge are regarded as the best-performing agents within this environment.\nHowever, researchers who test these agents, rather than training their own ones\nfrom scratch, often face challenges in utilizing them within customized test\nenvironments and scenarios. To address these challenges, we introduce PCLA\n(Pretrained CARLA Leaderboard Agents), an open-source Python testing framework\nthat includes nine high-performing pre-trained autonomous agents from the\nLeaderboard challenges. PCLA is the first infrastructure specifically designed\nfor testing various autonomous agents in arbitrary CARLA\nenvironments/scenarios. PCLA provides a simple way to deploy Leaderboard agents\nonto a vehicle without relying on the Leaderboard codebase, it allows\nresearchers to easily switch between agents without requiring modifications to\nCARLA versions or programming environments, and it is fully compatible with the\nlatest version of CARLA while remaining independent of the Leaderboard's\nspecific CARLA version. PCLA is publicly accessible at\nhttps://github.com/MasoudJTehrani/PCLA.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2025 13:29:35 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 09:14:35 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Tehrani", "Masoud Jamshidiyan", ""], ["Kim", "Jinhan", ""], ["Tonella", "Paolo", ""]], "extracted_entities": [{"text": "PCLA", "label": "Open-source LLMs"}, {"text": "PCLA", "label": "Open-source LLMs"}, {"text": "PCLA", "label": "Open-source LLMs"}, {"text": "PCLA", "label": "Open-source LLMs"}, {"text": "PCLA", "label": "Open-source LLMs"}]}
{"id": "2503.09494", "submitter": "Qi Xu", "authors": "Qi Xu and Annie Qu", "title": "Representation Retrieval Learning for Heterogeneous Data Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of big data, large-scale, multi-modal datasets are increasingly\nubiquitous, offering unprecedented opportunities for predictive modeling and\nscientific discovery. However, these datasets often exhibit complex\nheterogeneity, such as covariate shift, posterior drift, and missing\nmodalities, that can hinder the accuracy of existing prediction algorithms. To\naddress these challenges, we propose a novel Representation Retrieval ($R^2$)\nframework, which integrates a representation learning module (the representer)\nwith a sparsity-induced machine learning model (the learner). Moreover, we\nintroduce the notion of \"integrativeness\" for representers, characterized by\nthe effective data sources used in learning representers, and propose a\nSelective Integration Penalty (SIP) to explicitly improve the property.\nTheoretically, we demonstrate that the $R^2$ framework relaxes the conventional\nfull-sharing assumption in multi-task learning, allowing for partially shared\nstructures, and that SIP can improve the convergence rate of the excess risk\nbound. Extensive simulation studies validate the empirical performance of our\nframework, and applications to two real-world datasets further confirm its\nsuperiority over existing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2025 15:54:37 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 16:39:15 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Xu", "Qi", ""], ["Qu", "Annie", ""]], "extracted_entities": [{"text": "multi-task learning", "label": "Few-shot Learning"}]}
{"id": "2503.09532", "submitter": "Can Ludwig Rager", "authors": "Adam Karvonen, Can Rager, Johnny Lin, Curt Tigges, Joseph Bloom, David\n  Chanin, Yeu-Tong Lau, Eoin Farrell, Callum McDougall, Kola Ayonrinde, Matthew\n  Wearden, Arthur Conmy, Samuel Marks, Neel Nanda", "title": "SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language\n  Model Interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparse autoencoders (SAEs) are a popular technique for interpreting language\nmodel activations, and there is extensive recent work on improving SAE\neffectiveness. However, most prior work evaluates progress using unsupervised\nproxy metrics with unclear practical relevance. We introduce SAEBench, a\ncomprehensive evaluation suite that measures SAE performance across seven\ndiverse metrics, spanning interpretability, feature disentanglement and\npractical applications like unlearning. To enable systematic comparison, we\nopen-source a suite of over 200 SAEs across eight recently proposed SAE\narchitectures and training algorithms. Our evaluation reveals that gains on\nproxy metrics do not reliably translate to better practical performance. For\ninstance, while Matryoshka SAEs slightly underperform on existing proxy\nmetrics, they substantially outperform other architectures on feature\ndisentanglement metrics; moreover, this advantage grows with SAE scale. By\nproviding a standardized framework for measuring progress in SAE development,\nSAEBench enables researchers to study scaling trends and make nuanced\ncomparisons between different SAE architectures and training methodologies. Our\ninteractive interface enables researchers to flexibly visualize relationships\nbetween metrics across hundreds of open-source SAEs at: https://saebench.xyz\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2025 16:49:02 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 03:18:16 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Karvonen", "Adam", ""], ["Rager", "Can", ""], ["Lin", "Johnny", ""], ["Tigges", "Curt", ""], ["Bloom", "Joseph", ""], ["Chanin", "David", ""], ["Lau", "Yeu-Tong", ""], ["Farrell", "Eoin", ""], ["McDougall", "Callum", ""], ["Ayonrinde", "Kola", ""], ["Wearden", "Matthew", ""], ["Conmy", "Arthur", ""], ["Marks", "Samuel", ""], ["Nanda", "Neel", ""]], "extracted_entities": [{"text": "scaling trends", "label": "Scaling law"}]}
{"id": "2503.09533", "submitter": "Houyu Zhou", "authors": "Nguyen Thach, Fei Liu, Houyu Zhou, Hau Chan", "title": "Large Language Models for Multi-Facility Location Mechanism Design", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Designing strategyproof mechanisms for multi-facility location that optimize\nsocial costs based on agent preferences had been challenging due to the\nextensive domain knowledge required and poor worst-case guarantees. Recently,\ndeep learning models have been proposed as alternatives. However, these models\nrequire some domain knowledge and extensive hyperparameter tuning as well as\nlacking interpretability, which is crucial in practice when transparency of the\nlearned mechanisms is mandatory. In this paper, we introduce a novel approach,\nnamed LLMMech, that addresses these limitations by incorporating large language\nmodels (LLMs) into an evolutionary framework for generating interpretable,\nhyperparameter-free, empirically strategyproof, and nearly optimal mechanisms.\nOur experimental results, evaluated on various problem settings where the\nsocial cost is arbitrarily weighted across agents and the agent preferences may\nnot be uniformly distributed, demonstrate that the LLM-generated mechanisms\ngenerally outperform existing handcrafted baselines and deep learning models.\nFurthermore, the mechanisms exhibit impressive generalizability to\nout-of-distribution agent preferences and to larger instances with more agents.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2025 16:49:56 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 05:54:22 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Thach", "Nguyen", ""], ["Liu", "Fei", ""], ["Zhou", "Houyu", ""], ["Chan", "Hau", ""]], "extracted_entities": [{"text": "hyperparameter tuning", "label": "Fine-tuning"}, {"text": "large language\nmodels", "label": "Large Language Model"}]}
{"id": "2503.09567", "submitter": "Qiguang Chen", "authors": "Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng\n  Wang, Mengkang Hu, Yuhang Zhou, Te Gao, Wanxiang Che", "title": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning\n  Large Language Models", "comments": "Paper are available at https://long-cot.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent advancements in reasoning with large language models (RLLMs), such as\nOpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in\ncomplex domains like mathematics and coding. A central factor in their success\nlies in the application of long chain-of-thought (Long CoT) characteristics,\nwhich enhance reasoning abilities and enable the solution of intricate\nproblems. However, despite these developments, a comprehensive survey on Long\nCoT is still lacking, limiting our understanding of its distinctions from\ntraditional short chain-of-thought (Short CoT) and complicating ongoing debates\non issues like \"overthinking\" and \"test-time scaling.\" This survey seeks to\nfill this gap by offering a unified perspective on Long CoT. (1) We first\ndistinguish Long CoT from Short CoT and introduce a novel taxonomy to\ncategorize current reasoning paradigms. (2) Next, we explore the key\ncharacteristics of Long CoT: deep reasoning, extensive exploration, and\nfeasible reflection, which enable models to handle more complex tasks and\nproduce more efficient, coherent outcomes compared to the shallower Short CoT.\n(3) We then investigate key phenomena such as the emergence of Long CoT with\nthese characteristics, including overthinking, and test-time scaling, offering\ninsights into how these processes manifest in practice. (4) Finally, we\nidentify significant research gaps and highlight promising future directions,\nincluding the integration of multi-modal reasoning, efficiency improvements,\nand enhanced knowledge frameworks. By providing a structured overview, this\nsurvey aims to inspire future research and further the development of logical\nreasoning in artificial intelligence.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2025 17:35:03 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 04:34:15 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chen", "Qiguang", ""], ["Qin", "Libo", ""], ["Liu", "Jinhao", ""], ["Peng", "Dengyun", ""], ["Guan", "Jiannan", ""], ["Wang", "Peng", ""], ["Hu", "Mengkang", ""], ["Zhou", "Yuhang", ""], ["Gao", "Te", ""], ["Che", "Wanxiang", ""]], "extracted_entities": [{"text": "long chain-of-thought", "label": "Chain of thought"}, {"text": "Long CoT", "label": "Chain of thought"}, {"text": "Long\nCoT", "label": "Chain of thought"}, {"text": "short chain-of-thought", "label": "Chain of thought"}, {"text": "Short CoT", "label": "Chain of thought"}, {"text": "test-time scaling", "label": "Scaling law"}, {"text": "Long CoT", "label": "Chain of thought"}, {"text": "Long CoT", "label": "Chain of thought"}, {"text": "Short CoT", "label": "Chain of thought"}, {"text": "Long CoT", "label": "Chain of thought"}, {"text": "Short CoT", "label": "Chain of thought"}, {"text": "Long CoT", "label": "Chain of thought"}, {"text": "test-time scaling", "label": "Scaling law"}]}
{"id": "2503.09590", "submitter": "Md Mohaiminul Islam", "authors": "Md Mohaiminul Islam, Tushar Nagarajan, Huiyu Wang, Gedas Bertasius,\n  Lorenzo Torresani", "title": "BIMBA: Selective-Scan Compression for Long-Range Video Question\n  Answering", "comments": "Accepted by CVPR 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video Question Answering (VQA) in long videos poses the key challenge of\nextracting relevant information and modeling long-range dependencies from many\nredundant frames. The self-attention mechanism provides a general solution for\nsequence modeling, but it has a prohibitive cost when applied to a massive\nnumber of spatiotemporal tokens in long videos. Most prior methods rely on\ncompression strategies to lower the computational cost, such as reducing the\ninput length via sparse frame sampling or compressing the output sequence\npassed to the large language model (LLM) via space-time pooling. However, these\nnaive approaches over-represent redundant information and often miss salient\nevents or fast-occurring space-time patterns. In this work, we introduce BIMBA,\nan efficient state-space model to handle long-form videos. Our model leverages\nthe selective scan algorithm to learn to effectively select critical\ninformation from high-dimensional video and transform it into a reduced token\nsequence for efficient LLM processing. Extensive experiments demonstrate that\nBIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks,\nincluding PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and\nVideo-MME. Code, and models are publicly available at\nhttps://sites.google.com/view/bimba-mllm.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2025 17:57:32 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 17:14:31 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Islam", "Md Mohaiminul", ""], ["Nagarajan", "Tushar", ""], ["Wang", "Huiyu", ""], ["Bertasius", "Gedas", ""], ["Torresani", "Lorenzo", ""]], "extracted_entities": [{"text": "self-attention mechanism", "label": "Attention mechanism"}, {"text": "LLM", "label": "Large Language Model"}, {"text": "LLM", "label": "Large Language Model"}]}
{"id": "2503.09601", "submitter": "Itay Chachy", "authors": "Itay Chachy and Guy Yariv and Sagie Benaim", "title": "RewardSDS: Aligning Score Distillation via Reward-Weighted Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Score Distillation Sampling (SDS) has emerged as an effective technique for\nleveraging 2D diffusion priors for tasks such as text-to-3D generation. While\npowerful, SDS struggles with achieving fine-grained alignment to user intent.\nTo overcome this, we introduce RewardSDS, a novel approach that weights noise\nsamples based on alignment scores from a reward model, producing a weighted SDS\nloss. This loss prioritizes gradients from noise samples that yield aligned\nhigh-reward output. Our approach is broadly applicable and can extend SDS-based\nmethods. In particular, we demonstrate its applicability to Variational Score\nDistillation (VSD) by introducing RewardVSD. We evaluate RewardSDS and\nRewardVSD on text-to-image, 2D editing, and text-to-3D generation tasks,\nshowing significant improvements over SDS and VSD on a diverse set of metrics\nmeasuring generation quality and alignment to desired reward models, enabling\nstate-of-the-art performance. Project page is available at\nhttps://itaychachy.github.io/reward-sds/.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2025 17:59:47 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2025 13:28:22 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chachy", "Itay", ""], ["Yariv", "Guy", ""], ["Benaim", "Sagie", ""]], "extracted_entities": [{"text": "Score Distillation Sampling", "label": "Knowledge distillation"}, {"text": "Variational Score\nDistillation", "label": "Knowledge distillation"}]}
{"id": "2503.09915", "submitter": "Sai Madhav Modumudi", "authors": "M. M. Akbar and S. M. Modumudi", "title": "Near-Horizon Symmetries of Local Black Holes in General Relativity", "comments": "18 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "gr-qc", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyze the near-horizon symmetries of static, axisymmetric,\nfour-dimensional black holes with spherical and toroidal horizon topologies in\nvacuum general relativity. These black hole solutions, collectively referred to\nas local/distorted black holes, are known in closed form and are not\nasymptotically flat. Building on earlier works in the literature that primarily\nfocused on black holes with spherical topology, we compute the algebra of the\nKilling vector fields that preserve the asymptotic structure near the horizons\nand the algebra of the associated Noether-Wald charges under the boundary\nconditions that produce the spin-$s$ BMS$_d$ and the Heisenberg-like algebras.\nWe show that a similar analysis extends to all local axisymmetric black holes.\nThe toroidal topology of the holes changes the algebras considerably. For\nexample, one obtains two copies of spin-$s$ BMS$_3$ instead of spin-$s$\nBMS$_4$. We also revisit the thermodynamics of black holes under these boundary\nconditions. While previous studies suggested that spin-$s$ BMS$_d$ preserves\nthe first law of thermodynamics for isolated horizons ($\\kappa=\n\\text{const.}$), our analysis indicates that this is not generally the case\nwhen the spin parameter $s$ is nonzero. A nonzero $s$ can be seen as\nintroducing a conical singularity (in the Euclidean quantum gravity sense) or a\nHamiltonian that causes soft hairs to contribute to the energy. This leads us\nto interpret the spin-$s$ BMS$_d$ boundary condition as arising in the context\nof dynamical black holes.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 00:10:02 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Akbar", "M. M.", ""], ["Modumudi", "S. M.", ""]], "extracted_entities": [{"text": "first law of thermodynamics", "label": "Scaling law"}]}
{"id": "2503.09916", "submitter": "Jiaqi Sun", "authors": "Jiaqi Sun, Yujia Zheng, Xinshuai Dong, Haoyue Dai, Kun Zhang", "title": "Type Information-Assisted Self-Supervised Knowledge Graph Denoising", "comments": "Accepted by AISTATS 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge graphs serve as critical resources supporting intelligent systems,\nbut they can be noisy due to imperfect automatic generation processes. Existing\napproaches to noise detection often rely on external facts, logical rule\nconstraints, or structural embeddings. These methods are often challenged by\nimperfect entity alignment, flexible knowledge graph construction, and\noverfitting on structures. In this paper, we propose to exploit the consistency\nbetween entity and relation type information for noise detection, resulting a\nnovel self-supervised knowledge graph denoising method that avoids those\nproblems. We formalize type inconsistency noise as triples that deviate from\nthe majority with respect to type-dependent reasoning along the topological\nstructure. Specifically, we first extract a compact representation of a given\nknowledge graph via an encoder that models the type dependencies of triples.\nThen, the decoder reconstructs the original input knowledge graph based on the\ncompact representation. It is worth noting that, our proposal has the potential\nto address the problems of knowledge graph compression and completion, although\nthis is not our focus. For the specific task of noise detection, the\ndiscrepancy between the reconstruction results and the input knowledge graph\nprovides an opportunity for denoising, which is facilitated by the type\nconsistency embedded in our method. Experimental validation demonstrates the\neffectiveness of our approach in detecting potential noise in real-world data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 00:12:27 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Sun", "Jiaqi", ""], ["Zheng", "Yujia", ""], ["Dong", "Xinshuai", ""], ["Dai", "Haoyue", ""], ["Zhang", "Kun", ""]], "extracted_entities": [{"text": "structural embeddings", "label": "Embedding"}]}
{"id": "2503.09917", "submitter": "Filippo Mantovani", "authors": "Fabio Banchelli, Marta Garcia-Gasulla, Filippo Mantovani, Joan\n  Vinyals, Josep Pocurull, David Vicente, Beatriz Eguzkitza, Flavio C. C.\n  Galeazzo, Mario C. Acosta, Sergi Girona", "title": "Introducing MareNostrum5: A European pre-exascale energy-efficient\n  system designed to serve a broad spectrum of scientific workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  MareNostrum5 is a pre-exascale supercomputer at the Barcelona Supercomputing\nCenter (BSC), part of the EuroHPC Joint Undertaking. With a peak performance of\n314 petaflops, MareNostrum5 features a hybrid architecture comprising Intel\nSapphire Rapids CPUs, NVIDIA Hopper GPUs, and DDR5 and high-bandwidth memory\n(HBM), organized into four partitions optimized for diverse workloads. This\ndocument evaluates MareNostrum5 through micro-benchmarks (floating-point\nperformance, memory bandwidth, interconnect throughput), HPC benchmarks (HPL\nand HPCG), and application studies using Alya, OpenFOAM, and IFS. It highlights\nMareNostrum5's scalability, efficiency, and energy performance, utilizing the\nEAR (Energy Aware Runtime) framework to assess power consumption and the\neffects of direct liquid cooling. Additionally, HBM and DDR5 configurations are\ncompared to examine memory performance trade-offs. Designed to complement\nstandard technical documentation, this study provides insights to guide both\nnew and experienced users in optimizing their workloads and maximizing\nMareNostrum5's computational capabilities.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 00:18:43 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Banchelli", "Fabio", ""], ["Garcia-Gasulla", "Marta", ""], ["Mantovani", "Filippo", ""], ["Vinyals", "Joan", ""], ["Pocurull", "Josep", ""], ["Vicente", "David", ""], ["Eguzkitza", "Beatriz", ""], ["Galeazzo", "Flavio C. C.", ""], ["Acosta", "Mario C.", ""], ["Girona", "Sergi", ""]], "extracted_entities": [{"text": "Alya", "label": "ALBERT"}, {"text": "OpenFOAM", "label": "Open-source LLMs"}]}
{"id": "2503.09918", "submitter": "Carlos F. Sopuerta", "authors": "Michele Lenzi (1 and 2), Arnau Montava Agudo (1 and 3) and Carlos F.\n  Sopuerta (1 and 2) ((1) ICE-CSIC, (2) IEEC, (3) UIB)", "title": "Korteweg-de Vries Integrals for Modified Black Hole Potentials:\n  Instabilities and other Questions", "comments": "52 pages, 26 figures (50 files), JCAP style", "journal-ref": null, "doi": null, "report-no": null, "categories": "gr-qc astro-ph.HE hep-th nlin.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quasi-normal modes (QNMs) and greybody factors are some of the most\ncharacteristic features of the dynamics of black holes (BHs) and represent the\nbasis for a number of fundamental physics tests with gravitational wave\nobservations. It is therefore important to understand the properties of these\nquantities, naturally introduced within BH perturbation theory, in particular\nthe stability properties under modifications of the BH potential. Instabilities\nin the QNMs have been recently shown to appear in the BH pseudospectrum under\ncertain circumstances. In this work, we give a novel point of view based on the\nexistence of some recently discovered hidden symmetries in BH dynamics and the\nassociated infinite series of conserved quantities, the Korteweg-de Vries (KdV)\nintegrals. We provide different motivations to use the KdV integrals as\nindicators of some crucial BH spectral properties. In particular, by studying\nthem in different scenarios described by modified BH barriers, we find strong\nevidence that the KdV conserved quantities represent a useful tool to look for\ninstabilities in the BH spectrum of QNMs and in their greybody factors.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 00:18:56 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Lenzi", "Michele", "", "1 and 2"], ["Agudo", "Arnau Montava", "", "1 and 3"], ["Sopuerta", "Carlos F.", "", "1 and 2"]], "extracted_entities": [{"text": "Quasi-normal modes", "label": "LLMs"}, {"text": "greybody factors", "label": "LLMs"}, {"text": "QNMs", "label": "LLMs"}, {"text": "QNMs", "label": "LLMs"}, {"text": "greybody factors", "label": "LLMs"}]}
{"id": "2503.09920", "submitter": "Marcos Vinicius De Sousa Silva", "authors": "Carlos F. S. Pereira, Marcos V. de S. Silva, H. Belich, Denis C.\n  Rodrigues, J\\'ulio C. Fabris, and Manuel E. Rodrigues", "title": "Black-bounce solutions in a k-essence theory under the effects of\n  bumblebee gravity", "comments": "15 pages, 7 figures. Comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "gr-qc", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the present study, we analyze the effects of violation of Lorentz symmetry\nfor black-bounce solutions in a $k$-essence theory that has the form of a power\nlaw for the configuration $n=1/3$. We perform such analysis for a known model\nexplored in previous work $\\Sigma^2=x^2+a^2$ and complement the proposal with a\nnew black-bounce model for the area functions $\\Sigma^2_1=\\sqrt{x^4+d^4}$. This\nmodel has the Schwarzschild-de Sitter asymptomatic behavior for\n$x\\to{-\\infty}$, and we investigate the scalar field, potential, and energy\nconditions for both models. We have shown that the violation of Lorentz\nsymmetry can be generated through $k$-essence without the need for an\nadditional field. These results corroborate the validation of other previously\ninvestigated wormhole solutions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 00:23:13 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Pereira", "Carlos F. S.", ""], ["Silva", "Marcos V. de S.", ""], ["Belich", "H.", ""], ["Rodrigues", "Denis C.", ""], ["Fabris", "J\u00falio C.", ""], ["Rodrigues", "Manuel E.", ""]], "extracted_entities": [{"text": "power\nlaw", "label": "Scaling law"}]}
{"id": "2503.09925", "submitter": "Mahmoud Srewa", "authors": "Mahmoud Srewa, Tianyu Zhao, Salma Elmalaki", "title": "PluralLLM: Pluralistic Alignment in LLMs via Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ensuring Large Language Models (LLMs) align with diverse human preferences\nwhile preserving privacy and fairness remains a challenge. Existing methods,\nsuch as Reinforcement Learning from Human Feedback (RLHF), rely on centralized\ndata collection, making them computationally expensive and privacy-invasive. We\nintroduce PluralLLM a federated learning-based approach that enables multiple\nuser groups to collaboratively train a transformer-based preference predictor\nwithout sharing sensitive data, which can also serve as a reward model for\naligning LLMs. Our method leverages Federated Averaging (FedAvg) to aggregate\npreference updates efficiently, achieving 46% faster convergence, a 4%\nimprovement in alignment scores, and nearly the same group fairness measure as\nin centralized training. Evaluated on a Q/A preference alignment task,\nPluralLLM demonstrates that federated preference learning offers a scalable and\nprivacy-preserving alternative for aligning LLMs with diverse human values.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 00:45:27 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Srewa", "Mahmoud", ""], ["Zhao", "Tianyu", ""], ["Elmalaki", "Salma", ""]], "extracted_entities": [{"text": "Large Language Models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "privacy and fairness", "label": "Model Bias and Fairness"}, {"text": "Reinforcement Learning from Human Feedback", "label": "Few-shot Learning"}, {"text": "PluralLLM", "label": "LLM-based"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "PluralLLM", "label": "LLM-based"}, {"text": "federated preference learning", "label": "Few-shot Learning"}, {"text": "LLMs", "label": "Large Language Model"}]}
{"id": "2503.09927", "submitter": "Julia Ive", "authors": "Julia Ive and Olatomiwa Olukoya and Jonathan P. Funnell and James\n  Booker and Sze H M Lam and Ugan Reddy and Kawsar Noor and Richard JB Dobson\n  and Astri M.V. Luoma and Hani J Marcus", "title": "Developing and Evaluating an AI-Assisted Prediction Model for Unplanned\n  Intensive Care Admissions following Elective Neurosurgery using Natural\n  Language Processing within an Electronic Healthcare Record System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction: Timely care in a specialised neuro-intensive therapy unit (ITU)\nreduces mortality and hospital stays, with planned admissions being safer than\nunplanned ones. However, post-operative care decisions remain subjective. This\nstudy used artificial intelligence (AI), specifically natural language\nprocessing (NLP) to analyse electronic health records (EHRs) and predict ITU\nadmissions for elective surgery patients. Methods: This study analysed the EHRs\nof elective neurosurgery patients from University College London Hospital\n(UCLH) using NLP. Patients were categorised into planned high dependency unit\n(HDU) or ITU admission; unplanned HDU or ITU admission; or ward / overnight\nrecovery (ONR). The Medical Concept Annotation Tool (MedCAT) was used to\nidentify SNOMED-CT concepts within the clinical notes. We then explored the\nutility of these identified concepts for a range of AI algorithms trained to\npredict ITU admission. Results: The CogStack-MedCAT NLP model, initially\ntrained on hospital-wide EHRs, underwent two refinements: first with data from\npatients with Normal Pressure Hydrocephalus (NPH) and then with data from\nVestibular Schwannoma (VS) patients, achieving a concept detection F1-score of\n0.93. This refined model was then used to extract concepts from EHR notes of\n2,268 eligible neurosurgical patients. We integrated the extracted concepts\ninto AI models, including a decision tree model and a neural time-series model.\nUsing the simpler decision tree model, we achieved a recall of 0.87 (CI 0.82 -\n0.91) for ITU admissions, reducing the proportion of unplanned ITU cases missed\nby human experts from 36% to 4%. Conclusion: The NLP model, refined for\naccuracy, has proven its efficiency in extracting relevant concepts, providing\na reliable basis for predictive AI models to use in clinically valid\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 00:48:48 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Ive", "Julia", ""], ["Olukoya", "Olatomiwa", ""], ["Funnell", "Jonathan P.", ""], ["Booker", "James", ""], ["Lam", "Sze H M", ""], ["Reddy", "Ugan", ""], ["Noor", "Kawsar", ""], ["Dobson", "Richard JB", ""], ["Luoma", "Astri M. V.", ""], ["Marcus", "Hani J", ""]], "extracted_entities": [{"text": "decision tree model", "label": "Neural Language Model"}, {"text": "neural time-series model", "label": "Neural Language Model"}, {"text": "decision tree model", "label": "Neural Language Model"}]}
{"id": "2503.09928", "submitter": "Elden Elmanto", "authors": "Elden Elmanto, Dmitry Kubrak, Vladimir Sosnilo", "title": "On filtered algebraic $K$-theory of stacks I: characteristic zero", "comments": "75 pages, comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG math.AT math.KT math.RT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a compact Lie group $G$ acting on a space $X$, the classical\nAtiyah-Segal completion theorem identifies topological $K$-theory of the\nhomotopy quotient $X/G$ with an explicit completion of $G$-equivariant\ntopological $K$-theory of $X$. We prove an analog of this result for algebraic\n$K$-theory over a field of characteristic 0. In our setting $G$ is a reductive\ngroup that acts on a derived algebraic space $X$ with the assumption that all\nstabilizer groups are nice (in the sense of Alper). Our main result identifies\nthe value $R^{\\mathrm{dAff}}K([X/G])$ of right Kan extension of the $K$-theory\nfunctor from schemes to stacks with the completion of $K$-theory of the\ncategory $\\mathrm{Perf}([X/G])$ at the augmentation ideal of\n$K_0(\\mathrm{Rep}(G))$. The main novelty of our results is that $X$ is allowed\nto be singular or even derived. This generality is achieved by employing and\nimproving analogous versions of completion theorem for negative cyclic homology\n(after Ben-Zvi--Nadler and Chen) and for homotopy $K$-theory (after van den\nBergh--Tabuada). We also show that in the singular setting the completion\ntheorem does not necessarily hold without the nice stabilizer assumption. We\nview our results as a part of the general paradigm of extending the motivic\nfiltration on algebraic $K$-theory of schemes to algebraic $K$-theory of\nstacks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 01:00:16 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Elmanto", "Elden", ""], ["Kubrak", "Dmitry", ""], ["Sosnilo", "Vladimir", ""]], "extracted_entities": [{"text": "Alper", "label": "ALBERT"}]}
{"id": "2503.09929", "submitter": "Weiwei Zhou", "authors": "Weiwei Zhou, Chenkun Ling, Zefeng Cai", "title": "Emotion Recognition with CLIP and Sequential Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human emotion recognition plays a crucial role in facilitating seamless\ninteractions between humans and computers. In this paper, we present our\ninnovative methodology for tackling the Valence-Arousal (VA) Estimation\nChallenge, the Expression Recognition Challenge, and the Action Unit (AU)\nDetection Challenge, all within the framework of the 8th Workshop and\nCompetition on Affective Behavior Analysis in-the-wild (ABAW).\n  Our approach introduces a novel framework aimed at enhancing continuous\nemotion recognition. This is achieved by fine-tuning the CLIP model with the\naff-wild2 dataset, which provides annotated expression labels. The result is a\nfine-tuned model that serves as an efficient visual feature extractor,\nsignificantly improving its robustness. To further boost the performance of\ncontinuous emotion recognition, we incorporate Temporal Convolutional Network\n(TCN) modules alongside Transformer Encoder modules into our system\narchitecture. The integration of these advanced components allows our model to\noutperform baseline performance, demonstrating its ability to recognize human\nemotions with greater accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 01:02:06 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhou", "Weiwei", ""], ["Ling", "Chenkun", ""], ["Cai", "Zefeng", ""]], "extracted_entities": [{"text": "fine-tuning", "label": "Fine-tuning"}, {"text": "Transformer Encoder modules", "label": "Transformers"}]}
{"id": "2503.09933", "submitter": "Lueling Jia", "authors": "Lixiu Wang, Lueling Jia, Zijian Cao, Huiyuan Li, Zhimin Zhang", "title": "Fast Maxwell Solvers Based on Exact Discrete Eigen-Decompositions I.\n  Two-Dimensional Case", "comments": "24 pages, 7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose fast solvers for Maxwell's equations in rectangular\ndomains. We first discretize the simplified Maxwell's eigenvalue problems by\nemploying the lowest-order rectangular N\\'ed\\'elec elements and derive the\ndiscrete eigen-solutions explicitly, providing a Hodge-Helmholtz decomposition\nframework at the discrete level. Based on exact eigen-decompositions, we\nfurther design fast solvers for various Maxwell's source problems, guaranteeing\neither the divergence-free constraint or the Gauss's law at the discrete level.\nWith the help of fast sine/cosine transforms, the computational time grows\nasymptotically as $\\mathcal{O}(n^2\\log n)$ with $n$ being the number of grids\nin each direction. Our fast Maxwell solvers outperform other existing Maxwell\nsolvers in the literature and fully rival fast scalar Poisson/Helmholtz solvers\nbased on trigonometric transforms in either efficiency, robustness, or storage\ncomplexity. It is also utilized to perform an efficient pre-conditioning for\nsolving Maxwell's source problems with variable coefficients. Finally,\nnumerical experiments are carried out to illustrate the effectiveness and\nefficiency of the proposed fast solver.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 01:10:48 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Lixiu", ""], ["Jia", "Lueling", ""], ["Cao", "Zijian", ""], ["Li", "Huiyuan", ""], ["Zhang", "Zhimin", ""]], "extracted_entities": [{"text": "divergence-free constraint", "label": "Scaling law"}, {"text": "Gauss's law", "label": "Scaling law"}]}
{"id": "2503.09935", "submitter": "Fabricio M. Souza", "authors": "E. M. Fernandes, L. Sanz and F. M. Souza", "title": "Quantum Entanglement Response to Step-like Gate Modulation", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the influence of a step-like gate voltage on the entanglement\nformation of two interacting charge qubits, where charge is injected on demand\ninto the qubits. The gate voltage modulates the tunnel coupling between the\nqubits and two electronic reservoirs (leads), which supply the initial charges\nto the system. The qubits interact capacitively through Coulomb repulsion, and\nthe interplay between Coulomb interactions and hopping processes leads to the\nformation of entangled states. Our analysis focuses on how the physical\nparameters of the gate pulse affect the degree of entanglement. In pursuit of\nthis aim, we calculate fidelity, linear entropy, and negativity within the\nframework of density matrix formalism. Our analysis demonstrate how to optimize\nthe gate pulse to reach a ``sweet spot'' that maximizes entanglement, even in\nthe presence of additional dephasing sources. These results could contribute to\nthe future experimental realization of entanglement in interacting charge\nqubits.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 01:13:27 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Fernandes", "E. M.", ""], ["Sanz", "L.", ""], ["Souza", "F. M.", ""]], "extracted_entities": [{"text": "fidelity", "label": "quantisation"}, {"text": "linear entropy", "label": "quantisation"}]}
{"id": "2503.09938", "submitter": "Dongliang Zhou", "authors": "Sen Wang, Dongliang Zhou, Liang Xie, Chao Xu, Ye Yan, Erwei Yin", "title": "PanoGen++: Domain-Adapted Text-Guided Panoramic Environment Generation\n  for Vision-and-Language Navigation", "comments": "This paper was accepted by Neural Networks", "journal-ref": null, "doi": "10.1016/j.neunet.2025.107320", "report-no": null, "categories": "cs.CV cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-and-language navigation (VLN) tasks require agents to navigate\nthree-dimensional environments guided by natural language instructions,\noffering substantial potential for diverse applications. However, the scarcity\nof training data impedes progress in this field. This paper introduces\nPanoGen++, a novel framework that addresses this limitation by generating\nvaried and pertinent panoramic environments for VLN tasks. PanoGen++\nincorporates pre-trained diffusion models with domain-specific fine-tuning,\nemploying parameter-efficient techniques such as low-rank adaptation to\nminimize computational costs. We investigate two settings for environment\ngeneration: masked image inpainting and recursive image outpainting. The former\nmaximizes novel environment creation by inpainting masked regions based on\ntextual descriptions, while the latter facilitates agents' learning of spatial\nrelationships within panoramas. Empirical evaluations on room-to-room (R2R),\nroom-for-room (R4R), and cooperative vision-and-dialog navigation (CVDN)\ndatasets reveal significant performance enhancements: a 2.44% increase in\nsuccess rate on the R2R test leaderboard, a 0.63% improvement on the R4R\nvalidation unseen set, and a 0.75-meter enhancement in goal progress on the\nCVDN validation unseen set. PanoGen++ augments the diversity and relevance of\ntraining environments, resulting in improved generalization and efficacy in VLN\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 01:16:58 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Sen", ""], ["Zhou", "Dongliang", ""], ["Xie", "Liang", ""], ["Xu", "Chao", ""], ["Yan", "Ye", ""], ["Yin", "Erwei", ""]], "extracted_entities": [{"text": "PanoGen++", "label": "Generative Pre-trained Transformer (GPT)"}, {"text": "PanoGen++", "label": "Generative Pre-trained Transformer (GPT)"}, {"text": "domain-specific fine-tuning", "label": "Fine-tuning"}, {"text": "low-rank adaptation", "label": "Fine-tuning"}]}
{"id": "2503.09940", "submitter": "Kejin Wei", "authors": "Xitao Ji, Wenjie He, Junda Chen, Mingming Zhang, Yuqi Li, Ziwen Zhou,\n  Zhuoxuan Song, Hao Wu, Siqi Yan, Kejin Wei, Zhenrong Zhang, Shuang Wang, Ming\n  Tang", "title": "Quantum-Secured DSP-Lite Data Transmission Architectures for AI-Driven\n  Data Centres", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial intelligence-driven (AI-driven) data centres, which require\nhigh-performance, scalable, energy-efficient, and secure infrastructure, have\nled to unprecedented data traffic demands. These demands involve low latency,\nhigh bandwidth connections, low power consumption, and data confidentiality.\nHowever, conventional optical interconnect solutions, such as\nintensity-modulated direct detection and traditional coherent systems, cannot\naddress these requirements simultaneously. In particular, conventional\nencryption protocols that rely on complex algorithms are increasingly\nvulnerable to the rapid advancement of quantum computing. Here, we propose and\ndemonstrate a quantum-secured digital signal processing-lite (DSP-Lite) data\ntransmission architecture that meets all the stringent requirements for\nAI-driven data centre optical interconnects (AI-DCIs) scenarios. By integrating\na self-homodyne coherent (SHC) system and quantum key distribution (QKD)\nthrough the multicore-fibre-based space division multiplexing (SDM) technology,\nour scheme enables secure, high-capacity, and energy-efficient data\ntransmission while ensuring resilience against quantum computing threats. In\nour demonstration, we achieved an expandable transmission capacity of 2 Tbit\nper second (Tb/s) and a quantum secret key rate (SKR) of 229.2 kb/s, with a\nquantum bit error rate (QBER) of approximately 1.27% and with ultralow power\nconsumption. Our work paves the way for constructing secure, scalable, and\ncost-efficient data transmission frameworks, thus enabling the next generation\nof intelligent, leak-proof optical interconnects for data centres.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 01:25:46 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Ji", "Xitao", ""], ["He", "Wenjie", ""], ["Chen", "Junda", ""], ["Zhang", "Mingming", ""], ["Li", "Yuqi", ""], ["Zhou", "Ziwen", ""], ["Song", "Zhuoxuan", ""], ["Wu", "Hao", ""], ["Yan", "Siqi", ""], ["Wei", "Kejin", ""], ["Zhang", "Zhenrong", ""], ["Wang", "Shuang", ""], ["Tang", "Ming", ""]], "extracted_entities": [{"text": "quantum computing", "label": "quantisation"}]}
{"id": "2503.09942", "submitter": "Yasheng Sun", "authors": "Yasheng Sun, Zhiliang Xu, Hang Zhou, Jiazhi Guan, Quanwei Yang,\n  Kaisiyuan Wang, Borong Liang, Yingying Li, Haocheng Feng, Jingdong Wang,\n  Ziwei Liu, Koike Hideki", "title": "Cosh-DiT: Co-Speech Gesture Video Synthesis via Hybrid Audio-Visual\n  Diffusion Transformers", "comments": "Project Page: https://sunyasheng.github.io/projects/COSH-DIT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Co-speech gesture video synthesis is a challenging task that requires both\nprobabilistic modeling of human gestures and the synthesis of realistic images\nthat align with the rhythmic nuances of speech. To address these challenges, we\npropose Cosh-DiT, a Co-speech gesture video system with hybrid Diffusion\nTransformers that perform audio-to-motion and motion-to-video synthesis using\ndiscrete and continuous diffusion modeling, respectively. First, we introduce\nan audio Diffusion Transformer (Cosh-DiT-A) to synthesize expressive gesture\ndynamics synchronized with speech rhythms. To capture upper body, facial, and\nhand movement priors, we employ vector-quantized variational autoencoders\n(VQ-VAEs) to jointly learn their dependencies within a discrete latent space.\nThen, for realistic video synthesis conditioned on the generated speech-driven\nmotion, we design a visual Diffusion Transformer (Cosh-DiT-V) that effectively\nintegrates spatial and temporal contexts. Extensive experiments demonstrate\nthat our framework consistently generates lifelike videos with expressive\nfacial expressions and natural, smooth gestures that align seamlessly with\nspeech.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 01:36:05 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Sun", "Yasheng", ""], ["Xu", "Zhiliang", ""], ["Zhou", "Hang", ""], ["Guan", "Jiazhi", ""], ["Yang", "Quanwei", ""], ["Wang", "Kaisiyuan", ""], ["Liang", "Borong", ""], ["Li", "Yingying", ""], ["Feng", "Haocheng", ""], ["Wang", "Jingdong", ""], ["Liu", "Ziwei", ""], ["Hideki", "Koike", ""]], "extracted_entities": [{"text": "Diffusion\nTransformers", "label": "Transformers"}]}
{"id": "2503.09947", "submitter": "Xiaobo Xia", "authors": "Xiaobo Xia, Xiaofeng Liu, Jiale Liu, Kuai Fang, Lu Lu, Samet Oymak,\n  William S. Currie, Tongliang Liu", "title": "Identifying Trustworthiness Challenges in Deep Learning Models for\n  Continental-Scale Water Quality Prediction", "comments": "33 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Water quality is foundational to environmental sustainability, ecosystem\nresilience, and public health. Deep learning models, particularly Long\nShort-Term Memory (LSTM) networks, offer transformative potential for\nlarge-scale water quality prediction and scientific insights generation.\nHowever, their widespread adoption in high-stakes decision-making, such as\npollution mitigation and equitable resource allocation, is prevented by\nunresolved trustworthiness challenges including fairness, uncertainty,\ninterpretability, robustness, generalizability, and reproducibility. In this\nwork, we present the first comprehensive evaluation of trustworthiness in a\ncontinental-scale multi-task LSTM model predicting 20 water quality variables\n(encompassing physical/chemical processes, geochemical weathering, and nutrient\ncycling) across 482 U.S. basins. Our investigation uncovers systematic patterns\nof model performance disparities linked to basin characteristics, the inherent\ncomplexity of biogeochemical processes, and variable predictability,\nemphasizing critical performance fairness concerns. We further propose\nmethodological frameworks for quantitatively evaluating critical aspects of\ntrustworthiness, including uncertainty, interpretability, and robustness,\nidentifying key limitations that could challenge reliable real-world\ndeployment. This work serves as a timely call to action for advancing\ntrustworthy data-driven methods for water resources management and provides a\npathway to offering critical insights for researchers, decision-makers, and\npractitioners seeking to leverage artificial intelligence (AI) responsibly in\nenvironmental management.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 01:50:50 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Xia", "Xiaobo", ""], ["Liu", "Xiaofeng", ""], ["Liu", "Jiale", ""], ["Fang", "Kuai", ""], ["Lu", "Lu", ""], ["Oymak", "Samet", ""], ["Currie", "William S.", ""], ["Liu", "Tongliang", ""]], "extracted_entities": [{"text": "fairness", "label": "Model Bias and Fairness"}, {"text": "uncertainty", "label": "Model Bias and Fairness"}, {"text": "interpretability", "label": "Model Bias and Fairness"}, {"text": "robustness", "label": "Model Bias and Fairness"}, {"text": "fairness", "label": "Model Bias and Fairness"}, {"text": "uncertainty", "label": "Model Bias and Fairness"}, {"text": "interpretability", "label": "Model Bias and Fairness"}, {"text": "robustness", "label": "Model Bias and Fairness"}]}
{"id": "2503.09949", "submitter": "Yuanxin Liu", "authors": "Yuanxin Liu, Rui Zhu, Shuhuai Ren, Jiacong Wang, Haoyuan Guo, Xu Sun,\n  Lu Jiang", "title": "UVE: Are MLLMs Unified Evaluators for AI-Generated Videos?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With the rapid growth of video generative models (VGMs), it is essential to\ndevelop reliable and comprehensive automatic metrics for AI-generated videos\n(AIGVs). Existing methods either use off-the-shelf models optimized for other\ntasks or rely on human assessment data to train specialized evaluators. These\napproaches are constrained to specific evaluation aspects and are difficult to\nscale with the increasing demands for finer-grained and more comprehensive\nevaluations. To address this issue, this work investigates the feasibility of\nusing multimodal large language models (MLLMs) as a unified evaluator for\nAIGVs, leveraging their strong visual perception and language understanding\ncapabilities. To evaluate the performance of automatic metrics in unified AIGV\nevaluation, we introduce a benchmark called UVE-Bench. UVE-Bench collects\nvideos generated by state-of-the-art VGMs and provides pairwise human\npreference annotations across 15 evaluation aspects. Using UVE-Bench, we\nextensively evaluate 16 MLLMs. Our empirical results suggest that while\nadvanced MLLMs (e.g., Qwen2VL-72B and InternVL2.5-78B) still lag behind human\nevaluators, they demonstrate promising ability in unified AIGV evaluation,\nsignificantly surpassing existing specialized evaluation methods. Additionally,\nwe conduct an in-depth analysis of key design choices that impact the\nperformance of MLLM-driven evaluators, offering valuable insights for future\nresearch on AIGV evaluation. The code is available at\nhttps://github.com/bytedance/UVE.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 01:52:27 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Liu", "Yuanxin", ""], ["Zhu", "Rui", ""], ["Ren", "Shuhuai", ""], ["Wang", "Jiacong", ""], ["Guo", "Haoyuan", ""], ["Sun", "Xu", ""], ["Jiang", "Lu", ""]], "extracted_entities": [{"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}]}
{"id": "2503.09950", "submitter": "Yuxiang Fu", "authors": "Yuxiang Fu, Qi Yan, Lele Wang, Ke Li, Renjie Liao", "title": "MoFlow: One-Step Flow Matching for Human Trajectory Forecasting via\n  Implicit Maximum Likelihood Estimation based Distillation", "comments": "Accepted to CVPR 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of human trajectory forecasting, which\naims to predict the inherently multi-modal future movements of humans based on\ntheir past trajectories and other contextual cues. We propose a novel motion\nprediction conditional flow matching model, termed MoFlow, to predict K-shot\nfuture trajectories for all agents in a given scene. We design a novel flow\nmatching loss function that not only ensures at least one of the $K$ sets of\nfuture trajectories is accurate but also encourages all $K$ sets of future\ntrajectories to be diverse and plausible. Furthermore, by leveraging the\nimplicit maximum likelihood estimation (IMLE), we propose a novel distillation\nmethod for flow models that only requires samples from the teacher model.\nExtensive experiments on the real-world datasets, including SportVU NBA games,\nETH-UCY, and SDD, demonstrate that both our teacher flow model and the\nIMLE-distilled student model achieve state-of-the-art performance. These models\ncan generate diverse trajectories that are physically and socially plausible.\nMoreover, our one-step student model is $\\textbf{100}$ times faster than the\nteacher flow model during sampling. The code, model, and data are available at\nour project page: https://moflow-imle.github.io\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 01:53:05 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Fu", "Yuxiang", ""], ["Yan", "Qi", ""], ["Wang", "Lele", ""], ["Li", "Ke", ""], ["Liao", "Renjie", ""]], "extracted_entities": [{"text": "teacher model", "label": "AI model"}, {"text": "teacher flow model", "label": "AI model"}]}
{"id": "2503.09951", "submitter": "Xinglong Sun", "authors": "Xinglong Sun and Haijiang Sun and Shan Jiang and Jiacheng Wang and\n  Jiasong Wang", "title": "Target-aware Bidirectional Fusion Transformer for Aerial Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The trackers based on lightweight neural networks have achieved great success\nin the field of aerial remote sensing, most of which aggregate multi-stage deep\nfeatures to lift the tracking quality. However, existing algorithms usually\nonly generate single-stage fusion features for state decision, which ignore\nthat diverse kinds of features are required for identifying and locating the\nobject, limiting the robustness and precision of tracking. In this paper, we\npropose a novel target-aware Bidirectional Fusion transformer (BFTrans) for UAV\ntracking. Specifically, we first present a two-stream fusion network based on\nlinear self and cross attentions, which can combine the shallow and the deep\nfeatures from both forward and backward directions, providing the adjusted\nlocal details for location and global semantics for recognition. Besides, a\ntarget-aware positional encoding strategy is designed for the above fusion\nmodel, which is helpful to perceive the object-related attributes during the\nfusion phase. Finally, the proposed method is evaluated on several popular UAV\nbenchmarks, including UAV-123, UAV20L and UAVTrack112. Massive experimental\nresults demonstrate that our approach can exceed other state-of-the-art\ntrackers and run with an average speed of 30.5 FPS on embedded platform, which\nis appropriate for practical drone deployments.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 01:53:29 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Sun", "Xinglong", ""], ["Sun", "Haijiang", ""], ["Jiang", "Shan", ""], ["Wang", "Jiacheng", ""], ["Wang", "Jiasong", ""]], "extracted_entities": [{"text": "cross attentions", "label": "Attention mechanism"}]}
{"id": "2503.09956", "submitter": "Loc Nguyen", "authors": "Yu Qiao, Phuong-Nam Tran, Ji Su Yoon, Loc X. Nguyen, and Choong Seon\n  Hong", "title": "Exploring Mutual Empowerment Between Wireless Networks and RL-based\n  LLMs: A Survey", "comments": "25 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL)-based large language models (LLMs), such as\nChatGPT, DeepSeek, and Grok-3, have gained significant attention for their\nexceptional capabilities in natural language processing and multimodal data\nunderstanding. Meanwhile, the rapid expansion of information services has\ndriven the growing need for intelligence, efficient, and adaptable wireless\nnetworks. Wireless networks require the empowerment of RL-based LLMs while\nthese models also benefit from wireless networks to broaden their application\nscenarios. Specifically, RL-based LLMs can enhance wireless communication\nsystems through intelligent resource allocation, adaptive network optimization,\nand real-time decision-making. Conversely, wireless networks provide a vital\ninfrastructure for the efficient training, deployment, and distributed\ninference of RL-based LLMs, especially in decentralized and edge computing\nenvironments. This mutual empowerment highlights the need for a deeper\nexploration of the interplay between these two domains. We first review recent\nadvancements in wireless communications, highlighting the associated challenges\nand potential solutions. We then discuss the progress of RL-based LLMs,\nfocusing on key technologies for LLM training, challenges, and potential\nsolutions. Subsequently, we explore the mutual empowerment between these two\nfields, highlighting key motivations, open challenges, and potential solutions.\nFinally, we provide insights into future directions, applications, and their\nsocietal impact to further explore this intersection, paving the way for\nnext-generation intelligent communication systems. Overall, this survey\nprovides a comprehensive overview of the relationship between RL-based LLMs and\nwireless networks, offering a vision where these domains empower each other to\ndrive innovations.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 01:59:11 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Qiao", "Yu", ""], ["Tran", "Phuong-Nam", ""], ["Yoon", "Ji Su", ""], ["Nguyen", "Loc X.", ""], ["Hong", "Choong Seon", ""]], "extracted_entities": [{"text": "ChatGPT", "label": "ChatGPT"}, {"text": "DeepSeek", "label": "ChatGPT"}, {"text": "Grok-3", "label": "ChatGPT"}]}
{"id": "2503.09958", "submitter": "Zhenyu Liu", "authors": "Zhenyu Liu, Dongfang Li, Xinshuo Hu, Xinping Zhao, Yibin Chen, Baotian\n  Hu, Min Zhang", "title": "Take Off the Training Wheels Progressive In-Context Learning for\n  Effective Alignment", "comments": "15 pages, 9 figures, published in EMNLP2024", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent studies have explored the working mechanisms of In-Context Learning\n(ICL). However, they mainly focus on classification and simple generation\ntasks, limiting their broader application to more complex generation tasks in\npractice. To address this gap, we investigate the impact of demonstrations on\ntoken representations within the practical alignment tasks. We find that the\ntransformer embeds the task function learned from demonstrations into the\nseparator token representation, which plays an important role in the generation\nof prior response tokens. Once the prior response tokens are determined, the\ndemonstrations become redundant.Motivated by this finding, we propose an\nefficient Progressive In-Context Alignment (PICA) method consisting of two\nstages. In the first few-shot stage, the model generates several prior response\ntokens via standard ICL while concurrently extracting the ICL vector that\nstores the task function from the separator token representation. In the\nfollowing zero-shot stage, this ICL vector guides the model to generate\nresponses without further demonstrations.Extensive experiments demonstrate that\nour PICA not only surpasses vanilla ICL but also achieves comparable\nperformance to other alignment tuning methods. The proposed training-free\nmethod reduces the time cost (e.g., 5.45+) with improved alignment performance\n(e.g., 6.57+). Consequently, our work highlights the application of ICL for\nalignment and calls for a deeper understanding of ICL for complex generations.\nThe code will be available at https://github.com/HITsz-TMG/PICA.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 02:01:02 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Liu", "Zhenyu", ""], ["Li", "Dongfang", ""], ["Hu", "Xinshuo", ""], ["Zhao", "Xinping", ""], ["Chen", "Yibin", ""], ["Hu", "Baotian", ""], ["Zhang", "Min", ""]], "extracted_entities": [{"text": "In-Context Learning", "label": "Zero-shot Learning"}, {"text": "ICL", "label": "contextual Embedding"}, {"text": "ICL", "label": "contextual Embedding"}, {"text": "ICL", "label": "contextual Embedding"}, {"text": "ICL", "label": "contextual Embedding"}]}
{"id": "2503.09960", "submitter": "Muhammad Shahbaz Khan", "authors": "Muhammad Hassan Jamal, Abdulwahab Alazeb, Shahid Allah Bakhsh, Wadii\n  Boulila, Syed Aziz Shah, Aizaz Ahmad Khattak and Muhammad Shahbaz Khan", "title": "Optimizing Fire Safety: Reducing False Alarms Using Advanced Machine\n  Learning Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Fire safety practices are important to reduce the extent of destruction\ncaused by fire. While smoke alarms help save lives, firefighters struggle with\nthe increasing number of false alarms. This paper presents a precise and\nefficient Weighted ensemble model for decreasing false alarms. It estimates the\ndensity, computes weights according to the high and low-density regions,\nforwards the high region weights to KNN and low region weights to XGBoost and\ncombines the predictions. The proposed model is effective at reducing response\ntime, increasing fire safety, and minimizing the damage that fires cause. A\nspecifically designed dataset for smoke detection is utilized to test the\nproposed model. In addition, a variety of ML models, such as Logistic\nRegression (LR), Decision Tree (DT), Random Forest (RF), Nai:ve Bayes (NB),\nK-Nearest Neighbour (KNN), Support Vector Machine (SVM), Extreme Gradient\nBoosting (XGBoost), Adaptive Boosting (ADAB), have also been utilized. To\nmaximize the use of the smoke detection dataset, all the algorithms utilize the\nSMOTE re-sampling technique. After evaluating the assessment criteria, this\npaper presents a concise summary of the comprehensive findings obtained by\ncomparing the outcomes of all models.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 02:07:14 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Jamal", "Muhammad Hassan", ""], ["Alazeb", "Abdulwahab", ""], ["Bakhsh", "Shahid Allah", ""], ["Boulila", "Wadii", ""], ["Shah", "Syed Aziz", ""], ["Khattak", "Aizaz Ahmad", ""], ["Khan", "Muhammad Shahbaz", ""]], "extracted_entities": [{"text": "Extreme Gradient\nBoosting", "label": "Few-shot Learning"}]}
{"id": "2503.09961", "submitter": "Xin Zhu", "authors": "Xin Zhu, Hongyi Pan, Ahmet Enis Cetin", "title": "Edge-Fog Computing-Enabled EEG Data Compression via Asymmetrical\n  Variational Discrete Cosine Transform Network", "comments": "Accepted by the IEEE Internet of Things Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The large volume of electroencephalograph (EEG) data produced by\nbrain-computer interface (BCI) systems presents challenges for rapid\ntransmission over bandwidth-limited channels in Internet of Things (IoT)\nnetworks. To address the issue, we propose a novel multi-channel asymmetrical\nvariational discrete cosine transform (DCT) network for EEG data compression\nwithin an edge-fog computing framework. At the edge level, low-complexity DCT\ncompression units are designed using parallel trainable hard-thresholding and\nscaling operators to remove redundant data and extract the effective latent\nspace representation. At the fog level, an adaptive filter bank is applied to\nmerge important features from adjacent channels into each individual channel by\nleveraging inter-channel correlations. Then, the inverse DCT reconstructed\nmulti-head attention is developed to capture both local and global dependencies\nand reconstruct the original signals. Furthermore, by applying the principles\nof variational inference, a new evidence lower bound is formulated as the loss\nfunction, driving the model to balance compression efficiency and\nreconstruction accuracy. Experimental results on two public datasets\ndemonstrate that the proposed method achieves superior compression performance\nwithout sacrificing any useful information for BCI detection compared with\nstate-of-the-art techniques, indicating a feasible solution for EEG data\ncompression.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 02:07:17 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhu", "Xin", ""], ["Pan", "Hongyi", ""], ["Cetin", "Ahmet Enis", ""]], "extracted_entities": [{"text": "multi-head attention", "label": "Attention mechanism"}]}
{"id": "2503.09962", "submitter": "Jiayu Jiang", "authors": "Jiayu Jiang, Changxing Ding, Wentao Tan, Junhong Wang, Jin Tao,\n  Xiangmin Xu", "title": "Modeling Thousands of Human Annotators for Generalizable Text-to-Image\n  Person Re-identification", "comments": "CVPR 2025. Project website: https://github.com/sssaury/HAM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Text-to-image person re-identification (ReID) aims to retrieve the images of\nan interested person based on textual descriptions. One main challenge for this\ntask is the high cost in manually annotating large-scale databases, which\naffects the generalization ability of ReID models. Recent works handle this\nproblem by leveraging Multi-modal Large Language Models (MLLMs) to describe\npedestrian images automatically. However, the captions produced by MLLMs lack\ndiversity in description styles. To address this issue, we propose a Human\nAnnotator Modeling (HAM) approach to enable MLLMs to mimic the description\nstyles of thousands of human annotators. Specifically, we first extract style\nfeatures from human textual descriptions and perform clustering on them. This\nallows us to group textual descriptions with similar styles into the same\ncluster. Then, we employ a prompt to represent each of these clusters and apply\nprompt learning to mimic the description styles of different human annotators.\nFurthermore, we define a style feature space and perform uniform sampling in\nthis space to obtain more diverse clustering prototypes, which further enriches\nthe diversity of the MLLM-generated captions. Finally, we adopt HAM to\nautomatically annotate a massive-scale database for text-to-image ReID.\nExtensive experiments on this database demonstrate that it significantly\nimproves the generalization ability of ReID models.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 02:08:27 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Jiang", "Jiayu", ""], ["Ding", "Changxing", ""], ["Tan", "Wentao", ""], ["Wang", "Junhong", ""], ["Tao", "Jin", ""], ["Xu", "Xiangmin", ""]], "extracted_entities": [{"text": "Multi-modal Large Language Models", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "prompt", "label": "Prompting"}]}
{"id": "2503.09964", "submitter": "Usman Naseem", "authors": "Bhavik Chandna, Mariam Aboujenane, Usman Naseem", "title": "ExtremeAIGC: Benchmarking LMM Vulnerability to AI-Generated Extremist\n  Content", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large Multimodal Models (LMMs) are increasingly vulnerable to AI-generated\nextremist content, including photorealistic images and text, which can be used\nto bypass safety mechanisms and generate harmful outputs. However, existing\ndatasets for evaluating LMM robustness offer limited exploration of extremist\ncontent, often lacking AI-generated images, diverse image generation models,\nand comprehensive coverage of historical events, which hinders a complete\nassessment of model vulnerabilities. To fill this gap, we introduce\nExtremeAIGC, a benchmark dataset and evaluation framework designed to assess\nLMM vulnerabilities against such content. ExtremeAIGC simulates real-world\nevents and malicious use cases by curating diverse text- and image-based\nexamples crafted using state-of-the-art image generation techniques. Our study\nreveals alarming weaknesses in LMMs, demonstrating that even cutting-edge\nsafety measures fail to prevent the generation of extremist material. We\nsystematically quantify the success rates of various attack strategies,\nexposing critical gaps in current defenses and emphasizing the need for more\nrobust mitigation strategies.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 02:10:29 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chandna", "Bhavik", ""], ["Aboujenane", "Mariam", ""], ["Naseem", "Usman", ""]], "extracted_entities": [{"text": "Large Multimodal Models", "label": "Large Language Model"}, {"text": "LMMs", "label": "Large Language Model"}, {"text": "LMMs", "label": "Large Language Model"}]}
{"id": "2503.09965", "submitter": "Min Ju", "authors": "Min Ju, Xuhao Wu, Hong Shen", "title": "Effects of symmetry energy on the properties of hadron-quark mixed phase\n  in hybrid stars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "nucl-th", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study how the nuclear symmetry energy slope affects the properties of\nhadron-quark pasta phases and their sizes in massive hybrid stars ($> 2\nM_{\\odot}$). We utilize the relativistic mean-field model with a\ndensity-dependent isovector coupling constant fitted by varying values of the\nsymmetry energy slope $L$ to describe the hadronic matter, while the quark\nmatter is described by a modified MIT bag model with vector interactions. We\nconstruct the hadron-quark phase transition using the energy minimization (EM)\nmethod and compare with the Gibbs construction (GC) in the hybrid star. We find\nthat the massive hybrid stars generally tend to have a hadron-quark pasta phase\ncore rather than a pure quark core for most values of $L$, except in some\nspecial cases where they have a small pure quark core. Furthermore, we show\nthat the mass and size of the pasta phase core are strongly influenced by the\nsymmetry energy. The radius associated with the onset of pasta phases is also\ninfluenced by the symmetry energy. Additionally, we also evaluate the effects\nof the surface tension $\\sigma$, the bag constant $B$, and the vector\ninteraction $G_{V}$.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 02:12:20 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Ju", "Min", ""], ["Wu", "Xuhao", ""], ["Shen", "Hong", ""]], "extracted_entities": [{"text": "relativistic mean-field model", "label": "AI model"}]}
{"id": "2503.09968", "submitter": "Zihao Zhang", "authors": "Zihao Zhang and Aming Wu and Yahong Han", "title": "Style Evolving along Chain-of-Thought for Unknown-Domain Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a task of Single-Domain Generalized Object Detection (Single-DGOD)\nis proposed, aiming to generalize a detector to multiple unknown domains never\nseen before during training. Due to the unavailability of target-domain data,\nsome methods leverage the multimodal capabilities of vision-language models,\nusing textual prompts to estimate cross-domain information, enhancing the\nmodel's generalization capability. These methods typically use a single textual\nprompt, often referred to as the one-step prompt method. However, when dealing\nwith complex styles such as the combination of rain and night, we observe that\nthe performance of the one-step prompt method tends to be relatively weak. The\nreason may be that many scenes incorporate not just a single style but a\ncombination of multiple styles. The one-step prompt method may not effectively\nsynthesize combined information involving various styles. To address this\nlimitation, we propose a new method, i.e., Style Evolving along\nChain-of-Thought, which aims to progressively integrate and expand style\ninformation along the chain of thought, enabling the continual evolution of\nstyles. Specifically, by progressively refining style descriptions and guiding\nthe diverse evolution of styles, this approach enables more accurate simulation\nof various style characteristics and helps the model gradually learn and adapt\nto subtle differences between styles. Additionally, it exposes the model to a\nbroader range of style features with different data distributions, thereby\nenhancing its generalization capability in unseen domains. The significant\nperformance gains over five adverse-weather scenarios and the Real to Art\nbenchmark demonstrate the superiorities of our method.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 02:14:10 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhang", "Zihao", ""], ["Wu", "Aming", ""], ["Han", "Yahong", ""]], "extracted_entities": [{"text": "textual prompts", "label": "Prompting"}, {"text": "one-step prompt method", "label": "Prompting"}, {"text": "one-step prompt method", "label": "Prompting"}, {"text": "Chain-of-Thought", "label": "Chain of thought"}, {"text": "chain of thought", "label": "Chain of thought"}]}
{"id": "2503.09969", "submitter": "Nathan Drenkow", "authors": "Nathan Drenkow and Mitchell Pavlak and Keith Harrigian and Ayah\n  Zirikly and Adarsh Subbaswamy and Mathias Unberath", "title": "Detecting Dataset Bias in Medical AI: A Generalized and\n  Modality-Agnostic Auditing Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data-driven AI is establishing itself at the center of evidence-based\nmedicine. However, reports of shortcomings and unexpected behavior are growing\ndue to AI's reliance on association-based learning. A major reason for this\nbehavior: latent bias in machine learning datasets can be amplified during\ntraining and/or hidden during testing. We present a data modality-agnostic\nauditing framework for generating targeted hypotheses about sources of bias\nwhich we refer to as Generalized Attribute Utility and Detectability-Induced\nbias Testing (G-AUDIT) for datasets. Our method examines the relationship\nbetween task-level annotations and data properties including protected\nattributes (e.g., race, age, sex) and environment and acquisition\ncharacteristics (e.g., clinical site, imaging protocols). G-AUDIT automatically\nquantifies the extent to which the observed data attributes may enable shortcut\nlearning, or in the case of testing data, hide predictions made based on\nspurious associations. We demonstrate the broad applicability and value of our\nmethod by analyzing large-scale medical datasets for three distinct modalities\nand learning tasks: skin lesion classification in images, stigmatizing language\nclassification in Electronic Health Records (EHR), and mortality prediction for\nICU tabular data. In each setting, G-AUDIT successfully identifies subtle\nbiases commonly overlooked by traditional qualitative methods that focus\nprimarily on social and ethical objectives, underscoring its practical value in\nexposing dataset-level risks and supporting the downstream development of\nreliable AI systems. Our method paves the way for achieving deeper\nunderstanding of machine learning datasets throughout the AI development\nlife-cycle from initial prototyping all the way to regulation, and creates\nopportunities to reduce model bias, enabling safer and more trustworthy AI\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 02:16:48 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Drenkow", "Nathan", ""], ["Pavlak", "Mitchell", ""], ["Harrigian", "Keith", ""], ["Zirikly", "Ayah", ""], ["Subbaswamy", "Adarsh", ""], ["Unberath", "Mathias", ""]], "extracted_entities": [{"text": "association-based learning", "label": "Few-shot Learning"}, {"text": "shortcut\nlearning", "label": "Few-shot Learning"}, {"text": "social and ethical objectives", "label": "AI Ethics"}]}
{"id": "2503.09970", "submitter": "Sachin Vaidya", "authors": "Sachin Vaidya, Andr\\'e Grossi Fonseca, Mark R. Hirsbrunner, Taylor L.\n  Hughes, Marin Solja\\v{c}i\\'c", "title": "Quantized crystalline-electromagnetic responses in insulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mes-hall", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce new classes of gapped topological phases characterized by\nquantized crystalline-electromagnetic responses, termed \"multipolar Chern\ninsulators\". These systems are characterized by nonsymmorphic momentum-space\nsymmetries and mirror symmetries, leading to quantization of momentum-weighted\nBerry curvature multipole moments. We construct lattice models for such phases\nand confirm their quantized responses through numerical calculations. These\nsystems exhibit bound charge and momentum densities at lattice and magnetic\ndefects, and currents induced by electric or time-varying strain fields. Our\nwork extends the classification of topological matter by uncovering novel\nsymmetry-protected topological phases with quantized responses.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 02:19:21 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Vaidya", "Sachin", ""], ["Fonseca", "Andr\u00e9 Grossi", ""], ["Hirsbrunner", "Mark R.", ""], ["Hughes", "Taylor L.", ""], ["Solja\u010di\u0107", "Marin", ""]], "extracted_entities": [{"text": "quantization", "label": "quantisation"}]}
{"id": "2503.09972", "submitter": "Sergi Elizalde", "authors": "Sergi Elizalde", "title": "A bijection for descent sets of permutations with only even and only odd\n  cycles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that, when $n$ is even, the number of permutations of\n$\\{1,2,\\dots,n\\}$ all of whose cycles have odd length equals the number of\nthose all of whose cycles have even length. Adin, Heged\\H{u}s and Roichman\nrecently found a surprising refinement of this identity. They showed that, for\nany fixed set $J$, the equality still holds when restricting to permutations\nwith descent set $J$ on one side, and permutations with ascent set $J$ on the\nother. Their proof uses generating functions for higher Lie characters. They\nalso deduce a version for odd $n$.\n  Here we give a bijective proof of their result. We first use known bijections\nto restate the identity in terms of multisets of necklaces, and then describe a\nnew weight-preserving bijection between words all of whose Lyndon factors have\nodd length and are distinct, and words all of whose Lyndon factors have even\nlength. We also show that the corresponding equality about Lyndon\nfactorizations has a short proof using generating functions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 02:20:15 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Elizalde", "Sergi", ""]], "extracted_entities": [{"text": "Adin", "label": "ALBERT"}, {"text": "Heged", "label": "ALBERT"}, {"text": "Roichman", "label": "RoBERTa"}]}
{"id": "2503.09975", "submitter": "Joonhyung Lee", "authors": "Joonhyung Lee, Shmulik Markovich-Golan, Daniel Ohayon, Yair Hanani,\n  Gunho Park, Byeongwook Kim, Asaf Karnieli, Uri Livne, Haihao Shen, Tai Huang,\n  Se Jung Kwon, Dongsoo Lee", "title": "Faster Inference of LLMs using FP8 on the Intel Gaudi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-precision data types are essential in modern neural networks during both\ntraining and inference as they enhance throughput and computational capacity by\nbetter exploiting available hardware resources. Despite the incorporation of\nFP8 in commercially available neural network accelerators, a comprehensive\nexposition of its underlying mechanisms, along with rigorous performance and\naccuracy evaluations, is still lacking. In this work, we contribute in three\nsignificant ways. First, we analyze the implementation details and quantization\noptions associated with FP8 for inference on the Intel Gaudi AI accelerator.\nSecond, we empirically quantify the throughput improvements afforded by the use\nof FP8 at both the operator level and in end-to-end scenarios. Third, we assess\nthe accuracy impact of various FP8 quantization methods. Our experimental\nresults indicate that the Intel Gaudi 2 accelerator consistently achieves high\ncomputational unit utilization, frequently exceeding 90\\% MFU, while incurring\nan accuracy degradation of less than 1\\%.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 02:21:39 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Lee", "Joonhyung", ""], ["Markovich-Golan", "Shmulik", ""], ["Ohayon", "Daniel", ""], ["Hanani", "Yair", ""], ["Park", "Gunho", ""], ["Kim", "Byeongwook", ""], ["Karnieli", "Asaf", ""], ["Livne", "Uri", ""], ["Shen", "Haihao", ""], ["Huang", "Tai", ""], ["Kwon", "Se Jung", ""], ["Lee", "Dongsoo", ""]], "extracted_entities": [{"text": "quantization\noptions", "label": "quantisation"}, {"text": "quantization methods", "label": "quantisation"}]}
{"id": "2503.09978", "submitter": "Jiacheng Xie", "authors": "Jiacheng Xie, Hua-Chieh Shao, Yunxiang Li, Shunyu Yan, Chenyang Shen,\n  Jing Wang, You Zhang", "title": "A Conditional Point Cloud Diffusion Model for Deformable Liver Motion\n  Tracking Via a Single Arbitrarily-Angled X-ray Projection", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable liver motion tracking using a single X-ray projection enables\nreal-time motion monitoring and treatment intervention. We introduce a\nconditional point cloud diffusion model-based framework for accurate and robust\nliver motion tracking from arbitrarily angled single X-ray projections\n(PCD-Liver), which estimates volumetric liver motion by solving deformable\nvector fields (DVFs) of a prior liver surface point cloud based on a single\nX-ray image. The model is patient-specific and consists of two main components:\na rigid alignment model to estimate the liver's overall shifts and a\nconditional point cloud diffusion model that further corrects for liver surface\ndeformations. Conditioned on motion-encoded features extracted from a single\nX-ray projection via a geometry-informed feature pooling layer, the diffusion\nmodel iteratively solves detailed liver surface DVFs in a projection\nangle-agnostic manner. The liver surface motion estimated by PCD-Liver serves\nas a boundary condition for a U-Net-based biomechanical model to infer internal\nliver motion and localize liver tumors. A dataset of ten liver cancer patients\nwas used for evaluation. The accuracy of liver point cloud motion estimation\nwas assessed using root mean square error (RMSE) and 95th-percentile Hausdorff\ndistance (HD95), while liver tumor localization error was quantified using\ncenter-of-mass error (COME). The mean (standard deviation) RMSE, HD95, and COME\nof the prior liver or tumor before motion estimation were 8.86(1.51) mm,\n10.88(2.56) mm, and 9.41(3.08) mm, respectively. After PCD-Liver motion\nestimation, the corresponding values improved to 3.59(0.28) mm, 4.29(0.62) mm,\nand 3.45(0.96) mm. Under highly noisy conditions, PCD-Liver maintained stable\nperformance. This study presents an accurate and robust framework for\ndeformable liver motion estimation and tumor localization in image-guided\nradiotherapy.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 02:27:26 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Xie", "Jiacheng", ""], ["Shao", "Hua-Chieh", ""], ["Li", "Yunxiang", ""], ["Yan", "Shunyu", ""], ["Shen", "Chenyang", ""], ["Wang", "Jing", ""], ["Zhang", "You", ""]], "extracted_entities": [{"text": "rigid alignment model", "label": "AI model"}, {"text": "conditional point cloud diffusion model", "label": "AI model"}]}
{"id": "2503.09984", "submitter": "Parisa Ahmadi Ghomroudi", "authors": "Alessandro Grecucci, Parisa Ahmadi Ghomroudi, Carmen Morawetz, Valerie\n  Lesk, Irene Messina", "title": "Increased GM-WM in a prefrontal network and decreased GM in the insula\n  and the precuneus are associated with reappraisal usage: A data fusion\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotion regulation plays a crucial role in mental health, and difficulties in\nregulating emotions can contribute to psychological disorders. While\nreappraisal and suppression are well-studied strategies, the combined\ncontributions of gray matter (GM) and white matter (WM) to these strategies\nremain unclear due to methodological limitations in previous studies. To\naddress this, we applied a data fusion approach using Parallel Independent\nComponent Analysis (Parallel ICA) to GM and WM MRI images from 165 individuals.\nParallel ICA identified two networks associated with reappraisal usage. Network\n1 included a large lateral and medial prefrontal cortical network, overlapping\nwith the default mode network (DMN) and adjacent WM regions. Higher reappraisal\nfrequency was associated with greater GM-WM density within this network, and\nthis network was negatively correlated with perceived stress. Network 2\nincluded the insula, precuneus, sub-gyral, and lingual gyri in its GM portion,\nshowing a negative association with reappraisal usage. The WM portion, adjacent\nto regions of the central executive network (CEN), was positively associated\nwith reappraisal usage. Regarding suppression, no significant network was\nassociated with this strategy. This study provides new insights into individual\ndifferences in reappraisal use, showing a positive association between\nreappraisal frequency and increased gray and white matter concentration in a\nlarge frontal network, including regions of the frontal DMN and the CEN.\nConversely, subcortical areas exhibited reduced gray and white concentration.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 02:49:15 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Grecucci", "Alessandro", ""], ["Ghomroudi", "Parisa Ahmadi", ""], ["Morawetz", "Carmen", ""], ["Lesk", "Valerie", ""], ["Messina", "Irene", ""]], "extracted_entities": [{"text": "sub-gyral", "label": "Mistral"}]}
{"id": "2503.09985", "submitter": "Jingkai Sun", "authors": "Qiang Zhang, Jiahang Cao, Jingkai Sun, Gang Han, Wen Zhao, Yijie Guo,\n  Renjing Xu", "title": "ES-Parkour: Advanced Robot Parkour with Bio-inspired Event Camera and\n  Spiking Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, quadruped robotics has advanced significantly, particularly\nin perception and motion control via reinforcement learning, enabling complex\nmotions in challenging environments. Visual sensors like depth cameras enhance\nstability and robustness but face limitations, such as low operating\nfrequencies relative to joint control and sensitivity to lighting, which hinder\noutdoor deployment. Additionally, deep neural networks in sensor and control\nsystems increase computational demands. To address these issues, we introduce\nspiking neural networks (SNNs) and event cameras to perform a challenging\nquadruped parkour task. Event cameras capture dynamic visual data, while SNNs\nefficiently process spike sequences, mimicking biological perception.\nExperimental results demonstrate that this approach significantly outperforms\ntraditional models, achieving excellent parkour performance with just 11.7% of\nthe energy consumption of an artificial neural network (ANN)-based model,\nyielding an 88.3% energy reduction. By integrating event cameras with SNNs, our\nwork advances robotic reinforcement learning and opens new possibilities for\napplications in demanding environments.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 02:50:19 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhang", "Qiang", ""], ["Cao", "Jiahang", ""], ["Sun", "Jingkai", ""], ["Han", "Gang", ""], ["Zhao", "Wen", ""], ["Guo", "Yijie", ""], ["Xu", "Renjing", ""]], "extracted_entities": [{"text": "reinforcement learning", "label": "Zero-shot Learning"}]}
{"id": "2503.09986", "submitter": "Ling Liang", "authors": "Rohan Bhatnagar, Ling Liang, Krish Patel, and Haizhao Yang", "title": "From Equations to Insights: Unraveling Symbolic Structures in PDEs with\n  LLMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the remarkable success of artificial intelligence (AI) across\ndiverse fields, the application of AI to solve scientific problems-often\nformulated as partial differential equations (PDEs)-has garnered increasing\nattention. While most existing research concentrates on theoretical properties\n(such as well-posedness, regularity, and continuity) of the solutions,\nalongside direct AI-driven methods for solving PDEs, the challenge of\nuncovering symbolic relationships within these equations remains largely\nunexplored. In this paper, we propose leveraging large language models (LLMs)\nto learn such symbolic relationships. Our results demonstrate that LLMs can\neffectively predict the operators involved in PDE solutions by utilizing the\nsymbolic information in the PDEs. Furthermore, we show that discovering these\nsymbolic relationships can substantially improve both the efficiency and\naccuracy of the finite expression method for finding analytical approximation\nof PDE solutions, delivering a fully interpretable solution pipeline. This work\nopens new avenues for understanding the symbolic structure of scientific\nproblems and advancing their solution processes.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 02:52:20 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Bhatnagar", "Rohan", ""], ["Liang", "Ling", ""], ["Patel", "Krish", ""], ["Yang", "Haizhao", ""]], "extracted_entities": [{"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "LLM"}]}
{"id": "2503.09987", "submitter": "Jie Li", "authors": "Jie Li, Anusha Withana, Alexandra Diening, Kai Kunze, Masahiko Inami", "title": "Beyond Human: Cognitive and Physical Augmentation through AI, Robotics,\n  and XR -- Opportunities and Risks", "comments": "Workshop at the Augmented Humans (AHs) International Conference 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As human augmentation technologies evolve, the convergence of AI, robotics,\nand extended reality (XR) is redefining human potential -- enhancing cognition,\nperception, and physical abilities. However, these advancements also introduce\nethical dilemmas, security risks, and concerns over loss of control. This\nworkshop explores both the transformative potential and the unintended\nconsequences of augmentation technologies. Bringing together experts from HCI,\nneuroscience, robotics, and ethics, we will examine real-world applications,\nemerging risks, and governance strategies for responsible augmentation. The\nsession will feature keynote talks and interactive discussions, addressing\ntopics such as AI-enhanced cognition, wearable robotics, neural interfaces, and\nXR-driven augmentation. By fostering multidisciplinary dialogue, this workshop\naims to generate actionable insights for responsible innovation, proposing\nethical frameworks to balance human empowerment with risk mitigation. We invite\nresearchers, practitioners, and industry leaders to contribute their\nperspectives and help shape the future of human augmentation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 02:53:11 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Jie", ""], ["Withana", "Anusha", ""], ["Diening", "Alexandra", ""], ["Kunze", "Kai", ""], ["Inami", "Masahiko", ""]], "extracted_entities": [{"text": "ethics", "label": "AI Ethics"}]}
{"id": "2503.09989", "submitter": "Liam L.H. Lau", "authors": "Liam L.H. Lau, Andreas Gleis, Daniel Kaplan, Premala Chandra, Piers\n  Coleman", "title": "Oscillate and Renormalize: Fast Phonons Reshape the Kondo Effect in Flat\n  Band Systems", "comments": "12+4 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.str-el cond-mat.mes-hall", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We examine the interplay between electron correlations and phonons in an\nAnderson-Holstein impurity model with an Einstein phonon. When the phonons are\nslow compared to charge fluctuations (frequency $\\omega_0 \\ll U/2$, the onsite\nCoulomb scale $U/2$), we demonstrate analytically that the expected\nphonon-mediated reduction of interactions is completely suppressed, even in the\nstrong coupling regime. This suppression arises from the oscillator's inability\nto respond to rapid charge fluctuations, manifested as a compensation effect\nbetween the polaronic cloud and the excited-state phonons associated with\nvalence fluctuations. We identify a novel frozen mixed valence phase, above a\nthreshold dimensionless electron-phonon coupling $\\alpha^*$ when the phonons\nare slow, where the static phonon cloud locks the impurity into specific\nvalence configurations, potentially explaining the puzzling coexistence of\nmixed valence behavior and insulating properties in materials like rust.\nConversely, when the phonon is fast ($\\omega_0 \\gtrsim U/2$), the system\nexhibits conventional polaronic behavior with renormalized onsite interactions\neffectively $U_{\\text{eff}}$ due to phonon mediated attraction, with additional\nsatellite features in the local spectral function due to phonon excitations.\nUsing numerical renormalization group (NRG) calculations, a fully dynamic\nrenormalization technique, we confirm these behaviors in both regimes. These\nfindings have important implications for strongly correlated systems where\nphonon energy scales may be comparable to the Coulomb scale, such as in twisted\nbilayer graphene, necessitating careful consideration of interaction\nrenormalizations in theoretical models.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 02:55:43 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Lau", "Liam L. H.", ""], ["Gleis", "Andreas", ""], ["Kaplan", "Daniel", ""], ["Chandra", "Premala", ""], ["Coleman", "Piers", ""]], "extracted_entities": [{"text": "Coulomb scale", "label": "Scaling law"}, {"text": "Coulomb scale", "label": "Scaling law"}]}
{"id": "2503.09994", "submitter": "Yunxiao Wang", "authors": "Yunxiao Wang, Meng Liu, Rui Shao, Haoyu Zhang, Bin Wen, Fan Yang,\n  Tingting Gao, Di Zhang, Liqiang Nie", "title": "TIME: Temporal-sensitive Multi-dimensional Instruction Tuning and\n  Benchmarking for Video-LLMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video large language models have achieved remarkable performance in tasks\nsuch as video question answering, however, their temporal understanding remains\nsuboptimal. To address this limitation, we curate a dedicated instruction\nfine-tuning dataset that focuses on enhancing temporal comprehension across\nfive key dimensions. In order to reduce reliance on costly temporal\nannotations, we introduce a multi-task prompt fine-tuning approach that\nseamlessly integrates temporal-sensitive tasks into existing instruction\ndatasets without requiring additional annotations. Furthermore, we develop a\nnovel benchmark for temporal-sensitive video understanding that not only fills\nthe gaps in dimension coverage left by existing benchmarks but also rigorously\nfilters out potential shortcuts, ensuring a more accurate evaluation. Extensive\nexperimental results demonstrate that our approach significantly enhances the\ntemporal understanding of video-LLMs while avoiding reliance on shortcuts.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 03:05:11 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Yunxiao", ""], ["Liu", "Meng", ""], ["Shao", "Rui", ""], ["Zhang", "Haoyu", ""], ["Wen", "Bin", ""], ["Yang", "Fan", ""], ["Gao", "Tingting", ""], ["Zhang", "Di", ""], ["Nie", "Liqiang", ""]], "extracted_entities": [{"text": "video-LLMs", "label": "LLMs"}]}
{"id": "2503.09997", "submitter": "Song He", "authors": "Song He, Yi Li, Hao Ouyang, Yuan Sun", "title": "$T\\overline{T}$ Deformation: Introduction and Some Recent Advances", "comments": "100 pages, 1 figure. Comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-th gr-qc math-ph math.MP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This review explores recent advances in the theory of $T\\bar{T}$ deformation,\nan irrelevant yet solvable deformation of quantum field theories defined via\nthe quadratic form of the energy-momentum tensor. It addresses classical and\nquantum aspects, highlighting significant developments across various fields,\nincluding field theory, holography, and string theory. Classically, $T\\bar{T}$\ndeformation manifests through multiple geometric interpretations, notably\nrandom geometry, Jackiw-Teitelboim-like gravity, and uniform light-cone gauge\nframeworks. For quantum aspects, the deformation introduces notable features\nsuch as non-locality, UV-IR mixing, solvable renormalization structures, and\nintriguing modifications to correlation functions and entanglement properties.\nFurthermore, the paper examines the profound relationship between $T\\bar{T}$\ndeformation and holography, particularly within the mixed boundary\nconditions/cutoff AdS holography proposal and holographic entanglement entropy.\nConnections to string theory through single-trace deformations and their\nholographic duals further reveal the deformed structure of the worldsheet. This\nreview synthesizes recent developments and outlines potential directions for\nfuture research in the study of $T\\bar{T}$-like deformation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 03:11:07 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["He", "Song", ""], ["Li", "Yi", ""], ["Ouyang", "Hao", ""], ["Sun", "Yuan", ""]], "extracted_entities": [{"text": "quantum aspects", "label": "quantisation"}, {"text": "non-locality", "label": "quantisation"}, {"text": "UV-IR mixing", "label": "quantisation"}, {"text": "solvable renormalization structures", "label": "quantisation"}]}
{"id": "2503.09998", "submitter": "Erik Garcia Neefjes", "authors": "Erik Garc\\'ia Neefjes and Stuart C. Hawkins", "title": "Far-Field Sensitivity to Local Boundary Perturbations in 2D Wave\n  Scattering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We numerically investigate the sensitivity of the scattered wave field to\nperturbations in the shape of a scattering body illuminated by an incident\nplane wave. This study is motivated by recent work on the inverse problem of\nreconstructing a scatterer shape from measurements of the scattered wave at\nlarge distances from the scatterer. For this purpose we consider star-shaped\nscatterers represented using cubic splines, and our approach is based on a\nNystr\\\"om method-based discretisation of the shape derivative. Using the\nsingular value decomposition, we identify fundamental geometric modes that most\nstrongly influence the scattered wave, providing insight into the most visible\nboundary features in scattering data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 03:11:17 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Neefjes", "Erik Garc\u00eda", ""], ["Hawkins", "Stuart C.", ""]], "extracted_entities": [{"text": "cubic splines", "label": "Embedding"}, {"text": "singular value decomposition", "label": "quantisation"}]}
{"id": "2503.10003", "submitter": "Dongjun Hwang", "authors": "Shiwon Kim, Dongjun Hwang, Sungwon Woo, Rita Singh", "title": "A New Benchmark for Few-Shot Class-Incremental Learning: Redefining the\n  Upper Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class-incremental learning (CIL) aims to continuously adapt to emerging\nclasses while retaining knowledge of previously learned ones. Few-shot\nclass-incremental learning (FSCIL) presents an even greater challenge which\nrequires the model to learn incremental classes with only a limited number of\nsamples. In conventional CIL, joint training is widely considered the upper\nbound, serving as both a benchmark and a methodological guide. However, we find\nthat joint training fails to be a meaningful upper bound in FSCIL due to the\ninherent difficulty of inter-task class separation (ICS) caused by severe class\nimbalance. In this work, we introduce a new joint training benchmark tailored\nfor FSCIL by integrating imbalance-aware techniques, effectively bridging the\nperformance gap between base and incremental classes. Furthermore, we point out\ninconsistencies in the experimental setup and evaluation of existing FSCIL\nmethods. To ensure fair comparisons between different FSCIL approaches and\njoint training, we standardize training conditions and propose a unified\nevaluation protocol that simultaneously considers the validation set and\ncomputational complexity. By establishing a reliable upper bound and a\nstandardized evaluation framework for FSCIL, our work provides a clear\nbenchmark and a practical foundation for future research.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 03:25:29 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Kim", "Shiwon", ""], ["Hwang", "Dongjun", ""], ["Woo", "Sungwon", ""], ["Singh", "Rita", ""]], "extracted_entities": [{"text": "Class-incremental learning", "label": "Few-shot Learning"}, {"text": "Few-shot\nclass-incremental learning", "label": "Few-shot Learning"}, {"text": "FSCIL", "label": "Few-shot Learning"}, {"text": "CIL", "label": "Zero-shot Learning"}, {"text": "joint training", "label": "Zero-shot Learning"}, {"text": "joint training", "label": "Zero-shot Learning"}, {"text": "FSCIL", "label": "Zero-shot Learning"}, {"text": "FSCIL", "label": "Few-shot Learning"}, {"text": "FSCIL", "label": "Few-shot Learning"}, {"text": "FSCIL", "label": "Few-shot Learning"}, {"text": "joint training", "label": "Zero-shot Learning"}, {"text": "FSCIL", "label": "Few-shot Learning"}]}
{"id": "2503.10005", "submitter": "Yongqi Li", "authors": "Yongqi Li, Xiaowei Zhang", "title": "Adaptive Moment Estimation Optimization Algorithm Using Projection\n  Gradient for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks is challenging. To accelerate training and\nenhance performance, we propose PadamP, a novel optimization algorithm. PadamP\nis derived by applying the adaptive estimation of the p-th power of the\nsecond-order moments under scale invariance, enhancing projection adaptability\nby modifying the projection discrimination condition. It is integrated into\nAdam-type algorithms, accelerating training, boosting performance, and\nimproving generalization in deep learning. Combining projected gradient\nbenefits with adaptive moment estimation, PadamP tackles unconstrained\nnon-convex problems. Convergence for the non-convex case is analyzed, focusing\non the decoupling of first-order moment estimation coefficients and\nsecond-order moment estimation coefficients. Unlike prior work relying on , our\nproof generalizes the convergence theorem, enhancing practicality. Experiments\nusing VGG-16 and ResNet-18 on CIFAR-10 and CIFAR-100 show PadamP's\neffectiveness, with notable performance on CIFAR-10/100, especially for VGG-16.\nThe results demonstrate that PadamP outperforms existing algorithms in terms of\nconvergence speed and generalization ability, making it a valuable addition to\nthe field of deep learning optimization.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 03:31:08 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Yongqi", ""], ["Zhang", "Xiaowei", ""]], "extracted_entities": [{"text": "scale invariance", "label": "Scaling law"}]}
{"id": "2503.10006", "submitter": "Mahmoud Abdelgalil", "authors": "Mahmoud Abdelgalil, Jorge I. Poveda", "title": "On Persistently Resetting Learning Integrators: A Framework For\n  Model-Free Feedback Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study a novel class of algorithms for solving model-free feedback\noptimization problems in dynamical systems. The key novelty is the introduction\nof \\emph{persistent resetting learning integrators} (PRLI), which are\nintegrators that are reset at the same frequency at which the plant is dithered\nusing exploratory signals for model-free optimization. It is shown that PRLIs\ncan serve as core mechanisms for real-time gradient estimation in online\nfeedback-optimization tasks where only cost function measurements are\navailable. In particular, unlike existing approaches based on approximation\ntheory, such as averaging or finite-differences, PRLIs can produce global\nreal-time gradient estimates of cost functions, with uniformly bounded\nperturbations of arbitrarily small magnitude. In this sense, PRLIs function as\nrobust \\emph{hybrid} \"Oracles\" suitable for interconnection with discrete-time\noptimization algorithms that optimize the performance of continuous-time\ndynamical plants in closed-loop operation. Compared to existing methods, PRLIs\nyield \\emph{global} stability properties for a broad class of cost functions,\nsurpassing the local or semi-global guarantees offered by traditional\napproaches based on perturbation and approximation theory. The proposed\nframework naturally bridges physical systems, modeled as continuous-time plants\nwhere continuous exploration is essential, with digital algorithms, represented\nas discrete-time optimization methods. The main results are illustrated using\ndifferent numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 03:31:10 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Abdelgalil", "Mahmoud", ""], ["Poveda", "Jorge I.", ""]], "extracted_entities": [{"text": "PRLIs", "label": "LLMs"}, {"text": "PRLIs", "label": "LLMs"}, {"text": "PRLIs", "label": "LLMs"}, {"text": "PRLIs", "label": "LLMs"}]}
{"id": "2503.10008", "submitter": "Abu Saleh Musa Patoary", "authors": "Abu Musa Patoary, Amit Vikram and Victor Galitski", "title": "A discrete Fourier transform based quantum circuit for modular\n  multiplication in Shor$'$s algorithm", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Shor$'$s algorithm for the prime factorization of numbers provides an\nexponential speedup over the best known classical algorithms. However,\nnontrivial practical applications have remained out of reach due to\nexperimental limitations. The bottleneck of the experimental realization of the\nalgorithm is the modular exponentiation operation. In this paper, based on a\nrelation between the modular multiplication operator and generalizations of\ndiscrete Fourier transforms, we propose a quantum circuit for modular\nexponentiation. A distinctive feature of our proposal is that our circuit can\nbe entirely implemented in terms of the standard quantum circuit for the\ndiscrete Fourier transform and its variants. The gate-complexity of our\nproposal is $O(L^3)$ where L is the number of bits required to store the number\nbeing factorized. It is possible that such a proposal may provide easier\navenues for near-term generic implementations of Shor$'$s algorithm, in\ncontrast to existing realizations which have often explicitly adapted the\ncircuit to the number being factorized.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 03:39:13 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Patoary", "Abu Musa", ""], ["Vikram", "Amit", ""], ["Galitski", "Victor", ""]], "extracted_entities": [{"text": "modular\nexponentiation", "label": "quantisation"}]}
{"id": "2503.10009", "submitter": "Bowen Zhang", "authors": "Bowen Zhang, Pengcheng Luo", "title": "OR-LLM-Agent: Automating Modeling and Solving of Operations Research\n  Optimization Problem with Reasoning Large Language Model", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Operations Research (OR) has been widely applied in various fields such as\nresource allocation, production planning, and supply chain management. However,\naddressing real-world OR problems requires OR experts to perform mathematical\nmodeling and programmers to develop solution algorithms. This traditional\nmethod, heavily reliant on experts, is costly and has long development cycles,\nseverely limiting the widespread adoption of OR techniques. Few have considered\nusing Artificial Intelligence (AI) to replace professionals to achieve fully\nautomated solutions for OR problems. We propose OR-LLM-Agent, the first AI\nagent that enables end-to-end automation for solving real-world OR problems.\nOR-LLM-Agent leverages the Chain-of-Thought (CoT) reasoning capabilities of\nLarge Language Models (LLMs) to translate natural language problem descriptions\ninto formal mathematical models and automatically generate Gurobi solver code.\nIn OR-LLM-Agent, OR-CodeAgent is designed to automate code execution and repair\nwithin a sandbox environment, facilitating the derivation of the final\nsolution. Due to the lack of dedicated benchmark datasets for evaluating the\nautomated solving of OR problems, we construct a benchmark dataset comprising\n83 real-world OR problems described in natural language. We conduct comparative\nexperiments with state-of-the-art (SOTA) reasoning LLMs, including GPT-o3-mini,\nDeepSeek-R1, and Gemini 2.0 Flash Thinking. The OR-LLM-Agent achieved the\nhighest pass rate of 100% and the highest solution accuracy of 85%,\ndemonstrating the feasibility of automated OR problem-solving. Data and code\nhave been publicly available at https://github.com/bwz96sco/or_llm_agent.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 03:40:50 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhang", "Bowen", ""], ["Luo", "Pengcheng", ""]], "extracted_entities": [{"text": "Chain-of-Thought", "label": "Chain of thought"}, {"text": "Large Language Models", "label": "Large Language Model"}]}
{"id": "2503.10015", "submitter": "Berk Iskender", "authors": "Berk Iskender, Sushan Nakarmi, Nitin Daphalapurkar, Marc L. Klasky,\n  Yoram Bresler", "title": "RSR-NF: Neural Field Regularization by Static Restoration Priors for\n  Dynamic Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Dynamic imaging involves the reconstruction of a spatio-temporal object at\nall times using its undersampled measurements. In particular, in dynamic\ncomputed tomography (dCT), only a single projection at one view angle is\navailable at a time, making the inverse problem very challenging. Moreover,\nground-truth dynamic data is usually either unavailable or too scarce to be\nused for supervised learning techniques. To tackle this problem, we propose\nRSR-NF, which uses a neural field (NF) to represent the dynamic object and,\nusing the Regularization-by-Denoising (RED) framework, incorporates an\nadditional static deep spatial prior into a variational formulation via a\nlearned restoration operator. We use an ADMM-based algorithm with variable\nsplitting to efficiently optimize the variational objective. We compare RSR-NF\nto three alternatives: NF with only temporal regularization; a recent method\ncombining a partially-separable low-rank representation with RED using a\ndenoiser pretrained on static data; and a deep-image prior-based model. The\nfirst comparison demonstrates the reconstruction improvements achieved by\ncombining the NF representation with static restoration priors, whereas the\nother two demonstrate the improvement over state-of-the art techniques for dCT.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 03:50:47 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Iskender", "Berk", ""], ["Nakarmi", "Sushan", ""], ["Daphalapurkar", "Nitin", ""], ["Klasky", "Marc L.", ""], ["Bresler", "Yoram", ""]], "extracted_entities": [{"text": "NF", "label": "Neural Language Model"}, {"text": "NF", "label": "Neural Language Model"}]}
{"id": "2503.10017", "submitter": "Abhay Kumar Yadav", "authors": "Jingxing Li, Yongjae Lee, Abhay Kumar Yadav, Cheng Peng, Rama\n  Chellappa, Deliang Fan", "title": "Speedy MASt3R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image matching is a key component of modern 3D vision algorithms, essential\nfor accurate scene reconstruction and localization. MASt3R redefines image\nmatching as a 3D task by leveraging DUSt3R and introducing a fast reciprocal\nmatching scheme that accelerates matching by orders of magnitude while\npreserving theoretical guarantees. This approach has gained strong traction,\nwith DUSt3R and MASt3R collectively cited over 250 times in a short span,\nunderscoring their impact. However, despite its accuracy, MASt3R's inference\nspeed remains a bottleneck. On an A40 GPU, latency per image pair is 198.16 ms,\nmainly due to computational overhead from the ViT encoder-decoder and Fast\nReciprocal Nearest Neighbor (FastNN) matching.\n  To address this, we introduce Speedy MASt3R, a post-training optimization\nframework that enhances inference efficiency while maintaining accuracy. It\nintegrates multiple optimization techniques, including FlashMatch-an approach\nleveraging FlashAttention v2 with tiling strategies for improved efficiency,\ncomputation graph optimization via layer and tensor fusion having kernel\nauto-tuning with TensorRT (GraphFusion), and a streamlined FastNN pipeline that\nreduces memory access time from quadratic to linear while accelerating\nblock-wise correlation scoring through vectorized computation (FastNN-Lite).\nAdditionally, it employs mixed-precision inference with FP16/FP32 hybrid\ncomputations (HybridCast), achieving speedup while preserving numerical\nprecision. Evaluated on Aachen Day-Night, InLoc, 7-Scenes, ScanNet1500, and\nMegaDepth1500, Speedy MASt3R achieves a 54% reduction in inference time (198 ms\nto 91 ms per image pair) without sacrificing accuracy. This advancement enables\nreal-time 3D understanding, benefiting applications like mixed reality\nnavigation and large-scale 3D scene reconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 03:56:22 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Jingxing", ""], ["Lee", "Yongjae", ""], ["Yadav", "Abhay Kumar", ""], ["Peng", "Cheng", ""], ["Chellappa", "Rama", ""], ["Fan", "Deliang", ""]], "extracted_entities": [{"text": "kernel\nauto-tuning", "label": "Fine-tuning"}]}
{"id": "2503.10019", "submitter": "Izumi Hachisu", "authors": "Izumi Hachisu, Mariko Kato", "title": "A demarcation criterion for hydrogen burning of millinovae", "comments": "accepted for publication in ApJ, 14 pages including 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.SR astro-ph.HE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Millinovae are a new class of transient supersoft X-ray sources with no clear\nsignature of mass ejection. They show similar triangle shapes of $V/I$ band\nlight curves with thousand times fainter peaks than typical classical novae.\nMaccarone et al. regarded the prototype millinova, ASASSN-16oh, as a dwarf nova\nand interpreted the supersoft X-rays to originate from an accretion belt on a\nwhite dwarf (WD). Kato et al. proposed a nova model induced by a high-rate\nmass-accretion during a dwarf nova outburst; the X-rays originate from the\nphotosphere of a hydrogen-burning hot WD whereas the $V/I$ band photons are\nfrom the irradiated accretion disk. Because each peak brightness differs\nlargely from millinova to millinova, we suspect that not all the millinova\ncandidates host a hydrogen burning WD. Based on the light curve analysis of the\nclassical nova KT Eri that has a bright disk, we find that the disk is more\nthan two magnitudes brighter when the disk is irradiated by the hydrogen\nburning WD than when not irradiated. We present the demarcation criterion for\nhydrogen burning to be $I_{\\rm q} - I_{\\rm max} > 2.2$, where $I_q$ and $I_{\\rm\nmax}$ are the $I$ magnitudes in quiescence and at maximum light, respectively.\nAmong many candidates, this requirement is satisfied with the two millinovae in\nwhich soft X-rays were detected.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 03:59:37 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Hachisu", "Izumi", ""], ["Kato", "Mariko", ""]], "extracted_entities": [{"text": "Millinovae", "label": "LLMs"}, {"text": "millinovae", "label": "LLMs"}]}
{"id": "2503.10020", "submitter": "Ali Abedi", "authors": "Ali Abedi, Q. M. Jonathan Wu, Ning Zhang, Farhad Pourpanah", "title": "One-Shot Federated Unsupervised Domain Adaptation with Scaled Entropy\n  Attention and Multi-Source Smoothed Pseudo Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is a promising approach for privacy-preserving\ncollaborative learning. However, it faces significant challenges when dealing\nwith domain shifts, especially when each client has access only to its source\ndata and cannot share it during target domain adaptation. Moreover, FL methods\noften require high communication overhead due to multiple rounds of model\nupdates between clients and the server. We propose a one-shot Federated\nUnsupervised Domain Adaptation (FUDA) method to address these limitations.\nSpecifically, we introduce Scaled Entropy Attention (SEA) for model aggregation\nand Multi-Source Pseudo Labeling (MSPL) for target domain adaptation. SEA uses\nscaled prediction entropy on target domain to assign higher attention to\nreliable models. This improves the global model quality and ensures balanced\nweighting of contributions. MSPL distills knowledge from multiple source models\nto generate pseudo labels and manage noisy labels using smoothed soft-label\ncross-entropy (SSCE). Our approach outperforms state-of-the-art methods across\nfour standard benchmarks while reducing communication and computation costs,\nmaking it highly suitable for real-world applications. The implementation code\nwill be made publicly available upon publication.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 03:59:51 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Abedi", "Ali", ""], ["Wu", "Q. M. Jonathan", ""], ["Zhang", "Ning", ""], ["Pourpanah", "Farhad", ""]], "extracted_entities": [{"text": "Federated Learning", "label": "Few-shot Learning"}, {"text": "FL", "label": "Zero-shot Learning"}, {"text": "Scaled Entropy Attention", "label": "Attention mechanism"}]}
{"id": "2503.10023", "submitter": "Stephanie Hu", "authors": "Stephanie Hu, Xiaolu Guo", "title": "Using Context to Improve Word Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important step in understanding how children acquire languages is studying\nhow infants learn word segmentation. It has been established in previous\nresearch that infants may use statistical regularities in speech to learn word\nsegmentation. The research of Goldwater et al., demonstrated that incorporating\ncontext in models improves their ability to learn word segmentation. We\nimplemented two of their models, a unigram and bigram model, to examine how\ncontext can improve statistical word segmentation. The results are consistent\nwith our hypothesis that the bigram model outperforms the unigram model at\npredicting word segmentation. Extending the work of Goldwater et al., we also\nexplored basic ways to model how young children might use previously learned\nwords to segment new utterances.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 04:04:55 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Hu", "Stephanie", ""], ["Guo", "Xiaolu", ""]], "extracted_entities": [{"text": "context", "label": "contextual Embedding"}, {"text": "context", "label": "contextual Embedding"}]}
{"id": "2503.10026", "submitter": "Lingdi Meng", "authors": "Lingdi Meng, Tian-Cai Peng, Zhi Hu, Siqi Xu, Jiangshan Lan, Chandan\n  Mondal, Guo-Li Wang, Xingbo Zhao, James P. Vary", "title": "Basis light-front quantization for the $\\Lambda_b$ and $\\Sigma_b$\n  baryons", "comments": "arXiv admin note: text overlap with arXiv:2208.00355", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ph hep-th", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Within the basis light-front quantization framework, we compute the masses\nand light-front wave functions of the $\\Lambda_b$ baryon and its isospin\ntriplet counterparts $\\Sigma_b^+$, $\\Sigma_b^0$, and $\\Sigma_b^-$ using a\nlight-front effective Hamiltonian in the leading Fock sector. These wave\nfunctions are obtained as eigenstates of the effective Hamiltonian, which\nincorporates the one-gluon exchange interaction with fixed coupling and a\nthree-dimensional confinement potential. With the quark masses and the\ncouplings as adjustable parameters, the computed masses are set within the\nexperimental range. The resulting predictions for their electromagnetic\nproperties align well with other theoretical calculations. Additionally, the\nparton distribution functions (PDFs) of these baryons are obtained for the\nfirst time, with gluon and sea quark distributions dynamically generated\nthrough QCD evolution of the valence quark PDFs.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 04:08:07 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Meng", "Lingdi", ""], ["Peng", "Tian-Cai", ""], ["Hu", "Zhi", ""], ["Xu", "Siqi", ""], ["Lan", "Jiangshan", ""], ["Mondal", "Chandan", ""], ["Wang", "Guo-Li", ""], ["Zhao", "Xingbo", ""], ["Vary", "James P.", ""]], "extracted_entities": [{"text": "light-front effective Hamiltonian", "label": "quantisation"}, {"text": "effective Hamiltonian", "label": "quantisation"}]}
{"id": "2503.10030", "submitter": "Benniu Zhang", "authors": "Benniu Zhang, Liangshuo Zhang, Xiaodong Wu, Jigang Yu, Xiaochuan Cao,\n  Zhijian Zhang, Xin Li, Fupeng Zhou, Jinglin Pan, Haifei Jiang, Gang Zheng", "title": "Quantum Spin Correlation Amplification Enables Macroscopic Detection of\n  Atomic-Level Fatigue in Ferromagnetic Metals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural fatigue failures account for most of catastrophic metal component\nfailures, annually causing thousands of accidents, tens of thousands of\ncasualties, and $100 billion in global economic losses. Current detection\nmethods struggle to identify early-stage fatigue damage characterized by\nsub-nanometer atomic displacements and localized bond rupture. Here we present\na quantum-enhanced monitoring framework leveraging the fundamental symbiosis\nbetween metallic bonding forces and magnetic interactions. Through magnetic\nexcitation of quantum spin correlation in metallic structures, we establish a\nmacroscopic quantum spin correlation amplification technology that visualizes\nfatigue-induced magnetic flux variations corresponding to bond strength\ndegradation. Our multi-scale analysis integrates fatigue life prediction with\nquantum mechanical parameters (bonding force constants, crystal orbital overlap\npopulation) and ferromagnetic element dynamics, achieving unprecedented\nprediction accuracy (R^2>0.9, p<0.0001). In comprehensive fatigue trials\nencompassing 193 ferromagnetic metal specimens across 3,700 testing hours, this\nquantum magnetic signature consistently provided macroscopic fracture warnings\nprior to failure - a critical advance enabling 100% early detection success.\nThis transformative framework establishes the first operational platform for\npreemptive fatigue mitigation in critical infrastructure, offering a paradigm\nshift from post-failure analysis to quantum-enabled predictive maintenance.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 04:20:47 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhang", "Benniu", ""], ["Zhang", "Liangshuo", ""], ["Wu", "Xiaodong", ""], ["Yu", "Jigang", ""], ["Cao", "Xiaochuan", ""], ["Zhang", "Zhijian", ""], ["Li", "Xin", ""], ["Zhou", "Fupeng", ""], ["Pan", "Jinglin", ""], ["Jiang", "Haifei", ""], ["Zheng", "Gang", ""]], "extracted_entities": [{"text": "quantum spin correlation", "label": "quantisation"}, {"text": "crystal orbital overlap\npopulation", "label": "quantisation"}]}
{"id": "2503.10037", "submitter": "Sina Malakouti", "authors": "Sina Malakouti and Adriana Kovashka", "title": "Investigating and Improving Counter-Stereotypical Action Relation in\n  Text-to-Image Diffusion Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-to-image diffusion models consistently fail at generating\ncounter-stereotypical action relationships (e.g., \"mouse chasing cat\"),\ndefaulting to frequent stereotypes even when explicitly prompted otherwise.\nThrough systematic investigation, we discover this limitation stems from\ndistributional biases rather than inherent model constraints. Our key insight\nreveals that while models fail on rare compositions when their inversions are\ncommon, they can successfully generate similar intermediate compositions (e.g.,\n\"mouse chasing boy\"). To test this hypothesis, we develop a Role-Bridging\nDecomposition framework that leverages these intermediates to gradually teach\nrare relationships without architectural modifications. We introduce\nActionBench, a comprehensive benchmark specifically designed to evaluate\naction-based relationship generation across stereotypical and\ncounter-stereotypical configurations. Our experiments validate that\nintermediate compositions indeed facilitate counter-stereotypical generation,\nwith both automatic metrics and human evaluations showing significant\nimprovements over existing approaches. This work not only identifies\nfundamental biases in current text-to-image systems but demonstrates a\npromising direction for addressing them through compositional reasoning.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 04:38:02 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Malakouti", "Sina", ""], ["Kovashka", "Adriana", ""]], "extracted_entities": [{"text": "explicitly prompted", "label": "Prompting"}]}
{"id": "2503.10041", "submitter": "Zhenzhe Shao", "authors": "Jiachi Chen, Zhenzhe Shao, Shuo Yang, Yiming Shen, Yanlin Wang, Ting\n  Chen, Zhenyu Shan, Zibin Zheng", "title": "NumScout: Unveiling Numerical Defects in Smart Contracts using\n  LLM-Pruning Symbolic Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the Ethereum platform has witnessed a proliferation of smart\ncontracts, accompanied by exponential growth in total value locked (TVL).\nHigh-TVL smart contracts often require complex numerical computations,\nparticularly in mathematical financial models used by many decentralized\napplications (DApps). Improper calculations can introduce numerical defects,\nposing potential security risks. Existing research primarily focuses on\ntraditional numerical defects like integer overflow, and there is currently a\nlack of systematic research and effective detection methods targeting new types\nof numerical defects. In this paper, we identify five new types of numerical\ndefects through the analysis of 1,199 audit reports by utilizing the open card\nmethod. Each defect is defined and illustrated with a code example to highlight\nits features and potential consequences. We also propose NumScout, a symbolic\nexecution-based tool designed to detect these five defects. Specifically, the\ntool combines information from source code and bytecode, analyzing key\noperations such as comparisons and transfers, to effectively locate defects and\nreport them based on predefined detection patterns. Furthermore, NumScout uses\na large language model (LLM) to prune functions which are unrelated to\nnumerical operations. This step allows symbolic execution to quickly enter the\ntarget function and improve runtime speed by 28.4%. We run NumScout on 6,617\nreal-world contracts and evaluated its performance based on manually labeled\nresults. We find that 1,774 contracts contained at least one of the five\ndefects, and the tool achieved an overall precision of 89.7%.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 04:46:53 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chen", "Jiachi", ""], ["Shao", "Zhenzhe", ""], ["Yang", "Shuo", ""], ["Shen", "Yiming", ""], ["Wang", "Yanlin", ""], ["Chen", "Ting", ""], ["Shan", "Zhenyu", ""], ["Zheng", "Zibin", ""]], "extracted_entities": [{"text": "LLM", "label": "Large Language Model"}]}
{"id": "2503.10042", "submitter": "Ziyue Wang", "authors": "Ziyue Wang, Yurui Dong, Fuwen Luo, Minyuan Ruan, Zhili Cheng, Chi\n  Chen, Peng Li, Yang Liu", "title": "How Do Multimodal Large Language Models Handle Complex Multimodal\n  Reasoning? Placing Them in An Extensible Escape Game", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred\ninterest in complex multimodal reasoning tasks in the real-world and virtual\nenvironment, which require coordinating multiple abilities, including visual\nperception, visual reasoning, spatial awareness, and target deduction. However,\nexisting evaluations primarily assess the final task completion, often\ndegrading assessments to isolated abilities such as visual grounding and visual\nquestion answering. Less attention is given to comprehensively and\nquantitatively analyzing reasoning process in multimodal environments, which is\ncrucial for understanding model behaviors and underlying reasoning mechanisms\nbeyond merely task success. To address this, we introduce MM-Escape, an\nextensible benchmark for investigating multimodal reasoning, inspired by\nreal-world escape games. MM-Escape emphasizes intermediate model behaviors\nalongside final task completion. To achieve this, we develop EscapeCraft, a\ncustomizable and open environment that enables models to engage in free-form\nexploration for assessing multimodal reasoning. Extensive experiments show that\nMLLMs, regardless of scale, can successfully complete the simplest room escape\ntasks, with some exhibiting human-like exploration strategies. Yet, performance\ndramatically drops as task difficulty increases. Moreover, we observe that\nperformance bottlenecks vary across models, revealing distinct failure modes\nand limitations in their multimodal reasoning abilities, such as repetitive\ntrajectories without adaptive exploration, getting stuck in corners due to poor\nvisual spatial awareness, and ineffective use of acquired props, such as the\nkey. We hope our work sheds light on new challenges in multimodal reasoning,\nand uncovers potential improvements in MLLMs capabilities.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 04:48:43 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Ziyue", ""], ["Dong", "Yurui", ""], ["Luo", "Fuwen", ""], ["Ruan", "Minyuan", ""], ["Cheng", "Zhili", ""], ["Chen", "Chi", ""], ["Li", "Peng", ""], ["Liu", "Yang", ""]], "extracted_entities": [{"text": "Multimodal Large Language Models", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}]}
{"id": "2503.10043", "submitter": "Wenjie Li", "authors": "Wenjie Li, Heng Guo, Yuefeng Hou, and Zhanyu Ma", "title": "FourierSR: A Fourier Token-based Plugin for Efficient Image\n  Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image super-resolution (SR) aims to recover low-resolution images to\nhigh-resolution images, where improving SR efficiency is a high-profile\nchallenge. However, commonly used units in SR, like convolutions and\nwindow-based Transformers, have limited receptive fields, making it challenging\nto apply them to improve SR under extremely limited computational cost. To\naddress this issue, inspired by modeling convolution theorem through token mix,\nwe propose a Fourier token-based plugin called FourierSR to improve SR\nuniformly, which avoids the instability or inefficiency of existing token mix\ntechnologies when applied as plug-ins. Furthermore, compared to convolutions\nand windows-based Transformers, our FourierSR only utilizes Fourier transform\nand multiplication operations, greatly reducing complexity while having global\nreceptive fields. Experimental results show that our FourierSR as a\nplug-and-play unit brings an average PSNR gain of 0.34dB for existing efficient\nSR methods on Manga109 test set at the scale of x4, while the average increase\nin the number of Params and FLOPs is only 0.6% and 1.5% of original sizes. We\nwill release our codes upon acceptance.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 04:50:55 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Wenjie", ""], ["Guo", "Heng", ""], ["Hou", "Yuefeng", ""], ["Ma", "Zhanyu", ""]], "extracted_entities": [{"text": "window-based Transformers", "label": "Transformers"}]}
{"id": "2503.10045", "submitter": "Yaoting Jiang", "authors": "Meng Wang, Zi Yang, Ruifeng Zhao, Yaoting Jiang", "title": "CPLOYO: A Pulmonary Nodule Detection Model with Multi-Scale Feature\n  Fusion and Nonlinear Feature Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration of Internet of Things (IoT) technology in pulmonary nodule\ndetection significantly enhances the intelligence and real-time capabilities of\nthe detection system. Currently, lung nodule detection primarily focuses on the\nidentification of solid nodules, but different types of lung nodules correspond\nto various forms of lung cancer. Multi-type detection contributes to improving\nthe overall lung cancer detection rate and enhancing the cure rate. To achieve\nhigh sensitivity in nodule detection, targeted improvements were made to the\nYOLOv8 model. Firstly, the C2f\\_RepViTCAMF module was introduced to augment the\nC2f module in the backbone, thereby enhancing detection accuracy for small lung\nnodules and achieving a lightweight model design. Secondly, the MSCAF module\nwas incorporated to reconstruct the feature fusion section of the model,\nimproving detection accuracy for lung nodules of varying scales. Furthermore,\nthe KAN network was integrated into the model. By leveraging the KAN network's\npowerful nonlinear feature learning capability, detection accuracy for small\nlung nodules was further improved, and the model's generalization ability was\nenhanced. Tests conducted on the LUNA16 dataset demonstrate that the improved\nmodel outperforms the original model as well as other mainstream models such as\nYOLOv9 and RT-DETR across various evaluation metrics.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 04:51:57 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Meng", ""], ["Yang", "Zi", ""], ["Zhao", "Ruifeng", ""], ["Jiang", "Yaoting", ""]], "extracted_entities": [{"text": "nonlinear feature learning", "label": "Few-shot Learning"}]}
{"id": "2503.10047", "submitter": "Wenjie Li", "authors": "Wenjie Li, Heng Guo, Yuefeng Hou, Guangwei Gao, and Zhanyu Ma", "title": "Dual-domain Modulation Network for Lightweight Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lightweight image super-resolution (SR) aims to reconstruct high-resolution\nimages from low-resolution images with limited computational costs. We find\nexisting frequency-based SR methods cannot balance the reconstruction of\noverall structures and high-frequency parts. Meanwhile, these methods are\ninefficient for handling frequency features and unsuitable for lightweight SR.\nIn this paper, we show introducing both wavelet and Fourier information allows\nour model to consider both high-frequency features and overall SR structure\nreconstruction while reducing costs. Specifically, we propose a dual-domain\nmodulation network that utilize wavelet-domain modulation self-Transformer\n(WMT) plus Fourier supervision to modulate frequency features in addition to\nspatial domain modulation. Compared to existing frequency-based SR modules, our\nWMT is more suitable for frequency learning in lightweight SR. Experimental\nresults show that our method achieves a comparable PSNR of SRFormer and MambaIR\nwhile with less than 50% and 60% of their FLOPs and achieving inference speeds\n15.4x and 5.4x faster, respectively, demonstrating the effectiveness of our\nmethod on SR quality and lightweight. Codes will be released upon acceptance.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 04:59:46 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Wenjie", ""], ["Guo", "Heng", ""], ["Hou", "Yuefeng", ""], ["Gao", "Guangwei", ""], ["Ma", "Zhanyu", ""]], "extracted_entities": [{"text": "frequency learning", "label": "Few-shot Learning"}]}
{"id": "2503.10048", "submitter": "Bharat Srikishan", "authors": "Bharat Srikishan, Daniel O'Malley, Mohamed Mehana, Nicholas Lubbers,\n  Nikhil Muralidhar", "title": "Model-Agnostic Knowledge Guided Correction for Improved Neural Surrogate\n  Rollout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the evolution of physical systems is critical to many applications\nin science and engineering. As the evolution of these systems is governed by\npartial differential equations (PDEs), there are a number of computational\nsimulations which resolve these systems with high accuracy. However, as these\nsimulations incur high computational costs, they are infeasible to be employed\nfor large-scale analysis. A popular alternative to simulators are neural\nnetwork surrogates which are trained in a data-driven manner and are much more\ncomputationally efficient. However, these surrogate models suffer from high\nrollout error when used autoregressively, especially when confronted with\ntraining data paucity. Existing work proposes to improve surrogate rollout\nerror by either including physical loss terms directly in the optimization of\nthe model or incorporating computational simulators as `differentiable layers'\nin the neural network. Both of these approaches have their challenges, with\nphysical loss functions suffering from slow convergence for stiff PDEs and\nsimulator layers requiring gradients which are not always available, especially\nin legacy simulators. We propose the Hybrid PDE Predictor with Reinforcement\nLearning (HyPER) model: a model-agnostic, RL based, cost-aware model which\ncombines a neural surrogate, RL decision model, and a physics simulator (with\nor without gradients) to reduce surrogate rollout error significantly. In\naddition to reducing in-distribution rollout error by **47%-78%**, HyPER learns\nan intelligent policy that is adaptable to changing physical conditions and\nresistant to noise corruption. Code available at\nhttps://github.com/scailab/HyPER.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 05:00:23 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Srikishan", "Bharat", ""], ["O'Malley", "Daniel", ""], ["Mehana", "Mohamed", ""], ["Lubbers", "Nicholas", ""], ["Muralidhar", "Nikhil", ""]], "extracted_entities": [{"text": "partial differential equations", "label": "BERT"}, {"text": "PDEs", "label": "BERT"}, {"text": "neural\nnetwork surrogates", "label": "Neural Language Model"}, {"text": "PDEs", "label": "BERT"}]}
{"id": "2503.10049", "submitter": "Jianzong Wang", "authors": "Ziqi Jia, Junjie Li, Xiaoyang Qu, Jianzong Wang", "title": "Enhancing Multi-Agent Systems via Reinforcement Learning with LLM-based\n  Planner and Graph-based Policy", "comments": "Accepted by the 2025 IEEE International Conference on Robotics &\n  Automation (ICRA 2025)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent systems (MAS) have shown great potential in executing complex\ntasks, but coordination and safety remain significant challenges. Multi-Agent\nReinforcement Learning (MARL) offers a promising framework for agent\ncollaboration, but it faces difficulties in handling complex tasks and\ndesigning reward functions. The introduction of Large Language Models (LLMs)\nhas brought stronger reasoning and cognitive abilities to MAS, but existing\nLLM-based systems struggle to respond quickly and accurately in dynamic\nenvironments. To address these challenges, we propose LLM-based Graph\nCollaboration MARL (LGC-MARL), a framework that efficiently combines LLMs and\nMARL. This framework decomposes complex tasks into executable subtasks and\nachieves efficient collaboration among multiple agents through graph-based\ncoordination. Specifically, LGC-MARL consists of two main components: an LLM\nplanner and a graph-based collaboration meta policy. The LLM planner transforms\ncomplex task instructions into a series of executable subtasks, evaluates the\nrationality of these subtasks using a critic model, and generates an action\ndependency graph. The graph-based collaboration meta policy facilitates\ncommunication and collaboration among agents based on the action dependency\ngraph, and adapts to new task environments through meta-learning. Experimental\nresults on the AI2-THOR simulation platform demonstrate the superior\nperformance and scalability of LGC-MARL in completing various complex tasks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 05:02:49 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Jia", "Ziqi", ""], ["Li", "Junjie", ""], ["Qu", "Xiaoyang", ""], ["Wang", "Jianzong", ""]], "extracted_entities": [{"text": "Large Language Models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLM", "label": "Large Language Model"}, {"text": "LLM", "label": "Large Language Model"}]}
{"id": "2503.10052", "submitter": "Minjun Kim", "authors": "Minje Kim, Minjun Kim, Xu Yang", "title": "DTA: Dual Temporal-channel-wise Attention for Spiking Neural Networks", "comments": "Accepted by IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV) 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) present a more energy-efficient alternative to\nArtificial Neural Networks (ANNs) by harnessing spatio-temporal dynamics and\nevent-driven spikes. Effective utilization of temporal information is crucial\nfor SNNs, leading to the exploration of attention mechanisms to enhance this\ncapability. Conventional attention operations either apply identical operation\nor employ non-identical operations across target dimensions. We identify that\nthese approaches provide distinct perspectives on temporal information. To\nleverage the strengths of both operations, we propose a novel Dual\nTemporal-channel-wise Attention (DTA) mechanism that integrates both\nidentical/non-identical attention strategies. To the best of our knowledge,\nthis is the first attempt to concentrate on both the correlation and dependency\nof temporal-channel using both identical and non-identical attention\noperations. Experimental results demonstrate that the DTA mechanism achieves\nstate-of-the-art performance on both static datasets (CIFAR10, CIFAR100,\nImageNet-1k) and dynamic dataset (CIFAR10-DVS), elevating spike representation\nand capturing complex temporal-channel relationship. We open-source our code:\nhttps://github.com/MnJnKIM/DTA-SNN.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 05:09:48 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Kim", "Minje", ""], ["Kim", "Minjun", ""], ["Yang", "Xu", ""]], "extracted_entities": [{"text": "Conventional attention operations", "label": "Attention mechanism"}, {"text": "Dual\nTemporal-channel-wise Attention (DTA) mechanism", "label": "Attention mechanism"}, {"text": "DTA mechanism", "label": "Attention mechanism"}]}
{"id": "2503.10057", "submitter": "Ho Hin Lee", "authors": "Ho Hin Lee, Alberto Santamaria-Pang, Jameson Merkov, Matthew Lungren,\n  Ivan Tarapov", "title": "Multi-Modal Mamba Modeling for Survival Prediction (M4Survive): Adapting\n  Joint Foundation Model Representations", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate survival prediction in oncology requires integrating diverse imaging\nmodalities to capture the complex interplay of tumor biology. Traditional\nsingle-modality approaches often fail to leverage the complementary insights\nprovided by radiological and pathological assessments. In this work, we\nintroduce M4Survive (Multi-Modal Mamba Modeling for Survival Prediction), a\nnovel framework that learns joint foundation model representations using\nefficient adapter networks. Our approach dynamically fuses heterogeneous\nembeddings from a foundation model repository (e.g., MedImageInsight,\nBiomedCLIP, Prov-GigaPath, UNI2-h), creating a correlated latent space\noptimized for survival risk estimation. By leveraging Mamba-based adapters,\nM4Survive enables efficient multi-modal learning while preserving computational\nefficiency. Experimental evaluations on benchmark datasets demonstrate that our\napproach outperforms both unimodal and traditional static multi-modal baselines\nin survival prediction accuracy. This work underscores the potential of\nfoundation model-driven multi-modal fusion in advancing precision oncology and\npredictive analytics.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 05:18:32 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Lee", "Ho Hin", ""], ["Santamaria-Pang", "Alberto", ""], ["Merkov", "Jameson", ""], ["Lungren", "Matthew", ""], ["Tarapov", "Ivan", ""]], "extracted_entities": [{"text": "heterogeneous\nembeddings", "label": "Embedding"}, {"text": "MedImageInsight", "label": "Foundation Model"}, {"text": "BiomedCLIP", "label": "Foundation Model"}, {"text": "UNI2-h", "label": "Foundation Model"}, {"text": "multi-modal learning", "label": "Few-shot Learning"}]}
{"id": "2503.10061", "submitter": "Nicholas Roberts", "authors": "Nicholas Roberts, Niladri Chatterji, Sharan Narang, Mike Lewis,\n  Dieuwke Hupkes", "title": "Compute Optimal Scaling of Skills: Knowledge vs Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scaling laws are a critical component of the LLM development pipeline, most\nfamously as a way to forecast training decisions such as 'compute-optimally'\ntrading-off parameter count and dataset size, alongside a more recent growing\nlist of other crucial decisions. In this work, we ask whether compute-optimal\nscaling behaviour can be skill-dependent. In particular, we examine knowledge\nand reasoning-based skills such as knowledge-based QA and code generation, and\nwe answer this question in the affirmative: $\\textbf{scaling laws are\nskill-dependent}$. Next, to understand whether skill-dependent scaling is an\nartefact of the pretraining datamix, we conduct an extensive ablation of\ndifferent datamixes and find that, also when correcting for datamix\ndifferences, $\\textbf{knowledge and code exhibit fundamental differences in\nscaling behaviour}$. We conclude with an analysis of how our findings relate to\nstandard compute-optimal scaling using a validation set, and find that\n$\\textbf{a misspecified validation set can impact compute-optimal parameter\ncount by nearly 50%,}$ depending on its skill composition.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 05:21:22 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Roberts", "Nicholas", ""], ["Chatterji", "Niladri", ""], ["Narang", "Sharan", ""], ["Lewis", "Mike", ""], ["Hupkes", "Dieuwke", ""]], "extracted_entities": [{"text": "Scaling laws", "label": "Scaling law"}, {"text": "LLM", "label": "LLM"}, {"text": "scaling laws", "label": "Scaling law"}]}
{"id": "2503.10063", "submitter": "Luke Bauer", "authors": "Luke A. Bauer, Wenxuan Bao, and Vincent Bindschaedler", "title": "Provably Secure Covert Messaging Using Image-based Diffusion Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of securely and robustly embedding covert messages\ninto an image-based diffusion model's output. The sender and receiver want to\nexchange the maximum amount of information possible per diffusion sampled image\nwhile remaining undetected. The adversary wants to detect that such\ncommunication is taking place by identifying those diffusion samples that\ncontain covert messages. To maximize robustness to transformations of the\ndiffusion sample, a strategy is for the sender and the receiver to embed the\nmessage in the initial latents. We first show that prior work that attempted\nthis is easily broken because their embedding technique alters the latents'\ndistribution. We then propose a straightforward method to embed covert messages\nin the initial latent {\\em without} altering the distribution. We prove that\nour construction achieves indistinguishability to any probabilistic polynomial\ntime adversary. Finally, we discuss and analyze empirically the tradeoffs\nbetween embedding capacity, message recovery rates, and robustness. We find\nthat optimizing the inversion method for error correction is crucial for\nreliability.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 05:24:40 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Bauer", "Luke A.", ""], ["Bao", "Wenxuan", ""], ["Bindschaedler", "Vincent", ""]], "extracted_entities": [{"text": "embedding technique", "label": "Embedding"}]}
{"id": "2503.10065", "submitter": "Damien Teney", "authors": "Damien Teney, Liangze Jiang, Florin Gogianu, Ehsan Abbasnejad", "title": "Do We Always Need the Simplicity Bias? Looking for Optimal Inductive\n  Biases in the Wild", "comments": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural architectures tend to fit their data with relatively simple functions.\nThis \"simplicity bias\" is widely regarded as key to their success. This paper\nexplores the limits of this principle. Building on recent findings that the\nsimplicity bias stems from ReLU activations [96], we introduce a method to\nmeta-learn new activation functions and inductive biases better suited to\nspecific tasks.\n  Findings: We identify multiple tasks where the simplicity bias is inadequate\nand ReLUs suboptimal. In these cases, we learn new activation functions that\nperform better by inducing a prior of higher complexity. Interestingly, these\ncases correspond to domains where neural networks have historically struggled:\ntabular data, regression tasks, cases of shortcut learning, and algorithmic\ngrokking tasks. In comparison, the simplicity bias induced by ReLUs proves\nadequate on image tasks where the best learned activations are nearly identical\nto ReLUs and GeLUs.\n  Implications: Contrary to popular belief, the simplicity bias of ReLU\nnetworks is not universally useful. It is near-optimal for image\nclassification, but other inductive biases are sometimes preferable. We showed\nthat activation functions can control these inductive biases, but future\ntailored architectures might provide further benefits. Advances are still\nneeded to characterize a model's inductive biases beyond \"complexity\", and\ntheir adequacy with the data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 05:28:40 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Teney", "Damien", ""], ["Jiang", "Liangze", ""], ["Gogianu", "Florin", ""], ["Abbasnejad", "Ehsan", ""]], "extracted_entities": [{"text": "simplicity bias", "label": "Model Bias and Fairness"}, {"text": "simplicity bias", "label": "Model Bias and Fairness"}, {"text": "simplicity bias", "label": "Model Bias and Fairness"}, {"text": "shortcut learning", "label": "Few-shot Learning"}, {"text": "simplicity bias", "label": "Model Bias and Fairness"}, {"text": "simplicity bias", "label": "Model Bias and Fairness"}]}
{"id": "2503.10068", "submitter": "Han Liu", "authors": "Han Liu, Riqiang Gao, Sasa Grbic", "title": "AI-assisted Early Detection of Pancreatic Ductal Adenocarcinoma on\n  Contrast-enhanced CT", "comments": "1st place in the PANORAMA Challenge (Team DTI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pancreatic ductal adenocarcinoma (PDAC) is one of the most common and\naggressive types of pancreatic cancer. However, due to the lack of early and\ndisease-specific symptoms, most patients with PDAC are diagnosed at an advanced\ndisease stage. Consequently, early PDAC detection is crucial for improving\npatients' quality of life and expanding treatment options. In this work, we\ndevelop a coarse-to-fine approach to detect PDAC on contrast-enhanced CT scans.\nFirst, we localize and crop the region of interest from the low-resolution\nimages, and then segment the PDAC-related structures at a finer scale.\nAdditionally, we introduce two strategies to further boost detection\nperformance: (1) a data-splitting strategy for model ensembling, and (2) a\ncustomized post-processing function. We participated in the PANORAMA challenge\nand ranked 1st place for PDAC detection with an AUROC of 0.9263 and an AP of\n0.7243. Our code and models are publicly available at\nhttps://github.com/han-liu/PDAC_detection.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 05:31:18 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Liu", "Han", ""], ["Gao", "Riqiang", ""], ["Grbic", "Sasa", ""]], "extracted_entities": [{"text": "model ensembling", "label": "Embedding"}]}
{"id": "2503.10069", "submitter": "Xiangyu Shi", "authors": "Xiangyu Shi, Zerui Li, Wenqi Lyu, Jiatong Xia, Feras Dayoub, Yanyuan\n  Qiao, Qi Wu", "title": "SmartWay: Enhanced Waypoint Prediction and Backtracking for Zero-Shot\n  Vision-and-Language Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-and-Language Navigation (VLN) in continuous environments requires\nagents to interpret natural language instructions while navigating\nunconstrained 3D spaces. Existing VLN-CE frameworks rely on a two-stage\napproach: a waypoint predictor to generate waypoints and a navigator to execute\nmovements. However, current waypoint predictors struggle with spatial\nawareness, while navigators lack historical reasoning and backtracking\ncapabilities, limiting adaptability. We propose a zero-shot VLN-CE framework\nintegrating an enhanced waypoint predictor with a Multi-modal Large Language\nModel (MLLM)-based navigator. Our predictor employs a stronger vision encoder,\nmasked cross-attention fusion, and an occupancy-aware loss for better waypoint\nquality. The navigator incorporates history-aware reasoning and adaptive path\nplanning with backtracking, improving robustness. Experiments on R2R-CE and\nMP3D benchmarks show our method achieves state-of-the-art (SOTA) performance in\nzero-shot settings, demonstrating competitive results compared to fully\nsupervised methods. Real-world validation on Turtlebot 4 further highlights its\nadaptability.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 05:32:57 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Shi", "Xiangyu", ""], ["Li", "Zerui", ""], ["Lyu", "Wenqi", ""], ["Xia", "Jiatong", ""], ["Dayoub", "Feras", ""], ["Qiao", "Yanyuan", ""], ["Wu", "Qi", ""]], "extracted_entities": [{"text": "Multi-modal Large Language\nModel (MLLM)", "label": "Large Language Model"}]}
{"id": "2503.10071", "submitter": "Sunzida Siddique", "authors": "Mohd Ariful Haque, Justin Williams, Sunzida Siddique, Md. Hujaifa\n  Islam, Hasmot Ali, Kishor Datta Gupta, and Roy George", "title": "Advanced Tool Learning and Selection System (ATLASS): A Closed-Loop\n  Framework Using LLM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The combination of LLM agents with external tools enables models to solve\ncomplex tasks beyond their knowledge base. Human-designed tools are inflexible\nand restricted to solutions within the scope of pre-existing tools created by\nexperts. To address this problem, we propose ATLASS, an advanced tool learning\nand selection system designed as a closed-loop framework. It enables the LLM to\nsolve problems by dynamically generating external tools on demand. In this\nframework, agents play a crucial role in orchestrating tool selection,\nexecution, and refinement, ensuring adaptive problem-solving capabilities. The\noperation of ATLASS follows three phases: The first phase, Understanding Tool\nRequirements, involves the Agents determining whether tools are required and\nspecifying their functionality; the second phase, Tool Retrieval/Generation,\ninvolves the Agents retrieving or generating tools based on their availability;\nand the third phase, Task Solving, involves combining all the component tools\nnecessary to complete the initial task. The Tool Dataset stores the generated\ntools, ensuring reusability and minimizing inference cost. Current LLM-based\ntool generation systems have difficulty creating complex tools that need APIs\nor external packages. In ATLASS, we solve the problem by automatically setting\nup the environment, fetching relevant API documentation online, and using a\nPython interpreter to create a reliable, versatile tool that works in a wider\nrange of situations. OpenAI GPT-4.0 is used as the LLM agent, and safety and\nethical concerns are handled through human feedback before executing generated\ncode. By addressing the limitations of predefined toolsets and enhancing\nadaptability, ATLASS serves as a real-world solution that empowers users with\ndynamically generated tools for complex problem-solving.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 05:39:00 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Haque", "Mohd Ariful", ""], ["Williams", "Justin", ""], ["Siddique", "Sunzida", ""], ["Islam", "Md. Hujaifa", ""], ["Ali", "Hasmot", ""], ["Gupta", "Kishor Datta", ""], ["George", "Roy", ""]], "extracted_entities": [{"text": "LLM", "label": "LLM"}, {"text": "ATLASS", "label": "LLM-based"}, {"text": "LLM", "label": "LLM"}, {"text": "LLM-based", "label": "LLM-based"}, {"text": "GPT-4", "label": "GPT"}, {"text": "LLM", "label": "LLM"}, {"text": "safety and\nethical concerns", "label": "AI Ethics"}, {"text": "ATLASS", "label": "LLM-based"}]}
{"id": "2503.10072", "submitter": "Mia Mohammad Imran", "authors": "Mia Mohammad Imran, and Jaydeb Sarker", "title": "\"Silent Is Not Actually Silent\": An Investigation of Toxicity on Bug\n  Report Discussion", "comments": null, "journal-ref": "International Conference on the Foundations of Software\n  Engineering: Ideas, Visions and Reflections track - 2025", "doi": null, "report-no": null, "categories": "cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Toxicity in bug report discussions poses significant challenges to the\ncollaborative dynamics of open-source software development. Bug reports are\ncrucial for identifying and resolving defects, yet their inherently\nproblem-focused nature and emotionally charged context make them susceptible to\ntoxic interactions. This study explores toxicity in GitHub bug reports through\na qualitative analysis of 203 bug threads, including 81 toxic ones. Our\nfindings reveal that toxicity frequently arises from misaligned perceptions of\nbug severity and priority, unresolved frustrations with tools, and lapses in\nprofessional communication. These toxic interactions not only derail productive\ndiscussions but also reduce the likelihood of actionable outcomes, such as\nlinking issues with pull requests. Our preliminary findings offer actionable\nrecommendations to improve bug resolution by mitigating toxicity.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 05:39:29 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Imran", "Mia Mohammad", ""], ["Sarker", "Jaydeb", ""]], "extracted_entities": [{"text": "GitHub", "label": "Open-source LLMs"}]}
{"id": "2503.10076", "submitter": "Meiqi Wu", "authors": "Xinrang Ling, Chen Zhu, Meiqi Wu, Hangyu Li, Xiaokun Feng, Cundian\n  Yang, Aiming Hao, Jiashu Zhu, Jiahong Wu, Xiangxiang Chu", "title": "VMBench: A Benchmark for Perception-Aligned Video Motion Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Video generation has advanced rapidly, improving evaluation methods, yet\nassessing video's motion remains a major challenge. Specifically, there are two\nkey issues: 1) current motion metrics do not fully align with human\nperceptions; 2) the existing motion prompts are limited. Based on these\nfindings, we introduce VMBench--a comprehensive Video Motion Benchmark that has\nperception-aligned motion metrics and features the most diverse types of\nmotion. VMBench has several appealing properties: 1) Perception-Driven Motion\nEvaluation Metrics, we identify five dimensions based on human perception in\nmotion video assessment and develop fine-grained evaluation metrics, providing\ndeeper insights into models' strengths and weaknesses in motion quality. 2)\nMeta-Guided Motion Prompt Generation, a structured method that extracts\nmeta-information, generates diverse motion prompts with LLMs, and refines them\nthrough human-AI validation, resulting in a multi-level prompt library covering\nsix key dynamic scene dimensions. 3) Human-Aligned Validation Mechanism, we\nprovide human preference annotations to validate our benchmarks, with our\nmetrics achieving an average 35.3% improvement in Spearman's correlation over\nbaseline methods. This is the first time that the quality of motion in videos\nhas been evaluated from the perspective of human perception alignment.\nAdditionally, we will soon release VMBench at\nhttps://github.com/GD-AIGC/VMBench, setting a new standard for evaluating and\nadvancing motion generation models.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 05:54:42 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Ling", "Xinrang", ""], ["Zhu", "Chen", ""], ["Wu", "Meiqi", ""], ["Li", "Hangyu", ""], ["Feng", "Xiaokun", ""], ["Yang", "Cundian", ""], ["Hao", "Aiming", ""], ["Zhu", "Jiashu", ""], ["Wu", "Jiahong", ""], ["Chu", "Xiangxiang", ""]], "extracted_entities": [{"text": "Meta-Guided Motion Prompt Generation", "label": "Prompting"}, {"text": "LLMs", "label": "LLM"}]}
{"id": "2503.10079", "submitter": "Chunyi Li", "authors": "Chunyi Li, Xiaozhe Li, Zicheng Zhang, Yuan Tian, Ziheng Jia, Xiaohong\n  Liu, Xiongkuo Min, Jia Wang, Haodong Duan, Kai Chen, Guangtao Zhai", "title": "Information Density Principle for MLLM Benchmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the emergence of Multimodal Large Language Models (MLLMs), hundreds of\nbenchmarks have been developed to ensure the reliability of MLLMs in downstream\ntasks. However, the evaluation mechanism itself may not be reliable. For\ndevelopers of MLLMs, questions remain about which benchmark to use and whether\nthe test results meet their requirements. Therefore, we propose a critical\nprinciple of Information Density, which examines how much insight a benchmark\ncan provide for the development of MLLMs. We characterize it from four key\ndimensions: (1) Fallacy, (2) Difficulty, (3) Redundancy, (4) Diversity. Through\na comprehensive analysis of more than 10,000 samples, we measured the\ninformation density of 19 MLLM benchmarks. Experiments show that using the\nlatest benchmarks in testing can provide more insight compared to previous\nones, but there is still room for improvement in their information density. We\nhope this principle can promote the development and application of future MLLM\nbenchmarks. Project page: https://github.com/lcysyzxdxc/bench4bench\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 05:58:41 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Chunyi", ""], ["Li", "Xiaozhe", ""], ["Zhang", "Zicheng", ""], ["Tian", "Yuan", ""], ["Jia", "Ziheng", ""], ["Liu", "Xiaohong", ""], ["Min", "Xiongkuo", ""], ["Wang", "Jia", ""], ["Duan", "Haodong", ""], ["Chen", "Kai", ""], ["Zhai", "Guangtao", ""]], "extracted_entities": [{"text": "Multimodal Large Language Models", "label": "Large Language Model"}]}
{"id": "2503.10080", "submitter": "Zhen Qu", "authors": "Zhen Qu, Xian Tao, Xinyi Gong, Shichen Qu, Qiyu Chen, Zhengtao Zhang,\n  Xingang Wang, Guiguang Ding", "title": "Bayesian Prompt Flow Learning for Zero-Shot Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, vision-language models (e.g. CLIP) have demonstrated remarkable\nperformance in zero-shot anomaly detection (ZSAD). By leveraging auxiliary data\nduring training, these models can directly perform cross-category anomaly\ndetection on target datasets, such as detecting defects on industrial product\nsurfaces or identifying tumors in organ tissues. Existing approaches typically\nconstruct text prompts through either manual design or the optimization of\nlearnable prompt vectors. However, these methods face several challenges: 1)\nhandcrafted prompts require extensive expert knowledge and trial-and-error; 2)\nsingle-form learnable prompts struggle to capture complex anomaly semantics;\nand 3) an unconstrained prompt space limit generalization to unseen categories.\nTo address these issues, we propose Bayesian Prompt Flow Learning (Bayes-PFL),\nwhich models the prompt space as a learnable probability distribution from a\nBayesian perspective. Specifically, a prompt flow module is designed to learn\nboth image-specific and image-agnostic distributions, which are jointly\nutilized to regularize the text prompt space and enhance the model's\ngeneralization on unseen categories. These learned distributions are then\nsampled to generate diverse text prompts, effectively covering the prompt\nspace. Additionally, a residual cross-attention (RCA) module is introduced to\nbetter align dynamic text embeddings with fine-grained image features.\nExtensive experiments on 15 industrial and medical datasets demonstrate our\nmethod's superior performance.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 06:05:35 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Qu", "Zhen", ""], ["Tao", "Xian", ""], ["Gong", "Xinyi", ""], ["Qu", "Shichen", ""], ["Chen", "Qiyu", ""], ["Zhang", "Zhengtao", ""], ["Wang", "Xingang", ""], ["Ding", "Guiguang", ""]], "extracted_entities": [{"text": "text prompts", "label": "Prompting"}, {"text": "handcrafted prompts", "label": "Prompting"}, {"text": "single-form learnable prompts", "label": "Prompting"}, {"text": "Bayesian Prompt Flow Learning", "label": "Zero-shot Learning"}, {"text": "Bayes-PFL", "label": "Zero-shot Learning"}, {"text": "text prompts", "label": "Prompting"}, {"text": "dynamic text embeddings", "label": "Embedding"}]}
{"id": "2503.10081", "submitter": "Joonsung Jeon", "authors": "Joonsung Jeon, Woo Jae Kim, Suhyeon Ha, Sooel Son, Sung-eui Yoon", "title": "AdvPaint: Protecting Images from Inpainting Manipulation via Adversarial\n  Attention Disruption", "comments": "Accepted to ICLR 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The outstanding capability of diffusion models in generating high-quality\nimages poses significant threats when misused by adversaries. In particular, we\nassume malicious adversaries exploiting diffusion models for inpainting tasks,\nsuch as replacing a specific region with a celebrity. While existing methods\nfor protecting images from manipulation in diffusion-based generative models\nhave primarily focused on image-to-image and text-to-image tasks, the challenge\nof preventing unauthorized inpainting has been rarely addressed, often\nresulting in suboptimal protection performance. To mitigate inpainting abuses,\nwe propose ADVPAINT, a novel defensive framework that generates adversarial\nperturbations that effectively disrupt the adversary's inpainting tasks.\nADVPAINT targets the self- and cross-attention blocks in a target diffusion\ninpainting model to distract semantic understanding and prompt interactions\nduring image generation. ADVPAINT also employs a two-stage perturbation\nstrategy, dividing the perturbation region based on an enlarged bounding box\naround the object, enhancing robustness across diverse masks of varying shapes\nand sizes. Our experimental results demonstrate that ADVPAINT's perturbations\nare highly effective in disrupting the adversary's inpainting tasks,\noutperforming existing methods; ADVPAINT attains over a 100-point increase in\nFID and substantial decreases in precision.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 06:05:40 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Jeon", "Joonsung", ""], ["Kim", "Woo Jae", ""], ["Ha", "Suhyeon", ""], ["Son", "Sooel", ""], ["Yoon", "Sung-eui", ""]], "extracted_entities": [{"text": "prompt interactions", "label": "Prompting"}]}
{"id": "2503.10084", "submitter": "Juntai Cao", "authors": "Xiang Zhang, Juntai Cao, Jiaqi Wei, Chenyu You, Dujian Ding", "title": "Why Does Your CoT Prompt (Not) Work? Theoretical Analysis of Prompt\n  Space Complexity, its Interaction with Answer Space During CoT Reasoning with\n  LLMs: A Recurrent Perspective", "comments": "arXiv admin note: substantial text overlap with arXiv:2410.14198", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the remarkable successes of Large Language Models (LLMs), their\nfundamental Transformer architecture possesses inherent theoretical limitations\nthat restrict their capability to handle reasoning tasks with increasing\ncomputational complexity. Chain-of-Thought (CoT) prompting has emerged as a\npractical solution, supported by several theoretical studies. However, current\nCoT-based methods (including ToT, GoT, etc.) generally adopt a\n\"one-prompt-fits-all\" strategy, using fixed templates (e.g., \"think step by\nstep\") across diverse reasoning tasks. This method forces models to navigate an\nextremely complex prompt space to identify effective reasoning paths. The\ncurrent prompt designing research are also heavily relying on trial-and-error\nrather than theoretically informed guidance. In this paper, we provide a\nrigorous theoretical analysis of the complexity and interplay between two\ncrucial spaces: the prompt space (the space of potential prompt structures) and\nthe answer space (the space of reasoning solutions generated by LLMs) in CoT\nreasoning. We demonstrate how reliance on a single universal prompt (e.g. think\nstep by step) can negatively impact the theoretical computability of LLMs,\nillustrating that prompt complexity directly influences the structure and\neffectiveness of the navigation in answer space. Our analysis highlights that\nsometimes human supervision is critical for efficiently navigating the prompt\nspace. We theoretically and empirically show that task-specific prompting\nsignificantly outperforms unsupervised prompt generation, emphasizing the\nnecessity of thoughtful human guidance in CoT prompting.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 06:11:10 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhang", "Xiang", ""], ["Cao", "Juntai", ""], ["Wei", "Jiaqi", ""], ["You", "Chenyu", ""], ["Ding", "Dujian", ""]], "extracted_entities": [{"text": "Large Language Models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "Chain-of-Thought", "label": "Chain of thought"}, {"text": "think step by\nstep", "label": "Prompting"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "CoT", "label": "Chain of thought"}, {"text": "think\nstep by step", "label": "Prompting"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "task-specific prompting", "label": "Prompting"}, {"text": "CoT prompting", "label": "Prompting"}]}
{"id": "2503.10085", "submitter": "Menderes Iskin", "authors": "M. Iskin", "title": "Structure factors and quantum geometry in multiband BCS superconductors", "comments": "10 pages with 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.supr-con cond-mat.quant-gas cond-mat.str-el", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multiband BCS superconductors that exhibit time-reversal symmetry\nand uniform pairing, and analyze their dynamic density and spin structure\nfactors using linear-response theory within the mean-field BCS-BEC crossover\nframework at zero temperature. Our results for the multi-orbital Hubbard model\nsatisfy the associated f-sum rules in several limits. In particular, in the\nstrong-coupling limit, they coincide with those of a weakly-interacting Bose\ngas of Cooper pairs, where the low-energy collective Goldstone modes serve as\nBogoliubov phonons. We further reveal that the quantum-geometric origin of the\nlow-energy structure factors, along with related observables such as the\nsuperfluid-weight tensor and the effective-mass tensor of Cooper pairs, can be\ntraced all the way back to the effective-mass theorem for Bloch bands in this\nlimit. As an illustration, we investigate the pyrochlore-Hubbard model\nnumerically and demonstrate that the Goldstone modes are the only relevant\ncollective degrees of freedom in the flat-band regime.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 06:11:35 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Iskin", "M.", ""]], "extracted_entities": [{"text": "multi-orbital Hubbard model", "label": "AI model"}, {"text": "Cooper pairs", "label": "LLMs"}, {"text": "Goldstone modes", "label": "LLMs"}, {"text": "Cooper pairs", "label": "LLMs"}, {"text": "pyrochlore-Hubbard model", "label": "AI model"}, {"text": "Goldstone modes", "label": "LLMs"}]}
{"id": "2503.10086", "submitter": "Jiajun Deng", "authors": "Jiajun Deng, Yaolong Ju, Jing Yang, Simon Lui, Xunying Liu", "title": "Efficient Adapter Tuning for Joint Singing Voice Beat and Downbeat\n  Tracking with Self-supervised Learning Features", "comments": "Accepted by ISMIR2024", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singing voice beat tracking is a challenging task, due to the lack of musical\naccompaniment that often contains robust rhythmic and harmonic patterns,\nsomething most existing beat tracking systems utilize and can be essential for\nestimating beats. In this paper, a novel temporal convolutional network-based\nbeat-tracking approach featuring self-supervised learning (SSL) representations\nand adapter tuning is proposed to track the beat and downbeat of singing voices\njointly. The SSL DistilHuBERT representations are utilized to capture the\nsemantic information of singing voices and are further fused with the generic\nspectral features to facilitate beat estimation. Sources of variabilities that\nare particularly prominent with the non-homogeneous singing voice data are\nreduced by the efficient adapter tuning. Extensive experiments show that\nfeature fusion and adapter tuning improve the performance individually, and the\ncombination of both leads to significantly better performances than the\nun-adapted baseline system, with up to 31.6% and 42.4% absolute F1-score\nimprovements on beat and downbeat tracking, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 06:28:15 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Deng", "Jiajun", ""], ["Ju", "Yaolong", ""], ["Yang", "Jing", ""], ["Lui", "Simon", ""], ["Liu", "Xunying", ""]], "extracted_entities": [{"text": "adapter tuning", "label": "Fine-tuning"}, {"text": "adapter tuning", "label": "Fine-tuning"}, {"text": "adapter tuning", "label": "Fine-tuning"}]}
{"id": "2503.10091", "submitter": "Chengyu Tao", "authors": "Chengyu Tao, Xuanming Cao, and Juan Du", "title": "G$^{2}$SF-MIAD: Geometry-Guided Score Fusion for Multimodal Industrial\n  Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial quality inspection plays a critical role in modern manufacturing\nby identifying defective products during production. While single-modality\napproaches using either 3D point clouds or 2D RGB images suffer from\ninformation incompleteness, multimodal anomaly detection offers promise through\nthe complementary fusion of crossmodal data. However, existing methods face\nchallenges in effectively integrating unimodal results and improving\ndiscriminative power. To address these limitations, we first reinterpret memory\nbank-based anomaly scores in single modalities as isotropic Euclidean distances\nin local feature spaces. Dynamically evolving from Eulidean metrics, we propose\na novel \\underline{G}eometry-\\underline{G}uided \\underline{S}core\n\\underline{F}usion (G$^{2}$SF) framework that progressively learns an\nanisotropic local distance metric as a unified score for the fusion task.\nThrough a geometric encoding operator, a novel Local Scale Prediction Network\n(LSPN) is proposed to predict direction-aware scaling factors that characterize\nfirst-order local feature distributions, thereby enhancing discrimination\nbetween normal and anomalous patterns. Additionally, we develop specialized\nloss functions and score aggregation strategy from geometric priors to ensure\nboth metric generalization and efficacy. Comprehensive evaluations on the\nMVTec-3D AD dataset demonstrate the state-of-the-art detection performance of\nour method with low positive rate and better recall, which is essential in\nindustrial application, and detailed ablation analysis validates each\ncomponent's contribution.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 06:39:38 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Tao", "Chengyu", ""], ["Cao", "Xuanming", ""], ["Du", "Juan", ""]], "extracted_entities": [{"text": "direction-aware scaling factors", "label": "Scaling law"}]}
{"id": "2503.10093", "submitter": "Qiyuan Deng", "authors": "Qiyuan Deng, Xuefeng Bai, Kehai Chen, Yaowei Wang, Liqiang Nie, Min\n  Zhang", "title": "Representation-based Reward Modeling for Efficient Safety Alignment of\n  Large Language Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning (RL) algorithms for safety alignment of Large Language\nModels (LLMs), such as Direct Preference Optimization (DPO), encounter the\nchallenge of distribution shift. Current approaches typically address this\nissue through online sampling from the target policy, which requires\nsignificant computational resources. In this paper, we hypothesize that during\noff-policy training, while the ranking order of output generated by policy\nchanges, their overall distribution remains relatively stable. This stability\nallows the transformation of the sampling process from the target policy into a\nre-ranking of preference data. Building on this hypothesis, We propose a new\nframework that leverages the model's intrinsic safety judgment capability to\nextract reward signals, which are then used to calculate label confidence for\npreferences reordering. Extensive experimental results and theoretical analysis\ndemonstrate that the proposed method effectively addresses the distribution\nshift issue, remarkably enhancing the safety performance while reducing about\n300x computational overheads.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 06:40:34 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Deng", "Qiyuan", ""], ["Bai", "Xuefeng", ""], ["Chen", "Kehai", ""], ["Wang", "Yaowei", ""], ["Nie", "Liqiang", ""], ["Zhang", "Min", ""]], "extracted_entities": [{"text": "Large Language\nModels", "label": "Large Language Model"}]}
{"id": "2503.10094", "submitter": "Georgios Feretzakis", "authors": "Phoebe Koundouri, Conrad Landis, Georgios Feretzakis", "title": "Semantic Synergy: Unlocking Policy Insights and Learning Pathways\n  Through Advanced Skill Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This research introduces a comprehensive system based on state-of-the-art\nnatural language processing, semantic embedding, and efficient search\ntechniques for retrieving similarities and thus generating actionable insights\nfrom raw textual information. The system automatically extracts and aggregates\nnormalized competencies from multiple documents (such as policy files and\ncurricula vitae) and creates strong relationships between recognized\ncompetencies, occupation profiles, and related learning courses. To validate\nits performance, we conducted a multi-tier evaluation that included both\nexplicit and implicit skill references in synthetic and real-world documents.\nThe results showed near-human-level accuracy, with F1 scores exceeding 0.95 for\nexplicit skill detection and above 0.93 for implicit mentions. The system\nthereby establishes a sound foundation for supporting in-depth collaboration\nacross the AE4RIA network. The methodology involves a multi-stage pipeline\nbased on extensive preprocessing and data cleaning, semantic embedding and\nsegmentation via SentenceTransformer, and skill extraction using a FAISS-based\nsearch method. The extracted skills are associated with occupation frameworks\n(as formulated in the ESCO ontology) and with learning paths offered through\nthe Sustainable Development Goals Academy. Moreover, interactive visualization\nsoftware, implemented with Dash and Plotly, presents graphs and tables for\nreal-time exploration and informed decision-making by those involved in\npolicymaking, training and learning supply, career transitions, and\nrecruitment. Overall, this system, backed by rigorous validation, offers\npromising prospects for improved policymaking, human resource development, and\nlifelong learning by providing structured and actionable insights from raw,\ncomplex textual information.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 06:41:26 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Koundouri", "Phoebe", ""], ["Landis", "Conrad", ""], ["Feretzakis", "Georgios", ""]], "extracted_entities": [{"text": "semantic embedding", "label": "contextual Embedding"}, {"text": "semantic embedding", "label": "contextual Embedding"}]}
{"id": "2503.10095", "submitter": "Avinash Patil", "authors": "Avinash Patil, Amardeep Kour Gedhu", "title": "Cognitive-Mental-LLM: Leveraging Reasoning in Large Language Models for\n  Mental Health Prediction via Online Text", "comments": "8 pages, 4 Figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large Language Models (LLMs) have demonstrated potential in predicting mental\nhealth outcomes from online text, yet traditional classification methods often\nlack interpretability and robustness. This study evaluates structured reasoning\ntechniques-Chain-of-Thought (CoT), Self-Consistency (SC-CoT), and\nTree-of-Thought (ToT)-to improve classification accuracy across multiple mental\nhealth datasets sourced from Reddit. We analyze reasoning-driven prompting\nstrategies, including Zero-shot CoT and Few-shot CoT, using key performance\nmetrics such as Balanced Accuracy, F1 score, and Sensitivity/Specificity. Our\nfindings indicate that reasoning-enhanced techniques improve classification\nperformance over direct prediction, particularly in complex cases. Compared to\nbaselines such as Zero Shot non-CoT Prompting, and fine-tuned pre-trained\ntransformers such as BERT and Mental-RoBerta, and fine-tuned Open Source LLMs\nsuch as Mental Alpaca and Mental-Flan-T5, reasoning-driven LLMs yield notable\ngains on datasets like Dreaddit (+0.52\\% over M-LLM, +0.82\\% over BERT) and\nSDCNL (+4.67\\% over M-LLM, +2.17\\% over BERT). However, performance declines in\nDepression Severity, and CSSRS predictions suggest dataset-specific\nlimitations, likely due to our using a more extensive test set. Among prompting\nstrategies, Few-shot CoT consistently outperforms others, reinforcing the\neffectiveness of reasoning-driven LLMs. Nonetheless, dataset variability\nhighlights challenges in model reliability and interpretability. This study\nprovides a comprehensive benchmark of reasoning-based LLM techniques for mental\nhealth text classification. It offers insights into their potential for\nscalable clinical applications while identifying key challenges for future\nimprovements.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 06:42:37 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Patil", "Avinash", ""], ["Gedhu", "Amardeep Kour", ""]], "extracted_entities": [{"text": "Large Language Models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "Tree-of-Thought", "label": "Chain of thought"}, {"text": "Zero-shot CoT", "label": "Few-shot Learning"}, {"text": "Few-shot CoT", "label": "Few-shot Learning"}, {"text": "Zero Shot non-CoT Prompting", "label": "Prompting"}, {"text": "BERT", "label": "BERT"}, {"text": "Mental-RoBerta", "label": "RoBERTa"}, {"text": "Mental Alpaca", "label": "Open-source LLMs"}, {"text": "BERT", "label": "BERT"}, {"text": "SDCNL", "label": "Large Language Model"}, {"text": "BERT", "label": "BERT"}, {"text": "Few-shot CoT", "label": "Few-shot Learning"}]}
{"id": "2503.10099", "submitter": "Han Liu", "authors": "Lin Ao, Han Liu, Huafeng Zhang", "title": "AgentDAO: Synthesis of Proposal Transactions Via Abstract DAO Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the trend of decentralized governance is obvious (cryptocurrencies and\nblockchains are widely adopted by multiple sovereign countries), initiating\ngovernance proposals within Decentralized Autonomous Organizations (DAOs) is\nstill challenging, i.e., it requires providing a low-level transaction payload,\ntherefore posing significant barriers to broad community participation. To\naddress these challenges, we propose a multi-agent system powered by Large\nLanguage Models with a novel Label-Centric Retrieval algorithm to automate the\ntranslation from natural language inputs into executable proposal transactions.\nThe system incorporates DAOLang, a Domain-Specific Language to simplify the\nspecification of various governance proposals. The key optimization achieved by\nDAOLang is a semantic-aware abstraction of user input that reliably secures\nproposal generation with a low level of token demand. A preliminary evaluation\non real-world applications reflects the potential of DAOLang in terms of\ngenerating complicated types of proposals with existing foundation models, e.g.\nGPT-4o.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 06:52:18 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Ao", "Lin", ""], ["Liu", "Han", ""], ["Zhang", "Huafeng", ""]], "extracted_entities": [{"text": "Large\nLanguage Models", "label": "Large Language Model"}, {"text": "GPT-4o", "label": "GPT"}]}
{"id": "2503.10100", "submitter": "Tianhao Peng", "authors": "Tianhao Peng, Xuhong Li, Haitao Yuan, Yuchen Li, Haoyi Xiong", "title": "SOLA-GCL: Subgraph-Oriented Learnable Augmentation Method for Graph\n  Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph contrastive learning has emerged as a powerful technique for learning\ngraph representations that are robust and discriminative. However, traditional\napproaches often neglect the critical role of subgraph structures, particularly\nthe intra-subgraph characteristics and inter-subgraph relationships, which are\ncrucial for generating informative and diverse contrastive pairs. These\nsubgraph features are crucial as they vary significantly across different graph\ntypes, such as social networks where they represent communities, and\nbiochemical networks where they symbolize molecular interactions. To address\nthis issue, our work proposes a novel subgraph-oriented learnable augmentation\nmethod for graph contrastive learning, termed SOLA-GCL, that centers around\nsubgraphs, taking full advantage of the subgraph information for data\naugmentation. Specifically, SOLA-GCL initially partitions a graph into multiple\ndensely connected subgraphs based on their intrinsic properties. To preserve\nand enhance the unique characteristics inherent to subgraphs, a graph view\ngenerator optimizes augmentation strategies for each subgraph, thereby\ngenerating tailored views for graph contrastive learning. This generator uses a\ncombination of intra-subgraph and inter-subgraph augmentation strategies,\nincluding node dropping, feature masking, intra-edge perturbation, inter-edge\nperturbation, and subgraph swapping. Extensive experiments have been conducted\non various graph learning applications, ranging from social networks to\nmolecules, under semi-supervised learning, unsupervised learning, and transfer\nlearning settings to demonstrate the superiority of our proposed approach over\nthe state-of-the-art in GCL.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 06:52:39 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Peng", "Tianhao", ""], ["Li", "Xuhong", ""], ["Yuan", "Haitao", ""], ["Li", "Yuchen", ""], ["Xiong", "Haoyi", ""]], "extracted_entities": [{"text": "semi-supervised learning", "label": "Few-shot Learning"}, {"text": "unsupervised learning", "label": "Few-shot Learning"}, {"text": "transfer\nlearning", "label": "Few-shot Learning"}]}
{"id": "2503.10101", "submitter": "Byoung Ham", "authors": "Byoung S. Ham", "title": "Sagnac interferometer-based noise-free superresolution using\n  phase-controlled quantum erasers", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interferometer-based precision measurements have been intensively studied for\nsensing and metrology over the last half century. In classical optics, the\nresolution and phase sensitivity of an optical signal are confined by\ndiffraction limit and standard quantum limit (SQL), respectively. Quantum\nentanglement has been adapted to quantum sensing to overcome SQL over the last\ntwo decades. In quantum sensing, the use of higher-order entangled photon pairs\nof N00N states is key to beating diffraction limit and SQL with superresolution\nof the de Broglie wavelength. Recently, coherent light has been demonstrated\nfor the de Broglie wavelength resulting from phase-controlled quantum erasers.\nHere, a Sagnac interferometer-based de Broglie wavelength is proposed for\nnoise-free superresolution to solve the limited scalability in N00N-based\nquantum sensing. For this, the output port of a Sagnac interferometer is\nphase-manipulated for intensity product of multiple quantum erasers. Due to the\npractically unlimited scalability in the phase manipulation using a spatial\nlight modulator, the proposed Sagnac superresolution can be applicable to\nupgrade the state-of-art classical sensors such has grating-based wavelength\nmeters or cavity-based ring laser gyroscope to a quantum version.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 06:53:23 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Ham", "Byoung S.", ""]], "extracted_entities": [{"text": "Quantum\nentanglement", "label": "quantisation"}]}
{"id": "2503.10102", "submitter": "Junhao Wang", "authors": "Junhao Wang", "title": "Geometric Parameter Estimations of Perovskite Solar Cells Based on\n  Optical Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a non-invasive approach to estimate the layer thicknesses\nof perovskite solar cells. The thicknesses are predicted by a convolutional\nneural network that leverages the external quantum efficiency of a perovskite\nsolar cell. The network is trained in thickness ranges where the optical\nproperties are constant, and these ranges set the constraints for the network's\napplication. Due to light sensitivity issues with opaque perovskites, the\nconvolutional neural network showed better performance with transparent\nperovskites. To optimize the performance and reduce the root mean square error,\nwe tried different sampling methods, image specifications, and Bayesian\noptimization for hyperparameter tuning. While sampling methods showed marginal\nimprovement, implementing Bayesian optimization demonstrated high accuracy.\nOther minor optimization attempts include experimenting with input\nspecifications and pre-processing approaches. The results confirm the\nfeasibility, efficiency, and effectiveness of a convolution neural network for\npredicting perovskite solar cells' layer thicknesses based on controlled\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 06:54:12 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Junhao", ""]], "extracted_entities": [{"text": "convolutional\nneural network", "label": "AI model"}, {"text": "external quantum efficiency", "label": "quantisation"}, {"text": "Bayesian\noptimization", "label": "Fine-tuning"}, {"text": "hyperparameter tuning", "label": "Fine-tuning"}, {"text": "convolution neural network", "label": "AI model"}]}
{"id": "2503.10104", "submitter": "Yuheng Liang", "authors": "Yuheng Liang, Zheyu Wang, Feng Liu, Mingzhou Liu, Yu Yao", "title": "Mamba-VA: A Mamba-based Approach for Continuous Emotion Recognition in\n  Valence-Arousal Space", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous Emotion Recognition (CER) plays a crucial role in intelligent\nhuman-computer interaction, mental health monitoring, and autonomous driving.\nEmotion modeling based on the Valence-Arousal (VA) space enables a more nuanced\nrepresentation of emotional states. However, existing methods still face\nchallenges in handling long-term dependencies and capturing complex temporal\ndynamics. To address these issues, this paper proposes a novel emotion\nrecognition model, Mamba-VA, which leverages the Mamba architecture to\nefficiently model sequential emotional variations in video frames. First, the\nmodel employs a Masked Autoencoder (MAE) to extract deep visual features from\nvideo frames, enhancing the robustness of temporal information. Then, a\nTemporal Convolutional Network (TCN) is utilized for temporal modeling to\ncapture local temporal dependencies. Subsequently, Mamba is applied for\nlong-sequence modeling, enabling the learning of global emotional trends.\nFinally, a fully connected (FC) layer performs regression analysis to predict\ncontinuous valence and arousal values. Experimental results on the\nValence-Arousal (VA) Estimation task of the 8th competition on Affective\nBehavior Analysis in-the-wild (ABAW) demonstrate that the proposed model\nachieves valence and arousal scores of 0.5362 (0.5036) and 0.4310 (0.4119) on\nthe validation (test) set, respectively, outperforming the baseline. The source\ncode is available on GitHub:https://github.com/FreedomPuppy77/Charon.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 07:02:07 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Liang", "Yuheng", ""], ["Wang", "Zheyu", ""], ["Liu", "Feng", ""], ["Liu", "Mingzhou", ""], ["Yao", "Yu", ""]], "extracted_entities": [{"text": "GitHub", "label": "Open-source LLMs"}]}
{"id": "2503.10105", "submitter": "Shu-Xun Yang", "authors": "Shu-Xun Yang, Cunxiang Wang, Yidong Wang, Xiaotao Gu, Minlie Huang,\n  Jie Tang", "title": "StepMathAgent: A Step-Wise Agent for Evaluating Mathematical Processes\n  through Tree-of-Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Evaluating mathematical capabilities is critical for assessing the overall\nperformance of large language models (LLMs). However, existing evaluation\nmethods often focus solely on final answers, resulting in highly inaccurate and\nuninterpretable evaluation outcomes, as well as their failure to assess proof\nor open-ended problems. To address these issues, we propose a novel\nmathematical process evaluation agent based on Tree-of-Error, called\nStepMathAgent. This agent incorporates four internal core operations: logical\nstep segmentation, step scoring, score aggregation and error tree generation,\nalong with four external extension modules: difficulty calibration, simplicity\nevaluation, completeness validation and format assessment. Furthermore, we\nintroduce StepMathBench, a benchmark comprising 1,000 step-divided process\nevaluation instances, derived from 200 high-quality math problems grouped by\nproblem type, subject category and difficulty level. Experiments on\nStepMathBench show that our proposed StepMathAgent outperforms all\nstate-of-the-art methods, demonstrating human-aligned evaluation preferences\nand broad applicability to various scenarios. Our data and code are available\nat https://github.com/SHU-XUN/StepMathAgent.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 07:02:53 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Yang", "Shu-Xun", ""], ["Wang", "Cunxiang", ""], ["Wang", "Yidong", ""], ["Gu", "Xiaotao", ""], ["Huang", "Minlie", ""], ["Tang", "Jie", ""]], "extracted_entities": [{"text": "LLMs", "label": "Large Language Model"}]}
{"id": "2503.10106", "submitter": "Omar Anwar Dr", "authors": "Omar Anwar, Brent Groves, Luca Cortese, Adam B. Watts", "title": "GalProTE: Galactic Properties Mapping using Transformer Encoder", "comments": "26 pages, 18 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.GA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents GalProTE, a proof-of-concept Machine Learning model\nutilizing a Transformer Encoder to determine stellar age, metallicity, and dust\nattenuation from optical spectra. Designed for large astronomical surveys,\nGalProTE significantly accelerates processing while maintaining accuracy. Using\nthe E-MILES spectral library, we construct a dataset of 111,936 diverse\ntemplates by expanding 636 simple stellar population models with varying\nextinction, spectral combinations, and noise modifications. This ensures robust\ntraining over 4750 to 7100 Angstrom at 2.5 Angstrom resolution. GalProTE\nemploys four parallel attention-based encoders with varying kernel sizes to\ncapture spectral features. On synthetic test data, it achieves a mean squared\nerror (MSE) of 0.27% between input and predicted spectra. Validation on\nPHANGS-MUSE galaxies NGC4254 and NGC5068 confirms its ability to extract\nphysical parameters efficiently, with residuals averaging -0.02% and 0.28% and\nstandard deviations of 4.3% and 5.3%, respectively. To contextualize these\nresults, we compare GalProTE's age, metallicity, and dust attenuation maps with\npPXF, a state-of-the-art spectral fitting tool. While pPXF requires\napproximately 11 seconds per spectrum, GalProTE processes one in less than 4\nmilliseconds, offering a 2750 times speedup and consuming 68 times less power\nper spectrum. The strong agreement between pPXF and GalProTE highlights the\npotential of machine learning to enhance traditional methods, paving the way\nfor faster, energy-efficient, and scalable analyses of galactic properties in\nmodern surveys.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 07:06:45 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Anwar", "Omar", ""], ["Groves", "Brent", ""], ["Cortese", "Luca", ""], ["Watts", "Adam B.", ""]], "extracted_entities": [{"text": "GalProTE", "label": "AI model"}, {"text": "E-MILES spectral library", "label": "Open-source LLMs"}, {"text": "pPXF", "label": "AI model"}, {"text": "GalProTE", "label": "AI model"}]}
{"id": "2503.10108", "submitter": "Hui-Yun Cao", "authors": "Hui-Yun Cao and Hai-Qing Zhou", "title": "$\\gamma W$-exchange contributions in neutron $\\beta$ decay in the\n  forward-angle limit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "nucl-th", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, the contributions from $\\gamma W$-exchange in neutron $\\beta$\ndecay are estimated at the amplitude level. Using a general form for the\nelectromagnetic (EM) form factors (FFs) of the proton, the EM FFs of the\nneutron, and the weak FFs of the $Wnp$ interaction as inputs, we present\nanalytical expressions for the inner part of the $\\gamma W$-exchange amplitude\nunder the forward angle limit. The differences and relations between our method\nand those used in the references are discussed. To compare our numerical\nresults with those provided in the references, we consider three types of FFs\nas examples. The numerical results show that when the favored EM FFs are used,\nour result for the contribution from the Fermi part is consistent with those\nreported in the references, while our results for the Gamow-Teller parts are\nabout 7\\% and 13\\% larger than those reported in Ref.~\\cite{Hayen-2021}.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 07:08:19 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Cao", "Hui-Yun", ""], ["Zhou", "Hai-Qing", ""]], "extracted_entities": [{"text": "FFs", "label": "LLMs"}]}
{"id": "2503.10109", "submitter": "Xingxin Xu", "authors": "Xingxin Xu, Bing Cao, Yinan Xia, Pengfei Zhu, Qinghua Hu", "title": "Dream-IF: Dynamic Relative EnhAnceMent for Image Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image fusion aims to integrate comprehensive information from images acquired\nthrough multiple sources. However, images captured by diverse sensors often\nencounter various degradations that can negatively affect fusion quality.\nTraditional fusion methods generally treat image enhancement and fusion as\nseparate processes, overlooking the inherent correlation between them; notably,\nthe dominant regions in one modality of a fused image often indicate areas\nwhere the other modality might benefit from enhancement. Inspired by this\nobservation, we introduce the concept of dominant regions for image enhancement\nand present a Dynamic Relative EnhAnceMent framework for Image Fusion\n(Dream-IF). This framework quantifies the relative dominance of each modality\nacross different layers and leverages this information to facilitate reciprocal\ncross-modal enhancement. By integrating the relative dominance derived from\nimage fusion, our approach supports not only image restoration but also a\nbroader range of image enhancement applications. Furthermore, we employ\nprompt-based encoding to capture degradation-specific details, which\ndynamically steer the restoration process and promote coordinated enhancement\nin both multi-modal image fusion and image enhancement scenarios. Extensive\nexperimental results demonstrate that Dream-IF consistently outperforms its\ncounterparts.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 07:08:35 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Xu", "Xingxin", ""], ["Cao", "Bing", ""], ["Xia", "Yinan", ""], ["Zhu", "Pengfei", ""], ["Hu", "Qinghua", ""]], "extracted_entities": [{"text": "prompt-based encoding", "label": "Prompting"}]}
{"id": "2503.10112", "submitter": "Yanfeng Li", "authors": "Yanfeng Li and Kahou Chan and Yue Sun and Chantong Lam and Tong Tong\n  and Zitong Yu and Keren Fu and Xiaohong Liu and Tao Tan", "title": "MoEdit: On Learning Quantity Perception for Multi-object Image Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-object images are prevalent in various real-world scenarios, including\naugmented reality, advertisement design, and medical imaging. Efficient and\nprecise editing of these images is critical for these applications. With the\nadvent of Stable Diffusion (SD), high-quality image generation and editing have\nentered a new era. However, existing methods often struggle to consider each\nobject both individually and part of the whole image editing, both of which are\ncrucial for ensuring consistent quantity perception, resulting in suboptimal\nperceptual performance. To address these challenges, we propose MoEdit, an\nauxiliary-free multi-object image editing framework. MoEdit facilitates\nhigh-quality multi-object image editing in terms of style transfer, object\nreinvention, and background regeneration, while ensuring consistent quantity\nperception between inputs and outputs, even with a large number of objects. To\nachieve this, we introduce the Feature Compensation (FeCom) module, which\nensures the distinction and separability of each object attribute by minimizing\nthe in-between interlacing. Additionally, we present the Quantity Attention\n(QTTN) module, which perceives and preserves quantity consistency by effective\ncontrol in editing, without relying on auxiliary tools. By leveraging the SD\nmodel, MoEdit enables customized preservation and modification of specific\nconcepts in inputs with high quality. Experimental results demonstrate that our\nMoEdit achieves State-Of-The-Art (SOTA) performance in multi-object image\nediting. Data and codes will be available at\nhttps://github.com/Tear-kitty/MoEdit.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 07:13:54 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Yanfeng", ""], ["Chan", "Kahou", ""], ["Sun", "Yue", ""], ["Lam", "Chantong", ""], ["Tong", "Tong", ""], ["Yu", "Zitong", ""], ["Fu", "Keren", ""], ["Liu", "Xiaohong", ""], ["Tan", "Tao", ""]], "extracted_entities": [{"text": "Quantity Attention", "label": "Attention mechanism"}]}
{"id": "2503.10120", "submitter": "Bingchen Li", "authors": "Bingchen Li, Xin Li, Yiting Lu, Zhibo Chen", "title": "Hybrid Agents for Image Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Image Restoration (IR) studies typically focus on task-specific or\nuniversal modes individually, relying on the mode selection of users and\nlacking the cooperation between multiple task-specific/universal restoration\nmodes. This leads to insufficient interaction for unprofessional users and\nlimits their restoration capability for complicated real-world applications. In\nthis work, we present HybridAgent, intending to incorporate multiple\nrestoration modes into a unified image restoration model and achieve\nintelligent and efficient user interaction through our proposed hybrid agents.\nConcretely, we propose the hybrid rule of fast, slow, and feedback restoration\nagents. Here, the slow restoration agent optimizes the powerful multimodal\nlarge language model (MLLM) with our proposed instruction-tuning dataset to\nidentify degradations within images with ambiguous user prompts and invokes\nproper restoration tools accordingly. The fast restoration agent is designed\nbased on a lightweight large language model (LLM) via in-context learning to\nunderstand the user prompts with simple and clear requirements, which can\nobviate the unnecessary time/resource costs of MLLM. Moreover, we introduce the\nmixed distortion removal mode for our HybridAgents, which is crucial but not\nconcerned in previous agent-based works. It can effectively prevent the error\npropagation of step-by-step image restoration and largely improve the\nefficiency of the agent system. We validate the effectiveness of HybridAgent\nwith both synthetic and real-world IR tasks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 07:28:33 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Bingchen", ""], ["Li", "Xin", ""], ["Lu", "Yiting", ""], ["Chen", "Zhibo", ""]], "extracted_entities": [{"text": "HybridAgent", "label": "LLM-based"}, {"text": "MLLM", "label": "Large Language Model"}, {"text": "user prompts", "label": "Prompting"}, {"text": "lightweight large language model", "label": "Large Language Model"}, {"text": "LLM", "label": "LLM"}, {"text": "in-context learning", "label": "Few-shot Learning"}, {"text": "user prompts", "label": "Prompting"}, {"text": "MLLM", "label": "Large Language Model"}, {"text": "HybridAgents", "label": "LLM-powered"}, {"text": "HybridAgent", "label": "LLM-based"}]}
{"id": "2503.10121", "submitter": "Fernando Moreno", "authors": "Fernando Moreno and Emmanuel Jehin", "title": "Dust shells and dark linear structures on dust tails of historical and\n  recent long-period comets", "comments": "Accepted by A&A", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.EP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Context. Dust halos or shells, along with linear dark structures along the\naxes of dust tails, are commonly observed in many long-period comets near\nperihelion. Examples range from the recent C/2023 A3 (Tsuchinshan-ATLAS) to\nhistorical comets such as the Great Comet of 1874, C/1874 H1 (Coggia).\n  Aims. While dust halos can readily be modeled as spin-modulated activity\noriginating from the comet nucleus, their possible connection to those dark\nlinear features has, to our knowledge, not been investigated. The aim of this\npaper is to shed light on the formation of these remarkable structures by\nmodeling a sample of six long-period comets, using similar dust physical\nproperties and ejection parameters, to explore whether they share a common\norigin.\n  Methods. To model the dust features, we employed a Monte Carlo procedure to\ngenerate synthetic images. The particles ejected from the comet nucleus follow\na power-law size distribution and are released into interplanetary space at\nspeeds determined by the ratio of solar radiation pressure to solar gravity,\nthe heliocentric distance, and, as a new feature of the code, the solar zenith\nangle at the emission point.\n  Results. We demonstrate that, in all the cases analyzed, the dust shells form\nas a result of short-term events characterized by cyclically varying ejection\nof very small particles from large surface areas on the rotating nucleus. These\nevents are triggered as these areas become freshly exposed to solar radiation\nnear perihelion due to the high obliquity of the spin axes of their nuclei. The\ndark linear stripes along the tail axes may arise from a specific dependence of\nthe ejection speeds on the square root of the cosine of the zenith angle, as is\npredicted by hydrodynamical modeling, but their presence is also dependent on\nthe extent of the latitude region of emission that defines the velocity vector\nfield.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 07:28:37 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Moreno", "Fernando", ""], ["Jehin", "Emmanuel", ""]], "extracted_entities": [{"text": "power-law size distribution", "label": "Scaling law"}]}
{"id": "2503.10123", "submitter": "Zhi-Bo Chen", "authors": "Zhi-Bo Chen and Shao-Ming Fei", "title": "On Sufficient and Necessary Criteria of Multipartite Quantum\n  Entanglement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph math-ph math.MP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Based on the generalized Bloch representation, we study the separability and\ngenuine multipartite entanglement of arbitrary dimensional multipartite quantum\nstates. Some sufficient and some necessary criteria are presented. For certain\nstates, these criteria together are both sufficient and necessary. Detailed\nexamples show that our criteria are better than some existing ones in\nidentifying entanglement. Based on these criteria, the largest separable ball\naround the maximally mixed state for arbitrary multi-qubit systems is found,\nand it is proved that its radius is the constant 1. Furthermore, the criteria\nin this paper can be implemented experimentally.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 07:31:05 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chen", "Zhi-Bo", ""], ["Fei", "Shao-Ming", ""]], "extracted_entities": [{"text": "multipartite entanglement", "label": "quantisation"}]}
{"id": "2503.10125", "submitter": "Yi Wu", "authors": "Yi Wu, Lingting Zhu, Lei Liu, Wandi Qiao, Ziqiang Li, Lequan Yu, Bin\n  Li", "title": "Proxy-Tuning: Tailoring Multimodal Autoregressive Models for\n  Subject-Driven Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal autoregressive (AR) models, based on next-token prediction and\ntransformer architecture, have demonstrated remarkable capabilities in various\nmultimodal tasks including text-to-image (T2I) generation. Despite their strong\nperformance in general T2I tasks, our research reveals that these models\ninitially struggle with subject-driven image generation compared to dominant\ndiffusion models. To address this limitation, we introduce Proxy-Tuning,\nleveraging diffusion models to enhance AR models' capabilities in\nsubject-specific image generation. Our method reveals a striking weak-to-strong\nphenomenon: fine-tuned AR models consistently outperform their diffusion model\nsupervisors in both subject fidelity and prompt adherence. We analyze this\nperformance shift and identify scenarios where AR models excel, particularly in\nmulti-subject compositions and contextual understanding. This work not only\ndemonstrates impressive results in subject-driven AR image generation, but also\nunveils the potential of weak-to-strong generalization in the image generation\ndomain, contributing to a deeper understanding of different architectures'\nstrengths and limitations.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 07:32:57 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wu", "Yi", ""], ["Zhu", "Lingting", ""], ["Liu", "Lei", ""], ["Qiao", "Wandi", ""], ["Li", "Ziqiang", ""], ["Yu", "Lequan", ""], ["Li", "Bin", ""]], "extracted_entities": [{"text": "Proxy-Tuning", "label": "Fine-tuning"}, {"text": "prompt adherence", "label": "Prompting"}, {"text": "contextual understanding", "label": "contextual Embedding"}]}
{"id": "2503.10126", "submitter": "Keita Kume", "authors": "Satoshi Shoji, Wataru Yata, Keita Kume, Isao Yamada", "title": "An LiGME Regularizer of Designated Isolated Minimizers -- An Application\n  to Discrete-Valued Signal Estimation", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a regularized least squares estimation of discrete-valued signals, we\npropose an LiGME regularizer, as a nonconvex regularizer, of designated\nisolated minimizers. The proposed regularizer is designed as a Generalized\nMoreau Enhancement (GME) of the so-called SOAV convex regularizer. Every\ncandidate vector in the discrete-valued set is aimed to be assigned to an\nisolated local minimizer of the proposed regularizer while the overall\nconvexity of the regularized least squares model is maintained. Moreover, a\nglobal minimizer of the proposed model can be approximated iteratively by using\na variant of cLiGME algorithm. To enhance the accuracy of the proposed\nestimation, we also propose a pair of simple modifications, called respectively\nan iterative reweighting and a generalized superiorization. Numerical\nexperiments demonstrate the effectiveness of the proposed model and algorithms\nin a scenario of MIMO signal detection.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 07:34:54 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Shoji", "Satoshi", ""], ["Yata", "Wataru", ""], ["Kume", "Keita", ""], ["Yamada", "Isao", ""]], "extracted_entities": [{"text": "iterative reweighting", "label": "Fine-tuning"}]}
{"id": "2503.10127", "submitter": "Runze He", "authors": "Runze He, Bo Cheng, Yuhang Ma, Qingxiang Jia, Shanyuan Liu, Ao Ma,\n  Xiaoyu Wu, Liebucha Wu, Dawei Leng, Yuhui Yin", "title": "PlanGen: Towards Unified Layout Planning and Image Generation in\n  Auto-Regressive Vision Language Models", "comments": "15 pages, 12 figures, project page:\n  https://360cvgroup.github.io/PlanGen", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a unified layout planning and image generation\nmodel, PlanGen, which can pre-plan spatial layout conditions before generating\nimages. Unlike previous diffusion-based models that treat layout planning and\nlayout-to-image as two separate models, PlanGen jointly models the two tasks\ninto one autoregressive transformer using only next-token prediction. PlanGen\nintegrates layout conditions into the model as context without requiring\nspecialized encoding of local captions and bounding box coordinates, which\nprovides significant advantages over the previous embed-and-pool operations on\nlayout conditions, particularly when dealing with complex layouts. Unified\nprompting allows PlanGen to perform multitasking training related to layout,\nincluding layout planning, layout-to-image generation, image layout\nunderstanding, etc. In addition, PlanGen can be seamlessly expanded to\nlayout-guided image manipulation thanks to the well-designed modeling, with\nteacher-forcing content manipulation policy and negative layout guidance.\nExtensive experiments verify the effectiveness of our PlanGen in multiple\nlayoutrelated tasks, showing its great potential. Code is available at:\nhttps://360cvgroup.github.io/PlanGen.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 07:37:09 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["He", "Runze", ""], ["Cheng", "Bo", ""], ["Ma", "Yuhang", ""], ["Jia", "Qingxiang", ""], ["Liu", "Shanyuan", ""], ["Ma", "Ao", ""], ["Wu", "Xiaoyu", ""], ["Wu", "Liebucha", ""], ["Leng", "Dawei", ""], ["Yin", "Yuhui", ""]], "extracted_entities": [{"text": "Unified\nprompting", "label": "Prompting"}]}
{"id": "2503.10129", "submitter": "Namal Jayasuriya", "authors": "Namal Jayasuriya, Yi Guo, Wen Hu, Oula Ghannoum", "title": "Deep Learning-Based Direct Leaf Area Estimation using Two RGBD Datasets\n  for Model Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Estimation of a single leaf area can be a measure of crop growth and a\nphenotypic trait to breed new varieties. It has also been used to measure leaf\narea index and total leaf area. Some studies have used hand-held cameras, image\nprocessing 3D reconstruction and unsupervised learning-based methods to\nestimate the leaf area in plant images. Deep learning works well for object\ndetection and segmentation tasks; however, direct area estimation of objects\nhas not been explored. This work investigates deep learning-based leaf area\nestimation, for RGBD images taken using a mobile camera setup in real-world\nscenarios. A dataset for attached leaves captured with a top angle view and a\ndataset for detached single leaves were collected for model development and\ntesting. First, image processing-based area estimation was tested on manually\nsegmented leaves. Then a Mask R-CNN-based model was investigated, and modified\nto accept RGBD images and to estimate the leaf area. The detached-leaf data set\nwas then mixed with the attached-leaf plant data set to estimate the single\nleaf area for plant images, and another network design with two backbones was\nproposed: one for segmentation and the other for area estimation. Instead of\ntrying all possibilities or random values, an agile approach was used in\nhyperparameter tuning. The final model was cross-validated with 5-folds and\ntested with two unseen datasets: detached and attached leaves. The F1 score\nwith 90% IoA for segmentation result on unseen detached-leaf data was 1.0,\nwhile R-squared of area estimation was 0.81. For unseen plant data\nsegmentation, the F1 score with 90% IoA was 0.59, while the R-squared score was\n0.57. The research suggests using attached leaves with ground truth area to\nimprove the results.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 07:39:09 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Jayasuriya", "Namal", ""], ["Guo", "Yi", ""], ["Hu", "Wen", ""], ["Ghannoum", "Oula", ""]], "extracted_entities": [{"text": "hyperparameter tuning", "label": "Fine-tuning"}]}
{"id": "2503.10133", "submitter": "Vojtech Neuman", "authors": "Vojtech Neuman, Miloslav Capek, Lukas Jelinek", "title": "Towards Manufacturing-Friendly Shapes in Discrete Topology Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NA math.IT math.NA", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper deals with shape irregularity issues in discrete topology\noptimization algorithms whereby the design is created using the automated\ndistribution of material in the design region. Graph theory is employed to\nderive appropriate regularity measures for any discrete optimization algorithm.\nShape regularity is quantified by scalar figures ready to evaluate design\nchoices in the form of Pareto-frontiers. Developed metrics deal with\ninformation concerning material usage, problematic distribution, and features\nthat complicate manufacturing. The theory is verified by several examples\ndemonstrating the treatment of isolated islands of materials, point connections\nbetween material segments, or homogeneity.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 07:47:30 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Neuman", "Vojtech", ""], ["Capek", "Miloslav", ""], ["Jelinek", "Lukas", ""]], "extracted_entities": [{"text": "Pareto-frontiers", "label": "Scaling law"}]}
{"id": "2503.10135", "submitter": "Jinze Li", "authors": "Jinze Li, Yixing Xu, Haiduo Huang, Xuanwu Yin, Dong Li, Edith C.H.\n  Ngai, Emad Barsoum", "title": "Gumiho: A Hybrid Architecture to Prioritize Early Tokens in Speculative\n  Decoding", "comments": "Paper under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Speculative decoding (SPD) aims to accelerate the auto-regressive token\ngeneration process of a target Large Language Model (LLM). Some approaches\nemploy a draft model with multiple heads to predict a sequence of future\ntokens, where each head handles a token in the sequence. The target LLM\nverifies the predicted sequence and accepts aligned tokens, enabling efficient\nmulti-token generation. However, existing methods assume that all tokens within\na sequence are equally important, employing identical head structures and\nrelying on a single-generation paradigm, either serial or parallel. To this\nend, we theoretically demonstrate that initial tokens in the draft sequence are\nmore important than later ones. Building on this insight, we propose Gumiho, a\nhybrid model combining serial and parallel heads. Specifically, given the\ncritical importance of early tokens, we employ a sophisticated Transformer\narchitecture for the early draft heads in a serial configuration to improve\naccuracy. For later tokens, we utilize multiple lightweight MLP heads operating\nin parallel to enhance efficiency. By allocating more advanced model structures\nand longer running times to the early heads, Gumiho achieves improved overall\nperformance. The experimental results demonstrate that our method outperforms\nexisting approaches, fully validating its effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 07:55:38 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Jinze", ""], ["Xu", "Yixing", ""], ["Huang", "Haiduo", ""], ["Yin", "Xuanwu", ""], ["Li", "Dong", ""], ["Ngai", "Edith C. H.", ""], ["Barsoum", "Emad", ""]], "extracted_entities": [{"text": "target Large Language Model", "label": "Large Language Model"}, {"text": "LLM", "label": "Large Language Model"}, {"text": "LLM", "label": "Large Language Model"}]}
{"id": "2503.10140", "submitter": "Roberto Baldoni Prof", "authors": "Roberto Baldoni and Giuseppe Di Luna", "title": "Sovereignty in the digital era: the quest for continuous access to\n  dependable technological capabilities", "comments": null, "journal-ref": "IEEE Secur. Priv. 23(1): 91-96 (2025)", "doi": "10.1109/MSEC.2024.3500192", "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In an era where economies and societies are deeply integrated into\ncyberspace, achieving a robust level of digital sovereignty has become an\nessential goal for nations aiming to preserve their security and strategic\npolitical autonomy, particularly during turbulent geopolitical times marked by\ncomplex global supply chains of critical technologies that ties systemic\nrivals. Digital sovereignty is a multifaceted, interdisciplinary, and dynamic\npursuit that fundamentally relies on a nation's ability to have continuous\naccess to dependable technological capabilities (CTCs) for storing,\ntransferring, and processing domestically produced data. This paper identifies\nhow access continuity or technological dependability could be threatened by\nseveral malicious actions from cyberattacks, supply chain tamperings, political\nor economic actions. By examining different approaches adopted by countries\nlike the United States, China, and the European Union, we highlight different\nstrategies to get access to CTCs depending on their political, economic and\ninstitutional nature.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 07:58:08 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Baldoni", "Roberto", ""], ["Di Luna", "Giuseppe", ""]], "extracted_entities": [{"text": "CTCs", "label": "LLMs"}, {"text": "CTCs", "label": "LLMs"}]}
{"id": "2503.10143", "submitter": "Jinfeng Liu", "authors": "Jinfeng Liu and Lingtong Kong and Bo Li and Dan Xu", "title": "GaussHDR: High Dynamic Range Gaussian Splatting via Learning Unified 3D\n  and 2D Local Tone Mapping", "comments": "This paper is accepted by CVPR 2025. Project page is available at\n  https://liujf1226.github.io/GaussHDR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dynamic range (HDR) novel view synthesis (NVS) aims to reconstruct HDR\nscenes by leveraging multi-view low dynamic range (LDR) images captured at\ndifferent exposure levels. Current training paradigms with 3D tone mapping\noften result in unstable HDR reconstruction, while training with 2D tone\nmapping reduces the model's capacity to fit LDR images. Additionally, the\nglobal tone mapper used in existing methods can impede the learning of both HDR\nand LDR representations. To address these challenges, we present GaussHDR,\nwhich unifies 3D and 2D local tone mapping through 3D Gaussian splatting.\nSpecifically, we design a residual local tone mapper for both 3D and 2D tone\nmapping that accepts an additional context feature as input. We then propose\ncombining the dual LDR rendering results from both 3D and 2D local tone mapping\nat the loss level. Finally, recognizing that different scenes may exhibit\nvarying balances between the dual results, we introduce uncertainty learning\nand use the uncertainties for adaptive modulation. Extensive experiments\ndemonstrate that GaussHDR significantly outperforms state-of-the-art methods in\nboth synthetic and real-world scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 08:07:43 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Liu", "Jinfeng", ""], ["Kong", "Lingtong", ""], ["Li", "Bo", ""], ["Xu", "Dan", ""]], "extracted_entities": [{"text": "uncertainty learning", "label": "Few-shot Learning"}]}
{"id": "2503.10148", "submitter": "Jialin Zhu", "authors": "Jialin Zhu, Jiangbei Yue, Feixiang He, He Wang", "title": "3D Student Splatting and Scooping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, 3D Gaussian Splatting (3DGS) provides a new framework for novel\nview synthesis, and has spiked a new wave of research in neural rendering and\nrelated applications. As 3DGS is becoming a foundational component of many\nmodels, any improvement on 3DGS itself can bring huge benefits. To this end, we\naim to improve the fundamental paradigm and formulation of 3DGS. We argue that\nas an unnormalized mixture model, it needs to be neither Gaussians nor\nsplatting. We subsequently propose a new mixture model consisting of flexible\nStudent's t distributions, with both positive (splatting) and negative\n(scooping) densities. We name our model Student Splatting and Scooping, or SSS.\nWhen providing better expressivity, SSS also poses new challenges in learning.\nTherefore, we also propose a new principled sampling approach for optimization.\nThrough exhaustive evaluation and comparison, across multiple datasets,\nsettings, and metrics, we demonstrate that SSS outperforms existing methods in\nterms of quality and parameter efficiency, e.g. achieving matching or better\nquality with similar numbers of components, and obtaining comparable results\nwhile reducing the component number by as much as 82%.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 08:20:54 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhu", "Jialin", ""], ["Yue", "Jiangbei", ""], ["He", "Feixiang", ""], ["Wang", "He", ""]], "extracted_entities": [{"text": "3D Gaussian Splatting", "label": "Foundation Model"}, {"text": "3DGS", "label": "Foundation Model"}, {"text": "3DGS", "label": "Foundation Model"}, {"text": "3DGS", "label": "Foundation Model"}, {"text": "3DGS", "label": "Foundation Model"}, {"text": "Student Splatting and Scooping", "label": "Foundation Model"}]}
{"id": "2503.10149", "submitter": "Zhenxuan Zeng", "authors": "Zhenxuan Zeng, Qiao Wu, Xiyu Zhang, Lin Yuanbo Wu, Pei An, Jiaqi Yang,\n  Ji Wang, Peng Wang", "title": "Unlocking Generalization Power in LiDAR Point Cloud Registration", "comments": "Accepted by CVPR 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world environments, a LiDAR point cloud registration method with\nrobust generalization capabilities (across varying distances and datasets) is\ncrucial for ensuring safety in autonomous driving and other LiDAR-based\napplications. However, current methods fall short in achieving this level of\ngeneralization. To address these limitations, we propose UGP, a pruned\nframework designed to enhance generalization power for LiDAR point cloud\nregistration. The core insight in UGP is the elimination of cross-attention\nmechanisms to improve generalization, allowing the network to concentrate on\nintra-frame feature extraction. Additionally, we introduce a progressive\nself-attention module to reduce ambiguity in large-scale scenes and integrate\nBird's Eye View (BEV) features to incorporate semantic information about scene\nelements. Together, these enhancements significantly boost the network's\ngeneralization performance. We validated our approach through various\ngeneralization experiments in multiple outdoor scenes. In cross-distance\ngeneralization experiments on KITTI and nuScenes, UGP achieved state-of-the-art\nmean Registration Recall rates of 94.5% and 91.4%, respectively. In\ncross-dataset generalization from nuScenes to KITTI, UGP achieved a\nstate-of-the-art mean Registration Recall of 90.9%. Code will be available at\nhttps://github.com/peakpang/UGP.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 08:20:59 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zeng", "Zhenxuan", ""], ["Wu", "Qiao", ""], ["Zhang", "Xiyu", ""], ["Wu", "Lin Yuanbo", ""], ["An", "Pei", ""], ["Yang", "Jiaqi", ""], ["Wang", "Ji", ""], ["Wang", "Peng", ""]], "extracted_entities": [{"text": "cross-attention\nmechanisms", "label": "Attention mechanism"}]}
{"id": "2503.10150", "submitter": "Haoyu Huang", "authors": "Haoyu Huang, Yongfeng Huang, Junjie Yang, Zhenyu Pan, Yongqiang Chen,\n  Kaili Ma, Hongzhi Chen, James Cheng", "title": "Retrieval-Augmented Generation with Hierarchical Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph-based Retrieval-Augmented Generation (RAG) methods have significantly\nenhanced the performance of large language models (LLMs) in domain-specific\ntasks. However, existing RAG methods do not adequately utilize the naturally\ninherent hierarchical knowledge in human cognition, which limits the\ncapabilities of RAG systems. In this paper, we introduce a new RAG approach,\ncalled HiRAG, which utilizes hierarchical knowledge to enhance the semantic\nunderstanding and structure capturing capabilities of RAG systems in the\nindexing and retrieval processes. Our extensive experiments demonstrate that\nHiRAG achieves significant performance improvements over the state-of-the-art\nbaseline methods. The code of our proposed method is available at\n\\href{https://github.com/hhy-huang/HiRAG}{https://github.com/hhy-huang/HiRAG}.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 08:22:31 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Huang", "Haoyu", ""], ["Huang", "Yongfeng", ""], ["Yang", "Junjie", ""], ["Pan", "Zhenyu", ""], ["Chen", "Yongqiang", ""], ["Ma", "Kaili", ""], ["Chen", "Hongzhi", ""], ["Cheng", "James", ""]], "extracted_entities": [{"text": "large language models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "HiRAG", "label": "RAG"}, {"text": "HiRAG", "label": "RAG"}]}
{"id": "2503.10152", "submitter": "Shenghao Fu", "authors": "Shenghao Fu, Junkai Yan, Qize Yang, Xihan Wei, Xiaohua Xie, Wei-Shi\n  Zheng", "title": "A Hierarchical Semantic Distillation Framework for Open-Vocabulary\n  Object Detection", "comments": "Accepted to TMM 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-vocabulary object detection (OVD) aims to detect objects beyond the\ntraining annotations, where detectors are usually aligned to a pre-trained\nvision-language model, eg, CLIP, to inherit its generalizable recognition\nability so that detectors can recognize new or novel objects. However, previous\nworks directly align the feature space with CLIP and fail to learn the semantic\nknowledge effectively. In this work, we propose a hierarchical semantic\ndistillation framework named HD-OVD to construct a comprehensive distillation\nprocess, which exploits generalizable knowledge from the CLIP model in three\naspects. In the first hierarchy of HD-OVD, the detector learns fine-grained\ninstance-wise semantics from the CLIP image encoder by modeling relations among\nsingle objects in the visual space. Besides, we introduce text space\nnovel-class-aware classification to help the detector assimilate the highly\ngeneralizable class-wise semantics from the CLIP text encoder, representing the\nsecond hierarchy. Lastly, abundant image-wise semantics containing multi-object\nand their contexts are also distilled by an image-wise contrastive\ndistillation. Benefiting from the elaborated semantic distillation in triple\nhierarchies, our HD-OVD inherits generalizable recognition ability from CLIP in\ninstance, class, and image levels. Thus, we boost the novel AP on the OV-COCO\ndataset to 46.4% with a ResNet50 backbone, which outperforms others by a clear\nmargin. We also conduct extensive ablation studies to analyze how each\ncomponent works.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 08:27:18 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Fu", "Shenghao", ""], ["Yan", "Junkai", ""], ["Yang", "Qize", ""], ["Wei", "Xihan", ""], ["Xie", "Xiaohua", ""], ["Zheng", "Wei-Shi", ""]], "extracted_entities": [{"text": "image-wise contrastive\ndistillation", "label": "Knowledge distillation"}]}
{"id": "2503.10154", "submitter": "Junghyo Jo", "authors": "Yechan Lim, Sangwon Lee, Junghyo Jo", "title": "Data augmentation using diffusion models to enhance inverse Ising\n  inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying model parameters from observed configurations poses a fundamental\nchallenge in data science, especially with limited data. Recently, diffusion\nmodels have emerged as a novel paradigm in generative machine learning, capable\nof producing new samples that closely mimic observed data. These models learn\nthe gradient of model probabilities, bypassing the need for cumbersome\ncalculations of partition functions across all possible configurations. We\nexplore whether diffusion models can enhance parameter inference by augmenting\nsmall datasets. Our findings demonstrate this potential through a synthetic\ntask involving inverse Ising inference and a real-world application of\nreconstructing missing values in neural activity data. This study serves as a\nproof-of-concept for using diffusion models for data augmentation in\nphysics-related problems, thereby opening new avenues in data science.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 08:29:17 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Lim", "Yechan", ""], ["Lee", "Sangwon", ""], ["Jo", "Junghyo", ""]], "extracted_entities": [{"text": "generative machine learning", "label": "Few-shot Learning"}]}
{"id": "2503.10157", "submitter": "Liang Zheng", "authors": "Aogui Zhang, Xinye Peng, Liang Zheng", "title": "Exploring the multiplicity dependence of the flavor hierarchy for hadron\n  productions in high energy pp collisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ph nucl-th", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we perform a systematic study on the multiplicity dependence of\nhadron productions at mid-rapidity ($|y|<0.5$), ranging from the light to the\ncharm sector in pp collisions at $\\sqrt{s}=13$ TeV. This study utilizes a\nmulti-phase transport model (AMPT) coupled with PYTHIA8 initial conditions. We\nhave investigated the baryon to meson ratios as well as the strange to\nnon-strange meson ratios varying with the charged particle density. By tuning\nthe coalescence parameters, the AMPT model provides a reasonable description to\nthe experimental data for inclusive productions of both light and charm\nhadrons, comparable to the string fragmentation model calculations with color\nreconnection effects. Additionally, we have analyzed the relative production of\nhadrons by examining self-normalized particle ratios as a function of the\ncharged hadron density. Our findings suggest that parton evolution effects and\nthe coalescence hadronization process in AMPT model lead to a strong flavor\nhierarchy in the multiplicity dependence of the baryon to meson ratio.\nFurthermore, our investigation on the $p_T$ differential double ratio of baryon\nto meson fraction between high and low multiplicity events indicates distinct\nmodifications to the flavor associated baryon to meson ratio $p_T$ shape in\nhigh multiplicity events when comparing the coalescence hadronization model to\nthe color reconnection model. These observations highlight the importance of\nunderstanding the hadronization process in high-energy proton-proton collisions\nthrough a comprehensive multiplicity dependent multi-flavor analysis.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 08:35:12 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhang", "Aogui", ""], ["Peng", "Xinye", ""], ["Zheng", "Liang", ""]], "extracted_entities": [{"text": "tuning", "label": "Fine-tuning"}, {"text": "AMPT", "label": "AI model"}, {"text": "AMPT", "label": "AI model"}]}
{"id": "2503.10159", "submitter": "Wei Liu", "authors": "Jingwei Wang, Yuntian Chen, and Wei Liu", "title": "Categorize coalescing quasi-normal modes through far-field scattering\n  patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Resonances in the form of quasi-normal modes (QNMs) for open scattering\nsystems can be generally identified in the far field through peaks of\nscattering spectra (\\textit{e.g.} cross sections of scattering, extinction and\nabsorption). Nevertheless, when the resonant frequencies of different QNMs are\nspectrally overlapped or sufficiently close, the scattering peaks merge and it\nthen becomes extremely challenging to reveal the mode constituents underlying\nthe scattering spectra solely in the far field. Here in this work, we study\nopen systems scattering electromagnetic plane waves, and reveal that spectrally\nclose or even overlapped QNMs can be selectively excited through tuning\nincident directions and polarizations. Such a far-field technique can be\nfurther applied to categorize the nature of degenerate QNMs sharing identical\ncomplex eigenfrequencies (coalescent eigenvalues): at effectively Hermitian\ndegeneracies (conical or Dirac points), the eigenvectors are not coalescent and\nthus the QNMs can still be selectively excited, producing distinct scattering\npatterns; while for non-Hermitian degeneracies (exceptional points),\neigenvectors also coalesce and thus selective QNM excitation does not exist,\nleading to invariant scattering patterns. Our technique sheds new light on the\nborderlands of Mie theory, QNMs, non-Hermitian photonics and singular optics,\nwhich can empower new explorations and applications and cross-fertilize all\nthose disciplines.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 08:36:27 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Jingwei", ""], ["Chen", "Yuntian", ""], ["Liu", "Wei", ""]], "extracted_entities": [{"text": "QNMs", "label": "LLMs"}, {"text": "QNMs", "label": "LLMs"}, {"text": "QNMs", "label": "LLMs"}, {"text": "QNMs", "label": "LLMs"}, {"text": "Hermitian\ndegeneracies", "label": "LLMs"}, {"text": "QNMs", "label": "LLMs"}, {"text": "non-Hermitian degeneracies", "label": "LLMs"}, {"text": "QNMs", "label": "LLMs"}]}
{"id": "2503.10164", "submitter": "Zihan Liu", "authors": "Zihan Liu, Yuan-Hua Ni", "title": "Safety Control of Impulsive Systems with Control Barrier Functions and\n  Adaptive Gains", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the safety challenges in impulsive systems, where abrupt\nstate jumps introduce significant complexities into system dynamics. A unified\nframework is proposed by integrating Quadratic Programming (QP), Control\nBarrier Functions (CBFs), and adaptive gain mechanisms to ensure system safety\nduring impulsive events. The CBFs are constructed to enforce safety constraints\nby capturing the system's continuous dynamics and the effects of impulsive\nstate transitions. An adaptive gain mechanism dynamically adjusts control\ninputs based on the magnitudes of the impulses and the system's proximity to\nsafety boundaries, maintaining safety during instantaneous state jumps. A\ntailored QP formulation incorporates CBFs constraints and adaptive gain\nadjustments, optimizing control inputs while ensuring compliance with\nsafety-critical requirements. Theoretical analysis establishes the boundedness,\ncontinuity, and feasibility of the adaptive gain and the overall framework. The\neffectiveness of the method is demonstrated through simulations on a robotic\nmanipulator, showcasing its practical applicability to impulsive systems with\nstate jumps.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 08:38:47 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Liu", "Zihan", ""], ["Ni", "Yuan-Hua", ""]], "extracted_entities": [{"text": "Control\nBarrier Functions (CBFs)", "label": "LLMs"}, {"text": "adaptive gain mechanisms", "label": "LLMs"}, {"text": "CBFs", "label": "LLMs"}]}
{"id": "2503.10166", "submitter": "Pengfei Luo", "authors": "Pengfei Luo, Jingbo Zhou, Tong Xu, Yuan Xia, Linli Xu, Enhong Chen", "title": "ImageScope: Unifying Language-Guided Image Retrieval via Large\n  Multimodal Model Collective Reasoning", "comments": "WWW 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the proliferation of images in online content, language-guided image\nretrieval (LGIR) has emerged as a research hotspot over the past decade,\nencompassing a variety of subtasks with diverse input forms. While the\ndevelopment of large multimodal models (LMMs) has significantly facilitated\nthese tasks, existing approaches often address them in isolation, requiring the\nconstruction of separate systems for each task. This not only increases system\ncomplexity and maintenance costs, but also exacerbates challenges stemming from\nlanguage ambiguity and complex image content, making it difficult for retrieval\nsystems to provide accurate and reliable results. To this end, we propose\nImageScope, a training-free, three-stage framework that leverages collective\nreasoning to unify LGIR tasks. The key insight behind the unification lies in\nthe compositional nature of language, which transforms diverse LGIR tasks into\na generalized text-to-image retrieval process, along with the reasoning of LMMs\nserving as a universal verification to refine the results. To be specific, in\nthe first stage, we improve the robustness of the framework by synthesizing\nsearch intents across varying levels of semantic granularity using\nchain-of-thought (CoT) reasoning. In the second and third stages, we then\nreflect on retrieval results by verifying predicate propositions locally, and\nperforming pairwise evaluations globally. Experiments conducted on six LGIR\ndatasets demonstrate that ImageScope outperforms competitive baselines.\nComprehensive evaluations and ablation studies further confirm the\neffectiveness of our design.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 08:43:24 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Luo", "Pengfei", ""], ["Zhou", "Jingbo", ""], ["Xu", "Tong", ""], ["Xia", "Yuan", ""], ["Xu", "Linli", ""], ["Chen", "Enhong", ""]], "extracted_entities": [{"text": "ImageScope", "label": "Embedding"}, {"text": "LMMs", "label": "Large Language Model"}, {"text": "chain-of-thought (CoT)", "label": "Chain of thought"}, {"text": "ImageScope", "label": "Embedding"}]}
{"id": "2503.10167", "submitter": "Je Won Yeom", "authors": "Hyunbin Jin, Je Won Yeom, Seunghyun Bae, Taesup Kim", "title": "\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection\n  Decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large language models (LLMs) exhibit strong reasoning abilities, often\nattributed to few-shot or zero-shot chain-of-thought (CoT) prompting. While\neffective, these methods require labor-intensive prompt engineering, raising\nthe question of whether reasoning can be induced without reliance on explicit\nprompts. In this work, we unlock the reasoning capabilities of LLMs without\nexplicit prompting. Inspired by zero-shot CoT and CoT-decoding, we propose a\nnovel decoding strategy that systematically nudges LLMs to continue reasoning,\nthereby preventing immature reasoning processes. Specifically, we monitor the\nmodel's generation and inject a designated phrase whenever it is likely to\nconclude its response prematurely, before completing the reasoning process. Our\nexperimental evaluations on diverse reasoning benchmarks demonstrate that our\nproposed strategy substantially improves LLM reasoning capabilities,\nhighlighting the potential of decoding-based interventions as an alternative to\ntraditional prompting techniques.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 08:46:32 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Jin", "Hyunbin", ""], ["Yeom", "Je Won", ""], ["Bae", "Seunghyun", ""], ["Kim", "Taesup", ""]], "extracted_entities": [{"text": "Large language models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "zero-shot CoT", "label": "Chain of thought"}, {"text": "LLMs", "label": "Large Language Model"}]}
{"id": "2503.10170", "submitter": "Jianheng Liu", "authors": "Jianheng Liu, Yunfei Wan, Bowen Wang, Chunran Zheng, Jiarong Lin, and\n  Fu Zhang", "title": "GS-SDF: LiDAR-Augmented Gaussian Splatting and Neural SDF for\n  Geometrically Consistent Rendering and Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital twins are fundamental to the development of autonomous driving and\nembodied artificial intelligence. However, achieving high-granularity surface\nreconstruction and high-fidelity rendering remains a challenge. Gaussian\nsplatting offers efficient photorealistic rendering but struggles with\ngeometric inconsistencies due to fragmented primitives and sparse observational\ndata in robotics applications. Existing regularization methods, which rely on\nrender-derived constraints, often fail in complex environments. Moreover,\neffectively integrating sparse LiDAR data with Gaussian splatting remains\nchallenging. We propose a unified LiDAR-visual system that synergizes Gaussian\nsplatting with a neural signed distance field. The accurate LiDAR point clouds\nenable a trained neural signed distance field to offer a manifold geometry\nfield, This motivates us to offer an SDF-based Gaussian initialization for\nphysically grounded primitive placement and a comprehensive geometric\nregularization for geometrically consistent rendering and reconstruction.\nExperiments demonstrate superior reconstruction accuracy and rendering quality\nacross diverse trajectories. To benefit the community, the codes will be\nreleased at https://github.com/hku-mars/GS-SDF.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 08:53:38 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Liu", "Jianheng", ""], ["Wan", "Yunfei", ""], ["Wang", "Bowen", ""], ["Zheng", "Chunran", ""], ["Lin", "Jiarong", ""], ["Zhang", "Fu", ""]], "extracted_entities": [{"text": "neural signed distance field", "label": "Neural Language Model"}]}
{"id": "2503.10174", "submitter": "Maximilian Pierer Von Esch", "authors": "Maximilian Pierer von Esch, Andreas V\\\"olz, Knut Graichen", "title": "Sensitivity-Based Distributed Programming for Non-Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel sensitivity-based distributed programming (SBDP)\napproach for non-convex, large-scale nonlinear programs (NLP). The algorithm\nrelies on first-order sensitivities to cooperatively solve the central NLP in a\ndistributed manner with only neighbor-to-neighbor communication and\nparallelizable local computations. The scheme is based on primal decomposition\nand offers minimal algorithmic complexity. We derive sufficient local\nconvergence conditions for non-convex problems. Furthermore, we consider the\nSBDP method in a distributed optimal control context and derive favorable\nconvergence properties in this setting. We illustrate these theoretical\nfindings and the performance of the proposed algorithm with simulations of\nvarious distributed optimization and control problems.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 08:56:30 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["von Esch", "Maximilian Pierer", ""], ["V\u00f6lz", "Andreas", ""], ["Graichen", "Knut", ""]], "extracted_entities": [{"text": "distributed optimal control context", "label": "contextual Embedding"}]}
{"id": "2503.10176", "submitter": "Yuta Sato", "authors": "Yuta Sato", "title": "Uniform Lyndon interpolation for the pure logic of necessitation with a\n  modal reduction principle", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the uniform Lyndon interpolation property (ULIP) of some extensions\nof the pure logic of necessitation $\\mathbf{N}$. For any $m, n \\in \\mathbb{N}$,\n$\\mathbf{N}^+\\mathbf{A}_{m,n}$ is the logic obtained from $\\mathbf{N}$ by\nadding a single axiom $\\Box^n \\varphi \\to \\Box^m \\varphi$, which is a\n$\\Diamond$-free modal reduction principle, and a rule $\\frac{\\neg \\Box\n\\varphi}{\\neg \\Box \\Box \\varphi}$, which is required to make the logic complete\nwith respect to its Kripke-like semantics. We first introduce a sequent\ncalculus $\\mathbf{GN}^+\\mathbf{A}_{m,n}$ for $\\mathbf{N}^+\\mathbf{A}_{m,n}$ and\nshow that it enjoys cut elimination, proving Craig and Lyndon interpolation\nproperties as a consequence. We then construct an embedding of\n$\\mathbf{N}^+\\mathbf{A}_{m,n}$ into classical propositional logic\n$\\mathbf{Cl}$, which is then used to prove ULIP of\n$\\mathbf{N}^+\\mathbf{A}_{m,n}$ by reducing it to that of $\\mathbf{Cl}$. We also\nprove ULIP of $\\mathbf{NRA}_{m,n} = \\mathbf{N} + \\Box^n \\varphi \\to \\Box^m\n\\varphi + \\frac{\\neg \\varphi}{\\neg \\Box \\varphi}$ with a similar method.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 08:57:41 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Sato", "Yuta", ""]], "extracted_entities": [{"text": "embedding", "label": "Embedding"}]}
{"id": "2503.10177", "submitter": "Yirong Sun", "authors": "Yirong Sun, Yanjun Chen", "title": "PRISM: Preference Refinement via Implicit Scene Modeling for 3D\n  Vision-Language Preference-Based Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose PRISM, a novel framework designed to overcome the limitations of\n2D-based Preference-Based Reinforcement Learning (PBRL) by unifying 3D point\ncloud modeling and future-aware preference refinement. At its core, PRISM\nadopts a 3D Point Cloud-Language Model (3D-PC-LLM) to mitigate occlusion and\nviewpoint biases, ensuring more stable and spatially consistent preference\nsignals. Additionally, PRISM leverages Chain-of-Thought (CoT) reasoning to\nincorporate long-horizon considerations, thereby preventing the short-sighted\nfeedback often seen in static preference comparisons. In contrast to\nconventional PBRL techniques, this integration of 3D perception and\nfuture-oriented reasoning leads to significant gains in preference agreement\nrates, faster policy convergence, and robust generalization across unseen\nrobotic environments. Our empirical results, spanning tasks such as robotic\nmanipulation and autonomous navigation, highlight PRISM's potential for\nreal-world applications where precise spatial understanding and reliable\nlong-term decision-making are critical. By bridging 3D geometric awareness with\nCoT-driven preference modeling, PRISM establishes a comprehensive foundation\nfor scalable, human-aligned reinforcement learning.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 08:58:10 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Sun", "Yirong", ""], ["Chen", "Yanjun", ""]], "extracted_entities": [{"text": "PRISM", "label": "Foundation Model"}, {"text": "PRISM", "label": "Foundation Model"}, {"text": "PRISM", "label": "Foundation Model"}, {"text": "Chain-of-Thought", "label": "Chain of thought"}, {"text": "PRISM", "label": "Foundation Model"}]}
{"id": "2503.10178", "submitter": "Muhammad Atif Sultan", "authors": "M. Atif Sultan, Wei Hao, E. S. Swanson, Lei Chang", "title": "Bottomonium meson spectrum with quenched and unquenched quark models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An open question in hadronic phenomenology concerns the ``unquenching\"\neffects of higher Fock space components on the leading Fock space description\nof hadrons. We address this by making a comparison of the bottomonium spectrum\nas computed with the relativized Godfrey-Isgur quark model and an unquenched\ncoupled channel model driven by the ``$^3P_0$\" mechanism of hadronic decay. Our\nresults show that both models can describe the spectrum well, indicating that\nthe influence of coupled channel effects can be largely absorbed into the\nparameters of the quenched quark model. This conclusion is reinforced by a\nperturbative calculation that shows that the spin-dependence of mass splittings\ndue to mixing with the continuum recapitulates quenched quark model\nspin-dependent interactions. We also show that softening of the quark-antiquark\nwavefunction due to continuum mixing improves the description of vector\nbottomonium decay constants. Together, these results illustrate and\nsubstantiate the surprising robustness of simple constituent quark model\ndescriptions of hadrons.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 09:00:30 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Sultan", "M. Atif", ""], ["Hao", "Wei", ""], ["Swanson", "E. S.", ""], ["Chang", "Lei", ""]], "extracted_entities": [{"text": "unquenched\ncoupled channel model", "label": "AI model"}]}
{"id": "2503.10182", "submitter": "Mikhail Shkolnikov PhD", "authors": "Mikhail Shkolnikov and Peter Petrov", "title": "$PSL_2$ tropicalization and lines on surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG math.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper is based on a talk given by the first author at the G\\\"okova\nGeometry \\& Topology conference in May 2024. The subject is an interplay\nbetween the ideas of tropical geometry and two-by-two matrices with an\nintention to explore new types of geometries. More concretely, the article\ngives a preliminary account for a non-abelian version of phase tropicalization\nfor subvarieties of $PSL_2.$\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 09:10:27 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Shkolnikov", "Mikhail", ""], ["Petrov", "Peter", ""]], "extracted_entities": [{"text": "phase tropicalization", "label": "quantisation"}]}
{"id": "2503.10183", "submitter": "Shunqi Mao", "authors": "Shunqi Mao, Chaoyi Zhang, Weidong Cai", "title": "Through the Magnifying Glass: Adaptive Perception Magnification for\n  Hallucination-Free VLM Decoding", "comments": "19 pages, 5 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing vision-language models (VLMs) often suffer from visual\nhallucination, where the generated responses contain inaccuracies that are not\ngrounded in the visual input. Efforts to address this issue without model\nfinetuning primarily mitigate hallucination by reducing biases contrastively or\namplifying the weights of visual embedding during decoding. However, these\napproaches improve visual perception at the cost of impairing the language\nreasoning capability. In this work, we propose the Perception Magnifier (PM), a\nnovel visual decoding method that iteratively isolates relevant visual tokens\nbased on attention and magnifies the corresponding regions, spurring the model\nto concentrate on fine-grained visual details during decoding. Specifically, by\nmagnifying critical regions while preserving the structural and contextual\ninformation at each decoding step, PM allows the VLM to enhance its scrutiny of\nthe visual input, hence producing more accurate and faithful responses.\nExtensive experimental results demonstrate that PM not only achieves superior\nhallucination mitigation but also enhances language generation while preserving\nstrong reasoning capabilities.Code is available at\nhttps://github.com/ShunqiM/PM .\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 09:14:11 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Mao", "Shunqi", ""], ["Zhang", "Chaoyi", ""], ["Cai", "Weidong", ""]], "extracted_entities": [{"text": "visual embedding", "label": "Embedding"}, {"text": "attention", "label": "Attention mechanism"}, {"text": "spurring", "label": "Prompting"}]}
{"id": "2503.10186", "submitter": "Aamal Hussain", "authors": "Aamal Hussain, Dan Leonte, Francesco Belardinelli, Raphael Huser,\n  Dario Paccagnan", "title": "Multi-Agent Q-Learning Dynamics in Random Networks: Convergence due to\n  Exploration and Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT math.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Beyond specific settings, many multi-agent learning algorithms fail to\nconverge to an equilibrium solution, and instead display complex,\nnon-stationary behaviours such as recurrent or chaotic orbits. In fact, recent\nliterature suggests that such complex behaviours are likely to occur when the\nnumber of agents increases. In this paper, we study Q-learning dynamics in\nnetwork polymatrix games where the network structure is drawn from classical\nrandom graph models. In particular, we focus on the Erdos-Renyi model, a\nwell-studied model for social networks, and the Stochastic Block model, which\ngeneralizes the above by accounting for community structures within the\nnetwork. In each setting, we establish sufficient conditions under which the\nagents' joint strategies converge to a unique equilibrium. We investigate how\nthis condition depends on the exploration rates, payoff matrices and,\ncrucially, the sparsity of the network. Finally, we validate our theoretical\nfindings through numerical simulations and demonstrate that convergence can be\nreliably achieved in many-agent systems, provided network sparsity is\ncontrolled.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 09:16:51 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Hussain", "Aamal", ""], ["Leonte", "Dan", ""], ["Belardinelli", "Francesco", ""], ["Huser", "Raphael", ""], ["Paccagnan", "Dario", ""]], "extracted_entities": [{"text": "Erdos-Renyi model", "label": "AI model"}, {"text": "Stochastic Block model", "label": "AI model"}]}
{"id": "2503.10192", "submitter": "Miguel Romero-Arjona", "authors": "Miguel Romero-Arjona, Pablo Valle, Juan C. Alonso, Ana B. S\\'anchez,\n  Miriam Ugarte, Antonia Cazalilla, Vicente Cambr\\'on, Jos\\'e A. Parejo, Aitor\n  Arrieta, Sergio Segura", "title": "Red Teaming Contemporary AI Models: Insights from Spanish and Basque\n  Perspectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The battle for AI leadership is on, with OpenAI in the United States and\nDeepSeek in China as key contenders. In response to these global trends, the\nSpanish government has proposed ALIA, a public and transparent AI\ninfrastructure incorporating small language models designed to support Spanish\nand co-official languages such as Basque. This paper presents the results of\nRed Teaming sessions, where ten participants applied their expertise and\ncreativity to manually test three of the latest models from these\ninitiatives$\\unicode{x2013}$OpenAI o3-mini, DeepSeek R1, and ALIA\nSalamandra$\\unicode{x2013}$focusing on biases and safety concerns. The results,\nbased on 670 conversations, revealed vulnerabilities in all the models under\ntest, with biased or unsafe responses ranging from 29.5% in o3-mini to 50.6% in\nSalamandra. These findings underscore the persistent challenges in developing\nreliable and trustworthy AI systems, particularly those intended to support\nSpanish and Basque languages.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 09:27:24 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Romero-Arjona", "Miguel", ""], ["Valle", "Pablo", ""], ["Alonso", "Juan C.", ""], ["S\u00e1nchez", "Ana B.", ""], ["Ugarte", "Miriam", ""], ["Cazalilla", "Antonia", ""], ["Cambr\u00f3n", "Vicente", ""], ["Parejo", "Jos\u00e9 A.", ""], ["Arrieta", "Aitor", ""], ["Segura", "Sergio", ""]], "extracted_entities": [{"text": "ALIA", "label": "Open-source LLMs"}, {"text": "ALIA", "label": "Open-source LLMs"}]}
{"id": "2503.10193", "submitter": "Fernando Garc\\'ia-Casta\\~no", "authors": "Fernando Garc\\'ia-Casta\\~no and Miguel \\'Angel Melguizo-Padial", "title": "Theorems of nonlinear separation of co-radiant sets and optimality\n  conditions for approximate and proper approximate solutions of vector\n  optimization problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with \\(\\epsilon\\)-efficient and \\(\\epsilon\\)-proper\nefficient points with respect to a co-radiant set in a vector optimization\nproblem. In the first part of the paper, we establish a new nonlinear\nseparation theorem for co-radiant sets in normed spaces. Subsequently, we\nobtain necessary and sufficient conditions by means of scalarization for both\n\\(\\epsilon\\)-efficient and \\(\\epsilon\\)-proper efficient points in a general\nframework, without any requirements on the co-radiant set or any convexity\nassumption on the sets under consideration.Consequently, our results have a\nwider range of applicability than previously stated in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 09:27:35 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Garc\u00eda-Casta\u00f1o", "Fernando", ""], ["Melguizo-Padial", "Miguel \u00c1ngel", ""]], "extracted_entities": [{"text": "scalarization", "label": "quantisation"}]}
{"id": "2503.10197", "submitter": "Shuan Chen", "authors": "Shuan Chen, Kye Sung Park, Taewan Kim, Sunkyu Han and Yousung Jung", "title": "Predicting Chemical Reaction Outcomes Based on Electron Movements Using\n  Machine Learning", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately predicting chemical reaction outcomes and potential byproducts is\na fundamental task of modern chemistry, enabling the efficient design of\nsynthetic pathways and driving progress in chemical science. Reaction\nmechanism, which tracks electron movements during chemical reactions, is\ncritical for understanding reaction kinetics and identifying unexpected\nproducts. Here, we present Reactron, the first electron-based machine learning\nmodel for general reaction prediction. Reactron integrates electron movement\ninto its predictions, generating detailed arrow-pushing diagrams that elucidate\neach mechanistic step leading to product formation. We demonstrate the high\npredictive performance of Reactron over existing product-only models by a\nlarge-scale reaction outcome prediction benchmark, and the adaptability of the\nmodel to learn new reactivity upon providing a few examples. Furthermore, it\nexplores combinatorial reaction spaces, uncovering novel reactivities beyond\nits training data. With robust performance in both in- and out-of-distribution\npredictions, Reactron embodies human-like reasoning in chemistry and opens new\nfrontiers in reaction discovery and synthesis design.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 09:31:51 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chen", "Shuan", ""], ["Park", "Kye Sung", ""], ["Kim", "Taewan", ""], ["Han", "Sunkyu", ""], ["Jung", "Yousung", ""]], "extracted_entities": [{"text": "Reaction\nmechanism", "label": "Chain of thought"}, {"text": "Reactron", "label": "AI model"}, {"text": "Reactron", "label": "AI model"}, {"text": "Reactron", "label": "AI model"}, {"text": "Reactron", "label": "AI model"}]}
{"id": "2503.10198", "submitter": "Xiangjie Kong", "authors": "Xiangjie Kong, Zhenghao Chen, Weiyao Liu, Kaili Ning, Lechao Zhang,\n  Syauqie Muhammad Marier, Yichen Liu, Yuhao Chen, Feng Xia", "title": "Deep Learning for Time Series Forecasting: A Survey", "comments": null, "journal-ref": "Int. J. Mach. Learn. & Cyber. (2025)", "doi": "10.1007/s13042-025-02560-w", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series forecasting (TSF) has long been a crucial task in both industry\nand daily life. Most classical statistical models may have certain limitations\nwhen applied to practical scenarios in fields such as energy, healthcare,\ntraffic, meteorology, and economics, especially when high accuracy is required.\nWith the continuous development of deep learning, numerous new models have\nemerged in the field of time series forecasting in recent years. However,\nexisting surveys have not provided a unified summary of the wide range of model\narchitectures in this field, nor have they given detailed summaries of works in\nfeature extraction and datasets. To address this gap, in this review, we\ncomprehensively study the previous works and summarize the general paradigms of\nDeep Time Series Forecasting (DTSF) in terms of model architectures. Besides,\nwe take an innovative approach by focusing on the composition of time series\nand systematically explain important feature extraction methods. Additionally,\nwe provide an overall compilation of datasets from various domains in existing\nworks. Finally, we systematically emphasize the significant challenges faced\nand future research directions in this field.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 09:32:01 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Kong", "Xiangjie", ""], ["Chen", "Zhenghao", ""], ["Liu", "Weiyao", ""], ["Ning", "Kaili", ""], ["Zhang", "Lechao", ""], ["Marier", "Syauqie Muhammad", ""], ["Liu", "Yichen", ""], ["Chen", "Yuhao", ""], ["Xia", "Feng", ""]], "extracted_entities": [{"text": "deep learning", "label": "Few-shot Learning"}]}
{"id": "2503.10199", "submitter": "Ruibiao Song", "authors": "Ruibiao Song, Liying Zhang", "title": "Optimal Estimation and Uncertainty Quantification for Stochastic Inverse\n  Problems via Variational Bayesian Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian inversion method demonstrates significant potential for solving\ninverse problems, enabling both point estimation and uncertainty\nquantification. However, Bayesian maximum a posteriori (MAP) estimation may\nbecome unstable when handling data from diverse distributions (e.g., solutions\nof stochastic partial differential equations (SPDEs)). Additionally, Monte\nCarlo sampling methods are computationally expensive. To address these\nchallenges, we propose a novel two-stage optimization method based on optimal\ncontrol theory and variational Bayesian methods. This method not only achieves\nstable solutions for stochastic inverse problems but also efficiently\nquantifies the uncertainty of the solutions. In the first stage, we introduce a\nnew weighting formulation to ensure the stability of the Bayesian MAP\nestimation. In the second stage, we derive the necessary condition to\nefficiently quantify the uncertainty of the solutions, by combining the new\nweighting formula with variational inference. Furthermore, we establish an\nerror estimation theorem that relates the exact solution to the optimally\nestimated solution under different amounts of observed data. Finally, the\nefficiency of the proposed method is demonstrated through numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 09:34:33 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Song", "Ruibiao", ""], ["Zhang", "Liying", ""]], "extracted_entities": [{"text": "uncertainty\nquantification", "label": "quantisation"}]}
{"id": "2503.10200", "submitter": "Boyu Chen", "authors": "Boyu Chen, Zhengrong Yue, Siran Chen, Zikang Wang, Yang Liu, Peng Li,\n  Yali Wang", "title": "LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration\n  of MLLM Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Existing Multimodal Large Language Models (MLLMs) encounter significant\nchallenges in modeling the temporal context within long videos. Currently,\nmainstream Agent-based methods use external tools (e.g., search engine, memory\nbanks, OCR, retrieval models) to assist a single MLLM in answering long video\nquestions. Despite such tool-based support, a solitary MLLM still offers only a\npartial understanding of long videos, resulting in limited performance. In\norder to better address long video tasks, we introduce LVAgent, the first\nframework enabling multi-round dynamic collaboration of MLLM agents in long\nvideo understanding. Our methodology consists of four key steps: 1. Selection:\nWe pre-select appropriate agents from the model library to form optimal agent\nteams based on different tasks. 2. Perception: We design an effective retrieval\nscheme for long videos, improving the coverage of critical temporal segments\nwhile maintaining computational efficiency. 3. Action: Agents answer long\nvideo-related questions and exchange reasons. 4. Reflection: We evaluate the\nperformance of each agent in each round of discussion and optimize the agent\nteam for dynamic collaboration. The agents iteratively refine their answers by\nmulti-round dynamical collaboration of MLLM agents. LVAgent is the first agent\nsystem method that outperforms all closed-source models (including GPT-4o) and\nopen-source models (including InternVL-2.5 and Qwen2-VL) in the long video\nunderstanding tasks. Our LVAgent achieves an accuracy of 80% on four mainstream\nlong video understanding tasks. Notably, on the LongVideoBench dataset, LVAgent\nimproves accuracy by up to 14.3% compared with SOTA.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 09:35:09 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chen", "Boyu", ""], ["Yue", "Zhengrong", ""], ["Chen", "Siran", ""], ["Wang", "Zikang", ""], ["Liu", "Yang", ""], ["Li", "Peng", ""], ["Wang", "Yali", ""]], "extracted_entities": [{"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLM", "label": "Large Language Model"}]}
{"id": "2503.10205", "submitter": "Anthony Couthures", "authors": "Anthony Couthures, Vineeth S. Varma, Samson Lasaulce,\n  Irinel-Constantin Morarescu", "title": "Global synchronization of multi-agent systems with nonlinear\n  interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The paper addresses the synchronization of multi-agent systems with\ncontinuous-time dynamics interacting through a very general class of monotonic\ncontinuous signal functions that covers estimation biases, approximation of\ndiscrete quantization, or state-dependent estimation. Our analysis reveals\nthat, in the setup under consideration, synchronization equilibria are exactly\nthe fixed points of the signal function. We also derive intuitive stability\nconditions based on whether the signal underestimates or overestimates the\nstate of the agents around these fixed points. Moreover, we show that network\ntopology plays a crucial role in asymptotic synchronization. These results\nprovide interesting insights into the interplay between communication\nnonlinearity and network connectivity, paving the way for advanced coordination\nstrategies in complex systems.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 09:43:43 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Couthures", "Anthony", ""], ["Varma", "Vineeth S.", ""], ["Lasaulce", "Samson", ""], ["Morarescu", "Irinel-Constantin", ""]], "extracted_entities": [{"text": "discrete quantization", "label": "quantisation"}]}
{"id": "2503.10211", "submitter": "HengLyu Liu", "authors": "Henglyu Liu, Andong Chen, Kehai Chen, Xuefeng Bai, Meizhi Zhong, Yuan\n  Qiu, Min Zhang", "title": "Adaptive Inner Speech-Text Alignment for LLM-based Speech Translation", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancement of large language models (LLMs) has led to significant\nbreakthroughs across various tasks, laying the foundation for the development\nof LLM-based speech translation systems. Existing methods primarily focus on\naligning inputs and outputs across modalities while overlooking deeper semantic\nalignment within model representations. To address this limitation, we propose\nan Adaptive Inner Speech-Text Alignment (AI-STA) method to bridge the modality\ngap by explicitly aligning speech and text representations at selected layers\nwithin LLMs. To achieve this, we leverage the optimal transport (OT) theory to\nquantify fine-grained representation discrepancies between speech and text.\nFurthermore, we utilize the cross-modal retrieval technique to identify the\nlayers that are best suited for alignment and perform joint training on these\nlayers. Experimental results on speech translation (ST) tasks demonstrate that\nAI-STA significantly improves the translation performance of large speech-text\nmodels (LSMs), outperforming previous state-of-the-art approaches. Our findings\nhighlight the importance of inner-layer speech-text alignment in LLMs and\nprovide new insights into enhancing cross-modal learning.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 09:54:35 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Liu", "Henglyu", ""], ["Chen", "Andong", ""], ["Chen", "Kehai", ""], ["Bai", "Xuefeng", ""], ["Zhong", "Meizhi", ""], ["Qiu", "Yuan", ""], ["Zhang", "Min", ""]], "extracted_entities": [{"text": "large language models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "cross-modal learning", "label": "Few-shot Learning"}]}
{"id": "2503.10214", "submitter": "Zhiwu Wang", "authors": "Zhiwu Wang, Yichen Wu, Renzhen Wang, Haokun Lin, Quanziang Wang, Qian\n  Zhao, Deyu Meng", "title": "Singular Value Fine-tuning for Few-Shot Class-Incremental Learning", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class-Incremental Learning (CIL) aims to prevent catastrophic forgetting of\npreviously learned classes while sequentially incorporating new ones. The more\nchallenging Few-shot CIL (FSCIL) setting further complicates this by providing\nonly a limited number of samples for each new class, increasing the risk of\noverfitting in addition to standard CIL challenges. While catastrophic\nforgetting has been extensively studied, overfitting in FSCIL, especially with\nlarge foundation models, has received less attention. To fill this gap, we\npropose the Singular Value Fine-tuning for FSCIL (SVFCL) and compared it with\nexisting approaches for adapting foundation models to FSCIL, which primarily\nbuild on Parameter Efficient Fine-Tuning (PEFT) methods like prompt tuning and\nLow-Rank Adaptation (LoRA). Specifically, SVFCL applies singular value\ndecomposition to the foundation model weights, keeping the singular vectors\nfixed while fine-tuning the singular values for each task, and then merging\nthem. This simple yet effective approach not only alleviates the forgetting\nproblem but also mitigates overfitting more effectively while significantly\nreducing trainable parameters. Extensive experiments on four benchmark\ndatasets, along with visualizations and ablation studies, validate the\neffectiveness of SVFCL. The code will be made available.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 09:57:28 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Zhiwu", ""], ["Wu", "Yichen", ""], ["Wang", "Renzhen", ""], ["Lin", "Haokun", ""], ["Wang", "Quanziang", ""], ["Zhao", "Qian", ""], ["Meng", "Deyu", ""]], "extracted_entities": [{"text": "Few-shot CIL", "label": "Few-shot Learning"}, {"text": "FSCIL", "label": "Few-shot Learning"}, {"text": "CIL", "label": "Zero-shot Learning"}, {"text": "FSCIL", "label": "Few-shot Learning"}, {"text": "Singular Value Fine-tuning", "label": "Fine-tuning"}, {"text": "FSCIL", "label": "Few-shot Learning"}, {"text": "FSCIL", "label": "Few-shot Learning"}, {"text": "prompt tuning", "label": "Prompting"}]}
{"id": "2503.10215", "submitter": "Benjamin Heymann", "authors": "Benjamin Heymann", "title": "Adaptive Preference Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  AI alignment, the challenge of ensuring AI systems act in accordance with\nhuman values, has emerged as a critical problem in the development of systems\nsuch as foundation models and recommender systems. Still, the current dominant\napproach, reinforcement learning with human feedback (RLHF) faces known\ntheoretical limitations in aggregating diverse human preferences. Social choice\ntheory provides a framework to aggregate preferences, but was not developed for\nthe multidimensional applications typical of AI. Leveraging insights from a\nrecently published urn process, this work introduces a preference aggregation\nstrategy that adapts to the user's context and that inherits the good\nproperties of the maximal lottery, a Condorcet-consistent solution concept.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 09:57:41 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Heymann", "Benjamin", ""]], "extracted_entities": [{"text": "foundation models", "label": "Foundation Model"}, {"text": "reinforcement learning with human feedback", "label": "Few-shot Learning"}, {"text": "Social choice\ntheory", "label": "AI Ethics"}]}
{"id": "2503.10217", "submitter": "Shilong Wang", "authors": "Shilong Wang, Jianchun Liu, Hongli Xu, Jiaming Yan, Xianjun Gao", "title": "Efficient Federated Fine-Tuning of Large Language Models with Layer\n  Dropout", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning plays a crucial role in enabling pre-trained LLMs to evolve from\ngeneral language comprehension to task-specific expertise. To preserve user\ndata privacy, federated fine-tuning is often employed and has emerged as the de\nfacto paradigm. However, federated fine-tuning is prohibitively inefficient due\nto the tension between LLM complexity and the resource constraint of end\ndevices, incurring unaffordable fine-tuning overhead. Existing literature\nprimarily utilizes parameter-efficient fine-tuning techniques to mitigate\ncommunication costs, yet computational and memory burdens continue to pose\nsignificant challenges for developers. This work proposes DropPEFT, an\ninnovative federated PEFT framework that employs a novel stochastic transformer\nlayer dropout method, enabling devices to deactivate a considerable fraction of\nLLMs layers during training, thereby eliminating the associated computational\nload and memory footprint. In DropPEFT, a key challenge is the proper\nconfiguration of dropout ratios for layers, as overhead and training\nperformance are highly sensitive to this setting. To address this challenge, we\nadaptively assign optimal dropout-ratio configurations to devices through an\nexploration-exploitation strategy, achieving efficient and effective\nfine-tuning. Extensive experiments show that DropPEFT can achieve a\n1.3-6.3\\times speedup in model convergence and a 40%-67% reduction in memory\nfootprint compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 09:59:16 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Shilong", ""], ["Liu", "Jianchun", ""], ["Xu", "Hongli", ""], ["Yan", "Jiaming", ""], ["Gao", "Xianjun", ""]], "extracted_entities": [{"text": "Fine-tuning", "label": "Fine-tuning"}, {"text": "federated fine-tuning", "label": "Fine-tuning"}, {"text": "federated fine-tuning", "label": "Fine-tuning"}, {"text": "fine-tuning", "label": "Fine-tuning"}]}
{"id": "2503.10218", "submitter": "Yifeng Cai", "authors": "Yifeng Cai and Ziqi Zhang and Ding Li and Yao Guo and Xiangqun Chen", "title": "Moss: Proxy Model-based Full-Weight Aggregation in Federated Learning\n  with Heterogeneous Models", "comments": "Accepted by ACM IMWUT/Ubicomp 2025", "journal-ref": null, "doi": "10.1145/3712274", "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern Federated Learning (FL) has become increasingly essential for handling\nhighly heterogeneous mobile devices. Current approaches adopt a partial model\naggregation paradigm that leads to sub-optimal model accuracy and higher\ntraining overhead. In this paper, we challenge the prevailing notion of\npartial-model aggregation and propose a novel \"full-weight aggregation\" method\nnamed Moss, which aggregates all weights within heterogeneous models to\npreserve comprehensive knowledge. Evaluation across various applications\ndemonstrates that Moss significantly accelerates training, reduces on-device\ntraining time and energy consumption, enhances accuracy, and minimizes network\nbandwidth utilization when compared to state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 10:00:58 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Cai", "Yifeng", ""], ["Zhang", "Ziqi", ""], ["Li", "Ding", ""], ["Guo", "Yao", ""], ["Chen", "Xiangqun", ""]], "extracted_entities": [{"text": "Modern Federated Learning", "label": "Few-shot Learning"}]}
{"id": "2503.10221", "submitter": "Shaoshuai Chu", "authors": "Shaoshuai Chu, Alexander Kurganov and Ruixiao Xin", "title": "New More Efficient A-WENO Schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop new more efficient A-WENO schemes for both hyperbolic systems of\nconservation laws and nonconservative hyperbolic systems. The new schemes are a\nvery simple modification of the existing A-WENO schemes: They are obtained by a\nmore efficient evaluation of the high-order correction terms. We conduct\nseveral numerical experiments to demonstrate the performance of the introduced\nschemes.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 10:04:45 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chu", "Shaoshuai", ""], ["Kurganov", "Alexander", ""], ["Xin", "Ruixiao", ""]], "extracted_entities": [{"text": "conservation laws", "label": "Scaling law"}]}
{"id": "2503.10227", "submitter": "Samuel Kov\\'a\\v{c}ik", "authors": "Matej Hrmo, Samuel Kov\\'a\\v{c}ik, Patrik Rusn\\'ak and Juraj Tekel", "title": "Hydrogen Atom in a Fuzzy Spherical Cavity", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-th quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The fuzzy onion model formed by connecting a set of concentric fuzzy spheres\nof increasing radius is motivated by studies of quantum space but can also be\nused to study standard physics. The main feature of the model is that functions\nin three-dimensional space, for example, scalar fields or wavefunctions, are\nexpressed in terms of Hermitian matrices of a certain form. Relevant equations\nare then matrix equations, and some problems, such as searching for energy\nspectrum for fixed angular momentum quantum numbers $(l,m)$, can be expressed\nas an eigenvalue problem. We show how this simple approach can reproduce the\nresults of other studies analyzing the hydrogen atom in a spherical cavity. We\nalso test the effect of short distance quantum structure of the space on these\nsolutions -- not looking for the phenomenological consequences, as the scale of\nquantum space is many orders below the order of the Bohr radius, but to\nunderstand the effect quantum space in general. We observe a set of solutions\nwith no classical counterpart that has been described in a former theoretical\nstudy.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 10:11:38 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Hrmo", "Matej", ""], ["Kov\u00e1\u010dik", "Samuel", ""], ["Rusn\u00e1k", "Patrik", ""], ["Tekel", "Juraj", ""]], "extracted_entities": [{"text": "Bohr radius", "label": "Scaling law"}]}
{"id": "2503.10228", "submitter": "Andi Nika", "authors": "Andi Nika, Jonathan N\\\"other, Debmalya Mandal, Parameswaran\n  Kamalaruban, Adish Singla and Goran Radanovi\\'c", "title": "Policy Teaching via Data Poisoning in Learning from Human Preferences", "comments": "In AISTATS 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study data poisoning attacks in learning from human preferences. More\nspecifically, we consider the problem of teaching/enforcing a target policy\n$\\pi^\\dagger$ by synthesizing preference data. We seek to understand the\nsusceptibility of different preference-based learning paradigms to poisoned\npreference data by analyzing the number of samples required by the attacker to\nenforce $\\pi^\\dagger$. We first propose a general data poisoning formulation in\nlearning from human preferences and then study it for two popular paradigms,\nnamely: (a) reinforcement learning from human feedback (RLHF) that operates by\nlearning a reward model using preferences; (b) direct preference optimization\n(DPO) that directly optimizes policy using preferences. We conduct a\ntheoretical analysis of the effectiveness of data poisoning in a setting where\nthe attacker is allowed to augment a pre-existing dataset and also study its\nspecial case where the attacker can synthesize the entire preference dataset\nfrom scratch. As our main results, we provide lower/upper bounds on the number\nof samples required to enforce $\\pi^\\dagger$. Finally, we discuss the\nimplications of our results in terms of the susceptibility of these learning\nparadigms under such data poisoning attacks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 10:11:54 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Nika", "Andi", ""], ["N\u00f6ther", "Jonathan", ""], ["Mandal", "Debmalya", ""], ["Kamalaruban", "Parameswaran", ""], ["Singla", "Adish", ""], ["Radanovi\u0107", "Goran", ""]], "extracted_entities": [{"text": "reinforcement learning from human feedback", "label": "Few-shot Learning"}]}
{"id": "2503.10229", "submitter": "Andreas Spitz", "authors": "Julian Schelb, Orr Borin, David Garcia, Andreas Spitz", "title": "R.U.Psycho? Robust Unified Psychometric Testing of Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative language models are increasingly being subjected to psychometric\nquestionnaires intended for human testing, in efforts to establish their\ntraits, as benchmarks for alignment, or to simulate participants in social\nscience experiments. While this growing body of work sheds light on the\nlikeness of model responses to those of humans, concerns are warranted\nregarding the rigour and reproducibility with which these experiments may be\nconducted. Instabilities in model outputs, sensitivity to prompt design,\nparameter settings, and a large number of available model versions increase\ndocumentation requirements. Consequently, generalization of findings is often\ncomplex and reproducibility is far from guaranteed. In this paper, we present\nR.U.Psycho, a framework for designing and running robust and reproducible\npsychometric experiments on generative language models that requires limited\ncoding expertise. We demonstrate the capability of our framework on a variety\nof psychometric questionnaires, which lend support to prior findings in the\nliterature. R.U.Psycho is available as a Python package at\nhttps://github.com/julianschelb/rupsycho.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 10:12:34 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Schelb", "Julian", ""], ["Borin", "Orr", ""], ["Garcia", "David", ""], ["Spitz", "Andreas", ""]], "extracted_entities": [{"text": "prompt design", "label": "Prompting"}]}
{"id": "2503.10230", "submitter": "Gabriel Merlin", "authors": "Gabriel Merlin (MICS)", "title": "On biadjoint triangles with additional modifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lucatelli Nunes obtained a 2-categorical version of the adjoint triangle\ntheorem of Dubuc using the descent object of a specific diagram. In some cases,\nsuch a diagram can be filled with an extra cell. We show then how to obtain a\nbiadjoint as an inverter of this additional datum (under suitable hypotheses).\nThe problem addressed here is slightly different however: we still have a\ntriangle of pseudofunctors but the lifted biadjoint is not the same. The\nconstruction is simplified when the pseudofunctor whose left biadjoint is\nsought is fully faithful. As an example, we get the biadjoint of the inclusion\npseudofunctor of a bicategory associated to a KZ-monad preserving\npseudomonicity.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 10:13:29 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Merlin", "Gabriel", "", "MICS"]], "extracted_entities": [{"text": "biadjoint", "label": "BERT"}, {"text": "biadjoint", "label": "BERT"}, {"text": "biadjoint", "label": "BERT"}, {"text": "biadjoint", "label": "BERT"}, {"text": "pseudofunctor", "label": "BERT"}]}
{"id": "2503.10233", "submitter": "Laya Mahmoudi", "authors": "Samira Zangooei, Amirhossein Darmani, Hossein Farahmand Nezhad, Laya\n  Mahmoudi", "title": "ARLED: Leveraging LED-based ARMAN Model for Abstractive Summarization of\n  Persian Long Documents", "comments": "11 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing volume of textual data poses challenges in reading and\ncomprehending large documents, particularly for scholars who need to extract\nuseful information from research articles. Automatic text summarization has\nemerged as a powerful tool to condense lengthy documents into concise and\ninformative summaries. Depending on the approach used, text summarization can\nbe categorized as either extractive or abstractive. While extractive methods\nare commonly used due to their simplicity, they often miss important\ninformation. On the other hand, Abstractive Summarization can generate more\ncoherent and informative summaries by understanding the underlying meaning of\nthe text. Abstractive techniques have gained attention in various languages,\nand recent advancements have been achieved through pre-training models such as\nBERT, BART, and T5. However, the challenge of summarizing long documents\nremains, and alternative models like Longformer have been introduced to address\nthis limitation. In this context, this paper focuses on abstractive\nsummarization in the Persian language. The authors introduce a new dataset of\n300,000 full-text Persian papers obtained from the Ensani website and apply the\nARMAN model, based on the Longformer architecture, to generate summaries. The\nexperimental results demonstrate promising performance in Persian text\nsummarization. The paper provides a comprehensive overview of related work,\ndiscusses the methodology, presents the experimental results, and concludes\nwith future research directions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 10:16:46 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zangooei", "Samira", ""], ["Darmani", "Amirhossein", ""], ["Nezhad", "Hossein Farahmand", ""], ["Mahmoudi", "Laya", ""]], "extracted_entities": [{"text": "Abstractive Summarization", "label": "Knowledge distillation"}, {"text": "BERT", "label": "BERT"}, {"text": "BART", "label": "BERT"}]}
{"id": "2503.10237", "submitter": "Ji Li", "authors": "Ji Li and Chao Wang", "title": "Efficient Diffusion Posterior Sampling for Noisy Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The pretrained diffusion model as a strong prior has been leveraged to\naddress inverse problems in a zero-shot manner without task-specific\nretraining. Different from the unconditional generation, the measurement-guided\ngeneration requires estimating the expectation of clean image given the current\nimage and the measurement. With the theoretical expectation expression, the\ncrucial task of solving inverse problems is to estimate the noisy likelihood\nfunction at the intermediate image sample. Using the Tweedie's formula and the\nknown noise model, the existing diffusion posterior sampling methods perform\ngradient descent step with backpropagation through the pretrained diffusion\nmodel. To alleviate the costly computation and intensive memory consumption of\nthe backpropagation, we propose an alternative maximum-a-posteriori (MAP)-based\nsurrogate estimator to the expectation. With this approach and further density\napproximation, the MAP estimator for linear inverse problem is the solution to\na traditional regularized optimization, of which the loss comprises of data\nfidelity term and the diffusion model related prior term. Integrating the MAP\nestimator into a general denoising diffusion implicit model (DDIM)-like\nsampler, we achieve the general solving framework for inverse problems. Our\napproach highly resembles the existing $\\Pi$GDM without the manifold projection\noperation of the gradient descent direction. The developed method is also\nextended to nonlinear JPEG decompression. The performance of the proposed\nposterior sampling is validated across a series of inverse problems, where both\nVP and VE SDE-based pretrained diffusion models are taken into consideration.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 10:27:43 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Ji", ""], ["Wang", "Chao", ""]], "extracted_entities": [{"text": "measurement-guided\ngeneration", "label": "Few-shot Learning"}]}
{"id": "2503.10241", "submitter": "Sabrina Patania", "authors": "Dimitri Ognibene, Sabrina Patania, Luca Annese, Cansu Koyuturk, Franca\n  Garzotto, Giuseppe Vizzari, Azzurra Ruggeri, Simone Colombani", "title": "SCOOP: A Framework for Proactive Collaboration and Social Continual\n  Learning through Natural Language Interaction andCausal Reasoning", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal information-gathering settings, where users collaborate with AI in\ndynamic environments, are increasingly common. These involve complex processes\nwith textual and multimodal interactions, often requiring additional structural\ninformation via cost-incurring requests. AI helpers lack access to users' true\ngoals, beliefs, and preferences and struggle to integrate diverse information\neffectively.\n  We propose a social continual learning framework for causal knowledge\nacquisition and collaborative decision-making. It focuses on autonomous agents\nlearning through dialogues, question-asking, and interaction in open, partially\nobservable environments. A key component is a natural language oracle that\nanswers the agent's queries about environmental mechanisms and states, refining\ncausal understanding while balancing exploration or learning, and exploitation\nor knowledge use.\n  Evaluation tasks inspired by developmental psychology emphasize causal\nreasoning and question-asking skills. They complement benchmarks by assessing\nthe agent's ability to identify knowledge gaps, generate meaningful queries,\nand incrementally update reasoning. The framework also evaluates how knowledge\nacquisition costs are amortized across tasks within the same environment.\n  We propose two architectures: 1) a system combining Large Language Models\n(LLMs) with the ReAct framework and question-generation, and 2) an advanced\nsystem with a causal world model, symbolic, graph-based, or subsymbolic, for\nreasoning and decision-making. The latter builds a causal knowledge graph for\nefficient inference and adaptability under constraints. Challenges include\nintegrating causal reasoning into ReAct and optimizing exploration and\nquestion-asking in error-prone scenarios. Beyond applications, this framework\nmodels developmental processes combining causal reasoning, question generation,\nand social learning.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 10:32:50 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Ognibene", "Dimitri", ""], ["Patania", "Sabrina", ""], ["Annese", "Luca", ""], ["Koyuturk", "Cansu", ""], ["Garzotto", "Franca", ""], ["Vizzari", "Giuseppe", ""], ["Ruggeri", "Azzurra", ""], ["Colombani", "Simone", ""]], "extracted_entities": [{"text": "Large Language Models", "label": "Large Language Model"}]}
{"id": "2503.10242", "submitter": "Shaun Khoo", "authors": "Shaun Khoo, Gabriel Chua, Rachel Shong", "title": "MinorBench: A hand-built benchmark for content-based risks for children", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large Language Models (LLMs) are rapidly entering children's lives - through\nparent-driven adoption, schools, and peer networks - yet current AI ethics and\nsafety research do not adequately address content-related risks specific to\nminors. In this paper, we highlight these gaps with a real-world case study of\nan LLM-based chatbot deployed in a middle school setting, revealing how\nstudents used and sometimes misused the system. Building on these findings, we\npropose a new taxonomy of content-based risks for minors and introduce\nMinorBench, an open-source benchmark designed to evaluate LLMs on their ability\nto refuse unsafe or inappropriate queries from children. We evaluate six\nprominent LLMs under different system prompts, demonstrating substantial\nvariability in their child-safety compliance. Our results inform practical\nsteps for more robust, child-focused safety mechanisms and underscore the\nurgency of tailoring AI systems to safeguard young users.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 10:34:43 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Khoo", "Shaun", ""], ["Chua", "Gabriel", ""], ["Shong", "Rachel", ""]], "extracted_entities": [{"text": "Large Language Models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "AI ethics", "label": "AI Ethics"}, {"text": "LLM-based chatbot", "label": "ChatGPT"}, {"text": "MinorBench", "label": "Open-source LLMs"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "system prompts", "label": "Prompting"}]}
{"id": "2503.10245", "submitter": "Ratnangshu Das", "authors": "Mohd. Faizuddin Faruqui, Ratnangshu Das, Ravi Kumar L, Pushpak Jagtap", "title": "Reach-Avoid-Stay-Collision-Avoidance Negotiation Framework for\n  Multi-Agent Systems via Spatiotemporal Tubes", "comments": "Accepted in ECC 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a multi-agent negotiation-based framework to obtain\ncollision-free paths while performing prescribed-time reach-avoid-stay (RAS)\ntasks for agents with unknown dynamics and bounded disturbance. By employing\nspatiotemporal tubes to generate time-varying state constraints, we ensure that\nall agents adhere to RAS specifications using synthesized controllers. To\nprevent inter-agent collisions, a negotiation mechanism is proposed where\nsuccessful negotiations result in spatiotemporal tubes for each agent\nfulfilling desired tasks. This approach results in a completely distributed,\napproximation-free control law for each agent. The effectiveness of this\nmechanism was validated through simulations of multi-agent robot navigation and\ndrone navigation tasks involving prescribed-time RAS specifications and\ncollision avoidance.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 10:45:34 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Faruqui", "Mohd. Faizuddin", ""], ["Das", "Ratnangshu", ""], ["L", "Ravi Kumar", ""], ["Jagtap", "Pushpak", ""]], "extracted_entities": [{"text": "approximation-free control law", "label": "Scaling law"}]}
{"id": "2503.10247", "submitter": "Zhijie Zhu", "authors": "Zhijie Zhu, Lei Fan, Maurice Pagnucco, Yang Song", "title": "Interpretable Image Classification via Non-parametric Part Prototype\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classifying images with an interpretable decision-making process is a\nlong-standing problem in computer vision. In recent years, Prototypical Part\nNetworks has gained traction as an approach for self-explainable neural\nnetworks, due to their ability to mimic human visual reasoning by providing\nexplanations based on prototypical object parts. However, the quality of the\nexplanations generated by these methods leaves room for improvement, as the\nprototypes usually focus on repetitive and redundant concepts. Leveraging\nrecent advances in prototype learning, we present a framework for part-based\ninterpretable image classification that learns a set of semantically\ndistinctive object parts for each class, and provides diverse and comprehensive\nexplanations. The core of our method is to learn the part-prototypes in a\nnon-parametric fashion, through clustering deep features extracted from\nfoundation vision models that encode robust semantic information. To\nquantitatively evaluate the quality of explanations provided by ProtoPNets, we\nintroduce Distinctiveness Score and Comprehensiveness Score. Through evaluation\non CUB-200-2011, Stanford Cars and Stanford Dogs datasets, we show that our\nframework compares favourably against existing ProtoPNets while achieving\nbetter interpretability. Code is available at:\nhttps://github.com/zijizhu/proto-non-param.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 10:46:53 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhu", "Zhijie", ""], ["Fan", "Lei", ""], ["Pagnucco", "Maurice", ""], ["Song", "Yang", ""]], "extracted_entities": [{"text": "foundation vision models", "label": "Foundation Model"}]}
{"id": "2503.10248", "submitter": "Idan Horowitz", "authors": "Idan Horowitz and Ori Plonsky", "title": "LLM Agents Display Human Biases but Exhibit Distinct Learning Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the choice patterns of Large Language Models (LLMs) in the\ncontext of Decisions from Experience tasks that involve repeated choice and\nlearning from feedback, and compare their behavior to human participants. We\nfind that on the aggregate, LLMs appear to display behavioral biases similar to\nhumans: both exhibit underweighting rare events and correlation effects.\nHowever, more nuanced analyses of the choice patterns reveal that this happens\nfor very different reasons. LLMs exhibit strong recency biases, unlike humans,\nwho appear to respond in more sophisticated ways. While these different\nprocesses may lead to similar behavior on average, choice patterns contingent\non recent events differ vastly between the two groups. Specifically, phenomena\nsuch as ``surprise triggers change\" and the ``wavy recency effect of rare\nevents\" are robustly observed in humans, but entirely absent in LLMs. Our\nfindings provide insights into the limitations of using LLMs to simulate and\npredict humans in learning environments and highlight the need for refined\nanalyses of their behavior when investigating whether they replicate human\ndecision making tendencies.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 10:47:03 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Horowitz", "Idan", ""], ["Plonsky", "Ori", ""]], "extracted_entities": [{"text": "Large Language Models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}]}
{"id": "2503.10251", "submitter": "Stanislav Budzinskiy", "authors": "Stanislav Budzinskiy, Wenyi Fang, Longbin Zeng, Philipp Petersen", "title": "Numerical Error Analysis of Large Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large language models based on transformer architectures have become integral\nto state-of-the-art natural language processing applications. However, their\ntraining remains computationally expensive and exhibits instabilities, some of\nwhich are expected to be caused by finite-precision computations. We provide a\ntheoretical analysis of the impact of round-off errors within the forward pass\nof a transformer architecture which yields fundamental bounds for these\neffects. In addition, we conduct a series of numerical experiments which\ndemonstrate the practical relevance of our bounds. Our results yield concrete\nguidelines for choosing hyperparameters that mitigate round-off errors, leading\nto more robust and stable inference.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 10:53:17 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Budzinskiy", "Stanislav", ""], ["Fang", "Wenyi", ""], ["Zeng", "Longbin", ""], ["Petersen", "Philipp", ""]], "extracted_entities": [{"text": "transformer architectures", "label": "Transformers"}]}
{"id": "2503.10252", "submitter": "Zhi Chen", "authors": "Zhi Chen and Zecheng Zhao and Jingcai Guo and Jingjing Li and Zi Huang", "title": "SVIP: Semantically Contextualized Visual Patches for Zero-Shot Learning", "comments": "Pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Zero-shot learning (ZSL) aims to recognize unseen classes without labeled\ntraining examples by leveraging class-level semantic descriptors such as\nattributes. A fundamental challenge in ZSL is semantic misalignment, where\nsemantic-unrelated information involved in visual features introduce ambiguity\nto visual-semantic interaction. Unlike existing methods that suppress\nsemantic-unrelated information post hoc either in the feature space or the\nmodel space, we propose addressing this issue at the input stage, preventing\nsemantic-unrelated patches from propagating through the network. To this end,\nwe introduce Semantically contextualized VIsual Patches (SVIP) for ZSL, a\ntransformer-based framework designed to enhance visual-semantic alignment.\nSpecifically, we propose a self-supervised patch selection mechanism that\npreemptively learns to identify semantic-unrelated patches in the input space.\nThis is trained with the supervision from aggregated attention scores across\nall transformer layers, which estimate each patch's semantic score. As removing\nsemantic-unrelated patches from the input sequence may disrupt object\nstructure, we replace them with learnable patch embeddings. With initialization\nfrom word embeddings, we can ensure they remain semantically meaningful\nthroughout feature extraction. Extensive experiments on ZSL benchmarks\ndemonstrate that SVIP achieves state-of-the-art performance results while\nproviding more interpretable and semantically rich feature representations.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 10:59:51 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chen", "Zhi", ""], ["Zhao", "Zecheng", ""], ["Guo", "Jingcai", ""], ["Li", "Jingjing", ""], ["Huang", "Zi", ""]], "extracted_entities": [{"text": "Zero-shot learning", "label": "Few-shot Learning"}, {"text": "Semantically contextualized VIsual Patches", "label": "contextual Embedding"}, {"text": "aggregated attention scores", "label": "Attention mechanism"}, {"text": "learnable patch embeddings", "label": "contextual Embedding"}, {"text": "word embeddings", "label": "Embedding"}]}
{"id": "2503.10256", "submitter": "Yeonjin Chang", "authors": "Yeonjin Chang, Erqun Dong, Seunghyeon Seo, Nojun Kwak, Kwang Moo Yi", "title": "ROODI: Reconstructing Occluded Objects with Denoising Inpainters", "comments": "Project page: https://yeonjin-chang.github.io/ROODI/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the quality of novel-view images has improved dramatically with 3D\nGaussian Splatting, extracting specific objects from scenes remains\nchallenging. Isolating individual 3D Gaussian primitives for each object and\nhandling occlusions in scenes remain far from being solved. We propose a novel\nobject extraction method based on two key principles: (1) being object-centric\nby pruning irrelevant primitives; and (2) leveraging generative inpainting to\ncompensate for missing observations caused by occlusions. For pruning, we\nanalyze the local structure of primitives using K-nearest neighbors, and retain\nonly relevant ones. For inpainting, we employ an off-the-shelf diffusion-based\ninpainter combined with occlusion reasoning, utilizing the 3D representation of\nthe entire scene. Our findings highlight the crucial synergy between pruning\nand inpainting, both of which significantly enhance extraction performance. We\nevaluate our method on a standard real-world dataset and introduce a synthetic\ndataset for quantitative analysis. Our approach outperforms the\nstate-of-the-art, demonstrating its effectiveness in object extraction from\ncomplex scenes.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 11:16:21 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chang", "Yeonjin", ""], ["Dong", "Erqun", ""], ["Seo", "Seunghyeon", ""], ["Kwak", "Nojun", ""], ["Yi", "Kwang Moo", ""]], "extracted_entities": [{"text": "inpainting", "label": "Embedding"}, {"text": "inpainting", "label": "Embedding"}]}
{"id": "2503.10259", "submitter": "Yunpeng Qu", "authors": "Yunpeng Qu, Kun Yuan, Qizhi Xie, Ming Sun, Chao Zhou, Jian Wang", "title": "KVQ: Boosting Video Quality Assessment via Saliency-guided Local\n  Perception", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Quality Assessment (VQA), which intends to predict the perceptual\nquality of videos, has attracted increasing attention. Due to factors like\nmotion blur or specific distortions, the quality of different regions in a\nvideo varies. Recognizing the region-wise local quality within a video is\nbeneficial for assessing global quality and can guide us in adopting\nfine-grained enhancement or transcoding strategies. Due to the heavy cost of\nannotating region-wise quality, the lack of ground truth constraints from\nrelevant datasets further complicates the utilization of local perception.\nInspired by the Human Visual System (HVS) that links global quality to the\nlocal texture of different regions and their visual saliency, we propose a\nKaleidoscope Video Quality Assessment (KVQ) framework, which aims to\neffectively assess both saliency and local texture, thereby facilitating the\nassessment of global quality. Our framework extracts visual saliency and\nallocates attention using Fusion-Window Attention (FWA) while incorporating a\nLocal Perception Constraint (LPC) to mitigate the reliance of regional texture\nperception on neighboring areas. KVQ obtains significant improvements across\nmultiple scenarios on five VQA benchmarks compared to SOTA methods.\nFurthermore, to assess local perception, we establish a new Local Perception\nVisual Quality (LPVQ) dataset with region-wise annotations. Experimental\nresults demonstrate the capability of KVQ in perceiving local distortions. KVQ\nmodels and the LPVQ dataset will be available at\nhttps://github.com/qyp2000/KVQ.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 11:16:58 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Qu", "Yunpeng", ""], ["Yuan", "Kun", ""], ["Xie", "Qizhi", ""], ["Sun", "Ming", ""], ["Zhou", "Chao", ""], ["Wang", "Jian", ""]], "extracted_entities": [{"text": "Fusion-Window Attention", "label": "Attention mechanism"}]}
{"id": "2503.10261", "submitter": "Misa Kawaguchi", "authors": "Misa Kawaguchi, William Kai Alexander Worby, Yuto Yokoyama, Ryuta X.\n  Suzuki, Yuichiro Nagatsu, Yoshiyuki Tagawa", "title": "Flow birefringence of shear-thinning fluid in a Hele-Shaw cell", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cond-mat.soft", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Flow birefringence measurement is an emerging technique for visualizing\nstress fields in fluid flows. This study investigates flow birefringence in the\nsteady radial Hele-Shaw flow of a shear-thinning fluid. In this flow\nconfiguration, stress is dominant along the optical axis, challenging the\napplicability of the conventional stress-optic law (SOL). We conduct flow\nbirefringence measurements at various flow rates and compare the results with\ntheoretical predictions. The observed phase retardation cannot be\nquantitatively explained using the conventional SOL, but is successfully\ndescribed using the second-order SOL, which accounts for stress along the\noptical direction, using the results of rheo-optical measurements. Furthermore,\nwe investigate the shear-thinning effects on phase retardation from two\nperspectives: (i) stress changes resulting from viscosity variations and (ii)\nthe variation of the shear-dependent stress-optic coefficient in the\nsecond-order SOL. Our findings indicate that the latter is more significant and\nshear-thinning behavior suppresses radial variations in phase retardation. This\nstudy demonstrates that the combination of the second-order SOL and\nrheo-optical measurements is essential for an accurate interpretation of flow\nbirefringence in Hele-Shaw flow, providing a noninvasive approach for stress\nfield analysis in high-aspect-ratio geometries.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 11:18:54 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Kawaguchi", "Misa", ""], ["Worby", "William Kai Alexander", ""], ["Yokoyama", "Yuto", ""], ["Suzuki", "Ryuta X.", ""], ["Nagatsu", "Yuichiro", ""], ["Tagawa", "Yoshiyuki", ""]], "extracted_entities": [{"text": "conventional stress-optic law", "label": "Scaling law"}, {"text": "SOL", "label": "Scaling law"}, {"text": "second-order SOL", "label": "LLM"}, {"text": "second-order SOL", "label": "LLM"}]}
{"id": "2503.10263", "submitter": "Christian Sendlinger", "authors": "Christian Sendlinger, Jonas Kellerer, Felix Spanier", "title": "KARL -- A Monte Carlo model for atomic and molecular processes in the\n  tritium atmosphere of the KATRIN experiment", "comments": "accepted for publication in Computer Physics Communications, 60\n  pages, 28 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new parallelized simulation code is presented, which uses a Monte Carlo\nmethod to determine particle spectra in the KATRIN source. Reaction chains are\ngenerated from the decay of tritium within the source. The code includes all\nrelevant processes: elastic scattering, ionization, excitation (electric,\nvibrational, rotational), recombination and various clustering processes. The\nmain emphasis of the code is the calculation of particle spectra and particle\ndensities and currents at specific points within the source. It features a new\ntechnique to determine these quantities. It also calculates target fields for\nthe interaction of particles with each other as it is needed for recombination\nprocesses. The code has been designed for the KATRIN experiment but is easily\nadapt-able for other tritium based experiments like Project 8. Geometry and\nbackground tritium gas flow can be given as user input. The code is\nparallelized using MPI and writes output using HDF5. Input to the simulation is\nread from a JSON description.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 11:20:40 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Sendlinger", "Christian", ""], ["Kellerer", "Jonas", ""], ["Spanier", "Felix", ""]], "extracted_entities": [{"text": "Reaction chains", "label": "Chain of thought"}]}
{"id": "2503.10265", "submitter": "Chang Han Low", "authors": "Chang Han Low, Ziyue Wang, Tianyi Zhang, Zhitao Zeng, Zhu Zhuo,\n  Evangelos B. Mazomenos, Yueming Jin", "title": "SurgRAW: Multi-Agent Workflow with Chain-of-Thought Reasoning for\n  Surgical Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Integration of Vision-Language Models (VLMs) in surgical intelligence is\nhindered by hallucinations, domain knowledge gaps, and limited understanding of\ntask interdependencies within surgical scenes, undermining clinical\nreliability. While recent VLMs demonstrate strong general reasoning and\nthinking capabilities, they still lack the domain expertise and task-awareness\nrequired for precise surgical scene interpretation. Although Chain-of-Thought\n(CoT) can structure reasoning more effectively, current approaches rely on\nself-generated CoT steps, which often exacerbate inherent domain gaps and\nhallucinations. To overcome this, we present SurgRAW, a CoT-driven multi-agent\nframework that delivers transparent, interpretable insights for most tasks in\nrobotic-assisted surgery. By employing specialized CoT prompts across five\ntasks: instrument recognition, action recognition, action prediction, patient\ndata extraction, and outcome assessment, SurgRAW mitigates hallucinations\nthrough structured, domain-aware reasoning. Retrieval-Augmented Generation\n(RAG) is also integrated to external medical knowledge to bridge domain gaps\nand improve response reliability. Most importantly, a hierarchical agentic\nsystem ensures that CoT-embedded VLM agents collaborate effectively while\nunderstanding task interdependencies, with a panel discussion mechanism\npromotes logical consistency. To evaluate our method, we introduce\nSurgCoTBench, the first reasoning-based dataset with structured frame-level\nannotations. With comprehensive experiments, we demonstrate the effectiveness\nof proposed SurgRAW with 29.32% accuracy improvement over baseline VLMs on 12\nrobotic procedures, achieving the state-of-the-art performance and advancing\nexplainable, trustworthy, and autonomous surgical assistance.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 11:23:13 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Low", "Chang Han", ""], ["Wang", "Ziyue", ""], ["Zhang", "Tianyi", ""], ["Zeng", "Zhitao", ""], ["Zhuo", "Zhu", ""], ["Mazomenos", "Evangelos B.", ""], ["Jin", "Yueming", ""]], "extracted_entities": [{"text": "Chain-of-Thought", "label": "Chain of thought"}, {"text": "SurgRAW", "label": "RAG"}, {"text": "specialized CoT prompts", "label": "Prompting"}, {"text": "SurgRAW", "label": "RAG"}, {"text": "RAG", "label": "RAG"}, {"text": "SurgRAW", "label": "RAG"}]}
{"id": "2503.10270", "submitter": "Zexuan Yan", "authors": "Zexuan Yan, Yue Ma, Chang Zou, Wenteng Chen, Qifeng Chen, Linfeng\n  Zhang", "title": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 11:26:45 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Yan", "Zexuan", ""], ["Ma", "Yue", ""], ["Zou", "Chang", ""], ["Chen", "Wenteng", ""], ["Chen", "Qifeng", ""], ["Zhang", "Linfeng", ""]], "extracted_entities": [{"text": "prompt-guided image editing", "label": "Prompting"}]}
{"id": "2503.10274", "submitter": "Zhichao Zhang", "authors": "Yangfan He and Zhichao Zhang", "title": "Symplectic Wigner Distribution in the Linear Canonical Transform Domain:\n  Theory and Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT math.FA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper devotes to combine the chirp basis function transformation and\nsymplectic coordinates transformation to yield a novel Wigner distribution (WD)\nassociated with the linear canonical transform (LCT), named as the symplectic\nWD in the LCT domain (SWDL). It incorporates the merits of the symplectic WD\n(SWD) and the WD in the LCT domain (WDL), achieving stronger capability in the\nlinear frequency-modulated (LFM) signal frequency rate feature extraction while\nmaintaining the same level of computational complexity. Some essential\nproperties of the SWDL are derived, including marginal distributions, energy\nconservations, unique reconstruction, Moyal formula, complex conjugate\nsymmetry, time reversal symmetry, scaling property, time translation property,\nfrequency modulation property, and time translation and frequency modulation\nproperty. Heisenberg's uncertainty principles of the SWDL are formulated,\ngiving rise to three kinds of lower bounds attainable respectively by Gaussian\nenveloped complex exponential signal, Gaussian signal and Gaussian enveloped\nchirp signal. The optimal symplectic matrices corresponding to the highest\ntime-frequency resolution are generated by solving the lower bound optimization\n(minimization) problem. The time-frequency resolution of the SWDL is compared\nwith those of the SWD and WDL to demonstrate its superiority in LFM signals\ntime-frequency energy concentration. A synthesis example is also carried out to\nverify the feasibility and reliability of the theoretical analysis.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 11:32:16 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["He", "Yangfan", ""], ["Zhang", "Zhichao", ""]], "extracted_entities": [{"text": "Moyal formula", "label": "Scaling law"}, {"text": "scaling property", "label": "Scaling law"}]}
{"id": "2503.10277", "submitter": "Wilhelm Kerle-Malcharek", "authors": "Wilhelm Kerle-Malcharek and Karsten Klein and Martin Wikelski and Falk\n  Schreiber and Timm A. Wild", "title": "Resource efficient data transmission on animals based on machine\n  learning", "comments": "Submitted to Scientific Reports but not published, 23 pages, 5\n  figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.ET cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bio-loggers, electronic devices used to track animal behaviour through\nvarious sensors, have become essential in wildlife research.\n  Despite continuous improvements in their capabilities, bio-loggers still face\nsignificant limitations in storage, processing, and data transmission due to\nthe constraints of size and weight, which are necessary to avoid disturbing the\nanimals.\n  This study aims to explore how selective data transmission, guided by machine\nlearning, can reduce the energy consumption of bio-loggers, thereby extending\ntheir operational lifespan without requiring hardware modifications.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 11:38:50 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Kerle-Malcharek", "Wilhelm", ""], ["Klein", "Karsten", ""], ["Wikelski", "Martin", ""], ["Schreiber", "Falk", ""], ["Wild", "Timm A.", ""]], "extracted_entities": [{"text": "machine\nlearning", "label": "Few-shot Learning"}]}
{"id": "2503.10282", "submitter": "Samih Karroum", "authors": "Samih Karroum, Saad Mazhar", "title": "HyperArm Bandit Optimization: A Novel approach to Hyperparameter\n  Optimization and an Analysis of Bandit Algorithms in Stochastic and\n  Adversarial Settings", "comments": "41 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper explores the application of bandit algorithms in both stochastic\nand adversarial settings, with a focus on theoretical analysis and practical\napplications. The study begins by introducing bandit problems, distinguishing\nbetween stochastic and adversarial variants, and examining key algorithms such\nas Explore-Then-Commit (ETC), Upper Confidence Bound (UCB), and\nExponential-Weight Algorithm for Exploration and Exploitation (EXP3).\nTheoretical regret bounds are analyzed to compare the performance of these\nalgorithms. The paper then introduces a novel framework, HyperArm Bandit\nOptimization (HABO), which applies EXP3 to hyperparameter tuning in machine\nlearning models. Unlike traditional methods that treat entire configurations as\narms, HABO treats individual hyperparameters as super-arms, and its potential\nconfigurations as sub-arms, enabling dynamic resource allocation and efficient\nexploration. Experimental results demonstrate HABO's effectiveness in\nclassification and regression tasks, outperforming Bayesian Optimization in\nterms of computational efficiency and accuracy. The paper concludes with\ninsights into the convergence guarantees of HABO and its potential for scalable\nand robust hyperparameter optimization.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 11:50:28 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Karroum", "Samih", ""], ["Mazhar", "Saad", ""]], "extracted_entities": [{"text": "hyperparameter tuning", "label": "Fine-tuning"}]}
{"id": "2503.10284", "submitter": "Zhen Zhang", "authors": "Zhen Zhang, Meihan Liu, Bingsheng He", "title": "PyGDA: A Python Library for Graph Domain Adaptation", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph domain adaptation has emerged as a promising approach to facilitate\nknowledge transfer across different domains. Recently, numerous models have\nbeen proposed to enhance their generalization capabilities in this field.\nHowever, there is still no unified library that brings together existing\ntechniques and simplifies their implementation. To fill this gap, we introduce\nPyGDA, an open-source Python library tailored for graph domain adaptation. As\nthe first comprehensive library in this area, PyGDA covers more than 20 widely\nused graph domain adaptation methods together with different types of graph\ndatasets. Specifically, PyGDA offers modular components, enabling users to\nseamlessly build custom models with a variety of commonly used utility\nfunctions. To handle large-scale graphs, PyGDA includes support for features\nsuch as sampling and mini-batch processing, ensuring efficient computation. In\naddition, PyGDA also includes comprehensive performance benchmarks and\nwell-documented user-friendly API for both researchers and practitioners. To\nfoster convenient accessibility, PyGDA is released under the MIT license at\nhttps://github.com/pygda-team/pygda, and the API documentation is\nhttps://pygda.readthedocs.io/en/stable/.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 11:52:23 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhang", "Zhen", ""], ["Liu", "Meihan", ""], ["He", "Bingsheng", ""]], "extracted_entities": [{"text": "PyGDA", "label": "Open-source LLMs"}, {"text": "PyGDA", "label": "Open-source LLMs"}, {"text": "PyGDA", "label": "Open-source LLMs"}, {"text": "PyGDA", "label": "Open-source LLMs"}, {"text": "PyGDA", "label": "Open-source LLMs"}, {"text": "PyGDA", "label": "Open-source LLMs"}, {"text": "pygda", "label": "Open-source LLMs"}, {"text": "pygda", "label": "Open-source LLMs"}]}
{"id": "2503.10286", "submitter": "Zhiqi Li", "authors": "Zhiqi Li, Chengrui Dong, Yiming Chen, Zhangchi Huang, Peidong Liu", "title": "VicaSplat: A Single Run is All You Need for 3D Gaussian Splatting and\n  Camera Estimation from Unposed Video Frames", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present VicaSplat, a novel framework for joint 3D Gaussians reconstruction\nand camera pose estimation from a sequence of unposed video frames, which is a\ncritical yet underexplored task in real-world 3D applications. The core of our\nmethod lies in a novel transformer-based network architecture. In particular,\nour model starts with an image encoder that maps each image to a list of visual\ntokens. All visual tokens are concatenated with additional inserted learnable\ncamera tokens. The obtained tokens then fully communicate with each other\nwithin a tailored transformer decoder. The camera tokens causally aggregate\nfeatures from visual tokens of different views, and further modulate them\nframe-wisely to inject view-dependent features. 3D Gaussian splats and camera\npose parameters can then be estimated via different prediction heads.\nExperiments show that VicaSplat surpasses baseline methods for multi-view\ninputs, and achieves comparable performance to prior two-view approaches.\nRemarkably, VicaSplat also demonstrates exceptional cross-dataset\ngeneralization capability on the ScanNet benchmark, achieving superior\nperformance without any fine-tuning. Project page:\nhttps://lizhiqi49.github.io/VicaSplat.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 11:56:05 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Zhiqi", ""], ["Dong", "Chengrui", ""], ["Chen", "Yiming", ""], ["Huang", "Zhangchi", ""], ["Liu", "Peidong", ""]], "extracted_entities": [{"text": "fine-tuning", "label": "Fine-tuning"}]}
{"id": "2503.10287", "submitter": "Hao Zhou", "authors": "Hao Zhou, Xiaobao Guo, Yuzhe Zhu, Adams Wai-Kin Kong", "title": "MACS: Multi-source Audio-to-image Generation with Contextual\n  Significance and Semantic Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.GR eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propelled by the breakthrough in deep generative models, audio-to-image\ngeneration has emerged as a pivotal cross-model task that converts complex\nauditory signals into rich visual representations. However, previous works only\nfocus on single-source audio inputs for image generation, ignoring the\nmulti-source characteristic in natural auditory scenes, thus limiting the\nperformance in generating comprehensive visual content. To bridge this gap, a\nmethod called MACS is proposed to conduct multi-source audio-to-image\ngeneration. This is the first work that explicitly separates multi-source audio\nto capture the rich audio components before image generation. MACS is a\ntwo-stage method. In the first stage, multi-source audio inputs are separated\nby a weakly supervised method, where the audio and text labels are semantically\naligned by casting into a common space using the large pre-trained CLAP model.\nWe introduce a ranking loss to consider the contextual significance of the\nseparated audio signals. In the second stage, efficient image generation is\nachieved by mapping the separated audio signals to the generation condition\nusing only a trainable adapter and a MLP layer. We preprocess the LLP dataset\nas the first full multi-source audio-to-image generation benchmark. The\nexperiments are conducted on multi-source, mixed-source, and single-source\naudio-to-image generation tasks. The proposed MACS outperforms the current\nstate-of-the-art methods in 17 of the 21 evaluation indexes on all tasks and\ndelivers superior visual quality. The code will be publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 11:56:25 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhou", "Hao", ""], ["Guo", "Xiaobao", ""], ["Zhu", "Yuzhe", ""], ["Kong", "Adams Wai-Kin", ""]], "extracted_entities": [{"text": "publicly available", "label": "Open-source LLMs"}]}
{"id": "2503.10289", "submitter": "Zebin He", "authors": "Zebin He, Mingxin Yang, Shuhui Yang, Yixuan Tang, Tao Wang, Kaihao\n  Zhang, Guanying Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, Wenhan Luo", "title": "MaterialMVP: Illumination-Invariant Material Generation via Multi-view\n  PBR Diffusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physically-based rendering (PBR) has become a cornerstone in modern computer\ngraphics, enabling realistic material representation and lighting interactions\nin 3D scenes. In this paper, we present MaterialMVP, a novel end-to-end model\nfor generating PBR textures from 3D meshes and image prompts, addressing key\nchallenges in multi-view material synthesis. Our approach leverages Reference\nAttention to extract and encode informative latent from the input reference\nimages, enabling intuitive and controllable texture generation. We also\nintroduce a Consistency-Regularized Training strategy to enforce stability\nacross varying viewpoints and illumination conditions, ensuring\nillumination-invariant and geometrically consistent results. Additionally, we\npropose Dual-Channel Material Generation, which separately optimizes albedo and\nmetallic-roughness (MR) textures while maintaining precise spatial alignment\nwith the input images through Multi-Channel Aligned Attention. Learnable\nmaterial embeddings are further integrated to capture the distinct properties\nof albedo and MR. Experimental results demonstrate that our model generates PBR\ntextures with realistic behavior across diverse lighting scenarios,\noutperforming existing methods in both consistency and quality for scalable 3D\nasset creation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 11:57:30 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["He", "Zebin", ""], ["Yang", "Mingxin", ""], ["Yang", "Shuhui", ""], ["Tang", "Yixuan", ""], ["Wang", "Tao", ""], ["Zhang", "Kaihao", ""], ["Chen", "Guanying", ""], ["Liu", "Yuhong", ""], ["Jiang", "Jie", ""], ["Guo", "Chunchao", ""], ["Luo", "Wenhan", ""]], "extracted_entities": [{"text": "image prompts", "label": "Prompting"}, {"text": "Reference\nAttention", "label": "Attention mechanism"}, {"text": "Multi-Channel Aligned Attention", "label": "Attention mechanism"}, {"text": "Learnable\nmaterial embeddings", "label": "contextual Embedding"}]}
{"id": "2503.10291", "submitter": "Weiyun Wang", "authors": "Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu\n  Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, Lewei Lu, Haodong\n  Duan, Yu Qiao, Jifeng Dai, Wenhai Wang", "title": "VisualPRM: An Effective Process Reward Model for Multimodal Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce VisualPRM, an advanced multimodal Process Reward Model (PRM)\nwith 8B parameters, which improves the reasoning abilities of existing\nMultimodal Large Language Models (MLLMs) across different model scales and\nfamilies with Best-of-N (BoN) evaluation strategies. Specifically, our model\nimproves the reasoning performance of three types of MLLMs and four different\nmodel scales. Even when applied to the highly capable InternVL2.5-78B, it\nachieves a 5.9-point improvement across seven multimodal reasoning benchmarks.\nExperimental results show that our model exhibits superior performance compared\nto Outcome Reward Models and Self-Consistency during BoN evaluation. To\nfacilitate the training of multimodal PRMs, we construct a multimodal process\nsupervision dataset VisualPRM400K using an automated data pipeline. For the\nevaluation of multimodal PRMs, we propose VisualProcessBench, a benchmark with\nhuman-annotated step-wise correctness labels, to measure the abilities of PRMs\nto detect erroneous steps in multimodal reasoning tasks. We hope that our work\ncan inspire more future research and contribute to the development of MLLMs.\nOur model, data, and benchmark are released in\nhttps://internvl.github.io/blog/2025-03-13-VisualPRM/.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 12:03:37 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Weiyun", ""], ["Gao", "Zhangwei", ""], ["Chen", "Lianjie", ""], ["Chen", "Zhe", ""], ["Zhu", "Jinguo", ""], ["Zhao", "Xiangyu", ""], ["Liu", "Yangzhou", ""], ["Cao", "Yue", ""], ["Ye", "Shenglong", ""], ["Zhu", "Xizhou", ""], ["Lu", "Lewei", ""], ["Duan", "Haodong", ""], ["Qiao", "Yu", ""], ["Dai", "Jifeng", ""], ["Wang", "Wenhai", ""]], "extracted_entities": [{"text": "Multimodal Large Language Models", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}]}
{"id": "2503.10294", "submitter": "Hsuvas Borkakoty", "authors": "Hsuvas Borkakoty and Luis Espinosa-Anke", "title": "Wikipedia is Not a Dictionary, Delete! Text Classification as a Proxy\n  for Analysing Wiki Deletion Discussions", "comments": "Accepted to WNUT-2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated content moderation for collaborative knowledge hubs like Wikipedia\nor Wikidata is an important yet challenging task due to multiple factors. In\nthis paper, we construct a database of discussions happening around articles\nmarked for deletion in several Wikis and in three languages, which we then use\nto evaluate a range of LMs on different tasks (from predicting the outcome of\nthe discussion to identifying the implicit policy an individual comment might\nbe pointing to). Our results reveal, among others, that discussions leading to\ndeletion are easier to predict, and that, surprisingly, self-produced tags\n(keep, delete or redirect) don't always help guiding the classifiers,\npresumably because of users' hesitation or deliberation within comments.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 12:07:35 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Borkakoty", "Hsuvas", ""], ["Espinosa-Anke", "Luis", ""]], "extracted_entities": [{"text": "Wikipedia", "label": "Open-source LLMs"}]}
{"id": "2503.10297", "submitter": "Peyman Neshaastegaran", "authors": "Peyman Neshaastegaran, and Ming Jian", "title": "CoDiPhy: A General Framework for Applying Denoising Diffusion Models to\n  the Physical Layer of Wireless Communication Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative models, including denoising diffusion models (DM), are gaining\nattention in wireless applications due to their ability to learn complex data\ndistributions. In this paper, we propose CoDiPhy, a novel framework that\nleverages conditional denoising diffusion models to address a wide range of\nwireless physical layer problems. A key challenge of using DM is the need to\nassume or approximate Gaussian signal models. CoDiPhy addresses this by\nincorporating a conditional encoder as a guidance mechanism, mapping problem\nobservations to a latent space and removing the Gaussian constraint. By\ncombining conditional encoding, time embedding layers, and a U-Net-based main\nneural network, CoDiPhy introduces a noise prediction neural network, replacing\nthe conventional approach used in DM. This adaptation enables CoDiPhy to serve\nas an effective solution for a wide range of detection, estimation, and\npredistortion tasks. We demonstrate CoDiPhy's adaptability through two case\nstudies: an OFDM receiver for detection and phase noise compensation for\nestimation. In both cases, CoDiPhy outperforms conventional methods by a\nsignificant margin.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 12:18:25 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Neshaastegaran", "Peyman", ""], ["Jian", "Ming", ""]], "extracted_entities": [{"text": "conditional encoding", "label": "Embedding"}, {"text": "time embedding layers", "label": "Embedding"}, {"text": "noise prediction neural network", "label": "Neural Language Model"}]}
{"id": "2503.10301", "submitter": "Moreno La Quatra", "authors": "Moreno La Quatra, Juan Rafael Orozco-Arroyave, Marco Sabato\n  Siniscalchi", "title": "Bilingual Dual-Head Deep Model for Parkinson's Disease Detection from\n  Speech", "comments": "Accepted at ICASSP 2025 - Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses", "journal-ref": null, "doi": "10.1109/ICASSP49660.2025.10889445", "report-no": null, "categories": "eess.AS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to tackle the Parkinson's disease (PD) detection problem from\nthe speech signal in a bilingual setting by proposing an ad-hoc dual-head deep\nneural architecture for type-based binary classification. One head is\nspecialized for diadochokinetic patterns. The other head looks for natural\nspeech patterns present in continuous spoken utterances. Only one of the two\nheads is operative accordingly to the nature of the input. Speech\nrepresentations are extracted from self-supervised learning (SSL) models and\nwavelet transforms. Adaptive layers, convolutional bottlenecks, and contrastive\nlearning are exploited to reduce variations across languages. Our solution is\nassessed against two distinct datasets, EWA-DB, and PC-GITA, which cover Slovak\nand Spanish languages, respectively. Results indicate that conventional models\ntrained on a single language dataset struggle with cross-linguistic\ngeneralization, and naive combinations of datasets are suboptimal. In contrast,\nour model improves generalization on both languages, simultaneously.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 12:23:11 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["La Quatra", "Moreno", ""], ["Orozco-Arroyave", "Juan Rafael", ""], ["Siniscalchi", "Marco Sabato", ""]], "extracted_entities": [{"text": "contrastive\nlearning", "label": "Few-shot Learning"}, {"text": "PC-GITA", "label": "Large Language Model"}]}
{"id": "2503.10302", "submitter": "Shuvro Chowdhury", "authors": "Shuvro Chowdhury, Navid Anjum Aadit, Andrea Grimaldi, Eleonora\n  Raimondo, Atharva Raut, P. Aaron Lott, Johan H. Mentink, Marek M. Rams,\n  Federico Ricci-Tersenghi, Massimo Chiappini, Luke S. Theogarajan, Tathagata\n  Srimani, Giovanni Finocchio, Masoud Mohseni, and Kerem Y. Camsari", "title": "Pushing the Boundary of Quantum Advantage in Hard Combinatorial\n  Optimization with Probabilistic Computers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.dis-nn cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent demonstrations on specialized benchmarks have reignited excitement for\nquantum computers, yet whether they can deliver an advantage for practical\nreal-world problems remains an open question. Here, we show that probabilistic\ncomputers (p-computers) when co-designed with hardware to implement powerful\nMonte Carlo algorithms surpass state-of-the-art quantum annealers\n[\\href{https://www.nature.com/articles/s41586-023-05867-2}{King et al., Nature\n(2023)}] in solving hard optimization problems. We focus on two key algorithms:\ndiscrete-time simulated quantum annealing (DT-SQA) and adaptive parallel\ntempering (APT), both applied to 3D spin glasses. For DT-SQA, we find that\nincreasing the number of replicas improves residual energy scaling, while\nparallelizing fewer replicas across independent runs also achieves comparable\nscaling. Both strategies align with the theoretical expectations from extreme\nvalue theory. In addition, APT outperforms DT-SQA when supported by non-local\nisoenergetic cluster moves. Finite-size scaling analysis suggests a universal\nbehavior that explains the superior performance of APT over both DT-SQA and\nquantum annealing. We show that these algorithms are readily implementable in\nmodern hardware thanks to the mature semiconductor technology. Unlike software\nsimulations, replicas can be monolithically housed on a single chip and a large\nnumber of spins can be updated in parallel and asynchronously, similar to a\nquantum annealer. We project that custom Field Programmable Gate Arrays (FPGA)\nor specialized chips leveraging massive parallelism can further accelerate\nthese algorithms by orders of magnitude, while drastically improving energy\nefficiency. Our results challenge the notion of a practical quantum advantage\nin optimization and present p-computers as scalable, energy-efficient hardware\nfor real-world optimization problems.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 12:24:13 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chowdhury", "Shuvro", ""], ["Aadit", "Navid Anjum", ""], ["Grimaldi", "Andrea", ""], ["Raimondo", "Eleonora", ""], ["Raut", "Atharva", ""], ["Lott", "P. Aaron", ""], ["Mentink", "Johan H.", ""], ["Rams", "Marek M.", ""], ["Ricci-Tersenghi", "Federico", ""], ["Chiappini", "Massimo", ""], ["Theogarajan", "Luke S.", ""], ["Srimani", "Tathagata", ""], ["Finocchio", "Giovanni", ""], ["Mohseni", "Masoud", ""], ["Camsari", "Kerem Y.", ""]], "extracted_entities": [{"text": "residual energy scaling", "label": "Scaling law"}]}
{"id": "2503.10305", "submitter": "Emil Mededovic", "authors": "Emil Mededovic, Yuli Wu, Henning Konermann, Marcin Kopaczka, Mareike\n  Schulz, Rene Tolba, Johannes Stegmaier", "title": "Eye on the Target: Eye Tracking Meets Rodent Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing animal behavior from video recordings is crucial for scientific\nresearch, yet manual annotation remains labor-intensive and prone to\nsubjectivity. Efficient segmentation methods are needed to automate this\nprocess while maintaining high accuracy. In this work, we propose a novel\npipeline that utilizes eye-tracking data from Aria glasses to generate prompt\npoints, which are then used to produce segmentation masks via a fast zero-shot\nsegmentation model. Additionally, we apply post-processing to refine the\nprompts, leading to improved segmentation quality. Through our approach, we\ndemonstrate that combining eye-tracking-based annotation with smart prompt\nrefinement can enhance segmentation accuracy, achieving an improvement of 70.6%\nfrom 38.8 to 66.2 in the Jaccard Index for segmentation results in the rats\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 12:27:42 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Mededovic", "Emil", ""], ["Wu", "Yuli", ""], ["Konermann", "Henning", ""], ["Kopaczka", "Marcin", ""], ["Schulz", "Mareike", ""], ["Tolba", "Rene", ""], ["Stegmaier", "Johannes", ""]], "extracted_entities": [{"text": "prompt\npoints", "label": "Prompting"}]}
{"id": "2503.10306", "submitter": "Tolgahan Bardakci", "authors": "Tolgahan Bardakci, Serge Demeyer, Mutlu Beyazit", "title": "Test Amplification for REST APIs Using \"Out-of-the-box\" Large Language\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  REST APIs are an indispensable building block in today's cloud-native\napplications, so testing them is critically important. However, writing\nautomated tests for such REST APIs is challenging because one needs strong and\nreadable tests that exercise the boundary values of the protocol embedded in\nthe REST API. In this paper, we report our experience with using \"out of the\nbox\" large language models (ChatGPT and GitHub's Copilot) to amplify REST API\ntest suites. We compare the resulting tests based on coverage and\nunderstandability, and we derive a series of guidelines and lessons learned\nconcerning the prompts that result in the strongest test suite.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 12:30:14 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Bardakci", "Tolgahan", ""], ["Demeyer", "Serge", ""], ["Beyazit", "Mutlu", ""]], "extracted_entities": [{"text": "ChatGPT", "label": "ChatGPT"}, {"text": "GitHub", "label": "Open-source LLMs"}, {"text": "prompts", "label": "Prompting"}]}
{"id": "2503.10307", "submitter": "Martin C\\'ifka", "authors": "Georgy Ponimatkin, Martin C\\'ifka, Tom\\'a\\v{s} Sou\\v{c}ek, M\\'ed\\'eric\n  Fourmy, Yann Labb\\'e, Vladimir Petrik, Josef Sivic", "title": "6D Object Pose Tracking in Internet Videos for Robotic Manipulation", "comments": "Accepted to ICLR 2025. Project page available at\n  https://ponimatkin.github.io/wildpose/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We seek to extract a temporally consistent 6D pose trajectory of a\nmanipulated object from an Internet instructional video. This is a challenging\nset-up for current 6D pose estimation methods due to uncontrolled capturing\nconditions, subtle but dynamic object motions, and the fact that the exact mesh\nof the manipulated object is not known. To address these challenges, we present\nthe following contributions. First, we develop a new method that estimates the\n6D pose of any object in the input image without prior knowledge of the object\nitself. The method proceeds by (i) retrieving a CAD model similar to the\ndepicted object from a large-scale model database, (ii) 6D aligning the\nretrieved CAD model with the input image, and (iii) grounding the absolute\nscale of the object with respect to the scene. Second, we extract smooth 6D\nobject trajectories from Internet videos by carefully tracking the detected\nobjects across video frames. The extracted object trajectories are then\nretargeted via trajectory optimization into the configuration space of a\nrobotic manipulator. Third, we thoroughly evaluate and ablate our 6D pose\nestimation method on YCB-V and HOPE-Video datasets as well as a new dataset of\ninstructional videos manually annotated with approximate 6D object\ntrajectories. We demonstrate significant improvements over existing\nstate-of-the-art RGB 6D pose estimation methods. Finally, we show that the 6D\nobject motion estimated from Internet videos can be transferred to a 7-axis\nrobotic manipulator both in a virtual simulator as well as in a real world\nset-up. We also successfully apply our method to egocentric videos taken from\nthe EPIC-KITCHENS dataset, demonstrating potential for Embodied AI\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 12:33:34 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Ponimatkin", "Georgy", ""], ["C\u00edfka", "Martin", ""], ["Sou\u010dek", "Tom\u00e1\u0161", ""], ["Fourmy", "M\u00e9d\u00e9ric", ""], ["Labb\u00e9", "Yann", ""], ["Petrik", "Vladimir", ""], ["Sivic", "Josef", ""]], "extracted_entities": [{"text": "CAD model", "label": "AI model"}, {"text": "CAD model", "label": "AI model"}]}
{"id": "2503.10310", "submitter": "Shin Yoo Dr", "authors": "Shin Yoo and Robert Feldt and Somin Kim and Naryeong Kim", "title": "Capturing Semantic Flow of ML-based Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  ML-based systems are software systems that incorporates machine learning\ncomponents such as Deep Neural Networks (DNNs) or Large Language Models (LLMs).\nWhile such systems enable advanced features such as high performance computer\nvision, natural language processing, and code generation, their internal\nbehaviour remain largely opaque to traditional dynamic analysis such as\ntesting: existing analysis typically concern only what is observable from the\noutside, such as input similarity or class label changes. We propose semantic\nflow, a concept designed to capture the internal behaviour of ML-based system\nand to provide a platform for traditional dynamic analysis techniques to be\nadapted to. Semantic flow combines the idea of control flow with internal\nstates taken from executions of ML-based systems, such as activation values of\na specific layer in a DNN, or embeddings of LLM responses at a specific\ninference step of LLM agents. The resulting representation, summarised as\nsemantic flow graphs, can capture internal decisions that are not explicitly\nrepresented in the traditional control flow of ML-based systems. We propose the\nidea of semantic flow, introduce two examples using a DNN and an LLM agent, and\nfinally sketch its properties and how it can be used to adapt existing dynamic\nanalysis techniques for use in ML-based software systems.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 12:39:04 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Yoo", "Shin", ""], ["Feldt", "Robert", ""], ["Kim", "Somin", ""], ["Kim", "Naryeong", ""]], "extracted_entities": [{"text": "ML-based systems", "label": "LLM-based"}, {"text": "Large Language Models", "label": "Large Language Model"}, {"text": "ML-based systems", "label": "LLM-based"}, {"text": "embeddings", "label": "Embedding"}]}
{"id": "2503.10312", "submitter": "Theo Di Piazza", "authors": "Theo Di Piazza and Loic Boussel", "title": "PS3C: An Ensemble-Based Two-Step Framework for Classification of Pep\n  Smear Cell Images", "comments": "7 pages, 3 figures, Grand Challenge paper accepted at ISBI 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Early detection of cervical cancer is crucial for improving patient outcomes\nand reducing mortality by identifying precancerous lesions as soon as possible.\nAs a result, the use of pap smear screening has significantly increased,\nleading to a growing demand for automated tools that can assist cytologists\nmanaging their rising workload. To address this, the Pep Smear Cell\nClassification Challenge (PS3C) has been organized in association with ISBI in\n2025. This project aims to promote the development of automated tools for pep\nsmear images classification. The analyzed images are grouped into four\ncategories: healthy, unhealthy, both, and rubbish images which are considered\nas unsuitable for diagnosis. In this work, we propose a two-stage ensemble\napproach: first, a neural network determines whether an image is rubbish or\nnot. If not, a second neural network classifies the image as containing a\nhealthy cell, an unhealthy cell, or both.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 12:46:23 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Di Piazza", "Theo", ""], ["Boussel", "Loic", ""]], "extracted_entities": [{"text": "neural network", "label": "Neural Language Model"}, {"text": "neural network", "label": "Neural Language Model"}]}
{"id": "2503.10322", "submitter": "Haoxuan Li", "authors": "Haoxuan Li, Sixu Yan, Yuhan Li, Xinggang Wang", "title": "Towards Fast, Memory-based and Data-Efficient Vision-Language Policy", "comments": "11 pages, 7 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision Language Models (VLMs) pretrained on Internet-scale vision-language\ndata have demonstrated the potential to transfer their knowledge to robotic\nlearning. However, the existing paradigm encounters three critical challenges:\n(1) expensive inference cost resulting from large-scale model parameters, (2)\nfrequent domain shifts caused by mismatched data modalities, and (3) limited\ncapacity to handle past or future experiences. In this work, we propose\nLiteVLP, a lightweight, memory-based, and general-purpose vision-language\npolicy generation model. LiteVLP is built upon a pre-trained 1B-parameter VLM\nand fine-tuned on a tiny-scale and conversation-style robotic dataset. Through\nextensive experiments, we demonstrate that LiteVLP outperforms state-of-the-art\nvision-language policy on VIMA-Bench, with minimal training time. Furthermore,\nLiteVLP exhibits superior inference speed while maintaining exceptional high\naccuracy. In long-horizon manipulation tasks, LiteVLP also shows remarkable\nmemory ability, outperforming the best-performing baseline model by 18.8%.\nThese results highlight LiteVLP as a promising model to integrating the\nintelligence of VLMs into robotic learning.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 12:58:40 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Haoxuan", ""], ["Yan", "Sixu", ""], ["Li", "Yuhan", ""], ["Wang", "Xinggang", ""]], "extracted_entities": [{"text": "Vision Language Models", "label": "Large Language Model"}, {"text": "robotic\nlearning", "label": "Few-shot Learning"}, {"text": "fine-tuned", "label": "Fine-tuning"}, {"text": "robotic learning", "label": "Few-shot Learning"}]}
{"id": "2503.10324", "submitter": "Pingping Zhang Dr", "authors": "Yuhao Wang and Yongfeng Lv and Pingping Zhang and Huchuan Lu", "title": "IDEA: Inverted Text with Cooperative Deformable Aggregation for\n  Multi-modal Object Re-Identification", "comments": "This work is accepted by CVPR2025. More modifications may be\n  performed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal object Re-IDentification (ReID) aims to retrieve specific objects\nby utilizing complementary information from various modalities. However,\nexisting methods focus on fusing heterogeneous visual features, neglecting the\npotential benefits of text-based semantic information. To address this issue,\nwe first construct three text-enhanced multi-modal object ReID benchmarks. To\nbe specific, we propose a standardized multi-modal caption generation pipeline\nfor structured and concise text annotations with Multi-modal Large Language\nModels (MLLMs). Besides, current methods often directly aggregate multi-modal\ninformation without selecting representative local features, leading to\nredundancy and high complexity. To address the above issues, we introduce IDEA,\na novel feature learning framework comprising the Inverted Multi-modal Feature\nExtractor (IMFE) and Cooperative Deformable Aggregation (CDA). The IMFE\nutilizes Modal Prefixes and an InverseNet to integrate multi-modal information\nwith semantic guidance from inverted text. The CDA adaptively generates\nsampling positions, enabling the model to focus on the interplay between global\nfeatures and discriminative local features. With the constructed benchmarks and\nthe proposed modules, our framework can generate more robust multi-modal\nfeatures under complex scenarios. Extensive experiments on three multi-modal\nobject ReID benchmarks demonstrate the effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 13:00:31 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Yuhao", ""], ["Lv", "Yongfeng", ""], ["Zhang", "Pingping", ""], ["Lu", "Huchuan", ""]], "extracted_entities": [{"text": "Multi-modal Large Language\nModels", "label": "Large Language Model"}, {"text": "Modal Prefixes", "label": "Embedding"}]}
{"id": "2503.10325", "submitter": "Jianchun Liu", "authors": "Luyao Gao, Jianchun Liu, Hongli Xu, Liusheng Huang", "title": "Collaborative Speculative Inference for Efficient LLM Inference Serving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speculative inference is a promising paradigm employing small speculative\nmodels (SSMs) as drafters to generate draft tokens, which are subsequently\nverified in parallel by the target large language model (LLM). This approach\nenhances the efficiency of inference serving by reducing LLM inference latency\nand costs while preserving generation quality. However, existing speculative\nmethods face critical challenges, including inefficient resource utilization\nand limited draft acceptance, which constrain their scalability and overall\neffectiveness. To overcome these obstacles, we present CoSine, a novel\nspeculative inference system that decouples sequential speculative decoding\nfrom parallel verification, enabling efficient collaboration among multiple\nnodes. Specifically, CoSine routes inference requests to specialized drafters\nbased on their expertise and incorporates a confidence-based token fusion\nmechanism to synthesize outputs from cooperating drafters, ensuring\nhigh-quality draft generation. Additionally, CoSine dynamically orchestrates\nthe execution of speculative decoding and verification in a pipelined manner,\nemploying batch scheduling to selectively group requests and adaptive\nspeculation control to minimize idle periods. By optimizing parallel workflows\nthrough heterogeneous node collaboration, CoSine balances draft generation and\nverification throughput in real-time, thereby maximizing resource utilization.\nExperimental results demonstrate that CoSine achieves superior performance\ncompared to state-of-the-art speculative approaches. Notably, with equivalent\nresource costs, CoSine achieves up to a 23.2% decrease in latency and a 32.5%\nincrease in throughput compared to baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 13:03:38 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Gao", "Luyao", ""], ["Liu", "Jianchun", ""], ["Xu", "Hongli", ""], ["Huang", "Liusheng", ""]], "extracted_entities": [{"text": "target large language model", "label": "Large Language Model"}, {"text": "LLM", "label": "Large Language Model"}, {"text": "LLM", "label": "Large Language Model"}, {"text": "CoSine", "label": "LLM-based"}, {"text": "CoSine", "label": "LLM-based"}, {"text": "CoSine", "label": "LLM-based"}, {"text": "CoSine", "label": "LLM-based"}, {"text": "CoSine", "label": "LLM-based"}, {"text": "CoSine", "label": "LLM-based"}]}
{"id": "2503.10330", "submitter": "Peng Rao", "authors": "Peng Rao, Roderich Moessner, Johannes Knolle", "title": "Dynamical response theory of interacting Majorana fermions and its\n  application to generic Kitaev quantum spin liquids in a field", "comments": "19 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.str-el", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the appearance of Majorana fermions in a broad range of\ncorrelated and topological electronic systems, we develop a general method to\ncompute the dynamical response of interacting Majorana fermions in the\nrandom-phase approximation (RPA). This can be applied self-consistently on top\nof Majorana mean-field theory (MFT) backgrounds, thereby in particular\nproviding a powerful tool to analyse $\\textit{generic}$ behaviour in the\nvicinity of (various heavily studied) exactly soluble models. Prime examples\nare quantum spin liquids (QSL) with emergent Majorana excitations, with the\ncelebrated exact solution of Kitaev. We employ the RPA to study in considerable\ndetail phase structure and dynamics of the extended Kitaev honeycomb\n$KJ\\Gamma$-model, with and without an applied field. First, we benchmark our\nmethod with Kitaev's exactly soluble model, finding a remarkable agreement. The\ninteractions between Majorana fermions even turn out to mimic the effect of\nlocal $\\mathbb{Z}_2$ flux excitations, which we explain analytically. Second,\nwe show how small non-Kitaev couplings $J$ and $\\Gamma$ induce Majorana bound\nstates, resulting in sharp features in the dynamical structure factor in the\npresence of fractionalisation: such 'spinon excitons' naturally appear, and can\ncoexist and interact with the broad Majorana continuum. Third, for increasing\ncouplings or field, our theory predicts instabilities of the KQSL triggered by\nthe condensation of the sharp modes. From the high symmetry momenta of the\ncondensation we can deduce which magnetically ordered phases surround the KQSL,\nin good agreement with previous finite-size numerics. We discuss implications\nfor experiments and the broad range of applicability of our method to other QSL\nand Majorana systems.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 13:07:28 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Rao", "Peng", ""], ["Moessner", "Roderich", ""], ["Knolle", "Johannes", ""]], "extracted_entities": [{"text": "RPA", "label": "RAG"}, {"text": "fractionalisation", "label": "quantisation"}]}
{"id": "2503.10335", "submitter": "Enhua Xu", "authors": "Enhua Xu, William Dawson, Takahito Nakajima", "title": "Distributed implementation of tensor-product selected configuration\n  interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the hybrid \"QC+HPC\" strategy - where quantum computers\nscreen important determinants, followed by exact diagonalization on classical\ncomputers - has shown great potential in the study of strongly correlated\nsystems in quantum chemistry. Last year, an IBM team proposed a novel scheme\nthat utilizes quantum computers to select important bit strings that are then\nused to construct a spin-adapted determinant space via tensor products.\nInspired by this, we have specifically designed a completely new algorithm for\nthis tensor-product selected configuration interaction (SCI). Moreover, for the\nfirst time worldwide, we have implemented distributed storage of the CI vector\nin an SCI program, enabling efficient handling of large-scale computation.\nSince this study is independent and does not involve determinant selection by\nquantum computers, we employed our SCI program to conduct full configuration\ninteraction (FCI) computations. Our FCI calculations for N$_2$ (aug-cc-pVDZ)\nunder D$_{2h}$ symmetry and CN (cc-pVTZ) under C$_{2v}$ symmetry, involving\n$1.47 \\times 10^{11}$ and $4.86 \\times 10^{11}$ determinants, respectively,\nexceed the previous record of $2 \\times 10^{9}$ determinants computed with the\nDICE program [J. Chem. Phys. 149, 214110 (2018)] by more than two orders of\nmagnitude. These results set a new benchmark for SCI computations and lay the\ngroundwork for further advancements in SCI methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 13:13:59 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Xu", "Enhua", ""], ["Dawson", "William", ""], ["Nakajima", "Takahito", ""]], "extracted_entities": [{"text": "exact diagonalization", "label": "quantisation"}]}
{"id": "2503.10337", "submitter": "Vivek Chari", "authors": "Vivek Chari, Guanghui Qin, Benjamin Van Durme", "title": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 13:15:28 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chari", "Vivek", ""], ["Qin", "Guanghui", ""], ["Van Durme", "Benjamin", ""]], "extracted_entities": [{"text": "self-attention", "label": "Attention mechanism"}, {"text": "standard Transformers", "label": "Transformers"}, {"text": "KV-Distill", "label": "Generative Pre-trained Transformer (GPT)"}, {"text": "fine-tuned", "label": "Fine-tuning"}]}
{"id": "2503.10342", "submitter": "Qi Zhao", "authors": "Qi Zhao and Zhan Ma and Pan Zhou", "title": "DreamInsert: Zero-Shot Image-to-Video Object Insertion from A Single\n  Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in generative diffusion models have turned many dreams\ninto realities. For video object insertion, existing methods typically require\nadditional information, such as a reference video or a 3D asset of the object,\nto generate the synthetic motion. However, inserting an object from a single\nreference photo into a target background video remains an uncharted area due to\nthe lack of unseen motion information. We propose DreamInsert, which achieves\nImage-to-Video Object Insertion in a training-free manner for the first time.\nBy incorporating the trajectory of the object into consideration, DreamInsert\ncan predict the unseen object movement, fuse it harmoniously with the\nbackground video, and generate the desired video seamlessly. More\nsignificantly, DreamInsert is both simple and effective, achieving zero-shot\ninsertion without end-to-end training or additional fine-tuning on\nwell-designed image-video data pairs. We demonstrated the effectiveness of\nDreamInsert through a variety of experiments. Leveraging this capability, we\npresent the first results for Image-to-Video object insertion in a\ntraining-free manner, paving exciting new directions for future content\ncreation and synthesis. The code will be released soon.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 13:20:54 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhao", "Qi", ""], ["Ma", "Zhan", ""], ["Zhou", "Pan", ""]], "extracted_entities": [{"text": "DreamInsert", "label": "Embedding"}, {"text": "Image-to-Video Object Insertion", "label": "Embedding"}, {"text": "DreamInsert", "label": "Embedding"}, {"text": "DreamInsert", "label": "Embedding"}, {"text": "zero-shot\ninsertion", "label": "Zero-shot Learning"}, {"text": "additional fine-tuning", "label": "Fine-tuning"}, {"text": "DreamInsert", "label": "Embedding"}]}
{"id": "2503.10346", "submitter": "Xin Zhang", "authors": "Ji-Yu Song, Jing-Zhao Qi, Jing-Fei Zhang, Xin Zhang", "title": "Model-independent $H_0$ within FLRW: Joint constraints from GWTC-3\n  standard sirens and strong lensing time delays", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.CO gr-qc hep-ph hep-th", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hubble tension has emerged as a critical crisis in cosmology, with the\ncause remaining unclear. Determining the Hubble constant ($H_0$) independently\nof cosmological models and distance ladders will help resolve this crisis. In\nthis letter, we for the first time use 47 gravitational-wave (GW) standard\nsirens from the third Gravitational-Wave Transient Catalog to calibrate\ndistances in the strong lensing system, RXJ1131-1231, and constrain $H_0$\nthrough the distance-sum rule, with minimal cosmological assumptions. We assume\nthat light propagation over long distances is described by the\nFriedmann-Lemaitre-Robertson-Walker metric and that geometrical optics holds,\nbut we do not need to assume the universe's contents or the theory of gravity\non cosmological scales. Fixing $\\Omega_K=0$, we obtain\n$H_0=73.22^{+5.95}_{-5.43}$ ${\\rm km}~{\\rm s}^{-1}~{\\rm Mpc}^{-1}$ and\n$H_0=70.40^{+8.03}_{-5.60}$ ${\\rm km}~{\\rm s}^{-1}~{\\rm Mpc}^{-1}$ by using the\ndeflector galaxy's mass model and kinematic measurements to break mass-sheet\ntransform, respectively. When $\\Omega_K$ is not fixed, the central value of\n$H_0$ increases further. We find that our results are still dominated by\nstatistical errors, and at the same time, we notice the great potential of\nusing GW dark sirens to provide calibration, owing to their higher redshifts.\nWhen using 42 binary black holes and RXJ1131-1231, we obtain a $8.46 \\%$ $H_0$\nconstraint precision, which is better than that from the bright siren GW170817\nusing the Hubble law by about $40\\%$. In the future, as the redshift range of\nGW dark sirens increases, more and more SGLTDs can be included, and we can\nachieve high-precision, model-independent measurements of $H_0$ without the\nneed for GW bright sirens.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 13:26:07 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Song", "Ji-Yu", ""], ["Qi", "Jing-Zhao", ""], ["Zhang", "Jing-Fei", ""], ["Zhang", "Xin", ""]], "extracted_entities": [{"text": "Friedmann-Lemaitre-Robertson-Walker metric", "label": "Scaling law"}, {"text": "Hubble law", "label": "Scaling law"}]}
{"id": "2503.10348", "submitter": "Folkert Kuipers", "authors": "Folkert Kuipers", "title": "Quantum Theory, Gravity and Higher Order Geometry", "comments": "17+6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "gr-qc hep-th math-ph math.MP quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fact that quantum theory is non-differentiable, while general relativity\nis built on the assumption of differentiability sources an incompatibility\nbetween quantum theory and gravity. Higher order geometry addresses this issue\ndirectly by extending differential geometry, such that it can be applied to\ntheories that are non-differentiable, but have a certain degree of H\\\"older\nregularity. As this includes the path integral formulation of quantum theory,\nit provides a natural mathematical framework for describing the interplay\nbetween gravity and quantum theory. In this article, we review the motivation\nfor and the basic features of this framework and point towards future\ndevelopments.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 13:26:33 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Kuipers", "Folkert", ""]], "extracted_entities": [{"text": "quantum theory", "label": "quantisation"}]}
{"id": "2503.10350", "submitter": "Ali Salar", "authors": "Ali Salar, Qing Liu, Yingli Tian and Guoying Zhao", "title": "Enhancing Facial Privacy Protection via Weakening Diffusion Purification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of social media has led to the widespread sharing of\nindividual portrait images, which pose serious privacy risks due to the\ncapabilities of automatic face recognition (AFR) systems for mass surveillance.\nHence, protecting facial privacy against unauthorized AFR systems is essential.\nInspired by the generation capability of the emerging diffusion models, recent\nmethods employ diffusion models to generate adversarial face images for privacy\nprotection. However, they suffer from the diffusion purification effect,\nleading to a low protection success rate (PSR). In this paper, we first propose\nlearning unconditional embeddings to increase the learning capacity for\nadversarial modifications and then use them to guide the modification of the\nadversarial latent code to weaken the diffusion purification effect. Moreover,\nwe integrate an identity-preserving structure to maintain structural\nconsistency between the original and generated images, allowing human observers\nto recognize the generated image as having the same identity as the original.\nExtensive experiments conducted on two public datasets, i.e., CelebA-HQ and\nLADN, demonstrate the superiority of our approach. The protected faces\ngenerated by our method outperform those produced by existing facial privacy\nprotection approaches in terms of transferability and natural appearance.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 13:27:53 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Salar", "Ali", ""], ["Liu", "Qing", ""], ["Tian", "Yingli", ""], ["Zhao", "Guoying", ""]], "extracted_entities": [{"text": "unconditional embeddings", "label": "Embedding"}]}
{"id": "2503.10351", "submitter": "Sinuo Liu", "authors": "Sinuo Liu, Chenyang Lyu, Minghao Wu, Longyue Wang, Weihua Luo, Kaifu\n  Zhang", "title": "New Trends for Modern Machine Translation with Large Reasoning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in Large Reasoning Models (LRMs), particularly those\nleveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility\nfor Machine Translation (MT). This position paper argues that LRMs\nsubstantially transformed traditional neural MT as well as LLMs-based MT\nparadigms by reframing translation as a dynamic reasoning task that requires\ncontextual, cultural, and linguistic understanding and reasoning. We identify\nthree foundational shifts: 1) contextual coherence, where LRMs resolve\nambiguities and preserve discourse structure through explicit reasoning over\ncross-sentence and complex context or even lack of context; 2) cultural\nintentionality, enabling models to adapt outputs by inferring speaker intent,\naudience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can\nperform self-reflection during the inference time to correct the potential\nerrors in translation especially extremely noisy cases, showing better\nrobustness compared to simply mapping X->Y translation. We explore various\nscenarios in translation including stylized translation, document-level\ntranslation and multimodal translation by showcasing empirical examples that\ndemonstrate the superiority of LRMs in translation. We also identify several\ninteresting phenomenons for LRMs for MT including auto-pivot translation as\nwell as the critical challenges such as over-localisation in translation and\ninference efficiency. In conclusion, we think that LRMs redefine translation\nsystems not merely as text converters but as multilingual cognitive agents\ncapable of reasoning about meaning beyond the text. This paradigm shift reminds\nus to think of problems in translation beyond traditional translation scenarios\nin a much broader context with LRMs - what we can achieve on top of it.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 13:27:53 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Liu", "Sinuo", ""], ["Lyu", "Chenyang", ""], ["Wu", "Minghao", ""], ["Wang", "Longyue", ""], ["Luo", "Weihua", ""], ["Zhang", "Kaifu", ""]], "extracted_entities": [{"text": "Large Reasoning Models", "label": "Large Language Model"}, {"text": "Chain-of-Thought reasoning", "label": "Chain of thought"}, {"text": "contextual coherence", "label": "Chain of thought"}, {"text": "LRMs", "label": "Large Language Model"}, {"text": "LRMs", "label": "Large Language Model"}, {"text": "LRMs", "label": "Large Language Model"}, {"text": "LRMs", "label": "Large Language Model"}, {"text": "LRMs", "label": "Large Language Model"}, {"text": "LRMs", "label": "Large Language Model"}]}
{"id": "2503.10357", "submitter": "Viktor Moskvoretskii", "authors": "Viktor Moskvoretskii, Alina Lobanova, Ekaterina Neminova, Chris\n  Biemann, Alexander Panchenko, Irina Nikishina", "title": "Do I look like a `cat.n.01` to you? A Taxonomy Image Generation\n  Benchmark", "comments": "Labeled data and generated image Wordnet are published at\n  https://huggingface.co/collections/VityaVitalich/generated-image-wordnet-67d2c868ff1414ec2f8e0d3d", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper explores the feasibility of using text-to-image models in a\nzero-shot setup to generate images for taxonomy concepts. While text-based\nmethods for taxonomy enrichment are well-established, the potential of the\nvisual dimension remains unexplored. To address this, we propose a\ncomprehensive benchmark for Taxonomy Image Generation that assesses models'\nabilities to understand taxonomy concepts and generate relevant, high-quality\nimages. The benchmark includes common-sense and randomly sampled WordNet\nconcepts, alongside the LLM generated predictions. The 12 models are evaluated\nusing 9 novel taxonomy-related text-to-image metrics and human feedback.\nMoreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for\nimage generation. Experimental results show that the ranking of models differs\nsignificantly from standard T2I tasks. Playground-v2 and FLUX consistently\noutperform across metrics and subsets and the retrieval-based approach performs\npoorly. These findings highlight the potential for automating the curation of\nstructured data resources.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 13:37:54 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Moskvoretskii", "Viktor", ""], ["Lobanova", "Alina", ""], ["Neminova", "Ekaterina", ""], ["Biemann", "Chris", ""], ["Panchenko", "Alexander", ""], ["Nikishina", "Irina", ""]], "extracted_entities": [{"text": "LLM", "label": "LLM"}, {"text": "GPT-4", "label": "GPT"}]}
{"id": "2503.10358", "submitter": "Zirun Guo", "authors": "Zirun Guo, Tao Jin", "title": "ConceptGuard: Continual Personalized Text-to-Image Generation with\n  Forgetting and Confusion Mitigation", "comments": "Accepted at CVPR 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diffusion customization methods have achieved impressive results with only a\nminimal number of user-provided images. However, existing approaches customize\nconcepts collectively, whereas real-world applications often require sequential\nconcept integration. This sequential nature can lead to catastrophic\nforgetting, where previously learned concepts are lost. In this paper, we\ninvestigate concept forgetting and concept confusion in the continual\ncustomization. To tackle these challenges, we present ConceptGuard, a\ncomprehensive approach that combines shift embedding, concept-binding prompts\nand memory preservation regularization, supplemented by a priority queue which\ncan adaptively update the importance and occurrence order of different\nconcepts. These strategies can dynamically update, unbind and learn the\nrelationship of the previous concepts, thus alleviating concept forgetting and\nconfusion. Through comprehensive experiments, we show that our approach\noutperforms all the baseline methods consistently and significantly in both\nquantitative and qualitative analyses.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 13:39:24 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Guo", "Zirun", ""], ["Jin", "Tao", ""]], "extracted_entities": [{"text": "shift embedding", "label": "Embedding"}, {"text": "concept-binding prompts", "label": "Prompting"}]}
{"id": "2503.10362", "submitter": "Ruggero Bettinardi", "authors": "Ruggero G. Bettinardi, Mohamed Rahmouni and Ulysse Gimenez", "title": "BioSerenity-E1: a self-supervised EEG model for medical applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG eess.SP q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Electroencephalography (EEG) serves as an essential diagnostic tool in\nneurology; however, its accurate manual interpretation is a time-intensive\nprocess that demands highly specialized expertise, which remains relatively\nscarce and not consistently accessible. To address these limitations, the\nimplementation of automated pre-screening and analysis systems for EEG data\nholds considerable promise. Advances in self-supervised learning made it\npossible to pre-train complex deep learning architectures on large volumes of\nunlabeled EEG data to learn generalizable representations, that can later be\nused to enhance performance on multiple tasks while needing less downstream\ndata. In the present paper, we introduce BioSerenity-E1, the first of a family\nof self-supervised foundation models for clinical EEG applications that\ncombines spectral tokenization with masked prediction to achieve\nstate-of-the-art performance across relevant diagnostic tasks. The two-phase\nself-supervised pretraining framework initially acquires compressed EEG\nrepresentations via a transformer-based VQ-VAE architecture designed to\nreconstruct log-multitaper spectral projections, then implements extensive (70%\nblock) masked token prediction to force the model to learn complex\nspatiotemporal dependencies in EEG signals. BioSerenity-E1 achieves strong\nperformance across three clinical tasks, either in line or above\nstate-of-the-art methods: seizure detection (AUROC = 0.926, Sensitivity =\n0.909), normal/abnormal classification (AUPRC = 0.970 on proprietary data;\n0.910 on TUH-Abnormal), and multiclass pathology differentiation on unbalanced\ndata (Weighted F1 = 0.730). The utility of BioSerenity-E1 is further confirmed\nin low-data regimes scenarios, showing clear improvements in AUPRC (from +2% to\n17%) when trained on less than 10% of the available data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 13:42:46 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Bettinardi", "Ruggero G.", ""], ["Rahmouni", "Mohamed", ""], ["Gimenez", "Ulysse", ""]], "extracted_entities": [{"text": "self-supervised learning", "label": "Few-shot Learning"}, {"text": "BioSerenity-E1", "label": "Foundation Model"}, {"text": "spectral tokenization", "label": "Few-shot Learning"}, {"text": "BioSerenity-E1", "label": "Foundation Model"}, {"text": "BioSerenity-E1", "label": "Foundation Model"}]}
{"id": "2503.10365", "submitter": "Elad Richardson", "authors": "Elad Richardson, Kfir Goldberg, Yuval Alaluf, Daniel Cohen-Or", "title": "Piece it Together: Part-Based Concepting with IP-Priors", "comments": "Project page available at https://eladrich.github.io/PiT/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced generative models excel at synthesizing images but often rely on\ntext-based conditioning. Visual designers, however, often work beyond language,\ndirectly drawing inspiration from existing visual elements. In many cases,\nthese elements represent only fragments of a potential concept-such as an\nuniquely structured wing, or a specific hairstyle-serving as inspiration for\nthe artist to explore how they can come together creatively into a coherent\nwhole. Recognizing this need, we introduce a generative framework that\nseamlessly integrates a partial set of user-provided visual components into a\ncoherent composition while simultaneously sampling the missing parts needed to\ngenerate a plausible and complete concept. Our approach builds on a strong and\nunderexplored representation space, extracted from IP-Adapter+, on which we\ntrain IP-Prior, a lightweight flow-matching model that synthesizes coherent\ncompositions based on domain-specific priors, enabling diverse and\ncontext-aware generations. Additionally, we present a LoRA-based fine-tuning\nstrategy that significantly improves prompt adherence in IP-Adapter+ for a\ngiven task, addressing its common trade-off between reconstruction quality and\nprompt adherence.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 13:46:10 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Richardson", "Elad", ""], ["Goldberg", "Kfir", ""], ["Alaluf", "Yuval", ""], ["Cohen-Or", "Daniel", ""]], "extracted_entities": [{"text": "prompt adherence", "label": "Prompting"}, {"text": "prompt adherence", "label": "Prompting"}]}
{"id": "2503.10367", "submitter": "Yijiang Fan", "authors": "Yijiang Fan, Yuren Mao, Longbin Lai, Ying Zhang, Zhengping Qian,\n  Yunjun Gao", "title": "G-Boost: Boosting Private SLMs with General LLMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the limited computational resources, most Large Language Models (LLMs)\ndevelopers can only fine-tune Small Language Models (SLMs) on their own data.\nThese private SLMs typically have limited effectiveness. To boost the\nperformance of private SLMs, this paper proposes to ask general LLMs for help.\nThe general LLMs can be APIs or larger LLMs whose inference cost the developers\ncan afford. Specifically, we propose the G-Boost framework where a private SLM\nadaptively performs collaborative inference with a general LLM under the guide\nof process reward. Experiments demonstrate that our framework can significantly\nboost the performance of private SLMs.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 13:47:03 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Fan", "Yijiang", ""], ["Mao", "Yuren", ""], ["Lai", "Longbin", ""], ["Zhang", "Ying", ""], ["Qian", "Zhengping", ""], ["Gao", "Yunjun", ""]], "extracted_entities": [{"text": "Large Language Models", "label": "Large Language Model"}, {"text": "APIs", "label": "Open-source LLMs"}]}
{"id": "2503.10371", "submitter": "Min Hun Lee", "authors": "Heng Yim Nicole Oo, Min Hun Lee, Jeong Hoon Lim", "title": "A Multimodal Fusion Model Leveraging MLP Mixer and Handcrafted\n  Features-based Deep Learning Networks for Facial Palsy Detection", "comments": "PAKDD 2025. arXiv admin note: text overlap with arXiv:2405.16496", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Algorithmic detection of facial palsy offers the potential to improve current\npractices, which usually involve labor-intensive and subjective assessments by\nclinicians. In this paper, we present a multimodal fusion-based deep learning\nmodel that utilizes an MLP mixer-based model to process unstructured data (i.e.\nRGB images or images with facial line segments) and a feed-forward neural\nnetwork to process structured data (i.e. facial landmark coordinates, features\nof facial expressions, or handcrafted features) for detecting facial palsy. We\nthen contribute to a study to analyze the effect of different data modalities\nand the benefits of a multimodal fusion-based approach using videos of 20\nfacial palsy patients and 20 healthy subjects. Our multimodal fusion model\nachieved 96.00 F1, which is significantly higher than the feed-forward neural\nnetwork trained on handcrafted features alone (82.80 F1) and an MLP mixer-based\nmodel trained on raw RGB images (89.00 F1).\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 13:48:35 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Oo", "Heng Yim Nicole", ""], ["Lee", "Min Hun", ""], ["Lim", "Jeong Hoon", ""]], "extracted_entities": [{"text": "feed-forward neural\nnetwork", "label": "Neural Language Model"}, {"text": "feed-forward neural\nnetwork", "label": "Neural Language Model"}]}
{"id": "2503.10377", "submitter": "Qiaoling Chen", "authors": "Qiaoling Chen, Shenggui Li, Wei Gao, Peng Sun, Yonggang Wen, Tianwei\n  Zhang", "title": "SPPO:Efficient Long-sequence LLM Training via Adaptive Sequence Pipeline\n  Parallel Offloading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Large Language Models (LLMs) have exhibited remarkable\ncapabilities, driving advancements in real-world applications. However,\ntraining LLMs on increasingly long input sequences imposes significant\nchallenges due to high GPU memory and computational demands. Existing solutions\nface two key limitations: (1) memory reduction techniques, such as activation\nrecomputation and CPU offloading, compromise training efficiency; (2)\ndistributed parallelism strategies require excessive GPU resources, limiting\nthe scalability of input sequence length.\n  To address these gaps, we propose Adaptive Sequence Pipeline Parallel\nOffloading (SPPO), a novel LLM training framework that optimizes memory and\ncomputational resource efficiency for long-sequence training. SPPO introduces\nadaptive offloading, leveraging sequence-aware offloading, and two-level\nactivation management to reduce GPU memory consumption without degrading the\ntraining efficiency. Additionally, SPPO develops an adaptive pipeline\nscheduling approach with a heuristic solver and multiplexed sequence\npartitioning to improve computational resource efficiency. Experimental results\ndemonstrate that SPPO achieves up to 3.38x throughput improvement over\nMegatron-LM and DeepSpeed, realizing efficient training of a 7B LLM with\nsequence lengths of up to 4M tokens on only 128 A100 GPUs.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 13:55:22 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chen", "Qiaoling", ""], ["Li", "Shenggui", ""], ["Gao", "Wei", ""], ["Sun", "Peng", ""], ["Wen", "Yonggang", ""], ["Zhang", "Tianwei", ""]], "extracted_entities": [{"text": "Large Language Models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}]}
{"id": "2503.10389", "submitter": "Jad Wehbeh", "authors": "J. Wehbeh and E. C. Kerrigan", "title": "State-Dependent Uncertainty Modeling in Robust Optimal Control Problems\n  through Generalized Semi-Infinite Programming", "comments": "Submitted to the 2025 Mediteranean Control Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Generalized semi-infinite programs (generalized SIPs) are problems featuring\na finite number of decision variables but an infinite number of constraints.\nThey differ from standard SIPs in that their constraint set itself depends on\nthe choice of the decision variable. Generalized SIPs can be used to model\nrobust optimal control problems where the uncertainty itself is a function of\nthe state or control input, allowing for a less conservative alternative to\nassuming a uniform uncertainty set over the entire decision space. In this\nwork, we demonstrate how any generalized SIP can be converted to an\nexistence-constrained SIP through a reformulation of the constraints and solved\nusing a local reduction approach, which approximates the infinite constraint\nset by a finite number of scenarios. This transformation is then exploited to\nsolve nonlinear robust optimal control problems with state-dependent\nuncertainties. We showcase our proposed approach on a planar quadrotor\nsimulation where it recovers the true generalized SIP solution and outperforms\na SIP-based approach with uniform uncertainty bounds.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 14:07:12 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wehbeh", "J.", ""], ["Kerrigan", "E. C.", ""]], "extracted_entities": [{"text": "Generalized semi-infinite programs", "label": "LLMs"}, {"text": "generalized SIPs", "label": "LLMs"}, {"text": "Generalized SIPs", "label": "LLMs"}]}
{"id": "2503.10391", "submitter": "Yufan Deng", "authors": "Yufan Deng, Xun Guo, Yizhi Wang, Jacob Zhiyuan Fang, Angtian Wang,\n  Shenghai Yuan, Yiding Yang, Bo Liu, Haibin Huang, Chongyang Ma", "title": "CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video generation has witnessed remarkable progress with the advent of deep\ngenerative models, particularly diffusion models. While existing methods excel\nin generating high-quality videos from text prompts or single images,\npersonalized multi-subject video generation remains a largely unexplored\nchallenge. This task involves synthesizing videos that incorporate multiple\ndistinct subjects, each defined by separate reference images, while ensuring\ntemporal and spatial consistency. Current approaches primarily rely on mapping\nsubject images to keywords in text prompts, which introduces ambiguity and\nlimits their ability to model subject relationships effectively. In this paper,\nwe propose CINEMA, a novel framework for coherent multi-subject video\ngeneration by leveraging Multimodal Large Language Model (MLLM). Our approach\neliminates the need for explicit correspondences between subject images and\ntext entities, mitigating ambiguity and reducing annotation effort. By\nleveraging MLLM to interpret subject relationships, our method facilitates\nscalability, enabling the use of large and diverse datasets for training.\nFurthermore, our framework can be conditioned on varying numbers of subjects,\noffering greater flexibility in personalized content creation. Through\nextensive evaluations, we demonstrate that our approach significantly improves\nsubject consistency, and overall video coherence, paving the way for advanced\napplications in storytelling, interactive media, and personalized video\ngeneration.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 14:07:58 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Deng", "Yufan", ""], ["Guo", "Xun", ""], ["Wang", "Yizhi", ""], ["Fang", "Jacob Zhiyuan", ""], ["Wang", "Angtian", ""], ["Yuan", "Shenghai", ""], ["Yang", "Yiding", ""], ["Liu", "Bo", ""], ["Huang", "Haibin", ""], ["Ma", "Chongyang", ""]], "extracted_entities": [{"text": "text prompts", "label": "Prompting"}, {"text": "text prompts", "label": "Prompting"}, {"text": "Multimodal Large Language Model", "label": "Large Language Model"}, {"text": "MLLM", "label": "Large Language Model"}]}
{"id": "2503.10392", "submitter": "Fengxiang Wang", "authors": "Fengxiang Wang, Hongzhen Wang, Yulin Wang, Di Wang, Mingshuo Chen,\n  Haiyan Zhao, Yangang Sun, Shuo Wang, Long Lan, Wenjing Yang, Jing Zhang", "title": "RoMA: Scaling up Mamba-based Foundation Models for Remote Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in self-supervised learning for Vision Transformers (ViTs)\nhave fueled breakthroughs in remote sensing (RS) foundation models. However,\nthe quadratic complexity of self-attention poses a significant barrier to\nscalability, particularly for large models and high-resolution images. While\nthe linear-complexity Mamba architecture offers a promising alternative,\nexisting RS applications of Mamba remain limited to supervised tasks on small,\ndomain-specific datasets. To address these challenges, we propose RoMA, a\nframework that enables scalable self-supervised pretraining of Mamba-based RS\nfoundation models using large-scale, diverse, unlabeled data. RoMA enhances\nscalability for high-resolution images through a tailored auto-regressive\nlearning strategy, incorporating two key innovations: 1) a rotation-aware\npretraining mechanism combining adaptive cropping with angular embeddings to\nhandle sparsely distributed objects with arbitrary orientations, and 2)\nmulti-scale token prediction objectives that address the extreme variations in\nobject scales inherent to RS imagery. Systematic empirical studies validate\nthat Mamba adheres to RS data and parameter scaling laws, with performance\nscaling reliably as model and data size increase. Furthermore, experiments\nacross scene classification, object detection, and semantic segmentation tasks\ndemonstrate that RoMA-pretrained Mamba models consistently outperform ViT-based\ncounterparts in both accuracy and computational efficiency. The source code and\npretrained models will be released at https://github.com/MiliLab/RoMA.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 14:09:18 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Fengxiang", ""], ["Wang", "Hongzhen", ""], ["Wang", "Yulin", ""], ["Wang", "Di", ""], ["Chen", "Mingshuo", ""], ["Zhao", "Haiyan", ""], ["Sun", "Yangang", ""], ["Wang", "Shuo", ""], ["Lan", "Long", ""], ["Yang", "Wenjing", ""], ["Zhang", "Jing", ""]], "extracted_entities": [{"text": "Vision Transformers", "label": "Transformers"}, {"text": "Mamba", "label": "Foundation Model"}, {"text": "Mamba", "label": "Foundation Model"}, {"text": "RoMA", "label": "RoBERTa"}, {"text": "RoMA", "label": "RoBERTa"}, {"text": "angular embeddings", "label": "Embedding"}, {"text": "Mamba", "label": "Foundation Model"}, {"text": "parameter scaling laws", "label": "Scaling law"}, {"text": "Mamba", "label": "Foundation Model"}, {"text": "RoMA", "label": "RoBERTa"}]}
{"id": "2503.10396", "submitter": "Brian Pogue", "authors": "Brian W. Pogue and Alexander P. Niver", "title": "Analysis of the Institutional Free Market in Accredited Medical Physics\n  Graduate Programs", "comments": "10 pages, 5 figures, original scientific research paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph physics.app-ph physics.ed-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical Physics education is delivered through accredited programs with\nadmissions and funding for students determined by individual institutions\nproviding the educational experiences. Public data from accredited graduate\nprograms, along with funding data, were used to analyze institutional trends in\nthis educational market. Temporal trends from 2017 to 2023 show robust growth\nin MS graduates, increasing at an average of 17.7 per year, as compared to\nsteady but modest growth in PhDs, increasing by 3.6 per year. The current\nstatus is there are nearly two MS graduates for every PhD graduate. Trends in\nfunding show self-funding of students is a dominant pathway in domestic\nprograms. Those programs dominated by accredited MS education have their\nlargest fraction of faculty in radiation oncology departments, whereas those\ndominated by PhD education have their largest fraction of faculty in radiology\ndepartments. Overall NIH funding in the space of radiation diagnostics and\ntherapeutics has been largely static over this timeframe, but with a notable 5\nyear rise in NCI funding. This can be contrasted to a substantial 5X-6X rise in\nNIH funding for engineering research in this same period, with significant\nincreases in trainee funding there. Taken as a whole, this survey shows that\ngrowth in the field of medical physics education is dominated by MS graduates,\npresumably servicing the expanded growth needs for well-trained clinical\nphysicists. However, the research infrastructure that supports PhD training in\nmedical physics seems likely to be growing modestly and missing the growth\ntrend of NIH funding that appears to show substantially more growth in\nnon-accredited programs such as biomedical engineering. This data is useful to\ninforming accreditation guidance on numbers of graduates to match the workforce\nneeds or for inter-institutional planning around education goals.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 14:15:51 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Pogue", "Brian W.", ""], ["Niver", "Alexander P.", ""]], "extracted_entities": [{"text": "MS graduates", "label": "LLMs"}]}
{"id": "2503.10399", "submitter": "Andrey Savchenko", "authors": "Andrey V. Savchenko", "title": "HSEmotion Team at ABAW-8 Competition: Audiovisual Ambivalence/Hesitancy,\n  Emotional Mimicry Intensity and Facial Expression Recognition", "comments": "submitted to ABAW CVPR 2025 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents our results for the eighth Affective Behavior Analysis\nin-the-Wild (ABAW) competition. We combine facial emotional descriptors\nextracted by pre-trained models, namely, our EmotiEffLib library, with acoustic\nfeatures and embeddings of texts recognized from speech. The frame-level\nfeatures are aggregated and fed into simple classifiers, e.g., multi-layered\nperceptron (feed-forward neural network with one hidden layer), to predict\nambivalence/hesitancy and facial expressions. In the latter case, we also use\nthe pre-trained facial expression recognition model to select high-score video\nframes and prevent their processing with a domain-specific video classifier.\nThe video-level prediction of emotional mimicry intensity is implemented by\nsimply aggregating frame-level features and training a multi-layered\nperceptron. Experimental results for three tasks from the ABAW challenge\ndemonstrate that our approach significantly increases validation metrics\ncompared to existing baselines.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 14:21:46 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Savchenko", "Andrey V.", ""]], "extracted_entities": [{"text": "embeddings", "label": "Embedding"}]}
{"id": "2503.10400", "submitter": "Akira Sone", "authors": "Akira Sone and Akram Touil and Kenji Maeda and Paola Cappellaro and\n  Sebastian Deffner", "title": "No-go theorem for environment-assisted invariance in non-unitary\n  dynamics", "comments": "6+4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": "LA-UR-24-30981", "categories": "quant-ph cond-mat.stat-mech hep-th math-ph math.MP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We elucidate the requirements for quantum operations that achieve\nenvironment-assisted invariance (envariance), a symmetry of entanglement. While\nenvariance has traditionally been studied within the framework of local unitary\noperations, we extend the analysis to consider non-unitary local operations.\nFirst, we investigate the conditions imposed on operators acting on pure\nbipartite entanglement to attain envariance. We show that the local operations\nmust take a direct-sum form in their Kraus operator representations,\nestablishing decoherence-free subspaces. Furthermore, we prove that the unitary\noperation on the system's subspace uniquely determines the corresponding\nunitary operator on the environment's subspace. As an immediate consequence, we\ndemonstrate that environment-assisted shortcuts to adiabaticity cannot be\nachieved through non-unitary operations. In addition, we identify the\nrequirements that local operations must satisfy to ensure that the eternal\nblack hole states remain static in AdS/CFT.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 14:24:47 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Sone", "Akira", ""], ["Touil", "Akram", ""], ["Maeda", "Kenji", ""], ["Cappellaro", "Paola", ""], ["Deffner", "Sebastian", ""]], "extracted_entities": [{"text": "envariance", "label": "quantisation"}, {"text": "envariance", "label": "quantisation"}]}
{"id": "2503.10406", "submitter": "Yijing Lin", "authors": "Yijing Lin, Mengqi Huang, Shuhan Zhuang, Zhendong Mao", "title": "RealGeneral: Unifying Visual Generation via Temporal In-Context Learning\n  with Video Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unifying diverse image generation tasks within a single framework remains a\nfundamental challenge in visual generation. While large language models (LLMs)\nachieve unification through task-agnostic data and generation, existing visual\ngeneration models fail to meet these principles. Current approaches either rely\non per-task datasets and large-scale training or adapt pre-trained image models\nwith task-specific modifications, limiting their generalizability. In this\nwork, we explore video models as a foundation for unified image generation,\nleveraging their inherent ability to model temporal correlations. We introduce\nRealGeneral, a novel framework that reformulates image generation as a\nconditional frame prediction task, analogous to in-context learning in LLMs. To\nbridge the gap between video models and condition-image pairs, we propose (1) a\nUnified Conditional Embedding module for multi-modal alignment and (2) a\nUnified Stream DiT Block with decoupled adaptive LayerNorm and attention mask\nto mitigate cross-modal interference. RealGeneral demonstrates effectiveness in\nmultiple important visual generation tasks, e.g., it achieves a 14.5%\nimprovement in subject similarity for customized generation and a 10%\nenhancement in image quality for canny-to-image task. Project page:\nhttps://lyne1.github.io/RealGeneral/\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 14:31:52 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Lin", "Yijing", ""], ["Huang", "Mengqi", ""], ["Zhuang", "Shuhan", ""], ["Mao", "Zhendong", ""]], "extracted_entities": [{"text": "large language models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "video models", "label": "Large Language Model"}, {"text": "RealGeneral", "label": "Open-source LLMs"}, {"text": "in-context learning", "label": "Few-shot Learning"}, {"text": "video models", "label": "Large Language Model"}, {"text": "Unified Conditional Embedding", "label": "Embedding"}, {"text": "attention mask", "label": "Zero-shot Learning"}, {"text": "RealGeneral", "label": "Foundation Model"}]}
{"id": "2503.10408", "submitter": "Jonathan Shaki", "authors": "Jonathan Shaki, Emanuele La Malfa, Michael Wooldridge, Sarit Kraus", "title": "Understanding the Logical Capabilities of Large Language Models via\n  Out-of-Context Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the capabilities of Large Language Models (LLM) on binary relations,\na ubiquitous concept in math employed in most reasoning, math and logic\nbenchmarks. This work focuses on equality, inequality, and inclusion, along\nwith the properties they satisfy, such as ir/reflexivity, a/symmetry,\ntransitivity, and logical complexity (e.g., number of reasoning ``hops''). We\npropose an alternative to in-context learning that trains only the\nrepresentations of newly introduced tokens, namely out-of-context\nrepresentation learning. This method mitigates linguistic biases already\npresent in a model and, differently from in-context learning, does not rely on\nexternal information or illustrations. We argue out-of-context representation\nlearning as a better alternative to in-context learning and fine-tuning to\nevaluate the capabilities of LLMs on logic tasks that are the building blocks\nof more complex reasoning benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 14:32:30 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Shaki", "Jonathan", ""], ["La Malfa", "Emanuele", ""], ["Wooldridge", "Michael", ""], ["Kraus", "Sarit", ""]], "extracted_entities": [{"text": "in-context learning", "label": "Few-shot Learning"}, {"text": "out-of-context\nrepresentation learning", "label": "Few-shot Learning"}, {"text": "in-context learning", "label": "Few-shot Learning"}, {"text": "out-of-context representation\nlearning", "label": "Few-shot Learning"}, {"text": "in-context learning", "label": "Few-shot Learning"}, {"text": "fine-tuning", "label": "Fine-tuning"}]}
{"id": "2503.10419", "submitter": "Hendrik Scheidel", "authors": "Hendrik Scheidel, Camilo Gonzalez, Houshyar Asadi, Tobias Bellmann,\n  Andreas Seefried, Shady Mohamed, Saeid Nahavandi", "title": "A nonlinear real time capable motion cueing algorithm based on deep\n  reinforcement learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In motion simulation, motion cueing algorithms are used for the trajectory\nplanning of the motion simulator platform, where workspace limitations prevent\ndirect reproduction of reference trajectories. Strategies such as motion\nwashout, which return the platform to its center, are crucial in these\nsettings. For serial robotic MSPs with highly nonlinear workspaces, it is\nessential to maximize the efficient utilization of the MSPs kinematic and\ndynamic capabilities. Traditional approaches, including classical washout\nfiltering and linear model predictive control, fail to consider\nplatform-specific, nonlinear properties, while nonlinear model predictive\ncontrol, though comprehensive, imposes high computational demands that hinder\nreal-time, pilot-in-the-loop application without further simplification. To\novercome these limitations, we introduce a novel approach using deep\nreinforcement learning for motion cueing, demonstrated here for the first time\nin a 6-degree-of-freedom setting with full consideration of the MSPs kinematic\nnonlinearities. Previous work by the authors successfully demonstrated the\napplication of DRL to a simplified 2-DOF setup, which did not consider\nkinematic or dynamic constraints. This approach has been extended to all 6 DOF\nby incorporating a complete kinematic model of the MSP into the algorithm, a\ncrucial step for enabling its application on a real motion simulator. The\ntraining of the DRL-MCA is based on Proximal Policy Optimization in an\nactor-critic implementation combined with an automated hyperparameter\noptimization. After detailing the necessary training framework and the\nalgorithm itself, we provide a comprehensive validation, demonstrating that the\nDRL MCA achieves competitive performance against established algorithms.\nMoreover, it generates feasible trajectories by respecting all system\nconstraints and meets all real-time requirements with low...\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 14:39:19 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Scheidel", "Hendrik", ""], ["Gonzalez", "Camilo", ""], ["Asadi", "Houshyar", ""], ["Bellmann", "Tobias", ""], ["Seefried", "Andreas", ""], ["Mohamed", "Shady", ""], ["Nahavandi", "Saeid", ""]], "extracted_entities": [{"text": "deep\nreinforcement learning", "label": "Few-shot Learning"}]}
{"id": "2503.10421", "submitter": "Zhenwei Wang", "authors": "Zhenwei Wang, Ruibin Bai, Tiehua Zhang", "title": "Towards Constraint-Based Adaptive Hypergraph Learning for Solving\n  Vehicle Routing: An End-to-End Solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The application of learning based methods to vehicle routing problems has\nemerged as a pivotal area of research in combinatorial optimization. These\nproblems are characterized by vast solution spaces and intricate constraints,\nmaking traditional approaches such as exact mathematical models or heuristic\nmethods prone to high computational overhead or reliant on the design of\ncomplex heuristic operators to achieve optimal or near optimal solutions.\nMeanwhile, although some recent learning-based methods can produce good\nperformance for VRP with straightforward constraint scenarios, they often fail\nto effectively handle hard constraints that are common in practice. This study\nintroduces a novel end-to-end framework that combines constraint-oriented\nhypergraphs with reinforcement learning to address vehicle routing problems. A\ncentral innovation of this work is the development of a constraint-oriented\ndynamic hyperedge reconstruction strategy within an encoder, which\nsignificantly enhances hypergraph representation learning. Additionally, the\ndecoder leverages a double-pointer attention mechanism to iteratively generate\nsolutions. The proposed model is trained by incorporating asynchronous\nparameter updates informed by hypergraph constraints and optimizing a dual loss\nfunction comprising constraint loss and policy gradient loss. The experiment\nresults on benchmark datasets demonstrate that the proposed approach not only\neliminates the need for sophisticated heuristic operators but also achieves\nsubstantial improvements in solution quality.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 14:42:44 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Zhenwei", ""], ["Bai", "Ruibin", ""], ["Zhang", "Tiehua", ""]], "extracted_entities": [{"text": "reinforcement learning", "label": "Few-shot Learning"}, {"text": "hypergraph representation learning", "label": "Few-shot Learning"}, {"text": "double-pointer attention mechanism", "label": "Attention mechanism"}]}
{"id": "2503.10431", "submitter": "Artem Chernyshov", "authors": "Artem Chernyshov, John Nyberg, Vegard Holmstr{\\o}m, Md Abulkalam Azad,\n  Bj{\\o}rnar Grenne, H{\\aa}vard Dalen, Svein Arne Aase, Lasse Lovstakken,\n  Andreas {\\O}stvik", "title": "Low Complexity Point Tracking of the Myocardium in 2D Echocardiography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning methods for point tracking are applicable in 2D\nechocardiography, but do not yet take advantage of domain specifics that enable\nextremely fast and efficient configurations. We developed MyoTracker, a\nlow-complexity architecture (0.3M parameters) for point tracking in\nechocardiography. It builds on the CoTracker2 architecture by simplifying its\ncomponents and extending the temporal context to provide point predictions for\nthe entire sequence in a single step. We applied MyoTracker to the right\nventricular (RV) myocardium in RV-focused recordings and compared the results\nwith those of CoTracker2 and EchoTracker, another specialized point tracking\narchitecture for echocardiography. MyoTracker achieved the lowest average point\ntrajectory error at 2.00 $\\pm$ 0.53 mm. Calculating RV Free Wall Strain (RV\nFWS) using MyoTracker's point predictions resulted in a -0.3$\\%$ bias with\n95$\\%$ limits of agreement from -6.1$\\%$ to 5.4$\\%$ compared to reference\nvalues from commercial software. This range falls within the interobserver\nvariability reported in previous studies. The limits of agreement were wider\nfor both CoTracker2 and EchoTracker, worse than the interobserver variability.\nAt inference, MyoTracker used 67$\\%$ less GPU memory than CoTracker2 and 84$\\%$\nless than EchoTracker on large sequences (100 frames). MyoTracker was 74 times\nfaster during inference than CoTracker2 and 11 times faster than EchoTracker\nwith our setup. Maintaining the entire sequence in the temporal context was the\ngreatest contributor to MyoTracker's accuracy. Slight additional gains can be\nmade by re-enabling iterative refinement, at the cost of longer processing\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 14:53:00 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chernyshov", "Artem", ""], ["Nyberg", "John", ""], ["Holmstr\u00f8m", "Vegard", ""], ["Azad", "Md Abulkalam", ""], ["Grenne", "Bj\u00f8rnar", ""], ["Dalen", "H\u00e5vard", ""], ["Aase", "Svein Arne", ""], ["Lovstakken", "Lasse", ""], ["\u00d8stvik", "Andreas", ""]], "extracted_entities": [{"text": "MyoTracker", "label": "AI model"}, {"text": "CoTracker2", "label": "AI model"}, {"text": "temporal context", "label": "contextual Embedding"}, {"text": "CoTracker2", "label": "AI model"}, {"text": "EchoTracker", "label": "AI model"}, {"text": "MyoTracker", "label": "AI model"}, {"text": "CoTracker2", "label": "AI model"}, {"text": "MyoTracker", "label": "AI model"}, {"text": "CoTracker2", "label": "AI model"}, {"text": "temporal context", "label": "contextual Embedding"}]}
{"id": "2503.10432", "submitter": "Can Zheng", "authors": "Can Zheng, Jiguang He, Guofa Cai, Zitong Yu, Chung G. Kang", "title": "BeamLLM: Vision-Empowered mmWave Beam Prediction with Large Language\n  Models", "comments": "6 pages, 7 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose BeamLLM, a vision-aided millimeter-wave (mmWave)\nbeam prediction framework leveraging large language models (LLMs) to address\nthe challenges of high training overhead and latency in mmWave communication\nsystems. By combining computer vision (CV) with LLMs' cross-modal reasoning\ncapabilities, the framework extracts user equipment (UE) positional features\nfrom RGB images and aligns visual-temporal features with LLMs' semantic space\nthrough reprogramming techniques. Evaluated on a realistic\nvehicle-to-infrastructure (V2I) scenario, the proposed method achieves 61.01%\ntop-1 accuracy and 97.39% top-3 accuracy in standard prediction tasks,\nsignificantly outperforming traditional deep learning models. In few-shot\nprediction scenarios, the performance degradation is limited to 12.56% (top-1)\nand 5.55% (top-3) from time sample 1 to 10, demonstrating superior prediction\ncapability.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 14:55:59 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zheng", "Can", ""], ["He", "Jiguang", ""], ["Cai", "Guofa", ""], ["Yu", "Zitong", ""], ["Kang", "Chung G.", ""]], "extracted_entities": [{"text": "BeamLLM", "label": "LLM"}, {"text": "large language models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "few-shot\nprediction scenarios", "label": "Few-shot Learning"}]}
{"id": "2503.10434", "submitter": "Derun Li", "authors": "Derun Li, Jianwei Ren, Yue Wang, Xin Wen, Pengxiang Li, Leimeng Xu,\n  Kun Zhan, Zhongpu Xia, Peng Jia, Xianpeng Lang, Ningyi Xu, Hang Zhao", "title": "Finetuning Generative Trajectory Model with Reinforcement Learning from\n  Human Feedback", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generating human-like and adaptive trajectories is essential for autonomous\ndriving in dynamic environments. While generative models have shown promise in\nsynthesizing feasible trajectories, they often fail to capture the nuanced\nvariability of human driving styles due to dataset biases and distributional\nshifts. To address this, we introduce TrajHF, a human feedback-driven\nfinetuning framework for generative trajectory models, designed to align motion\nplanning with diverse driving preferences. TrajHF incorporates\nmulti-conditional denoiser and reinforcement learning with human feedback to\nrefine multi-modal trajectory generation beyond conventional imitation\nlearning. This enables better alignment with human driving preferences while\nmaintaining safety and feasibility constraints. TrajHF achieves PDMS of 93.95\non NavSim benchmark, significantly exceeding other methods. TrajHF sets a new\nparadigm for personalized and adaptable trajectory generation in autonomous\ndriving.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 14:56:17 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Derun", ""], ["Ren", "Jianwei", ""], ["Wang", "Yue", ""], ["Wen", "Xin", ""], ["Li", "Pengxiang", ""], ["Xu", "Leimeng", ""], ["Zhan", "Kun", ""], ["Xia", "Zhongpu", ""], ["Jia", "Peng", ""], ["Lang", "Xianpeng", ""], ["Xu", "Ningyi", ""], ["Zhao", "Hang", ""]], "extracted_entities": [{"text": "reinforcement learning", "label": "Few-shot Learning"}, {"text": "conventional imitation\nlearning", "label": "Few-shot Learning"}]}
{"id": "2503.10437", "submitter": "Wanhua Li", "authors": "Wanhua Li, Renping Zhou, Jiawei Zhou, Yingwei Song, Johannes Herter,\n  Minghan Qin, Gao Huang, Hanspeter Pfister", "title": "4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large\n  Language Models", "comments": "CVPR 2025. Project Page: https://4d-langsplat.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning 4D language fields to enable time-sensitive, open-ended language\nqueries in dynamic scenes is essential for many real-world applications. While\nLangSplat successfully grounds CLIP features into 3D Gaussian representations,\nachieving precision and efficiency in 3D static scenes, it lacks the ability to\nhandle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot\ncapture temporal dynamics in videos. Real-world environments are inherently\ndynamic, with object semantics evolving over time. Building a precise 4D\nlanguage field necessitates obtaining pixel-aligned, object-wise video\nfeatures, which current vision models struggle to achieve. To address these\nchallenges, we propose 4D LangSplat, which learns 4D language fields to handle\ntime-agnostic or time-sensitive open-vocabulary queries in dynamic scenes\nefficiently. 4D LangSplat bypasses learning the language field from vision\nfeatures and instead learns directly from text generated from object-wise video\ncaptions via Multimodal Large Language Models (MLLMs). Specifically, we propose\na multimodal object-wise video prompting method, consisting of visual and text\nprompts that guide MLLMs to generate detailed, temporally consistent,\nhigh-quality captions for objects throughout a video. These captions are\nencoded using a Large Language Model into high-quality sentence embeddings,\nwhich then serve as pixel-aligned, object-specific feature supervision,\nfacilitating open-vocabulary text queries through shared embedding spaces.\nRecognizing that objects in 4D scenes exhibit smooth transitions across states,\nwe further propose a status deformable network to model these continuous\nchanges over time effectively. Our results across multiple benchmarks\ndemonstrate that 4D LangSplat attains precise and efficient results for both\ntime-sensitive and time-agnostic open-vocabulary queries.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 14:58:22 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Wanhua", ""], ["Zhou", "Renping", ""], ["Zhou", "Jiawei", ""], ["Song", "Yingwei", ""], ["Herter", "Johannes", ""], ["Qin", "Minghan", ""], ["Huang", "Gao", ""], ["Pfister", "Hanspeter", ""]], "extracted_entities": [{"text": "3D Gaussian representations", "label": "Embedding"}, {"text": "visual and text\nprompts", "label": "Prompting"}, {"text": "Large Language Model", "label": "Large Language Model"}, {"text": "high-quality sentence embeddings", "label": "Embedding"}, {"text": "shared embedding spaces", "label": "contextual Embedding"}]}
{"id": "2503.10439", "submitter": "Simone Magistri Mr", "authors": "Simone Magistri, Tomaso Trinci, Albin Soutif-Cormerais, Joost van de\n  Weijer, Andrew D. Bagdanov", "title": "EFC++: Elastic Feature Consolidation with Prototype Re-balancing for\n  Cold Start Exemplar-free Incremental Learning", "comments": "Submitted on July 2024. Under Review. arXiv admin note: text overlap\n  with arXiv:2402.03917", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a\nsequence of tasks without having access to previous task data. In this paper,\nwe consider the challenging Cold Start scenario in which insufficient data is\navailable in the first task to learn a high-quality backbone. This is\nespecially challenging for EFCIL since it requires high plasticity, resulting\nin feature drift which is difficult to compensate for in the exemplar-free\nsetting. To address this problem, we propose an effective approach to\nconsolidate feature representations by regularizing drift in directions highly\nrelevant to previous tasks and employs prototypes to reduce task-recency bias.\nOur approach, which we call Elastic Feature Consolidation++ (EFC++) exploits a\ntractable second-order approximation of feature drift based on a proposed\nEmpirical Feature Matrix (EFM). The EFM induces a pseudo-metric in feature\nspace which we use to regularize feature drift in important directions and to\nupdate Gaussian prototypes. In addition, we introduce a post-training prototype\nre-balancing phase that updates classifiers to compensate for feature drift.\nExperimental results on CIFAR-100, Tiny-ImageNet, ImageNet-Subset, ImageNet-1K\nand DomainNet demonstrate that EFC++ is better able to learn new tasks by\nmaintaining model plasticity and significantly outperform the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:01:19 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Magistri", "Simone", ""], ["Trinci", "Tomaso", ""], ["Soutif-Cormerais", "Albin", ""], ["van de Weijer", "Joost", ""], ["Bagdanov", "Andrew D.", ""]], "extracted_entities": [{"text": "Exemplar-Free Class Incremental Learning", "label": "Few-shot Learning"}]}
{"id": "2503.10440", "submitter": "Sarah M\\\"uller", "authors": "Gustav Schmidt, Holger Heidrich, Philipp Berens, Sarah M\\\"uller", "title": "Learning Disease State from Noisy Ordinal Disease Progression Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning from noisy ordinal labels is a key challenge in medical imaging. In\nthis work, we ask whether ordinal disease progression labels (better, worse, or\nstable) can be used to learn a representation allowing to classify disease\nstate. For neovascular age-related macular degeneration (nAMD), we cast the\nproblem of modeling disease progression between medical visits as a\nclassification task with ordinal ranks. To enhance generalization, we tailor\nour model to the problem setting by (1) independent image encoding, (2)\nantisymmetric logit space equivariance, and (3) ordinal scale awareness. In\naddition, we address label noise by learning an uncertainty estimate for loss\nre-weighting. Our approach learns an interpretable disease representation\nenabling strong few-shot performance for the related task of nAMD activity\nclassification from single images, despite being trained only on image pairs\nwith ordinal disease progression labels.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:04:27 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Schmidt", "Gustav", ""], ["Heidrich", "Holger", ""], ["Berens", "Philipp", ""], ["M\u00fcller", "Sarah", ""]], "extracted_entities": [{"text": "independent image encoding", "label": "Embedding"}]}
{"id": "2503.10442", "submitter": "Adnan Tahirovic", "authors": "Adnan Tahirovic and Azra Redzovic", "title": "Optimal Estimation for Continuous-Time Nonlinear Systems Using\n  State-Dependent Riccati Equation (SDRE)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a unified approach for state estimation and control of\nnonlinear dynamic systems, employing the State-Dependent Riccati Equation\n(SDRE) framework. The proposed approach naturally extends classical linear\nquadratic Gaussian (LQG) methods into nonlinear scenarios, avoiding\nlinearization by using state-dependent coefficient (SDC) matrices. An\nSDRE-based Kalman filter (SDRE-KF) is integrated within an SDRE-based control\nstructure, providing a coherent and intuitive strategy for nonlinear system\nanalysis and control design. To evaluate the effectiveness and robustness of\nthe proposed methodology, comparative simulations are conducted on two\nbenchmark nonlinear systems: a simple pendulum and a Van der Pol oscillator.\nResults demonstrate that the SDRE-KF achieves comparable or superior estimation\naccuracy compared to traditional methods, including the Extended Kalman Filter\n(EKF) and Particle Filter (PF). These findings underline the potential of the\nunified SDRE-based approach as a viable alternative for nonlinear state\nestimation and control, providing valuable insights for both educational\npurposes and practical engineering applications.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:04:59 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Tahirovic", "Adnan", ""], ["Redzovic", "Azra", ""]], "extracted_entities": [{"text": "linearization", "label": "quantisation"}]}
{"id": "2503.10444", "submitter": "Aliaksei Kachanovich", "authors": "Aliaksei Kachanovich", "title": "Background processes in Higgs decay to Z gamma", "comments": "7 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ph hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ATLAS and CMS Collaborations reported that the observed number of Higgs\nboson decays into a $Z$ boson and a photon is $\\mu = 2.2 \\pm 0.7$ times higher\nthan predicted by the Standard Model. Initially, this discrepancy was\nattributed to a modification of the $HZ\\gamma$ vertex. In the $H \\to Z\\gamma$\nprocess, this decay is reconstructed from $H \\to \\ell\\ell\\gamma$, where $\\ell$\nrepresents either an electron or a muon. In this study, an investigation is\nconducted to examine this anomaly by exploring potential additional background\ncontributions to $H \\to \\ell\\ell\\gamma$ from various subprocesses within and\nbeyond the Standard Model.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:10:52 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Kachanovich", "Aliaksei", ""]], "extracted_entities": [{"text": "Standard Model", "label": "Foundation Model"}, {"text": "Standard Model", "label": "Foundation Model"}]}
{"id": "2503.10445", "submitter": "Huiyun Tang", "authors": "Huiyun Tang, Bj\\\"orn Rohles, Yuwei Chuai, Gabriele Lenzini, Anastasia\n  Sergeeva", "title": "More Than Just Warnings:Exploring the Ways of Communicating Credibility\n  Assessment on Social Media", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Reducing the spread of misinformation is challenging. AI-based fact\nverification systems offer a promising solution by addressing the high costs\nand slow pace of traditional fact-checking. However, the problem of how to\neffectively communicate the results to users remains unsolved. Warning labels\nmay seem an easy solution, but they fail to account for fuzzy misinformation\nthat is not entirely fake. Additionally, users' limited attention spans and\nsocial media information should be taken into account while designing the\npresentation. The online experiment (n = 537) investigates the impact of\nsources and granularity on users' perception of information veracity and the\nsystem's usefulness and trustworthiness. Findings show that fine-grained\nindicators enhance nuanced opinions, information awareness, and the intention\nto use fact-checking systems. Source differences had minimal impact on opinions\nand perceptions, except for informativeness. Qualitative findings suggest the\nproposed indicators promote critical thinking. We discuss implications for\ndesigning concise, user-friendly AI fact-checking feedback.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:10:55 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Tang", "Huiyun", ""], ["Rohles", "Bj\u00f6rn", ""], ["Chuai", "Yuwei", ""], ["Lenzini", "Gabriele", ""], ["Sergeeva", "Anastasia", ""]], "extracted_entities": [{"text": "limited attention spans", "label": "Attention mechanism"}]}
{"id": "2503.10446", "submitter": "Jakaria Islam Emon", "authors": "Jakaria Islam Emon, Md Abu Salek and Kazi Tamanna Alam", "title": "Whisper Speaker Identification: Leveraging Pre-Trained Multilingual\n  Transformers for Robust Speaker Embeddings", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Speaker identification in multilingual settings presents unique challenges,\nparticularly when conventional models are predominantly trained on English\ndata. In this paper, we propose WSI (Whisper Speaker Identification), a\nframework that repurposes the encoder of the Whisper automatic speech\nrecognition model pre trained on extensive multilingual data to generate robust\nspeaker embeddings via a joint loss optimization strategy that leverages online\nhard triplet mining and self supervised Normalized Temperature-scaled Cross\nEntropy loss. By capitalizing on Whisper language-agnostic acoustic\nrepresentations, our approach effectively distinguishes speakers across diverse\nlanguages and recording conditions. Extensive evaluations on multiple corpora,\nincluding VoxTube (multilingual), JVS (Japanese), CallHome (German, Spanish,\nChinese, and Japanese), and Voxconverse (English), demonstrate that WSI\nconsistently outperforms state-of-the-art baselines, namely Pyannote Embedding,\nECAPA TDNN, and Xvector, in terms of lower equal error rates and higher AUC\nscores. These results validate our hypothesis that a multilingual pre-trained\nASR encoder, combined with joint loss optimization, substantially improves\nspeaker identification performance in non-English languages.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:11:28 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Emon", "Jakaria Islam", ""], ["Salek", "Md Abu", ""], ["Alam", "Kazi Tamanna", ""]], "extracted_entities": [{"text": "Pyannote Embedding", "label": "Embedding"}]}
{"id": "2503.10451", "submitter": "Martin Roelfs", "authors": "Martin Roelfs", "title": "The Willing Kingdon Clifford Algebra Library", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kingdon is an open-source Python package designed to seamlessly integrate\nGeometric Algebra (GA) into existing workflows. Unlike previous GA libraries,\nkingdon is input-type-agnostic, and hence supports GA's over e.g. PyTorch\ntensors, NumPy arrays, or SymPy symbolic expressions, to name but a few.\nDespite this refusal to specialize, it delivers high performance by\nsymbolically optimizing operators and leveraging input sparsity for\nJust-In-Time compiled expressions. Additionally, its visualization capabilities\nin Jupyter notebooks using ganja align with the rapid prototyping workflow\ncommon to scientific research.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:16:57 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Roelfs", "Martin", ""]], "extracted_entities": [{"text": "Kingdon", "label": "Open-source LLMs"}, {"text": "kingdon", "label": "Open-source LLMs"}]}
{"id": "2503.10452", "submitter": "Wenhao Hu", "authors": "Wenhao Hu, Jinhao Duan, Chunchen Wei, Li Zhang, Yue Zhang, Kaidi Xu", "title": "DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large\n  Language Models in Code Generation", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid advancement of large language models (LLMs) has significantly\nimproved their performance in code generation tasks. However, existing code\nbenchmarks remain static, consisting of fixed datasets with predefined\nproblems. This makes them vulnerable to memorization during training, where\nLLMs recall specific test cases instead of generalizing to new problems,\nleading to data contamination and unreliable evaluation results. To address\nthese issues, we introduce DynaCode, a dynamic, complexity-aware benchmark that\novercomes the limitations of static datasets. DynaCode evaluates LLMs\nsystematically using a complexity-aware metric, incorporating both code\ncomplexity and call-graph structures. DynaCode achieves large-scale diversity,\ngenerating up to 189 million unique nested code problems across four distinct\nlevels of code complexity, referred to as units, and 16 types of call graphs.\nResults on 12 latest LLMs show an average performance drop of 16.8% to 45.7%\ncompared to MBPP+, a static code generation benchmark, with performance\nprogressively decreasing as complexity increases. This demonstrates DynaCode's\nability to effectively differentiate LLMs. Additionally, by leveraging call\ngraphs, we gain insights into LLM behavior, particularly their preference for\nhandling subfunction interactions within nested code.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:18:56 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Hu", "Wenhao", ""], ["Duan", "Jinhao", ""], ["Wei", "Chunchen", ""], ["Zhang", "Li", ""], ["Zhang", "Yue", ""], ["Xu", "Kaidi", ""]], "extracted_entities": [{"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}]}
{"id": "2503.10457", "submitter": "Bousselham El Haddaoui Mr", "authors": "Bousselham El Haddaoui, Raddouane Chiheb, Rdouan Faizi, Abdellatif El\n  Afia", "title": "Sentiment Analysis in SemEval: A Review of Sentiment Identification\n  Approaches", "comments": null, "journal-ref": "International Journal of Electrical and Computer Engineering\n  (IJECE), 13(3), 3322-3338 (2023)", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media platforms are becoming the foundations of social interactions\nincluding messaging and opinion expression. In this regard, Sentiment Analysis\ntechniques focus on providing solutions to ensure the retrieval and analysis of\ngenerated data including sentiments, emotions, and discussed topics.\nInternational competitions such as the International Workshop on Semantic\nEvaluation (SemEval) have attracted many researchers and practitioners with a\nspecial research interest in building sentiment analysis systems. In our work,\nwe study top-ranking systems for each SemEval edition during the 2013-2021\nperiod, a total of 658 teams participated in these editions with increasing\ninterest over years. We analyze the proposed systems marking the evolution of\nresearch trends with a focus on the main components of sentiment analysis\nsystems including data acquisition, preprocessing, and classification. Our\nstudy shows an active use of preprocessing techniques, an evolution of features\nengineering and word representation from lexicon-based approaches to word\nembeddings, and the dominance of neural networks and transformers over the\nclassification phase fostering the use of ready-to-use models. Moreover, we\nprovide researchers with insights based on experimented systems which will\nallow rapid prototyping of new systems and help practitioners build for future\nSemEval editions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:25:23 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Haddaoui", "Bousselham El", ""], ["Chiheb", "Raddouane", ""], ["Faizi", "Rdouan", ""], ["Afia", "Abdellatif El", ""]], "extracted_entities": [{"text": "word\nembeddings", "label": "Embedding"}, {"text": "transformers", "label": "Transformers"}]}
{"id": "2503.10460", "submitter": "Haosheng Zou", "authors": "Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin\n  Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng,\n  Shousheng Jia, Xiangzheng Zhang", "title": "Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and\n  Beyond", "comments": "all release at https://github.com/Qihoo360/Light-R1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents our work on the Light-R1 series, with models, data, and\ncode all released.\n  We first focus on training long COT models from scratch, specifically\nstarting from models initially lacking long COT capabilities. Using a\ncurriculum training recipe consisting of two-stage SFT and semi-on-policy DPO,\nwe train our model Light-R1-32B from Qwen2.5-32B-Instruct, resulting in\nsuperior math performance compared to DeepSeek-R1-Distill-Qwen-32B. Despite\nbeing trained exclusively on math data, Light-R1-32B shows strong\ngeneralization across other domains. In the subsequent phase of this work, we\nhighlight the significant benefit of the 3k dataset constructed for the second\nSFT stage on enhancing other models. By fine-tuning DeepSeek-R1-Distilled\nmodels using this dataset, we obtain new SOTA models in 7B and 14B, while the\n32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.\n  Furthermore, we extend our work by applying reinforcement learning,\nspecifically GRPO, on long-COT models to further improve reasoning performance.\nWe successfully train our final Light-R1-14B-DS with RL, achieving SOTA\nperformance among 14B parameter models in math. With AIME24 & 25 scores of 74.0\nand 60.2 respectively, Light-R1-14B-DS surpasses even many 32B models and\nDeepSeek-R1-Distill-Llama-70B. Its RL training also exhibits well expected\nbehavior, showing simultaneous increase in response length and reward score.\n  The Light-R1 series of work validates training long-COT models from scratch,\nshowcases the art in SFT data and releases SOTA models from RL.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:29:22 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wen", "Liang", ""], ["Cai", "Yunke", ""], ["Xiao", "Fenrui", ""], ["He", "Xin", ""], ["An", "Qi", ""], ["Duan", "Zhenyu", ""], ["Du", "Yimin", ""], ["Liu", "Junchen", ""], ["Tang", "Lifu", ""], ["Lv", "Xiaowei", ""], ["Zou", "Haosheng", ""], ["Deng", "Yongchao", ""], ["Jia", "Shousheng", ""], ["Zhang", "Xiangzheng", ""]], "extracted_entities": [{"text": "two-stage SFT", "label": "BERT"}, {"text": "SFT", "label": "BERT"}, {"text": "reinforcement learning", "label": "Zero-shot Learning"}, {"text": "SFT", "label": "BERT"}]}
{"id": "2503.10462", "submitter": "Vighnesh Dattatraya Naik", "authors": "Ao Chen, Vighnesh Dattatraya Naik, Markus Heyl", "title": "Convolutional transformer wave functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural quantum states have recently achieved remarkable performance in\nsolving challenging quantum many-body problems. While transformer networks\nappear particularly promising due to their success in computer science, we show\nthat previously reported transformer wave functions haven't so far been capable\nto utilize their full power. Here, we introduce the convolutional transformer\nwave function (CTWF). We show that our CTWFs exhibit superior performance in\nground-state search and non-equilibrium dynamics compared to previous results,\ndemonstrating promising capacity in complex quantum problems.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:32:21 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chen", "Ao", ""], ["Naik", "Vighnesh Dattatraya", ""], ["Heyl", "Markus", ""]], "extracted_entities": [{"text": "complex quantum problems", "label": "quantisation"}]}
{"id": "2503.10465", "submitter": "Pablo Sanchez Puertas", "authors": "Enrique Ruiz Arriola, Pablo Sanchez-Puertas, Christian Weiss", "title": "Pion transverse charge density from $e^+$$e^-$ annihilation data and\n  logarithmic dispersion relations", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": "JLAB-THY-25-4251", "categories": "hep-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The transverse charge density of the pion is extracted from a dispersive\nanalysis of the $e^+e^- \\rightarrow \\pi^+\\pi^-$ exclusive annihilation data. A\nlogarithmic dispersion relation is used to compute the unknown phase of the\ntimelike pion form factor from the modulus obtained from the annihilation cross\nsection. The method is model-independent and permits quantitative uncertainty\nestimates. The density is obtained with few-percent accuracy down to $b \\sim\n0.1$ fm; at smaller distances it depends qualitatively on the assumed\nhigh-energy behavior of the timelike form factor. Implications for pion\nstructure and the relevance of pQCD asymptotics are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:38:10 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Arriola", "Enrique Ruiz", ""], ["Sanchez-Puertas", "Pablo", ""], ["Weiss", "Christian", ""]], "extracted_entities": [{"text": "logarithmic dispersion relation", "label": "Scaling law"}]}
{"id": "2503.10468", "submitter": "Yifeng Yang", "authors": "Yifeng Yang, Lin Zhu, Zewen Sun, Hengyu Liu, Qinying Gu, Nanyang Ye", "title": "OODD: Test-time Out-of-Distribution Detection with Dynamic Dictionary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Out-of-distribution (OOD) detection remains challenging for deep learning\nmodels, particularly when test-time OOD samples differ significantly from\ntraining outliers. We propose OODD, a novel test-time OOD detection method that\ndynamically maintains and updates an OOD dictionary without fine-tuning. Our\napproach leverages a priority queue-based dictionary that accumulates\nrepresentative OOD features during testing, combined with an informative inlier\nsampling strategy for in-distribution (ID) samples. To ensure stable\nperformance during early testing, we propose a dual OOD stabilization mechanism\nthat leverages strategically generated outliers derived from ID data. To our\nbest knowledge, extensive experiments on the OpenOOD benchmark demonstrate that\nOODD significantly outperforms existing methods, achieving a 26.0% improvement\nin FPR95 on CIFAR-100 Far OOD detection compared to the state-of-the-art\napproach. Furthermore, we present an optimized variant of the KNN-based OOD\ndetection framework that achieves a 3x speedup while maintaining detection\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:41:56 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Yang", "Yifeng", ""], ["Zhu", "Lin", ""], ["Sun", "Zewen", ""], ["Liu", "Hengyu", ""], ["Gu", "Qinying", ""], ["Ye", "Nanyang", ""]], "extracted_entities": [{"text": "fine-tuning", "label": "Fine-tuning"}, {"text": "priority queue-based dictionary", "label": "Embedding"}]}
{"id": "2503.10471", "submitter": "Liming Wu", "authors": "Liming Wu, Wenbing Huang, Rui Jiao, Jianxing Huang, Liwei Liu, Yipeng\n  Zhou, Hao Sun, Yang Liu, Fuchun Sun, Yuxiang Ren, Jirong Wen", "title": "Siamese Foundation Models for Crystal Structure Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crystal Structure Prediction (CSP), which aims to generate stable crystal\nstructures from compositions, represents a critical pathway for discovering\nnovel materials. While structure prediction tasks in other domains, such as\nproteins, have seen remarkable progress, CSP remains a relatively underexplored\narea due to the more complex geometries inherent in crystal structures. In this\npaper, we propose Siamese foundation models specifically designed to address\nCSP. Our pretrain-finetune framework, named DAO, comprises two complementary\nfoundation models: DAO-G for structure generation and DAO-P for energy\nprediction. Experiments on CSP benchmarks (MP-20 and MPTS-52) demonstrate that\nour DAO-G significantly surpasses state-of-the-art (SOTA) methods across all\nmetrics. Extensive ablation studies further confirm that DAO-G excels in\ngenerating diverse polymorphic structures, and the dataset relaxation and\nenergy guidance provided by DAO-P are essential for enhancing DAO-G's\nperformance. When applied to three real-world superconductors\n($\\text{CsV}_3\\text{Sb}_5$, $ \\text{Zr}_{16}\\text{Rh}_8\\text{O}_4$ and\n$\\text{Zr}_{16}\\text{Pd}_8\\text{O}_4$) that are known to be challenging to\nanalyze, our foundation models achieve accurate critical temperature\npredictions and structure generations. For instance, on\n$\\text{CsV}_3\\text{Sb}_5$, DAO-G generates a structure close to the\nexperimental one with an RMSE of 0.0085; DAO-P predicts the $T_c$ value with\nhigh accuracy (2.26 K vs. the ground-truth value of 2.30 K). In contrast,\nconventional DFT calculators like Quantum Espresso only successfully derive the\nstructure of the first superconductor within an acceptable time, while the RMSE\nis nearly 8 times larger, and the computation speed is more than 1000 times\nslower. These compelling results collectively highlight the potential of our\napproach for advancing materials science research and development.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:44:16 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wu", "Liming", ""], ["Huang", "Wenbing", ""], ["Jiao", "Rui", ""], ["Huang", "Jianxing", ""], ["Liu", "Liwei", ""], ["Zhou", "Yipeng", ""], ["Sun", "Hao", ""], ["Liu", "Yang", ""], ["Sun", "Fuchun", ""], ["Ren", "Yuxiang", ""], ["Wen", "Jirong", ""]], "extracted_entities": [{"text": "DAO-G", "label": "Foundation Model"}, {"text": "DAO-P", "label": "Foundation Model"}, {"text": "DAO-G", "label": "Foundation Model"}, {"text": "DAO-G", "label": "Foundation Model"}, {"text": "DAO-P", "label": "Foundation Model"}, {"text": "DAO-G", "label": "Foundation Model"}, {"text": "DAO-G", "label": "Foundation Model"}, {"text": "DAO-P", "label": "Foundation Model"}]}
{"id": "2503.10472", "submitter": "Chao Zhou", "authors": "Chao Zhou, Changsheng You, Beixiong Zheng, Xiaodan Shao, and Rui Zhang", "title": "Rotatable Antennas for Integrated Sensing and Communications", "comments": "This work is submitted to IEEE for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we propose to deploy rotatable antennas (RAs) at the base\nstation (BS) to enhance both communication and sensing (C&S) performances, by\nexploiting a new spatial degree-of-freedom (DoF) offered by array rotation.\nSpecifically, we formulate a multi-objective optimization problem to\nsimultaneously maximize the sum-rate of multiple communication users and\nminimize the Cram\\'er-Rao bound (CRB) for target angle estimation, by jointly\noptimizing the transmit beamforming vectors and the array rotation angle at the\nBS. To solve this problem, we first equivalently decompose it into two\nsubproblems, corresponding to an inner problem for beamforming optimization and\nan outer problem for array rotation optimization. Although these two\nsubproblems are non-convex, we obtain their high-quality solutions by applying\nthe block coordinate descent (BCD) technique and one-dimensional exhaustive\nsearch, respectively. Moreover, we show that for the communication-only case,\nRAs provide an additional rotation gain to improve communication performance;\nwhile for the sensing-only case, the equivalent spatial aperture can be\nenlarged by RAs for achieving higher sensing accuracy. Finally, numerical\nresults are presented to showcase the performance gains of RAs over\nfixed-rotation antennas in integrated sensing and communications (ISAC).\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:44:35 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhou", "Chao", ""], ["You", "Changsheng", ""], ["Zheng", "Beixiong", ""], ["Shao", "Xiaodan", ""], ["Zhang", "Rui", ""]], "extracted_entities": [{"text": "RAs", "label": "LLMs"}, {"text": "RAs", "label": "LLMs"}, {"text": "RAs", "label": "LLMs"}]}
{"id": "2503.10478", "submitter": "Lei Wu", "authors": "Liyan Luo and Songyan Tian and Lei Wu", "title": "Multiscale simulation of interacting turbulent and rarefied gas flows in\n  the DSMC framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph physics.flu-dyn", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A multiscale stochastic-deterministic coupling method is proposed to\ninvestigate the complex interactions between turbulent and rarefied gas flows\nwithin a unified framework. This method intermittently integrates the general\nsynthetic iterative scheme with the shear stress transport turbulence model\ninto the direct simulation Monte Carlo (DSMC) approach, enabling the simulation\nof gas flows across the free-molecular, transition, slip, and turbulent\nregimes. First, the macroscopic synthetic equations, derived directly from\nDSMC, are coupled with the turbulence model to establish a constitutive\nrelation that incorporates not only turbulent and laminar transport\ncoefficients but also higher-order terms accounting for rarefaction effects.\nSecond, the macroscopic properties, statistically sampled over specific time\nintervals in DSMC, along with the turbulent properties provided by the\nturbulence model, serve as initial conditions for solving the macroscopic\nsynthetic equations. Finally, the simulation particles in DSMC are updated\nbased on the macroscopic properties obtained from the synthetic equations.\nNumerical simulations demonstrate that the proposed method asymptotically\nconverges to either the turbulence model or DSMC results, adaptively adjusting\nto different flow regimes. Then, this coupling method is applied to simulate an\nopposing jet surrounded by hypersonic rarefied gas flows, revealing significant\nvariations in surface properties due to the interplay of turbulent and rarefied\neffects. This study presents an efficient methodology for simulating the\ncomplex interplay between rarefied and turbulent flows, establishing a\nfoundational framework for investigating the coupled effects of turbulence,\nhypersonic conditions, and chemical reactions in rarefied gas dynamics in the\nfuture.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:49:21 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Luo", "Liyan", ""], ["Tian", "Songyan", ""], ["Wu", "Lei", ""]], "extracted_entities": [{"text": "shear stress transport turbulence model", "label": "Foundation Model"}, {"text": "turbulence model", "label": "Foundation Model"}, {"text": "turbulence model", "label": "Foundation Model"}, {"text": "DSMC", "label": "Foundation Model"}, {"text": "turbulence model", "label": "Foundation Model"}, {"text": "DSMC", "label": "Foundation Model"}]}
{"id": "2503.10480", "submitter": "Siyin Wang", "authors": "Siyin Wang, Zhaoye Fei, Qinyuan Cheng, Shiduo Zhang, Panpan Cai,\n  Jinlan Fu, Xipeng Qiu", "title": "World Modeling Makes a Better Planner: Dual Preference Optimization for\n  Embodied Task Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in large vision-language models (LVLMs) have shown promise\nfor embodied task planning, yet they struggle with fundamental challenges like\ndependency constraints and efficiency. Existing approaches either solely\noptimize action selection or leverage world models during inference,\noverlooking the benefits of learning to model the world as a way to enhance\nplanning capabilities. We propose Dual Preference Optimization (D$^2$PO), a new\nlearning framework that jointly optimizes state prediction and action selection\nthrough preference learning, enabling LVLMs to understand environment dynamics\nfor better planning. To automatically collect trajectories and stepwise\npreference data without human annotation, we introduce a tree search mechanism\nfor extensive exploration via trial-and-error. Extensive experiments on\nVoTa-Bench demonstrate that our D$^2$PO-based method significantly outperforms\nexisting methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and\nLLaMA-3.2 (11B), achieving superior task success rates with more efficient\nexecution paths.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:49:56 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Siyin", ""], ["Fei", "Zhaoye", ""], ["Cheng", "Qinyuan", ""], ["Zhang", "Shiduo", ""], ["Cai", "Panpan", ""], ["Fu", "Jinlan", ""], ["Qiu", "Xipeng", ""]], "extracted_entities": [{"text": "LVLMs", "label": "Large Language Model"}, {"text": "preference learning", "label": "Few-shot Learning"}, {"text": "LVLMs", "label": "Large Language Model"}, {"text": "GPT-4o", "label": "GPT"}]}
{"id": "2503.10481", "submitter": "Jiren Sun", "authors": "Jiren Sun and Thomas D. Cook", "title": "On the Proportional Principal Stratum Hazards Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical trials involving both mortality and morbidity, an active\ntreatment can influence the observed risk of the first non-fatal event either\ndirectly, through its effect on the non-fatal event process, or indirectly,\nthrough its effect on the death process, or both. Discerning the direct effect\nof treatment on the first non-fatal event holds clinical interest. However,\nwith the competing risk of death, the Cox proportional hazards model that\ntreats death as non-informative censoring and evaluates treatment effects on\ntime to the first non-fatal event provides an estimate of the cause-specific\nhazard ratio, which may not correspond to the direct effect. To obtain the\ndirect effect on the first non-fatal event, within the principal stratification\nframework, we define the principal stratum hazard and introduce the\nProportional Principal Stratum Hazards model. This model estimates the\nprincipal stratum hazard ratio, which reflects the direct effect on the first\nnon-fatal event in the presence of death and simplifies to the hazard ratio in\nthe absence of death. The principal stratum membership is identified using the\nshared frailty model, which assumes independence between the first non-fatal\nevent process and the potential death process from the counterfactual arm,\nconditional on per-subject random frailty. Simulation studies are conducted to\nverify the reliability of our estimators. We illustrate the method using the\nCarvedilol Prospective Randomized Cumulative Survival trial which involves\nheart-failure events.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:50:01 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Sun", "Jiren", ""], ["Cook", "Thomas D.", ""]], "extracted_entities": [{"text": "Cox proportional hazards model", "label": "AI model"}, {"text": "Proportional Principal Stratum Hazards model", "label": "AI model"}, {"text": "shared frailty model", "label": "AI model"}]}
{"id": "2503.10486", "submitter": "Gaurav Kumar Gupta", "authors": "Gaurav Kumar Gupta and Pranal Pande", "title": "LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3\n  Mini Across Chronic Health Conditions", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large Language Models (LLMs) are revolutionizing medical diagnostics by\nenhancing both disease classification and clinical decision-making. In this\nstudy, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek\nR1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We\nassessed their predictive accuracy at both the disease and category levels, as\nwell as the reliability of their confidence scores. DeepSeek R1 achieved a\ndisease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3\nMini, which attained 72% and 75% respectively. Notably, DeepSeek R1\ndemonstrated exceptional performance in Mental Health, Neurological Disorders,\nand Oncology, where it reached 100% accuracy, while O3 Mini excelled in\nAutoimmune Disease classification with 100% accuracy. Both models, however,\nstruggled with Respiratory Disease classification, recording accuracies of only\n40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of\nconfidence scores revealed that DeepSeek R1 provided high-confidence\npredictions in 92% of cases, compared to 68% for O3 Mini. Ethical\nconsiderations regarding bias, model interpretability, and data privacy are\nalso discussed to ensure the responsible integration of LLMs into clinical\npractice. Overall, our findings offer valuable insights into the strengths and\nlimitations of LLM-based diagnostic systems and provide a roadmap for future\nenhancements in AI-driven healthcare.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:54:26 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Gupta", "Gaurav Kumar", ""], ["Pande", "Pranal", ""]], "extracted_entities": [{"text": "Ethical\nconsiderations", "label": "AI Ethics"}, {"text": "bias", "label": "Model Bias and Fairness"}, {"text": "LLMs", "label": "Large Language Model"}]}
{"id": "2503.10489", "submitter": "Shuqi Lu", "authors": "Shuqi Lu, Xiaohong Ji, Bohang Zhang, Lin Yao, Siyuan Liu, Zhifeng Gao,\n  Linfeng Zhang, Guolin Ke", "title": "Representation Learning, Large-Scale 3D Molecular Pretraining, Molecular\n  Property", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular pretrained representations (MPR) has emerged as a powerful approach\nfor addressing the challenge of limited supervised data in applications such as\ndrug discovery and material design. While early MPR methods relied on 1D\nsequences and 2D graphs, recent advancements have incorporated 3D\nconformational information to capture rich atomic interactions. However, these\nprior models treat molecules merely as discrete atom sets, overlooking the\nspace surrounding them. We argue from a physical perspective that only modeling\nthese discrete points is insufficient. We first present a simple yet insightful\nobservation: naively adding randomly sampled virtual points beyond atoms can\nsurprisingly enhance MPR performance. In light of this, we propose a principled\nframework that incorporates the entire 3D space spanned by molecules. We\nimplement the framework via a novel Transformer-based architecture, dubbed\nSpaceFormer, with three key components: (1) grid-based space discretization;\n(2) grid sampling/merging; and (3) efficient 3D positional encoding. Extensive\nexperiments show that SpaceFormer significantly outperforms previous 3D MPR\nmodels across various downstream tasks with limited data, validating the\nbenefit of leveraging the additional 3D space beyond atoms in MPR models.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:55:01 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Lu", "Shuqi", ""], ["Ji", "Xiaohong", ""], ["Zhang", "Bohang", ""], ["Yao", "Lin", ""], ["Liu", "Siyuan", ""], ["Gao", "Zhifeng", ""], ["Zhang", "Linfeng", ""], ["Ke", "Guolin", ""]], "extracted_entities": [{"text": "SpaceFormer", "label": "Transformer-based model"}, {"text": "grid-based space discretization", "label": "quantisation"}, {"text": "efficient 3D positional encoding", "label": "quantisation"}, {"text": "SpaceFormer", "label": "Transformer-based model"}]}
{"id": "2503.10492", "submitter": "Pranav Vaidhyanathan", "authors": "Lucas Schorling, Pranav Vaidhyanathan, Jonas Schuff, Miguel J.\n  Carballido, Dominik Zumb\\\"uhl, Gerard Milburn, Florian Marquardt, Jakob\n  Foerster, Michael A. Osborne, and Natalia Ares", "title": "Meta-learning characteristics and dynamics of quantum systems", "comments": "6+1 pages, 4 figures. L. Schorling and P. Vaidhyanathan contributed\n  equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.mes-hall cs.LG physics.comp-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  While machine learning holds great promise for quantum technologies, most\ncurrent methods focus on predicting or controlling a specific quantum system.\nMeta-learning approaches, however, can adapt to new systems for which little\ndata is available, by leveraging knowledge obtained from previous data\nassociated with similar systems. In this paper, we meta-learn dynamics and\ncharacteristics of closed and open two-level systems, as well as the Heisenberg\nmodel. Based on experimental data of a Loss-DiVincenzo spin-qubit hosted in a\nGe/Si core/shell nanowire for different gate voltage configurations, we predict\nqubit characteristics i.e. $g$-factor and Rabi frequency using meta-learning.\nThe algorithm we introduce improves upon previous state-of-the-art\nmeta-learning methods for physics-based systems by introducing novel techniques\nsuch as adaptive learning rates and a global optimizer for improved robustness\nand increased computational efficiency. We benchmark our method against other\nmeta-learning methods, a vanilla transformer, and a multilayer perceptron, and\ndemonstrate improved performance.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:56:58 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Schorling", "Lucas", ""], ["Vaidhyanathan", "Pranav", ""], ["Schuff", "Jonas", ""], ["Carballido", "Miguel J.", ""], ["Zumb\u00fchl", "Dominik", ""], ["Milburn", "Gerard", ""], ["Marquardt", "Florian", ""], ["Foerster", "Jakob", ""], ["Osborne", "Michael A.", ""], ["Ares", "Natalia", ""]], "extracted_entities": [{"text": "meta-learning", "label": "Few-shot Learning"}]}
{"id": "2503.10494", "submitter": "Hanxu Hu", "authors": "Hanxu Hu, Jannis Vamvas, Rico Sennrich", "title": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:57:50 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Hu", "Hanxu", ""], ["Vamvas", "Jannis", ""], ["Sennrich", "Rico", ""]], "extracted_entities": [{"text": "LLMs", "label": "LLM"}, {"text": "LLMs", "label": "LLM"}, {"text": "LLMs", "label": "LLM"}]}
{"id": "2503.10496", "submitter": "Eirik H{\\o}yheim", "authors": "Eirik H{\\o}yheim, Lars Skaaret-Lund, Solve S{\\ae}b{\\o}, Aliaksandr\n  Hubin", "title": "Explainable Bayesian deep learning through input-skip Latent Binary\n  Bayesian Neural Networks", "comments": "44 pages, 19 tables, 25 figures. Code available at\n  https://github.com/eirihoyh/ISLaB-LBBNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling natural phenomena with artificial neural networks (ANNs) often\nprovides highly accurate predictions. However, ANNs often suffer from\nover-parameterization, complicating interpretation and raising uncertainty\nissues. Bayesian neural networks (BNNs) address the latter by representing\nweights as probability distributions, allowing for predictive uncertainty\nevaluation. Latent binary Bayesian neural networks (LBBNNs) further handle\nstructural uncertainty and sparsify models by removing redundant weights. This\narticle advances LBBNNs by enabling covariates to skip to any succeeding layer\nor be excluded, simplifying networks and clarifying input impacts on\npredictions. Ultimately, a linear model or even a constant can be found to be\noptimal for a specific problem at hand. Furthermore, the input-skip LBBNN\napproach reduces network density significantly compared to standard LBBNNs,\nachieving over 99% reduction for small networks and over 99.9% for larger ones,\nwhile still maintaining high predictive accuracy and uncertainty measurement.\nFor example, on MNIST, we reached 97% accuracy and great calibration with just\n935 weights, reaching state-of-the-art for compression of neural networks.\nFurthermore, the proposed method accurately identifies the true covariates and\nadjusts for system non-linearity. The main contribution is the introduction of\nactive paths, enhancing directly designed global and local explanations within\nthe LBBNN framework, that have theoretical guarantees and do not require post\nhoc external tools for explanations.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:59:03 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["H\u00f8yheim", "Eirik", ""], ["Skaaret-Lund", "Lars", ""], ["S\u00e6b\u00f8", "Solve", ""], ["Hubin", "Aliaksandr", ""]], "extracted_entities": [{"text": "Bayesian neural networks", "label": "AI model"}]}
{"id": "2503.10497", "submitter": "Weihao Xuan", "authors": "Weihao Xuan, Rui Yang, Heli Qi, Qingcheng Zeng, Yunze Xiao, Yun Xing,\n  Junjue Wang, Huitao Li, Xin Li, Kunyu Yu, Nan Liu, Qingyu Chen, Douglas\n  Teodoro, Edison Marrese-Taylor, Shijian Lu, Yusuke Iwasawa, Yutaka Matsuo,\n  Irene Li", "title": "MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional benchmarks struggle to evaluate increasingly sophisticated\nlanguage models in multilingual and culturally diverse contexts. To address\nthis gap, we introduce MMLU-ProX, a comprehensive multilingual benchmark\ncovering 13 typologically diverse languages with approximately 11,829 questions\nper language. Building on the challenging reasoning-focused design of MMLU-Pro,\nour framework employs a semi-automatic translation process: translations\ngenerated by state-of-the-art large language models (LLMs) are rigorously\nevaluated by expert annotators to ensure conceptual accuracy, terminological\nconsistency, and cultural relevance. We comprehensively evaluate 25\nstate-of-the-art LLMs using 5-shot chain-of-thought (CoT) and zero-shot\nprompting strategies, analyzing their performance across linguistic and\ncultural boundaries. Our experiments reveal consistent performance degradation\nfrom high-resource languages to lower-resource ones, with the best models\nachieving over 70% accuracy on English but dropping to around 40% for languages\nlike Swahili, highlighting persistent gaps in multilingual capabilities despite\nrecent advances. MMLU-ProX is an ongoing project; we are expanding our\nbenchmark by incorporating additional languages and evaluating more language\nmodels to provide a more comprehensive assessment of multilingual capabilities.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 15:59:20 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Xuan", "Weihao", ""], ["Yang", "Rui", ""], ["Qi", "Heli", ""], ["Zeng", "Qingcheng", ""], ["Xiao", "Yunze", ""], ["Xing", "Yun", ""], ["Wang", "Junjue", ""], ["Li", "Huitao", ""], ["Li", "Xin", ""], ["Yu", "Kunyu", ""], ["Liu", "Nan", ""], ["Chen", "Qingyu", ""], ["Teodoro", "Douglas", ""], ["Marrese-Taylor", "Edison", ""], ["Lu", "Shijian", ""], ["Iwasawa", "Yusuke", ""], ["Matsuo", "Yutaka", ""], ["Li", "Irene", ""]], "extracted_entities": [{"text": "state-of-the-art large language models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "5-shot chain-of-thought (CoT)", "label": "Chain of thought"}, {"text": "zero-shot\nprompting strategies", "label": "Prompting"}]}
{"id": "2503.10499", "submitter": "John Fernley", "authors": "John Fernley", "title": "The Second Phase Transition of the Contact Process on a Random Regular\n  Graph", "comments": "21 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regular tree corresponds to the random regular graph as its local limit.\nFor this reason the famous double phase transition of the contact process on\nregular tree has been seen to correspond to a phase transition on the large\nrandom regular graph, at least at the first critical value. In this article, we\nfind a phase transition on that large finite graph at the second critical\nvalue: between linear reinfections and reinfections following a long healthy\nperiod.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:01:58 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Fernley", "John", ""]], "extracted_entities": [{"text": "regular tree", "label": "Large Language Model"}, {"text": "regular tree", "label": "Large Language Model"}]}
{"id": "2503.10501", "submitter": "Xudong Tan", "authors": "Xudong Tan, Peng Ye, Chongjun Tu, Jianjian Cao, Yaoxin Yang, Lin\n  Zhang, Dongzhan Zhou, Tao Chen", "title": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:04:31 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Tan", "Xudong", ""], ["Ye", "Peng", ""], ["Tu", "Chongjun", ""], ["Cao", "Jianjian", ""], ["Yang", "Yaoxin", ""], ["Zhang", "Lin", ""], ["Zhou", "Dongzhan", ""], ["Chen", "Tao", ""]], "extracted_entities": [{"text": "Multimodal Large Language Models", "label": "Large Language Model"}]}
{"id": "2503.10508", "submitter": "Daou Zhang", "authors": "Yuhan Wang, Cheng Liu, Daou Zhang and Weichao Wu", "title": "Hoi2Anomaly: An Explainable Anomaly Detection Approach Guided by\n  Human-Object Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the domain of Image Anomaly Detection (IAD), Existing methods frequently\nexhibit a paucity of fine-grained, interpretable semantic information,\nresulting in the detection of anomalous entities or activities that are\nsusceptible to machine illusions. This deficiency often leads to the detection\nof anomalous entities or actions that are susceptible to machine illusions and\nlack sufficient explanation. In this thesis, we propose a novel approach to\nanomaly detection, termed Hoi2Anomaly, which aims to achieve precise\ndiscrimination and localization of anomalies. The proposed methodology involves\nthe construction of a multi-modal instruction tuning dataset comprising\nhuman-object interaction (HOI) pairs in anomalous scenarios. Second, we have\ntrained an HOI extractor in threat scenarios to localize and match anomalous\nactions and entities. Finally, explanatory content is generated for the\ndetected anomalous HOI by fine-tuning the visual language pretraining (VLP)\nframework. The experimental results demonstrate that Hoi2Anomaly surpasses\nexisting generative approaches in terms of precision and explainability. We\nwill release Hoi2Anomaly for the advancement of the field of anomaly detection.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:09:51 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Yuhan", ""], ["Liu", "Cheng", ""], ["Zhang", "Daou", ""], ["Wu", "Weichao", ""]], "extracted_entities": [{"text": "fine-tuning", "label": "Fine-tuning"}]}
{"id": "2503.10509", "submitter": "Sahar Admoni", "authors": "Sahar Admoni, Omer Ben-Porat, Ofra Amir", "title": "SySLLM: Generating Synthesized Policy Summaries for Reinforcement\n  Learning Agents Using Large Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Policies generated by Reinforcement Learning (RL) algorithms can be difficult\nto describe to users, as they result from the interplay between complex reward\nstructures and neural network-based representations. This combination often\nleads to unpredictable behaviors, making policies challenging to analyze and\nposing significant obstacles to fostering human trust in real-world\napplications. Global policy summarization methods aim to describe agent\nbehavior through a demonstration of actions in a subset of world-states.\nHowever, users can only watch a limited number of demonstrations, restricting\ntheir understanding of policies. Moreover, those methods overly rely on user\ninterpretation, as they do not synthesize observations into coherent patterns.\nIn this work, we present SySLLM (Synthesized Summary using LLMs), a novel\nmethod that employs synthesis summarization, utilizing large language models'\n(LLMs) extensive world knowledge and ability to capture patterns, to generate\ntextual summaries of policies. Specifically, an expert evaluation demonstrates\nthat the proposed approach generates summaries that capture the main insights\ngenerated by experts while not resulting in significant hallucinations.\nAdditionally, a user study shows that SySLLM summaries are preferred over\ndemonstration-based policy summaries and match or surpass their performance in\nobjective agent identification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:10:14 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Admoni", "Sahar", ""], ["Ben-Porat", "Omer", ""], ["Amir", "Ofra", ""]], "extracted_entities": [{"text": "LLMs", "label": "LLMs"}, {"text": "large language models", "label": "Large Language Model"}]}
{"id": "2503.10510", "submitter": "Rajiv Krishnakumar", "authors": "Rajiv Krishnakumar, Julien Baglio, Frederik F. Fl\\\"other, Christian\n  Ruiz, Stefan Habringer, Nicole H. Romano", "title": "Extreme Learning Machines for Attention-based Multiple Instance Learning\n  in Whole-Slide Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG quant-ph", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Whole-slide image classification represents a key challenge in computational\npathology and medicine. Attention-based multiple instance learning (MIL) has\nemerged as an effective approach for this problem. However, the effect of\nattention mechanism architecture on model performance is not well-documented\nfor biomedical imagery. In this work, we compare different methods and\nimplementations of MIL, including deep learning variants. We introduce a new\nmethod using higher-dimensional feature spaces for deep MIL. We also develop a\nnovel algorithm for whole-slide image classification where extreme machine\nlearning is combined with attention-based MIL to improve sensitivity and reduce\ntraining complexity. We apply our algorithms to the problem of detecting\ncirculating rare cells (CRCs), such as erythroblasts, in peripheral blood. Our\nresults indicate that nonlinearities play a key role in the classification, as\nremoving them leads to a sharp decrease in stability in addition to a decrease\nin average area under the curve (AUC) of over 4%. We also demonstrate a\nconsiderable increase in robustness of the model with improvements of over 10%\nin average AUC when higher-dimensional feature spaces are leveraged. In\naddition, we show that extreme learning machines can offer clear improvements\nin terms of training efficiency by reducing the number of trained parameters by\na factor of 5 whilst still maintaining the average AUC to within 1.5% of the\ndeep MIL model. Finally, we discuss options of enriching the classical\ncomputing framework with quantum algorithms in the future. This work can thus\nhelp pave the way towards more accurate and efficient single-cell diagnostics,\none of the building blocks of precision medicine.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:14:08 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Krishnakumar", "Rajiv", ""], ["Baglio", "Julien", ""], ["Fl\u00f6ther", "Frederik F.", ""], ["Ruiz", "Christian", ""], ["Habringer", "Stefan", ""], ["Romano", "Nicole H.", ""]], "extracted_entities": [{"text": "attention mechanism", "label": "Attention mechanism"}, {"text": "attention-based MIL", "label": "Few-shot Learning"}, {"text": "quantum algorithms", "label": "quantisation"}]}
{"id": "2503.10511", "submitter": "Zachary Metzler", "authors": "Zachary Metzler and Zorawar Wadiasingh", "title": "Irradiated Pulsar Planets and Companions as 511 keV Positron\n  Annihilation Line Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.HE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Millisecond pulsars (MSPs) are prolific GeV {\\gamma}-ray emitters, and nearly\n80% of Fermi-LAT MSPs reside in compact binaries. We demonstrate that the\ncompanions in these compact MSPs binaries are also 511 keV annihilation line\nemitters using MEGAlib simulations (a high energy radiation transport software\nbuilt with Geant4) to compute the particle showers and resulting backsplash\nemission from the pulsar irradiation. The 511 keV signal exhibits strong flux\nmodulation and red/blueshifts associated with a binary orbit, enabling powerful\ncoherent searches. Measuring the 511 keV emission would enable direct\n{\\gamma}-ray characterization of unusual pulsar exoplanets and companions, and\nallow one to identify the unambiguous presence of active pulsars whose beams do\nnot intercept Earth. Intriguingly, the 511 keV flux is brightest for\nultra-compact systems against which pulsar surveys are systematically biased.\nThese ultra-compact systems are also possibly prime LISA galactic sources. This\nnecessitates future joint LISA-MeV {\\gamma}-ray techniques to characterize MSP\nbinaries. These MSP binaries may also contribute to the puzzling source of the\nexcess 511 keV photons near the galactic bulge and center.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:15:37 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Metzler", "Zachary", ""], ["Wadiasingh", "Zorawar", ""]], "extracted_entities": [{"text": "MSPs", "label": "LLMs"}]}
{"id": "2503.10512", "submitter": "Devjeet Roy", "authors": "Hooman Shahrokhi, Devjeet Raj Roy, Yan Yan, Venera Arnaoudova and\n  Janaradhan Rao Doppa", "title": "Conformal Prediction Sets for Deep Generative Models via Reduction to\n  Conformal Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We consider the problem of generating valid and small prediction sets by\nsampling outputs (e.g., software code and natural language text) from a\nblack-box deep generative model for a given input (e.g., textual prompt). The\nvalidity of a prediction set is determined by a user-defined binary\nadmissibility function depending on the target application. For example,\nrequiring at least one program in the set to pass all test cases in code\ngeneration application. To address this problem, we develop a simple and\neffective conformal inference algorithm referred to as Generative Prediction\nSets (GPS). Given a set of calibration examples and black-box access to a deep\ngenerative model, GPS can generate prediction sets with provable guarantees.\nThe key insight behind GPS is to exploit the inherent structure within the\ndistribution over the minimum number of samples needed to obtain an admissible\noutput to develop a simple conformal regression approach over the minimum\nnumber of samples. Experiments on multiple datasets for code and math word\nproblems using different large language models demonstrate the efficacy of GPS\nover state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:16:23 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Shahrokhi", "Hooman", ""], ["Roy", "Devjeet Raj", ""], ["Yan", "Yan", ""], ["Arnaoudova", "Venera", ""], ["Doppa", "Janaradhan Rao", ""]], "extracted_entities": [{"text": "textual prompt", "label": "Prompting"}]}
{"id": "2503.10515", "submitter": "Florian Eichin", "authors": "Florian Eichin, Yang Janet Liu, Barbara Plank, Michael A. Hedderich", "title": "Probing LLMs for Multilingual Discourse Generalization Through a Unified\n  Label Set", "comments": "18 pages, 7 figures, 3 tables, code:\n  https://github.com/mainlp/discourse_probes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse understanding is essential for many NLP tasks, yet most existing\nwork remains constrained by framework-dependent discourse representations. This\nwork investigates whether large language models (LLMs) capture discourse\nknowledge that generalizes across languages and frameworks. We address this\nquestion along two dimensions: (1) developing a unified discourse relation\nlabel set to facilitate cross-lingual and cross-framework discourse analysis,\nand (2) probing LLMs to assess whether they encode generalizable discourse\nabstractions. Using multilingual discourse relation classification as a\ntestbed, we examine a comprehensive set of 23 LLMs of varying sizes and\nmultilingual capabilities. Our results show that LLMs, especially those with\nmultilingual training corpora, can generalize discourse information across\nlanguages and frameworks. Further layer-wise analyses reveal that language\ngeneralization at the discourse level is most salient in the intermediate\nlayers. Lastly, our error analysis provides an account of challenging relation\nclasses.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:20:25 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Eichin", "Florian", ""], ["Liu", "Yang Janet", ""], ["Plank", "Barbara", ""], ["Hedderich", "Michael A.", ""]], "extracted_entities": [{"text": "large language models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}]}
{"id": "2503.10521", "submitter": "Enzo Putti-Garcia", "authors": "Enzo Putti-Garcia, Andrii Tykhonov, Andrii Kotenko, Hugo Boutin,\n  Manbing Li, Paul Coppin, Andrea Serpolla, Jennifer Maria Frieden, Chiara\n  Perrina, Xin Wu", "title": "Energy Reconstruction of Non-fiducial Electron-Positron Events in the\n  DAMPE Experiment Using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.HE hep-ex physics.ins-det", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The Dark Matter Particle Explorer (DAMPE) is a space-based Cosmic-Ray (CR)\nobservatory with the aim, among others, to study Cosmic-Ray Electrons (CREs) up\nto 10 TeV. Due to the low CRE rate at multi-TeV energies, we aim to increasing\nthe acceptance by selecting events outside the fiducial volume. The complex\ntopology of non-fiducial events requires the development of a novel energy\nreconstruction method. We propose the usage of Convolutional Neural Networks\nfor a regression task to recover an accurate estimation of the initial energy.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:30:28 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Putti-Garcia", "Enzo", ""], ["Tykhonov", "Andrii", ""], ["Kotenko", "Andrii", ""], ["Boutin", "Hugo", ""], ["Li", "Manbing", ""], ["Coppin", "Paul", ""], ["Serpolla", "Andrea", ""], ["Frieden", "Jennifer Maria", ""], ["Perrina", "Chiara", ""], ["Wu", "Xin", ""]], "extracted_entities": [{"text": "Cosmic-Ray Electrons (CREs)", "label": "LLMs"}]}
{"id": "2503.10523", "submitter": "Yongqi Wang", "authors": "Jun Yu, Yongqi Wang, Lei Wang, Yang Zheng, Shengfan Xu", "title": "Interactive Multimodal Fusion with Temporal Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents our method for the estimation of valence-arousal (VA) in\nthe 8th Affective Behavior Analysis in-the-Wild (ABAW) competition. Our\napproach integrates visual and audio information through a multimodal\nframework. The visual branch uses a pre-trained ResNet model to extract spatial\nfeatures from facial images. The audio branches employ pre-trained VGG models\nto extract VGGish and LogMel features from speech signals. These features\nundergo temporal modeling using Temporal Convolutional Networks (TCNs). We then\napply cross-modal attention mechanisms, where visual features interact with\naudio features through query-key-value attention structures. Finally, the\nfeatures are concatenated and passed through a regression layer to predict\nvalence and arousal. Our method achieves competitive performance on the\nAff-Wild2 dataset, demonstrating effective multimodal fusion for VA estimation\nin-the-wild.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:31:56 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Yu", "Jun", ""], ["Wang", "Yongqi", ""], ["Wang", "Lei", ""], ["Zheng", "Yang", ""], ["Xu", "Shengfan", ""]], "extracted_entities": [{"text": "cross-modal attention mechanisms", "label": "Attention mechanism"}, {"text": "query-key-value attention structures", "label": "Attention mechanism"}]}
{"id": "2503.10526", "submitter": "Zheng Wang", "authors": "Zengrong Lin, Zheng Wang, Tianwen Qian, Pan Mu, Sixian Chan, Cong Bai", "title": "NeighborRetr: Balancing Hub Centrality in Cross-Modal Retrieval", "comments": "Accepted at CVPR 2025, 18 pages, 7 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal retrieval aims to bridge the semantic gap between different\nmodalities, such as visual and textual data, enabling accurate retrieval across\nthem. Despite significant advancements with models like CLIP that align\ncross-modal representations, a persistent challenge remains: the hubness\nproblem, where a small subset of samples (hubs) dominate as nearest neighbors,\nleading to biased representations and degraded retrieval accuracy. Existing\nmethods often mitigate hubness through post-hoc normalization techniques,\nrelying on prior data distributions that may not be practical in real-world\nscenarios. In this paper, we directly mitigate hubness during training and\nintroduce NeighborRetr, a novel method that effectively balances the learning\nof hubs and adaptively adjusts the relations of various kinds of neighbors. Our\napproach not only mitigates the hubness problem but also enhances retrieval\nperformance, achieving state-of-the-art results on multiple cross-modal\nretrieval benchmarks. Furthermore, NeighborRetr demonstrates robust\ngeneralization to new domains with substantial distribution shifts,\nhighlighting its effectiveness in real-world applications. We make our code\npublicly available at: https://github.com/zzezze/NeighborRetr .\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:33:55 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Lin", "Zengrong", ""], ["Wang", "Zheng", ""], ["Qian", "Tianwen", ""], ["Mu", "Pan", ""], ["Chan", "Sixian", ""], ["Bai", "Cong", ""]], "extracted_entities": [{"text": "hubness\nproblem", "label": "Model Bias and Fairness"}]}
{"id": "2503.10527", "submitter": "Luca Trautmann", "authors": "Luca Trautmann, Peter Wijeratne, Itamar Ronen, and Ivor Simpson", "title": "How Should We Evaluate Uncertainty in Accelerated MRI Reconstruction?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing accelerated MRI is an ill-posed problem. Machine learning has\nrecently shown great promise at this task, but current approaches to\nquantifying uncertainty focus on measuring the variability in pixelwise\nintensity variation. Although these provide interpretable maps, they lack\nstructural understanding and they do not have a clear relationship to how the\ndata will be analysed subsequently. In this paper, we propose a new approach to\nevaluating reconstruction variability based on apparent anatomical changes in\nthe reconstruction, which is more tightly related to common downstream tasks.\nWe use image registration and segmentation to evaluate several common MRI\nreconstruction approaches, where uncertainty is measured via ensembling, for\naccelerated imaging. We demonstrate the intrinsic variability in reconstructed\nimages and show that models with high scores on often used quality metrics such\nas SSIM and PSNR, can nonetheless display high levels of variance and bias in\nanatomical measures.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:34:22 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Trautmann", "Luca", ""], ["Wijeratne", "Peter", ""], ["Ronen", "Itamar", ""], ["Simpson", "Ivor", ""]], "extracted_entities": [{"text": "Machine learning", "label": "Few-shot Learning"}, {"text": "ensembling", "label": "Embedding"}]}
{"id": "2503.10528", "submitter": "Will Boney", "authors": "Will Boney", "title": "A Module-theoretic Introduction to Abstract Elementary Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO math.RA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The first-order model theory of modules has been studied for decades. More\nrecently, the model theoretic study of nonelementary classes of\nmodules--especially Abstract Elementary Classes of modules--has produced\ninteresting results. This survey aims to discuss these recent results and give\nan introduction to the framework of Abstract Elementary Classes for module\ntheorists.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:35:20 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Boney", "Will", ""]], "extracted_entities": [{"text": "Abstract Elementary Classes of modules", "label": "LLMs"}]}
{"id": "2503.10529", "submitter": "Zilu Guo", "authors": "Zilu Guo, Hongbin Lin, Zhihao Yuan, Chaoda Zheng, Pengshuo Qiu,\n  Dongzhi Jiang, Renrui Zhang, Chun-Mei Feng, Zhen Li", "title": "PiSA: A Self-Augmented Data Engine and Training Strategy for 3D\n  Understanding with Large Models", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D Multimodal Large Language Models (MLLMs) have recently made substantial\nadvancements. However, their potential remains untapped, primarily due to the\nlimited quantity and suboptimal quality of 3D datasets. Current approaches\nattempt to transfer knowledge from 2D MLLMs to expand 3D instruction data, but\nstill face modality and domain gaps. To this end, we introduce PiSA-Engine\n(Point-Self-Augmented-Engine), a new framework for generating instruction\npoint-language datasets enriched with 3D spatial semantics. We observe that\nexisting 3D MLLMs offer a comprehensive understanding of point clouds for\nannotation, while 2D MLLMs excel at cross-validation by providing complementary\ninformation. By integrating holistic 2D and 3D insights from off-the-shelf\nMLLMs, PiSA-Engine enables a continuous cycle of high-quality data generation.\nWe select PointLLM as the baseline and adopt this co-evolution training\nframework to develop an enhanced 3D MLLM, termed PointLLM-PiSA. Additionally,\nwe identify limitations in previous 3D benchmarks, which often feature coarse\nlanguage captions and insufficient category diversity, resulting in inaccurate\nevaluations. To address this gap, we further introduce PiSA-Bench, a\ncomprehensive 3D benchmark covering six key aspects with detailed and diverse\nlabels. Experimental results demonstrate PointLLM-PiSA's state-of-the-art\nperformance in zero-shot 3D object captioning and generative classification on\nour PiSA-Bench, achieving significant improvements of 46.45% (+8.33%) and\n63.75% (+16.25%), respectively. We will release the code, datasets, and\nbenchmark.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:37:26 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Guo", "Zilu", ""], ["Lin", "Hongbin", ""], ["Yuan", "Zhihao", ""], ["Zheng", "Chaoda", ""], ["Qiu", "Pengshuo", ""], ["Jiang", "Dongzhi", ""], ["Zhang", "Renrui", ""], ["Feng", "Chun-Mei", ""], ["Li", "Zhen", ""]], "extracted_entities": [{"text": "3D Multimodal Large Language Models", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}, {"text": "MLLMs", "label": "Large Language Model"}]}
{"id": "2503.10531", "submitter": "Sylvain Prolhac", "authors": "Sylvain Prolhac", "title": "Current fluctuations for the second class particle : joint statistics", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider TASEP with a single second class particle and periodic boundary\nconditions. Using Bethe ansatz, we compute stationary large deviations for the\njoint statistics of the current of first and second class particles. At large\nscales, the generating function of the joint cumulants shows an unexpected\nconnection to current fluctuations of TASEP with open boundaries.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:39:21 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Prolhac", "Sylvain", ""]], "extracted_entities": [{"text": "large\nscales", "label": "Scaling law"}]}
{"id": "2503.10535", "submitter": "Michael Atambo", "authors": "Michael O. Atambo, Korir Kiptiemoi", "title": "On the Automation of High Throughput Modeling of Adsorption In Porous\n  Zeolytic Imidazolate Frameworks", "comments": "6 Pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The zeolitic imidazolate frameworks (ZIF) have emerged as a promising\ncandidate for catalysis, carbon-dioxide (CO$_{2}$) capture and storage as well\nas flue gas separation due to their tunable porosity and chemical stability.\nZIFs consists of transition metals in a tetrahedral coordination with\nimidazolate linkers, allowing for structural modifications that can enhance\nCO$_{2}$ adsorption and storage. To systematically design and optimize ZIFs for\nsuperior CO$_{2}$ capture performance, theoretical modelling provides an\neffective approach, though hindered by the sheer size of the search space, with\nhundred to thousands of possible sites per ZIF, and thousands of possible ZIF\nthat can be synthesized. In this work, we employ a high-throughput techniques\nunderpinned by first-principle Density Functional Theory (DFT) to develop a\nsystematic automated method for characterizing adsorption energetics in ZIFs,\nguiding the rational design and selection of desirable ZIF structures. Using\nhigh-throughput computing tools, we perform pore and pore size analysis, and\nimplement automated CO$_{2}$ molecule placement algorithm, accounting for\npositional symmetry, crystallographic orientation, and collision detection. The\nresults obtained are in good agreement with previous studies, demonstrating the\nreliability of the approach in accelerating the discovery of next-generation\nZIFs for CO$_{2}$ capture and storage.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:47:53 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Atambo", "Michael O.", ""], ["Kiptiemoi", "Korir", ""]], "extracted_entities": [{"text": "ZIFs", "label": "LLMs"}, {"text": "ZIFs", "label": "LLMs"}, {"text": "ZIF", "label": "LLMs"}, {"text": "ZIFs", "label": "LLMs"}, {"text": "ZIF", "label": "LLMs"}, {"text": "ZIFs", "label": "LLMs"}]}
{"id": "2503.10536", "submitter": "Philipp Maass", "authors": "Seemant Mishra, Artem Ryabov, and Philipp Maass", "title": "Phase locking and fractional Shapiro steps in collective dynamics of\n  microparticles", "comments": "6 pages, 4 figures plus supplemental material with 3 figures", "journal-ref": "Phys. Rev. Lett. 134, 107102 (2025)", "doi": "10.1103/PhysRevLett.134.107102", "report-no": null, "categories": "cond-mat.stat-mech cond-mat.soft", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In driven nonlinear systems, phase locking is an intriguing effect leading to\nrobust stationary states that are stable over extended ranges of control\nparameters. Recent experiments allow for exploring microscopic mechanisms\nunderlying such phenomena in collective dynamics of micro- and nanoparticles.\nHere we show that phase-locked dynamics of hardcore-interacting microparticles\nin a densely populated periodic potential under time-periodic driving arises\nfrom running solitary cluster waves. We explain how values of phase-locked\ncurrents are related to soliton velocities and why collective particle dynamics\nsynchronize with the driving for certain particle diameters only. Our analysis\nis based on an effective potential for the solitary wave propagation and a unit\ndisplacement law, saying that the total average shift of all particle positions\nper soliton period equals one wavelength of the periodic potential.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:50:21 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Mishra", "Seemant", ""], ["Ryabov", "Artem", ""], ["Maass", "Philipp", ""]], "extracted_entities": [{"text": "unit\ndisplacement law", "label": "Scaling law"}]}
{"id": "2503.10538", "submitter": "Teresa Head-Gordon", "authors": "Eric C.-Y. Yuan, Yunsheng Liu, Junmin Chen, Peichen Zhong, Sanjeev\n  Raja, Tobias Kreiman, Santiago Vargas, Wenbin Xu, Martin Head-Gordon, Chao\n  Yang, Samuel M. Blau, Aditi Krishnapriyan, Teresa Head-Gordon", "title": "Foundation Models for Atomistic Simulation of Chemistry and Materials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Given the power of large language and large vision models, it is of profound\nand fundamental interest to ask if a foundational model based on data and\nparameter scaling laws and pre-training strategies is possible for learned\nsimulations of chemistry and materials. The scaling of large and diverse\ndatasets and highly expressive architectures for chemical and materials\nsciences should result in a foundation model that is more efficient and broadly\ntransferable, robust to out-of-distribution challenges, and easily fine-tuned\nto a variety of downstream observables, when compared to specific training from\nscratch on targeted applications in atomistic simulation. In this Perspective\nwe aim to cover the rapidly advancing field of machine learned interatomic\npotentials (MLIP), and to illustrate a path to create chemistry and materials\nMLIP foundation models at larger scale.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:52:12 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Yuan", "Eric C. -Y.", ""], ["Liu", "Yunsheng", ""], ["Chen", "Junmin", ""], ["Zhong", "Peichen", ""], ["Raja", "Sanjeev", ""], ["Kreiman", "Tobias", ""], ["Vargas", "Santiago", ""], ["Xu", "Wenbin", ""], ["Head-Gordon", "Martin", ""], ["Yang", "Chao", ""], ["Blau", "Samuel M.", ""], ["Krishnapriyan", "Aditi", ""], ["Head-Gordon", "Teresa", ""]], "extracted_entities": [{"text": "data and\nparameter scaling laws", "label": "Scaling law"}, {"text": "foundation model", "label": "Foundation Model"}]}
{"id": "2503.10541", "submitter": "Satyabrata Jana", "authors": "Ankit Abhinav, Satyabrata Jana, Abhishek Sahu", "title": "Towards Transitive-free Digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a digraph $D$, an arc $e=(x,y) $ in $D$ is considered transitive if there\nis a path from $x$ to $y$ in $D- e$. A digraph is transitive-free if it does\nnot contain any transitive arc. In the Transitive-free Vertex Deletion (TVD)\nproblem, the goal is to find at most $k$ vertices $S$ such that $D-S$ has no\ntransitive arcs. In our work, we study a more general version of the TVD\nproblem, denoted by $\\ell$-Relaxed Transitive-free Vertex Deletion\n($\\ell$-RTVD), where we look for at most $k$ vertices $S$ such that $D-S$ has\nno more than $\\ell$ transitive arcs. We explore $\\ell$-RTVD on various\nwell-known graph classes of digraphs such as directed acyclic graphs (DAGs),\nplanar DAGs, $\\alpha$-bounded digraphs, tournaments, and their multiple\ngeneralizations such as in-tournaments, out-tournaments, local tournaments,\nacyclic local tournaments, and obtain the following results. Although the\nproblem admits polynomial-time algorithms in tournaments, $\\alpha$-bounded\ndigraphs, and acyclic local tournaments for fixed values of $\\ell$, it remains\nNP-hard even in planar DAGs with maximum degree 6. In the parameterized realm,\nfor $\\ell$-RTVD on in-tournaments and out-tournaments, we obtain polynomial\nkernels parameterized by $k+\\ell$ for bounded independence number. But the\nproblem remains fixed-parameter intractable on DAGs when parameterized by $k$.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:56:45 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Abhinav", "Ankit", ""], ["Jana", "Satyabrata", ""], ["Sahu", "Abhishek", ""]], "extracted_entities": [{"text": "acyclic local tournaments", "label": "LLMs"}]}
{"id": "2503.10544", "submitter": "Jing Xu", "authors": "Jing Xu, Franziska Boenisch, Iyiola Emmanuel Olatunji, and Adam\n  Dziedzic", "title": "DP-GPL: Differentially Private Graph Prompt Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have shown remarkable performance in various\napplications. Recently, graph prompt learning has emerged as a powerful GNN\ntraining paradigm, inspired by advances in language and vision foundation\nmodels. Here, a GNN is pre-trained on public data and then adapted to sensitive\ntasks using lightweight graph prompts. However, using prompts from sensitive\ndata poses privacy risks. In this work, we are the first to investigate these\npractical risks in graph prompts by instantiating a membership inference attack\nthat reveals significant privacy leakage. We also find that the standard\nprivacy method, DP-SGD, fails to provide practical privacy-utility trade-offs\nin graph prompt learning, likely due to the small number of sensitive data\npoints used to learn the prompts. As a solution, we propose DP-GPL for\ndifferentially private graph prompt learning based on the PATE framework, that\ngenerates a graph prompt with differential privacy guarantees. Our evaluation\nacross various graph prompt learning methods, GNN architectures, and\npre-training strategies demonstrates that our algorithm achieves high utility\nat strong privacy, effectively mitigating privacy concerns while preserving the\npowerful capabilities of prompted GNNs as powerful foundation models in the\ngraph domain.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:58:07 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Xu", "Jing", ""], ["Boenisch", "Franziska", ""], ["Olatunji", "Iyiola Emmanuel", ""], ["Dziedzic", "Adam", ""]], "extracted_entities": [{"text": "graph prompt learning", "label": "Prompting"}, {"text": "graph prompts", "label": "Prompting"}, {"text": "graph prompts", "label": "Prompting"}, {"text": "graph prompt learning", "label": "Prompting"}, {"text": "prompts", "label": "Prompting"}, {"text": "graph prompt learning", "label": "Prompting"}, {"text": "graph prompt learning", "label": "Prompting"}]}
{"id": "2503.10546", "submitter": "Mingtong Zhang", "authors": "Zixian Liu, Mingtong Zhang, Yunzhu Li", "title": "KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation", "comments": "Project website: http://kuda-dynamics.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid advancement of large language models (LLMs) and\nvision-language models (VLMs), significant progress has been made in developing\nopen-vocabulary robotic manipulation systems. However, many existing approaches\noverlook the importance of object dynamics, limiting their applicability to\nmore complex, dynamic tasks. In this work, we introduce KUDA, an\nopen-vocabulary manipulation system that integrates dynamics learning and\nvisual prompting through keypoints, leveraging both VLMs and learning-based\nneural dynamics models. Our key insight is that a keypoint-based target\nspecification is simultaneously interpretable by VLMs and can be efficiently\ntranslated into cost functions for model-based planning. Given language\ninstructions and visual observations, KUDA first assigns keypoints to the RGB\nimage and queries the VLM to generate target specifications. These abstract\nkeypoint-based representations are then converted into cost functions, which\nare optimized using a learned dynamics model to produce robotic trajectories.\nWe evaluate KUDA on a range of manipulation tasks, including free-form language\ninstructions across diverse object categories, multi-object interactions, and\ndeformable or granular objects, demonstrating the effectiveness of our\nframework. The project page is available at http://kuda-dynamics.github.io.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 16:59:17 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Liu", "Zixian", ""], ["Zhang", "Mingtong", ""], ["Li", "Yunzhu", ""]], "extracted_entities": [{"text": "large language models", "label": "Large Language Model"}, {"text": "LLMs", "label": "Large Language Model"}, {"text": "VLMs", "label": "Large Language Model"}, {"text": "dynamics learning", "label": "Few-shot Learning"}, {"text": "visual prompting", "label": "Prompting"}, {"text": "VLMs", "label": "Large Language Model"}, {"text": "learning-based\nneural dynamics models", "label": "Neural Language Model"}, {"text": "VLMs", "label": "Large Language Model"}]}
{"id": "2503.10549", "submitter": "Youngjin Kwon", "authors": "Youngjin Kwon, Xiao Zhang", "title": "MASQUE: A Text-Guided Diffusion-Based Framework for Localized and\n  Customized Adversarial Makeup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As facial recognition is increasingly adopted for government and commercial\nservices, its potential misuse has raised serious concerns about privacy and\ncivil rights. To counteract, various anti-facial recognition techniques have\nbeen proposed for privacy protection by adversarially perturbing face images,\namong which generative makeup-based approaches are the most popular. However,\nthese methods, designed primarily to impersonate specific target identities,\ncan only achieve weak dodging success rates while increasing the risk of\ntargeted abuse. In addition, they often introduce global visual artifacts or a\nlack of adaptability to accommodate diverse makeup prompts, compromising user\nsatisfaction. To address the above limitations, we develop MASQUE, a novel\ndiffusion-based framework that generates localized adversarial makeups guided\nby user-defined text prompts. Built upon precise null-text inversion,\ncustomized cross-attention fusion with masking, and a pairwise adversarial\nguidance mechanism using images of the same individual, MASQUE achieves robust\ndodging performance without requiring any external identity. Comprehensive\nevaluations on open-source facial recognition models and commercial APIs\ndemonstrate that MASQUE significantly improves dodging success rates over all\nbaselines, along with higher perceptual fidelity and stronger adaptability to\nvarious text makeup prompts.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:05:53 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Kwon", "Youngjin", ""], ["Zhang", "Xiao", ""]], "extracted_entities": [{"text": "civil rights", "label": "AI Ethics"}, {"text": "user-defined text prompts", "label": "Attention mechanism"}, {"text": "pairwise adversarial\nguidance mechanism", "label": "Attention mechanism"}, {"text": "open-source facial recognition models", "label": "Open-source LLMs"}, {"text": "MASQUE", "label": "Generative Pre-trained Transformer (GPT)"}]}
{"id": "2503.10553", "submitter": "Eric Boltersdorf", "authors": "Eric Boltersdorf and Thilo vom H\\\"ovel and Jeremy Andrew Mor\\'in\n  Nenoff and Frank Vewinger and Martin Weitz", "title": "Two-photon spectroscopy and a verification of the Kennard-Stepanov\n  relation in high-pressure two-species xenon-noble gas mixtures", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Between the absorption and the emission spectral lineshapes of dense atomic\nand molecular media, such as dye solutions and alkali-noble buffer gas mixtures\nat high pressure, in many cases there exists a universal scaling, the\nKennard-Stepanov relation, which is a manifestation of detailed balance. This\nrelation plays a crucial role in recent Bose-Einstein condensation experiments\nof visible-spectral-photons in e.g. dye-solution-filled optical microcavities.\nIt has recently been proposed to use high-pressure xenon-noble gas mixtures as\na thermalization medium for vacuum-ultraviolet regime photons, so as to extend\nthe achievable wavelength range of such Bose-Einstein-condensed optical sources\nfrom the visible to the vacuum-ultraviolet regime. In this work, we report\ntwo-photon excitation spectroscopy measurements of ground state ($5p^6$) xenon\natoms subject to up to 80bar of helium or krypton buffer gas pressure,\nrespectively, in the 220nm - 260nm wavelength range. The study of such\ntwo-photon spectra is of interest e.g. for the exploration of possible pumping\nschemes of a future vacuum-ultraviolet photon Bose-Einstein condensate. We have\nalso recorded absorption and emission spectra of the $5p^6 \\leftrightarrow\n5p^56s$ single-photon transition near 147nm wavelength of xenon atoms subject\nto 80bar of krypton buffer gas pressure. We find that the ratio of absorption\nand emission follows a Kennard-Stepanov scaling, which suggests that such gas\nmixtures are promising candidates as a thermalization medium for a\nBose-Einstein condensate of vacuum-ultraviolet photons.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:09:05 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Boltersdorf", "Eric", ""], ["H\u00f6vel", "Thilo vom", ""], ["Nenoff", "Jeremy Andrew Mor\u00edn", ""], ["Vewinger", "Frank", ""], ["Weitz", "Martin", ""]], "extracted_entities": [{"text": "universal scaling", "label": "Scaling law"}, {"text": "Kennard-Stepanov relation", "label": "Scaling law"}, {"text": "Kennard-Stepanov scaling", "label": "Scaling law"}]}
{"id": "2503.10556", "submitter": "Jaan Aru", "authors": "Brett Puppart and Jaan Aru", "title": "Short-term AI literacy intervention does not reduce over-reliance on\n  incorrect ChatGPT recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this study, we examined whether a short-form AI literacy intervention\ncould reduce the adoption of incorrect recommendations from large language\nmodels. High school seniors were randomly assigned to either a control or an\nintervention group, which received an educational text explaining ChatGPT's\nworking mechanism, limitations, and proper use. Participants solved math\npuzzles with the help of ChatGPT's recommendations, which were incorrect in\nhalf of the cases. Results showed that students adopted incorrect suggestions\n52.1% of the time, indicating widespread over-reliance. The educational\nintervention did not significantly reduce over-reliance. Instead, it led to an\nincrease in ignoring ChatGPT's correct recommendations. We conclude that the\nusage of ChatGPT is associated with over-reliance and it is not trivial to\nincrease AI literacy to counter over-reliance.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:10:33 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Puppart", "Brett", ""], ["Aru", "Jaan", ""]], "extracted_entities": [{"text": "ChatGPT", "label": "ChatGPT"}, {"text": "ChatGPT", "label": "ChatGPT"}, {"text": "ChatGPT", "label": "ChatGPT"}, {"text": "ChatGPT", "label": "ChatGPT"}]}
{"id": "2503.10559", "submitter": "Georg J\\\"ager", "authors": "Georg J\\\"ager and Nils-Jonathan Friedrich and Hauke Petersen and\n  Benjamin Noack", "title": "Towards Safe Path Tracking Using the Simplex Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Robot navigation in complex environments necessitates controllers that are\nadaptive and safe. Traditional controllers like Regulated Pure Pursuit, Dynamic\nWindow Approach, and Model-Predictive Path Integral, while reliable, struggle\nto adapt to dynamic conditions. Reinforcement Learning offers adaptability but\nlacks formal safety guarantees. To address this, we propose a path tracking\ncontroller leveraging the Simplex architecture. It combines a Reinforcement\nLearning controller for adaptiveness and performance with a high-assurance\ncontroller providing safety and stability. Our contribution is twofold. We\nfirstly discuss general stability and safety considerations for designing\ncontrollers using the Simplex architecture. Secondly, we present a\nSimplex-based path tracking controller. Our simulation results, supported by\npreliminary in-field tests, demonstrate the controller's effectiveness in\nmaintaining safety while achieving comparable performance to state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:11:55 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["J\u00e4ger", "Georg", ""], ["Friedrich", "Nils-Jonathan", ""], ["Petersen", "Hauke", ""], ["Noack", "Benjamin", ""]], "extracted_entities": [{"text": "Reinforcement Learning", "label": "Few-shot Learning"}, {"text": "Reinforcement\nLearning", "label": "Few-shot Learning"}]}
{"id": "2503.10560", "submitter": "Nicolas Pr\\\"ollochs", "authors": "Kirill Solovev, Nicolas Pr\\\"ollochs", "title": "References to unbiased sources increase the helpfulness of community\n  fact-checks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community-based fact-checking is a promising approach to address\nmisinformation on social media at scale. However, an understanding of what\nmakes community-created fact-checks helpful to users is still in its infancy.\nIn this paper, we analyze the determinants of the helpfulness of\ncommunity-created fact-checks. For this purpose, we draw upon a unique dataset\nof real-world community-created fact-checks and helpfulness ratings from X's\n(formerly Twitter) Community Notes platform. Our empirical analysis implies\nthat the key determinant of helpfulness in community-based fact-checking is\nwhether users provide links to external sources to underpin their assertions.\nOn average, the odds for community-created fact-checks to be perceived as\nhelpful are 2.70 times higher if they provide links to external sources.\nFurthermore, we demonstrate that the helpfulness of community-created\nfact-checks varies depending on their level of political bias. Here, we find\nthat community-created fact-checks linking to high-bias sources (of either\npolitical side) are perceived as significantly less helpful. This suggests that\nthe rating mechanism on the Community Notes platform successfully penalizes\none-sidedness and politically motivated reasoning. These findings have\nimportant implications for social media platforms, which can utilize our\nresults to optimize their community-based fact-checking systems.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:12:01 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Solovev", "Kirill", ""], ["Pr\u00f6llochs", "Nicolas", ""]], "extracted_entities": [{"text": "political bias", "label": "Model Bias and Fairness"}, {"text": "rating mechanism", "label": "Attention mechanism"}]}
{"id": "2503.10561", "submitter": "Soham Das", "authors": "Soham Das, Santiago Paternain, Luiz F. O. Chamon and Ceyhun Eksin", "title": "The Lagrangian Method for Solving Constrained Markov Games", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose the concept of a Lagrangian game to solve constrained Markov\ngames. Such games model scenarios where agents face cost constraints in\naddition to their individual rewards, that depend on both agent joint actions\nand the evolving environment state over time. Constrained Markov games form the\nformal mechanism behind safe multiagent reinforcement learning, providing a\nstructured model for dynamic multiagent interactions in a multitude of\nsettings, such as autonomous teams operating under local energy and time\nconstraints, for example. We develop a primal-dual approach in which agents\nsolve a Lagrangian game associated with the current Lagrange multiplier,\nsimulate cost and reward trajectories over a fixed horizon, and update the\nmultiplier using accrued experience. This update rule generates a new\nLagrangian game, initiating the next iteration. Our key result consists in\nshowing that the sequence of solutions to these Lagrangian games yields a\nnonstationary Nash solution for the original constrained Markov game.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:12:14 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Das", "Soham", ""], ["Paternain", "Santiago", ""], ["Chamon", "Luiz F. O.", ""], ["Eksin", "Ceyhun", ""]], "extracted_entities": [{"text": "safe multiagent reinforcement learning", "label": "Few-shot Learning"}]}
{"id": "2503.10565", "submitter": "Marek Gazdzicki", "authors": "Marek Gazdzicki, Daniel Kikola, Ivan Pidhurskyi, Leonardo Tinti", "title": "Apparent teleportation of indistinguishable particles", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nucl-th hep-ph quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Teleportation, introduced in science fiction literature, is an instantaneous\nchange of the position of a microscopic object. Two teleportation-like\nphenomena were predicted by quantum mechanics: quantum teleportation and,\nrecently, quantum particle teleportation. The former is investigated\nexperimentally and has applications in quantum communication and computing.\n  Here, we introduced the third teleportation-like phenomenon - an apparent\nteleportation. It seems to be a natural consequence of elementary particles and\nantiparticles of the Standard Model being indistinguishable. We give an example\nof a process leading to the apparent teleportation within a toy model of\nboson-like particles. It utilizes the local transport of particles and\nantiparticles and the local creation and annihilation of particle-antiparticle\npairs. Furthermore, we suggest a method to observe the apparent teleportation\nin nucleus-nucleus collisions at properly selected collision energy. The method\nrequires the measurement of correlations between momenta of charm and anticharm\nhadrons in collisions with a single $c\\bar{c}$ pair being produced. The\nultimate prediction following the apparent teleportation hypothesis is the\nuncorrelated emission of charm and anticharm hadrons. It can be tested by\ncontemporary experiments.\n  Observing the apparent teleportation would uncover the basic transport\nproperties of indistinguishable particles. In particular, the apparent\nteleportation may explain the rapid thermalisation of the system created in\ncollisions of two atomic nuclei. Theoretical and experimental efforts are\nneeded to observe the apparent teleportation processes and study their\nproperties.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:15:43 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Gazdzicki", "Marek", ""], ["Kikola", "Daniel", ""], ["Pidhurskyi", "Ivan", ""], ["Tinti", "Leonardo", ""]], "extracted_entities": [{"text": "Standard Model", "label": "Foundation Model"}, {"text": "rapid thermalisation", "label": "quantisation"}]}
{"id": "2503.10566", "submitter": "Egor Zverev", "authors": "Egor Zverev, Evgenii Kortukov, Alexander Panfilov, Soroush Tabesh,\n  Alexandra Volkova, Sebastian Lapuschkin, Wojciech Samek, Christoph H. Lampert", "title": "ASIDE: Architectural Separation of Instructions and Data in Language\n  Models", "comments": "ICLR 2025 Workshop on Building Trust in Language Models and\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite their remarkable performance, large language models lack elementary\nsafety features, and this makes them susceptible to numerous malicious attacks.\nIn particular, previous work has identified the absence of an intrinsic\nseparation between instructions and data as a root cause for the success of\nprompt injection attacks. In this work, we propose an architectural change,\nASIDE, that allows the model to clearly separate between instructions and data\nby using separate embeddings for them. Instead of training the embeddings from\nscratch, we propose a method to convert an existing model to ASIDE form by\nusing two copies of the original model's embeddings layer, and applying an\northogonal rotation to one of them. We demonstrate the effectiveness of our\nmethod by showing (1) highly increased instruction-data separation scores\nwithout a loss in model capabilities and (2) competitive results on prompt\ninjection benchmarks, even without dedicated safety training. Additionally, we\nstudy the working mechanism behind our method through an analysis of model\nrepresentations.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:17:17 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zverev", "Egor", ""], ["Kortukov", "Evgenii", ""], ["Panfilov", "Alexander", ""], ["Tabesh", "Soroush", ""], ["Volkova", "Alexandra", ""], ["Lapuschkin", "Sebastian", ""], ["Samek", "Wojciech", ""], ["Lampert", "Christoph H.", ""]], "extracted_entities": [{"text": "embeddings", "label": "Embedding"}, {"text": "embeddings", "label": "Embedding"}, {"text": "embeddings", "label": "Embedding"}]}
{"id": "2503.10567", "submitter": "Nannan Wu", "authors": "Nannan Wu, Zengqiang Yan, Nong Sang, Li Yu, Chang Wen Chen", "title": "FedPCA: Noise-Robust Fair Federated Learning via Performance-Capacity\n  Analysis", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a model that effectively handles both common and rare data-i.e.,\nachieving performance fairness-is crucial in federated learning (FL). While\nexisting fair FL methods have shown effectiveness, they remain vulnerable to\nmislabeled data. Ensuring robustness in fair FL is therefore essential.\nHowever, fairness and robustness inherently compete, which causes robust\nstrategies to hinder fairness. In this paper, we attribute this competition to\nthe homogeneity in loss patterns exhibited by rare and mislabeled data clients,\npreventing existing loss-based fair and robust FL methods from effectively\ndistinguishing and handling these two distinct client types. To address this,\nwe propose performance-capacity analysis, which jointly considers model\nperformance on each client and its capacity to handle the dataset, measured by\nloss and a newly introduced feature dispersion score. This allows mislabeled\nclients to be identified by their significantly deviated performance relative\nto capacity while preserving rare data clients. Building on this, we introduce\nFedPCA, an FL method that robustly achieves fairness. FedPCA first identifies\nmislabeled clients via a Gaussian Mixture Model on loss-dispersion pairs, then\napplies fairness and robustness strategies in global aggregation and local\ntraining by adjusting client weights and selectively using reliable data.\nExtensive experiments on three datasets demonstrate FedPCA's effectiveness in\ntackling this complex challenge. Code will be publicly available upon\nacceptance.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:18:18 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wu", "Nannan", ""], ["Yan", "Zengqiang", ""], ["Sang", "Nong", ""], ["Yu", "Li", ""], ["Chen", "Chang Wen", ""]], "extracted_entities": [{"text": "fairness-is", "label": "Model Bias and Fairness"}, {"text": "federated learning", "label": "Few-shot Learning"}, {"text": "FL", "label": "Few-shot Learning"}, {"text": "fairness", "label": "Model Bias and Fairness"}, {"text": "fairness", "label": "Model Bias and Fairness"}, {"text": "fairness", "label": "Model Bias and Fairness"}, {"text": "fairness", "label": "Model Bias and Fairness"}]}
{"id": "2503.10568", "submitter": "Haopeng Li", "authors": "Haopeng Li, Jinyue Yang, Guoqi Li, Huan Wang", "title": "Autoregressive Image Generation with Randomized Parallel Decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:19:51 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Haopeng", ""], ["Yang", "Jinyue", ""], ["Li", "Guoqi", ""], ["Wang", "Huan", ""]], "extracted_entities": [{"text": "causal attention mechanism", "label": "Attention mechanism"}]}
{"id": "2503.10569", "submitter": "Mingzhou Yin", "authors": "Mingzhou Yin, Matthias A. M\\\"uller", "title": "Low-Rank Matrix Regression via Least-Angle Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix regression is a fundamental problem in data science with\nvarious applications in systems and control. Nuclear norm regularization has\nbeen widely applied to solve this problem due to its convexity. However, it\nsuffers from high computational complexity and the inability to directly\nspecify the rank. This work introduces a novel framework for low-rank matrix\nregression that addresses both unstructured and Hankel matrices. By decomposing\nthe low-rank matrix into rank-1 bases, the problem is reformulated as an\ninfinite-dimensional sparse learning problem. The least-angle regression (LAR)\nalgorithm is then employed to solve this problem efficiently. For unstructured\nmatrices, a closed-form LAR solution is derived with equivalence to a\nnormalized nuclear norm regularization problem. For Hankel matrices, a\nreal-valued polynomial basis reformulation enables effective LAR\nimplementation. Two numerical examples in network modeling and system\nrealization demonstrate that the proposed approach significantly outperforms\nthe nuclear norm method in terms of estimation accuracy and computational\nefficiency.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:21:11 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Yin", "Mingzhou", ""], ["M\u00fcller", "Matthias A.", ""]], "extracted_entities": [{"text": "infinite-dimensional sparse learning problem", "label": "Few-shot Learning"}]}
{"id": "2503.10570", "submitter": "Joshua Lackman", "authors": "Joshua Lackman", "title": "A Simple Description of the Hyperk\\\"{a}hler Structure of the Cotangent\n  Bundle of Projective Space via Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.SG hep-th math-ph math.AG math.MP math.QA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization identifies the cotangent bundle of projective space with the\n(non-Hermitian) rank-$1$ projections of a Hilbert space. We use this\nidentification to study the natural geometric structures of these cotangent\nbundles and those of Grassmanians. In particular, we show that the quantization\nmap is an isometric and complex embedding\n$T^*\\mathbb{P}\\mathcal{H}\\hookrightarrow\\mathcal{B}(\\mathcal{H})\\backslash\\{0\\}.$\nHere, the metric on the domain is the hyperk\\\"{a}hler metric and the metric on\nthe codomain is the one whose K\\\"{a}hler potential is the Hilbert-Schmidt norm.\nThe K\\\"{a}hler potential pulled back to $T^*\\mathbb{P}\\mathcal{H}$ equals the\ntrace-class norm. Using this, we give a complete, simple and explicit\ndescription of the hyperk\\\"{a}hler structure. Our constructions are functorial,\ncoordinate-free and reduction-free.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:23:09 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Lackman", "Joshua", ""]], "extracted_entities": [{"text": "Quantization", "label": "quantisation"}]}
{"id": "2503.10571", "submitter": "Yongchang Hao", "authors": "Yongchang Hao, Mengyao Zhai, Hossein Hajimirsadeghi, Sepidehsadat\n  Hosseini, Frederick Tung", "title": "Radar: Fast Long-Context Decoding for Any Transformer", "comments": "Accepted @ ICLR 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Transformer models have demonstrated exceptional performance across a wide\nrange of applications. Though forming the foundation of Transformer models, the\ndot-product attention does not scale well to long-context data since its time\nrequirement grows quadratically with context length. In this work, we propose\nRadar, a training-free approach that accelerates inference by dynamically\nsearching for the most important context tokens. For any pre-trained\nTransformer, Radar can reduce the decoding time complexity without training or\nheuristically evicting tokens. Moreover, we provide theoretical justification\nfor our approach, demonstrating that Radar can reliably identify the most\nimportant tokens with high probability. We conduct extensive comparisons with\nthe previous methods on a wide range of tasks. The results demonstrate that\nRadar achieves the state-of-the-art performance across different architectures\nwith reduced time complexity, offering a practical solution for efficient\nlong-context processing of Transformers.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:23:10 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Hao", "Yongchang", ""], ["Zhai", "Mengyao", ""], ["Hajimirsadeghi", "Hossein", ""], ["Hosseini", "Sepidehsadat", ""], ["Tung", "Frederick", ""]], "extracted_entities": [{"text": "Transformer models", "label": "Foundation Model"}, {"text": "dot-product attention", "label": "Attention mechanism"}, {"text": "Radar", "label": "Transformers"}, {"text": "Transformers", "label": "Transformers"}]}
{"id": "2503.10574", "submitter": "Naomi Sagan", "authors": "Naomi Sagan, Amir Dembo, Tsachy Weissman", "title": "The LZ78 Source", "comments": "33 pages, 7 figures, submitted to IEEE Transactions on Information\n  Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study a family of processes generated according to sequential probability\nassignments induced by the LZ78 universal compressor. We characterize entropic\nand distributional properties such as their entropy and relative entropy rates,\nfinite-state compressibility and log loss of their realizations, and the\nempirical distributions that they induce. Though not quite stationary, these\nsources are \"almost stationary and ergodic;\" similar to stationary and ergodic\nprocesses, they satisfy a Shannon-McMillan-Breiman-type property: the\nnormalized log probability of their realizations converges almost surely to\ntheir entropy rate. Further, they are locally \"almost i.i.d.\" in the sense that\nthe finite-dimensional empirical distributions of their realizations converge\nalmost surely to a deterministic i.i.d. law. However, unlike stationary ergodic\nsources, the finite-state compressibility of their realizations is almost\nsurely strictly larger than their entropy rate by a \"Jensen gap.\" We present\nsimulations demonstrating the theoretical results. Among their potential uses,\nthese sources allow to gauge the performance of sequential probability models\non non-Markovian non-stationary data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:24:43 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Sagan", "Naomi", ""], ["Dembo", "Amir", ""], ["Weissman", "Tsachy", ""]], "extracted_entities": [{"text": "deterministic i.i.d. law", "label": "Scaling law"}]}
{"id": "2503.10579", "submitter": "Chaoqun Wang", "authors": "Chaoqun Wang, Xiaobin Hong, Wenzhong Li, and Ruimao Zhang", "title": "Semantic-Supervised Spatial-Temporal Fusion for LiDAR-based 3D Object\n  Detection", "comments": "Accepted by ICRA2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  LiDAR-based 3D object detection presents significant challenges due to the\ninherent sparsity of LiDAR points. A common solution involves long-term\ntemporal LiDAR data to densify the inputs. However, efficiently leveraging\nspatial-temporal information remains an open problem. In this paper, we propose\na novel Semantic-Supervised Spatial-Temporal Fusion (ST-Fusion) method, which\nintroduces a novel fusion module to relieve the spatial misalignment caused by\nthe object motion over time and a feature-level semantic supervision to\nsufficiently unlock the capacity of the proposed fusion module. Specifically,\nthe ST-Fusion consists of a Spatial Aggregation (SA) module and a Temporal\nMerging (TM) module. The SA module employs a convolutional layer with\nprogressively expanding receptive fields to aggregate the object features from\nthe local regions to alleviate the spatial misalignment, the TM module\ndynamically extracts object features from the preceding frames based on the\nattention mechanism for a comprehensive sequential presentation. Besides, in\nthe semantic supervision, we propose a Semantic Injection method to enrich the\nsparse LiDAR data via injecting the point-wise semantic labels, using it for\ntraining a teacher model and providing a reconstruction target at the feature\nlevel supervised by the proposed object-aware loss. Extensive experiments on\nvarious LiDAR-based detectors demonstrate the effectiveness and universality of\nour proposal, yielding an improvement of approximately +2.8% in NDS based on\nthe nuScenes benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:30:20 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Chaoqun", ""], ["Hong", "Xiaobin", ""], ["Li", "Wenzhong", ""], ["Zhang", "Ruimao", ""]], "extracted_entities": [{"text": "attention mechanism", "label": "Attention mechanism"}]}
{"id": "2503.10581", "submitter": "Tobias Binninger", "authors": "Tobias Binninger, Yin-Ying Ting, Konstantin K\\\"oster, Nils Bruch,\n  Payam Kaghazchi, Piotr M. Kowalski, Michael H. Eikerling", "title": "Simulating charging characteristics of lithium iron phosphate by\n  electro-ionic optimization on a quantum annealer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid evolution of quantum computing hardware opens up new avenues in the\nsimulation of energy materials. Today's quantum annealers are able to tackle\ncomplex combinatorial optimization problems. A formidable challenge of this\ntype is posed by materials with site-occupational disorder for which atomic\narrangements with a low, or lowest, energy must be found. In this article, a\nmethod is presented for the identification of the correlated ground-state\ndistribution of both lithium ions and redox electrons in lithium iron phosphate\n(LFP), a widely employed cathode material in lithium-ion batteries. The\npoint-charge Coulomb energy model employed correctly reproduces the LFP\ncharging characteristics. As is shown, grand-canonical transformation of the\nenergy cost function makes the combinatorial distribution problem solvable on\nquantum annealing (QA) hardware. The QA output statistics follow a\npseudo-thermal behavior characterized by a problem-dependent effective sampling\ntemperature, which has bearings on the estimated scaling of the QA performance\nwith system size. This work demonstrates the potential of quantum computation\nfor the joint optimization of electronic and ionic degrees of freedom in energy\nmaterials.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:32:03 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Binninger", "Tobias", ""], ["Ting", "Yin-Ying", ""], ["K\u00f6ster", "Konstantin", ""], ["Bruch", "Nils", ""], ["Kaghazchi", "Payam", ""], ["Kowalski", "Piotr M.", ""], ["Eikerling", "Michael H.", ""]], "extracted_entities": [{"text": "point-charge Coulomb energy model", "label": "AI model"}]}
{"id": "2503.10584", "submitter": "Jun-Kun Zhao", "authors": "Jun-Kun Zhao and Li Li", "title": "Holographic study of shear viscosity and butterfly velocity for magnetic\n  field-driven quantum criticality", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-th cond-mat.str-el", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the shear viscosity and butterfly velocity of a magnetic\nfield-induced quantum phase transition in five dimensional\nEinstein-Maxwell-Chern-Simons theory, which is holographically dual to a class\nof strongly coupled quantum field theories with chiral anomalies. Our analysis\nreveals that the ratio of longitudinal shear viscosity to entropy density\n$\\eta_\\parallel/s$ exhibits a pronounced non-monotonic dependence on\ntemperature $T$ when the magnetic field $B$ is slightly below the critical\nvalue $B_c$ of the quantum phase transition. In particular, it can develop a\ndistinct minimum at an intermediate temperature. This contrasts sharply with\nthe monotonic temperature scaling observed at and above $B_c$, where\n$\\eta_\\parallel/s$ follows the scaling $T^{2/3}$ at $B=B_c$ and transitions to\n$T^2$ for $B>B_c$ as $T\\to0$. The non-vanishing of $\\eta_\\parallel/s$ for\n$B<B_c$ in the zero temperature limit suggests that it could serve as a good\norder parameter of the quantum phase transition. We also find that all\nbutterfly velocities change dramatically near the quantum phase transition, and\nthus their derivatives with respect to $B$ can be independently used to detect\nthe quantum critical point.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:34:44 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhao", "Jun-Kun", ""], ["Li", "Li", ""]], "extracted_entities": [{"text": "monotonic temperature scaling", "label": "Scaling law"}]}
{"id": "2503.10586", "submitter": "Chaoqun Wang", "authors": "Chaoqun Wang, Jie Yang, Xiaobin Hong, and Ruimao Zhang", "title": "Unlock the Power of Unlabeled Data in Language Driving Model", "comments": "Accepted by ICRA2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent Vision-based Large Language Models~(VisionLLMs) for autonomous driving\nhave seen rapid advancements. However, such promotion is extremely dependent on\nlarge-scale high-quality annotated data, which is costly and labor-intensive.\nTo address this issue, we propose unlocking the value of abundant yet unlabeled\ndata to improve the language-driving model in a semi-supervised learning\nmanner. Specifically, we first introduce a series of template-based prompts to\nextract scene information, generating questions that create pseudo-answers for\nthe unlabeled data based on a model trained with limited labeled data. Next, we\npropose a Self-Consistency Refinement method to improve the quality of these\npseudo-annotations, which are later used for further training. By utilizing a\npre-trained VisionLLM (e.g., InternVL), we build a strong Language Driving\nModel (LDM) for driving scene question-answering, outperforming previous\nstate-of-the-art methods. Extensive experiments on the DriveLM benchmark show\nthat our approach performs well with just 5% labeled data, achieving\ncompetitive performance against models trained with full datasets. In\nparticular, our LDM achieves 44.85% performance with limited labeled data,\nincreasing to 54.27% when using unlabeled data, while models trained with full\ndatasets reach 60.68% on the DriveLM benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:36:36 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Wang", "Chaoqun", ""], ["Yang", "Jie", ""], ["Hong", "Xiaobin", ""], ["Zhang", "Ruimao", ""]], "extracted_entities": [{"text": "template-based prompts", "label": "Prompting"}]}
{"id": "2503.10587", "submitter": "Justin Sahs", "authors": "Justin Sahs, Ryan Pyle, Fabio Anselmi, Ankit Patel", "title": "The Spectral Bias of Shallow Neural Network Learning is Shaped by the\n  Choice of Non-linearity", "comments": "18 pages, 10 figures in main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite classical statistical theory predicting severe overfitting, modern\nmassively overparameterized neural networks still generalize well. This\nunexpected property is attributed to the network's so-called implicit bias,\nwhich describes its propensity to converge to solutions that generalize\neffectively, among the many possible that correctly label the training data.\nThe aim of our research is to explore this bias from a new perspective,\nfocusing on how non-linear activation functions contribute to shaping it.\nFirst, we introduce a reparameterization which removes a continuous weight\nrescaling symmetry. Second, in the kernel regime, we leverage this\nreparameterization to generalize recent findings that relate shallow Neural\nNetworks to the Radon transform, deriving an explicit formula for the implicit\nbias induced by a broad class of activation functions. Specifically, by\nutilizing the connection between the Radon transform and the Fourier transform,\nwe interpret the kernel regime's inductive bias as minimizing a spectral\nseminorm that penalizes high-frequency components, in a manner dependent on the\nactivation function. Finally, in the adaptive regime, we demonstrate the\nexistence of local dynamical attractors that facilitate the formation of\nclusters of hyperplanes where the input to a neuron's activation function is\nzero, yielding alignment between many neurons' response functions. We confirm\nthese theoretical results with simulations. All together, our work provides a\ndeeper understanding of the mechanisms underlying the generalization\ncapabilities of overparameterized neural networks and its relation with the\nimplicit bias, offering potential pathways for designing more efficient and\nrobust models.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:36:46 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Sahs", "Justin", ""], ["Pyle", "Ryan", ""], ["Anselmi", "Fabio", ""], ["Patel", "Ankit", ""]], "extracted_entities": [{"text": "implicit bias", "label": "Model Bias and Fairness"}, {"text": "continuous weight\nrescaling symmetry", "label": "Scaling law"}, {"text": "Radon transform", "label": "BERT"}, {"text": "implicit\nbias", "label": "Model Bias and Fairness"}, {"text": "Radon transform", "label": "BERT"}, {"text": "implicit bias", "label": "Model Bias and Fairness"}]}
{"id": "2503.10589", "submitter": "Yuwei Guo", "authors": "Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng\n  Yang, Dahua Lin, Lu Jiang", "title": "Long Context Tuning for Video Generation", "comments": "Project Page: https://guoyww.github.io/projects/long-context-video/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:40:07 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Guo", "Yuwei", ""], ["Yang", "Ceyuan", ""], ["Yang", "Ziyan", ""], ["Ma", "Zhibei", ""], ["Lin", "Zhijie", ""], ["Yang", "Zhenheng", ""], ["Lin", "Dahua", ""], ["Jiang", "Lu", ""]], "extracted_entities": [{"text": "Long Context Tuning", "label": "Fine-tuning"}, {"text": "full attention mechanisms", "label": "Attention mechanism"}, {"text": "interleaved 3D position embedding", "label": "contextual Embedding"}, {"text": "bidirectional attention", "label": "Attention mechanism"}, {"text": "context-causal attention", "label": "Attention mechanism"}]}
{"id": "2503.10596", "submitter": "Tianheng Cheng", "authors": "Rui Hu, Lianghui Zhu, Yuxuan Zhang, Tianheng Cheng, Lei Liu, Heng Liu,\n  Longjin Ran, Xiaoxin Chen, Wenyu Liu, Xinggang Wang", "title": "GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding", "comments": "Work in progress. Code: https://github.com/hustvl/GroundingSuite", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pixel grounding, encompassing tasks such as Referring Expression Segmentation\n(RES), has garnered considerable attention due to its immense potential for\nbridging the gap between vision and language modalities. However, advancements\nin this domain are currently constrained by limitations inherent in existing\ndatasets, including limited object categories, insufficient textual diversity,\nand a scarcity of high-quality annotations. To mitigate these limitations, we\nintroduce GroundingSuite, which comprises: (1) an automated data annotation\nframework leveraging multiple Vision-Language Model (VLM) agents; (2) a\nlarge-scale training dataset encompassing 9.56 million diverse referring\nexpressions and their corresponding segmentations; and (3) a meticulously\ncurated evaluation benchmark consisting of 3,800 images. The GroundingSuite\ntraining dataset facilitates substantial performance improvements, enabling\nmodels trained on it to achieve state-of-the-art results. Specifically, a cIoU\nof 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the\nGroundingSuite annotation framework demonstrates superior efficiency compared\nto the current leading data annotation method, i.e., $4.5 \\times$ faster than\nthe GLaMM.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:43:10 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Hu", "Rui", ""], ["Zhu", "Lianghui", ""], ["Zhang", "Yuxuan", ""], ["Cheng", "Tianheng", ""], ["Liu", "Lei", ""], ["Liu", "Heng", ""], ["Ran", "Longjin", ""], ["Chen", "Xiaoxin", ""], ["Liu", "Wenyu", ""], ["Wang", "Xinggang", ""]], "extracted_entities": [{"text": "Pixel grounding", "label": "Embedding"}]}
{"id": "2503.10597", "submitter": "Yang Zheng", "authors": "Yang Zheng, Menglei Chai, Delio Vicini, Yuxiao Zhou, Yinghao Xu,\n  Leonidas Guibas, Gordon Wetzstein, Thabo Beeler", "title": "GroomLight: Hybrid Inverse Rendering for Relightable Human Hair\n  Appearance Modeling", "comments": "Project Page: https://syntec-research.github.io/GroomLight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GroomLight, a novel method for relightable hair appearance\nmodeling from multi-view images. Existing hair capture methods struggle to\nbalance photorealistic rendering with relighting capabilities. Analytical\nmaterial models, while physically grounded, often fail to fully capture\nappearance details. Conversely, neural rendering approaches excel at view\nsynthesis but generalize poorly to novel lighting conditions. GroomLight\naddresses this challenge by combining the strengths of both paradigms. It\nemploys an extended hair BSDF model to capture primary light transport and a\nlight-aware residual model to reconstruct the remaining details. We further\npropose a hybrid inverse rendering pipeline to optimize both components,\nenabling high-fidelity relighting, view synthesis, and material editing.\nExtensive evaluations on real-world hair data demonstrate state-of-the-art\nperformance of our method.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:43:12 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zheng", "Yang", ""], ["Chai", "Menglei", ""], ["Vicini", "Delio", ""], ["Zhou", "Yuxiao", ""], ["Xu", "Yinghao", ""], ["Guibas", "Leonidas", ""], ["Wetzstein", "Gordon", ""], ["Beeler", "Thabo", ""]], "extracted_entities": [{"text": "extended hair BSDF model", "label": "Neural Language Model"}, {"text": "light-aware residual model", "label": "Neural Language Model"}]}
{"id": "2503.10601", "submitter": "Manuel Rispler", "authors": "Berat Yenilen and Arnau Sala and Hendrik Bluhm and Markus M\\\"uller and\n  Manuel Rispler", "title": "Performance of the spin qubit shuttling architecture for a surface code\n  implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Qubit shuttling promises to advance some quantum computing platforms to the\nqubit register sizes needed for effective quantum error correction (QEC), but\nalso introduces additional errors whose impact must be evaluated. The\nestablished method to investigate the performance of QEC codes in a realistic\nscenario is to employ a standard noise model known as circuit-level noise,\nwhere all quantum operations are modeled as noisy. In the present work, we take\nthis noise model and single out the effect of shuttling errors by introducing\nthem as an additional so-called error location. This hardware abstraction is\nmotivated by the SpinBus architecture and allows a systematic numerical\ninvestigation to map out the resulting two-dimensional parameter space. To this\nend, we take the Surface code and perform large scale simulations, most notably\nextracting the threshold across said two-dimensional parameter space. We study\ntwo scenarios for shuttling errors, depolarization on the one hand and\ndephasing on the other hand. For a purely dephasing shuttling error, we find a\nthreshold of several percent, provided that all other operations have a high\nfidelity. The qubit overhead needed to reach a logical error rate of $10^{-12}$\n(known as the \"teraquop\" regime~\\cite{Gidney2021Jul}) increases only moderately\nfor shuttling error rates up to about 1 \\% per shuttling operation. The error\nrates at which practically useful, i.e. well below threshold error correction\nis predicted to be possible are comfortably higher than what is expected to be\nachievable for spin qubits. Our results thus show that it is reasonable to\nexpect shuttling operations to fall below threshold already at surprisingly\nlarge error rates. With realistic efforts in the near term, this offers\npositive prospects for spin qubit based quantum processors as a viable avenue\nfor scalable fault-tolerant error-corrected quantum computing.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:46:02 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Yenilen", "Berat", ""], ["Sala", "Arnau", ""], ["Bluhm", "Hendrik", ""], ["M\u00fcller", "Markus", ""], ["Rispler", "Manuel", ""]], "extracted_entities": [{"text": "depolarization", "label": "quantisation"}]}
{"id": "2503.10602", "submitter": "Jinhao Duan", "authors": "Jinhao Duan, Fei Kong, Hao Cheng, James Diffenderfer, Bhavya\n  Kailkhura, Lichao Sun, Xiaofeng Zhu, Xiaoshuang Shi, Kaidi Xu", "title": "TruthPrInt: Mitigating LVLM Object Hallucination Via Latent\n  Truthful-Guided Pre-Intervention", "comments": "15 pages, 9 figures, the first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object Hallucination (OH) has been acknowledged as one of the major\ntrustworthy challenges in Large Vision-Language Models (LVLMs). Recent\nadvancements in Large Language Models (LLMs) indicate that internal states,\nsuch as hidden states, encode the \"overall truthfulness\" of generated\nresponses. However, it remains under-explored how internal states in LVLMs\nfunction and whether they could serve as \"per-token\" hallucination indicators,\nwhich is essential for mitigating OH. In this paper, we first conduct an\nin-depth exploration of LVLM internal states in relation to OH issues and\ndiscover that (1) LVLM internal states are high-specificity per-token\nindicators of hallucination behaviors. Moreover, (2) different LVLMs encode\nuniversal patterns of hallucinations in common latent subspaces, indicating\nthat there exist \"generic truthful directions\" shared by various LVLMs. Based\non these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt)\nthat first learns the truthful direction of LVLM decoding and then applies\ntruthful-guided inference-time intervention during LVLM decoding. We further\npropose ComnHallu to enhance both cross-LVLM and cross-data hallucination\ndetection transferability by constructing and aligning hallucination latent\nsubspaces. We evaluate TruthPrInt in extensive experimental settings, including\nin-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks.\nExperimental results indicate that TruthPrInt significantly outperforms\nstate-of-the-art methods. Codes will be available at\nhttps://github.com/jinhaoduan/TruthPrInt.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:46:06 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Duan", "Jinhao", ""], ["Kong", "Fei", ""], ["Cheng", "Hao", ""], ["Diffenderfer", "James", ""], ["Kailkhura", "Bhavya", ""], ["Sun", "Lichao", ""], ["Zhu", "Xiaofeng", ""], ["Shi", "Xiaoshuang", ""], ["Xu", "Kaidi", ""]], "extracted_entities": [{"text": "Large Vision-Language Models", "label": "Large Language Model"}, {"text": "LVLMs", "label": "Large Language Model"}, {"text": "Large Language Models", "label": "Large Language Model"}, {"text": "LVLMs", "label": "Large Language Model"}, {"text": "LVLMs", "label": "Large Language Model"}, {"text": "LVLMs", "label": "Large Language Model"}, {"text": "ComnHallu", "label": "Llama"}, {"text": "LVLMs", "label": "Large Language Model"}]}
{"id": "2503.10604", "submitter": "Yingshuang Zou", "authors": "Yingshuang Zou, Yikang Ding, Chuanrui Zhang, Jiazhe Guo, Bohan Li,\n  Xiaoyang Lyu, Feiyang Tan, Xiaojuan Qi, Haoqian Wang", "title": "MuDG: Taming Multi-modal Diffusion with Gaussian Splatting for Urban\n  Scene Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthroughs in radiance fields have significantly advanced 3D scene\nreconstruction and novel view synthesis (NVS) in autonomous driving.\nNevertheless, critical limitations persist: reconstruction-based methods\nexhibit substantial performance deterioration under significant viewpoint\ndeviations from training trajectories, while generation-based techniques\nstruggle with temporal coherence and precise scene controllability. To overcome\nthese challenges, we present MuDG, an innovative framework that integrates\nMulti-modal Diffusion model with Gaussian Splatting (GS) for Urban Scene\nReconstruction. MuDG leverages aggregated LiDAR point clouds with RGB and\ngeometric priors to condition a multi-modal video diffusion model, synthesizing\nphotorealistic RGB, depth, and semantic outputs for novel viewpoints. This\nsynthesis pipeline enables feed-forward NVS without computationally intensive\nper-scene optimization, providing comprehensive supervision signals to refine\n3DGS representations for rendering robustness enhancement under extreme\nviewpoint changes. Experiments on the Open Waymo Dataset demonstrate that MuDG\noutperforms existing methods in both reconstruction and synthesis quality.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:48:41 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zou", "Yingshuang", ""], ["Ding", "Yikang", ""], ["Zhang", "Chuanrui", ""], ["Guo", "Jiazhe", ""], ["Li", "Bohan", ""], ["Lyu", "Xiaoyang", ""], ["Tan", "Feiyang", ""], ["Qi", "Xiaojuan", ""], ["Wang", "Haoqian", ""]], "extracted_entities": [{"text": "Multi-modal Diffusion model", "label": "AI model"}, {"text": "Open Waymo Dataset", "label": "Open-source LLMs"}]}
{"id": "2503.10607", "submitter": "Brittany Richman", "authors": "Brittany Richman, C. J. Lobb, and Jacob M. Taylor", "title": "Utilizing discrete variable representations for decoherence-accurate\n  numerical simulation of superconducting circuits", "comments": "26 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the prevalence of superconducting platforms for uses in quantum\ncomputing and quantum sensing, the simulation of quantum superconducting\ncircuits has become increasingly important for identifying system\ncharacteristics and modeling their relevant dynamics. Various numerical tools\nand software packages have been developed with this purpose in mind, typically\nutilizing the harmonic oscillator basis or the charge basis to represent a\nHamiltonian. In this work, we instead consider the use of discrete variable\nrepresentations (DVRs) to model superconducting circuits. In particular, we use\n`sinc DVRs' of both charge number and phase to approximate the eigenenergies of\nseveral prototypical examples, exploring their use and effectiveness in the\nnumerical analysis of superconducting circuits. We find that not only are these\nDVRs capable of achieving decoherence-accurate simulation, i.e., accuracy at\nthe resolution of experiments subject to decay, decoherence, and dephasing,\nthey also demonstrate improvements in efficiency with smaller basis sizes and\nbetter convergence over standard approaches, showing that DVRs are an\nadvantageous alternative for representing superconducting circuits.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:52:42 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Richman", "Brittany", ""], ["Lobb", "C. J.", ""], ["Taylor", "Jacob M.", ""]], "extracted_entities": [{"text": "discrete variable\nrepresentations", "label": "LLMs"}, {"text": "DVRs", "label": "LLMs"}, {"text": "DVRs", "label": "LLMs"}, {"text": "DVRs", "label": "LLMs"}, {"text": "DVRs", "label": "LLMs"}]}
{"id": "2503.10609", "submitter": "Antonio Raffaelli", "authors": "Antonio Raffaelli, Mario Ballardini", "title": "Knot reconstruction of the scalar primordial power spectrum with Planck,\n  ACT, and SPT CMB data", "comments": "20 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate a non-parametric Bayesian method for reconstructing the\nprimordial power spectrum (PPS) of scalar perturbations using temperature and\npolarisation data from the {\\em Planck}, ACT, and SPT CMB experiments. This\nreconstruction method is based on linear splines for the PPS between nodes in\n$k$-space whose amplitudes and positions are allowed to vary. All three data\nsets consistently show no significant deviations from a power-law form in the\nrange $0.005 \\lesssim k\\,\\mathrm{Mpc} \\lesssim 0.16$ independent of the number\nof knots adopted to perform the reconstruction. The addition of high-resolution\nCMB measurements from ACT and SPT slightly improves the range of scales of the\nscalar PPS which are well constrained around a power law up to $k \\simeq\n0.25\\,\\mathrm{Mpc}^{-1}$ and $k \\simeq 0.2\\,\\mathrm{Mpc}^{-1}$, respectively.\nAt large scales, a potential oscillatory feature in the primordial power\nspectrum appears when we consider six or more nodes. We test the robustness of\nthe methodology and our results by varying the detailed number of knots from\n$N=2$ to $N=10$. We have used the reconstructed scalar PPS to derive several\nquantities related to inflationary related to inflationary dynamics, such as\nthe effective scalar spectral index, which describes the dependence of the PPS\non the scales and parameters associated with the effective field theory of\ninflation, to provide information on possible departures from the standard\nsingle-field canonical case. Finally, we investigate whether the excess of\nsmoothing in the region of the region of the acoustic peaks of the CMB\nanisotropy temperature power spectrum in the \\textit{Planck} PR3 data is\ndegenerate with our reconstructions of the PPS, but find no significant\ncorrelation between them.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:53:07 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Raffaelli", "Antonio", ""], ["Ballardini", "Mario", ""]], "extracted_entities": [{"text": "power law", "label": "Scaling law"}]}
{"id": "2503.10610", "submitter": "Javier A. Acevedo Barroso", "authors": "J. A. Acevedo Barroso (1), B. Cl\\'ement (1 and 2), F. Courbin (1 and 3\n  and 4), R. Gavazzi (5 and 6), C. Lemon (1 and 7), K. Rojas (8 and 9), D.\n  Scott (10), S. Gwyn (11), F. Hammer (12), M. J. Hudson (13 and 14 and 15), E.\n  A. Magnier (16) ((1) Institute of Physics, Laboratory of Astrophysics, Ecole\n  Polytechnique F\\'ed\\'erale de Lausanne (EPFL), Observatoire de Sauverny, 1290\n  Versoix, Switzerland, (2) SCITAS, Ecole Polytechnique F\\'ed\\'erale de\n  Lausanne (EPFL), 1015 Lausanne, Switzerland, (3) Institut de Ci\\`encies del\n  Cosmos (ICCUB), Universitat de Barcelona (IEEC-UB), Mart\\'i i Franqu\\`es 1,\n  08028 Barcelona, Spain, (4) Instituci\\'o Catalana de Recerca i Estudis\n  Avan\\c{c}ats (ICREA), Passeig de Llu\\'is Companys 23, 08010 Barcelona, Spain,\n  (5) Laboratoire d'Astrophysique de Marseille, UMR7326, Aix-Marseille\n  Universit\\'e, CNRS, CNES, 13013 Marseille, France, (6) Institut\n  d'Astrophysique de Paris, UMR 7095, CNRS, and Sorbonne Universit\\'e, 98 bis\n  boulevard Arago, 75014 Paris, France, (7) Oskar Klein Centre, Department of\n  Physics, Stockholm University, SE-106 91 Stockholm, Sweden, (8) University of\n  Applied Sciences and Arts of Northwestern Switzerland, School of Engineering,\n  5210 Windisch, Switzerland, (9) Institute of Cosmology and Gravitation,\n  University of Portsmouth, Burnaby Rd, Portsmouth PO1 3FX, UK, (10) Department\n  of Physics and Astronomy, University of British Columbia, Vancouver, BC V6T\n  1Z1, Canada, (11) Canadian Astronomy Data Centre, Herzberg Astronomy and\n  Astrophysics, National Research Council, 5071 West Saanich Rd Victoria BC V9E\n  2E7, (12) LIRA, Observatoire de Paris, Universit\\'e PSL, CNRS, Place Jules\n  Janssen 92195, Meudon, France, (13) Department of Physics and Astronomy,\n  University of Waterloo, Waterloo, ON, N2L 3G1, Canada, (14) Waterloo Centre\n  for Astrophysics, Waterloo, ON, N2L 3G1, Canada, (15) Perimeter Institute for\n  Theoretical Physics, 31 Caroline St. N., Waterloo, ON, N2L 2Y5, Canada, (16)\n  Institute for Astronomy, University of Hawaii, 2680 Woodlawn Drive, Honolulu,\n  HI 96822, USA)", "title": "Searching for strong lensing by late-type galaxies in UNIONS", "comments": "22 pages, 10 figures, submitted to A&A", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.GA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent wide-field galaxy surveys have led to an explosion in numbers of\ngalaxy-scale strong gravitational lens candidates. However, the vast majority\nfeature massive luminous red galaxies as the main deflectors, with late-type\ngalaxies being vastly under-represented. This work presents a dedicated search\nfor lensing by edge-on late-type galaxies in the Ultraviolet Near Infrared\nOptical Northern Survey (UNIONS). The search covers $3600$ deg$^2$ of $r$-band\nobservations taken from the Canada-France-Hawaii Telescope. We consider all\nsources with magnitudes in the range $17 < r < 20.5$, without any colour\npreselection, yielding a parent sample of seven million sources. We\ncharacterise our parent sample via the visual inspection of $120\\,000$ sources\nselected at random. From it, we estimate, with a 68\\% confidence interval, that\n1 in every $30\\,000$ sources is an edge-on lens candidate, with at least eight\nhigh-quality candidates in the parent sample. This corresponds to 1 candidate\nper $17\\,000$ edge-on late-type galaxies. Our search relies on a convolutional\nneural network (CNN) to select a reduced sample of candidates, followed by a\nvisual inspection to curate the final sample. The CNN is trained from scratch\nusing simulated $r$-band observations of edge-on lenses, and real observations\nof non-lenses. We find 61 good edge-on lens candidates using the CNN. Moreover,\ncombining the CNN candidates with those found serendipitously, and those\nidentified while characterising the parent sample, we discovered 4 grade A, 20\ngrade B, and 58 grade C edge-on lens candidates; effectively doubling the known\nsample of these systems. We also discovered 16 grade A, 16 grade B, and 18\ngrade C lens candidates of other types. Finally, based on the characterisation\nof the parent sample, we estimate that our search found around 60\\% of the\nbright grade A and B edge-on lens candidates within the parent sample.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:53:27 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Barroso", "J. A. Acevedo", "", "1 and 2"], ["Cl\u00e9ment", "B.", "", "1 and 2"], ["Courbin", "F.", "", "1 and 3\n  and 4"], ["Gavazzi", "R.", "", "5 and 6"], ["Lemon", "C.", "", "1 and 7"], ["Rojas", "K.", "", "8 and 9"], ["Scott", "D.", "", "13 and 14 and 15"], ["Gwyn", "S.", "", "13 and 14 and 15"], ["Hammer", "F.", "", "13 and 14 and 15"], ["Hudson", "M. J.", "", "13 and 14 and 15"], ["Magnier", "E. A.", ""]], "extracted_entities": [{"text": "CNN", "label": "Neural Language Model"}, {"text": "CNN", "label": "Neural Language Model"}, {"text": "CNN", "label": "Neural Language Model"}]}
{"id": "2503.10613", "submitter": "Advait Gupta", "authors": "Advait Gupta, NandaKiran Velaga, Dang Nguyen, Tianyi Zhou", "title": "CoSTA$\\ast$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text-to-image models like stable diffusion and DALLE-3 still struggle with\nmulti-turn image editing. We decompose such a task as an agentic workflow\n(path) of tool use that addresses a sequence of subtasks by AI tools of varying\ncosts. Conventional search algorithms require expensive exploration to find\ntool paths. While large language models (LLMs) possess prior knowledge of\nsubtask planning, they may lack accurate estimations of capabilities and costs\nof tools to determine which to apply in each subtask. Can we combine the\nstrengths of both LLMs and graph search to find cost-efficient tool paths? We\npropose a three-stage approach \"CoSTA*\" that leverages LLMs to create a subtask\ntree, which helps prune a graph of AI tools for the given task, and then\nconducts A* search on the small subgraph to find a tool path. To better balance\nthe total cost and quality, CoSTA* combines both metrics of each tool on every\nsubtask to guide the A* search. Each subtask's output is then evaluated by a\nvision-language model (VLM), where a failure will trigger an update of the\ntool's cost and quality on the subtask. Hence, the A* search can recover from\nfailures quickly to explore other paths. Moreover, CoSTA* can automatically\nswitch between modalities across subtasks for a better cost-quality trade-off.\nWe build a novel benchmark of challenging multi-turn image editing, on which\nCoSTA* outperforms state-of-the-art image-editing models or agents in terms of\nboth cost and quality, and performs versatile trade-offs upon user preference.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:55:45 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Gupta", "Advait", ""], ["Velaga", "NandaKiran", ""], ["Nguyen", "Dang", ""], ["Zhou", "Tianyi", ""]], "extracted_entities": [{"text": "large language models", "label": "Large Language Model"}]}
{"id": "2503.10614", "submitter": "Bolin Chen", "authors": "Bolin Chen, Baoquan Zhao, Haoran Xie, Yi Cai, Qing Li, Xudong Mao", "title": "ConsisLoRA: Enhancing Content and Style Consistency for LoRA-based Style\n  Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer involves transferring the style from a reference image to the\ncontent of a target image. Recent advancements in LoRA-based (Low-Rank\nAdaptation) methods have shown promise in effectively capturing the style of a\nsingle image. However, these approaches still face significant challenges such\nas content inconsistency, style misalignment, and content leakage. In this\npaper, we comprehensively analyze the limitations of the standard diffusion\nparameterization, which learns to predict noise, in the context of style\ntransfer. To address these issues, we introduce ConsisLoRA, a LoRA-based method\nthat enhances both content and style consistency by optimizing the LoRA weights\nto predict the original image rather than noise. We also propose a two-step\ntraining strategy that decouples the learning of content and style from the\nreference image. To effectively capture both the global structure and local\ndetails of the content image, we introduce a stepwise loss transition strategy.\nAdditionally, we present an inference guidance method that enables continuous\ncontrol over content and style strengths during inference. Through both\nqualitative and quantitative evaluations, our method demonstrates significant\nimprovements in content and style consistency while effectively reducing\ncontent leakage.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:55:58 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chen", "Bolin", ""], ["Zhao", "Baoquan", ""], ["Xie", "Haoran", ""], ["Cai", "Yi", ""], ["Li", "Qing", ""], ["Mao", "Xudong", ""]], "extracted_entities": [{"text": "ConsisLoRA", "label": "LLM-based"}]}
{"id": "2503.10615", "submitter": "Yang Yi", "authors": "Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao\n  Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, Wei Chen", "title": "R1-Onevision: Advancing Generalized Multimodal Reasoning through\n  Cross-Modal Formalization", "comments": "Code and Model: https://github.com/Fancy-MLLM/R1-onevision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large Language Models have demonstrated remarkable reasoning capability in\ncomplex textual tasks. However, multimodal reasoning, which requires\nintegrating visual and textual information, remains a significant challenge.\nExisting visual-language models often struggle to effectively analyze and\nreason visual content, resulting in suboptimal performance on complex reasoning\ntasks. Moreover, the absence of comprehensive benchmarks hinders the accurate\nassessment of multimodal reasoning capabilities. In this paper, we introduce\nR1-Onevision, a multimodal reasoning model designed to bridge the gap between\nvisual perception and deep reasoning. To achieve this, we propose a cross-modal\nreasoning pipeline that transforms images into formal textural representations,\nenabling precise language-based reasoning. Leveraging this pipeline, we\nconstruct the R1-Onevision dataset which provides detailed, step-by-step\nmultimodal reasoning annotations across diverse domains. We further develop the\nR1-Onevision model through supervised fine-tuning and reinforcement learning to\ncultivate advanced reasoning and robust generalization abilities. To\ncomprehensively evaluate multimodal reasoning performance across different\ngrades, we introduce R1-Onevision-Bench, a benchmark aligned with human\neducational stages, covering exams from junior high school to university and\nbeyond. Experimental results show that R1-Onevision achieves state-of-the-art\nperformance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple\nchallenging multimodal reasoning benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:56:05 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Yang", "Yi", ""], ["He", "Xiaoxuan", ""], ["Pan", "Hongkun", ""], ["Jiang", "Xiyan", ""], ["Deng", "Yan", ""], ["Yang", "Xingtao", ""], ["Lu", "Haoyu", ""], ["Yin", "Dacheng", ""], ["Rao", "Fengyun", ""], ["Zhu", "Minfeng", ""], ["Zhang", "Bo", ""], ["Chen", "Wei", ""]], "extracted_entities": [{"text": "supervised fine-tuning", "label": "Fine-tuning"}, {"text": "reinforcement learning", "label": "Few-shot Learning"}]}
{"id": "2503.10617", "submitter": "Andy Zhou", "authors": "Andy Zhou", "title": "Compositional Subspace Representation Fine-tuning for Adaptive Large\n  Language Models", "comments": "Accepted to ICLR 2025 SCOPE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adapting large language models to multiple tasks can cause cross-skill\ninterference, where improvements for one skill degrade another. While methods\nsuch as LoRA impose orthogonality constraints at the weight level, they do not\nfully address interference in hidden-state representations. We propose\nCompositional Subspace Representation Fine-tuning (CS-ReFT), a novel\nrepresentation-based approach that learns multiple orthonormal subspace\ntransformations, each specializing in a distinct skill, and composes them via a\nlightweight router. By isolating these subspace edits in the hidden state,\nrather than weight matrices, CS-ReFT prevents cross-task conflicts more\neffectively. On the AlpacaEval benchmark, applying CS-ReFT to Llama-2-7B\nachieves a 93.94% win rate, surpassing GPT-3.5 Turbo (86.30%) while requiring\nonly 0.0098% of model parameters. These findings show that specialized\nrepresentation edits, composed via a simple router, significantly enhance\nmulti-task instruction following with minimal overhead.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:57:04 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhou", "Andy", ""]], "extracted_entities": [{"text": "Compositional Subspace Representation Fine-tuning", "label": "Fine-tuning"}, {"text": "CS-ReFT", "label": "Fine-tuning"}, {"text": "CS-ReFT", "label": "Fine-tuning"}]}
{"id": "2503.10618", "submitter": "Chen Chen", "authors": "Chen Chen, Rui Qian, Wenze Hu, Tsu-Jui Fu, Lezhi Li, Bowen Zhang, Alex\n  Schwing, Wei Liu, Yinfei Yang", "title": "DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture\n  Design in Text to Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we empirically study Diffusion Transformers (DiTs) for\ntext-to-image generation, focusing on architectural choices, text-conditioning\nstrategies, and training protocols. We evaluate a range of DiT-based\narchitectures--including PixArt-style and MMDiT variants--and compare them with\na standard DiT variant which directly processes concatenated text and noise\ninputs. Surprisingly, our findings reveal that the performance of standard DiT\nis comparable with those specialized models, while demonstrating superior\nparameter-efficiency, especially when scaled up. Leveraging the layer-wise\nparameter sharing strategy, we achieve a further reduction of 66% in model size\ncompared to an MMDiT architecture, with minimal performance impact. Building on\nan in-depth analysis of critical components such as text encoders and\nVariational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With\nsupervised and reward fine-tuning, DiT-Air achieves state-of-the-art\nperformance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly\ncompetitive, surpassing most existing models despite its compact size.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:57:25 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Chen", "Chen", ""], ["Qian", "Rui", ""], ["Hu", "Wenze", ""], ["Fu", "Tsu-Jui", ""], ["Li", "Lezhi", ""], ["Zhang", "Bowen", ""], ["Schwing", "Alex", ""], ["Liu", "Wei", ""], ["Yang", "Yinfei", ""]], "extracted_entities": [{"text": "Diffusion Transformers", "label": "Transformers"}, {"text": "PixArt-style", "label": "Transformers"}, {"text": "MMDiT variants", "label": "Transformers"}, {"text": "supervised and reward fine-tuning", "label": "Fine-tuning"}, {"text": "DiT-Air", "label": "Transformer-based model"}]}
{"id": "2503.10619", "submitter": "Andy Zhou", "authors": "Andy Zhou", "title": "Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with\n  Tree Search", "comments": "Accepted to ICLR 2025 Trustworthy LLM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Siege, a multi-turn adversarial framework that models the\ngradual erosion of Large Language Model (LLM) safety through a tree search\nperspective. Unlike single-turn jailbreaks that rely on one meticulously\nengineered prompt, Siege expands the conversation at each turn in a\nbreadth-first fashion, branching out multiple adversarial prompts that exploit\npartial compliance from previous responses. By tracking these incremental\npolicy leaks and re-injecting them into subsequent queries, Siege reveals how\nminor concessions can accumulate into fully disallowed outputs. Evaluations on\nthe JailbreakBench dataset show that Siege achieves a 100% success rate on\nGPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries\nthan baselines such as Crescendo or GOAT. This tree search methodology offers\nan in-depth view of how model safeguards degrade over successive dialogue\nturns, underscoring the urgency of robust multi-turn testing procedures for\nlanguage models.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:57:32 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhou", "Andy", ""]], "extracted_entities": [{"text": "adversarial prompts", "label": "Prompting"}, {"text": "GPT-3", "label": "GPT"}, {"text": "GPT-4", "label": "GPT"}]}
{"id": "2503.10620", "submitter": "Ben Peters", "authors": "Kshitij Ambilduke, Ben Peters, Sonal Sannigrahi, Anil Keshwani, Tsz\n  Kin Lam, Bruno Martins, Marcely Zanon Boito, Andr\\'e F.T. Martins", "title": "From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large language models (LLMs) have shown remarkable performance and\ngeneralization capabilities across multiple languages and tasks, making them\nvery attractive targets for multi-modality integration (e.g., images or\nspeech). In this work, we extend an existing LLM to the speech modality via\nspeech discretization and continued pre-training. In particular, we are\ninterested in multilingual LLMs, such as TOWER, as their pre-training setting\nallows us to treat discretized speech input as an additional translation\nlanguage. The resulting open-source model, SPIRE, is able to transcribe and\ntranslate English speech input while maintaining TOWER's original performance\non translation-related tasks, showcasing that discretized speech input\nintegration as an additional language is feasible during LLM adaptation. We\nmake our code and models available to the community.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:57:32 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Ambilduke", "Kshitij", ""], ["Peters", "Ben", ""], ["Sannigrahi", "Sonal", ""], ["Keshwani", "Anil", ""], ["Lam", "Tsz Kin", ""], ["Martins", "Bruno", ""], ["Boito", "Marcely Zanon", ""], ["Martins", "Andr\u00e9 F. T.", ""]], "extracted_entities": [{"text": "speech discretization", "label": "Embedding"}]}
{"id": "2503.10621", "submitter": "Ayesha Ishaq Ms", "authors": "Ayesha Ishaq, Jean Lahoud, Ketan More, Omkar Thawakar, Ritesh Thawkar,\n  Dinura Dissanayake, Noor Ahsan, Yuhao Li, Fahad Shahbaz Khan, Hisham\n  Cholakkal, Ivan Laptev, Rao Muhammad Anwer, Salman Khan", "title": "DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model\n  for Driving Scenario Understanding", "comments": "8 pages, 4 figures, 3 tables, github:\n  https://github.com/ayesha-ishaq/DriveLMM-o1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While large multimodal models (LMMs) have demonstrated strong performance\nacross various Visual Question Answering (VQA) tasks, certain challenges\nrequire complex multi-step reasoning to reach accurate answers. One\nparticularly challenging task is autonomous driving, which demands thorough\ncognitive processing before decisions can be made. In this domain, a sequential\nand interpretive understanding of visual cues is essential for effective\nperception, prediction, and planning. Nevertheless, common VQA benchmarks often\nfocus on the accuracy of the final answer while overlooking the reasoning\nprocess that enables the generation of accurate responses. Moreover, existing\nmethods lack a comprehensive framework for evaluating step-by-step reasoning in\nrealistic driving scenarios. To address this gap, we propose DriveLMM-o1, a new\ndataset and benchmark specifically designed to advance step-wise visual\nreasoning for autonomous driving. Our benchmark features over 18k VQA examples\nin the training set and more than 4k in the test set, covering diverse\nquestions on perception, prediction, and planning, each enriched with\nstep-by-step reasoning to ensure logical inference in autonomous driving\nscenarios. We further introduce a large multimodal model that is fine-tuned on\nour reasoning dataset, demonstrating robust performance in complex driving\nscenarios. In addition, we benchmark various open-source and closed-source\nmethods on our proposed dataset, systematically comparing their reasoning\ncapabilities for autonomous driving tasks. Our model achieves a +7.49% gain in\nfinal answer accuracy, along with a 3.62% improvement in reasoning score over\nthe previous best open-source model. Our framework, dataset, and model are\navailable at https://github.com/ayesha-ishaq/DriveLMM-o1.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:59:01 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Ishaq", "Ayesha", ""], ["Lahoud", "Jean", ""], ["More", "Ketan", ""], ["Thawakar", "Omkar", ""], ["Thawkar", "Ritesh", ""], ["Dissanayake", "Dinura", ""], ["Ahsan", "Noor", ""], ["Li", "Yuhao", ""], ["Khan", "Fahad Shahbaz", ""], ["Cholakkal", "Hisham", ""], ["Laptev", "Ivan", ""], ["Anwer", "Rao Muhammad", ""], ["Khan", "Salman", ""]], "extracted_entities": [{"text": "fine-tuned", "label": "Fine-tuning"}, {"text": "DriveLMM-o1", "label": "Open-source LLMs"}]}
{"id": "2503.10622", "submitter": "Zhuang Liu", "authors": "Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, Zhuang Liu", "title": "Transformers without Normalization", "comments": "CVPR 2025; Project page: https://jiachenzhu.github.io/DyT/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalization layers are ubiquitous in modern neural networks and have long\nbeen considered essential. This work demonstrates that Transformers without\nnormalization can achieve the same or better performance using a remarkably\nsimple technique. We introduce Dynamic Tanh (DyT), an element-wise operation\n$DyT($x$) = \\tanh(\\alpha $x$)$, as a drop-in replacement for normalization\nlayers in Transformers. DyT is inspired by the observation that layer\nnormalization in Transformers often produces tanh-like, $S$-shaped input-output\nmappings. By incorporating DyT, Transformers without normalization can match or\nexceed the performance of their normalized counterparts, mostly without\nhyperparameter tuning. We validate the effectiveness of Transformers with DyT\nacross diverse settings, ranging from recognition to generation, supervised to\nself-supervised learning, and computer vision to language models. These\nfindings challenge the conventional understanding that normalization layers are\nindispensable in modern neural networks, and offer new insights into their role\nin deep networks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:59:06 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhu", "Jiachen", ""], ["Chen", "Xinlei", ""], ["He", "Kaiming", ""], ["LeCun", "Yann", ""], ["Liu", "Zhuang", ""]], "extracted_entities": [{"text": "Transformers", "label": "Transformers"}, {"text": "Transformers", "label": "Transformers"}, {"text": "DyT", "label": "Transformers"}, {"text": "Transformers", "label": "Transformers"}, {"text": "DyT", "label": "Transformers"}, {"text": "Transformers", "label": "Transformers"}, {"text": "hyperparameter tuning", "label": "Fine-tuning"}, {"text": "Transformers", "label": "Transformers"}, {"text": "supervised to\nself-supervised learning", "label": "Few-shot Learning"}]}
{"id": "2503.10625", "submitter": "Lingteng Qiu", "authors": "Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei\n  Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, Liefeng Bo", "title": "LHM: Large Animatable Human Reconstruction Model from a Single Image in\n  Seconds", "comments": "Project Page: https://lingtengqiu.github.io/LHM/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Animatable 3D human reconstruction from a single image is a challenging\nproblem due to the ambiguity in decoupling geometry, appearance, and\ndeformation. Recent advances in 3D human reconstruction mainly focus on static\nhuman modeling, and the reliance of using synthetic 3D scans for training\nlimits their generalization ability. Conversely, optimization-based video\nmethods achieve higher fidelity but demand controlled capture conditions and\ncomputationally intensive refinement processes. Motivated by the emergence of\nlarge reconstruction models for efficient static reconstruction, we propose LHM\n(Large Animatable Human Reconstruction Model) to infer high-fidelity avatars\nrepresented as 3D Gaussian splatting in a feed-forward pass. Our model\nleverages a multimodal transformer architecture to effectively encode the human\nbody positional features and image features with attention mechanism, enabling\ndetailed preservation of clothing geometry and texture. To further boost the\nface identity preservation and fine detail recovery, we propose a head feature\npyramid encoding scheme to aggregate multi-scale features of the head regions.\nExtensive experiments demonstrate that our LHM generates plausible animatable\nhuman in seconds without post-processing for face and hands, outperforming\nexisting methods in both reconstruction accuracy and generalization ability.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:59:21 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Qiu", "Lingteng", ""], ["Gu", "Xiaodong", ""], ["Li", "Peihao", ""], ["Zuo", "Qi", ""], ["Shen", "Weichao", ""], ["Zhang", "Junfei", ""], ["Qiu", "Kejie", ""], ["Yuan", "Weihao", ""], ["Chen", "Guanying", ""], ["Dong", "Zilong", ""], ["Bo", "Liefeng", ""]], "extracted_entities": [{"text": "attention mechanism", "label": "Attention mechanism"}]}
{"id": "2503.10626", "submitter": "Berat Mert Albaba", "authors": "Mert Albaba, Chenhao Li, Markos Diomataris, Omid Taheri, Andreas\n  Krause, Michael Black", "title": "NIL: No-data Imitation Learning by Leveraging Pre-trained Video\n  Diffusion Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Acquiring physically plausible motor skills across diverse and unconventional\nmorphologies-including humanoid robots, quadrupeds, and animals-is essential\nfor advancing character simulation and robotics. Traditional methods, such as\nreinforcement learning (RL) are task- and body-specific, require extensive\nreward function engineering, and do not generalize well. Imitation learning\noffers an alternative but relies heavily on high-quality expert demonstrations,\nwhich are difficult to obtain for non-human morphologies. Video diffusion\nmodels, on the other hand, are capable of generating realistic videos of\nvarious morphologies, from humans to ants. Leveraging this capability, we\npropose a data-independent approach for skill acquisition that learns 3D motor\nskills from 2D-generated videos, with generalization capability to\nunconventional and non-human forms. Specifically, we guide the imitation\nlearning process by leveraging vision transformers for video-based comparisons\nby calculating pair-wise distance between video embeddings. Along with\nvideo-encoding distance, we also use a computed similarity between segmented\nvideo frames as a guidance reward. We validate our method on locomotion tasks\ninvolving unique body configurations. In humanoid robot locomotion tasks, we\ndemonstrate that 'No-data Imitation Learning' (NIL) outperforms baselines\ntrained on 3D motion-capture data. Our results highlight the potential of\nleveraging generative video models for physically plausible skill learning with\ndiverse morphologies, effectively replacing data collection with data\ngeneration for imitation learning.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:59:24 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Albaba", "Mert", ""], ["Li", "Chenhao", ""], ["Diomataris", "Markos", ""], ["Taheri", "Omid", ""], ["Krause", "Andreas", ""], ["Black", "Michael", ""]], "extracted_entities": [{"text": "Imitation learning", "label": "Few-shot Learning"}, {"text": "imitation\nlearning", "label": "Few-shot Learning"}, {"text": "vision transformers", "label": "Transformers"}, {"text": "video embeddings", "label": "Embedding"}, {"text": "Imitation Learning", "label": "Few-shot Learning"}, {"text": "imitation learning", "label": "Few-shot Learning"}]}
{"id": "2503.10627", "submitter": "Ziyu Guo", "authors": "Ziyu Guo, Ray Zhang, Hao Chen, Jialin Gao, Dongzhi Jiang, Jiaze Wang,\n  Pheng-Ann Heng", "title": "SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of\n  LMMs on Multi-modal Scientific Problems", "comments": "Initially released in September 2024. Project page:\n  https://sciverse-cuhk.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid advancement of Large Multi-modal Models (LMMs) has enabled their\napplication in scientific problem-solving, yet their fine-grained capabilities\nremain under-explored. In this paper, we introduce SciVerse, a multi-modal\nscientific evaluation benchmark to thoroughly assess LMMs across 5,735 test\ninstances in five distinct versions. We aim to investigate three key dimensions\nof LMMs: scientific knowledge comprehension, multi-modal content\ninterpretation, and Chain-of-Thought (CoT) reasoning. To unveil whether LMMs\npossess sufficient scientific expertise, we first transform each problem into\nthree versions containing different levels of knowledge required for solving,\ni.e., Knowledge-free, -lite, and -rich. Then, to explore how LMMs interpret\nmulti-modal scientific content, we annotate another two versions, i.e.,\nVision-rich and -only, marking more question information from texts to\ndiagrams. Comparing the results of different versions, SciVerse systematically\nexamines the professional knowledge stock and visual perception skills of LMMs\nin scientific domains. In addition, to rigorously assess CoT reasoning, we\npropose a new scientific CoT evaluation strategy, conducting a step-wise\nassessment on knowledge and logical errors in model outputs. Our extensive\nevaluation of different LMMs on SciVerse reveals critical limitations in their\nscientific proficiency and provides new insights into future developments.\nProject page: https://sciverse-cuhk.github.io\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:59:32 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Guo", "Ziyu", ""], ["Zhang", "Ray", ""], ["Chen", "Hao", ""], ["Gao", "Jialin", ""], ["Jiang", "Dongzhi", ""], ["Wang", "Jiaze", ""], ["Heng", "Pheng-Ann", ""]], "extracted_entities": [{"text": "Large Multi-modal Models", "label": "Large Language Model"}, {"text": "LMMs", "label": "Large Language Model"}, {"text": "LMMs", "label": "Large Language Model"}, {"text": "LMMs", "label": "Large Language Model"}, {"text": "Chain-of-Thought (CoT) reasoning", "label": "Chain of thought"}, {"text": "LMMs", "label": "Large Language Model"}, {"text": "LMMs", "label": "Large Language Model"}, {"text": "LMMs", "label": "Large Language Model"}, {"text": "CoT reasoning", "label": "Chain of thought"}, {"text": "LMMs", "label": "Large Language Model"}]}
{"id": "2503.10628", "submitter": "Tianjiao Yu", "authors": "Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh\n  Juvekar, Tal August, Ismini Lourentzou", "title": "Uncertainty in Action: Confidence Elicitation in Embodied Agents", "comments": "Project page: https://plan-lab.github.io/ece/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Expressing confidence is challenging for embodied agents navigating dynamic\nmultimodal environments, where uncertainty arises from both perception and\ndecision-making processes. We present the first work investigating embodied\nconfidence elicitation in open-ended multimodal environments. We introduce\nElicitation Policies, which structure confidence assessment across inductive,\ndeductive, and abductive reasoning, along with Execution Policies, which\nenhance confidence calibration through scenario reinterpretation, action\nsampling, and hypothetical reasoning. Evaluating agents in calibration and\nfailure prediction tasks within the Minecraft environment, we show that\nstructured reasoning approaches, such as Chain-of-Thoughts, improve confidence\ncalibration. However, our findings also reveal persistent challenges in\ndistinguishing uncertainty, particularly under abductive settings, underscoring\nthe need for more sophisticated embodied confidence elicitation methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:59:41 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Yu", "Tianjiao", ""], ["Shah", "Vedant", ""], ["Wahed", "Muntasir", ""], ["Nguyen", "Kiet A.", ""], ["Juvekar", "Adheesh", ""], ["August", "Tal", ""], ["Lourentzou", "Ismini", ""]], "extracted_entities": [{"text": "Chain-of-Thoughts", "label": "Chain of thought"}]}
{"id": "2503.10629", "submitter": "Hashmat Shadab Malik", "authors": "Hashmat Shadab Malik, Shahina Kunhimon, Muzammal Naseer, Fahad Shahbaz\n  Khan, Salman Khan", "title": "Hierarchical Self-Supervised Adversarial Training for Robust Vision\n  Models in Histopathology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial attacks pose significant challenges for vision models in critical\nfields like healthcare, where reliability is essential. Although adversarial\ntraining has been well studied in natural images, its application to biomedical\nand microscopy data remains limited. Existing self-supervised adversarial\ntraining methods overlook the hierarchical structure of histopathology images,\nwhere patient-slide-patch relationships provide valuable discriminative\nsignals. To address this, we propose Hierarchical Self-Supervised Adversarial\nTraining (HSAT), which exploits these properties to craft adversarial examples\nusing multi-level contrastive learning and integrate it into adversarial\ntraining for enhanced robustness. We evaluate HSAT on multiclass histopathology\ndataset OpenSRH and the results show that HSAT outperforms existing methods\nfrom both biomedical and natural image domains. HSAT enhances robustness,\nachieving an average gain of 54.31% in the white-box setting and reducing\nperformance drops to 3-4% in the black-box setting, compared to 25-30% for the\nbaseline. These results set a new benchmark for adversarial training in this\ndomain, paving the way for more robust models. Our Code for training and\nevaluation is available at https://github.com/HashmatShadab/HSAT.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:59:47 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Malik", "Hashmat Shadab", ""], ["Kunhimon", "Shahina", ""], ["Naseer", "Muzammal", ""], ["Khan", "Fahad Shahbaz", ""], ["Khan", "Salman", ""]], "extracted_entities": [{"text": "multi-level contrastive learning", "label": "Few-shot Learning"}]}
{"id": "2503.10630", "submitter": "Hang Yin", "authors": "Hang Yin, Xiuwei Xu, Lingqing Zhao, Ziwei Wang, Jie Zhou, Jiwen Lu", "title": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation", "comments": "Accepted to CVPR 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:59:48 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Yin", "Hang", ""], ["Xu", "Xiuwei", ""], ["Zhao", "Lingqing", ""], ["Wang", "Ziwei", ""], ["Zhou", "Jie", ""], ["Lu", "Jiwen", ""]], "extracted_entities": [{"text": "large language models", "label": "Large Language Model"}, {"text": "LLM", "label": "Large Language Model"}, {"text": "LLM", "label": "LLM"}]}
{"id": "2503.10632", "submitter": "Subhajit Maity", "authors": "Subhajit Maity, Killian Hitsman, Xin Li, Aritra Dutta", "title": "Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision\n  Transformers?", "comments": "Preprint, Appendix included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of\nlearnable activation functions with the potential to capture more complex\nrelationships from data. Although KANs are useful in finding symbolic\nrepresentations and continual learning of one-dimensional functions, their\neffectiveness in diverse machine learning (ML) tasks, such as vision, remains\nquestionable. Presently, KANs are deployed by replacing multilayer perceptrons\n(MLPs) in deep network architectures, including advanced architectures such as\nvision Transformers (ViTs). In this paper, we are the first to design a general\nlearnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate\non any choice of basis. However, the computing and memory costs of training\nthem motivated us to propose a more modular version, and we designed particular\nlearnable attention, called Fourier-KArAt. Fourier-KArAt and its variants\neither outperform their ViT counterparts or show comparable performance on\nCIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures'\nperformance and generalization capacity by analyzing their loss landscapes,\nweight distributions, optimizer path, attention visualization, and spectral\nbehavior, and contrast them with vanilla ViTs. The goal of this paper is not to\nproduce parameter- and compute-efficient attention, but to encourage the\ncommunity to explore KANs in conjunction with more advanced architectures that\nrequire a careful understanding of learnable activations. Our open-source code\nand implementation details are available on: https://subhajitmaity.me/KArAt\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:59:52 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Maity", "Subhajit", ""], ["Hitsman", "Killian", ""], ["Li", "Xin", ""], ["Dutta", "Aritra", ""]], "extracted_entities": [{"text": "vision Transformers", "label": "Transformers"}, {"text": "ViTs", "label": "Transformers"}, {"text": "ViTs", "label": "Transformers"}]}
{"id": "2503.10634", "submitter": "Junkun Chen", "authors": "Yanming Zhang, Jun-Kun Chen, Jipeng Lyu, Yu-Xiong Wang", "title": "V2Edit: Versatile Video Diffusion Editor for Videos and 3D Scenes", "comments": "Project Website: https://immortalco.github.io/V2Edit/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces V$^2$Edit, a novel training-free framework for\ninstruction-guided video and 3D scene editing. Addressing the critical\nchallenge of balancing original content preservation with editing task\nfulfillment, our approach employs a progressive strategy that decomposes\ncomplex editing tasks into a sequence of simpler subtasks. Each subtask is\ncontrolled through three key synergistic mechanisms: the initial noise, noise\nadded at each denoising step, and cross-attention maps between text prompts and\nvideo content. This ensures robust preservation of original video elements\nwhile effectively applying the desired edits. Beyond its native video editing\ncapability, we extend V$^2$Edit to 3D scene editing via a\n\"render-edit-reconstruct\" process, enabling high-quality, 3D-consistent edits\neven for tasks involving substantial geometric changes such as object\ninsertion. Extensive experiments demonstrate that our V$^2$Edit achieves\nhigh-quality and successful edits across various challenging video editing\ntasks and complex 3D scene editing tasks, thereby establishing state-of-the-art\nperformance in both domains.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:59:55 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Zhang", "Yanming", ""], ["Chen", "Jun-Kun", ""], ["Lyu", "Jipeng", ""], ["Wang", "Yu-Xiong", ""]], "extracted_entities": [{"text": "cross-attention maps", "label": "Attention mechanism"}, {"text": "text prompts", "label": "Prompting"}]}
{"id": "2503.10635", "submitter": "Zhiqiang Shen", "authors": "Zhaoyi Li and Xiaohan Zhao and Dong-Dong Wu and Jiacheng Cui and\n  Zhiqiang Shen", "title": "A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90%\n  Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1", "comments": "Code at: https://github.com/VILA-Lab/M-Attack", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite promising performance on open-source large vision-language models\n(LVLMs), transfer-based targeted attacks often fail against black-box\ncommercial LVLMs. Analyzing failed adversarial perturbations reveals that the\nlearned perturbations typically originate from a uniform distribution and lack\nclear semantic details, resulting in unintended responses. This critical\nabsence of semantic information leads commercial LVLMs to either ignore the\nperturbation entirely or misinterpret its embedded semantics, thereby causing\nthe attack to fail. To overcome these issues, we notice that identifying core\nsemantic objects is a key objective for models trained with various datasets\nand methodologies. This insight motivates our approach that refines semantic\nclarity by encoding explicit semantic details within local regions, thus\nensuring interoperability and capturing finer-grained features, and by\nconcentrating modifications on semantically rich areas rather than applying\nthem uniformly. To achieve this, we propose a simple yet highly effective\nsolution: at each optimization step, the adversarial image is cropped randomly\nby a controlled aspect ratio and scale, resized, and then aligned with the\ntarget image in the embedding space. Experimental results confirm our\nhypothesis. Our adversarial examples crafted with local-aggregated\nperturbations focused on crucial regions exhibit surprisingly good\ntransferability to commercial LVLMs, including GPT-4.5, GPT-4o,\nGemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning\nmodels like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach\nachieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly\noutperforming all prior state-of-the-art attack methods. Our optimized\nadversarial examples under different configurations and training code are\navailable at https://github.com/VILA-Lab/M-Attack.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:59:55 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Li", "Zhaoyi", ""], ["Zhao", "Xiaohan", ""], ["Wu", "Dong-Dong", ""], ["Cui", "Jiacheng", ""], ["Shen", "Zhiqiang", ""]], "extracted_entities": [{"text": "LVLMs", "label": "Large Language Model"}, {"text": "LVLMs", "label": "Large Language Model"}, {"text": "embedding space", "label": "Embedding"}, {"text": "LVLMs", "label": "Large Language Model"}, {"text": "GPT-4.5", "label": "GPT-4"}, {"text": "GPT-4o", "label": "GPT-4"}, {"text": "Gemini-2.0-flash", "label": "GPT-2"}, {"text": "Claude-3.5-sonnet", "label": "GPT-4"}, {"text": "Claude-3.7-sonnet", "label": "GPT-2"}, {"text": "Claude-3.7-thinking", "label": "GPT-2"}, {"text": "GPT-4.5", "label": "GPT"}]}
{"id": "2503.10637", "submitter": "Rohit Gandikota", "authors": "Rohit Gandikota, David Bau", "title": "Distilling Diversity and Control in Diffusion Models", "comments": "Project Page: https://distillation.baulab.info", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distilled diffusion models suffer from a critical limitation: reduced sample\ndiversity compared to their base counterparts. In this work, we uncover that\ndespite this diversity loss, distilled models retain the fundamental concept\nrepresentations of base models. We demonstrate control distillation - where\ncontrol mechanisms like Concept Sliders and LoRAs trained on base models can be\nseamlessly transferred to distilled models and vice-versa, effectively\ndistilling control without any retraining. This preservation of\nrepresentational structure prompted our investigation into the mechanisms of\ndiversity collapse during distillation. To understand how distillation affects\ndiversity, we introduce Diffusion Target (DT) Visualization, an analysis and\ndebugging tool that reveals how models predict final outputs at intermediate\nsteps. Through DT-Visualization, we identify generation artifacts,\ninconsistencies, and demonstrate that initial diffusion timesteps\ndisproportionately determine output diversity, while later steps primarily\nrefine details. Based on these insights, we introduce diversity distillation -\na hybrid inference approach that strategically employs the base model for only\nthe first critical timestep before transitioning to the efficient distilled\nmodel. Our experiments demonstrate that this simple modification not only\nrestores the diversity capabilities from base to distilled models but\nsurprisingly exceeds it, while maintaining nearly the computational efficiency\nof distilled inference, all without requiring additional training or model\nmodifications. Our code and data are available at\nhttps://distillation.baulab.info\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:59:56 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Gandikota", "Rohit", ""], ["Bau", "David", ""]], "extracted_entities": [{"text": "base models", "label": "Foundation Model"}, {"text": "distillation", "label": "Knowledge distillation"}, {"text": "base models", "label": "Foundation Model"}, {"text": "distillation", "label": "Knowledge distillation"}, {"text": "diversity distillation", "label": "Knowledge distillation"}, {"text": "distillation", "label": "Knowledge distillation"}]}
{"id": "2503.10639", "submitter": "Rongyao Fang", "authors": "Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin\n  Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Xihui Liu, Hongsheng Li", "title": "GoT: Unleashing Reasoning Capability of Multimodal Large Language Model\n  for Visual Generation and Editing", "comments": "Dataset and models are released in https://github.com/rongyaofang/GoT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Current image generation and editing methods primarily process textual\nprompts as direct inputs without reasoning about visual composition and\nexplicit operations. We present Generation Chain-of-Thought (GoT), a novel\nparadigm that enables generation and editing through an explicit language\nreasoning process before outputting images. This approach transforms\nconventional text-to-image generation and editing into a reasoning-guided\nframework that analyzes semantic relationships and spatial arrangements. We\ndefine the formulation of GoT and construct large-scale GoT datasets containing\nover 9M samples with detailed reasoning chains capturing semantic-spatial\nrelationships. To leverage the advantages of GoT, we implement a unified\nframework that integrates Qwen2.5-VL for reasoning chain generation with an\nend-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance\nModule. Experiments show our GoT framework achieves excellent performance on\nboth generation and editing tasks, with significant improvements over\nbaselines. Additionally, our approach enables interactive visual generation,\nallowing users to explicitly modify reasoning steps for precise image\nadjustments. GoT pioneers a new direction for reasoning-driven visual\ngeneration and editing, producing images that better align with human intent.\nTo facilitate future research, we make our datasets, code, and pretrained\nmodels publicly available at https://github.com/rongyaofang/GoT.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2025 17:59:59 GMT"}], "update_date": "2025-03-14", "authors_parsed": [["Fang", "Rongyao", ""], ["Duan", "Chengqi", ""], ["Wang", "Kun", ""], ["Huang", "Linjiang", ""], ["Li", "Hao", ""], ["Yan", "Shilin", ""], ["Tian", "Hao", ""], ["Zeng", "Xingyu", ""], ["Zhao", "Rui", ""], ["Dai", "Jifeng", ""], ["Liu", "Xihui", ""], ["Li", "Hongsheng", ""]], "extracted_entities": [{"text": "textual\nprompts", "label": "Prompting"}, {"text": "Generation Chain-of-Thought", "label": "Chain of thought"}, {"text": "GoT", "label": "Chain of thought"}, {"text": "GoT", "label": "Chain of thought"}]}
