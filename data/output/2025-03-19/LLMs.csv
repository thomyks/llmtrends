id,submitter,authors,title,comments,journal-ref,doi,report-no,categories,license,abstract,versions,update_date,authors_parsed,extracted_entities,assigned_concept,matched_keyword,similarity_score
2503.07384,Gonzalo Mancera,"Gonzalo Mancera, Daniel DeAlcala, Julian Fierrez, Ruben Tolosana,
  Aythami Morales","Is My Text in Your AI Model? Gradient-based Membership Inference Test
  applied to LLMs",,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  This work adapts and studies the gradient-based Membership Inference Test
(gMINT) to the classification of text based on LLMs. MINT is a general approach
intended to determine if given data was used for training machine learning
models, and this work focuses on its application to the domain of Natural
Language Processing. Using gradient-based analysis, the MINT model identifies
whether particular data samples were included during the language model
training phase, addressing growing concerns about data privacy in machine
learning. The method was evaluated in seven Transformer-based models and six
datasets comprising over 2.5 million sentences, focusing on text classification
tasks. Experimental results demonstrate MINTs robustness, achieving AUC scores
between 85% and 99%, depending on data size and model architecture. These
findings highlight MINTs potential as a scalable and reliable tool for auditing
machine learning models, ensuring transparency, safeguarding sensitive data,
and fostering ethical compliance in the deployment of AI/NLP technologies.
","[{'version': 'v1', 'created': 'Mon, 10 Mar 2025 14:32:56 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 12:37:37 GMT'}]",2025-03-14,"[['Mancera', 'Gonzalo', ''], ['DeAlcala', 'Daniel', ''], ['Fierrez', 'Julian', ''], ['Tolosana', 'Ruben', ''], ['Morales', 'Aythami', '']]","[{'text': 'LLMs', 'label': 'LLMs'}, {'text': 'ethical compliance', 'label': 'AI Ethics'}]",LLMs,LLMs,1.000000238418579
2503.09994,Yunxiao Wang,"Yunxiao Wang, Meng Liu, Rui Shao, Haoyu Zhang, Bin Wen, Fan Yang,
  Tingting Gao, Di Zhang, Liqiang Nie","TIME: Temporal-sensitive Multi-dimensional Instruction Tuning and
  Benchmarking for Video-LLMs",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Video large language models have achieved remarkable performance in tasks
such as video question answering, however, their temporal understanding remains
suboptimal. To address this limitation, we curate a dedicated instruction
fine-tuning dataset that focuses on enhancing temporal comprehension across
five key dimensions. In order to reduce reliance on costly temporal
annotations, we introduce a multi-task prompt fine-tuning approach that
seamlessly integrates temporal-sensitive tasks into existing instruction
datasets without requiring additional annotations. Furthermore, we develop a
novel benchmark for temporal-sensitive video understanding that not only fills
the gaps in dimension coverage left by existing benchmarks but also rigorously
filters out potential shortcuts, ensuring a more accurate evaluation. Extensive
experimental results demonstrate that our approach significantly enhances the
temporal understanding of video-LLMs while avoiding reliance on shortcuts.
","[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 03:05:11 GMT'}]",2025-03-14,"[['Wang', 'Yunxiao', ''], ['Liu', 'Meng', ''], ['Shao', 'Rui', ''], ['Zhang', 'Haoyu', ''], ['Wen', 'Bin', ''], ['Yang', 'Fan', ''], ['Gao', 'Tingting', ''], ['Zhang', 'Di', ''], ['Nie', 'Liqiang', '']]","[{'text': 'video-LLMs', 'label': 'LLMs'}]",LLMs,video-LLMs,0.7106761336326599
2503.10509,Sahar Admoni,"Sahar Admoni, Omer Ben-Porat, Ofra Amir","SySLLM: Generating Synthesized Policy Summaries for Reinforcement
  Learning Agents Using Large Language Models",,,,,cs.LG,http://creativecommons.org/licenses/by-sa/4.0/,"  Policies generated by Reinforcement Learning (RL) algorithms can be difficult
to describe to users, as they result from the interplay between complex reward
structures and neural network-based representations. This combination often
leads to unpredictable behaviors, making policies challenging to analyze and
posing significant obstacles to fostering human trust in real-world
applications. Global policy summarization methods aim to describe agent
behavior through a demonstration of actions in a subset of world-states.
However, users can only watch a limited number of demonstrations, restricting
their understanding of policies. Moreover, those methods overly rely on user
interpretation, as they do not synthesize observations into coherent patterns.
In this work, we present SySLLM (Synthesized Summary using LLMs), a novel
method that employs synthesis summarization, utilizing large language models'
(LLMs) extensive world knowledge and ability to capture patterns, to generate
textual summaries of policies. Specifically, an expert evaluation demonstrates
that the proposed approach generates summaries that capture the main insights
generated by experts while not resulting in significant hallucinations.
Additionally, a user study shows that SySLLM summaries are preferred over
demonstration-based policy summaries and match or surpass their performance in
objective agent identification tasks.
","[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 16:10:14 GMT'}]",2025-03-14,"[['Admoni', 'Sahar', ''], ['Ben-Porat', 'Omer', ''], ['Amir', 'Ofra', '']]","[{'text': 'LLMs', 'label': 'LLMs'}, {'text': 'large language models', 'label': 'Large Language Model'}]",LLMs,LLMs,1.000000238418579
