id,submitter,authors,title,comments,journal-ref,doi,report-no,categories,license,abstract,versions,update_date,authors_parsed,extracted_entities,assigned_concept,matched_keyword,similarity_score
2503.02597,Wei-Yao Wang,"Wei-Yao Wang, Zhao Wang, Helen Suzuki, Yoshiyuki Kobayashi","Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual
  Attention for Multimodal LLMs",Preprint,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Recent Multimodal Large Language Models (MLLMs) have demonstrated significant
progress in perceiving and reasoning over multimodal inquiries, ushering in a
new research era for foundation models. However, vision-language misalignment
in MLLMs has emerged as a critical challenge, where the textual responses
generated by these models are not factually aligned with the given text-image
inputs. Existing efforts to address vision-language misalignment have focused
on developing specialized vision-language connectors or leveraging visual
instruction tuning from diverse domains. In this paper, we tackle this issue
from a fundamental yet unexplored perspective by revisiting the core
architecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs
consisting of a causal attention mechanism, which limits the ability of the
earlier modalities (e.g., images) to incorporate information from the latter
modalities (e.g., text). To address this problem, we propose \MapleLeaf AKI, a
novel MLLM that unlocks causal attention into modality-mutual attention (MMA)
to enable image tokens to attend to text tokens. This simple yet effective
design allows AKI to achieve superior performance in 12 multimodal
understanding benchmarks (+7.2% on average) without introducing additional
parameters and increasing training time. Our MMA design is intended to be
generic, allowing for application across various modalities, and scalable to
accommodate diverse multimodal scenarios. The code and model are publicly
available at https://github.com/sony/aki to encourage further advancements in
MLLMs across various directions.
","[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 13:18:33 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 01:48:08 GMT'}]",2025-03-14,"[['Wang', 'Wei-Yao', ''], ['Wang', 'Zhao', ''], ['Suzuki', 'Helen', ''], ['Kobayashi', 'Yoshiyuki', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'visual\ninstruction tuning', 'label': 'Fine-tuning'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'decoder-only LLMs', 'label': 'LLMs'}, {'text': 'causal attention mechanism', 'label': 'Attention mechanism'}, {'text': 'causal attention', 'label': 'Attention mechanism'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Foundation Model,foundation models,0.9628887176513672
2503.09160,Hao Feng,"Hao Feng and Zhi Zuo and Jia-Hui Pan and Ka-Hei Hui and Yihua Shao and
  Qi Dou and Wei Xie and Zhengzhe Liu",WonderVerse: Extendable 3D Scene Generation with Video Generative Models,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce \textit{WonderVerse}, a simple but effective framework for
generating extendable 3D scenes. Unlike existing methods that rely on iterative
depth estimation and image inpainting, often leading to geometric distortions
and inconsistencies, WonderVerse leverages the powerful world-level priors
embedded within video generative foundation models to create highly immersive
and geometrically coherent 3D environments. Furthermore, we propose a new
technique for controllable 3D scene extension to substantially increase the
scale of the generated environments. Besides, we introduce a novel abnormal
sequence detection module that utilizes camera trajectory to address geometric
inconsistency in the generated videos. Finally, WonderVerse is compatible with
various 3D reconstruction methods, allowing both efficient and high-quality
generation. Extensive experiments on 3D scene generation demonstrate that our
WonderVerse, with an elegant and simple pipeline, delivers extendable and
highly-realistic 3D scenes, markedly outperforming existing works that rely on
more complex architectures.
","[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 08:44:51 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 15:29:28 GMT'}]",2025-03-14,"[['Feng', 'Hao', ''], ['Zuo', 'Zhi', ''], ['Pan', 'Jia-Hui', ''], ['Hui', 'Ka-Hei', ''], ['Shao', 'Yihua', ''], ['Dou', 'Qi', ''], ['Xie', 'Wei', ''], ['Liu', 'Zhengzhe', '']]","[{'text': 'video generative foundation models', 'label': 'Foundation Model'}]",Foundation Model,video generative foundation models,0.5319663882255554
2503.10215,Benjamin Heymann,Benjamin Heymann,Adaptive Preference Aggregation,,,,,cs.AI cs.GT,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  AI alignment, the challenge of ensuring AI systems act in accordance with
human values, has emerged as a critical problem in the development of systems
such as foundation models and recommender systems. Still, the current dominant
approach, reinforcement learning with human feedback (RLHF) faces known
theoretical limitations in aggregating diverse human preferences. Social choice
theory provides a framework to aggregate preferences, but was not developed for
the multidimensional applications typical of AI. Leveraging insights from a
recently published urn process, this work introduces a preference aggregation
strategy that adapts to the user's context and that inherits the good
properties of the maximal lottery, a Condorcet-consistent solution concept.
","[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 09:57:41 GMT'}]",2025-03-14,"[['Heymann', 'Benjamin', '']]","[{'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'reinforcement learning with human feedback', 'label': 'Few-shot Learning'}, {'text': 'Social choice\ntheory', 'label': 'AI Ethics'}]",Foundation Model,foundation models,0.9628887176513672
2503.10247,Zhijie Zhu,"Zhijie Zhu, Lei Fan, Maurice Pagnucco, Yang Song","Interpretable Image Classification via Non-parametric Part Prototype
  Learning",,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Classifying images with an interpretable decision-making process is a
long-standing problem in computer vision. In recent years, Prototypical Part
Networks has gained traction as an approach for self-explainable neural
networks, due to their ability to mimic human visual reasoning by providing
explanations based on prototypical object parts. However, the quality of the
explanations generated by these methods leaves room for improvement, as the
prototypes usually focus on repetitive and redundant concepts. Leveraging
recent advances in prototype learning, we present a framework for part-based
interpretable image classification that learns a set of semantically
distinctive object parts for each class, and provides diverse and comprehensive
explanations. The core of our method is to learn the part-prototypes in a
non-parametric fashion, through clustering deep features extracted from
foundation vision models that encode robust semantic information. To
quantitatively evaluate the quality of explanations provided by ProtoPNets, we
introduce Distinctiveness Score and Comprehensiveness Score. Through evaluation
on CUB-200-2011, Stanford Cars and Stanford Dogs datasets, we show that our
framework compares favourably against existing ProtoPNets while achieving
better interpretability. Code is available at:
https://github.com/zijizhu/proto-non-param.
","[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 10:46:53 GMT'}]",2025-03-14,"[['Zhu', 'Zhijie', ''], ['Fan', 'Lei', ''], ['Pagnucco', 'Maurice', ''], ['Song', 'Yang', '']]","[{'text': 'foundation vision models', 'label': 'Foundation Model'}]",Foundation Model,foundation vision models,0.7662065029144287
2503.10538,Teresa Head-Gordon,"Eric C.-Y. Yuan, Yunsheng Liu, Junmin Chen, Peichen Zhong, Sanjeev
  Raja, Tobias Kreiman, Santiago Vargas, Wenbin Xu, Martin Head-Gordon, Chao
  Yang, Samuel M. Blau, Aditi Krishnapriyan, Teresa Head-Gordon",Foundation Models for Atomistic Simulation of Chemistry and Materials,,,,,physics.chem-ph,http://creativecommons.org/licenses/by-sa/4.0/,"  Given the power of large language and large vision models, it is of profound
and fundamental interest to ask if a foundational model based on data and
parameter scaling laws and pre-training strategies is possible for learned
simulations of chemistry and materials. The scaling of large and diverse
datasets and highly expressive architectures for chemical and materials
sciences should result in a foundation model that is more efficient and broadly
transferable, robust to out-of-distribution challenges, and easily fine-tuned
to a variety of downstream observables, when compared to specific training from
scratch on targeted applications in atomistic simulation. In this Perspective
we aim to cover the rapidly advancing field of machine learned interatomic
potentials (MLIP), and to illustrate a path to create chemistry and materials
MLIP foundation models at larger scale.
","[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 16:52:12 GMT'}]",2025-03-14,"[['Yuan', 'Eric C. -Y.', ''], ['Liu', 'Yunsheng', ''], ['Chen', 'Junmin', ''], ['Zhong', 'Peichen', ''], ['Raja', 'Sanjeev', ''], ['Kreiman', 'Tobias', ''], ['Vargas', 'Santiago', ''], ['Xu', 'Wenbin', ''], ['Head-Gordon', 'Martin', ''], ['Yang', 'Chao', ''], ['Blau', 'Samuel M.', ''], ['Krishnapriyan', 'Aditi', ''], ['Head-Gordon', 'Teresa', '']]","[{'text': 'data and\nparameter scaling laws', 'label': 'Scaling law'}, {'text': 'foundation model', 'label': 'Foundation Model'}]",Foundation Model,foundation model,1.0
