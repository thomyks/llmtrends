id,submitter,authors,title,comments,journal-ref,doi,report-no,categories,license,abstract,versions,update_date,authors_parsed,extracted_entities,assigned_concept,matched_keyword,similarity_score
2210.05715,Joseba Fernandez de Landa,Joseba Fernandez de Landa and Rodrigo Agerri,"Language Independent Stance Detection: Social Interaction-based
  Embeddings and Large Language Models",,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  The large majority of the research performed on stance detection has been
focused on developing more or less sophisticated text classification systems,
even when many benchmarks are based on social network data such as Twitter.
This paper aims to take on the stance detection task by placing the emphasis
not so much on the text itself but on the interaction data available on social
networks. More specifically, we propose a new method to leverage social
information such as friends and retweets by generating Relational Embeddings,
namely, dense vector representations of interaction pairs. Our experiments on
seven publicly available datasets and four different languages (Basque,
Catalan, Italian, and Spanish) show that combining our relational embeddings
with discriminative textual methods helps to substantially improve performance,
obtaining state-of-the-art results for six out of seven evaluation settings,
outperforming strong baselines based on Large Language Models, or other popular
interaction-based approaches such as DeepWalk or node2vec.
","[{'version': 'v1', 'created': 'Tue, 11 Oct 2022 18:13:43 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 09:17:32 GMT'}]",2025-02-28,"[['de Landa', 'Joseba Fernandez', ''], ['Agerri', 'Rodrigo', '']]","[{'text': 'Relational Embeddings', 'label': 'Embedding'}, {'text': 'seven publicly available datasets', 'label': 'Open-source LLMs'}, {'text': 'relational embeddings', 'label': 'Embedding'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2308.12219,Jiasheng Ye,"Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, Quanquan Gu","Diffusion Language Models Can Perform Many Tasks with Scaling and
  Instruction-Finetuning","add results on reasoning and multimodality; add discussions on latest
  progress",,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  The recent surge of generative AI has been fueled by the generative power of
diffusion probabilistic models and the scalable capabilities of large language
models. Despite their potential, it remains elusive whether diffusion language
models can solve general language tasks comparable to their autoregressive
counterparts. This paper demonstrates that scaling diffusion models w.r.t.
data, sizes, and tasks can effectively make them strong language learners. We
build competent diffusion language models at scale by first acquiring knowledge
from massive data via masked language modeling pretraining thanks to their
intrinsic connections. We then reprogram pretrained masked language models into
diffusion language models via diffusive adaptation, wherein task-specific
finetuning and instruction finetuning are explored to unlock their versatility
in solving general language tasks. Experiments show that scaling diffusion
language models consistently improves performance across downstream language
tasks. We further discover that instruction finetuning can elicit zero-shot and
few-shot in-context learning abilities that help tackle many unseen tasks by
following natural language instructions, and show promise in advanced and
challenging abilities such as reasoning.
","[{'version': 'v1', 'created': 'Wed, 23 Aug 2023 16:01:12 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Aug 2023 16:32:31 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 05:09:09 GMT'}]",2025-02-25,"[['Ye', 'Jiasheng', ''], ['Zheng', 'Zaixiang', ''], ['Bao', 'Yu', ''], ['Qian', 'Lihua', ''], ['Gu', 'Quanquan', '']]","[{'text': 'large language\nmodels', 'label': 'Large Language Model'}, {'text': 'diffusion language\nmodels', 'label': 'Large Language Model'}, {'text': 'diffusion language models', 'label': 'Large Language Model'}, {'text': 'diffusion language models', 'label': 'Large Language Model'}, {'text': 'task-specific\nfinetuning', 'label': 'Fine-tuning'}, {'text': 'instruction finetuning', 'label': 'Fine-tuning'}, {'text': 'diffusion\nlanguage models', 'label': 'Large Language Model'}, {'text': 'instruction finetuning', 'label': 'Fine-tuning'}]",Large Language Model,"large language
models",0.9664971828460693
2310.04579,Tao Li,"Tao Li, Juan Guevara, Xinhong Xie, and Quanyan Zhu","Self-Confirming Transformer for Belief-Conditioned Adaptation in Offline
  Multi-Agent Reinforcement Learning",,,,,cs.LG cs.MA,http://creativecommons.org/licenses/by/4.0/,"  Offline reinforcement learning (RL) suffers from the distribution shift
between the offline dataset and the online environment. In multi-agent RL
(MARL), this distribution shift may arise from the nonstationary opponents in
the online testing who display distinct behaviors from those recorded in the
offline dataset. Hence, the key to the broader deployment of offline MARL is
the online adaptation to nonstationary opponents. Recent advances in foundation
models, e.g., large language models, have demonstrated the generalization
ability of the transformer, an emerging neural network architecture, in
sequence modeling, of which offline RL is a special case. One naturally wonders
\textit{whether offline-trained transformer-based RL policies adapt to
nonstationary opponents online}. We propose a novel auto-regressive training to
equip transformer agents with online adaptability based on the idea of
self-augmented pre-conditioning. The transformer agent first learns offline to
predict the opponent's action based on past observations. When deployed online,
such a fictitious opponent play, referred to as the belief, is fed back to the
transformer, together with other environmental feedback, to generate future
actions conditional on the belief. Motivated by self-confirming equilibrium in
game theory, the training loss consists of belief consistency loss, requiring
the beliefs to match the opponent's actual actions and best response loss,
mandating the agent to behave optimally under the belief. We evaluate the
online adaptability of the proposed self-confirming transformer (SCT) in a
structured environment, iterated prisoner's dilemma games, to demonstrate SCT's
belief consistency and equilibrium behaviors as well as more involved
multi-particle environments to showcase its superior performance against
nonstationary opponents over prior transformers and offline MARL baselines.
","[{'version': 'v1', 'created': 'Fri, 6 Oct 2023 20:43:08 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 06:49:47 GMT'}]",2025-02-25,"[['Li', 'Tao', ''], ['Guevara', 'Juan', ''], ['Xie', 'Xinhong', ''], ['Zhu', 'Quanyan', '']]","[{'text': 'foundation\nmodels', 'label': 'Foundation Model'}, {'text': 'large language models', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2402.05374,Youngsik Yun,Youngsik Yun and Jihie Kim,CIC: A Framework for Culturally-Aware Image Captioning,Accepted by IJCAI 2024,,10.24963/ijcai.2024/180,,cs.CV cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Image Captioning generates descriptive sentences from images using
Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved
greatly. However, current methods lack the generation of detailed descriptive
captions for the cultural elements depicted in the images, such as the
traditional clothing worn by people from Asian cultural groups. In this paper,
we propose a new framework, Culturally-aware Image Captioning (CIC), that
generates captions and describes cultural elements extracted from cultural
visual elements in images representing cultures. Inspired by methods combining
visual modality and Large Language Models (LLMs) through appropriate prompts,
our framework (1) generates questions based on cultural categories from images,
(2) extracts cultural visual elements from Visual Question Answering (VQA)
using generated questions, and (3) generates culturally-aware captions using
LLMs with the prompts. Our human evaluation conducted on 45 participants from 4
different cultural groups with a high understanding of the corresponding
culture shows that our proposed framework generates more culturally descriptive
captions when compared to the image captioning baseline based on VLPs.
Resources can be found at https://shane3606.github.io/cic..
","[{'version': 'v1', 'created': 'Thu, 8 Feb 2024 03:12:25 GMT'}, {'version': 'v2', 'created': 'Thu, 2 May 2024 02:41:50 GMT'}, {'version': 'v3', 'created': 'Mon, 19 Aug 2024 00:52:51 GMT'}, {'version': 'v4', 'created': 'Mon, 9 Dec 2024 15:39:30 GMT'}, {'version': 'v5', 'created': 'Mon, 24 Feb 2025 06:56:33 GMT'}]",2025-02-25,"[['Yun', 'Youngsik', ''], ['Kim', 'Jihie', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'prompts', 'label': 'Prompting'}]",Large Language Model,Large Language Models,0.9664971828460693
2402.13758,Zheheng Luo,"Zheheng Luo, Qianqian Xie, Sophia Ananiadou","Factual consistency evaluation of summarization in the Era of large
  language models",published on ESWA,,,,cs.CL,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Factual inconsistency with source documents in automatically generated
summaries can lead to misinformation or pose risks. Existing factual
consistency (FC) metrics are constrained by their performance, efficiency, and
explainability. Recent advances in Large language models (LLMs) have
demonstrated remarkable potential in text evaluation but their effectiveness in
assessing FC in summarization remains underexplored. Prior research has mostly
focused on proprietary LLMs, leaving essential factors that affect their
assessment capabilities unexplored. Additionally, current FC evaluation
benchmarks are restricted to news articles, casting doubt on the generality of
the FC methods tested on them. In this paper, we first address the gap by
introducing TreatFact a dataset of LLM-generated summaries of clinical texts,
annotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC
evaluation across news and clinical domains and analyse the impact of model
size, prompts, pre-training and fine-tuning data. Our findings reveal that
despite proprietary models prevailing on the task, open-source LLMs lag behind.
Nevertheless, there is potential for enhancing the performance of open-source
LLMs through increasing model size, expanding pre-training data, and developing
well-curated fine-tuning data. Experiments on TreatFact suggest that both
previous methods and LLM-based evaluators are unable to capture factual
inconsistencies in clinical summaries, posing a new challenge for FC
evaluation.
","[{'version': 'v1', 'created': 'Wed, 21 Feb 2024 12:35:19 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 09:26:10 GMT'}]",2025-02-28,"[['Luo', 'Zheheng', ''], ['Xie', 'Qianqian', ''], ['Ananiadou', 'Sophia', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2402.16319,Runyu Peng,"Runyu Peng, Yunhua Zhou, Qipeng Guo, Yang Gao, Hang Yan, Xipeng Qiu,
  Dahua Lin",Data-free Weight Compress and Denoise for Large Language Models,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) are reshaping the research landscape in
artificial intelligence, particularly as model parameters scale up
significantly, unlocking remarkable capabilities across various domains.
Nevertheless, the scalability of model parameters faces constraints due to
limitations in GPU memory and computational speed. To address these
constraints, various weight compression methods have emerged, such as Pruning
and Quantization. Given the low-rank nature of weight matrices in language
models, the reduction of weights through matrix decomposition undoubtedly holds
significant potential and promise. In this paper, drawing upon the intrinsic
structure of LLMs, we propose a novel approach termed Data-free Joint Rank-k
Approximation for compressing the parameter matrices. Significantly, our method
is characterized by without necessitating additional involvement of any corpus,
while simultaneously preserving orthogonality in conjunction with pruning and
quantization methods. We achieve a model pruning of 80% parameters while
retaining 93.43% of the original performance without any calibration data.
Additionally, we explore the fundamental properties of the weight matrix of
LLMs undergone Rank-k Approximation and conduct comprehensive experiments to
elucidate our hypothesis.
","[{'version': 'v1', 'created': 'Mon, 26 Feb 2024 05:51:47 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 07:41:18 GMT'}]",2025-02-25,"[['Peng', 'Runyu', ''], ['Zhou', 'Yunhua', ''], ['Guo', 'Qipeng', ''], ['Gao', 'Yang', ''], ['Yan', 'Hang', ''], ['Qiu', 'Xipeng', ''], ['Lin', 'Dahua', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'Pruning', 'label': 'quantisation'}, {'text': 'Quantization', 'label': 'quantisation'}, {'text': 'pruning', 'label': 'quantisation'}]",Large Language Model,Large Language Models,0.9664971828460693
2404.08948,Chenhui Cui,"Chenhui Cui, Tao Li, Junjie Wang, Chunyang Chen, Dave Towey, Rubing
  Huang","Large Language Models for Mobile GUI Text Input Generation: An Empirical
  Study",,,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Mobile applications have become an essential part of our daily lives, making
ensuring their quality an important activity. Graphical User Interface (GUI)
testing is a quality assurance method that has frequently been used for mobile
apps. When conducting GUI testing, it is important to generate effective text
inputs for the text-input components. Some GUIs require these text inputs to be
able to move from one page to the next: This can be a challenge to achieving
complete UI exploration. Recently, Large Language Models (LLMs) have
demonstrated excellent text-generation capabilities. To the best of our
knowledge, there has not yet been any empirical study to evaluate different
pre-trained LLMs' effectiveness at generating text inputs for mobile GUI
testing. This paper reports on a large-scale empirical study that extensively
investigates the effectiveness of nine state-of-the-art LLMs in Android
text-input generation for UI pages. We collected 114 UI pages from 62
open-source Android apps and extracted contextual information from the UI pages
to construct prompts for LLMs to generate text inputs. The experimental results
show that some LLMs can generate more effective and higher-quality text inputs,
achieving a 50.58% to 66.67% page-pass-through rate (PPTR). We also found that
using more complete UI contextual information can increase the PPTRs of LLMs
for generating text inputs. We conducted an experiment to evaluate the
bug-detection capabilities of LLMs by directly generating invalid text inputs.
We collected 37 real-world bugs related to text inputs. The results show that
using LLMs to directly generate invalid text inputs for bug detection is
insufficient: The bug-detection rates of the nine LLMs are all less than 23%.
In addition, we also describe six insights gained regarding the use of LLMs for
Android testing: These insights will benefit the Android testing community.
","[{'version': 'v1', 'created': 'Sat, 13 Apr 2024 09:56:50 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 06:23:35 GMT'}]",2025-02-27,"[['Cui', 'Chenhui', ''], ['Li', 'Tao', ''], ['Wang', 'Junjie', ''], ['Chen', 'Chunyang', ''], ['Towey', 'Dave', ''], ['Huang', 'Rubing', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2404.09656,Boris Shaposhnikov,"Alexey Gorbatovski, Boris Shaposhnikov, Alexey Malakhov, Nikita
  Surnachev, Yaroslav Aksenov, Ian Maksimov, Nikita Balagansky, Daniil Gavrilov",Learn Your Reference Model for Real Good Alignment,,,,,cs.LG cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Despite the fact that offline methods for Large Language Models (LLMs)
alignment do not require a direct reward model, they remain susceptible to
overoptimization. This issue arises when the trained model deviates excessively
from the reference policy, leading to a decrease in sample quality. We propose
a new paradigm of offline alignment methods, called Trust Region (including
variants TR-DPO, TR-IPO, TR-KTO), which dynamically updates the reference
policy throughout the training process. Our results show that TR alignment
methods effectively mitigate overoptimization, enabling models to maintain
strong performance even when substantially deviating from the initial reference
policy. We demonstrate the efficacy of these approaches not only through toy
examples that exhibit reduced overoptimization, but also through direct,
side-by-side comparisons in specific tasks such as helpful and harmless
dialogue, as well as summarization, where they surpass conventional methods.
Additionally, we report significant improvements in general-purpose assistant
setups with the Llama3 model on the AlpacaEval 2 and Arena-Hard benchmarks,
highlighting the advantages of Trust Region methods over classical approaches.
","[{'version': 'v1', 'created': 'Mon, 15 Apr 2024 10:44:31 GMT'}, {'version': 'v2', 'created': 'Tue, 21 May 2024 15:04:12 GMT'}, {'version': 'v3', 'created': 'Fri, 11 Oct 2024 13:42:12 GMT'}, {'version': 'v4', 'created': 'Tue, 25 Feb 2025 10:19:35 GMT'}]",2025-02-26,"[['Gorbatovski', 'Alexey', ''], ['Shaposhnikov', 'Boris', ''], ['Malakhov', 'Alexey', ''], ['Surnachev', 'Nikita', ''], ['Aksenov', 'Yaroslav', ''], ['Maksimov', 'Ian', ''], ['Balagansky', 'Nikita', ''], ['Gavrilov', 'Daniil', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'summarization', 'label': 'Knowledge distillation'}, {'text': 'Llama3', 'label': 'Llama'}]",Large Language Model,Large Language Models,0.9664971828460693
2404.10171,Thanh-Dung Le,"Boammani Aser Lompo, Thanh-Dung Le, Philippe Jouvet, Rita Noumeir","Are Medium-Sized Transformers Models still Relevant for Medical Records
  Processing?",Under revision,,,,eess.SP,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  As large language models (LLMs) become the standard in many NLP applications,
we explore the potential of medium-sized pretrained transformer models as a
viable alternative for medical record processing. Medical records generated by
healthcare professionals during patient admissions remain underutilized due to
challenges such as complex medical terminology, the limited ability of
pretrained models to interpret numerical data, and the scarcity of annotated
training datasets. Objective: This study aims to classify numerical values
extracted from medical records into seven distinct physiological categories
using CamemBERT-bio. Previous research has suggested that transformer-based
models may underperform compared to traditional NLP approaches in this context.
Methods: To enhance the performance of CamemBERT-bio, we propose two key
innovations: (1) incorporating keyword embeddings to refine the model's
attention mechanisms and (2) adopting a number-agnostic strategy by removing
numerical values from the text to encourage context-driven learning.
Additionally, we assess the criticality of extracted numerical data by
verifying whether values fall within established standard ranges. Results: Our
findings demonstrate significant performance improvements, with CamemBERT-bio
achieving an F1 score of 0.89 - an increase of over 20% compared to the 0.73 F1
score of traditional methods and only 0.06 units lower than GPT-4. These
results were obtained despite the use of small and imbalanced training
datasets.
","[{'version': 'v1', 'created': 'Mon, 15 Apr 2024 22:50:42 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 19:45:11 GMT'}]",2025-02-28,"[['Lompo', 'Boammani Aser', ''], ['Le', 'Thanh-Dung', ''], ['Jouvet', 'Philippe', ''], ['Noumeir', 'Rita', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'keyword embeddings', 'label': 'contextual Embedding'}, {'text': 'attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'context-driven learning', 'label': 'Few-shot Learning'}]",Large Language Model,large language models,0.9664971828460693
2405.11874,Zhiyu Li,"Qingchen Yu, Zifan Zheng, Shichao Song, Zhiyu Li, Feiyu Xiong, Bo
  Tang, Ding Chen","xFinder: Large Language Models as Automated Evaluators for Reliable
  Evaluation",Accepted by ICLR 2025,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The continuous advancement of large language models (LLMs) has brought
increasing attention to the critical issue of developing fair and reliable
methods for evaluating their performance. Particularly, the emergence of
cheating phenomena, such as test set leakage and prompt format overfitting,
poses significant challenges to the reliable evaluation of LLMs. As evaluation
frameworks commonly use Regular Expression (RegEx) for answer extraction,
models may adjust their responses to fit formats easily handled by RegEx.
Nevertheless, the key answer extraction module based on RegEx frequently
suffers from extraction errors. Furthermore, recent studies proposing
fine-tuned LLMs as judge models for automated evaluation face challenges in
terms of generalization ability and fairness. This paper comprehensively
analyzes the entire LLM evaluation chain and demonstrates that optimizing the
key answer extraction module improves extraction accuracy and enhances
evaluation reliability. Our findings suggest that improving the key answer
extraction module can lead to higher judgment accuracy and improved evaluation
efficiency compared to the judge models. To address these issues, we propose
xFinder, a novel evaluator for answer extraction and matching in LLM
evaluation. As part of this process, we create a specialized dataset, the
\textbf{K}ey \textbf{A}nswer \textbf{F}inder (KAF) dataset, to ensure effective
model training and evaluation. Generalization tests and real-world evaluations
show that the smallest xFinder model, with only 500 million parameters,
achieves an average extraction accuracy of 93.42\%. In contrast, RegEx accuracy
in the best evaluation framework is 74.38\%. The final judgment accuracy of
xFinder reaches 97.61\%, outperforming existing evaluation frameworks and judge
models.
","[{'version': 'v1', 'created': 'Mon, 20 May 2024 08:30:13 GMT'}, {'version': 'v2', 'created': 'Thu, 23 May 2024 07:00:45 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Feb 2025 11:04:02 GMT'}]",2025-02-26,"[['Yu', 'Qingchen', ''], ['Zheng', 'Zifan', ''], ['Song', 'Shichao', ''], ['Li', 'Zhiyu', ''], ['Xiong', 'Feiyu', ''], ['Tang', 'Bo', ''], ['Chen', 'Ding', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}]",Large Language Model,large language models,0.9664971828460693
2405.14117,Chen Yuheng,"Yuheng Chen, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao","Knowledge Localization: Mission Not Accomplished? Enter Query
  Localization!",ICLR 2025 Spotlight,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) store extensive factual knowledge, but the
mechanisms behind how they store and express this knowledge remain unclear. The
Knowledge Neuron (KN) thesis is a prominent theory for explaining these
mechanisms. This theory is based on the Knowledge Localization (KL) assumption,
which suggests that a fact can be localized to a few knowledge storage units,
namely knowledge neurons.
  However, this assumption has two limitations: first, it may be too rigid
regarding knowledge storage, and second, it neglects the role of the attention
module in knowledge expression.
  In this paper, we first re-examine the KL assumption and demonstrate that its
limitations do indeed exist. To address these, we then present two new
findings, each targeting one of the limitations: one focusing on knowledge
storage and the other on knowledge expression. We summarize these findings as
\textbf{Query Localization} (QL) assumption and argue that the KL assumption
can be viewed as a simplification of the QL assumption. Based on QL assumption,
we further propose the Consistency-Aware KN modification method, which improves
the performance of knowledge modification, further validating our new
assumption. We conduct 39 sets of experiments, along with additional
visualization experiments, to rigorously confirm our conclusions. Code is
available at https://github.com/heng840/KnowledgeLocalization.
","[{'version': 'v1', 'created': 'Thu, 23 May 2024 02:44:12 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 12:29:11 GMT'}]",2025-02-28,"[['Chen', 'Yuheng', ''], ['Cao', 'Pengfei', ''], ['Chen', 'Yubo', ''], ['Liu', 'Kang', ''], ['Zhao', 'Jun', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'attention\nmodule', 'label': 'Attention mechanism'}]",Large Language Model,Large language models,0.9664971828460693
2405.15349,Deng Jingcheng,"Jingcheng Deng, Zihao Wei, Liang Pang, Hanxing Ding, Huawei Shen,
  Xueqi Cheng","Everything is Editable: Extend Knowledge Editing to Unstructured Data in
  Large Language Models",ICLR 2025,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent knowledge editing methods have primarily focused on modifying
structured knowledge in large language models. However, this task setting
overlooks the fact that a significant portion of real-world knowledge is stored
in an unstructured format, characterized by long-form content, noise, and a
complex yet comprehensive nature. Techniques like ""local layer key-value
storage"" and ""term-driven optimization"", as used in previous methods like
MEMIT, are not effective for handling unstructured knowledge. To address these
challenges, we propose a novel Unstructured Knowledge Editing method, namely
UnKE, which extends previous assumptions in the layer dimension and token
dimension. Firstly, in the layer dimension, we propose non-local block
key-value storage to replace local layer key-value storage, increasing the
representation ability of key-value pairs and incorporating attention layer
knowledge. Secondly, in the token dimension, we replace ""term-driven
optimization"" with ""cause-driven optimization"", which edits the last token
directly while preserving context, avoiding the need to locate terms and
preventing the loss of context information. Results on newly proposed
unstructured knowledge editing dataset (UnKEBench) and traditional structured
datasets demonstrate that UnKE achieves remarkable performance, surpassing
strong baselines. In addition, UnKE has robust batch editing and sequential
editing capabilities.
","[{'version': 'v1', 'created': 'Fri, 24 May 2024 08:42:40 GMT'}, {'version': 'v2', 'created': 'Fri, 18 Oct 2024 04:32:49 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Feb 2025 03:33:47 GMT'}]",2025-02-26,"[['Deng', 'Jingcheng', ''], ['Wei', 'Zihao', ''], ['Pang', 'Liang', ''], ['Ding', 'Hanxing', ''], ['Shen', 'Huawei', ''], ['Cheng', 'Xueqi', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2405.20318,Zhijing Jin,"Roberto Ceraolo, Dmitrii Kharlapenko, Ahmad Khan, Am\'elie Reymond,
  Rada Mihalcea, Bernhard Sch\""olkopf, Mrinmaya Sachan, Zhijing Jin","Quriosity: Analyzing Human Questioning Behavior and Causal Inquiry
  through Curiosity-Driven Queries",,,,,cs.CL cs.AI cs.LG stat.ML,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Recent progress in Large Language Model (LLM) technology has changed our role
in interacting with these models. Instead of primarily testing these models
with questions we already know answers to, we are now using them for queries
where the answers are unknown to us, driven by human curiosity. This shift
highlights the growing need to understand curiosity-driven human questions -
those that are more complex, open-ended, and reflective of real-world needs. To
this end, we present Quriosity, a collection of 13.5K naturally occurring
questions from three diverse sources: human-to-search-engine queries,
human-to-human interactions, and human-to-LLM conversations. Our comprehensive
collection enables a rich understanding of human curiosity across various
domains and contexts. Our analysis reveals a significant presence of causal
questions (up to 42%) in the dataset, for which we develop an iterative prompt
improvement framework to identify all causal queries and examine their unique
linguistic properties, cognitive complexity and source distribution. Our paper
paves the way for future work on causal question identification and open-ended
chatbot interactions.
","[{'version': 'v1', 'created': 'Thu, 30 May 2024 17:55:28 GMT'}, {'version': 'v2', 'created': 'Thu, 24 Oct 2024 09:21:38 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 16:42:25 GMT'}]",2025-02-25,"[['Ceraolo', 'Roberto', ''], ['Kharlapenko', 'Dmitrii', ''], ['Khan', 'Ahmad', ''], ['Reymond', 'Amélie', ''], ['Mihalcea', 'Rada', ''], ['Schölkopf', 'Bernhard', ''], ['Sachan', 'Mrinmaya', ''], ['Jin', 'Zhijing', '']]","[{'text': 'Large Language Model', 'label': 'Large Language Model'}, {'text': 'iterative prompt\nimprovement framework', 'label': 'Prompting'}, {'text': 'open-ended\nchatbot interactions', 'label': 'ChatGPT'}]",Large Language Model,Large Language Model,1.0
2406.00023,Jing Li,"Jing Li, Zhijie Sun, Dachao Lin, Xuan He, Binfan Zheng, Yi Lin,
  Rongqian Zhao, Xin Chen","Expert-Token Resonance MoE: Bidirectional Routing with Efficiency
  Affinity-Driven Active Selection",,,,,cs.CL,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Mixture-of-Experts (MoE) architectures have emerged as a paradigm-shifting
approach for large language models (LLMs), offering unprecedented computational
efficiency. However, these architectures grapple with challenges of token
distribution imbalance and expert homogenization, impeding optimal semantic
generalization. We propose a novel expert routing framework that incorporates:
(1) An efficient routing mechanism with lightweight computation. (2) An
adaptive bidirectional selection mechanism leveraging resonance between experts
and tokens. (3) A module that determines the lower bounds of expert capacity
based on dynamic token distribution analysis, specifically designed to address
drop-and-pad strategies. It is also integrated with orthogonal feature
extraction module and an optimized loss function for expert localization. This
framework effectively reduces expert homogeneity while enhancing the
performance of the expert selection module. Additionally, we introduce a local
expert strategy that simultaneously improves load balancing and reduces network
communication overhead. It achieves a 40\% reduction in token processed by each
expert without compromising model convergence or efficacy. When coupled with
communication optimizations, the training efficiency improvements of 5.4\% to
46.6\% can be observed. After supervised fine-tuning, it exhibits performance
gains of 9.7\% to 14.1\% across GDAD, GPQA, and TeleQnA benchmarks.
","[{'version': 'v1', 'created': 'Fri, 24 May 2024 02:50:44 GMT'}, {'version': 'v2', 'created': 'Fri, 30 Aug 2024 11:32:48 GMT'}, {'version': 'v3', 'created': 'Wed, 26 Feb 2025 03:28:51 GMT'}]",2025-02-27,"[['Li', 'Jing', ''], ['Sun', 'Zhijie', ''], ['Lin', 'Dachao', ''], ['He', 'Xuan', ''], ['Zheng', 'Binfan', ''], ['Lin', 'Yi', ''], ['Zhao', 'Rongqian', ''], ['Chen', 'Xin', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}]",Large Language Model,large language models,0.9664971828460693
2406.00034,Yinghao Zhu,"Tianlong Wang, Xianfeng Jiao, Yinghao Zhu, Zhongzhi Chen, Yifan He, Xu
  Chu, Junyi Gao, Yasha Wang, Liantao Ma","Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement
  Method for Diverse Hallucinations Categories",ACM TheWebConf 2025 Conference (WWW 2025) Research Track,,10.1145/3696410.3714640,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent studies have indicated that Large Language Models (LLMs) harbor an
inherent understanding of truthfulness, yet often fail to consistently express
it and generate false statements. This gap between ""knowing"" and ""telling""
poses a challenge for ensuring the truthfulness of generated content. Inspired
by recent work on the practice of encoding human-interpretable concepts
linearly within large language models, we treat truthfulness as a specially
linearly encoded concept within LLMs, and introduce Adaptive Activation
Steering (ACT), a tuning-free method that adaptively shifts LLM's activations
in the ""truthful"" direction during inference. ACT addresses diverse categories
of hallucinations by utilizing diverse truthfulness-related steering vectors
and adjusting the steering intensity adaptively. Applied as an add-on across
various models, ACT significantly improves truthfulness in LLaMA ($\uparrow$
142%), LLaMA2 ($\uparrow$ 24%), Alpaca ($\uparrow$ 36%), Vicuna ($\uparrow$
28%), LLaMA2-Chat ($\uparrow$ 19%), and LLaMA3($\uparrow$ 34%). Furthermore, we
verify ACT's scalability across larger models (13B, 33B, 65B), underscoring the
adaptability of ACT to large-scale language models. Our code is available at
https://github.com/tianlwang/ACT.
","[{'version': 'v1', 'created': 'Sun, 26 May 2024 21:39:53 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 14:07:05 GMT'}]",2025-02-27,"[['Wang', 'Tianlong', ''], ['Jiao', 'Xianfeng', ''], ['Zhu', 'Yinghao', ''], ['Chen', 'Zhongzhi', ''], ['He', 'Yifan', ''], ['Chu', 'Xu', ''], ['Gao', 'Junyi', ''], ['Wang', 'Yasha', ''], ['Ma', 'Liantao', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Alpaca', 'label': 'Llama'}]",Large Language Model,Large Language Models,0.9664971828460693
2406.05127,Xiangtai Li Dr,"Shengqiong Wu, Hao Fei, Xiangtai Li, Jiayi Ji, Hanwang Zhang, Tat-Seng
  Chua, Shuicheng Yan",Towards Semantic Equivalence of Tokenization in Multimodal LLM,ICLR-2025. The project page: https://chocowu.github.io/SeTok-web/,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal Large Language Models (MLLMs) have demonstrated exceptional
capabilities in processing vision-language tasks. One of the crux of MLLMs lies
in vision tokenization, which involves efficiently transforming input visual
signals into feature representations that are most beneficial for LLMs.
However, existing vision tokenizers, essential for semantic alignment between
vision and language, remain problematic. Existing methods aggressively fragment
visual input, corrupting the visual semantic integrity. To address this, this
paper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),
which groups visual features into semantic units via a dynamic clustering
algorithm, flexibly determining the number of tokens based on image complexity.
The resulting vision tokens effectively preserve semantic integrity and capture
both low-frequency and high-frequency visual features. The proposed MLLM
(Setokim) equipped with SeTok significantly demonstrates superior performance
across various tasks, as evidenced by our experimental results. The project
page is at https://chocowu.github.io/SeTok-web/.
","[{'version': 'v1', 'created': 'Fri, 7 Jun 2024 17:55:43 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Jun 2024 17:35:45 GMT'}, {'version': 'v3', 'created': 'Wed, 9 Oct 2024 12:01:24 GMT'}, {'version': 'v4', 'created': 'Wed, 26 Feb 2025 02:55:53 GMT'}]",2025-02-27,"[['Wu', 'Shengqiong', ''], ['Fei', 'Hao', ''], ['Li', 'Xiangtai', ''], ['Ji', 'Jiayi', ''], ['Zhang', 'Hanwang', ''], ['Chua', 'Tat-Seng', ''], ['Yan', 'Shuicheng', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2406.05516,Hengguan Huang,"Hengguan Huang, Xing Shen, Songtao Wang, Lingfa Meng, Dianbo Liu, Hao
  Wang, Samir Bhatt",Verbalized Probabilistic Graphical Modeling,,,,,cs.LG cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Human cognition excels at transcending sensory input and forming latent
representations that structure our understanding of the world. Although Large
Language Models (LLMs) can produce chain-of-thought reasoning, they lack a
principled framework to capture latent structures and model uncertainty,
especially in compositional reasoning tasks. We propose Verbalized
Probabilistic Graphical Modeling (vPGM), a Bayesian prompting framework that
guides LLMs to simulate key principles of Probabilistic Graphical Models (PGMs)
in natural language. Unlike many traditional probabilistic methods requiring
substantial domain expertise or specialized training, vPGM bypasses
expert-driven model design, making it well-suited for scenarios with limited
assumptions or scarce data. We evaluated our model on several compositional
reasoning tasks, both close-ended and open-ended. Our results indicate that the
model effectively enhances confidence calibration and text generation quality.
","[{'version': 'v1', 'created': 'Sat, 8 Jun 2024 16:35:31 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 13:56:16 GMT'}]",2025-02-27,"[['Huang', 'Hengguan', ''], ['Shen', 'Xing', ''], ['Wang', 'Songtao', ''], ['Meng', 'Lingfa', ''], ['Liu', 'Dianbo', ''], ['Wang', 'Hao', ''], ['Bhatt', 'Samir', '']]","[{'text': 'Large\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'chain-of-thought reasoning', 'label': 'Chain of thought'}, {'text': 'Bayesian prompting', 'label': 'Prompting'}, {'text': 'vPGM', 'label': 'Prompting'}]",Large Language Model,"Large
Language Models",0.9664971828460693
2406.09179,Qizhou Wang,"Qizhou Wang, Bo Han, Puning Yang, Jianing Zhu, Tongliang Liu, Masashi
  Sugiyama",Towards Effective Evaluations and Comparisons for LLM Unlearning Methods,,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The imperative to eliminate undesirable data memorization underscores the
significance of machine unlearning for large language models (LLMs). Recent
research has introduced a series of promising unlearning methods, notably
boosting the practical significance of the field. Nevertheless, adopting a
proper evaluation framework to reflect the true unlearning efficacy is also
essential yet has not received adequate attention. This paper seeks to refine
the evaluation of LLM unlearning by addressing two key challenges -- a) the
robustness of evaluation metrics and b) the trade-offs between competing goals.
The first challenge stems from findings that current metrics are susceptible to
various red teaming scenarios. It indicates that they may not reflect the true
extent of knowledge retained by LLMs but rather tend to mirror superficial
model behaviors, thus prone to attacks. We address this issue by devising and
assessing a series of candidate metrics, selecting the most robust ones under
various types of attacks. The second challenge arises from the conflicting
goals of eliminating unwanted knowledge while retaining those of others. This
trade-off between unlearning and retention often fails to conform the Pareto
frontier, rendering it subtle to compare the efficacy between methods that
excel only in either unlearning or retention. We handle this issue by proposing
a calibration method that can restore the original performance on non-targeted
data after unlearning, thereby allowing us to focus exclusively on assessing
the strength of unlearning. Our evaluation framework notably enhances the
effectiveness when assessing and comparing various LLM unlearning methods,
further allowing us to benchmark existing works, identify their proper
hyper-parameters, and explore new tricks to enhance their practical efficacy.
","[{'version': 'v1', 'created': 'Thu, 13 Jun 2024 14:41:00 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 03:42:38 GMT'}]",2025-02-26,"[['Wang', 'Qizhou', ''], ['Han', 'Bo', ''], ['Yang', 'Puning', ''], ['Zhu', 'Jianing', ''], ['Liu', 'Tongliang', ''], ['Sugiyama', 'Masashi', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2406.12030,Yongting Zhang,"Yongting Zhang, Lu Chen, Guodong Zheng, Yifeng Gao, Rui Zheng, Jinlan
  Fu, Zhenfei Yin, Senjie Jin, Yu Qiao, Xuanjing Huang, Feng Zhao, Tao Gui,
  Jing Shao","SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision
  Language Model",,,,,cs.CV cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The emergence of Vision Language Models (VLMs) has brought unprecedented
advances in understanding multimodal information. The combination of textual
and visual semantics in VLMs is highly complex and diverse, making the safety
alignment of these models challenging. Furthermore, due to the limited study on
the safety alignment of VLMs, there is a lack of large-scale, high-quality
datasets. To address these limitations, we propose a Safety Preference
Alignment dataset for Vision Language Models named SPA-VL. In terms of breadth,
SPA-VL covers 6 harmfulness domains, 13 categories, and 53 subcategories, and
contains 100,788 samples of the quadruple (question, image, chosen response,
rejected response). In terms of depth, the responses are collected from 12
open-source (e.g., QwenVL) and closed-source (e.g., Gemini) VLMs to ensure
diversity. The construction of preference data is fully automated, and the
experimental results indicate that models trained with alignment techniques on
the SPA-VL dataset exhibit substantial improvements in harmlessness and
helpfulness while maintaining core capabilities. SPA-VL, as a large-scale,
high-quality, and diverse dataset, represents a significant milestone in
ensuring that VLMs achieve both harmlessness and helpfulness.
","[{'version': 'v1', 'created': 'Mon, 17 Jun 2024 18:57:37 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 04:18:50 GMT'}]",2025-02-28,"[['Zhang', 'Yongting', ''], ['Chen', 'Lu', ''], ['Zheng', 'Guodong', ''], ['Gao', 'Yifeng', ''], ['Zheng', 'Rui', ''], ['Fu', 'Jinlan', ''], ['Yin', 'Zhenfei', ''], ['Jin', 'Senjie', ''], ['Qiao', 'Yu', ''], ['Huang', 'Xuanjing', ''], ['Zhao', 'Feng', ''], ['Gui', 'Tao', ''], ['Shao', 'Jing', '']]","[{'text': 'Vision Language Models', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'Vision Language Models', 'label': 'Large Language Model'}, {'text': 'QwenVL', 'label': 'Open-source LLMs'}, {'text': 'Gemini', 'label': 'Open-source LLMs'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}]",Large Language Model,Vision Language Models,0.5921616554260254
2406.18849,Zhongqi Wang,"Jie Zhang, Zhongqi Wang, Mengqi Lei, Zheng Yuan, Bei Yan, Shiguang
  Shan, Xilin Chen","Dysca: A Dynamic and Scalable Benchmark for Evaluating Perception
  Ability of LVLMs",Accepted by ICLR2025,,,,cs.CV,http://creativecommons.org/licenses/by-sa/4.0/,"  Currently many benchmarks have been proposed to evaluate the perception
ability of the Large Vision-Language Models (LVLMs). However, most benchmarks
conduct questions by selecting images from existing datasets, resulting in the
potential data leakage. Besides, these benchmarks merely focus on evaluating
LVLMs on the realistic style images and clean scenarios, leaving the
multi-stylized images and noisy scenarios unexplored. In response to these
challenges, we propose a dynamic and scalable benchmark named Dysca for
evaluating LVLMs by leveraging synthesis images. Specifically, we leverage
Stable Diffusion and design a rule-based method to dynamically generate novel
images, questions and the corresponding answers. We consider 51 kinds of image
styles and evaluate the perception capability in 20 subtasks. Moreover, we
conduct evaluations under 4 scenarios (i.e., Clean, Corruption, Print Attacking
and Adversarial Attacking) and 3 question types (i.e., Multi-choices,
True-or-false and Free-form). Thanks to the generative paradigm, Dysca serves
as a scalable benchmark for easily adding new subtasks and scenarios. A total
of 24 advanced open-source LVLMs and 2 close-source LVLMs are evaluated on
Dysca, revealing the drawbacks of current LVLMs. The benchmark is released at
https://github.com/Robin-WZQ/Dysca.
","[{'version': 'v1', 'created': 'Thu, 27 Jun 2024 02:40:35 GMT'}, {'version': 'v2', 'created': 'Fri, 26 Jul 2024 03:18:35 GMT'}, {'version': 'v3', 'created': 'Fri, 24 Jan 2025 13:58:49 GMT'}, {'version': 'v4', 'created': 'Mon, 24 Feb 2025 01:56:43 GMT'}]",2025-02-25,"[['Zhang', 'Jie', ''], ['Wang', 'Zhongqi', ''], ['Lei', 'Mengqi', ''], ['Yuan', 'Zheng', ''], ['Yan', 'Bei', ''], ['Shan', 'Shiguang', ''], ['Chen', 'Xilin', '']]","[{'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'Stable Diffusion', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Vision-Language Models,0.7742220759391785
2406.19859,Zhi-Qi Cheng,"Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Qi He, Wangmeng
  Xiang, Hanyuan Chen, Jin-Peng Lan, Xianhui Lin, Kang Zhu, Bin Luo, Yifeng
  Geng, Xuansong Xie, Alexander G. Hauptmann","MetaDesigner: Advancing Artistic Typography Through AI-Driven,
  User-Centric, and Multilingual WordArt Synthesis","Accepted by ICLR 2025, Project:
  https://modelscope.cn/studios/WordArt/WordArt",,,,cs.AI cs.HC cs.MM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  MetaDesigner introduces a transformative framework for artistic typography
synthesis, powered by Large Language Models (LLMs) and grounded in a
user-centric design paradigm. Its foundation is a multi-agent system comprising
the Pipeline, Glyph, and Texture agents, which collectively orchestrate the
creation of customizable WordArt, ranging from semantic enhancements to
intricate textural elements. A central feedback mechanism leverages insights
from both multimodal models and user evaluations, enabling iterative refinement
of design parameters. Through this iterative process, MetaDesigner dynamically
adjusts hyperparameters to align with user-defined stylistic and thematic
preferences, consistently delivering WordArt that excels in visual quality and
contextual resonance. Empirical evaluations underscore the system's versatility
and effectiveness across diverse WordArt applications, yielding outputs that
are both aesthetically compelling and context-sensitive.
","[{'version': 'v1', 'created': 'Fri, 28 Jun 2024 11:58:26 GMT'}, {'version': 'v2', 'created': 'Thu, 4 Jul 2024 15:47:40 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Feb 2025 20:28:02 GMT'}, {'version': 'v4', 'created': 'Thu, 27 Feb 2025 08:36:29 GMT'}]",2025-02-28,"[['He', 'Jun-Yan', ''], ['Cheng', 'Zhi-Qi', ''], ['Li', 'Chenyang', ''], ['Sun', 'Jingdong', ''], ['He', 'Qi', ''], ['Xiang', 'Wangmeng', ''], ['Chen', 'Hanyuan', ''], ['Lan', 'Jin-Peng', ''], ['Lin', 'Xianhui', ''], ['Zhu', 'Kang', ''], ['Luo', 'Bin', ''], ['Geng', 'Yifeng', ''], ['Xie', 'Xuansong', ''], ['Hauptmann', 'Alexander G.', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2407.00047,Archit Patke,"Archit Patke, Dhemath Reddy, Saurabh Jha, Haoran Qiu, Christian Pinto,
  Chandra Narayanaswami, Zbigniew Kalbarczyk, Ravishankar Iyer",Queue management for slo-oriented large language model serving,,,10.1145/3698038.369852,,cs.DC cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large language model (LLM) serving is becoming an increasingly critical
workload for cloud providers. Existing LLM serving systems focus on interactive
requests, such as chatbots and coding assistants, with tight latency SLO
requirements. However, when such systems execute batch requests that have
relaxed SLOs along with interactive requests, it leads to poor multiplexing and
inefficient resource utilization. To address these challenges, we propose QLM,
a queue management system for LLM serving. QLM maintains batch and interactive
requests across different models and SLOs in a request queue. Optimal ordering
of the request queue is critical to maintain SLOs while ensuring high resource
utilization. To generate this optimal ordering, QLM uses a Request Waiting Time
(RWT) Estimator that estimates the waiting times for requests in the request
queue. These estimates are used by a global scheduler to orchestrate LLM
Serving Operations (LSOs) such as request pulling, request eviction, load
balancing, and model swapping. Evaluation on heterogeneous GPU devices and
models with real-world LLM serving dataset shows that QLM improves SLO
attainment by 40-90% and throughput by 20-400% while maintaining or improving
device utilization compared to other state-of-the-art LLM serving systems.
QLM's evaluation is based on the production requirements of a cloud provider.
QLM is publicly available at https://www.github.com/QLM-project/QLM.
","[{'version': 'v1', 'created': 'Wed, 5 Jun 2024 21:17:34 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 17:54:13 GMT'}]",2025-02-26,"[['Patke', 'Archit', ''], ['Reddy', 'Dhemath', ''], ['Jha', 'Saurabh', ''], ['Qiu', 'Haoran', ''], ['Pinto', 'Christian', ''], ['Narayanaswami', 'Chandra', ''], ['Kalbarczyk', 'Zbigniew', ''], ['Iyer', 'Ravishankar', '']]","[{'text': 'Large language model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'chatbots', 'label': 'ChatGPT'}, {'text': 'coding assistants', 'label': 'ChatGPT'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]",Large Language Model,Large language model,1.0
2407.00075,Anton Xue,"Anton Xue, Avishree Khare, Rajeev Alur, Surbhi Goel, Eric Wong","Logicbreaks: A Framework for Understanding Subversion of Rule-based
  Inference",,,,,cs.AI cs.CL cs.CR cs.LG,http://creativecommons.org/licenses/by/4.0/,"  We study how to subvert large language models (LLMs) from following
prompt-specified rules. We first formalize rule-following as inference in
propositional Horn logic, a mathematical system in which rules have the form
""if $P$ and $Q$, then $R$"" for some propositions $P$, $Q$, and $R$. Next, we
prove that although small transformers can faithfully follow such rules,
maliciously crafted prompts can still mislead both theoretical constructions
and models learned from data. Furthermore, we demonstrate that popular attack
algorithms on LLMs find adversarial prompts and induce attention patterns that
align with our theory. Our novel logic-based framework provides a foundation
for studying LLMs in rule-based settings, enabling a formal analysis of tasks
like logical reasoning and jailbreak attacks.
","[{'version': 'v1', 'created': 'Fri, 21 Jun 2024 19:18:16 GMT'}, {'version': 'v2', 'created': 'Tue, 1 Oct 2024 20:42:41 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Feb 2025 19:08:08 GMT'}, {'version': 'v4', 'created': 'Thu, 27 Feb 2025 17:49:33 GMT'}]",2025-02-28,"[['Xue', 'Anton', ''], ['Khare', 'Avishree', ''], ['Alur', 'Rajeev', ''], ['Goel', 'Surbhi', ''], ['Wong', 'Eric', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'small transformers', 'label': 'Transformers'}, {'text': 'maliciously crafted prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'attention patterns', 'label': 'Attention mechanism'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2407.00468,Liang Chen,"Jinsheng Huang, Liang Chen, Taian Guo, Fu Zeng, Yusheng Zhao, Bohan
  Wu, Ye Yuan, Haozhe Zhao, Zhihui Guo, Yichi Zhang, Jingyang Yuan, Wei Ju,
  Luchen Liu, Tianyu Liu, Baobao Chang, Ming Zhang","MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and
  Efficient Evaluation","18 pages, code released at https://github.com/chenllliang/MMEvalPro,
  Homepage at https://mmevalpro.github.io/",,,,cs.CV cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large Multimodal Models (LMMs) exhibit impressive cross-modal understanding
and reasoning abilities, often assessed through multiple-choice questions
(MCQs) that include an image, a question, and several options. However, many
benchmarks used for such evaluations suffer from systematic biases. Remarkably,
Large Language Models (LLMs) without any visual perception capabilities achieve
non-trivial performance, undermining the credibility of these evaluations. To
address this issue while maintaining the efficiency of MCQ evaluations, we
propose MMEvalPro, a benchmark designed to avoid Type-I errors through a
trilogy evaluation pipeline and more rigorous metrics. For each original
question from existing benchmarks, human annotators augment it by creating one
perception question and one knowledge anchor question through a meticulous
annotation process. MMEvalPro comprises $2,138$ question triplets, totaling
$6,414$ distinct questions. Two-thirds of these questions are manually labeled
by human experts, while the rest are sourced from existing benchmarks (MMMU,
ScienceQA, and MathVista). Compared with the existing benchmarks, our
experiments with the latest LLMs and LMMs demonstrate that MMEvalPro is more
challenging (the best LMM lags behind human performance by $31.73\%$, compared
to an average gap of $8.03\%$ in previous benchmarks) and more trustworthy (the
best LLM trails the best LMM by $23.09\%$, whereas the gap for previous
benchmarks is just $14.64\%$). Our in-depth analysis explains the reason for
the large performance gap and justifies the trustworthiness of evaluation,
underscoring its significant potential for advancing future research.
","[{'version': 'v1', 'created': 'Sat, 29 Jun 2024 15:28:45 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 15:10:56 GMT'}]",2025-02-28,"[['Huang', 'Jinsheng', ''], ['Chen', 'Liang', ''], ['Guo', 'Taian', ''], ['Zeng', 'Fu', ''], ['Zhao', 'Yusheng', ''], ['Wu', 'Bohan', ''], ['Yuan', 'Ye', ''], ['Zhao', 'Haozhe', ''], ['Guo', 'Zhihui', ''], ['Zhang', 'Yichi', ''], ['Yuan', 'Jingyang', ''], ['Ju', 'Wei', ''], ['Liu', 'Luchen', ''], ['Liu', 'Tianyu', ''], ['Chang', 'Baobao', ''], ['Zhang', 'Ming', '']]","[{'text': 'Large Multimodal Models', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ScienceQA', 'label': 'Open-source LLMs'}, {'text': 'LMMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2407.02936,Zike Yuan,"Zike Yuan, Ming Liu, Hui Wang, Bing Qin","GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large
  Language Models",,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Evaluating the graph comprehension and reasoning abilities of Large Language
Models (LLMs) is challenging and often incomplete. Existing benchmarks focus
primarily on pure graph understanding, lacking a comprehensive evaluation
across all graph types and detailed capability definitions. This paper presents
GraCoRe, a benchmark for systematically assessing LLMs' graph comprehension and
reasoning. GraCoRe uses a three-tier hierarchical taxonomy to categorize and
test models on pure graph and heterogeneous graphs, subdividing capabilities
into 10 distinct areas tested through 19 tasks. Our benchmark includes 11
datasets with 5,140 graphs of varying complexity. We evaluate four
closed-source and eight open-source LLMs, conducting thorough analyses from
both ability and task perspectives. Key findings reveal that OpenAI o1 model
has amazing comprehension and reasoning capabilities, semantic enrichment
enhances reasoning performance, node ordering impacts task success, and the
ability to process longer texts does not necessarily improve graph
comprehension or reasoning.GraCoRe is open-sourced at
https://github.com/ZIKEYUAN/GraCoRe
","[{'version': 'v1', 'created': 'Wed, 3 Jul 2024 09:12:38 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 09:17:32 GMT'}]",2025-02-27,"[['Yuan', 'Zike', ''], ['Liu', 'Ming', ''], ['Wang', 'Hui', ''], ['Qin', 'Bing', '']]","[{'text': 'Large Language\nModels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'semantic enrichment', 'label': 'Embedding'}]",Large Language Model,"Large Language
Models",0.9664971828460693
2407.06172,Jin Peng Zhou,"Jin Peng Zhou, Christian K. Belardi, Ruihan Wu, Travis Zhang, Carla P.
  Gomes, Wen Sun, Kilian Q. Weinberger",On Speeding Up Language Model Evaluation,ICLR 2025,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Developing prompt-based methods with Large Language Models (LLMs) requires
making numerous decisions, which give rise to a combinatorial search problem
over hyper-parameters. This exhaustive evaluation can be time-consuming and
costly. In this paper, we propose an $\textit{adaptive}$ approach to explore
this space. We are exploiting the fact that often only few samples are needed
to identify clearly superior or inferior settings, and that many evaluation
tests are highly correlated. We lean on multi-armed bandits to sequentially
identify the next (method, validation sample)-pair to evaluate and utilize
low-rank matrix factorization to fill in missing evaluations. We carefully
assess the efficacy of our approach on several competitive benchmark problems
and show that it can identify the top-performing method using only 5-15% of the
typical resources -- resulting in 85-95% LLM cost savings. Our code is
available at https://github.com/kilian-group/banditeval.
","[{'version': 'v1', 'created': 'Mon, 8 Jul 2024 17:48:42 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Aug 2024 22:31:35 GMT'}, {'version': 'v3', 'created': 'Wed, 26 Feb 2025 21:53:59 GMT'}]",2025-02-28,"[['Zhou', 'Jin Peng', ''], ['Belardi', 'Christian K.', ''], ['Wu', 'Ruihan', ''], ['Zhang', 'Travis', ''], ['Gomes', 'Carla P.', ''], ['Sun', 'Wen', ''], ['Weinberger', 'Kilian Q.', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2407.13522,Abhishek Kumar Singh,"Abhishek Kumar Singh, Vishwajeet kumar, Rudra Murthy, Jaydeep Sen,
  Ashish Mittal, Ganesh Ramakrishnan","INDIC QA BENCHMARK: A Multilingual Benchmark to Evaluate Question
  Answering capability of LLMs for Indic Languages",,,,,cs.LG,http://creativecommons.org/publicdomain/zero/1.0/,"  Large Language Models (LLMs) perform well on unseen tasks in English, but
their abilities in non English languages are less explored due to limited
benchmarks and training data. To bridge this gap, we introduce the Indic QA
Benchmark, a large dataset for context grounded question answering in 11 major
Indian languages, covering both extractive and abstractive tasks. Evaluations
of multilingual LLMs, including instruction finetuned versions, revealed weak
performance in low resource languages due to a strong English language bias in
their training data. We also investigated the Translate Test paradigm,where
inputs are translated to English for processing and the results are translated
back into the source language for output. This approach outperformed
multilingual LLMs, particularly in low resource settings. By releasing Indic
QA, we aim to promote further research into LLMs question answering
capabilities in low resource languages. This benchmark offers a critical
resource to address existing limitations and foster multilingual understanding.
","[{'version': 'v1', 'created': 'Thu, 18 Jul 2024 13:57:16 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 05:37:48 GMT'}]",2025-02-25,"[['Singh', 'Abhishek Kumar', ''], ['kumar', 'Vishwajeet', ''], ['Murthy', 'Rudra', ''], ['Sen', 'Jaydeep', ''], ['Mittal', 'Ashish', ''], ['Ramakrishnan', 'Ganesh', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2407.14845,Arun Verma,"Ze Yu Zhang, Arun Verma, Finale Doshi-Velez, Bryan Kian Hsiang Low","Understanding the Relationship between Prompts and Response Uncertainty
  in Large Language Models","22 pages, Preprint",,,,cs.LG cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) are widely used in decision-making, but their
reliability, especially in critical tasks like healthcare, is not
well-established. Therefore, understanding how LLMs reason and make decisions
is crucial for their safe deployment. This paper investigates how the
uncertainty of responses generated by LLMs relates to the information provided
in the input prompt. Leveraging the insight that LLMs learn to infer latent
concepts during pretraining, we propose a prompt-response concept model that
explains how LLMs generate responses and helps understand the relationship
between prompts and response uncertainty. We show that the uncertainty
decreases as the prompt's informativeness increases, similar to epistemic
uncertainty. Our detailed experimental results on real-world datasets validate
our proposed model.
","[{'version': 'v1', 'created': 'Sat, 20 Jul 2024 11:19:58 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Aug 2024 02:23:12 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 17:06:21 GMT'}]",2025-02-25,"[['Zhang', 'Ze Yu', ''], ['Verma', 'Arun', ''], ['Doshi-Velez', 'Finale', ''], ['Low', 'Bryan Kian Hsiang', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompt', 'label': 'Prompting'}]",Large Language Model,Large language models,0.9664971828460693
2407.15073,Hao Duong Le,"Hao Duong Le, Xin Xia and Zhang Chen",Multi-Agent Causal Discovery Using Large Language Models,,,,,cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Causal discovery aims to identify causal relationships between variables and
is a critical research area in machine learning. Traditional methods focus on
statistical or machine learning algorithms to uncover causal links from
structured data, often overlooking the valuable contextual information provided
by metadata. Large language models (LLMs) have shown promise in creating
unified causal discovery frameworks by incorporating both structured data and
metadata. However, their potential in multi-agent settings remains largely
unexplored. To address this gap, we introduce the Multi-Agent Causal Discovery
Framework (MAC), which consists of two key modules: the Debate-Coding Module
(DCM) and the Meta-Debate Module (MDM). The DCM begins with a multi-agent
debating and coding process, where agents use both structured data and metadata
to collaboratively select the most suitable statistical causal discovery (SCD)
method. The selected SCD is then applied to the structured data to generate an
initial causal graph. This causal graph is transformed into causal metadata
through the Meta Fusion mechanism. With all the metadata, MDM then refines the
causal structure by leveraging a multi-agent debating framework. Extensive
experiments across five datasets demonstrate that MAC outperforms both
traditional statistical causal discovery methods and existing LLM-based
approaches, achieving state-of-the-art performance.
","[{'version': 'v1', 'created': 'Sun, 21 Jul 2024 06:21:47 GMT'}, {'version': 'v2', 'created': 'Thu, 10 Oct 2024 02:48:42 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 02:47:56 GMT'}]",2025-02-25,"[['Le', 'Hao Duong', ''], ['Xia', 'Xin', ''], ['Chen', 'Zhang', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Meta Fusion mechanism', 'label': 'Embedding'}, {'text': 'MDM', 'label': 'LLM'}, {'text': 'MAC', 'label': 'LLM'}, {'text': 'existing LLM-based\napproaches', 'label': 'LLM'}]",Large Language Model,Large language models,0.9664971828460693
2408.07413,Chenhui Hu,"Chenhui Hu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao","Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge
  Editing for Large Language Models",To be published in AAAI 2025 (Oral),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge editing aims to update outdated or incorrect knowledge in large
language models (LLMs). However, current knowledge editing methods have limited
scalability for lifelong editing. This study explores the fundamental reason
why knowledge editing fails in lifelong editing. We begin with the closed-form
solution derived from linear associative memory, which underpins
state-of-the-art knowledge editing methods. We extend the solution from single
editing to lifelong editing, and through rigorous mathematical derivation,
identify an interference term in the final solution, suggesting that editing
knowledge may impact irrelevant knowledge. Further analysis of the interference
term reveals a close relationship with superposition between knowledge
representations. When knowledge superposition does not exist in language
models, the interference term vanishes, allowing for lossless knowledge
editing. Experiments across numerous language models reveal that knowledge
superposition is universal, exhibiting high kurtosis, zero mean, and
heavy-tailed distributions with clear scaling laws. Ultimately, by combining
theory and experiments, we demonstrate that knowledge superposition is the
fundamental reason for the failure of lifelong editing. Moreover, this is the
first study to investigate knowledge editing from the perspective of
superposition and provides a comprehensive observation of superposition across
numerous real-world language models. Code available at
https://github.com/ChenhuiHu/knowledge_in_superposition.
","[{'version': 'v1', 'created': 'Wed, 14 Aug 2024 09:43:32 GMT'}, {'version': 'v2', 'created': 'Sun, 12 Jan 2025 06:07:15 GMT'}, {'version': 'v3', 'created': 'Wed, 26 Feb 2025 09:13:06 GMT'}]",2025-02-27,"[['Hu', 'Chenhui', ''], ['Cao', 'Pengfei', ''], ['Chen', 'Yubo', ''], ['Liu', 'Kang', ''], ['Zhao', 'Jun', '']]","[{'text': 'large\nlanguage models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'clear scaling laws', 'label': 'Scaling law'}]",Large Language Model,"large
language models",0.9664971828460693
2408.10573,Junhao Chen,Junhao Chen and Bowen Wang and Zhouqiang Jiang and Yuta Nakashima,"Putting People in LLMs' Shoes: Generating Better Answers via Question
  Rewriter","7 pages, 4 figures, 5 tables and accepted at AAAI 2025 Main
  Conference",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have demonstrated significant capabilities,
particularly in the domain of question answering (QA). However, their
effectiveness in QA is often undermined by the vagueness of user questions. To
address this issue, we introduce single-round instance-level prompt
optimization, referred to as question rewriter. By enhancing the
intelligibility of human questions for black-box LLMs, our question rewriter
improves the quality of generated answers. The rewriter is optimized using
direct preference optimization based on feedback collected from automatic
criteria for evaluating generated answers; therefore, its training does not
require costly human annotations. The experiments across multiple black-box
LLMs and long-form question answering (LFQA) datasets demonstrate the efficacy
of our method. This paper provides a practical framework for training question
rewriters and sets a precedent for future explorations in prompt optimization
within LFQA tasks. Code is available at
https://github.com/3244we/Question-Rewriter.
","[{'version': 'v1', 'created': 'Tue, 20 Aug 2024 06:24:47 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 03:13:27 GMT'}]",2025-02-26,"[['Chen', 'Junhao', ''], ['Wang', 'Bowen', ''], ['Jiang', 'Zhouqiang', ''], ['Nakashima', 'Yuta', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'single-round instance-level prompt\noptimization', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'direct preference optimization', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompt optimization', 'label': 'Prompting'}]",Large Language Model,Large Language Models,0.9664971828460693
2408.10593,Eui Jun Hwang,"Eui Jun Hwang, Sukmin Cho, Junmyeong Lee, Jong C. Park","An Efficient Sign Language Translation Using Spatial Configuration and
  Motion Dynamics with LLMs",Accepted to NAACL 2025 main,,,,cs.CL cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Gloss-free Sign Language Translation (SLT) converts sign videos directly into
spoken language sentences without relying on glosses. Recently, Large Language
Models (LLMs) have shown remarkable translation performance in gloss-free
methods by harnessing their powerful natural language generation capabilities.
However, these methods often rely on domain-specific fine-tuning of visual
encoders to achieve optimal results. By contrast, this paper emphasizes the
importance of capturing the spatial configurations and motion dynamics inherent
in sign language. With this in mind, we introduce Spatial and Motion-based Sign
Language Translation (SpaMo), a novel LLM-based SLT framework. The core idea of
SpaMo is simple yet effective. We first extract spatial and motion features
using off-the-shelf visual encoders and then input these features into an LLM
with a language prompt. Additionally, we employ a visual-text alignment process
as a warm-up before the SLT supervision. Our experiments demonstrate that SpaMo
achieves state-of-the-art performance on two popular datasets, PHOENIX14T and
How2Sign.
","[{'version': 'v1', 'created': 'Tue, 20 Aug 2024 07:10:40 GMT'}, {'version': 'v2', 'created': 'Sun, 15 Dec 2024 06:18:53 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 06:04:45 GMT'}]",2025-02-25,"[['Hwang', 'Eui Jun', ''], ['Cho', 'Sukmin', ''], ['Lee', 'Junmyeong', ''], ['Park', 'Jong C.', '']]","[{'text': 'Large Language\nModels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'domain-specific fine-tuning', 'label': 'Fine-tuning'}, {'text': 'language prompt', 'label': 'Prompting'}]",Large Language Model,"Large Language
Models",0.9664971828460693
2408.11843,Ruizhe Chen,"Ruizhe Chen, Yichen Li, Jianfei Yang, Joey Tianyi Zhou, Jian Wu,
  Zuozhu Liu",Identifying and Mitigating Social Bias Knowledge in Language Models,"NAACL 2025 Findings. arXiv admin note: substantial text overlap with
  arXiv:2405.09341",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Generating fair and accurate predictions plays a pivotal role in deploying
large language models (LLMs) in the real world. However, existing debiasing
methods inevitably generate unfair or incorrect predictions as they are
designed and evaluated to achieve parity across different social groups but
leave aside individual commonsense facts, resulting in modified knowledge that
elicits unreasonable or undesired predictions. In this paper, we first
establish a new bias mitigation benchmark, BiaScope, which systematically
assesses performance by leveraging newly constructed datasets and metrics on
knowledge retention and generalization. Then, we propose a novel debiasing
approach, Fairness Stamp (FAST), which enables fine-grained calibration of
individual social biases. FAST identifies the decisive layer responsible for
storing social biases and then calibrates its outputs by integrating a small
modular network, considering both bias mitigation and knowledge-preserving
demands. Comprehensive experiments demonstrate that FAST surpasses
state-of-the-art baselines with superior debiasing performance while not
compromising the overall model capability for knowledge retention and
downstream predictions. This highlights the potential of fine-grained debiasing
strategies to achieve fairness in LLMs.
","[{'version': 'v1', 'created': 'Wed, 7 Aug 2024 17:14:58 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 10:11:06 GMT'}]",2025-02-28,"[['Chen', 'Ruizhe', ''], ['Li', 'Yichen', ''], ['Yang', 'Jianfei', ''], ['Zhou', 'Joey Tianyi', ''], ['Wu', 'Jian', ''], ['Liu', 'Zuozhu', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'fine-grained calibration', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2408.13704,Yicheng Wang,"Yicheng Wang, Jiayi Yuan, Yu-Neng Chuang, Zhuoer Wang, Yingchi Liu,
  Mark Cusick, Param Kulkarni, Zhengping Ji, Yasser Ibrahim, Xia Hu",DHP Benchmark: Are LLMs Good NLG Evaluators?,,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) are increasingly serving as evaluators in
Natural Language Generation (NLG) tasks; this is often referred to as
``LLM-as-a-judge'' paradigm. However, the capabilities of LLMs in evaluating
NLG quality remain underexplored. Current studies depend on human assessments
and simple metrics that fail to capture the discernment of LLMs across diverse
NLG tasks. To address this gap, we propose the Discernment of Hierarchical
Perturbation (DHP) benchmarking framework, which provides quantitative
discernment scores for LLMs. This framework leverages hierarchically perturbed
text data and statistical tests to systematically measure the NLG evaluation
capabilities of LLMs. We re-established six evaluation datasets for this
benchmark, covering four NLG tasks: Summarization, Story Completion, Question
Answering, and Translation. Our comprehensive benchmarking of five major LLM
families provides critical insight into their strengths and limitations as NLG
evaluators. Our dataset is available at
https://huggingface.co/datasets/YCWANGVINCE/DHP_Benchmark.
","[{'version': 'v1', 'created': 'Sun, 25 Aug 2024 02:01:38 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 01:51:06 GMT'}]",2025-02-26,"[['Wang', 'Yicheng', ''], ['Yuan', 'Jiayi', ''], ['Chuang', 'Yu-Neng', ''], ['Wang', 'Zhuoer', ''], ['Liu', 'Yingchi', ''], ['Cusick', 'Mark', ''], ['Kulkarni', 'Param', ''], ['Ji', 'Zhengping', ''], ['Ibrahim', 'Yasser', ''], ['Hu', 'Xia', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
