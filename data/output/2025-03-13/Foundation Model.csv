id,submitter,authors,title,comments,journal-ref,doi,report-no,categories,license,abstract,versions,update_date,authors_parsed,extracted_entities,assigned_concept,matched_keyword,similarity_score
2403.07066,Benedikt Maier,"Philip Harris, Michael Kagan, Jeffrey Krupa, Benedikt Maier, Nathaniel
  Woodward","Re-Simulation-based Self-Supervised Learning for Pre-Training Foundation
  Models","14 pages, 8 figures","Phys. Rev. D 111 (2025) 3, 032010",10.1103/PhysRevD.111.032010,,hep-ph cs.LG hep-ex,http://creativecommons.org/licenses/by/4.0/,"  Self-Supervised Learning (SSL) is at the core of training modern large
machine learning models, providing a scheme for learning powerful
representations that can be used in a variety of downstream tasks. However, SSL
strategies must be adapted to the type of training data and downstream tasks
required. We propose RS3L (""Re-simulation-based self-supervised representation
learning""), a novel simulation-based SSL strategy that employs a method of
re-simulation to drive data augmentation for contrastive learning in the
physical sciences, particularly, in fields that rely on stochastic simulators.
By intervening in the middle of the simulation process and re-running
simulation components downstream of the intervention, we generate multiple
realizations of an event, thus producing a set of augmentations covering all
physics-driven variations available in the simulator. Using experiments from
high-energy physics, we explore how this strategy may enable the development of
a foundation model; we show how RS3L pre-training enables powerful performance
in downstream tasks such as discrimination of a variety of objects and
uncertainty mitigation. In addition to our results, we make the RS3L dataset
publicly available for further studies on how to improve SSL strategies.
","[{'version': 'v1', 'created': 'Mon, 11 Mar 2024 18:00:47 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 00:08:55 GMT'}]",2025-02-26,"[['Harris', 'Philip', ''], ['Kagan', 'Michael', ''], ['Krupa', 'Jeffrey', ''], ['Maier', 'Benedikt', ''], ['Woodward', 'Nathaniel', '']]","[{'text': 'Self-Supervised Learning', 'label': 'Few-shot Learning'}, {'text': 'foundation model', 'label': 'Foundation Model'}]",Foundation Model,foundation model,1.0
2405.17842,Akio Hayakawa,"Akio Hayakawa, Masato Ishii, Takashi Shibuya, Yuki Mitsufuji","MMDisCo: Multi-Modal Discriminator-Guided Cooperative Diffusion for
  Joint Audio and Video Generation",ICLR 2025,,,,cs.CV cs.LG cs.MM cs.SD eess.AS,http://creativecommons.org/licenses/by/4.0/,"  This study aims to construct an audio-video generative model with minimal
computational cost by leveraging pre-trained single-modal generative models for
audio and video. To achieve this, we propose a novel method that guides
single-modal models to cooperatively generate well-aligned samples across
modalities. Specifically, given two pre-trained base diffusion models, we train
a lightweight joint guidance module to adjust scores separately estimated by
the base models to match the score of joint distribution over audio and video.
We show that this guidance can be computed using the gradient of the optimal
discriminator, which distinguishes real audio-video pairs from fake ones
independently generated by the base models. Based on this analysis, we
construct a joint guidance module by training this discriminator. Additionally,
we adopt a loss function to stabilize the discriminator's gradient and make it
work as a noise estimator, as in standard diffusion models. Empirical
evaluations on several benchmark datasets demonstrate that our method improves
both single-modal fidelity and multimodal alignment with relatively few
parameters. The code is available at: https://github.com/SonyResearch/MMDisCo.
","[{'version': 'v1', 'created': 'Tue, 28 May 2024 05:43:03 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 09:34:26 GMT'}]",2025-02-26,"[['Hayakawa', 'Akio', ''], ['Ishii', 'Masato', ''], ['Shibuya', 'Takashi', ''], ['Mitsufuji', 'Yuki', '']]","[{'text': 'base models', 'label': 'Foundation Model'}, {'text': 'base models', 'label': 'Foundation Model'}]",Foundation Model,base models,0.5099614858627319
2406.04508,Dujian Ding,"Dujian Ding, Bicheng Xu, Laks V.S. Lakshmanan","OCCAM: Towards Cost-Efficient and Accuracy-Aware Classification
  Inference",ICLR 2025 (main conference),,,,cs.CV cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Classification tasks play a fundamental role in various applications,
spanning domains such as healthcare, natural language processing and computer
vision. With the growing popularity and capacity of machine learning models,
people can easily access trained classifiers as a service online or offline.
However, model use comes with a cost and classifiers of higher capacity (such
as large foundation models) usually incur higher inference costs. To harness
the respective strengths of different classifiers, we propose a principled
approach, OCCAM, to compute the best classifier assignment strategy over
classification queries (termed as the optimal model portfolio) so that the
aggregated accuracy is maximized, under user-specified cost budgets. Our
approach uses an unbiased and low-variance accuracy estimator and effectively
computes the optimal solution by solving an integer linear programming problem.
On a variety of real-world datasets, OCCAM achieves 40% cost reduction with
little to no accuracy drop.
","[{'version': 'v1', 'created': 'Thu, 6 Jun 2024 21:05:39 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 03:15:20 GMT'}]",2025-02-26,"[['Ding', 'Dujian', ''], ['Xu', 'Bicheng', ''], ['Lakshmanan', 'Laks V. S.', '']]","[{'text': 'large foundation models', 'label': 'Foundation Model'}]",Foundation Model,large foundation models,0.8749241828918457
