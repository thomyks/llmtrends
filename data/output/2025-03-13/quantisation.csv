id,submitter,authors,title,comments,journal-ref,doi,report-no,categories,license,abstract,versions,update_date,authors_parsed,extracted_entities,assigned_concept,matched_keyword,similarity_score
1804.10646,Ben Webster,Michael McBreen and Ben Webster,Homological Mirror Symmetry for Hypertoric Varieties I,41 pages. v4: Final published version,Geom. Topol. 28 (2024) 1005-1063,10.2140/gt.2024.28.1005,,math.AG math.RT math.SG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider homological mirror symmetry in the context of hypertoric
varieties, showing that appropriate categories of B-branes (that is, coherent
sheaves) on an additive hypertoric variety match a category of A-branes on a
Dolbeault hypertoric manifold for the same underlying combinatorial data. For
technical reasons, the category of A-branes we consider is the modules over a
deformation quantization (that is, DQ-modules). We consider objects in this
category equipped with an analogue of a Hodge structure, which corresponds to a
$\mathbb{G}_m$-action on the dual side of the mirror symmetry.
  This result is based on hands-on calculations in both categories. We analyze
coherent sheaves by constructing a tilting generator, using the characteristic
$p$ approach of Kaledin; the result is a sum of line bundles, which can be
described using a simple combinatorial rule. The endomorphism algebra $H$ of
this tilting generator has a simple quadratic presentation in the grading
induced by $\mathbb{G}_m$-equivariance. In fact, we can confirm it is Koszul,
and compute its Koszul dual $H^!$.
  We then show that this same algebra appears as an Ext-algebra of simple
A-branes in a Dolbeault hypertoric manifold. The $\mathbb{G}_m$-equivariant
grading on coherent sheaves matches a Hodge grading in this category.
","[{'version': 'v1', 'created': 'Fri, 27 Apr 2018 18:52:14 GMT'}, {'version': 'v2', 'created': 'Fri, 19 Oct 2018 19:19:20 GMT'}, {'version': 'v3', 'created': 'Sat, 2 Oct 2021 03:11:14 GMT'}, {'version': 'v4', 'created': 'Thu, 27 Feb 2025 15:44:11 GMT'}]",2025-02-28,"[['McBreen', 'Michael', ''], ['Webster', 'Ben', '']]","[{'text': 'deformation quantization', 'label': 'quantisation'}]",quantisation,deformation quantization,0.6073967218399048
2212.05948,Farhad Shirani Chaharsooghi,"Marian Temprana Alonso, Xuyang Liu, Hamidreza Aghasi, Farhad Shirani",Non-Linear Analog Processing in MIMO Systems with Coarse Quantization,arXiv admin note: substantial text overlap with arXiv:2208.04450,,,,cs.IT cs.SY eess.SP eess.SY math.IT,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Analog to digital converters (ADCs) are a major contributor to the power
consumption of multiple-input multiple-output (MIMO) receivers in large
bandwidth millimeter-wave systems. Prior works have considered two mitigating
solutions to reduce the ADC power consumption: i) decreasing the number of ADCs
via analog and hybrid beamforming, and ii) decreasing the ADC resolution, i.e.,
utilizing one-bit and few-bit ADCs. These mitigating solutions lead to
performance loss in terms of achievable rates due to increased quantization
error. In this work, the use of nonlinear analog operators such as envelope
detectors and polynomial operators, prior to sampling and quantization is
considered, as a way to reduce the aforementioned rate-loss. The receiver
architecture consists of linear combiners, nonlinear analog operators, and
few-bit ADCs. The fundamental performance limits of the resulting communication
system, in terms of achievable rates, are investigated under various
assumptions on the set of implementable analog operators. Extensive numerical
evaluations are provided to evaluate the set of achievable rates and the power
consumption of the proposed receiver architectures. Circuit simulations and
measurement results, based on both 22 nm FDSOI CMOS technology and 65 nm Bulk
CMOS transistor technologies, are provided to justify the power efficiency of
the proposed receiver architectures.
","[{'version': 'v1', 'created': 'Mon, 12 Dec 2022 15:04:43 GMT'}, {'version': 'v2', 'created': 'Fri, 15 Mar 2024 19:47:37 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 02:50:39 GMT'}]",2025-02-25,"[['Alonso', 'Marian Temprana', ''], ['Liu', 'Xuyang', ''], ['Aghasi', 'Hamidreza', ''], ['Shirani', 'Farhad', '']]","[{'text': 'quantization\nerror', 'label': 'quantisation'}, {'text': 'quantization', 'label': 'quantisation'}]",quantisation,quantization,0.813445508480072
2303.14731,Mrinal Kanti Roychowdhury,"Amit Priyadarshi, Mrinal K. Roychowdhury, Manuj Verma","Quantization dimensions for inhomogeneous bi-Lipschitz Iterated Function
  Systems",,,,,math.PR,http://creativecommons.org/licenses/by/4.0/,"  Let $\nu$ be a Borel probability measure on a $d$-dimensional Euclidean space
$\mathbb{R}^d$, $d\geq 1$, with a compact support, and let $(p_0, p_1, p_2,
\ldots, p_N)$ be a probability vector with $p_j>0$ for $0\leq j\leq N$. Let
$\{S_j: 1\leq j\leq N\}$ be a set of contractive mappings on $\mathbb{R}^d$.
Then, a Borel probability measure $\mu$ on $\mathbb R^d$ such that
$\mu=\sum_{j=1}^N p_j\mu\circ S_j^{-1}+p_0\nu$ is called an inhomogeneous
measure, also known as a condensation measure on $\mathbb{R}^d$. For a given
$r\in (0, +\infty)$, the quantization dimension of order $r$, if it exists,
denoted by $D_r(\mu)$, of a Borel probability measure $\mu$ on $\mathbb{R}^d$
represents the speed at which the $n$th quantization error of order $r$
approaches to zero as the number of elements $n$ in an optimal set of $n$-means
for $\mu$ tends to infinity. In this paper, we investigate the quantization
dimension for such a condensation measure.
","[{'version': 'v1', 'created': 'Sun, 26 Mar 2023 14:22:15 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 18:46:48 GMT'}]",2025-02-25,"[['Priyadarshi', 'Amit', ''], ['Roychowdhury', 'Mrinal K.', ''], ['Verma', 'Manuj', '']]","[{'text': 'quantization dimension', 'label': 'quantisation'}, {'text': 'quantization\ndimension', 'label': 'quantisation'}]",quantisation,quantization dimension,0.6371288299560547
2307.08423,Xuan Zhang,"Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen
  Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, Keir Adams, Maurice Weiler,
  Xiner Li, Tianfan Fu, Yucheng Wang, Alex Strasser, Haiyang Yu, YuQing Xie,
  Xiang Fu, Shenglong Xu, Yi Liu, Yuanqi Du, Alexandra Saxton, Hongyi Ling,
  Hannah Lawrence, Hannes St\""ark, Shurui Gui, Carl Edwards, Nicholas Gao,
  Adriana Ladera, Tailin Wu, Elyssa F. Hofgard, Aria Mansouri Tehrani, Rui
  Wang, Ameya Daigavane, Montgomery Bohde, Jerry Kurtin, Qian Huang, Tuong
  Phung, Minkai Xu, Chaitanya K. Joshi, Simon V. Mathis, Kamyar
  Azizzadenesheli, Ada Fang, Al\'an Aspuru-Guzik, Erik Bekkers, Michael
  Bronstein, Marinka Zitnik, Anima Anandkumar, Stefano Ermon, Pietro Li\`o,
  Rose Yu, Stephan G\""unnemann, Jure Leskovec, Heng Ji, Jimeng Sun, Regina
  Barzilay, Tommi Jaakkola, Connor W. Coley, Xiaoning Qian, Xiaofeng Qian, Tess
  Smidt, Shuiwang Ji","Artificial Intelligence for Science in Quantum, Atomistic, and Continuum
  Systems",,,,,cs.LG physics.comp-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Advances in artificial intelligence (AI) are fueling a new paradigm of
discoveries in natural sciences. Today, AI has started to advance natural
sciences by improving, accelerating, and enabling our understanding of natural
phenomena at a wide range of spatial and temporal scales, giving rise to a new
area of research known as AI for science (AI4Science). Being an emerging
research paradigm, AI4Science is unique in that it is an enormous and highly
interdisciplinary area. Thus, a unified and technical treatment of this field
is needed yet challenging. This work aims to provide a technically thorough
account of a subarea of AI4Science; namely, AI for quantum, atomistic, and
continuum systems. These areas aim at understanding the physical world from the
subatomic (wavefunctions and electron density), atomic (molecules, proteins,
materials, and interactions), to macro (fluids, climate, and subsurface) scales
and form an important subarea of AI4Science. A unique advantage of focusing on
these areas is that they largely share a common set of challenges, thereby
allowing a unified and foundational treatment. A key common challenge is how to
capture physics first principles, especially symmetries, in natural systems by
deep learning methods. We provide an in-depth yet intuitive account of
techniques to achieve equivariance to symmetry transformations. We also discuss
other common technical challenges, including explainability,
out-of-distribution generalization, knowledge transfer with foundation and
large language models, and uncertainty quantification. To facilitate learning
and education, we provide categorized lists of resources that we found to be
useful. We strive to be thorough and unified and hope this initial effort may
trigger more community interests and efforts to further advance AI4Science.
","[{'version': 'v1', 'created': 'Mon, 17 Jul 2023 12:14:14 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Nov 2023 18:25:03 GMT'}, {'version': 'v3', 'created': 'Sun, 13 Oct 2024 15:56:41 GMT'}, {'version': 'v4', 'created': 'Wed, 26 Feb 2025 18:45:58 GMT'}]",2025-02-27,"[['Zhang', 'Xuan', ''], ['Wang', 'Limei', ''], ['Helwig', 'Jacob', ''], ['Luo', 'Youzhi', ''], ['Fu', 'Cong', ''], ['Xie', 'Yaochen', ''], ['Liu', 'Meng', ''], ['Lin', 'Yuchao', ''], ['Xu', 'Zhao', ''], ['Yan', 'Keqiang', ''], ['Adams', 'Keir', ''], ['Weiler', 'Maurice', ''], ['Li', 'Xiner', ''], ['Fu', 'Tianfan', ''], ['Wang', 'Yucheng', ''], ['Strasser', 'Alex', ''], ['Yu', 'Haiyang', ''], ['Xie', 'YuQing', ''], ['Fu', 'Xiang', ''], ['Xu', 'Shenglong', ''], ['Liu', 'Yi', ''], ['Du', 'Yuanqi', ''], ['Saxton', 'Alexandra', ''], ['Ling', 'Hongyi', ''], ['Lawrence', 'Hannah', ''], ['St√§rk', 'Hannes', ''], ['Gui', 'Shurui', ''], ['Edwards', 'Carl', ''], ['Gao', 'Nicholas', ''], ['Ladera', 'Adriana', ''], ['Wu', 'Tailin', ''], ['Hofgard', 'Elyssa F.', ''], ['Tehrani', 'Aria Mansouri', ''], ['Wang', 'Rui', ''], ['Daigavane', 'Ameya', ''], ['Bohde', 'Montgomery', ''], ['Kurtin', 'Jerry', ''], ['Huang', 'Qian', ''], ['Phung', 'Tuong', ''], ['Xu', 'Minkai', ''], ['Joshi', 'Chaitanya K.', ''], ['Mathis', 'Simon V.', ''], ['Azizzadenesheli', 'Kamyar', ''], ['Fang', 'Ada', ''], ['Aspuru-Guzik', 'Al√°n', ''], ['Bekkers', 'Erik', ''], ['Bronstein', 'Michael', ''], ['Zitnik', 'Marinka', ''], ['Anandkumar', 'Anima', ''], ['Ermon', 'Stefano', ''], ['Li√≤', 'Pietro', ''], ['Yu', 'Rose', ''], ['G√ºnnemann', 'Stephan', ''], ['Leskovec', 'Jure', ''], ['Ji', 'Heng', ''], ['Sun', 'Jimeng', ''], ['Barzilay', 'Regina', ''], ['Jaakkola', 'Tommi', ''], ['Coley', 'Connor W.', ''], ['Qian', 'Xiaoning', ''], ['Qian', 'Xiaofeng', ''], ['Smidt', 'Tess', ''], ['Ji', 'Shuiwang', '']]","[{'text': 'uncertainty quantification', 'label': 'quantisation'}]",quantisation,uncertainty quantification,0.571454644203186
2312.07950,Xiaoyu Liu,"Xin Ding, Xiaoyu Liu, Zhijun Tu, Yun Zhang, Wei Li, Jie Hu, Hanting
  Chen, Yehui Tang, Zhiwei Xiong, Baoqun Yin, Yunhe Wang",CBQ: Cross-Block Quantization for Large Language Models,,,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Post-training quantization (PTQ) has played a key role in compressing large
language models (LLMs) with ultra-low costs. However, existing PTQ methods only
focus on handling the outliers within one layer or one block, which ignores the
dependency of blocks and leads to severe performance degradation in low-bit
settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ
method for LLMs. CBQ employs a cross-block dependency using a homologous
reconstruction scheme, establishing long-range dependencies across multiple
blocks to minimize error accumulation. Furthermore, CBQ incorporates a
coarse-to-fine preprocessing (CFP) strategy for suppressing weight and
activation outliers, coupled with an adaptive LoRA-Rounding technique for
precise weight quantization. These innovations enable CBQ to not only handle
extreme outliers effectively but also improve overall quantization accuracy.
Extensive experiments show that CBQ achieves superior low-bit quantization
(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across
various LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model
within only 4.3 hours on a single GPU, achieving a commendable tradeoff between
performance and quantization efficiency.
","[{'version': 'v1', 'created': 'Wed, 13 Dec 2023 07:56:27 GMT'}, {'version': 'v2', 'created': 'Fri, 2 Feb 2024 06:55:52 GMT'}, {'version': 'v3', 'created': 'Wed, 27 Mar 2024 04:51:51 GMT'}, {'version': 'v4', 'created': 'Mon, 15 Apr 2024 10:57:16 GMT'}, {'version': 'v5', 'created': 'Tue, 25 Feb 2025 09:14:18 GMT'}]",2025-02-26,"[['Ding', 'Xin', ''], ['Liu', 'Xiaoyu', ''], ['Tu', 'Zhijun', ''], ['Zhang', 'Yun', ''], ['Li', 'Wei', ''], ['Hu', 'Jie', ''], ['Chen', 'Hanting', ''], ['Tang', 'Yehui', ''], ['Xiong', 'Zhiwei', ''], ['Yin', 'Baoqun', ''], ['Wang', 'Yunhe', '']]","[{'text': 'Post-training quantization', 'label': 'quantisation'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'precise weight quantization', 'label': 'quantisation'}, {'text': 'low-bit quantization', 'label': 'quantisation'}, {'text': 'W4A4', 'label': 'GPT'}, {'text': 'W4A8', 'label': 'GPT'}, {'text': 'W2A16', 'label': 'GPT'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",quantisation,Post-training quantization,0.6493542194366455
2402.02593,Vivswan Shah,Vivswan Shah and Nathan Youngblood,"Leveraging Continuously Differentiable Activation Functions for Learning
  in Quantized Noisy Environments",,,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Real-world analog systems intrinsically suffer from noise that can impede
model convergence and accuracy on a variety of deep learning models. We
demonstrate that differentiable activations like GELU and SiLU enable robust
propagation of gradients which help to mitigate analog quantization error that
is ubiquitous to all analog systems. We perform analysis and training of
convolutional, linear, and transformer networks in the presence of quantized
noise. Here, we are able to demonstrate that continuously differentiable
activation functions are significantly more noise resilient over conventional
rectified activations. As in the case of ReLU, the error in gradients are 100x
higher than those in GELU near zero. Our findings provide guidance for
selecting appropriate activations to realize performant and reliable hardware
implementations across several machine learning domains such as computer
vision, signal processing, and beyond. Code available at:
\href{https://github.com/Vivswan/GeLUReLUInterpolation}{https://github.com/Vivswan/GeLUReLUInterpolation}.}
","[{'version': 'v1', 'created': 'Sun, 4 Feb 2024 20:01:22 GMT'}, {'version': 'v2', 'created': 'Mon, 27 Jan 2025 21:15:38 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 09:50:27 GMT'}]",2025-02-25,"[['Shah', 'Vivswan', ''], ['Youngblood', 'Nathan', '']]","[{'text': 'analog quantization error', 'label': 'quantisation'}, {'text': 'quantized\nnoise', 'label': 'quantisation'}]",quantisation,"quantized
noise",0.5843287706375122
2404.05368,Vojtech Mrazek,"Jan Klhufek, Miroslav Safar, Vojtech Mrazek, Zdenek Vasicek, Lukas
  Sekanina","Exploring Quantization and Mapping Synergy in Hardware-Aware Deep Neural
  Network Accelerators","To appear at the 2024 27th International Symposium on Design &
  Diagnostics of Electronic Circuits & Systems (DDECS)",,,,cs.AR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Energy efficiency and memory footprint of a convolutional neural network
(CNN) implemented on a CNN inference accelerator depend on many factors,
including a weight quantization strategy (i.e., data types and bit-widths) and
mapping (i.e., placement and scheduling of DNN elementary operations on
hardware units of the accelerator). We show that enabling rich mixed
quantization schemes during the implementation can open a previously hidden
space of mappings that utilize the hardware resources more effectively. CNNs
utilizing quantized weights and activations and suitable mappings can
significantly improve trade-offs among the accuracy, energy, and memory
requirements compared to less carefully optimized CNN implementations. To find,
analyze, and exploit these mappings, we: (i) extend a general-purpose
state-of-the-art mapping tool (Timeloop) to support mixed quantization, which
is not currently available; (ii) propose an efficient multi-objective
optimization algorithm to find the most suitable bit-widths and mapping for
each DNN layer executed on the accelerator; and (iii) conduct a detailed
experimental evaluation to validate the proposed method. On two CNNs
(MobileNetV1 and MobileNetV2) and two accelerators (Eyeriss and Simba) we show
that for a given quality metric (such as the accuracy on ImageNet), energy
savings are up to 37% without any accuracy drop.
","[{'version': 'v1', 'created': 'Mon, 8 Apr 2024 10:10:30 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 13:11:04 GMT'}]",2025-02-26,"[['Klhufek', 'Jan', ''], ['Safar', 'Miroslav', ''], ['Mrazek', 'Vojtech', ''], ['Vasicek', 'Zdenek', ''], ['Sekanina', 'Lukas', '']]","[{'text': 'weight quantization strategy', 'label': 'quantisation'}, {'text': 'mixed quantization', 'label': 'quantisation'}]",quantisation,mixed quantization,0.6759530305862427
2405.11563,Kwangjae Lee,"Kwangjae Lee, Jung Hoon Lee, and Wan Choi","User-Centric Association and Feedback Bit Allocation for FDD Cell-Free
  Massive MIMO",,,,,cs.IT math.IT,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we introduce a novel approach to user-centric association and
feedback bit allocation for the downlink of a cell-free massive MIMO (CF-mMIMO)
system, operating under limited feedback constraints. In CF-mMIMO systems
employing frequency division duplexing, each access point (AP) relies on
channel information provided by its associated user equipments (UEs) for
beamforming design. Since the uplink control channel is typically shared among
UEs, we take account of each AP's total feedback budget, which is distributed
among its associated UEs. By employing the Saleh-Valenzuela multi-resolvable
path channel model with different average path gains, we first identify
necessary feedback information for each UE, along with an appropriate codebook
structure. This structure facilitates adaptive quantization of multiple paths
based on their dominance. We then formulate a joint optimization problem
addressing user-centric UE-AP association and feedback bit allocation. To
address this challenge, we analyze the impact of feedback bit allocation and
derive our proposed scheme from the solution of an alternative optimization
problem aimed at devising long-term policies, explicitly considering the
effects of feedback bit allocation. Numerical results show that our proposed
scheme effectively enhances the performance of conventional approaches in
CF-mMIMO systems.
","[{'version': 'v1', 'created': 'Sun, 19 May 2024 14:29:02 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 02:30:09 GMT'}]",2025-02-27,"[['Lee', 'Kwangjae', ''], ['Lee', 'Jung Hoon', ''], ['Choi', 'Wan', '']]","[{'text': 'Saleh-Valenzuela multi-resolvable\npath channel model', 'label': 'AI model'}, {'text': 'adaptive quantization', 'label': 'quantisation'}]",quantisation,adaptive quantization,0.6232469081878662
2406.08155,Pingzhi Li,"Pingzhi Li, Xiaolong Jin, Zhen Tan, Yu Cheng, Tianlong Chen","QuantMoE-Bench: Examining Post-Training Quantization for
  Mixture-of-Experts","Our code for reproducing all our experiments is provided at
  https://github.com/UNITES-Lab/moe-quantization",,,,cs.LG cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Mixture-of-Experts (MoE) is a promising way to scale up the learning capacity
of large language models. It increases the number of parameters while keeping
FLOPs nearly constant during inference through sparse activation. Yet, it still
suffers from significant memory overheads due to the vast parameter size,
necessitating model compression techniques. Post-training quantization offers a
powerful approach for model compression. Existing methods adopt a fixed
quantization precision for the entire MoE model. This rigid setup can lead to
suboptimal performance, without considering the inherent sparse structure. For
example, MoE's sparse routing mechanism leads to different activation patterns,
where shared experts are accessed by all tokens while token-conditioned experts
are selectively activated. This activation disparity suggests different
quantization requirements, with consistently activated shared experts
potentially needing higher precision to maintain model quality. In this paper,
we study a fine-grained precision setup for MoE quantization. We explore MoE
structure-aware quantization heuristics, ranging from coarse (e.g., MoE layers)
to fine granularity (e.g., linear layers). Our investigations reveal critical
principles, where different MoE structures require varying numbers of bits for
effective quantization. Conclusions are supported by extensive benchmarking
across two representative MoE models and six tasks including commonsense
reasoning and natural language understanding. We further show that an MoE
quantized in a fined-grained mixed precision achieved state-of-the-art 65.35%
performance on average compared to the baseline 64.30% (i.e., GPTQ). Moreover,
based on the findings, we introduce novel data-driven techniques for optimizing
bit allocation in MoE quantization, including the outlier-aware linear layer
scorer and MoE block importance predictor.
","[{'version': 'v1', 'created': 'Wed, 12 Jun 2024 12:44:48 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 18:29:54 GMT'}]",2025-02-26,"[['Li', 'Pingzhi', ''], ['Jin', 'Xiaolong', ''], ['Tan', 'Zhen', ''], ['Cheng', 'Yu', ''], ['Chen', 'Tianlong', '']]","[{'text': 'model compression techniques', 'label': 'quantisation'}, {'text': 'Post-training quantization', 'label': 'quantisation'}]",quantisation,Post-training quantization,0.6493542194366455
2406.13489,Daniele Proverbio,"Uros Sutulovic, Daniele Proverbio, Rami Katz, Giulia Giordano","Efficient gPC-based quantification of probabilistic robustness for
  systems in neuroscience",,,,,q-bio.QM,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Robustness analysis is very important in biology and neuroscience, to unravel
behavioral patterns of systems that are conserved despite large parametric
uncertainties. To make studies of probabilistic robustness more efficient and
scalable in addressing complex neuroscience models, we propose an alternative
to computationally expensive Monte Carlo (MC) methods by introducing and
analysing the generalised polynomial chaos (gPC) framework for uncertainty
quantification. We consider both intrusive and non-intrusive gPC approaches,
which turn out to be scalable and allow for a fast comprehensive exploration of
parameter spaces. Focusing on widely used models of neural dynamics as case
studies, we explore the trade-off between efficiency and accuracy of gPC
methods, and we select effective computational settings to investigate
parametric uncertainties in models that feature multiple dynamic regimes.
","[{'version': 'v1', 'created': 'Wed, 19 Jun 2024 12:19:03 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 12:29:10 GMT'}]",2025-02-25,"[['Sutulovic', 'Uros', ''], ['Proverbio', 'Daniele', ''], ['Katz', 'Rami', ''], ['Giordano', 'Giulia', '']]","[{'text': 'uncertainty\nquantification', 'label': 'quantisation'}]",quantisation,"uncertainty
quantification",0.571454644203186
2407.03312,Maike Holthuijzen,"Maike F. Holthuijzen, Robert B. Gramacy, Cayelan C. Carey, Dave M.
  Higdon, R. Quinn Thomas","Synthesizing data products, mathematical models, and observational
  measurements for lake temperature forecasting","20 pages, 9 figures",,,,stat.AP,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  We present a novel forecasting framework for lake water temperature, which is
crucial for managing lake ecosystems and drinking water resources. The General
Lake Model (GLM) has been previously used for this purpose, but, similar to
many process-based simulation models, it: requires a large number of inputs,
many of which are stochastic; presents challenges for uncertainty
quantification (UQ); and can exhibit model bias. To address these issues, we
propose a Gaussian process (GP) surrogate-based forecasting approach that
efficiently handles large, high-dimensional data and accounts for
input-dependent variability and systematic GLM bias. We validate the proposed
approach and compare it with other forecasting methods, including a
climatological model and raw GLM simulations. Our results demonstrate that our
bias-corrected GP surrogate (GPBC) can outperform competing approaches in terms
of forecast accuracy and UQ up to two weeks into the future.
","[{'version': 'v1', 'created': 'Wed, 3 Jul 2024 17:54:57 GMT'}, {'version': 'v2', 'created': 'Mon, 9 Dec 2024 17:26:55 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Feb 2025 15:58:48 GMT'}]",2025-02-26,"[['Holthuijzen', 'Maike F.', ''], ['Gramacy', 'Robert B.', ''], ['Carey', 'Cayelan C.', ''], ['Higdon', 'Dave M.', ''], ['Thomas', 'R. Quinn', '']]","[{'text': 'uncertainty\nquantification (UQ)', 'label': 'quantisation'}]",quantisation,"uncertainty
quantification (UQ)",0.5492583513259888
2407.17329,Erell Gachon,"Erell Gachon, J\'er\'emie Bigot, Elsa Cazelles, Audrey Bidet,
  Jean-Philippe Vial, Pierre-Yves Dumas, Aguirre Mimoun","Low dimensional representation of multi-patient flow cytometry datasets
  using optimal transport for minimal residual disease detection in leukemia",,,,,stat.ML cs.LG math.ST stat.ME stat.TH,http://creativecommons.org/licenses/by/4.0/,"  Representing and quantifying Minimal Residual Disease (MRD) in Acute Myeloid
Leukemia (AML), a type of cancer that affects the blood and bone marrow, is
essential in the prognosis and follow-up of AML patients. As traditional
cytological analysis cannot detect leukemia cells below 5\%, the analysis of
flow cytometry dataset is expected to provide more reliable results. In this
paper, we explore statistical learning methods based on optimal transport (OT)
to achieve a relevant low-dimensional representation of multi-patient flow
cytometry measurements (FCM) datasets considered as high-dimensional
probability distributions. Using the framework of OT, we justify the use of the
K-means algorithm for dimensionality reduction of multiple large-scale point
clouds through mean measure quantization by merging all the data into a single
point cloud. After this quantization step, the visualization of the intra and
inter-patients FCM variability is carried out by embedding low-dimensional
quantized probability measures into a linear space using either Wasserstein
Principal Component Analysis (PCA) through linearized OT or log-ratio PCA of
compositional data. Using a publicly available FCM dataset and a FCM dataset
from Bordeaux University Hospital, we demonstrate the benefits of our approach
over the popular kernel mean embedding technique for statistical learning from
multiple high-dimensional probability distributions. We also highlight the
usefulness of our methodology for low-dimensional projection and clustering
patient measurements according to their level of MRD in AML from FCM. In
particular, our OT-based approach allows a relevant and informative
two-dimensional representation of the results of the FlowSom algorithm, a
state-of-the-art method for the detection of MRD in AML using multi-patient
FCM.
","[{'version': 'v1', 'created': 'Wed, 24 Jul 2024 14:53:01 GMT'}, {'version': 'v2', 'created': 'Mon, 23 Sep 2024 08:09:01 GMT'}, {'version': 'v3', 'created': 'Wed, 26 Feb 2025 08:11:39 GMT'}]",2025-02-27,"[['Gachon', 'Erell', ''], ['Bigot', 'J√©r√©mie', ''], ['Cazelles', 'Elsa', ''], ['Bidet', 'Audrey', ''], ['Vial', 'Jean-Philippe', ''], ['Dumas', 'Pierre-Yves', ''], ['Mimoun', 'Aguirre', '']]","[{'text': 'mean measure quantization', 'label': 'quantisation'}]",quantisation,mean measure quantization,0.5048162937164307
2407.20827,Nicolas Fabre,"Thomas Pousset, Maxime Federico, Romain All\'eaume and Nicolas Fabre",Kramers-Kronig detection in the quantum regime,,,,,quant-ph,http://creativecommons.org/licenses/by/4.0/,"  We investigate the quantization of Kramers-Kronig detection technique
initially developped for classical optical communications. It consists in
mixing the unknown field with a strong monochromatic local oscillator on an
unbalanced beamsplitter. A single output of the beamsplitter undergoes a direct
detection of the optical intensity by means of a single photodiode. When the
measured output verifies signal processing constraints, namely, the minimal
phase and the single sideband constraints, Kramers-Kronig detection
reconstructs the phase of the signal from the intensity measurements via a
digitally computed Hilbert transform. The local oscillator being known,
Kramers-Kronig detection allows for reconstructing the quadratures of the
unknown field. We show that this result holds in the quantum regime up to first
order in the local oscillator amplitude and thus that Kramers-Kronig detection
acts as a coherent detection able to measure both quadratures, making it a
Gaussian measurement similar to double homodyne detection. We also study in
details the phase information measured by Kramers-Kronig detection for bosonic
coherent states, monomode pure states and mixed states. Finally, we propose and
investigate a spectral tomography protocol for single-photon states that is
inspired by Kramers-Kronig detection and relies on a spectral engineering of
the single-photon.
","[{'version': 'v1', 'created': 'Tue, 30 Jul 2024 13:47:31 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 12:56:35 GMT'}]",2025-02-27,"[['Pousset', 'Thomas', ''], ['Federico', 'Maxime', ''], ['All√©aume', 'Romain', ''], ['Fabre', 'Nicolas', '']]","[{'text': 'quantization', 'label': 'quantisation'}]",quantisation,quantization,0.813445508480072
2408.00391,Alexander Schenkel,"Cameron Kemp, Robert Laugwitz, Alexander Schenkel",Infinitesimal 2-braidings from 2-shifted Poisson structures,"v2: 39 pages. Final version accepted for publication in Journal of
  Geometry and Physics",,10.1016/j.geomphys.2025.105456,,math.QA math-ph math.AG math.MP,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  It is shown that every $2$-shifted Poisson structure on a finitely generated
semi-free commutative differential graded algebra $A$ defines a very explicit
infinitesimal $2$-braiding on the homotopy $2$-category of the symmetric
monoidal dg-category of finitely generated semi-free $A$-dg-modules. This
provides a concrete realization, to first order in the deformation parameter
$\hbar$, of the abstract deformation quantization results in derived algebraic
geometry due to Calaque, Pantev, To\""en, Vaqui\'e and Vezzosi. Of particular
interest is the case when $A$ is the Chevalley-Eilenberg algebra of a Lie
$N$-algebra, where the braided monoidal deformations developed in this paper
may be interpreted as candidates for representation categories of `higher
quantum groups'.
","[{'version': 'v1', 'created': 'Thu, 1 Aug 2024 08:59:34 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 09:21:58 GMT'}]",2025-02-28,"[['Kemp', 'Cameron', ''], ['Laugwitz', 'Robert', ''], ['Schenkel', 'Alexander', '']]","[{'text': 'deformation quantization', 'label': 'quantisation'}]",quantisation,deformation quantization,0.6073967218399048
