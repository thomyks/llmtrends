id,submitter,authors,title,comments,journal-ref,doi,report-no,categories,license,abstract,versions,update_date,authors_parsed,extracted_entities,assigned_concept,matched_keyword,similarity_score
2304.13343,Xinnian Liang,"Bing Wang, Xinnian Liang, Jian Yang, Hui Huang, Shuangzhi Wu, Peihao
  Wu, Lu Lu, Zejun Ma, Zhoujun Li","SCM: Enhancing Large Language Model with Self-Controlled Memory
  Framework",Accepted by DASFAA 2025 main conference,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Large Language Models (LLMs) are constrained by their inability to process
lengthy inputs, resulting in the loss of critical historical information. To
address this limitation, in this paper, we propose the Self-Controlled Memory
(SCM) framework to enhance the ability of LLMs to maintain long-term memory and
recall relevant information. Our SCM framework comprises three key components:
an LLM-based agent serving as the backbone of the framework, a memory stream
storing agent memories, and a memory controller updating memories and
determining when and how to utilize memories from memory stream. Additionally,
the proposed SCM is able to process ultra-long texts without any modification
or fine-tuning, which can integrate with any instruction following LLMs in a
plug-and-play paradigm. Furthermore, we annotate a dataset to evaluate the
effectiveness of SCM for handling lengthy inputs. The annotated dataset covers
three tasks: long-term dialogues, book summarization, and meeting
summarization. Experimental results demonstrate that our method achieves better
retrieval recall and generates more informative responses compared to
competitive baselines in long-term dialogues.
(https://github.com/wbbeyourself/SCM4LLMs)
","[{'version': 'v1', 'created': 'Wed, 26 Apr 2023 07:25:31 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Feb 2024 16:01:39 GMT'}, {'version': 'v3', 'created': 'Thu, 19 Sep 2024 13:38:51 GMT'}, {'version': 'v4', 'created': 'Tue, 18 Mar 2025 02:16:56 GMT'}]",2025-03-19,"[['Wang', 'Bing', ''], ['Liang', 'Xinnian', ''], ['Yang', 'Jian', ''], ['Huang', 'Hui', ''], ['Wu', 'Shuangzhi', ''], ['Wu', 'Peihao', ''], ['Lu', 'Lu', ''], ['Ma', 'Zejun', ''], ['Li', 'Zhoujun', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM-based agent', 'label': 'LLM-based'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'book summarization', 'label': 'Knowledge distillation'}, {'text': 'meeting\nsummarization', 'label': 'Knowledge distillation'}]",Large Language Model,Large Language Models,0.9664971828460693
2305.10361,Eilam Shapira,"Eilam Shapira, Omer Madmon, Reut Apel, Moshe Tennenholtz, Roi Reichart","Human Choice Prediction in Language-based Persuasion Games:
  Simulation-based Off-Policy Evaluation",,,,,cs.LG cs.AI cs.GT,http://creativecommons.org/licenses/by/4.0/,"  Recent advances in Large Language Models (LLMs) have spurred interest in
designing LLM-based agents for tasks that involve interaction with human and
artificial agents. This paper addresses a key aspect in the design of such
agents: predicting human decisions in off-policy evaluation (OPE). We focus on
language-based persuasion games, where an expert aims to influence the
decision-maker through verbal messages. In our OPE framework, the prediction
model is trained on human interaction data collected from encounters with one
set of expert agents, and its performance is evaluated on interactions with a
different set of experts. Using a dedicated application, we collected a dataset
of 87K decisions from humans playing a repeated decision-making game with
artificial agents. To enhance off-policy performance, we propose a simulation
technique involving interactions across the entire agent space and simulated
decision-makers. Our learning strategy yields significant OPE gains, e.g.,
improving prediction accuracy in the top 15% challenging cases by 7.1%. Our
code and the large dataset we collected and generated are submitted as
supplementary material and publicly available in our GitHub repository:
https://github.com/eilamshapira/HumanChoicePrediction
","[{'version': 'v1', 'created': 'Wed, 17 May 2023 16:38:11 GMT'}, {'version': 'v2', 'created': 'Tue, 23 May 2023 18:58:21 GMT'}, {'version': 'v3', 'created': 'Wed, 29 Nov 2023 13:46:53 GMT'}, {'version': 'v4', 'created': 'Wed, 28 Feb 2024 21:36:54 GMT'}, {'version': 'v5', 'created': 'Thu, 20 Mar 2025 14:27:22 GMT'}]",2025-03-21,"[['Shapira', 'Eilam', ''], ['Madmon', 'Omer', ''], ['Apel', 'Reut', ''], ['Tennenholtz', 'Moshe', ''], ['Reichart', 'Roi', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2311.04928,Marios Papachristou,"Marios Papachristou, Longqi Yang, Chin-Chia Hsu",Leveraging Large Language Models for Collective Decision-Making,To appear at ACM CSCW 2025,,,,cs.CL cs.AI cs.HC cs.SI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  In various work contexts, such as meeting scheduling, collaborating, and
project planning, collective decision-making is essential but often challenging
due to diverse individual preferences, varying work focuses, and power dynamics
among members. To address this, we propose a system leveraging Large Language
Models (LLMs) to facilitate group decision-making by managing conversations and
balancing preferences among individuals. Our system aims to extract individual
preferences from each member's conversation with the system and suggest options
that satisfy the preferences of the members. We specifically apply this system
to corporate meeting scheduling. We create synthetic employee profiles and
simulate conversations at scale, leveraging LLMs to evaluate the system
performance as a novel approach to conducting a user study. Our results
indicate efficient coordination with reduced interactions between the members
and the LLM-based system. The system refines and improves its proposed options
over time, ensuring that many of the members' individual preferences are
satisfied in an equitable way. Finally, we conduct a survey study involving
human participants to assess our system's ability to aggregate preferences and
reasoning about them. Our findings show that the system exhibits strong
performance in both dimensions.
","[{'version': 'v1', 'created': 'Fri, 3 Nov 2023 18:27:21 GMT'}, {'version': 'v2', 'created': 'Wed, 24 Jan 2024 19:38:52 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 15:50:13 GMT'}]",2025-03-18,"[['Papachristou', 'Marios', ''], ['Yang', 'Longqi', ''], ['Hsu', 'Chin-Chia', '']]","[{'text': 'Large Language\nModels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}]",Large Language Model,"Large Language
Models",0.9664971828460693
2312.14898,Laura Plein,"Wendk\^uuni C. Ou\'edraogo, Laura Plein, Kader Kabor\'e, Andrew Habib,
  Jacques Klein, David Lo, Tegawend\'e F. Bissyand\'e","Enriching Automatic Test Case Generation by Extracting Relevant Test
  Inputs from Bug Reports","Accepted at Empirical Software Engineering (EMSE) journal on 4 March
  2025",,,,cs.SE,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  The quality of software is closely tied to the effectiveness of the tests it
undergoes. Manual test writing, though crucial for bug detection, is
time-consuming, which has driven significant research into automated test case
generation. However, current methods often struggle to generate relevant
inputs, limiting the effectiveness of the tests produced. To address this, we
introduce BRMiner, a novel approach that leverages Large Language Models (LLMs)
in combination with traditional techniques to extract relevant inputs from bug
reports, thereby enhancing automated test generation tools. In this study, we
evaluate BRMiner using the Defects4J benchmark and test generation tools such
as EvoSuite and Randoop. Our results demonstrate that BRMiner achieves a
Relevant Input Rate (RIR) of 60.03% and a Relevant Input Extraction Accuracy
Rate (RIEAR) of 31.71%, significantly outperforming methods that rely on LLMs
alone. The integration of BRMiner's input enhances EvoSuite ability to generate
more effective test, leading to increased code coverage, with gains observed in
branch, instruction, method, and line coverage across multiple projects.
Furthermore, BRMiner facilitated the detection of 58 unique bugs, including
those that were missed by traditional baseline approaches. Overall, BRMiner's
combination of LLM filtering with traditional input extraction techniques
significantly improves the relevance and effectiveness of automated test
generation, advancing the detection of bugs and enhancing code coverage,
thereby contributing to higher-quality software development.
","[{'version': 'v1', 'created': 'Fri, 22 Dec 2023 18:19:33 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 20:06:00 GMT'}]",2025-03-21,"[['Ouédraogo', 'Wendkûuni C.', ''], ['Plein', 'Laura', ''], ['Kaboré', 'Kader', ''], ['Habib', 'Andrew', ''], ['Klein', 'Jacques', ''], ['Lo', 'David', ''], ['Bissyandé', 'Tegawendé F.', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'EvoSuite', 'label': 'Open-source LLMs'}, {'text': 'Randoop', 'label': 'Open-source LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'EvoSuite', 'label': 'Open-source LLMs'}]",Large Language Model,Large Language Models,0.9664971828460693
2401.09002,Mingyu Jin,"Dong Shu, Chong Zhang, Mingyu Jin, Zihao Zhou, Lingyao Li, Yongfeng
  Zhang","AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on
  Large Language Models",Accepted by ACM SIGKDD Explorations 2025,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Jailbreak attacks represent one of the most sophisticated threats to the
security of large language models (LLMs). To deal with such risks, we introduce
an innovative framework that can help evaluate the effectiveness of jailbreak
attacks on LLMs. Unlike traditional binary evaluations focusing solely on the
robustness of LLMs, our method assesses the attacking prompts' effectiveness.
We present two distinct evaluation frameworks: a coarse-grained evaluation and
a fine-grained evaluation. Each framework uses a scoring range from 0 to 1,
offering unique perspectives and allowing for the assessment of attack
effectiveness in different scenarios. Additionally, we develop a comprehensive
ground truth dataset specifically tailored for jailbreak prompts. This dataset
is a crucial benchmark for our current study and provides a foundational
resource for future research. By comparing with traditional evaluation methods,
our study shows that the current results align with baseline metrics while
offering a more nuanced and fine-grained assessment. It also helps identify
potentially harmful attack prompts that might appear harmless in traditional
evaluations. Overall, our work establishes a solid foundation for assessing a
broader range of attack prompts in prompt injection.
","[{'version': 'v1', 'created': 'Wed, 17 Jan 2024 06:42:44 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Feb 2024 02:20:31 GMT'}, {'version': 'v3', 'created': 'Wed, 20 Mar 2024 14:08:39 GMT'}, {'version': 'v4', 'created': 'Wed, 31 Jul 2024 06:46:44 GMT'}, {'version': 'v5', 'created': 'Sat, 3 Aug 2024 06:39:25 GMT'}, {'version': 'v6', 'created': 'Tue, 18 Mar 2025 01:50:42 GMT'}]",2025-03-19,"[['Shu', 'Dong', ''], ['Zhang', 'Chong', ''], ['Jin', 'Mingyu', ''], ['Zhou', 'Zihao', ''], ['Li', 'Lingyao', ''], ['Zhang', 'Yongfeng', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'jailbreak prompts', 'label': 'Prompting'}, {'text': 'prompt injection', 'label': 'Prompting'}]",Large Language Model,large language models,0.9664971828460693
2404.13274,Mar Gonzalez-Franco,"Mustafa Doga Dogan, and Eric J. Gonzalez, and Karan Ahuja, and Ruofei
  Du, and Andrea Cola\c{c}o, and Johnny Lee, and Mar Gonzalez-Franco, and David
  Kim","Augmented Object Intelligence: Making the Analog World Interactable with
  XR-Objects",2024 ACM Symposium on User Interface Software and Technology (UIST),,10.1145/3654777.3676379,,cs.HC cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Seamless integration of physical objects as interactive digital entities
remains a challenge for spatial computing. This paper introduces Augmented
Object Intelligence (AOI), a novel XR interaction paradigm designed to blur the
lines between digital and physical by equipping real-world objects with the
ability to interact as if they were digital, where every object has the
potential to serve as a portal to vast digital functionalities. Our approach
utilizes object segmentation and classification, combined with the power of
Multimodal Large Language Models (MLLMs), to facilitate these interactions. We
implement the AOI concept in the form of XR-Objects, an open-source prototype
system that provides a platform for users to engage with their physical
environment in rich and contextually relevant ways. This system enables analog
objects to not only convey information but also to initiate digital actions,
such as querying for details or executing tasks. Our contributions are
threefold: (1) we define the AOI concept and detail its advantages over
traditional AI assistants, (2) detail the XR-Objects system's open-source
design and implementation, and (3) show its versatility through a variety of
use cases and a user study.
","[{'version': 'v1', 'created': 'Sat, 20 Apr 2024 05:14:52 GMT'}, {'version': 'v2', 'created': 'Tue, 23 Apr 2024 03:09:15 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Aug 2024 07:55:44 GMT'}, {'version': 'v4', 'created': 'Tue, 18 Mar 2025 23:29:40 GMT'}]",2025-03-20,"[['Dogan', 'Mustafa Doga', ''], ['Gonzalez', 'Eric J.', ''], ['Ahuja', 'Karan', ''], ['Du', 'Ruofei', ''], ['Colaço', 'Andrea', ''], ['Lee', 'Johnny', ''], ['Gonzalez-Franco', 'Mar', ''], ['Kim', 'David', '']]","[{'text': 'Multimodal Large Language Models (MLLMs)', 'label': 'Large Language Model'}, {'text': 'XR-Objects', 'label': 'Open-source LLMs'}, {'text': 'XR-Objects', 'label': 'Open-source LLMs'}]",Large Language Model,Multimodal Large Language Models (MLLMs),0.7274739742279053
2404.18212,Noam Rotstein,"Navve Wasserman, Noam Rotstein, Roy Ganz, Ron Kimmel",Paint by Inpaint: Learning to Add Image Objects by Removing Them First,,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Image editing has advanced significantly with the introduction of
text-conditioned diffusion models. Despite this progress, seamlessly adding
objects to images based on textual instructions without requiring user-provided
input masks remains a challenge. We address this by leveraging the insight that
removing objects (Inpaint) is significantly simpler than its inverse process of
adding them (Paint), attributed to inpainting models that benefit from
segmentation mask guidance. Capitalizing on this realization, by implementing
an automated and extensive pipeline, we curate a filtered large-scale image
dataset containing pairs of images and their corresponding object-removed
versions. Using these pairs, we train a diffusion model to inverse the
inpainting process, effectively adding objects into images. Unlike other
editing datasets, ours features natural target images instead of synthetic ones
while ensuring source-target consistency by construction. Additionally, we
utilize a large Vision-Language Model to provide detailed descriptions of the
removed objects and a Large Language Model to convert these descriptions into
diverse, natural-language instructions. Our quantitative and qualitative
results show that the trained model surpasses existing models in both object
addition and general editing tasks. Visit our project page for the released
dataset and trained models at https://rotsteinnoam.github.io/Paint-by-Inpaint.
","[{'version': 'v1', 'created': 'Sun, 28 Apr 2024 15:07:53 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 13:48:18 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 06:59:54 GMT'}]",2025-03-21,"[['Wasserman', 'Navve', ''], ['Rotstein', 'Noam', ''], ['Ganz', 'Roy', ''], ['Kimmel', 'Ron', '']]","[{'text': 'Large Language Model', 'label': 'Large Language Model'}]",Large Language Model,Large Language Model,1.0
2404.18400,Parshin Shojaee,"Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani,
  Chandan K Reddy","LLM-SR: Scientific Equation Discovery via Programming with Large
  Language Models",ICLR 2025 Oral,,,,cs.LG cs.AI cs.CL cs.NE,http://creativecommons.org/licenses/by/4.0/,"  Mathematical equations have been unreasonably effective in describing complex
natural phenomena across various scientific disciplines. However, discovering
such insightful equations from data presents significant challenges due to the
necessity of navigating extremely large combinatorial hypothesis spaces.
Current methods of equation discovery, commonly known as symbolic regression
techniques, largely focus on extracting equations from data alone, often
neglecting the domain-specific prior knowledge that scientists typically depend
on. They also employ limited representations such as expression trees,
constraining the search space and expressiveness of equations. To bridge this
gap, we introduce LLM-SR, a novel approach that leverages the extensive
scientific knowledge and robust code generation capabilities of Large Language
Models (LLMs) to discover scientific equations from data. Specifically, LLM-SR
treats equations as programs with mathematical operators and combines LLMs'
scientific priors with evolutionary search over equation programs. The LLM
iteratively proposes new equation skeleton hypotheses, drawing from its domain
knowledge, which are then optimized against data to estimate parameters. We
evaluate LLM-SR on four benchmark problems across diverse scientific domains
(e.g., physics, biology), which we carefully designed to simulate the discovery
process and prevent LLM recitation. Our results demonstrate that LLM-SR
discovers physically accurate equations that significantly outperform
state-of-the-art symbolic regression baselines, particularly in out-of-domain
test settings. We also show that LLM-SR's incorporation of scientific priors
enables more efficient equation space exploration than the baselines. Code and
data are available: https://github.com/deep-symbolic-mathematics/LLM-SR
","[{'version': 'v1', 'created': 'Mon, 29 Apr 2024 03:30:06 GMT'}, {'version': 'v2', 'created': 'Sun, 2 Jun 2024 20:17:59 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 16:37:17 GMT'}]",2025-03-21,"[['Shojaee', 'Parshin', ''], ['Meidani', 'Kazem', ''], ['Gupta', 'Shashank', ''], ['Farimani', 'Amir Barati', ''], ['Reddy', 'Chandan K', '']]","[{'text': 'LLM-SR', 'label': 'LLM'}, {'text': 'Large Language\nModels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM-SR', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM-SR', 'label': 'LLM'}]",Large Language Model,"Large Language
Models",0.9664971828460693
2405.18376,Dongjie Chen,"Dongjie Chen, Kartik Patwari, Zhengfeng Lai, Xiaoguang Zhu, Sen-ching
  Cheung, Chen-Nee Chuah","Empowering Source-Free Domain Adaptation via MLLM-Guided
  Reliability-Based Curriculum Learning",,,,,cs.LG cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model
to a target domain using only unlabeled target data. Current SFDA methods face
challenges in effectively leveraging pre-trained knowledge and exploiting
target domain data. Multimodal Large Language Models (MLLMs) offer remarkable
capabilities in understanding visual and textual information, but their
applicability to SFDA poses challenges such as instruction-following failures,
intensive computational demands, and difficulties in performance measurement
prior to adaptation. To alleviate these issues, we propose
$\textbf{Reliability-based Curriculum Learning (RCL)}$, a novel framework that
integrates multiple MLLMs for knowledge exploitation via pseudo-labeling in
SFDA. Our framework incorporates Reliable Knowledge Transfer, Self-correcting
and MLLM-guided Knowledge Expansion, and Multi-hot Masking Refinement to
progressively exploit unlabeled data in the target domain. RCL achieves
state-of-the-art (SOTA) performance on multiple SFDA benchmarks, e.g.,
$\textbf{+9.4%}$ on DomainNet, demonstrating its effectiveness in enhancing
adaptability and robustness without requiring access to source data. Our code
is available at: https://github.com/Dong-Jie-Chen/RCL.
","[{'version': 'v1', 'created': 'Tue, 28 May 2024 17:18:17 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 00:11:36 GMT'}]",2025-03-18,"[['Chen', 'Dongjie', ''], ['Patwari', 'Kartik', ''], ['Lai', 'Zhengfeng', ''], ['Zhu', 'Xiaoguang', ''], ['Cheung', 'Sen-ching', ''], ['Chuah', 'Chen-Nee', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'Multi-hot Masking Refinement', 'label': 'Zero-shot Learning'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2406.06918,Dewu Zheng,"Dewu Zheng, Yanlin Wang, Ensheng Shi, Ruikai Zhang, Yuchi Ma, Hongyu
  Zhang, Zibin Zheng","HumanEvo: An Evolution-aware Benchmark for More Realistic Evaluation of
  Repository-level Code Generation",To appear at ICSE 2025,47th International Conference on Software Engineering (ICSE 2025),,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  To evaluate the repository-level code generation capabilities of Large
Language Models (LLMs) in complex real-world software development scenarios,
many evaluation methods have been developed. These methods typically leverage
contextual code from the latest version of a project to assist LLMs in
accurately generating the desired function. However, such evaluation methods
fail to consider the dynamic evolution of software projects over time, which we
refer to as evolution-ignored settings. This in turn results in inaccurate
evaluation of LLMs' performance. In this paper, we conduct an empirical study
to deeply understand LLMs' code generation performance within settings that
reflect the evolution nature of software development. To achieve this, we first
construct an evolution-aware repository-level code generation dataset, namely
HumanEvo, equipped with an automated execution-based evaluation tool. Second,
we manually categorize HumanEvo according to dependency levels to more
comprehensively analyze the model's performance in generating functions with
different dependency levels. Third, we conduct extensive experiments on
HumanEvo with seven representative and diverse LLMs to verify the effectiveness
of the proposed benchmark. We obtain several important findings through our
experimental study. For example, we find that previous evolution-ignored
evaluation methods result in inflated performance of LLMs, with performance
overestimations ranging from 10.0% to 61.1% under different context acquisition
methods, compared to the evolution-aware evaluation approach. Based on the
findings, we give actionable suggestions for more realistic evaluation of LLMs
on code generation. We also build a shared evolution-aware code generation
toolbox to facilitate future research.
","[{'version': 'v1', 'created': 'Tue, 11 Jun 2024 03:19:18 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 04:58:23 GMT'}]",2025-03-19,"[['Zheng', 'Dewu', ''], ['Wang', 'Yanlin', ''], ['Shi', 'Ensheng', ''], ['Zhang', 'Ruikai', ''], ['Ma', 'Yuchi', ''], ['Zhang', 'Hongyu', ''], ['Zheng', 'Zibin', '']]","[{'text': 'Large\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,"Large
Language Models",0.9664971828460693
2406.10638,Yexin Liu,"Yexin Liu, Zhengyang Liang, Yueze Wang, Xianfeng Wu, Feilong Tang,
  Muyang He, Jian Li, Zheng Liu, Harry Yang, Sernam Lim, Bo Zhao","Unveiling the Ignorance of MLLMs: Seeing Clearly, Answering Incorrectly",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal Large Language Models (MLLMs) have displayed remarkable
performance in multi-modal tasks, particularly in visual comprehension.
However, we reveal that MLLMs often generate incorrect answers even when they
understand the visual content. To this end, we manually construct a benchmark
with 12 categories and design evaluation metrics that assess the degree of
error in MLLM responses even when the visual content is seemingly understood.
Based on this benchmark, we test 15 leading MLLMs and analyze the distribution
of attention maps and logits of some MLLMs. Our investigation identifies two
primary issues: 1) most instruction tuning datasets predominantly feature
questions that 'directly' relate to the visual content, leading to a bias in
MLLMs' responses to other indirect questions, and 2) MLLMs' attention to visual
tokens is notably lower than to system and question tokens. We further observe
that attention scores between questions and visual tokens as well as the
model's confidence in the answers are lower in response to misleading questions
than to straightforward ones. To address the first challenge, we introduce a
paired positive and negative data construction pipeline to diversify the
dataset. For the second challenge, we propose to enhance the model's focus on
visual content during decoding by refining the text and visual prompt. For the
text prompt, we propose a content guided refinement strategy that performs
preliminary visual content analysis to generate structured information before
answering the question. Additionally, we employ a visual attention refinement
strategy that highlights question-relevant visual tokens to increase the
model's attention to visual content that aligns with the question. Extensive
experiments demonstrate that these challenges can be significantly mitigated
with our proposed dataset and techniques.
","[{'version': 'v1', 'created': 'Sat, 15 Jun 2024 13:58:26 GMT'}, {'version': 'v2', 'created': 'Tue, 17 Dec 2024 06:48:10 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 05:52:59 GMT'}]",2025-03-20,"[['Liu', 'Yexin', ''], ['Liang', 'Zhengyang', ''], ['Wang', 'Yueze', ''], ['Wu', 'Xianfeng', ''], ['Tang', 'Feilong', ''], ['He', 'Muyang', ''], ['Li', 'Jian', ''], ['Liu', 'Zheng', ''], ['Yang', 'Harry', ''], ['Lim', 'Sernam', ''], ['Zhao', 'Bo', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'attention scores', 'label': 'Attention mechanism'}, {'text': 'visual prompt', 'label': 'Prompting'}, {'text': 'text prompt', 'label': 'Prompting'}, {'text': 'content guided refinement strategy', 'label': 'Fine-tuning'}, {'text': 'attention', 'label': 'Attention mechanism'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2406.13803,Sam Musker,"Sam Musker, Alex Duchnowski, Rapha\""el Milli\`ere, Ellie Pavlick",LLMs as Models for Analogical Reasoning,"The title has been changed from Semantic Structure-Mapping in LLM and
  Human Analogical Reasoning to LLMs as Models for Analogical Reasoning to
  improve clarity and accuracy",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Analogical reasoning-the capacity to identify and map structural
relationships between different domains-is fundamental to human cognition and
learning. Recent studies have shown that large language models (LLMs) can
sometimes match humans in analogical reasoning tasks, opening the possibility
that analogical reasoning might emerge from domain general processes. However,
it is still debated whether these emergent capacities are largely superficial
and limited to simple relations seen during training or whether they rather
encompass the flexible representational and mapping capabilities which are the
focus of leading cognitive models of analogy. In this study, we introduce novel
analogical reasoning tasks that require participants to map between
semantically contentful words and sequences of letters and other abstract
characters. This task necessitates the ability to flexibly re-represent rich
semantic information-an ability which is known to be central to human analogy
but which is thus far not well-captured by existing cognitive theories and
models. We assess the performance of both human participants and LLMs on tasks
focusing on reasoning from semantic structure and semantic content, introducing
variations that test the robustness of their analogical inferences. Advanced
LLMs match human performance across several conditions, though humans and LLMs
respond differently to certain task variations and semantic distractors. Our
results thus provide new evidence that LLMs might offer a how-possibly
explanation of human analogical reasoning in contexts that are not yet well
modeled by existing theories, but that even today's best models are unlikely to
yield how-actually explanations.
","[{'version': 'v1', 'created': 'Wed, 19 Jun 2024 20:07:37 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 17:49:06 GMT'}]",2025-03-19,"[['Musker', 'Sam', ''], ['Duchnowski', 'Alex', ''], ['Millière', 'Raphaël', ''], ['Pavlick', 'Ellie', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2406.14703,Seungwon Lim,"Seungbeen Lee, Seungwon Lim, Seungju Han, Giyeong Oh, Hyungjoo Chae,
  Jiwan Chung, Minju Kim, Beong-woo Kwak, Yeonsoo Lee, Dongha Lee, Jinyoung
  Yeo, Youngjae Yu","Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality
  Testset designed for LLMs with Psychometrics",Accepted to NAACL2025 Findings,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Recent advancements in Large Language Models (LLMs) have led to their
adaptation in various domains as conversational agents. We wonder: can
personality tests be applied to these agents to analyze their behavior, similar
to humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice
questions designed to assess the personality of LLMs. TRAIT is built on two
psychometrically validated small human questionnaires, Big Five Inventory (BFI)
and Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a
variety of real-world scenarios. TRAIT also outperforms existing personality
tests for LLMs in terms of reliability and validity, achieving the highest
scores across four key metrics: Content Validity, Internal Validity, Refusal
Rate, and Reliability. Using TRAIT, we reveal two notable insights into
personalities of LLMs: 1) LLMs exhibit distinct and consistent personality,
which is highly influenced by their training data (e.g., data used for
alignment tuning), and 2) current prompting techniques have limited
effectiveness in eliciting certain traits, such as high psychopathy or low
conscientiousness, suggesting the need for further research in this direction.
","[{'version': 'v1', 'created': 'Thu, 20 Jun 2024 19:50:56 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Oct 2024 14:01:14 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 15:37:42 GMT'}]",2025-03-20,"[['Lee', 'Seungbeen', ''], ['Lim', 'Seungwon', ''], ['Han', 'Seungju', ''], ['Oh', 'Giyeong', ''], ['Chae', 'Hyungjoo', ''], ['Chung', 'Jiwan', ''], ['Kim', 'Minju', ''], ['Kwak', 'Beong-woo', ''], ['Lee', 'Yeonsoo', ''], ['Lee', 'Dongha', ''], ['Yeo', 'Jinyoung', ''], ['Yu', 'Youngjae', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'TRAIT', 'label': 'ChatGPT'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'alignment tuning', 'label': 'Fine-tuning'}, {'text': 'current prompting techniques', 'label': 'Prompting'}]",Large Language Model,Large Language Models,0.9664971828460693
2406.16144,Zezhong Wang Mr.,"Zezhong Wang, Xingshan Zeng, Weiwen Liu, Yufei Wang, Liangyou Li,
  Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong",Chain-of-Probe: Examining the Necessity and Accuracy of CoT Step-by-Step,Accepted by Findings of NAACL 2025,,,,cs.CL,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Current research found the issue of Early Answering in large language models
(LLMs), where the models already have an answer before generating the
Chain-of-Thought (CoT). This phenomenon suggests a potential lack of necessary
dependency between the predicted answer and the reasoning process.
Consequently, two important questions arise: (1) Is CoT still necessary if the
model already has an answer? (2) Can the correctness of the answer serve as
valid evidence for the correctness of CoT? To address these questions, we
propose a method, namely Chain-of-Probe (CoP), to probe changes in the mind
during the model's reasoning. The probing results show that in a significant
number of question-answer cases, CoT appears to be unnecessary, and this
necessity correlates with the simplicity of the task, defined by reasoning
steps required. Furthermore, by analyzing patterns in mind change, we examine
the correctness of the model's reasoning. Our validation reveals that many
responses, although correct in their final answer, contain errors in their
reasoning process. To this end, we propose a strategic approach based on CoP to
prioritize answers with correct reasoning among multiple candidates, thereby
bolstering the reliability of the model's reasoning.
","[{'version': 'v1', 'created': 'Sun, 23 Jun 2024 15:50:22 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 06:04:48 GMT'}]",2025-03-18,"[['Wang', 'Zezhong', ''], ['Zeng', 'Xingshan', ''], ['Liu', 'Weiwen', ''], ['Wang', 'Yufei', ''], ['Li', 'Liangyou', ''], ['Wang', 'Yasheng', ''], ['Shang', 'Lifeng', ''], ['Jiang', 'Xin', ''], ['Liu', 'Qun', ''], ['Wong', 'Kam-Fai', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'CoT', 'label': 'Chain of thought'}, {'text': 'CoT', 'label': 'Chain of thought'}, {'text': 'Chain-of-Probe', 'label': 'Chain of thought'}, {'text': 'CoT', 'label': 'Chain of thought'}]",Large Language Model,large language models,0.9664971828460693
2406.18326,Huixuan Zhang,"Huixuan Zhang, Yun Lin, Xiaojun Wan","PaCoST: Paired Confidence Significance Testing for Benchmark
  Contamination Detection in Large Language Models",Accepted by EMNLP 2024 Findings,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) are known to be trained on vast amounts of data,
which may unintentionally or intentionally include data from commonly used
benchmarks. This inclusion can lead to cheatingly high scores on model
leaderboards, yet result in disappointing performance in real-world
applications. To address this benchmark contamination problem, we first propose
a set of requirements that practical contamination detection methods should
follow. Following these proposed requirements, we introduce PaCoST, a Paired
Confidence Significance Testing to effectively detect benchmark contamination
in LLMs. Our method constructs a counterpart for each piece of data with the
same distribution, and performs statistical analysis of the corresponding
confidence to test whether the model is significantly more confident under the
original benchmark. We validate the effectiveness of PaCoST and apply it on
popular open-source models and benchmarks. We find that almost all models and
benchmarks we tested are suspected contaminated more or less. We finally call
for new LLM evaluation methods.
","[{'version': 'v1', 'created': 'Wed, 26 Jun 2024 13:12:40 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 08:57:39 GMT'}]",2025-03-19,"[['Zhang', 'Huixuan', ''], ['Lin', 'Yun', ''], ['Wan', 'Xiaojun', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2406.18842,Saleh Afroogh,"Junfeng Jiao, Saleh Afroogh, Kevin Chen, David Atkinson, Amit
  Dhurandhar","The global landscape of academic guidelines for generative AI and Large
  Language Models",,,,,cs.CY cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The integration of Generative Artificial Intelligence (GAI) and Large
Language Models (LLMs) in academia has spurred a global discourse on their
potential pedagogical benefits and ethical considerations. Positive reactions
highlight some potential, such as collaborative creativity, increased access to
education, and empowerment of trainers and trainees. However, negative
reactions raise concerns about ethical complexities, balancing innovation and
academic integrity, unequal access, and misinformation risks. Through a
systematic survey and text-mining-based analysis of global and national
directives, insights from independent research, and eighty university-level
guidelines, this study provides a nuanced understanding of the opportunities
and challenges posed by GAI and LLMs in education. It emphasizes the importance
of balanced approaches that harness the benefits of these technologies while
addressing ethical considerations and ensuring equitable access and educational
outcomes. The paper concludes with recommendations for fostering responsible
innovation and ethical practices to guide the integration of GAI and LLMs in
academia.
","[{'version': 'v1', 'created': 'Sun, 26 May 2024 15:28:24 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Jun 2024 02:54:06 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 16:42:30 GMT'}]",2025-03-19,"[['Jiao', 'Junfeng', ''], ['Afroogh', 'Saleh', ''], ['Chen', 'Kevin', ''], ['Atkinson', 'David', ''], ['Dhurandhar', 'Amit', '']]","[{'text': 'Large\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ethical considerations', 'label': 'AI Ethics'}, {'text': 'ethical complexities', 'label': 'AI Ethics'}, {'text': 'GAI', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ethical considerations', 'label': 'AI Ethics'}, {'text': 'ethical practices', 'label': 'AI Ethics'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,"Large
Language Models",0.9664971828460693
2407.00890,Shubhranshu Shekhar,Andrea Carriero and Davide Pettenuzzo and Shubhranshu Shekhar,Macroeconomic Forecasting with Large Language Models,,,,,econ.EM cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents a comparative analysis evaluating the accuracy of Large
Language Models (LLMs) against traditional macro time series forecasting
approaches. In recent times, LLMs have surged in popularity for forecasting due
to their ability to capture intricate patterns in data and quickly adapt across
very different domains. However, their effectiveness in forecasting
macroeconomic time series data compared to conventional methods remains an area
of interest. To address this, we conduct a rigorous evaluation of LLMs against
traditional macro forecasting methods, using as common ground the FRED-MD
database. Our findings provide valuable insights into the strengths and
limitations of LLMs in forecasting macroeconomic time series, shedding light on
their applicability in real-world scenarios
","[{'version': 'v1', 'created': 'Mon, 1 Jul 2024 01:25:26 GMT'}, {'version': 'v2', 'created': 'Mon, 6 Jan 2025 01:27:48 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 18:55:51 GMT'}]",2025-03-21,"[['Carriero', 'Andrea', ''], ['Pettenuzzo', 'Davide', ''], ['Shekhar', 'Shubhranshu', '']]","[{'text': 'Large\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,"Large
Language Models",0.9664971828460693
2407.09283,Sangpil Youm,"Sangpil Youm, Brodie Mather, Chathuri Jayaweera, Juliana Prada, Bonnie
  Dorr",DAHRS: Divergence-Aware Hallucination-Remediated SRL Projection,"15 pages, 6 figures, Accepted to The 29th International Conference on
  Natural Language & Information Systems (NLDB 2024)",,10.1007/978-3-031-70239-6_29,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Semantic role labeling (SRL) enriches many downstream applications, e.g.,
machine translation, question answering, summarization, and stance/belief
detection. However, building multilingual SRL models is challenging due to the
scarcity of semantically annotated corpora for multiple languages. Moreover,
state-of-the-art SRL projection (XSRL) based on large language models (LLMs)
yields output that is riddled with spurious role labels. Remediation of such
hallucinations is not straightforward due to the lack of explainability of
LLMs. We show that hallucinated role labels are related to naturally occurring
divergence types that interfere with initial alignments. We implement
Divergence-Aware Hallucination-Remediated SRL projection (DAHRS), leveraging
linguistically-informed alignment remediation followed by greedy First-Come
First-Assign (FCFA) SRL projection. DAHRS improves the accuracy of SRL
projection without additional transformer-based machinery, beating XSRL in both
human and automatic comparisons, and advancing beyond headwords to accommodate
phrase-level SRL projection (e.g., EN-FR, EN-ES). Using CoNLL-2009 as our
ground truth, we achieve a higher word-level F1 over XSRL: 87.6% vs. 77.3%
(EN-FR) and 89.0% vs. 82.7% (EN-ES). Human phrase-level assessments yield 89.1%
(EN-FR) and 91.0% (EN-ES). We also define a divergence metric to adapt our
approach to other language pairs (e.g., English-Tagalog).
","[{'version': 'v1', 'created': 'Fri, 12 Jul 2024 14:13:59 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 13:41:21 GMT'}]",2025-03-20,"[['Youm', 'Sangpil', ''], ['Mather', 'Brodie', ''], ['Jayaweera', 'Chathuri', ''], ['Prada', 'Juliana', ''], ['Dorr', 'Bonnie', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2407.21368,Danfeng Guo,Danfeng Guo and Demetri Terzopoulos,"Prompting Medical Large Vision-Language Models to Diagnose Pathologies
  by Visual Question Answering","Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2025:004",Machine.Learning.for.Biomedical.Imaging. 3 (2025),10.59275/j.melba.2025-1a8b,,cs.CV cs.AI cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large Vision-Language Models (LVLMs) have achieved significant success in
recent years, and they have been extended to the medical domain. Although
demonstrating satisfactory performance on medical Visual Question Answering
(VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem,
which makes them fail to diagnose complex pathologies. Moreover, they readily
fail to learn minority pathologies due to imbalanced training data. We propose
two prompting strategies for MLVLMs that reduce hallucination and improve VQA
performance. In the first strategy, we provide a detailed explanation of the
queried pathology. In the second strategy, we fine-tune a cheap, weak learner
to achieve high performance on a specific metric, and textually provide its
judgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our
methods significantly improve the diagnostic F1 score, with the highest
increase being 0.27. We also demonstrate that our prompting strategies can be
extended to general LVLM domains. Based on POPE metrics, it effectively
suppresses the false negative predictions of existing LVLMs and improves Recall
by approximately 0.07.
","[{'version': 'v1', 'created': 'Wed, 31 Jul 2024 06:34:38 GMT'}, {'version': 'v2', 'created': 'Sat, 1 Mar 2025 06:14:00 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 00:27:45 GMT'}]",2025-03-18,"[['Guo', 'Danfeng', ''], ['Terzopoulos', 'Demetri', '']]","[{'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'prompting strategies', 'label': 'Prompting'}, {'text': 'MLVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Vision-Language Models,0.7742220759391785
2408.10641,Yuxiao Wang,"Yuxiao Wang, Yu Lei, Li Cui, Weiying Xue, Qi Liu, Zhenao Wei",A Review of Human-Object Interaction Detection,"Accepted by 2024 2nd International Conference on Computer, Vision and
  Intelligent Technology (ICCVIT)",,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Human-object interaction (HOI) detection plays a key role in high-level
visual understanding, facilitating a deep comprehension of human activities.
Specifically, HOI detection aims to locate the humans and objects involved in
interactions within images or videos and classify the specific interactions
between them. The success of this task is influenced by several key factors,
including the accurate localization of human and object instances, as well as
the correct classification of object categories and interaction relationships.
This paper systematically summarizes and discusses the recent work in
image-based HOI detection. First, the mainstream datasets involved in HOI
relationship detection are introduced. Furthermore, starting with two-stage
methods and end-to-end one-stage detection approaches, this paper
comprehensively discusses the current developments in image-based HOI
detection, analyzing the strengths and weaknesses of these two methods.
Additionally, the advancements of zero-shot learning, weakly supervised
learning, and the application of large-scale language models in HOI detection
are discussed. Finally, the current challenges in HOI detection are outlined,
and potential research directions and future trends are explored.
","[{'version': 'v1', 'created': 'Tue, 20 Aug 2024 08:32:39 GMT'}, {'version': 'v2', 'created': 'Mon, 9 Dec 2024 09:27:29 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 02:22:59 GMT'}]",2025-03-19,"[['Wang', 'Yuxiao', ''], ['Lei', 'Yu', ''], ['Cui', 'Li', ''], ['Xue', 'Weiying', ''], ['Liu', 'Qi', ''], ['Wei', 'Zhenao', '']]","[{'text': 'zero-shot learning', 'label': 'Few-shot Learning'}, {'text': 'weakly supervised\nlearning', 'label': 'Few-shot Learning'}, {'text': 'large-scale language models', 'label': 'Large Language Model'}]",Large Language Model,large-scale language models,0.9086186289787292
2409.13642,Md Nakhla Rafi,"Md Nakhla Rafi, Dong Jae Kim, Tse-Hsun Chen, Shaowei Wang","A Multi-Agent Approach to Fault Localization via Graph-Based Retrieval
  and Reflexion",,,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Identifying and resolving software faults remains a challenging and
resource-intensive process. Traditional fault localization techniques, such as
Spectrum-Based Fault Localization (SBFL), leverage statistical analysis of test
coverage but often suffer from limited accuracy. While learning-based
approaches improve fault localization, they demand extensive training datasets
and high computational resources. Recent advances in Large Language Models
(LLMs) offer new opportunities by enhancing code understanding and reasoning.
However, existing LLM-based fault localization techniques face significant
challenges, including token limitations, performance degradation with long
inputs, and scalability issues in complex software systems. To overcome these
obstacles, we propose LLM4FL, a multi-agent fault localization framework that
utilizes three specialized LLM agents. First, the Context Extraction Agent
applies an order-sensitive segmentation strategy to partition large coverage
data within the LLM's token limit, analyze failure context, and prioritize
failure-related methods. The Debugger Agent then processes the extracted data,
which employs graph-based retrieval-augmented code navigation to reason about
failure causes and rank suspicious methods. Finally, the Reviewer Agent
re-evaluates the identified faulty methods using verbal reinforcement learning,
engaging in self-criticism and iterative refinement. Evaluated on the Defects4J
(V2.0.0) benchmark, which includes 675 faults from 14 Java projects, LLM4FL
achieves an 18.55\% improvement in Top-1 accuracy over AutoFL and 4.82\% over
SoapFL. It outperforms supervised techniques such as DeepFL and Grace, all
without requiring task-specific training. Furthermore, its coverage
segmentation and prompt chaining strategies enhance performance, increasing
Top-1 accuracy by up to 22\%.
","[{'version': 'v1', 'created': 'Fri, 20 Sep 2024 16:47:34 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 04:22:52 GMT'}]",2025-03-20,"[['Rafi', 'Md Nakhla', ''], ['Kim', 'Dong Jae', ''], ['Chen', 'Tse-Hsun', ''], ['Wang', 'Shaowei', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'verbal reinforcement learning', 'label': 'Few-shot Learning'}]",Large Language Model,Large Language Models,0.9664971828460693
2409.18042,Kai Chen,"Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu,
  Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, Dingdong Wang, Kun Xiang, Haoyuan
  Li, Haoli Bai, Jianhua Han, Xiaohui Li, Weike Jin, Nian Xie, Yu Zhang, James
  T. Kwok, Hengshuang Zhao, Xiaodan Liang, Dit-Yan Yeung, Xiao Chen, Zhenguo
  Li, Wei Zhang, Qun Liu, Jun Yao, Lanqing Hong, Lu Hou, Hang Xu","EMOVA: Empowering Language Models to See, Hear and Speak with Vivid
  Emotions",Accepted by CVPR 2025. Project Page: https://emova-ollm.github.io/,,,,cs.CV cs.CL,http://creativecommons.org/licenses/by/4.0/,"  GPT-4o, an omni-modal model that enables vocal conversations with diverse
emotions and tones, marks a milestone for omni-modal foundation models.
However, empowering Large Language Models to perceive and generate images,
texts, and speeches end-to-end with publicly available data remains challenging
for the open-source community. Existing vision-language models rely on external
tools for speech processing, while speech-language models still suffer from
limited or totally without vision-understanding capabilities. To address this
gap, we propose the EMOVA (EMotionally Omni-present Voice Assistant), to enable
Large Language Models with end-to-end speech abilities while maintaining the
leading vision-language performance. With a semantic-acoustic disentangled
speech tokenizer, we surprisingly notice that omni-modal alignment can further
enhance vision-language and speech abilities compared with the bi-modal aligned
counterparts. Moreover, a lightweight style module is introduced for the
flexible speech style controls including emotions and pitches. For the first
time, EMOVA achieves state-of-the-art performance on both the vision-language
and speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue
with vivid emotions.
","[{'version': 'v1', 'created': 'Thu, 26 Sep 2024 16:44:02 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Oct 2024 06:25:52 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 14:51:04 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 08:47:39 GMT'}]",2025-03-21,"[['Chen', 'Kai', ''], ['Gou', 'Yunhao', ''], ['Huang', 'Runhui', ''], ['Liu', 'Zhili', ''], ['Tan', 'Daxin', ''], ['Xu', 'Jing', ''], ['Wang', 'Chunwei', ''], ['Zhu', 'Yi', ''], ['Zeng', 'Yihan', ''], ['Yang', 'Kuo', ''], ['Wang', 'Dingdong', ''], ['Xiang', 'Kun', ''], ['Li', 'Haoyuan', ''], ['Bai', 'Haoli', ''], ['Han', 'Jianhua', ''], ['Li', 'Xiaohui', ''], ['Jin', 'Weike', ''], ['Xie', 'Nian', ''], ['Zhang', 'Yu', ''], ['Kwok', 'James T.', ''], ['Zhao', 'Hengshuang', ''], ['Liang', 'Xiaodan', ''], ['Yeung', 'Dit-Yan', ''], ['Chen', 'Xiao', ''], ['Li', 'Zhenguo', ''], ['Zhang', 'Wei', ''], ['Liu', 'Qun', ''], ['Yao', 'Jun', ''], ['Hong', 'Lanqing', ''], ['Hou', 'Lu', ''], ['Xu', 'Hang', '']]","[{'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'open-source community', 'label': 'Open-source LLMs'}, {'text': 'vision-language models', 'label': 'Large Language Model'}, {'text': 'speech-language models', 'label': 'Large Language Model'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2409.19606,Defa Zhu,"Defa Zhu, Hongzhi Huang, Zihao Huang, Yutao Zeng, Yunyao Mao, Banggu
  Wu, Qiyang Min, Xun Zhou",Hyper-Connections,,,,,cs.LG cs.CL cs.CV cs.NE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present hyper-connections, a simple yet effective method that can serve as
an alternative to residual connections. This approach specifically addresses
common drawbacks observed in residual connection variants, such as the seesaw
effect between gradient vanishing and representation collapse. Theoretically,
hyper-connections allow the network to adjust the strength of connections
between features at different depths and dynamically rearrange layers. We
conduct experiments focusing on the pre-training of large language models,
including dense and sparse models, where hyper-connections show significant
performance improvements over residual connections. Additional experiments
conducted on vision tasks also demonstrate similar improvements. We anticipate
that this method will be broadly applicable and beneficial across a wide range
of AI problems.
","[{'version': 'v1', 'created': 'Sun, 29 Sep 2024 07:57:07 GMT'}, {'version': 'v2', 'created': 'Thu, 28 Nov 2024 08:09:05 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 10:12:54 GMT'}]",2025-03-19,"[['Zhu', 'Defa', ''], ['Huang', 'Hongzhi', ''], ['Huang', 'Zihao', ''], ['Zeng', 'Yutao', ''], ['Mao', 'Yunyao', ''], ['Wu', 'Banggu', ''], ['Min', 'Qiyang', ''], ['Zhou', 'Xun', '']]","[{'text': 'residual connections', 'label': 'Embedding'}, {'text': 'large language models', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2409.19753,Yike Wu,"Yike Wu, Yi Huang, Nan Hu, Yuncheng Hua, Guilin Qi, Jiaoyan Chen, Jeff
  Z. Pan","CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex
  Knowledge Graph Question Answering",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent studies have explored the use of Large Language Models (LLMs) with
Retrieval Augmented Generation (RAG) for Knowledge Graph Question Answering
(KGQA). They typically require rewriting retrieved subgraphs into natural
language formats comprehensible to LLMs. However, when tackling complex
questions, the knowledge rewritten by existing methods may include irrelevant
information, omit crucial details, or fail to align with the question's
semantics. To address them, we propose a novel rewriting method CoTKR,
Chain-of-Thought Enhanced Knowledge Rewriting, for generating reasoning traces
and corresponding knowledge in an interleaved manner, thereby mitigating the
limitations of single-step knowledge rewriting. Additionally, to bridge the
preference gap between the knowledge rewriter and the question answering (QA)
model, we propose a training strategy PAQAF, Preference Alignment from Question
Answering Feedback, for leveraging feedback from the QA model to further
optimize the knowledge rewriter. We conduct experiments using various LLMs
across several KGQA benchmarks. Experimental results demonstrate that, compared
with previous knowledge rewriting methods, CoTKR generates the most beneficial
knowledge representation for QA models, which significantly improves the
performance of LLMs in KGQA.
","[{'version': 'v1', 'created': 'Sun, 29 Sep 2024 16:08:45 GMT'}, {'version': 'v2', 'created': 'Tue, 8 Oct 2024 14:20:15 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 09:37:10 GMT'}]",2025-03-20,"[['Wu', 'Yike', ''], ['Huang', 'Yi', ''], ['Hu', 'Nan', ''], ['Hua', 'Yuncheng', ''], ['Qi', 'Guilin', ''], ['Chen', 'Jiaoyan', ''], ['Pan', 'Jeff Z.', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2409.20089,Lei Yu,"Lei Yu, Virginie Do, Karen Hambardzumyan, Nicola Cancedda",Robust LLM safeguarding via refusal feature adversarial training,,,,,cs.LG cs.CL cs.CR,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) are vulnerable to adversarial attacks that can
elicit harmful responses. Defending against such attacks remains challenging
due to the opacity of jailbreaking mechanisms and the high computational cost
of training LLMs robustly. We demonstrate that adversarial attacks share a
universal mechanism for circumventing LLM safeguards that works by ablating a
dimension in the residual stream embedding space called the refusal feature. We
further show that the operation of refusal feature ablation (RFA) approximates
the worst-case perturbation of offsetting model safety. Based on these
findings, we propose Refusal Feature Adversarial Training (ReFAT), a novel
algorithm that efficiently performs LLM adversarial training by simulating the
effect of input-level attacks via RFA. Experiment results show that ReFAT
significantly improves the robustness of three popular LLMs against a wide
range of adversarial attacks, with considerably less computational overhead
compared to existing adversarial training methods.
","[{'version': 'v1', 'created': 'Mon, 30 Sep 2024 08:41:39 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 15:28:18 GMT'}]",2025-03-21,"[['Yu', 'Lei', ''], ['Do', 'Virginie', ''], ['Hambardzumyan', 'Karen', ''], ['Cancedda', 'Nicola', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'residual stream embedding space', 'label': 'Embedding'}, {'text': 'refusal feature', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2410.02650,Ali Satvaty,"Ali Satvaty, Suzan Verberne, Fatih Turkmen",Undesirable Memorization in Large Language Models: A Survey,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  While recent research increasingly showcases the remarkable capabilities of
Large Language Models (LLMs), it is equally crucial to examine their associated
risks. Among these, privacy and security vulnerabilities are particularly
concerning, posing significant ethical and legal challenges. At the heart of
these vulnerabilities stands memorization, which refers to a model's tendency
to store and reproduce phrases from its training data. This phenomenon has been
shown to be a fundamental source to various privacy and security attacks
against LLMs. In this paper, we provide a taxonomy of the literature on LLM
memorization, exploring it across three dimensions: granularity,
retrievability, and desirability. Next, we discuss the metrics and methods used
to quantify memorization, followed by an analysis of the causes and factors
that contribute to memorization phenomenon. We then explore strategies that are
used so far to mitigate the undesirable aspects of this phenomenon. We conclude
our survey by identifying potential research topics for the near future,
including methods to balance privacy and performance, and the analysis of
memorization in specific LLM contexts such as conversational agents,
retrieval-augmented generation, and diffusion language models. Given the rapid
research pace in this field, we also maintain a dedicated repository of the
references discussed in this survey which will be regularly updated to reflect
the latest developments.
","[{'version': 'v1', 'created': 'Thu, 3 Oct 2024 16:34:46 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 18:50:38 GMT'}]",2025-03-21,"[['Satvaty', 'Ali', ''], ['Verberne', 'Suzan', ''], ['Turkmen', 'Fatih', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ethical and legal challenges', 'label': 'AI Ethics'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'desirability', 'label': 'Model Bias and Fairness'}, {'text': 'conversational agents', 'label': 'ChatGPT'}]",Large Language Model,Large Language Models,0.9664971828460693
2410.03781,Romain Puech,"Romain Puech, Jakub Macina, Julia Chatain, Mrinmaya Sachan, Manu Kapur","Towards the Pedagogical Steering of Large Language Models for Tutoring:
  A Case Study with Modeling Productive Failure","19 pages, 10 figures, 6 tables",,,,cs.HC cs.AI cs.CY cs.MA,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  One-to-one tutoring is one of the most efficient methods of teaching. With
the growing popularity of Large Language Models (LLMs), there have been efforts
to create LLM based conversational tutors which can expand the benefits of one
to one tutoring to everyone. However, current LLMs are trained primarily to be
helpful assistants and lack crucial pedagogical skills. For example, they often
quickly reveal the solution to the student and fail to plan for a richer multi
turn pedagogical interaction. To use LLMs in pedagogical settings, they need to
be steered to use effective teaching strategies: a problem we introduce as
Pedagogical Steering. We develop StratL, an algorithm to optimize LLM prompts
and steer it to follow a predefined multi-turn tutoring plan represented as a
transition graph. As a case study, we create a prototype tutor for high school
math following Productive Failure (PF), an advanced and effective learning
design. To validate our approach in a real-world setting, we run a field study
with 17 high school students in Singapore and show that StratL succeeds in
steering the LLM to follow the PF tutoring strategy. Finally, we highlight
challenges in Pedagogical Steering of LLMs and offer opportunities for further
improvements by publishing a dataset of PF problems and our code.
","[{'version': 'v1', 'created': 'Thu, 3 Oct 2024 16:15:41 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 19:44:51 GMT'}]",2025-03-20,"[['Puech', 'Romain', ''], ['Macina', 'Jakub', ''], ['Chatain', 'Julia', ''], ['Sachan', 'Mrinmaya', ''], ['Kapur', 'Manu', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Pedagogical Steering', 'label': 'Prompting'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'Pedagogical Steering', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2410.03834,Tao Feng,"Tao Feng, Yanzhen Shen, Jiaxuan You",GraphRouter: A Graph-based Router for LLM Selections,,,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The rapidly growing number and variety of Large Language Models (LLMs)
present significant challenges in efficiently selecting the appropriate LLM for
a given query, especially considering the trade-offs between performance and
computational cost. Current LLM selection methods often struggle to generalize
across new LLMs and different tasks because of their limited ability to
leverage contextual interactions among tasks, queries, and LLMs, as well as
their dependence on a transductive learning framework. To address these
shortcomings, we introduce a novel inductive graph framework, named as
GraphRouter, which fully utilizes the contextual information among tasks,
queries, and LLMs to enhance the LLM selection process. GraphRouter constructs
a heterogeneous graph comprising task, query, and LLM nodes, with interactions
represented as edges, which efficiently captures the contextual information
between the query's requirements and the LLM's capabilities. Through an
innovative edge prediction mechanism, GraphRouter is able to predict attributes
(the effect and cost of LLM response) of potential edges, allowing for
optimized recommendations that adapt to both existing and newly introduced LLMs
without requiring retraining. Comprehensive experiments across three distinct
effect-cost weight scenarios have shown that GraphRouter substantially
surpasses existing routers, delivering a minimum performance improvement of
12.3%. In addition, it achieves enhanced generalization across new LLMs
settings and supports diverse tasks with at least a 9.5% boost in effect and a
significant reduction in computational demands. This work endeavors to apply a
graph-based approach for the contextual and adaptive selection of LLMs,
offering insights for real-world applications. Our codes for GraphRouter is
released at https://github.com/ulab-uiuc/GraphRouter.
","[{'version': 'v1', 'created': 'Fri, 4 Oct 2024 18:02:48 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 15:08:47 GMT'}]",2025-03-18,"[['Feng', 'Tao', ''], ['Shen', 'Yanzhen', ''], ['You', 'Jiaxuan', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2410.05406,Carlo Bosio,Carlo Bosio and Mark W. Mueller,"Synthesizing Interpretable Control Policies through Large Language Model
  Guided Search","8 pages, 7 figures, conference paper",,,,cs.AI cs.SY eess.SY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The combination of Large Language Models (LLMs), systematic evaluation, and
evolutionary algorithms has enabled breakthroughs in combinatorial optimization
and scientific discovery. We propose to extend this powerful combination to the
control of dynamical systems, generating interpretable control policies capable
of complex behaviors. With our novel method, we represent control policies as
programs in standard languages like Python. We evaluate candidate controllers
in simulation and evolve them using a pre-trained LLM. Unlike conventional
learning-based control techniques, which rely on black-box neural networks to
encode control policies, our approach enhances transparency and
interpretability. We still take advantage of the power of large AI models, but
only at the policy design phase, ensuring that all system components remain
interpretable and easily verifiable at runtime. Additionally, the use of
standard programming languages makes it straightforward for humans to finetune
or adapt the controllers based on their expertise and intuition. We illustrate
our method through its application to the synthesis of an interpretable control
policy for the pendulum swing-up and the ball in cup tasks. We make the code
available at
https://github.com/muellerlab/synthesizing_interpretable_control_policies.git.
","[{'version': 'v1', 'created': 'Mon, 7 Oct 2024 18:12:20 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 21:49:35 GMT'}]",2025-03-19,"[['Bosio', 'Carlo', ''], ['Mueller', 'Mark W.', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2410.06981,Michael Lan,"Michael Lan, Philip Torr, Austin Meek, Ashkan Khakzar, David Krueger,
  Fazl Barez","Sparse Autoencoders Reveal Universal Feature Spaces Across Large
  Language Models",,,,,cs.LG cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We investigate feature universality in large language models (LLMs), a
research field that aims to understand how different models similarly represent
concepts in the latent spaces of their intermediate layers. Demonstrating
feature universality allows discoveries about latent representations to
generalize across several models. However, comparing features across LLMs is
challenging due to polysemanticity, in which individual neurons often
correspond to multiple features rather than distinct ones, making it difficult
to disentangle and match features across different models. To address this
issue, we employ a method known as dictionary learning by using sparse
autoencoders (SAEs) to transform LLM activations into more interpretable spaces
spanned by neurons corresponding to individual features. After matching feature
neurons across models via activation correlation, we apply representational
space similarity metrics on SAE feature spaces across different LLMs. Our
experiments reveal significant similarities in SAE feature spaces across
various LLMs, providing new evidence for feature universality.
","[{'version': 'v1', 'created': 'Wed, 9 Oct 2024 15:18:57 GMT'}, {'version': 'v2', 'created': 'Fri, 31 Jan 2025 15:27:10 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 00:31:46 GMT'}]",2025-03-18,"[['Lan', 'Michael', ''], ['Torr', 'Philip', ''], ['Meek', 'Austin', ''], ['Khakzar', 'Ashkan', ''], ['Krueger', 'David', ''], ['Barez', 'Fazl', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'dictionary learning', 'label': 'Few-shot Learning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2410.08437,Daniel Bramblett,"Rushang Karia, Daniel Bramblett, Daksh Dobhal, Siddharth Srivastava","AutoEval: Autonomous Evaluation of LLMs for Truth Maintenance and
  Reasoning Tasks",,,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents AutoEval, a novel benchmark for scaling Large Language
Model (LLM) assessment in formal tasks with clear notions of correctness, such
as truth maintenance in translation and logical reasoning. AutoEval is the
first benchmarking paradigm that offers several key advantages necessary for
scaling objective evaluation of LLMs without human labeling: (a) ability to
evaluate LLMs of increasing sophistication by auto-generating tasks at
different levels of difficulty; (b) auto-generation of ground truth that
eliminates dependence on expensive and time-consuming human annotation; (c) the
use of automatically generated, randomized datasets that mitigate the ability
of successive LLMs to overfit to static datasets used in many contemporary
benchmarks. Empirical analysis shows that an LLM's performance on AutoEval is
highly indicative of its performance on a diverse array of other benchmarks
focusing on translation and reasoning tasks, making it a valuable autonomous
evaluation paradigm in settings where hand-curated datasets can be hard to
obtain and/or update.
","[{'version': 'v1', 'created': 'Fri, 11 Oct 2024 00:56:37 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 21:03:16 GMT'}]",2025-03-19,"[['Karia', 'Rushang', ''], ['Bramblett', 'Daniel', ''], ['Dobhal', 'Daksh', ''], ['Srivastava', 'Siddharth', '']]","[{'text': 'Large Language\nModel', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]",Large Language Model,"Large Language
Model",1.0
2410.09907,Alexandru-Iulius Jerpelea,Ecaterina \c{S}tef\u{a}nescu and Alexandru-Iulius Jerpelea,Reddit is all you need: Authorship profiling for Romanian,"10 pages, 5 tables and 1 figure, published and presented at The 19th
  International Conference on Linguistic Resources and Tools for Natural
  Language Processing (ConsILR 2024)",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Authorship profiling is the process of identifying an author's
characteristics based on their writings. This centuries old problem has become
more intriguing especially with recent developments in Natural Language
Processing (NLP). In this paper, we introduce a corpus of short texts in the
Romanian language, annotated with certain author characteristic keywords; to
our knowledge, the first of its kind. In order to do this, we exploit a social
media platform called Reddit. We leverage its thematic community-based
structure (subreddits structure), which offers information about the author's
background. We infer an user's demographic and some broad personal traits, such
as age category, employment status, interests, and social orientation based on
the subreddit and other cues. We thus obtain a 23k+ samples corpus, extracted
from 100+ Romanian subreddits. We analyse our dataset, and finally, we
fine-tune and evaluate Large Language Models (LLMs) to prove baselines
capabilities for authorship profiling using the corpus, indicating the need for
further research in the field. We publicly release all our resources.
","[{'version': 'v1', 'created': 'Sun, 13 Oct 2024 16:27:31 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 19:48:28 GMT'}]",2025-03-20,"[['Ştefănescu', 'Ecaterina', ''], ['Jerpelea', 'Alexandru-Iulius', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2410.10491,Aritra Bhowmik,"Aritra Bhowmik, Mohammad Mahdi Derakhshani, Dennis Koelma, Yuki M.
  Asano, Martin R. Oswald, Cees G. M. Snoek",TWIST & SCOUT: Grounding Multimodal LLM-Experts by Forget-Free Tuning,,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Spatial awareness is key to enable embodied multimodal AI systems. Yet,
without vast amounts of spatial supervision, current Multimodal Large Language
Models (MLLMs) struggle at this task. In this paper, we introduce TWIST &
SCOUT, a framework that equips pre-trained MLLMs with visual grounding ability
without forgetting their existing image and language understanding skills. To
this end, we propose TWIST, a twin-expert stepwise tuning module that modifies
the decoder of the language model using one frozen module pre-trained on image
understanding tasks and another learnable one for visual grounding tasks. This
allows the MLLM to retain previously learned knowledge and skills, while
acquiring what is missing. To fine-tune the model effectively, we generate a
high-quality synthetic dataset we call SCOUT, which mimics human reasoning in
visual grounding. This dataset provides rich supervision signals, describing a
step-by-step multimodal reasoning process, thereby simplifying the task of
visual grounding. We evaluate our approach on several standard benchmark
datasets, encompassing grounded image captioning, zero-shot localization, and
visual grounding tasks. Our method consistently delivers strong performance
across all tasks, while retaining the pre-trained image understanding
capabilities.
","[{'version': 'v1', 'created': 'Mon, 14 Oct 2024 13:35:47 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 15:32:47 GMT'}]",2025-03-21,"[['Bhowmik', 'Aritra', ''], ['Derakhshani', 'Mohammad Mahdi', ''], ['Koelma', 'Dennis', ''], ['Asano', 'Yuki M.', ''], ['Oswald', 'Martin R.', ''], ['Snoek', 'Cees G. M.', '']]","[{'text': 'Multimodal Large Language\nModels', 'label': 'Large Language Model'}, {'text': 'TWIST', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'zero-shot localization', 'label': 'Zero-shot Learning'}]",Large Language Model,"Multimodal Large Language
Models",0.7649828195571899
2410.10624,Zechen Li,"Zechen Li, Shohreh Deldari, Linyao Chen, Hao Xue and Flora D. Salim","SensorLLM: Aligning Large Language Models with Motion Sensors for Human
  Activity Recognition",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce SensorLLM, a two-stage framework that enables Large Language
Models (LLMs) to perform human activity recognition (HAR) from sensor data.
Despite their strong reasoning and generalization capabilities, LLMs remain
underutilized for motion sensor data due to the lack of semantic context in
time-series, computational constraints, and challenges in processing numerical
inputs. SensorLLM addresses these limitations through a Sensor-Language
Alignment stage, where we introduce special tokens for each sensor channel and
automatically generate textual trend descriptions. This alignment enables LLMs
to capture numerical variations, channel-specific features, and data of varying
duration--without requiring human annotations. In the subsequent Task-Aware
Tuning stage, we refine the model for HAR classification, achieving performance
that matches or surpasses state-of-the-art methods. Our results demonstrate
that SensorLLM evolves into an effective sensor learner, reasoner, and
classifier through Sensor-Language Alignment, generalizing across diverse HAR
datasets. We believe this work establishes a foundation for future research on
time-series and text alignment, paving the way for foundation models in sensor
data analysis.
","[{'version': 'v1', 'created': 'Mon, 14 Oct 2024 15:30:41 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 09:28:43 GMT'}]",2025-03-18,"[['Li', 'Zechen', ''], ['Deldari', 'Shohreh', ''], ['Chen', 'Linyao', ''], ['Xue', 'Hao', ''], ['Salim', 'Flora D.', '']]","[{'text': 'SensorLLM', 'label': 'Foundation Model'}, {'text': 'Large Language\nModels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'SensorLLM', 'label': 'Foundation Model'}, {'text': 'Sensor-Language\nAlignment', 'label': 'contextual Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Task-Aware\nTuning stage', 'label': 'Fine-tuning'}, {'text': 'SensorLLM', 'label': 'Foundation Model'}, {'text': 'Sensor-Language Alignment', 'label': 'Embedding'}, {'text': 'foundation models', 'label': 'Foundation Model'}]",Large Language Model,"Large Language
Models",0.9664971828460693
2410.11374,Yoonjeon Kim,"Yoonjeon Kim, Soohyun Ryu, Yeonsung Jung, Hyunkoo Lee, Joowon Kim,
  June Yong Yang, Jaeryong Hwang, Eunho Yang","Preserve or Modify? Context-Aware Evaluation for Balancing Preservation
  and Modification in Text-Guided Image Editing",accepted to CVPR 2025,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The development of vision-language and generative models has significantly
advanced text-guided image editing, which seeks the preservation of core
elements in the source image while implementing modifications based on the
target text. However, existing metrics have a context-blindness problem,
indiscriminately applying the same evaluation criteria on completely different
pairs of source image and target text, biasing towards either modification or
preservation. Directional CLIP similarity, the only metric that considers both
source image and target text, is also biased towards modification aspects and
attends to irrelevant editing regions of the image. We propose AugCLIP, a
context-aware metric that adaptively coordinates preservation and modification
aspects, depending on the specific context of a given source image and target
text. This is done by deriving the CLIP representation of an ideally edited
image, that preserves the source image with necessary modifications to align
with target text. More specifically, using a multi-modal large language model,
AugCLIP augments the textual descriptions of the source and target, then
calculates a modification vector through a hyperplane that separates source and
target attributes in CLIP space. Extensive experiments on five benchmark
datasets, encompassing a diverse range of editing scenarios, show that AugCLIP
aligns remarkably well with human evaluation standards, outperforming existing
metrics. The code is available at https://github.com/augclip/augclip_eval.
","[{'version': 'v1', 'created': 'Tue, 15 Oct 2024 08:12:54 GMT'}, {'version': 'v2', 'created': 'Wed, 4 Dec 2024 07:35:20 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 07:36:52 GMT'}]",2025-03-21,"[['Kim', 'Yoonjeon', ''], ['Ryu', 'Soohyun', ''], ['Jung', 'Yeonsung', ''], ['Lee', 'Hyunkoo', ''], ['Kim', 'Joowon', ''], ['Yang', 'June Yong', ''], ['Hwang', 'Jaeryong', ''], ['Yang', 'Eunho', '']]","[{'text': 'multi-modal large language model', 'label': 'Large Language Model'}]",Large Language Model,multi-modal large language model,0.8085874915122986
2410.12444,Mengze Hong,"Mengze Hong, Chen Jason Zhang, Di Jiang, Yuanfeng Song, Lu Wang,
  Yuanqin He, Zhiyang Su, Qing Li","Expanding Chatbot Knowledge in Customer Service: Context-Aware Similar
  Question Generation Using Large Language Models",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Service chatbots play an important role in enhancing customer support by
delivering timely responses to diverse queries. Traditionally, these chatbots
rely on retrieval-based methods constrained by a predefined knowledge base of
question-answer (QA) pairs to guarantee reliable responses. To effectively
handle varied customer inquiries, augmenting the knowledge base with similar
questions that maintain semantic consistency and linguistic variability is
crucial. This paper presents methodologies for a novel approach that utilizes
Large Language Models (LLMs) for generating similar questions and selecting an
optimal subset of questions for knowledge base augmentation in industrial
chatbots. Specifically, we define the SQG task in the context of LLM training
and propose a one-to-many objective that incorporates contextual information.
We also introduce an optimization framework that selects a diverse subset of
similar questions within predefined resource constraints. Experimental results
demonstrate significant improvements over traditional methods, achieving
greater semantic diversity while aligning with source QA pairs, with over 120%
relative improvement in meeting business-specific requirements with human
evaluation. Combined with several best practices, we provide a robust,
application-driven solution for enhancing chatbot performance and improving
customer service satisfaction.
","[{'version': 'v1', 'created': 'Wed, 16 Oct 2024 10:48:14 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 06:22:38 GMT'}]",2025-03-20,"[['Hong', 'Mengze', ''], ['Zhang', 'Chen Jason', ''], ['Jiang', 'Di', ''], ['Song', 'Yuanfeng', ''], ['Wang', 'Lu', ''], ['He', 'Yuanqin', ''], ['Su', 'Zhiyang', ''], ['Li', 'Qing', '']]","[{'text': 'Service chatbots', 'label': 'ChatGPT'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2410.12782,Yida Yin,"Yida Yin, Zekai Wang, Yuvan Sharma, Dantong Niu, Trevor Darrell, Roei
  Herzig",In-Context Learning Enables Robot Action Prediction in LLMs,Published in ICRA 2025,,,,cs.RO cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, Large Language Models (LLMs) have achieved remarkable success using
in-context learning (ICL) in the language domain. However, leveraging the ICL
capabilities within LLMs to directly predict robot actions remains largely
unexplored. In this paper, we introduce RoboPrompt, a framework that enables
off-the-shelf text-only LLMs to directly predict robot actions through ICL
without training. Our approach first heuristically identifies keyframes that
capture important moments from an episode. Next, we extract end-effector
actions from these keyframes as well as the estimated initial object poses, and
both are converted into textual descriptions. Finally, we construct a
structured template to form ICL demonstrations from these textual descriptions
and a task instruction. This enables an LLM to directly predict robot actions
at test time. Through extensive experiments and analysis, RoboPrompt shows
stronger performance over zero-shot and ICL baselines in simulated and
real-world settings. Our project page is available at
https://davidyyd.github.io/roboprompt.
","[{'version': 'v1', 'created': 'Wed, 16 Oct 2024 17:56:49 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 10:43:54 GMT'}]",2025-03-18,"[['Yin', 'Yida', ''], ['Wang', 'Zekai', ''], ['Sharma', 'Yuvan', ''], ['Niu', 'Dantong', ''], ['Darrell', 'Trevor', ''], ['Herzig', 'Roei', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'RoboPrompt', 'label': 'Open-source LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'RoboPrompt', 'label': 'Open-source LLMs'}, {'text': 'zero-shot', 'label': 'Zero-shot Learning'}, {'text': 'ICL', 'label': 'Few-shot Learning'}]",Large Language Model,Large Language Models,0.9664971828460693
2410.13788,Michael J.Q. Zhang,"Michael J.Q. Zhang, W. Bradley Knox, Eunsol Choi","Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying
  Questions",Presented at ICLR 2025,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  Large language models (LLMs) must often respond to highly ambiguous user
requests. In such cases, the LLM's best response may be to ask a clarifying
question to elicit more information. Existing LLMs often respond by
presupposing a single interpretation of such ambiguous requests, frustrating
users who intended a different interpretation. We speculate this is caused by
current preference data labeling practice, where LLM responses are evaluated
only on their prior contexts. To address this, we assign preference labels by
simulating their expected outcomes in future turns. This allows LLMs to learn
to ask clarifying questions when it can generate responses that are tailored to
each user interpretation in future turns. On open-domain QA datasets with
multiple annotations, we evaluate systems based on their ability to ask
clarifying questions to recover each user's interpretation and expected answer.
We compare systems trained using our proposed preference labeling methods
against standard methods, which assign preferences based on only prior context.
Our method achieves a 5% improvement in F1 measured against the answer set from
different interpretations of each query, showing the value of modeling future
conversation turns. We further demonstrate that our method can be used to train
models to judiciously determine when to ask clarifying questions, directly
answering the question when clarification is unnecessary. In our experiments,
we find that our method achieves a 3% improvement in accuracy of such judgments
over existing methods.
","[{'version': 'v1', 'created': 'Thu, 17 Oct 2024 17:29:04 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 14:17:47 GMT'}]",2025-03-19,"[['Zhang', 'Michael J. Q.', ''], ['Knox', 'W. Bradley', ''], ['Choi', 'Eunsol', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2410.17250,Shota Onohara,"Shota Onohara, Atsuyuki Miyai, Yuki Imajuku, Kazuki Egashira, Jeonghun
  Baek, Xiang Yue, Graham Neubig, Kiyoharu Aizawa","JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding
  Benchmark for Culture-aware Evaluation","Accepted at NAACL 2025. Project page:
  https://mmmu-japanese-benchmark.github.io/JMMMU/",,,,cs.CL cs.AI cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Accelerating research on Large Multimodal Models (LMMs) in non-English
languages is crucial for enhancing user experiences across broader populations.
In this paper, we introduce JMMMU (Japanese MMMU), the first large-scale
Japanese benchmark designed to evaluate LMMs on expert-level tasks based on the
Japanese cultural context. To facilitate comprehensive culture-aware
evaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA)
subset, where the culture-independent subjects (e.g., Math) are selected and
translated into Japanese, enabling one-to-one comparison with its English
counterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly
crafted subjects that reflect Japanese cultural context. Using the CA subset,
we observe performance drop in many LMMs when evaluated in Japanese, which is
purely attributable to language variation. Using the CS subset, we reveal their
inadequate Japanese cultural understanding. Further, by combining both subsets,
we identify that some LMMs perform well on the CA subset but not on the CS
subset, exposing a shallow understanding of the Japanese language that lacks
depth in cultural understanding. We hope this work will not only help advance
LMM performance in Japanese but also serve as a guideline to create
high-standard, culturally diverse benchmarks for multilingual LMM development.
The project page is https://mmmu-japanese-benchmark.github.io/JMMMU/.
","[{'version': 'v1', 'created': 'Tue, 22 Oct 2024 17:59:56 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 08:24:14 GMT'}]",2025-03-20,"[['Onohara', 'Shota', ''], ['Miyai', 'Atsuyuki', ''], ['Imajuku', 'Yuki', ''], ['Egashira', 'Kazuki', ''], ['Baek', 'Jeonghun', ''], ['Yue', 'Xiang', ''], ['Neubig', 'Graham', ''], ['Aizawa', 'Kiyoharu', '']]","[{'text': 'Large Multimodal Models', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}]",Large Language Model,Large Multimodal Models,0.573912501335144
2410.18325,Kim Sung-Bin,"Kim Sung-Bin, Oh Hyun-Bin, JungMok Lee, Arda Senocak, Joon Son Chung,
  Tae-Hyun Oh","AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large
  Language Models",ICLR 2025,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Following the success of Large Language Models (LLMs), expanding their
boundaries to new modalities represents a significant paradigm shift in
multimodal understanding. Human perception is inherently multimodal, relying
not only on text but also on auditory and visual cues for a complete
understanding of the world. In recognition of this fact, audio-visual LLMs have
recently emerged. Despite promising developments, the lack of dedicated
benchmarks poses challenges for understanding and evaluating models. In this
work, we show that audio-visual LLMs struggle to discern subtle relationships
between audio and visual signals, leading to hallucinations and highlighting
the need for reliable benchmarks. To address this, we introduce AVHBench, the
first comprehensive benchmark specifically designed to evaluate the perception
and comprehension capabilities of audio-visual LLMs. Our benchmark includes
tests for assessing hallucinations, as well as the cross-modal matching and
reasoning abilities of these models. Our results reveal that most existing
audio-visual LLMs struggle with hallucinations caused by cross-interactions
between modalities, due to their limited capacity to perceive complex
multimodal signals and their relationships. Additionally, we demonstrate that
simple training with our AVHBench improves robustness of audio-visual LLMs
against hallucinations. Dataset: https://github.com/kaist-ami/AVHBench
","[{'version': 'v1', 'created': 'Wed, 23 Oct 2024 23:36:06 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 08:14:35 GMT'}]",2025-03-18,"[['Sung-Bin', 'Kim', ''], ['Hyun-Bin', 'Oh', ''], ['Lee', 'JungMok', ''], ['Senocak', 'Arda', ''], ['Chung', 'Joon Son', ''], ['Oh', 'Tae-Hyun', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'audio-visual LLMs', 'label': 'Large Language Model'}, {'text': 'audio-visual LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2410.19482,Jamie Hayes,"Jamie Hayes, Marika Swanberg, Harsh Chaudhari, Itay Yona, Ilia
  Shumailov, Milad Nasr, Christopher A. Choquette-Choo, Katherine Lee, A. Feder
  Cooper",Measuring memorization in language models via probabilistic extraction,NAACL 25,,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) are susceptible to memorizing training data,
raising concerns about the potential extraction of sensitive information at
generation time. Discoverable extraction is the most common method for
measuring this issue: split a training example into a prefix and suffix, then
prompt the LLM with the prefix, and deem the example extractable if the LLM
generates the matching suffix using greedy sampling. This definition yields a
yes-or-no determination of whether extraction was successful with respect to a
single query. Though efficient to compute, we show that this definition is
unreliable because it does not account for non-determinism present in more
realistic (non-greedy) sampling schemes, for which LLMs produce a range of
outputs for the same prompt. We introduce probabilistic discoverable
extraction, which, without additional cost, relaxes discoverable extraction by
considering multiple queries to quantify the probability of extracting a target
sequence. We evaluate our probabilistic measure across different models,
sampling schemes, and training-data repetitions, and find that this measure
provides more nuanced information about extraction risk compared to traditional
discoverable extraction.
","[{'version': 'v1', 'created': 'Fri, 25 Oct 2024 11:37:04 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 14:25:10 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 15:35:56 GMT'}]",2025-03-21,"[['Hayes', 'Jamie', ''], ['Swanberg', 'Marika', ''], ['Chaudhari', 'Harsh', ''], ['Yona', 'Itay', ''], ['Shumailov', 'Ilia', ''], ['Nasr', 'Milad', ''], ['Choquette-Choo', 'Christopher A.', ''], ['Lee', 'Katherine', ''], ['Cooper', 'A. Feder', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2410.21113,Jo\~ao Pereira,"Joao Pereira, Vasco Lopes, David Semedo, Joao Neves",Zero-Shot Action Recognition in Surveillance Videos,,,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The growing demand for surveillance in public spaces presents significant
challenges due to the shortage of human resources. Current AI-based video
surveillance systems heavily rely on core computer vision models that require
extensive finetuning, which is particularly difficult in surveillance settings
due to limited datasets and difficult setting (viewpoint, low quality, etc.).
In this work, we propose leveraging Large Vision-Language Models (LVLMs), known
for their strong zero and few-shot generalization, to tackle video
understanding tasks in surveillance. Specifically, we explore VideoLLaMA2, a
state-of-the-art LVLM, and an improved token-level sampling method,
Self-Reflective Sampling (Self-ReS). Our experiments on the UCF-Crime dataset
show that VideoLLaMA2 represents a significant leap in zero-shot performance,
with 20% boost over the baseline. Self-ReS additionally increases zero-shot
action recognition performance to 44.6%. These results highlight the potential
of LVLMs, paired with improved sampling techniques, for advancing surveillance
video analysis in diverse scenarios.
","[{'version': 'v1', 'created': 'Mon, 28 Oct 2024 15:13:53 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 13:30:27 GMT'}]",2025-03-19,"[['Pereira', 'Joao', ''], ['Lopes', 'Vasco', ''], ['Semedo', 'David', ''], ['Neves', 'Joao', '']]","[{'text': 'extensive finetuning', 'label': 'Fine-tuning'}, {'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'VideoLLaMA2', 'label': 'Large Language Model'}, {'text': 'Self-Reflective Sampling', 'label': 'Few-shot Learning'}]",Large Language Model,Large Vision-Language Models,0.7742220759391785
2411.10156,Libo Wang,Libo Wang,"Mitigating Sycophancy in Decoder-Only Transformer Architectures:
  Synthetic Data Intervention","The data set, experimental process, code and data results have been
  uploaded to Github repository, the link is
  https://github.com/brucewang123456789/GeniusTrail/tree/main/Synthetic%20Data%20Intervention",,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  To address the sycophancy problem caused by reinforcement learning from human
feedback in large language models, this research applies synthetic data
intervention technology to the decoder-only transformer architecture. Based on
the research gaps in the existing literature, the researcher designed an
experimental process to reduce the tendency of models to cater by generating
diversified data, and used GPT4o as an experimental tool for verification. The
experiment used 100 true and false questions, and compared the performance of
the model trained with synthetic data intervention and the original untrained
model on multiple indicators. The results show that the SDI training model
supports the technology in terms of accuracy rate and sycophancy rate and has
significant effectiveness in reducing sycophancy phenomena.
","[{'version': 'v1', 'created': 'Fri, 15 Nov 2024 12:59:46 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Nov 2024 11:52:09 GMT'}, {'version': 'v3', 'created': 'Fri, 17 Jan 2025 09:49:37 GMT'}, {'version': 'v4', 'created': 'Fri, 24 Jan 2025 19:52:57 GMT'}, {'version': 'v5', 'created': 'Thu, 20 Mar 2025 13:29:49 GMT'}]",2025-03-21,"[['Wang', 'Libo', '']]","[{'text': 'reinforcement learning', 'label': 'Few-shot Learning'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'GPT4o', 'label': 'GPT'}]",Large Language Model,large language models,0.9664971828460693
2411.12951,Minjoon Jung,"Minjoon Jung, Junbin Xiao, Byoung-Tak Zhang, Angela Yao","On the Consistency of Video Large Language Models in Temporal
  Comprehension",Accepted to CVPR'25,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Video large language models (Video-LLMs) can temporally ground language
queries and retrieve video moments. Yet, such temporal comprehension
capabilities are neither well-studied nor understood. So we conduct a study on
prediction consistency -- a key indicator for robustness and trustworthiness of
temporal grounding. After the model identifies an initial moment within the
video content, we apply a series of probes to check if the model's responses
align with this initial grounding as an indicator of reliable comprehension.
Our results reveal that current Video-LLMs are sensitive to variations in video
contents, language queries, and task settings, unveiling severe deficiencies in
maintaining consistency. We further explore common prompting and
instruction-tuning methods as potential solutions, but find that their
improvements are often unstable. To that end, we propose event temporal
verification tuning that explicitly accounts for consistency, and demonstrate
significant improvements for both grounding and consistency. Our data and code
are open-sourced at https://github.com/minjoong507/Consistency-of-Video-LLM.
","[{'version': 'v1', 'created': 'Wed, 20 Nov 2024 00:47:17 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 02:21:38 GMT'}]",2025-03-18,"[['Jung', 'Minjoon', ''], ['Xiao', 'Junbin', ''], ['Zhang', 'Byoung-Tak', ''], ['Yao', 'Angela', '']]","[{'text': 'Video large language models', 'label': 'Large Language Model'}, {'text': 'Video-LLMs', 'label': 'Large Language Model'}, {'text': 'Video-LLMs', 'label': 'Large Language Model'}, {'text': 'event temporal\nverification tuning', 'label': 'Fine-tuning'}]",Large Language Model,Video large language models,0.7616418600082397
2411.18688,Soumya Suvra Ghosal Mr.,"Soumya Suvra Ghosal, Souradip Chakraborty, Vaibhav Singh, Tianrui
  Guan, Mengdi Wang, Ahmad Beirami, Furong Huang, Alvaro Velasquez, Dinesh
  Manocha, Amrit Singh Bedi","Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via
  Inference-Time Alignment",Accepted to CVPR 2025,,,,cs.CR cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the widespread deployment of Multimodal Large Language Models (MLLMs)
for visual-reasoning tasks, improving their safety has become crucial. Recent
research indicates that despite training-time safety alignment, these models
remain vulnerable to jailbreak attacks. In this work, we first highlight an
important safety gap to describe that alignment achieved solely through safety
training may be insufficient against jailbreak attacks. To address this
vulnerability, we propose Immune, an inference-time defense framework that
leverages a safe reward model through controlled decoding to defend against
jailbreak attacks. Additionally, we provide a mathematical characterization of
Immune, offering insights on why it improves safety against jailbreaks.
Extensive evaluations on diverse jailbreak benchmarks using recent MLLMs reveal
that Immune effectively enhances model safety while preserving the model's
original capabilities. For instance, against text-based jailbreak attacks on
LLaVA-1.6, Immune reduces the attack success rate by 57.82% and 16.78% compared
to the base MLLM and state-of-the-art defense strategy, respectively.
","[{'version': 'v1', 'created': 'Wed, 27 Nov 2024 19:00:10 GMT'}, {'version': 'v2', 'created': 'Fri, 20 Dec 2024 18:48:27 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 16:07:09 GMT'}]",2025-03-21,"[['Ghosal', 'Soumya Suvra', ''], ['Chakraborty', 'Souradip', ''], ['Singh', 'Vaibhav', ''], ['Guan', 'Tianrui', ''], ['Wang', 'Mengdi', ''], ['Beirami', 'Ahmad', ''], ['Huang', 'Furong', ''], ['Velasquez', 'Alvaro', ''], ['Manocha', 'Dinesh', ''], ['Bedi', 'Amrit Singh', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2411.19146,Ido Galil,"Akhiad Bercovich, Tomer Ronen, Talor Abramovich, Nir Ailon, Nave
  Assaf, Mohammad Dabbah, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak
  Golan, Netanel Haber, Ehud Karpas, Roi Koren, Itay Levy, Pavlo Molchanov,
  Shahar Mor, Zach Moshe, Najeeb Nabwani, Omri Puny, Ran Rubin, Itamar Schen,
  Ido Shahaf, Oren Tropp, Omer Ullman Argov, Ran Zilberstein, Ran El-Yaniv",Puzzle: Distillation-Based NAS for Inference-Optimized LLMs,,,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) offer remarkable capabilities, yet their high
inference costs restrict wider adoption. While increasing parameter counts
improves accuracy, it also broadens the gap between state-of-the-art
capabilities and practical deployability. We present Puzzle, a hardware-aware
framework that accelerates the inference of LLMs while preserving their
capabilities. Using neural architecture search (NAS) at a large-scale, Puzzle
optimizes models with tens of billions of parameters. Our approach utilizes
blockwise local knowledge distillation (BLD) for parallel architecture
exploration and employs mixed-integer programming for precise constraint
optimization.
  We showcase our framework's impact via Llama-3.1-Nemotron-51B-Instruct
(Nemotron-51B), a publicly available model derived from Llama-3.1-70B-Instruct.
Nemotron-51B achieves a 2.17x inference throughput speedup, fitting on a single
NVIDIA H100 GPU while retaining 98.4% of the original model's benchmark
accuracies. Notably, it is the most accurate model supporting single H100 GPU
inference with large batch sizes, despite training on only 45B tokens, far
fewer than the 15T used to train Llama-70B. Lastly, we derive
Llama-3.3-Nemotron-49B-Super-Base to demonstrate Puzzle can retain long-context
and that lightweight alignment on these derived models allows them to surpass
the parent model in specific capabilities. Our work establishes that powerful
LLM models can be optimized for efficient deployment with only negligible loss
in quality, underscoring that inference performance, not parameter count alone,
should guide model selection.
","[{'version': 'v1', 'created': 'Thu, 28 Nov 2024 13:45:42 GMT'}, {'version': 'v2', 'created': 'Tue, 3 Dec 2024 09:06:33 GMT'}, {'version': 'v3', 'created': 'Sun, 8 Dec 2024 15:55:59 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 14:50:04 GMT'}]",2025-03-21,"[['Bercovich', 'Akhiad', ''], ['Ronen', 'Tomer', ''], ['Abramovich', 'Talor', ''], ['Ailon', 'Nir', ''], ['Assaf', 'Nave', ''], ['Dabbah', 'Mohammad', ''], ['Galil', 'Ido', ''], ['Geifman', 'Amnon', ''], ['Geifman', 'Yonatan', ''], ['Golan', 'Izhak', ''], ['Haber', 'Netanel', ''], ['Karpas', 'Ehud', ''], ['Koren', 'Roi', ''], ['Levy', 'Itay', ''], ['Molchanov', 'Pavlo', ''], ['Mor', 'Shahar', ''], ['Moshe', 'Zach', ''], ['Nabwani', 'Najeeb', ''], ['Puny', 'Omri', ''], ['Rubin', 'Ran', ''], ['Schen', 'Itamar', ''], ['Shahaf', 'Ido', ''], ['Tropp', 'Oren', ''], ['Argov', 'Omer Ullman', ''], ['Zilberstein', 'Ran', ''], ['El-Yaniv', 'Ran', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Puzzle', 'label': 'Open-source LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Puzzle', 'label': 'Open-source LLMs'}, {'text': 'blockwise local knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'Puzzle', 'label': 'Open-source LLMs'}]",Large Language Model,Large language models,0.9664971828460693
2411.19488,Jun Gao,"Jun Gao, Yongqi Li, Ziqiang Cao, Wenjie Li",Interleaved-Modal Chain-of-Thought,CVPR 2025 Main Conference,,,,cs.CV cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Chain-of-Thought (CoT) prompting elicits large language models (LLMs) to
produce a series of intermediate reasoning steps before arriving at the final
answer. However, when transitioning to vision-language models (VLMs), their
text-only rationales struggle to express the fine-grained associations with the
original image. In this paper, we propose an image-incorporated multimodal
Chain-of-Thought, named \textbf{Interleaved-modal Chain-of-Thought (ICoT)},
which generates sequential reasoning steps consisting of paired visual and
textual rationales to infer the final answer. Intuitively, the novel ICoT
requires VLMs to enable the generation of fine-grained interleaved-modal
content, which is hard for current VLMs to fulfill. Considering that the
required visual information is usually part of the input image, we propose
\textbf{Attention-driven Selection (ADS)} to realize ICoT over existing VLMs.
ADS intelligently inserts regions of the input image to generate the
interleaved-modal reasoning steps with ignorable additional latency. ADS relies
solely on the attention map of VLMs without the need for parameterization, and
therefore it is a plug-and-play strategy that can be generalized to a spectrum
of VLMs. We apply ADS to realize ICoT on two popular VLMs of different
architectures. Extensive evaluations of three benchmarks have shown that ICoT
prompting achieves substantial performance (up to 14\%) and interpretability
improvements compared to existing multimodal CoT prompting methods.
","[{'version': 'v1', 'created': 'Fri, 29 Nov 2024 06:06:35 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 09:01:38 GMT'}]",2025-03-18,"[['Gao', 'Jun', ''], ['Li', 'Yongqi', ''], ['Cao', 'Ziqiang', ''], ['Li', 'Wenjie', '']]","[{'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'vision-language models', 'label': 'Large Language Model'}, {'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'ICoT', 'label': 'Chain of thought'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'ICoT', 'label': 'Chain of thought'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'attention map', 'label': 'Attention mechanism'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'ICoT', 'label': 'Chain of thought'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'ICoT\nprompting', 'label': 'Prompting'}]",Large Language Model,large language models,0.9664971828460693
2412.01250,Francesco Taioli,"Francesco Taioli, Edoardo Zorzi, Gianni Franchi, Alberto Castellini,
  Alessandro Farinelli, Marco Cristani, Yiming Wang","Collaborative Instance Object Navigation: Leveraging
  Uncertainty-Awareness to Minimize Human-Agent Dialogues",https://intelligolabs.github.io/CoIN/,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Language-driven instance object navigation assumes that human users initiate
the task by providing a detailed description of the target instance to the
embodied agent. While this description is crucial for distinguishing the target
from visually similar instances in a scene, providing it prior to navigation
can be demanding for human. To bridge this gap, we introduce Collaborative
Instance object Navigation (CoIN), a new task setting where the agent actively
resolve uncertainties about the target instance during navigation in natural,
template-free, open-ended dialogues with human. We propose a novel
training-free method, Agent-user Interaction with UncerTainty Awareness
(AIUTA), which operates independently from the navigation policy, and focuses
on the human-agent interaction reasoning with Vision-Language Models (VLMs) and
Large Language Models (LLMs). First, upon object detection, a Self-Questioner
model initiates a self-dialogue within the agent to obtain a complete and
accurate observation description with a novel uncertainty estimation technique.
Then, an Interaction Trigger module determines whether to ask a question to the
human, continue or halt navigation, minimizing user input. For evaluation, we
introduce CoIN-Bench, with a curated dataset designed for challenging
multi-instance scenarios. CoIN-Bench supports both online evaluation with
humans and reproducible experiments with simulated user-agent interactions. On
CoIN-Bench, we show that AIUTA serves as a competitive baseline, while existing
language-driven instance navigation methods struggle in complex multi-instance
scenes. Code and benchmark will be available upon acceptance at
https://intelligolabs.github.io/CoIN/
","[{'version': 'v1', 'created': 'Mon, 2 Dec 2024 08:16:38 GMT'}, {'version': 'v2', 'created': 'Sun, 16 Mar 2025 17:46:20 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 16:09:20 GMT'}]",2025-03-19,"[['Taioli', 'Francesco', ''], ['Zorzi', 'Edoardo', ''], ['Franchi', 'Gianni', ''], ['Castellini', 'Alberto', ''], ['Farinelli', 'Alessandro', ''], ['Cristani', 'Marco', ''], ['Wang', 'Yiming', '']]","[{'text': 'Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'Interaction Trigger module', 'label': 'Prompting'}]",Large Language Model,Large Language Models,0.9664971828460693
2412.01262,Michelle Elizabeth,"Michelle Elizabeth, Morgan Veyret, Miguel Couceiro, Ondrej Dusek and
  Lina M. Rojas-Barahona","Exploring ReAct Prompting for Task-Oriented Dialogue: Insights and
  Shortcomings",,,,,cs.CL cs.AI cs.HC,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) gained immense popularity due to their
impressive capabilities in unstructured conversations. Empowering LLMs with
advanced prompting strategies such as reasoning and acting (ReAct) (Yao et al.,
2022) has shown promise in solving complex tasks traditionally requiring
reinforcement learning. In this work, we apply the ReAct strategy to guide LLMs
performing task-oriented dialogue (TOD). We evaluate ReAct-based LLMs
(ReAct-LLMs) both in simulation and with real users. While ReAct-LLMs severely
underperform state-of-the-art approaches on success rate in simulation, this
difference becomes less pronounced in human evaluation. Moreover, compared to
the baseline, humans report higher subjective satisfaction with ReAct-LLM
despite its lower success rate, most likely thanks to its natural and
confidently phrased responses.
","[{'version': 'v1', 'created': 'Mon, 2 Dec 2024 08:30:22 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 10:01:21 GMT'}]",2025-03-18,"[['Elizabeth', 'Michelle', ''], ['Veyret', 'Morgan', ''], ['Couceiro', 'Miguel', ''], ['Dusek', 'Ondrej', ''], ['Rojas-Barahona', 'Lina M.', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'reasoning and acting', 'label': 'Prompting'}, {'text': 'ReAct', 'label': 'Prompting'}, {'text': 'reinforcement learning', 'label': 'Few-shot Learning'}, {'text': 'ReAct', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ReAct-LLMs', 'label': 'LLM-based'}, {'text': 'ReAct-LLM', 'label': 'LLM-based'}]",Large Language Model,Large language models,0.9664971828460693
2412.08897,Sam Adam-Day,Lewis Hammond and Sam Adam-Day,Neural Interactive Proofs,"ICLR'25 camera-ready version; 51 pages, 17 figures",,,,cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider the problem of how a trusted, but computationally bounded agent
(a 'verifier') can learn to interact with one or more powerful but untrusted
agents ('provers') in order to solve a given task. More specifically, we study
the case in which agents are represented using neural networks and refer to
solutions of this problem as neural interactive proofs. First we introduce a
unifying framework based on prover-verifier games, which generalises previously
proposed interaction protocols. We then describe several new protocols for
generating neural interactive proofs, and provide a theoretical comparison of
both new and existing approaches. Finally, we support this theory with
experiments in two domains: a toy graph isomorphism problem that illustrates
the key ideas, and a code validation task using large language models. In so
doing, we aim to create a foundation for future work on neural interactive
proofs and their application in building safer AI systems.
","[{'version': 'v1', 'created': 'Thu, 12 Dec 2024 03:21:53 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 17:16:02 GMT'}]",2025-03-19,"[['Hammond', 'Lewis', ''], ['Adam-Day', 'Sam', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2412.09668,Messi H.J. Lee,"Messi H.J. Lee, Soyeon Jeon","Vision-Language Models Generate More Homogeneous Stories for
  Phenotypically Black Individuals",,,,,cs.CV,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Vision-Language Models (VLMs) extend Large Language Models' capabilities by
integrating image processing, but concerns persist about their potential to
reproduce and amplify human biases. While research has documented how these
models perpetuate stereotypes across demographic groups, most work has focused
on between-group biases rather than within-group differences. This study
investigates homogeneity bias-the tendency to portray groups as more uniform
than they are-within Black Americans, examining how perceived racial
phenotypicality influences VLMs' outputs. Using computer-generated images that
systematically vary in phenotypicality, we prompted VLMs to generate stories
about these individuals and measured text similarity to assess content
homogeneity. Our findings reveal three key patterns: First, VLMs generate
significantly more homogeneous stories about Black individuals with higher
phenotypicality compared to those with lower phenotypicality. Second, stories
about Black women consistently display greater homogeneity than those about
Black men across all models tested. Third, in two of three VLMs, this
homogeneity bias is primarily driven by a pronounced interaction where
phenotypicality strongly influences content variation for Black women but has
minimal impact for Black men. These results demonstrate how intersectionality
shapes AI-generated representations and highlight the persistence of
stereotyping that mirror documented biases in human perception, where increased
racial phenotypicality leads to greater stereotyping and less individualized
representation.
","[{'version': 'v1', 'created': 'Thu, 12 Dec 2024 18:53:49 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 15:50:45 GMT'}]",2025-03-21,"[['Lee', 'Messi H. J.', ''], ['Jeon', 'Soyeon', '']]","[{'text': 'Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'homogeneity bias-the', 'label': 'Model Bias and Fairness'}, {'text': 'prompted', 'label': 'Prompting'}, {'text': 'homogeneity bias', 'label': 'Model Bias and Fairness'}]",Large Language Model,Large Language Models,0.9664971828460693
2412.12687,Seungeun Oh,"Seungeun Oh, Jinhyuk Kim, Jihong Park, Seung-Woo Ko, Tony Q. S. Quek,
  Seong-Lyun Kim","Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large
  Language Models","7 pages, 6 figures; to be presented at IEEE International Conference
  on Machine Learning for Communication and Networking (ICMLCN) 2025",,,,cs.LG cs.DC cs.IT cs.NI eess.SP math.IT,http://creativecommons.org/licenses/by/4.0/,"  This paper studies a hybrid language model (HLM) architecture that integrates
a small language model (SLM) operating on a mobile device with a large language
model (LLM) hosted at the base station (BS) of a wireless network. The HLM
token generation process follows the speculative inference principle: the SLM's
vocabulary distribution is uploaded to the LLM, which either accepts or rejects
it, with rejected tokens being resampled by the LLM. While this approach
ensures alignment between the vocabulary distributions of the SLM and LLM, it
suffers from low token throughput due to uplink transmission and the
computation costs of running both language models. To address this, we propose
a novel HLM structure coined Uncertainty-aware opportunistic HLM (U-HLM),
wherein the SLM locally measures its output uncertainty and skips both uplink
transmissions and LLM operations for tokens that are likely to be accepted.
This opportunistic skipping is enabled by our empirical finding of a linear
correlation between the SLM's uncertainty and the LLM's rejection probability.
We analytically derive the uncertainty threshold and evaluate its expected risk
of rejection. Simulations show that U-HLM reduces uplink transmissions and LLM
computations by 45.93%, while achieving up to 97.54% of the LLM's inference
accuracy and 2.54$\times$ faster token throughput than HLM without skipping.
","[{'version': 'v1', 'created': 'Tue, 17 Dec 2024 09:08:18 GMT'}, {'version': 'v2', 'created': 'Wed, 18 Dec 2024 08:14:35 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 10:50:58 GMT'}]",2025-03-19,"[['Oh', 'Seungeun', ''], ['Kim', 'Jinhyuk', ''], ['Park', 'Jihong', ''], ['Ko', 'Seung-Woo', ''], ['Quek', 'Tony Q. S.', ''], ['Kim', 'Seong-Lyun', '']]","[{'text': 'SLM', 'label': 'Large Language Model'}, {'text': 'large language\nmodel', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'SLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'SLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'SLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'SLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]",Large Language Model,"large language
model",1.0
2412.13871,Zonghao Guo,"Yipeng Zhang, Yifan Liu, Zonghao Guo, Yidan Zhang, Xuesong Yang,
  Xiaoying Zhang, Chi Chen, Jun Song, Bo Zheng, Yuan Yao, Zhiyuan Liu, Tat-Seng
  Chua, Maosong Sun","LLaVA-UHD v2: an MLLM Integrating High-Resolution Semantic Pyramid via
  Hierarchical Window Transformer",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Vision transformers (ViTs) are widely employed in multimodal large language
models (MLLMs) for visual encoding. However, they exhibit inferior performance
on tasks regarding fine-grained visual perception. We attribute this to the
limitations of ViTs in capturing diverse multi-modal visual levels, such as
low-level details. To address this issue, we present LLaVA-UHD v2, an MLLM with
advanced perception abilities by introducing a well-designed vision-language
projector, the Hierarchical window (Hiwin) transformer. Hiwin transformer
enhances MLLM's ability to capture diverse multi-modal visual granularities, by
incorporating our constructed high-resolution semantic pyramid. Specifically,
Hiwin transformer comprises two key modules: (i) a visual detail injection
module, which progressively injects low-level visual details into high-level
language-aligned semantics features, thereby forming an inverse semantic
pyramid (ISP), and (ii) a hierarchical window attention module, which leverages
cross-scale windows to condense multi-level semantics from the ISP. Extensive
experiments show that LLaVA-UHD v2 outperforms compared MLLMs on a wide range
of benchmarks. Notably, our design achieves an average boost of 3.7% across 14
benchmarks compared with the baseline method, 9.3% on DocVQA for instance. All
the data and code will be publicly available to facilitate future research.
","[{'version': 'v1', 'created': 'Wed, 18 Dec 2024 14:07:46 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 10:04:22 GMT'}]",2025-03-20,"[['Zhang', 'Yipeng', ''], ['Liu', 'Yifan', ''], ['Guo', 'Zonghao', ''], ['Zhang', 'Yidan', ''], ['Yang', 'Xuesong', ''], ['Zhang', 'Xiaoying', ''], ['Chen', 'Chi', ''], ['Song', 'Jun', ''], ['Zheng', 'Bo', ''], ['Yao', 'Yuan', ''], ['Liu', 'Zhiyuan', ''], ['Chua', 'Tat-Seng', ''], ['Sun', 'Maosong', '']]","[{'text': 'Vision transformers', 'label': 'Transformers'}, {'text': 'multimodal large language\nmodels', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'Hiwin transformer', 'label': 'Transformers'}, {'text': 'Hiwin transformer', 'label': 'Transformers'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,"multimodal large language
models",0.7649828195571899
2412.14672,Estelle Aflalo Guez,"Estelle Aflalo, Gabriela Ben Melech Stan, Tiep Le, Man Luo, Shachar
  Rosenman, Sayak Paul, Shao-Yen Tseng, Vasudev Lal","FiVL: A Framework for Improved Vision-Language Alignment through the
  Lens of Training, Evaluation and Explainability",,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large Vision Language Models (LVLMs) have achieved significant progress in
integrating visual and textual inputs for multimodal reasoning. However, a
recurring challenge is ensuring these models utilize visual information as
effectively as linguistic content when both modalities are necessary to
formulate an accurate answer. We hypothesize that hallucinations arise due to
the lack of effective visual grounding in current LVLMs. Furthermore, current
vision-language benchmarks are not specifically measuring the degree to which
the answer require the visual input. This limitation makes it challenging to
confirm that the image is truly necessary, particularly in tasks like visual
question answering. In this work, we introduce FiVL, a novel method for
constructing datasets designed to train LVLMs for enhanced visual grounding and
also evaluate their effectiveness in achieving it. We demonstrate the value of
our datasets through three approaches. First, we introduce a novel training
task based on our augmented training dataset, resulting in better performance
than the baseline. Second, we present benchmarks to assess the model's ability
to use image as substantive evidence, rather than relying solely on linguistic
priors. Finally, we identify attention heads with the strongest vision-language
alignment, enabling explainability on visual-driven hallucinations. The code is
available at https://github.com/IntelLabs/fivl.
","[{'version': 'v1', 'created': 'Thu, 19 Dec 2024 09:24:10 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 12:04:30 GMT'}]",2025-03-20,"[['Aflalo', 'Estelle', ''], ['Stan', 'Gabriela Ben Melech', ''], ['Le', 'Tiep', ''], ['Luo', 'Man', ''], ['Rosenman', 'Shachar', ''], ['Paul', 'Sayak', ''], ['Tseng', 'Shao-Yen', ''], ['Lal', 'Vasudev', '']]","[{'text': 'Large Vision Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'attention heads', 'label': 'Attention mechanism'}]",Large Language Model,Large Vision Language Models,0.7264450788497925
2412.20227,Shuguang Chen,Shuguang Chen and Guang Lin,"LLM Reasoning Engine: Specialized Training for Enhanced Mathematical
  Reasoning",Accepted to NAACL 2025 KnowledgeNLP,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have shown remarkable performance in various
natural language processing tasks but face challenges in mathematical
reasoning, where complex problem-solving requires both linguistic understanding
and mathematical reasoning skills. Existing approaches to address this
challenge often rely on ensemble methods and suffer from the problem of data
scarcity in target domains. In this work, we present a novel method to enhance
LLMs' capabilities in mathematical reasoning tasks. Motivated by the need to
bridge this gap, our approach incorporates a question paraphrase strategy,
which aims at diversifying the linguistic forms of mathematical questions to
improve generalization. Additionally, specialized training objectives are
employed to guide the model's learning process, focusing on enhancing its
understanding of mathematical concepts and reasoning processes. We conduct
experiments on four datasets using different LLMs, and demonstrate the
effectiveness of our approach in improving LLMs' performance on mathematical
reasoning tasks. Our findings underscore the significance of our methodology in
the advancement of large language models and its potential implications for
real-world applications that require mathematical reasoning abilities.
","[{'version': 'v1', 'created': 'Sat, 28 Dec 2024 17:48:33 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:56:49 GMT'}]",2025-03-20,"[['Chen', 'Shuguang', ''], ['Lin', 'Guang', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2412.20760,Huihan Li,"Huihan Li, Arnav Goel, Keyu He, Xiang Ren",Attributing Culture-Conditioned Generations to Pretraining Corpora,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  In open-ended generative tasks like narrative writing or dialogue, large
language models often exhibit cultural biases, showing limited knowledge and
generating templated outputs for less prevalent cultures. Recent works show
that these biases may stem from uneven cultural representation in pretraining
corpora. This work investigates how pretraining leads to biased
culture-conditioned generations by analyzing how models associate entities with
cultures based on pretraining data patterns. We propose the MEMOed framework
(MEMOrization from pretraining document) to determine whether a generation for
a culture arises from memorization. Using MEMOed on culture-conditioned
generations about food and clothing for 110 cultures, we find that
high-frequency cultures in pretraining data yield more generations with
memorized symbols, while some low-frequency cultures produce none.
Additionally, the model favors generating entities with extraordinarily high
frequency regardless of the conditioned culture, reflecting biases toward
frequent pretraining terms irrespective of relevance. We hope that the MEMOed
framework and our insights will inspire more works on attributing model
performance on pretraining data.
","[{'version': 'v1', 'created': 'Mon, 30 Dec 2024 07:09:25 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 19:08:17 GMT'}]",2025-03-21,"[['Li', 'Huihan', ''], ['Goel', 'Arnav', ''], ['He', 'Keyu', ''], ['Ren', 'Xiang', '']]","[{'text': 'large\nlanguage models', 'label': 'Large Language Model'}]",Large Language Model,"large
language models",0.9664971828460693
2501.00959,Saleh Afroogh,"Junfeng Jiao, Saleh Afroogh, Kevin Chen, David Atkinson, Amit
  Dhurandhar","IGGA: A Dataset of Industrial Guidelines and Policy Statements for
  Generative AIs",,,,,cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper introduces IGGA, a dataset of 160 industry guidelines and policy
statements for the use of Generative AIs (GAIs) and Large Language Models
(LLMs) in industry and workplace settings, collected from official company
websites, and trustworthy news sources. The dataset contains 104,565 words and
serves as a valuable resource for natural language processing tasks commonly
applied in requirements engineering, such as model synthesis, abstraction
identification, and document structure assessment. Additionally, IGGA can be
further annotated to function as a benchmark for various tasks, including
ambiguity detection, requirements categorization, and the identification of
equivalent requirements. Our methodologically rigorous approach ensured a
thorough examination, with a selection of reputable and influential companies
that represent a diverse range of global institutions across six continents.
The dataset captures perspectives from fourteen industry sectors, including
technology, finance, and both public and private institutions, offering a broad
spectrum of insights into the integration of GAIs and LLMs in industry.
","[{'version': 'v1', 'created': 'Wed, 1 Jan 2025 21:31:47 GMT'}, {'version': 'v2', 'created': 'Fri, 3 Jan 2025 19:17:56 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 16:44:15 GMT'}]",2025-03-19,"[['Jiao', 'Junfeng', ''], ['Afroogh', 'Saleh', ''], ['Chen', 'Kevin', ''], ['Atkinson', 'David', ''], ['Dhurandhar', 'Amit', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2501.02063,Saleh Afroogh,"Junfeng Jiao, Saleh Afroogh, Kevin Chen, David Atkinson, Amit
  Dhurandhar","AGGA: A Dataset of Academic Guidelines for Generative AI and Large
  Language Models","arXiv admin note: text overlap with arXiv:2406.18842,
  arXiv:2501.00959",,,,cs.CL cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This study introduces AGGA, a dataset comprising 80 academic guidelines for
the use of Generative AIs (GAIs) and Large Language Models (LLMs) in academic
settings, meticulously collected from official university websites. The dataset
contains 188,674 words and serves as a valuable resource for natural language
processing tasks commonly applied in requirements engineering, such as model
synthesis, abstraction identification, and document structure assessment.
Additionally, AGGA can be further annotated to function as a benchmark for
various tasks, including ambiguity detection, requirements categorization, and
the identification of equivalent requirements. Our methodologically rigorous
approach ensured a thorough examination, with a selection of universities that
represent a diverse range of global institutions, including top-ranked
universities across six continents. The dataset captures perspectives from a
variety of academic fields, including humanities, technology, and both public
and private institutions, offering a broad spectrum of insights into the
integration of GAIs and LLMs in academia.
","[{'version': 'v1', 'created': 'Fri, 3 Jan 2025 19:16:36 GMT'}, {'version': 'v2', 'created': 'Tue, 7 Jan 2025 19:12:22 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 16:45:54 GMT'}]",2025-03-19,"[['Jiao', 'Junfeng', ''], ['Afroogh', 'Saleh', ''], ['Chen', 'Kevin', ''], ['Atkinson', 'David', ''], ['Dhurandhar', 'Amit', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2501.11425,Siyu Yuan,"Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, Jiecao Chen","Agent-R: Training Language Model Agents to Reflect via Iterative
  Self-Training",,,,,cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Large Language Models (LLMs) agents are increasingly pivotal for addressing
complex tasks in interactive environments. Existing work mainly focuses on
enhancing performance through behavior cloning from stronger experts, yet such
approaches often falter in real-world applications, mainly due to the inability
to recover from errors. However, step-level critique data is difficult and
expensive to collect. Automating and dynamically constructing self-critique
datasets is thus crucial to empowering models with intelligent agent
capabilities. In this work, we propose an iterative self-training framework,
Agent-R, that enables language Agent to Reflect on the fly. Unlike traditional
methods that reward or penalize actions based on correctness, Agent-R leverages
MCTS to construct training data that recover correct trajectories from
erroneous ones. A key challenge of agent reflection lies in the necessity for
timely revision rather than waiting until the end of a rollout. To address
this, we introduce a model-guided critique construction mechanism: the actor
model identifies the first error step (within its current capability) in a
failed trajectory. Starting from it, we splice it with the adjacent correct
path, which shares the same parent node in the tree. This strategy enables the
model to learn reflection based on its current policy, therefore yielding
better learning efficiency. To further explore the scalability of this
self-improvement paradigm, we investigate iterative refinement of both error
correction capabilities and dataset construction. Our findings demonstrate that
Agent-R continuously improves the model's ability to recover from errors and
enables timely error correction. Experiments on three interactive environments
show that Agent-R effectively equips agents to correct erroneous actions while
avoiding loops, achieving superior performance compared to baseline methods
(+5.59%).
","[{'version': 'v1', 'created': 'Mon, 20 Jan 2025 11:46:04 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 09:28:09 GMT'}]",2025-03-20,"[['Yuan', 'Siyu', ''], ['Chen', 'Zehui', ''], ['Xi', 'Zhiheng', ''], ['Ye', 'Junjie', ''], ['Du', 'Zhengyin', ''], ['Chen', 'Jiecao', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2501.12372,Yeounoh Chung,"Yeounoh Chung, Gaurav T. Kakkar, Yu Gan, Brenton Milne, Fatma Ozcan","Is Long Context All You Need? Leveraging LLM's Extended Context for
  NL2SQL","13 pages, 6 figures, VLDB 2025",,,,cs.DB cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Large Language Models (LLMs) have demonstrated impressive capabilities across
a range of natural language processing tasks. In particular, improvements in
reasoning abilities and the expansion of context windows have opened new
avenues for leveraging these powerful models. NL2SQL is challenging in that the
natural language question is inherently ambiguous, while the SQL generation
requires a precise understanding of complex data schema and semantics. One
approach to this semantic ambiguous problem is to provide more and sufficient
contextual information.
  In this work, we explore the performance and the latency trade-offs of the
extended context window (a.k.a., long context) offered by Google's
state-of-the-art LLM (\textit{gemini-1.5-pro}). We study the impact of various
contextual information, including column example values, question and SQL query
pairs, user-provided hints, SQL documentation, and schema. To the best of our
knowledge, this is the first work to study how the extended context window and
extra contextual information can help NL2SQL generation with respect to both
accuracy and latency cost. We show that long context LLMs are robust and do not
get lost in the extended contextual information. Additionally, our long-context
NL2SQL pipeline based on Google's \textit{gemini-pro-1.5} achieve strong
performances on various benchmark datasets without finetuning and expensive
self-consistency based techniques.
","[{'version': 'v1', 'created': 'Tue, 21 Jan 2025 18:52:15 GMT'}, {'version': 'v2', 'created': 'Sat, 1 Feb 2025 02:00:46 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Feb 2025 23:39:12 GMT'}, {'version': 'v4', 'created': 'Fri, 7 Mar 2025 23:17:42 GMT'}, {'version': 'v5', 'created': 'Thu, 20 Mar 2025 17:39:13 GMT'}]",2025-03-21,"[['Chung', 'Yeounoh', ''], ['Kakkar', 'Gaurav T.', ''], ['Gan', 'Yu', ''], ['Milne', 'Brenton', ''], ['Ozcan', 'Fatma', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'extended context window', 'label': 'contextual Embedding'}, {'text': 'long context', 'label': 'contextual Embedding'}, {'text': 'extended context window', 'label': 'contextual Embedding'}, {'text': 'finetuning', 'label': 'Fine-tuning'}]",Large Language Model,Large Language Models,0.9664971828460693
2501.13947,Wenli Yang,"Wenli Yang, Lilian Some, Michael Bain, Byeong Kang","A Comprehensive Survey on Integrating Large Language Models with
  Knowledge-Based Methods",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The rapid development of artificial intelligence has led to marked progress
in the field. One interesting direction for research is whether Large Language
Models (LLMs) can be integrated with structured knowledge-based systems. This
approach aims to combine the generative language understanding of LLMs and the
precise knowledge representation systems by which they are integrated. This
article surveys the relationship between LLMs and knowledge bases, looks at how
they can be applied in practice, and discusses related technical, operational,
and ethical challenges. Utilizing a comprehensive examination of the
literature, the study both identifies important issues and assesses existing
solutions. It demonstrates the merits of incorporating generative AI into
structured knowledge-base systems concerning data contextualization, model
accuracy, and utilization of knowledge resources. The findings give a full list
of the current situation of research, point out the main gaps, and propose
helpful paths to take. These insights contribute to advancing AI technologies
and support their practical deployment across various sectors.
","[{'version': 'v1', 'created': 'Sun, 19 Jan 2025 23:25:21 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 23:27:43 GMT'}]",2025-03-21,"[['Yang', 'Wenli', ''], ['Some', 'Lilian', ''], ['Bain', 'Michael', ''], ['Kang', 'Byeong', '']]","[{'text': 'Large Language\nModels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'data contextualization', 'label': 'contextual Embedding'}]",Large Language Model,"Large Language
Models",0.9664971828460693
2501.14892,Hang Luo,"Hang Luo, Jian Zhang, Chujun Li","Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in
  Graph-Augmented LLMs","18 pages, 3 figures, 3 tables",,,,cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In knowledge-intensive tasks, especially in high-stakes domains like medicine
and law, it is critical not only to retrieve relevant information but also to
provide causal reasoning and explainability. Large language models (LLMs) have
achieved remarkable performance in natural language understanding and
generation tasks. However, they often suffer from limitations such as
difficulty in incorporating new knowledge, generating hallucinations, and
explaining their reasoning process. To address these challenges, integrating
knowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has
emerged as an effective solution. Traditional Graph RAG methods often rely on
simple graph traversal or semantic similarity, which do not capture causal
relationships or align well with the model's internal reasoning steps. This
paper proposes a novel pipeline that filters large knowledge graphs to
emphasize cause-effect edges, aligns the retrieval process with the model's
chain-of-thought (CoT), and enhances reasoning through multi-stage path
improvements. Experiments on medical question-answering tasks show consistent
gains, with up to a 10\% absolute improvement across multiple large language
models (LLMs). This approach demonstrates the value of combining causal
reasoning with stepwise retrieval, leading to more interpretable and logically
grounded solutions for complex queries.
","[{'version': 'v1', 'created': 'Fri, 24 Jan 2025 19:31:06 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 14:32:08 GMT'}]",2025-03-18,"[['Luo', 'Hang', ''], ['Zhang', 'Jian', ''], ['Li', 'Chujun', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Graph RAG', 'label': 'RAG'}, {'text': 'Graph RAG', 'label': 'RAG'}, {'text': 'chain-of-thought (CoT)', 'label': 'Chain of thought'}, {'text': 'large language\nmodels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2501.15890,Karahan Sar{\i}ta\c{s},"Karahan Sar{\i}ta\c{s}, Peter Dayan, Tingke Shen, Surabhi S Nath","Complexity in Complexity: Understanding Visual Complexity Through
  Structure, Color, and Surprise",,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Understanding how humans perceive visual complexity is a key area of study in
visual cognition. Previous approaches to modeling visual complexity assessments
have often resulted in intricate, difficult-to-interpret algorithms that employ
numerous features or sophisticated deep learning architectures. While these
complex models achieve high performance on specific datasets, they often
sacrifice interpretability, making it challenging to understand the factors
driving human perception of complexity. Recently (Shen, et al. 2024) proposed
an interpretable segmentation-based model that accurately predicted complexity
across various datasets, supporting the idea that complexity can be explained
simply. In this work, we investigate the failure of their model to capture
structural, color and surprisal contributions to complexity. To this end, we
propose Multi-Scale Sobel Gradient (MSG) which measures spatial intensity
variations, Multi-Scale Unique Color (MUC) which quantifies colorfulness across
multiple scales, and surprise scores generated using a Large Language Model. We
test our features on existing benchmarks and a novel dataset (Surprising Visual
Genome) containing surprising images from Visual Genome. Our experiments
demonstrate that modeling complexity accurately is not as simple as previously
thought, requiring additional perceptual and semantic factors to address
dataset biases. Our model improves predictive performance while maintaining
interpretability, offering deeper insights into how visual complexity is
perceived and assessed. Our code, analysis and data are available at
https://github.com/Complexity-Project/Complexity-in-Complexity.
","[{'version': 'v1', 'created': 'Mon, 27 Jan 2025 09:32:56 GMT'}, {'version': 'v2', 'created': 'Wed, 5 Feb 2025 19:36:23 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 12:06:51 GMT'}]",2025-03-21,"[['Sarıtaş', 'Karahan', ''], ['Dayan', 'Peter', ''], ['Shen', 'Tingke', ''], ['Nath', 'Surabhi S', '']]","[{'text': 'Large Language Model', 'label': 'Large Language Model'}]",Large Language Model,Large Language Model,1.0
2502.06759,Gaetano Rossiello,"Gaetano Rossiello, Nhan Pham, Michael Glass, Junkyu Lee, Dharmashankar
  Subramanian",Rationalization Models for Text-to-SQL,Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs,,,,cs.CL cs.AI cs.DB,http://creativecommons.org/licenses/by/4.0/,"  We introduce a framework for generating Chain-of-Thought (CoT) rationales to
enhance text-to-SQL model fine-tuning. These rationales consist of intermediate
SQL statements and explanations, serving as incremental steps toward
constructing the final SQL query. The process begins with manually annotating a
small set of examples, which are then used to prompt a large language model in
an iterative, dynamic few-shot knowledge distillation procedure from a teacher
model. A rationalization model is subsequently trained on the validated
decomposed queries, enabling extensive synthetic CoT annotations for
text-to-SQL datasets. To evaluate the approach, we fine-tune small language
models with and without these rationales on the BIRD dataset. Results indicate
that step-by-step query generation improves execution accuracy, especially for
moderately and highly complex queries, while also enhancing explainability.
","[{'version': 'v1', 'created': 'Mon, 10 Feb 2025 18:38:57 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Feb 2025 17:12:34 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 17:37:30 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 13:46:48 GMT'}]",2025-03-21,"[['Rossiello', 'Gaetano', ''], ['Pham', 'Nhan', ''], ['Glass', 'Michael', ''], ['Lee', 'Junkyu', ''], ['Subramanian', 'Dharmashankar', '']]","[{'text': 'prompt', 'label': 'Prompting'}, {'text': 'large language model', 'label': 'Large Language Model'}, {'text': 'few-shot knowledge distillation', 'label': 'Knowledge distillation'}]",Large Language Model,large language model,1.0
2502.07058,Zixin Tang,"Zixin Tang, Chieh-Yang Huang, Tsung-Che Li, Ho Yin Sam Ng, Hen-Hsen
  Huang, Ting-Hao 'Kenneth' Huang","Using Contextually Aligned Online Reviews to Measure LLMs' Performance
  Disparities Across Language Varieties","Accepted by 2025 Annual Conference of the Nations of the Americas
  Chapter of the Association for Computational Linguistics (NAACL), theme track",,,,cs.CL cs.HC,http://creativecommons.org/licenses/by/4.0/,"  A language can have different varieties. These varieties can affect the
performance of natural language processing (NLP) models, including large
language models (LLMs), which are often trained on data from widely spoken
varieties. This paper introduces a novel and cost-effective approach to
benchmark model performance across language varieties. We argue that
international online review platforms, such as Booking.com, can serve as
effective data sources for constructing datasets that capture comments in
different language varieties from similar real-world scenarios, like reviews
for the same hotel with the same rating using the same language (e.g., Mandarin
Chinese) but different language varieties (e.g., Taiwan Mandarin, Mainland
Mandarin). To prove this concept, we constructed a contextually aligned dataset
comprising reviews in Taiwan Mandarin and Mainland Mandarin and tested six LLMs
in a sentiment analysis task. Our results show that LLMs consistently
underperform in Taiwan Mandarin.
","[{'version': 'v1', 'created': 'Mon, 10 Feb 2025 21:49:35 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Feb 2025 04:55:27 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 15:01:11 GMT'}]",2025-03-21,"[['Tang', 'Zixin', ''], ['Huang', 'Chieh-Yang', ''], ['Li', 'Tsung-Che', ''], ['Ng', 'Ho Yin Sam', ''], ['Huang', 'Hen-Hsen', ''], ['Huang', ""Ting-Hao 'Kenneth'"", '']]","[{'text': 'natural language processing (NLP) models', 'label': 'NLP model'}, {'text': 'large\nlanguage models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,"large
language models",0.9664971828460693
2502.08020,Ziyao Wang,"Ziyao Wang, Muneeza Azmat, Ang Li, Raya Horesh, Mikhail Yurochkin","Speculate, then Collaborate: Fusing Knowledge of Language Models during
  Decoding",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) often excel in specific domains but fall short
in others due to the limitations of their training. Thus, enabling LLMs to
solve problems collaboratively by integrating their complementary knowledge
promises to improve their performance across domains. To realize this
potential, we introduce a novel Collaborative Speculative Decoding (CoSD)
algorithm that enables efficient LLM knowledge fusion at test time without
requiring additional model training. CoSD employs a draft model to generate
initial sequences and an easy-to-learn rule or decision tree to decide when to
invoke an assistant model to improve these drafts. CoSD not only enhances
knowledge fusion but also improves inference efficiency, is transferable across
domains and models, and offers greater explainability. Experimental results
demonstrate that CoSD improves accuracy by up to 10\% across benchmarks
compared to existing methods, providing a scalable and effective solution for
LLM-based applications
","[{'version': 'v1', 'created': 'Tue, 11 Feb 2025 23:40:53 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 16:26:10 GMT'}]",2025-03-20,"[['Wang', 'Ziyao', ''], ['Azmat', 'Muneeza', ''], ['Li', 'Ang', ''], ['Horesh', 'Raya', ''], ['Yurochkin', 'Mikhail', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2502.12509,Kangda Wei,"Kangda Wei, Xi Shi, Jonathan Tong, Sai Ramana Reddy, Anandhavelu
  Natarajan, Rajiv Jain, Aparna Garimella, Ruihong Huang",LegalCore: A Dataset for Event Coreference Resolution in Legal Documents,Need company internal approval before public release,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Recognizing events and their coreferential mentions in a document is
essential for understanding semantic meanings of text. The existing research on
event coreference resolution is mostly limited to news articles. In this paper,
we present the first dataset for the legal domain, LegalCore, which has been
annotated with comprehensive event and event coreference information. The legal
contract documents we annotated in this dataset are several times longer than
news articles, with an average length of around 25k tokens per document. The
annotations show that legal documents have dense event mentions and feature
both short-distance and super long-distance coreference links between event
mentions. We further benchmark mainstream Large Language Models (LLMs) on this
dataset for both event detection and event coreference resolution tasks, and
find that this dataset poses significant challenges for state-of-the-art
open-source and proprietary LLMs, which perform significantly worse than a
supervised baseline. We will publish the dataset as well as the code.
","[{'version': 'v1', 'created': 'Tue, 18 Feb 2025 03:47:53 GMT'}, {'version': 'v2', 'created': 'Mon, 3 Mar 2025 19:36:00 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 16:53:11 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 16:45:57 GMT'}]",2025-03-21,"[['Wei', 'Kangda', ''], ['Shi', 'Xi', ''], ['Tong', 'Jonathan', ''], ['Reddy', 'Sai Ramana', ''], ['Natarajan', 'Anandhavelu', ''], ['Jain', 'Rajiv', ''], ['Garimella', 'Aparna', ''], ['Huang', 'Ruihong', '']]","[{'text': 'mainstream Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,mainstream Large Language Models,0.848070502281189
2502.13145,Bencheng Liao,"Bencheng Liao and Hongyuan Tao and Qian Zhang and Tianheng Cheng and
  Yingyue Li and Haoran Yin and Wenyu Liu and Xinggang Wang","Multimodal Mamba: Decoder-only Multimodal State Space Model via
  Quadratic to Linear Distillation",Code and model are available at https://github.com/hustvl/mmMamba,,,,cs.CV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Recent Multimodal Large Language Models (MLLMs) have achieved remarkable
performance but face deployment challenges due to their quadratic computational
complexity, growing Key-Value cache requirements, and reliance on separate
vision encoders. We propose mmMamba, a framework for developing
linear-complexity native multimodal state space models through progressive
distillation from existing MLLMs using moderate academic computational
resources. Our approach enables the direct conversion of trained decoder-only
MLLMs to linear-complexity architectures without requiring pre-trained
RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba
from trained Transformer and a three-stage distillation recipe, which can
effectively transfer the knowledge from Transformer to Mamba while preserving
multimodal capabilities. Our method also supports flexible hybrid architectures
that combine Transformer and Mamba layers for customizable
efficiency-performance trade-offs. Distilled from the Transformer-based
decoder-only HoVLE, mmMamba-linear achieves competitive performance against
existing linear and quadratic-complexity VLMs, while mmMamba-hybrid further
improves performance significantly, approaching HoVLE's capabilities. At 103K
tokens, mmMamba-linear demonstrates 20.6$\times$ speedup and 75.8% GPU memory
reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\times$ speedup
and 60.2% memory savings. Code and models are released at
https://github.com/hustvl/mmMamba
","[{'version': 'v1', 'created': 'Tue, 18 Feb 2025 18:59:57 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 07:02:33 GMT'}]",2025-03-19,"[['Liao', 'Bencheng', ''], ['Tao', 'Hongyuan', ''], ['Zhang', 'Qian', ''], ['Cheng', 'Tianheng', ''], ['Li', 'Yingyue', ''], ['Yin', 'Haoran', ''], ['Liu', 'Wenyu', ''], ['Wang', 'Xinggang', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'progressive\ndistillation', 'label': 'Knowledge distillation'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'Mamba', 'label': 'LLM'}, {'text': 'Mamba', 'label': 'LLM'}, {'text': 'HoVLE', 'label': 'LLM'}, {'text': 'HoVLE', 'label': 'LLM'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2502.16182,Shivank Garg,"Shivank Garg, Ayush Singh, Shweta Singh, Paras Chopra",IPO: Your Language Model is Secretly a Preference Classifier,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Reinforcement learning from human feedback (RLHF) has emerged as the primary
method for aligning large language models (LLMs) with human preferences. While
it enables LLMs to achieve human-level alignment, it often incurs significant
computational and financial costs due to its reliance on training external
reward models or human-labeled preferences. In this work, we propose Implicit
Preference Optimization (IPO), an alternative approach that leverages
generative LLMs as preference classifiers, thereby reducing the dependence on
external human feedback or reward models to obtain preferences. We conduct a
comprehensive evaluation on the preference classification ability of LLMs using
RewardBench, assessing models across different sizes, architectures, and
training levels to validate our hypothesis. Furthermore, we investigate the
self-improvement capabilities of LLMs by generating multiple responses for a
given instruction and employing the model itself as a preference classifier for
Direct Preference Optimization (DPO)-based training. Our findings demonstrate
that models trained through IPO achieve performance comparable to those
utilizing state-of-the-art reward models for obtaining preferences.
","[{'version': 'v1', 'created': 'Sat, 22 Feb 2025 10:59:11 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 10:52:45 GMT'}]",2025-03-21,"[['Garg', 'Shivank', ''], ['Singh', 'Ayush', ''], ['Singh', 'Shweta', ''], ['Chopra', 'Paras', '']]","[{'text': 'Reinforcement learning from human feedback', 'label': 'Zero-shot Learning'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2502.17848,Jianghao Chen,"Jianghao Chen, Zhenlin Wei, Zhenjiang Ren, Ziyong Li, Jiajun Zhang","LR$^2$Bench: Evaluating Long-chain Reflective Reasoning Capabilities of
  Large Language Models via Constraint Satisfaction Problems",Submitted to ACL ARR 2025 February,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent progress in o1-like models has significantly enhanced the reasoning
abilities of Large Language Models (LLMs), empowering them to tackle
increasingly complex tasks through reflection capabilities, such as making
assumptions, backtracking, and self-refinement. However, effectively evaluating
such reflection capabilities remains challenging due to the lack of appropriate
benchmarks. To bridge this gap, we introduce LR$^2$Bench, a novel benchmark
designed to evaluate the Long-chain Reflective Reasoning capabilities of LLMs.
LR$^2$Bench comprises 850 samples across six Constraint Satisfaction Problems
(CSPs) where reflective reasoning is crucial for deriving solutions that meet
all given constraints. Each type of task focuses on distinct constraint
patterns, such as knowledge-based, logical, and spatial constraints, providing
a comprehensive evaluation of diverse problem-solving scenarios. We conduct
extensive evaluation on both conventional models and o1-like models. Our
experimental results reveal that even the most advanced reasoning-specific
models, such as DeepSeek-R1 and OpenAI o1-preview, struggle with tasks in
LR$^2$Bench, achieving an average Exact Match score of only 20.0% and 23.6%,
respectively. These findings underscore the significant room for improvement in
the reflective reasoning capabilities of current LLMs. The leaderboard of our
benchmark is available at https://huggingface.co/spaces/UltraRonin/LR2Bench
","[{'version': 'v1', 'created': 'Tue, 25 Feb 2025 04:51:17 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 07:36:01 GMT'}]",2025-03-18,"[['Chen', 'Jianghao', ''], ['Wei', 'Zhenlin', ''], ['Ren', 'Zhenjiang', ''], ['Li', 'Ziyong', ''], ['Zhang', 'Jiajun', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'knowledge-based', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2502.20808,Wang Peijie,"Peijie Wang, Zhong-Zhi Li, Fei Yin, Dekang Ran, Cheng-Lin Liu",MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts,47 pages,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal Large Language Models (MLLMs) have shown promising capabilities in
mathematical reasoning within visual contexts across various datasets. However,
most existing multimodal math benchmarks are limited to single-visual contexts,
which diverges from the multi-visual scenarios commonly encountered in
real-world mathematical applications. To address this gap, we introduce
MV-MATH: a meticulously curated dataset of 2,009 high-quality mathematical
problems. Each problem integrates multiple images interleaved with text,
derived from authentic K-12 scenarios, and enriched with detailed annotations.
MV-MATH includes multiple-choice, free-form, and multi-step questions, covering
11 subject areas across 3 difficulty levels, and serves as a comprehensive and
rigorous benchmark for assessing MLLMs' mathematical reasoning in multi-visual
contexts. Through extensive experimentation, we observe that MLLMs encounter
substantial challenges in multi-visual math tasks, with a considerable
performance gap relative to human capabilities on MV-MATH. Furthermore, we
analyze the performance and error patterns of various models, providing
insights into MLLMs' mathematical reasoning capabilities within multi-visual
settings.
","[{'version': 'v1', 'created': 'Fri, 28 Feb 2025 07:50:36 GMT'}, {'version': 'v2', 'created': 'Mon, 3 Mar 2025 03:43:03 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 14:02:51 GMT'}]",2025-03-19,"[['Wang', 'Peijie', ''], ['Li', 'Zhong-Zhi', ''], ['Yin', 'Fei', ''], ['Ran', 'Dekang', ''], ['Liu', 'Cheng-Lin', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2503.00847,Johannes Daxenberger,"Moritz Altemeyer, Steffen Eger, Johannes Daxenberger, Tim Altendorf,
  Philipp Cimiano, Benjamin Schiller","Argument Summarization and its Evaluation in the Era of Large Language
  Models",,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  Large Language Models (LLMs) have revolutionized various Natural Language
Generation (NLG) tasks, including Argument Summarization (ArgSum), a key
subfield of Argument Mining (AM). This paper investigates the integration of
state-of-the-art LLMs into ArgSum, including for its evaluation. In particular,
we propose a novel prompt-based evaluation scheme, and validate it through a
novel human benchmark dataset. Our work makes three main contributions: (i) the
integration of LLMs into existing ArgSum frameworks, (ii) the development of a
new LLM-based ArgSum system, benchmarked against prior methods, and (iii) the
introduction of an advanced LLM-based evaluation scheme. We demonstrate that
the use of LLMs substantially improves both the generation and evaluation of
argument summaries, achieving state-of-the-art results and advancing the field
of ArgSum.
","[{'version': 'v1', 'created': 'Sun, 2 Mar 2025 10:49:10 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 20:25:48 GMT'}]",2025-03-19,"[['Altemeyer', 'Moritz', ''], ['Eger', 'Steffen', ''], ['Daxenberger', 'Johannes', ''], ['Altendorf', 'Tim', ''], ['Cimiano', 'Philipp', ''], ['Schiller', 'Benjamin', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.01090,Haowen Pan,"Haowen Pan, Xiaozhi Wang, Yixin Cao, Zenglin Shi, Xun Yang, Juanzi Li,
  Meng Wang","Precise Localization of Memories: A Fine-grained Neuron-level Knowledge
  Editing Technique for LLMs",ICLR 2025,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge editing aims to update outdated information in Large Language
Models (LLMs). A representative line of study is locate-then-edit methods,
which typically employ causal tracing to identify the modules responsible for
recalling factual knowledge about entities. However, we find these methods are
often sensitive only to changes in the subject entity, leaving them less
effective at adapting to changes in relations. This limitation results in poor
editing locality, which can lead to the persistence of irrelevant or inaccurate
facts, ultimately compromising the reliability of LLMs. We believe this issue
arises from the insufficient precision of knowledge localization. To address
this, we propose a Fine-grained Neuron-level Knowledge Editing (FiNE) method
that enhances editing locality without affecting overall success rates. By
precisely identifying and modifying specific neurons within feed-forward
networks, FiNE significantly improves knowledge localization and editing.
Quantitative experiments demonstrate that FiNE efficiently achieves better
overall performance compared to existing techniques, providing new insights
into the localization and modification of knowledge within LLMs.
","[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 01:30:28 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 07:34:41 GMT'}]",2025-03-18,"[['Pan', 'Haowen', ''], ['Wang', 'Xiaozhi', ''], ['Cao', 'Yixin', ''], ['Shi', 'Zenglin', ''], ['Yang', 'Xun', ''], ['Li', 'Juanzi', ''], ['Wang', 'Meng', '']]","[{'text': 'Large Language\nModels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,"Large Language
Models",0.9664971828460693
2503.01611,David Ponce,"David Ponce, Thierry Etchegoyhen","In-context Learning vs. Instruction Tuning: The Case of Small and
  Multilingual Language Models",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Instruction following is a critical ability for Large Language Models to
perform downstream tasks. The standard approach to instruction alignment has
relied on a specific phase of model tuning over curated instruction datasets,
optionally complemented with an alignment step over human preferences. Recent
work has shown the potential of in-context learning (ICL) alternatives to guide
base models towards instruction following. This type of approach is
particularly relevant to extend instruction following across languages and
models of varying sizes adapted to different types of usage. In this work we
compare ICL and instruction fine-tuning in English, French and Spanish, on
Small Language Models, and provide experimental results on applying Direct
Preference Optimisation (DPO) over base models. Our results show that scenarios
involving multilingual and smaller models result in downgraded ICL instruction
following performance, only partially mitigated by DPO alignment. This study
aims to further our understanding of current strengths and limitations of
alternative methods for instruction following.
","[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 14:47:23 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 15:32:53 GMT'}]",2025-03-18,"[['Ponce', 'David', ''], ['Etchegoyhen', 'Thierry', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}, {'text': 'base models', 'label': 'Foundation Model'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'instruction fine-tuning', 'label': 'Fine-tuning'}, {'text': 'ICL', 'label': 'contextual Embedding'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.01754,Guande Wu,"Guande Wu, Huan Song, Yawei Wang, Qiaojing Yan, Yijun Tian, Lin Lee
  Cheong, Panpan Xu","SDRT: Enhance Vision-Language Models by Self-Distillation with Diverse
  Reasoning Traces",,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Reasoning is increasingly crucial for various tasks. While chain-of-thought
prompting enables large language models to leverage reasoning effectively,
harnessing the reasoning capabilities of Vision-Language Models (VLMs) remains
challenging. To solve this problem, we propose a novel self-distillation
framework that enhances the reasoning capabilities of the model. The proposed
framework introduces several key innovations. We start by employing a prompt
library tailored to visual reasoning tasks to generate diverse in-context
questions and utilize a two-step reasoning procedure to derive reasoning-guided
responses. These responses are then used for self-distillation, enabling the
model to internalize the reasoning process. Additionally, we improve the model
architecture with several innovative components, including an intervention
adapter for efficient parameter updates, a cross-modal skip connection to
facilitate information exchange between modalities, and an ensemble learning
algorithm to integrate diverse reasoning from multiple in-context questions.
Extensive experiments show that our method significantly improves the baseline
performance across five VQA datasets.
","[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 17:24:42 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 08:05:25 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 18:35:44 GMT'}]",2025-03-21,"[['Wu', 'Guande', ''], ['Song', 'Huan', ''], ['Wang', 'Yawei', ''], ['Yan', 'Qiaojing', ''], ['Tian', 'Yijun', ''], ['Cheong', 'Lin Lee', ''], ['Xu', 'Panpan', '']]","[{'text': 'Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'prompt\nlibrary', 'label': 'Prompting'}]",Large Language Model,Vision-Language Models,0.6500363945960999
2503.03586,Chong Wang,"Alperen Yildiz, Sin G. Teo, Yiling Lou, Yebo Feng, Chong Wang, Dinil
  M. Divakaran","Benchmarking LLMs and LLM-based Agents in Practical Vulnerability
  Detection for Code Repositories",,,,,cs.CR,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have shown promise in software vulnerability
detection, particularly on function-level benchmarks like Devign and BigVul.
However, real-world detection requires interprocedural analysis, as
vulnerabilities often emerge through multi-hop function calls rather than
isolated functions. While repository-level benchmarks like ReposVul and VulEval
introduce interprocedural context, they remain computationally expensive, lack
pairwise evaluation of vulnerability fixes, and explore limited context
retrieval, limiting their practicality.
  We introduce JitVul, a JIT vulnerability detection benchmark linking each
function to its vulnerability-introducing and fixing commits. Built from 879
CVEs spanning 91 vulnerability types, JitVul enables comprehensive evaluation
of detection capabilities. Our results show that ReAct Agents, leveraging
thought-action-observation and interprocedural context, perform better than
LLMs in distinguishing vulnerable from benign code. While prompting strategies
like Chain-of-Thought help LLMs, ReAct Agents require further refinement. Both
methods show inconsistencies, either misidentifying vulnerabilities or
over-analyzing security guards, indicating significant room for improvement.
","[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 15:22:24 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 05:30:00 GMT'}]",2025-03-19,"[['Yildiz', 'Alperen', ''], ['Teo', 'Sin G.', ''], ['Lou', 'Yiling', ''], ['Feng', 'Yebo', ''], ['Wang', 'Chong', ''], ['Divakaran', 'Dinil M.', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'interprocedural context', 'label': 'contextual Embedding'}, {'text': 'interprocedural context', 'label': 'contextual Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompting strategies', 'label': 'Prompting'}, {'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.04784,Jiexiong Liu,"Cheng Li, Jiexiong Liu, Yixuan Chen, Yanqin Jia, Zhepeng Li","KunlunBaize: LLM with Multi-Scale Convolution and Multi-Token Prediction
  Under TransformerX Framework",21 pages,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models have demonstrated remarkable performance across various
tasks, yet they face challenges such as low computational efficiency, gradient
vanishing, and difficulties in capturing complex feature interactions. To
address these limitations, a novel framework has been proposed. This framework
incorporates a learnable dense residual skip connection mechanism, a
TransformerX module a transformer based component integrating multiscale
convolution and adaptive activation functions and a multitoken prediction
interaction module. The learnable dense residual connections enhance
information flow and feature capture across layers. Within the TransformerX
module, large convolutional kernels aggregate semantic information from
extensive text segments, while smaller convolutions focus on local word order
and syntactic structures. The adaptive activation function dynamically adjusts
its parameters based on the semantic features of the input text, improving the
model's ability to handle diverse semantic expressions and complex
relationships. The multitoken prediction module boosts data utilization and
accelerates inference by predicting multiple future tokens. These components
significantly enhance the performance and efficiency of large language models.
","[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 01:56:09 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 01:59:26 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 03:04:01 GMT'}]",2025-03-21,"[['Li', 'Cheng', ''], ['Liu', 'Jiexiong', ''], ['Chen', 'Yixuan', ''], ['Jia', 'Yanqin', ''], ['Li', 'Zhepeng', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2503.04872,Guangxiang Zhao,"Lin Sun, Guangxiang Zhao, Xiaoqi Jian, Yuhan Wu, Weihong Lin, Yongfu
  Zhu, Change Jia, Linglin Zhang, Jinzhu Wu, Junfeng Ran, Sai-er Hu, Zihan
  Jiang, Junting Zhou, Wenrui Liu, Bin Cui, Tong Yang, Xiangzheng Zhang",TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation,Preprint,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The challenge of reducing the size of Large Language Models (LLMs) while
maintaining their performance has gained significant attention. However,
existing methods, such as model distillation and transfer learning, often fail
to achieve high accuracy. To address this limitation, we introduce the
Branch-Merge distillation approach, which enhances model compression through
two phases: (1) the Branch Phase, where knowledge from a large teacher model is
\textit{selectively distilled} into specialized student models via
domain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where
these student models are merged to enable cross-domain knowledge transfer and
improve generalization. We validate our distillation approach using DeepSeek-R1
as the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting
merged model, TinyR1-32B-Preview, outperforms its counterpart
DeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics
(+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving
near-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge
distillation approach provides a scalable solution for creating smaller,
high-performing LLMs with reduced computational cost and time.
","[{'version': 'v1', 'created': 'Thu, 6 Mar 2025 16:25:53 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 10:36:30 GMT'}]",2025-03-18,"[['Sun', 'Lin', ''], ['Zhao', 'Guangxiang', ''], ['Jian', 'Xiaoqi', ''], ['Wu', 'Yuhan', ''], ['Lin', 'Weihong', ''], ['Zhu', 'Yongfu', ''], ['Jia', 'Change', ''], ['Zhang', 'Linglin', ''], ['Wu', 'Jinzhu', ''], ['Ran', 'Junfeng', ''], ['Hu', 'Sai-er', ''], ['Jiang', 'Zihan', ''], ['Zhou', 'Junting', ''], ['Liu', 'Wenrui', ''], ['Cui', 'Bin', ''], ['Yang', 'Tong', ''], ['Zhang', 'Xiangzheng', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'model distillation', 'label': 'Knowledge distillation'}, {'text': 'transfer learning', 'label': 'Few-shot Learning'}, {'text': 'Branch-Merge distillation', 'label': 'Knowledge distillation'}, {'text': 'domain-specific supervised fine-tuning', 'label': 'Fine-tuning'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.07459,Xiangru Tang,"Xiangru Tang, Daniel Shao, Jiwoong Sohn, Jiapeng Chen, Jiayi Zhang,
  Jinyu Xiang, Fang Wu, Yilun Zhao, Chenglin Wu, Wenqi Shi, Arman Cohan, Mark
  Gerstein","MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for
  Complex Medical Reasoning",,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have shown impressive performance on existing
medical question-answering benchmarks. This high performance makes it
increasingly difficult to meaningfully evaluate and differentiate advanced
methods. We present MedAgentsBench, a benchmark that focuses on challenging
medical questions requiring multi-step clinical reasoning, diagnosis
formulation, and treatment planning-scenarios where current models still
struggle despite their strong performance on standard tests. Drawing from seven
established medical datasets, our benchmark addresses three key limitations in
existing evaluations: (1) the prevalence of straightforward questions where
even base models achieve high performance, (2) inconsistent sampling and
evaluation protocols across studies, and (3) lack of systematic analysis of the
interplay between performance, cost, and inference time. Through experiments
with various base models and reasoning methods, we demonstrate that the latest
thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in
complex medical reasoning tasks. Additionally, advanced search-based agent
methods offer promising performance-to-cost ratios compared to traditional
approaches. Our analysis reveals substantial performance gaps between model
families on complex questions and identifies optimal model selections for
different computational constraints. Our benchmark and evaluation framework are
publicly available at https://github.com/gersteinlab/medagents-benchmark.
","[{'version': 'v1', 'created': 'Mon, 10 Mar 2025 15:38:44 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 01:30:56 GMT'}]",2025-03-21,"[['Tang', 'Xiangru', ''], ['Shao', 'Daniel', ''], ['Sohn', 'Jiwoong', ''], ['Chen', 'Jiapeng', ''], ['Zhang', 'Jiayi', ''], ['Xiang', 'Jinyu', ''], ['Wu', 'Fang', ''], ['Zhao', 'Yilun', ''], ['Wu', 'Chenglin', ''], ['Shi', 'Wenqi', ''], ['Cohan', 'Arman', ''], ['Gerstein', 'Mark', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'DeepSeek R1', 'label': 'Foundation Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.07604,Tianhe Lin,"Tianhe Lin, Jian Xie, Siyu Yuan, Deqing Yang",Implicit Reasoning in Transformers is Reasoning through Shortcuts,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Test-time compute is emerging as a new paradigm for enhancing language
models' complex multi-step reasoning capabilities, as demonstrated by the
success of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit
reasoning in test-time compute, implicit reasoning is more inference-efficient,
requiring fewer generated tokens. However, why does the advanced reasoning
capability fail to emerge in the implicit reasoning style? In this work, we
train GPT-2 from scratch on a curated multi-step mathematical reasoning dataset
and conduct analytical experiments to investigate how language models perform
implicit reasoning in multi-step tasks. Our findings reveal: 1) Language models
can perform step-by-step reasoning and achieve high accuracy in both in-domain
and out-of-domain tests via implicit reasoning. However, this capability only
emerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning
abilities emerging from training on unfixed-pattern data tend to overfit a
specific pattern and fail to generalize further. Notably, this limitation is
also observed in state-of-the-art large language models. These findings suggest
that language models acquire implicit reasoning through shortcut learning,
enabling strong performance on tasks with similar patterns while lacking
generalization.
","[{'version': 'v1', 'created': 'Mon, 10 Mar 2025 17:58:31 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 12:08:17 GMT'}]",2025-03-19,"[['Lin', 'Tianhe', ''], ['Xie', 'Jian', ''], ['Yuan', 'Siyu', ''], ['Yang', 'Deqing', '']]","[{'text': 'OpenAI', 'label': 'Open-source LLMs'}, {'text': 'state-of-the-art large language models', 'label': 'Large Language Model'}, {'text': 'shortcut learning', 'label': 'Few-shot Learning'}]",Large Language Model,state-of-the-art large language models,0.8680342435836792
2503.08144,Fei Wang,"Fei Wang, Chengcheng Chen, Hongyu Chen, Yugang Chang, Weiming Zeng","Bring Remote Sensing Object Detect Into Nature Language Model: Using SFT
  Method",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, large language models (LLMs) and vision-language models (VLMs) have
achieved significant success, demonstrating remarkable capabilities in
understanding various images and videos, particularly in classification and
detection tasks. However, due to the substantial differences between remote
sensing images and conventional optical images, these models face considerable
challenges in comprehension, especially in detection tasks. Directly prompting
VLMs with detection instructions often leads to unsatisfactory results. To
address this issue, this letter explores the application of VLMs for object
detection in remote sensing images. Specifically, we constructed supervised
fine-tuning (SFT) datasets using publicly available remote sensing object
detection datasets, including SSDD, HRSID, and NWPU-VHR-10. In these new
datasets, we converted annotation information into JSON-compliant natural
language descriptions, facilitating more effective understanding and training
for the VLM. We then evaluate the detection performance of various fine-tuning
strategies for VLMs and derive optimized model weights for object detection in
remote sensing images. Finally, we evaluate the model's prior knowledge
capabilities using natural language queries. Experimental results demonstrate
that, without modifying the model architecture, remote sensing object detection
can be effectively achieved using natural language alone. Additionally, the
model exhibits the ability to perform certain vision question answering (VQA)
tasks. Our datasets and related code will be released soon.
","[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 08:02:54 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 13:21:00 GMT'}]",2025-03-21,"[['Wang', 'Fei', ''], ['Chen', 'Chengcheng', ''], ['Chen', 'Hongyu', ''], ['Chang', 'Yugang', ''], ['Zeng', 'Weiming', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2503.09454,Malik Marmonier,"Malik Marmonier, Rachel Bawden, Beno\^it Sagot",Explicit Learning and the LLM in Machine Translation,,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  This study explores the capacity of large language models (LLMs) for explicit
learning, a process involving the assimilation of metalinguistic explanations
to carry out language tasks. Using constructed languages generated by
cryptographic means as controlled test environments, we designed experiments to
assess an LLM's ability to explicitly learn and apply grammar rules. Our
results demonstrate that while LLMs possess a measurable capacity for explicit
learning, this ability diminishes as the complexity of the linguistic phenomena
at hand increases. Supervised fine-tuning on chains of thought significantly
enhances LLM performance but struggles to generalize to typologically novel or
more complex linguistic features. These findings point to the need for more
diverse training sets and alternative fine-tuning strategies to further improve
explicit learning by LLMs.
","[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 14:57:08 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:23:04 GMT'}]",2025-03-20,"[['Marmonier', 'Malik', ''], ['Bawden', 'Rachel', ''], ['Sagot', 'Benoît', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'explicit\nlearning', 'label': 'Few-shot Learning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'explicit\nlearning', 'label': 'Few-shot Learning'}, {'text': 'Supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'chains of thought', 'label': 'Chain of thought'}, {'text': 'alternative fine-tuning strategies', 'label': 'Fine-tuning'}, {'text': 'explicit learning', 'label': 'Few-shot Learning'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2503.09620,Qitan Lv,"Qitan Lv, Tianyu Liu, Hong Wang",Exploiting Edited Large Language Models as General Scientific Optimizers,,,,,math.OC cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have been widely adopted in mathematical
optimization in scientific scenarios for their extensive knowledge and advanced
reasoning capabilities. Existing methods mainly focus on utilizing LLMs to
solve optimization problems in a prompt-based manner, which takes observational
feedback as additional textual descriptions. However, due to LLM's \textbf{high
sensitivity to the prompts} and \textbf{tendency to get lost in lengthy
prompts}, these methods struggle to effectively utilize the {observational}
feedback from each optimization step, which severely hinders the applications
for real-world scenarios. To address these challenges, we propose a
conceptually simple and general {bi-level} optimization method, namely
\textbf{G}eneral \textbf{S}cientific \textbf{O}ptimizers (GSO). Specifically,
GSO first utilizes inner-level simulators as experimental platforms to evaluate
the current solution and provide observational feedback. Then, LLMs serve as
knowledgeable and versatile scientists, generating new solutions by refining
potential errors from the feedback as the outer-level optimization. Finally,
simulations together with the expert knowledge in LLMs are jointly updated with
bi-level interactions via model editing. Extensive experiments show that GSO
consistently outperforms existing state-of-the-art methods using \textit{six}
different LLM backbones on \textit{seven} different tasks, demonstrating the
effectiveness and a wide range of applications.
","[{'version': 'v1', 'created': 'Sat, 8 Mar 2025 18:01:11 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 05:40:49 GMT'}]",2025-03-18,"[['Lv', 'Qitan', ''], ['Liu', 'Tianyu', ''], ['Wang', 'Hong', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompt-based', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2503.09647,Ryan Wei Heng Quek,"Ryan Quek Wei Heng, Edoardo Vittori, Keane Ong, Rui Mao, Erik Cambria,
  Gianmarco Mengaldo",Leveraging LLMS for Top-Down Sector Allocation In Automated Trading,,,,,cs.CE q-fin.PM,http://creativecommons.org/licenses/by/4.0/,"  This paper introduces a methodology leveraging Large Language Models (LLMs)
for sector-level portfolio allocation through systematic analysis of
macroeconomic conditions and market sentiment. Our framework emphasizes
top-down sector allocation by processing multiple data streams simultaneously,
including policy documents, economic indicators, and sentiment patterns.
Empirical results demonstrate superior risk-adjusted returns compared to
traditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and
portfolio return of 8.79% versus -0.61 and -1.39% respectively. These results
suggest that LLM-based systematic macro analysis presents a viable approach for
enhancing automated portfolio allocation decisions at the sector level.
","[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 08:41:36 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 14:37:14 GMT'}]",2025-03-19,"[['Heng', 'Ryan Quek Wei', ''], ['Vittori', 'Edoardo', ''], ['Ong', 'Keane', ''], ['Mao', 'Rui', ''], ['Cambria', 'Erik', ''], ['Mengaldo', 'Gianmarco', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.09657,Guanchen Li,"Guanchen Li, Yixing Xu, Zeping Li, Ji Liu, Xuanwu Yin, Dong Li, Emad
  Barsoum","T\'yr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via
  Global Sparsity Distribution Optimization",,,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Structural pruning enhances hardware-agnostic inference efficiency for large
language models (LLMs) but often struggles to maintain performance. Local
pruning performs efficient layer-by-layer compression but ignores global
topology. Global pruning has the potential to find the optimal solution
although resource-intensive. However, existing methods tend to rank structural
saliency uniformly, ignoring inter-structure dependencies and failing to
achieve end-to-end optimization. To address these limitations, we propose
T\'yr-the-Pruner, an efficient end-to-end search-based global structural
pruning framework. This framework constructs a supernet by repeatedly applying
local pruning across a range of sparsity ratios to each layer in an LLM, with
the core goal of determining the optimal sparsity distribution under a target
overall sparsity ratio. Concretely, we introduce an effective local pruning and
an expectation error accumulation approach to improve supernet construction.
Furthermore, we employ an iterative prune-and-search strategy with
coarse-to-fine sparsity granularity to ensure efficient search convergence.
Experimental results show that T\'yr-the-Pruner achieves state-of-the-art
structural pruning, retaining 97% of the dense model's performance while
removing a challenging 50% of Llama-3.1-70B's parameters.
","[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 11:52:49 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 01:51:05 GMT'}]",2025-03-19,"[['Li', 'Guanchen', ''], ['Xu', 'Yixing', ''], ['Li', 'Zeping', ''], ['Liu', 'Ji', ''], ['Yin', 'Xuanwu', ''], ['Li', 'Dong', ''], ['Barsoum', 'Emad', '']]","[{'text': 'large\nlanguage models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,"large
language models",0.9664971828460693
2503.10167,Je Won Yeom,"Hyunbin Jin, Je Won Yeom, Seunghyun Bae, Taesup Kim","""Well, Keep Thinking"": Enhancing LLM Reasoning with Adaptive Injection
  Decoding",,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) exhibit strong reasoning abilities, often
attributed to few-shot or zero-shot chain-of-thought (CoT) prompting. While
effective, these methods require labor-intensive prompt engineering, raising
the question of whether reasoning can be induced without reliance on explicit
prompts. In this work, we unlock the reasoning capabilities of LLMs without
explicit prompting. Inspired by zero-shot CoT and CoT-decoding, we propose a
novel decoding strategy that systematically nudges LLMs to continue reasoning,
thereby preventing immature reasoning processes. Specifically, we monitor the
model's generation and inject a designated phrase whenever it is likely to
conclude its response prematurely, before completing the reasoning process. Our
experimental evaluations on diverse reasoning benchmarks demonstrate that our
proposed strategy substantially improves LLM reasoning capabilities,
highlighting the potential of decoding-based interventions as an alternative to
traditional prompting techniques.
","[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 08:46:32 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 00:25:47 GMT'}]",2025-03-19,"[['Jin', 'Hyunbin', ''], ['Yeom', 'Je Won', ''], ['Bae', 'Seunghyun', ''], ['Kim', 'Taesup', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'zero-shot CoT', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2503.10905,Zhuoyan Xu,"Zhuoyan Xu, Khoi Duc Nguyen, Preeti Mukherjee, Saurabh Bagchi, Somali
  Chaterji, Yingyu Liang, Yin Li",Learning to Inference Adaptively for Multimodal Large Language Models,,,,,cs.AI cs.CV cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Multimodal Large Language Models (MLLMs) have shown impressive capabilities
in reasoning, yet come with substantial computational cost, limiting their
deployment in resource-constrained settings. Despite recent efforts on
improving the efficiency of MLLMs, prior solutions fall short in responding to
varying runtime conditions, in particular changing resource availability (e.g.,
contention due to the execution of other programs on the device). To bridge
this gap, we introduce AdaLLaVA, an adaptive inference framework that learns to
dynamically reconfigure operations in an MLLM during inference, accounting for
the input data and a latency budget. We conduct extensive experiments across
benchmarks involving question-answering, reasoning, and hallucination. Our
results show that AdaLLaVA effectively adheres to input latency budget,
achieving varying accuracy and latency tradeoffs at runtime. Further, we
demonstrate that AdaLLaVA adapts to both input latency and content, can be
integrated with token selection for enhanced efficiency, and generalizes across
MLLMs. Our project webpage with code release is at
https://zhuoyan-xu.github.io/ada-llava/.
","[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 21:39:38 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 20:35:28 GMT'}]",2025-03-19,"[['Xu', 'Zhuoyan', ''], ['Nguyen', 'Khoi Duc', ''], ['Mukherjee', 'Preeti', ''], ['Bagchi', 'Saurabh', ''], ['Chaterji', 'Somali', ''], ['Liang', 'Yingyu', ''], ['Li', 'Yin', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2503.11197,Gang Li,"Gang Li, Jizhong Liu, Heinrich Dinkel, Yadong Niu, Junbo Zhang, Jian
  Luan","Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study
  on Audio Question Answering",,,,,cs.SD cs.AI cs.CL eess.AS,http://creativecommons.org/licenses/by/4.0/,"  Recently, reinforcement learning (RL) has been shown to greatly enhance the
reasoning capabilities of large language models (LLMs), and RL-based approaches
have been progressively applied to visual multimodal tasks. However, the audio
modality has largely been overlooked in these developments. Thus, we conduct a
series of RL explorations in audio understanding and reasoning, specifically
focusing on the audio question answering (AQA) task. We leverage the group
relative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and
our experiments demonstrated state-of-the-art performance on the MMAU Test-mini
benchmark, achieving an accuracy rate of 64.5%. The main findings in this
technical report are as follows: 1) The GRPO algorithm can be effectively
applied to large audio language models (LALMs), even when the model has only
8.2B parameters; 2) With only 38k post-training samples, RL significantly
outperforms supervised fine-tuning (SFT), indicating that RL-based approaches
can be effective without large datasets; 3) The explicit reasoning process has
not shown significant benefits for AQA tasks, and how to efficiently utilize
deep thinking remains an open question for further research; 4) LALMs still lag
far behind humans auditory-language reasoning, suggesting that the RL-based
approaches warrant further exploration. Our project is available at
https://github.com/xiaomi-research/r1-aqa and
https://huggingface.co/mispeech/r1-aqa.
","[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 08:43:53 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 04:20:29 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 16:33:16 GMT'}]",2025-03-20,"[['Li', 'Gang', ''], ['Liu', 'Jizhong', ''], ['Dinkel', 'Heinrich', ''], ['Niu', 'Yadong', ''], ['Zhang', 'Junbo', ''], ['Luan', 'Jian', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'LALMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2503.11227,Jian Zhang,"Jian Zhang, Bifan Wei, Shihao Qi, haiping Zhu, Jun Liu, Qika Lin","GKG-LLM: A Unified Framework for Generalized Knowledge Graph
  Construction",,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The construction of Generalized Knowledge Graph (GKG), including knowledge
graph, event knowledge graph and commonsense knowledge graph, is fundamental
for various natural language processing tasks. Current studies typically
construct these types of graph separately, overlooking holistic insights and
potential unification that could be beneficial in computing resources and usage
perspectives. However, a key challenge in developing a unified framework for
GKG is obstacles arising from task-specific differences. In this study, we
propose a unified framework for constructing generalized knowledge graphs to
address this challenge. First, we collect data from 15 sub-tasks in 29 datasets
across the three types of graphs, categorizing them into in-sample,
counter-task, and out-of-distribution (OOD) data. Then, we propose a
three-stage curriculum learning fine-tuning framework, by iteratively injecting
knowledge from the three types of graphs into the Large Language Models.
Extensive experiments show that our proposed model improves the construction of
all three graph types across in-domain, OOD and counter-task data.
","[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 09:23:22 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 06:41:34 GMT'}]",2025-03-18,"[['Zhang', 'Jian', ''], ['Wei', 'Bifan', ''], ['Qi', 'Shihao', ''], ['Zhu', 'haiping', ''], ['Liu', 'Jun', ''], ['Lin', 'Qika', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.11280,Bryan Wilie,"Bryan Wilie, Samuel Cahyawijaya, Junxian He, Pascale Fung",High-Dimensional Interlingual Representations of Large Language Models,,,,,cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  Large language models (LLMs) trained on massive multilingual datasets hint at
the formation of interlingual constructs--a shared subspace in the
representation space. However, evidence regarding this phenomenon is mixed,
leaving it unclear whether these models truly develop unified interlingual
representations, or present a partially aligned constructs. We explore 31
diverse languages varying on their resource-levels, typologies, and
geographical regions; and find that multilingual LLMs exhibit inconsistent
cross-lingual alignments. To address this, we propose an interlingual
representation framework identifying both the shared interlingual semantic
subspace and fragmented components, existed due to representational
limitations. We introduce Interlingual Local Overlap (ILO) score to quantify
interlingual alignment by comparing the local neighborhood structures of
high-dimensional representations. We utilize ILO to investigate the impact of
single-language fine-tuning on the interlingual representations in multilingual
LLMs. Our results indicate that training exclusively on a single language
disrupts the alignment in early layers, while freezing these layers preserves
the alignment of interlingual representations, leading to improved
cross-lingual generalization. These results validate our framework and metric
for evaluating interlingual representation, and further underscore that
interlingual alignment is crucial for scalable multilingual learning.
","[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 10:39:27 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 12:16:42 GMT'}]",2025-03-20,"[['Wilie', 'Bryan', ''], ['Cahyawijaya', 'Samuel', ''], ['He', 'Junxian', ''], ['Fung', 'Pascale', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'single-language fine-tuning', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'scalable multilingual learning', 'label': 'Few-shot Learning'}]",Large Language Model,Large language models,0.9664971828460693
2503.11302,Michael Hanna,"Michael Hanna, Sandro Pezzelle, Yonatan Belinkov","Are formal and functional linguistic mechanisms dissociated in language
  models?","35 pages, 10 figures, 3 tables. Code available at
  https://github.com/hannamw/formal-functional-dissociation",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Although large language models (LLMs) are increasingly capable, these
capabilities are unevenly distributed: they excel at formal linguistic tasks,
such as producing fluent, grammatical text, but struggle more with functional
linguistic tasks like reasoning and consistent fact retrieval. Inspired by
neuroscience, recent work suggests that to succeed on both formal and
functional linguistic tasks, LLMs should use different mechanisms for each;
such localization could either be built-in or emerge spontaneously through
training. In this paper, we ask: do current models, with fast-improving
functional linguistic abilities, exhibit distinct localization of formal and
functional linguistic mechanisms? We answer this by finding and comparing the
""circuits"", or minimal computational subgraphs, responsible for various formal
and functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that
while there is indeed little overlap between circuits for formal and functional
tasks, there is also little overlap between formal linguistic tasks, as exists
in the human brain. Thus, a single formal linguistic network, unified and
distinct from functional task circuits, remains elusive. However, in terms of
cross-task faithfulness - the ability of one circuit to solve another's task -
we observe a separation between formal and functional mechanisms, suggesting
that shared mechanisms between formal tasks may exist.
","[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 11:11:03 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 12:17:11 GMT'}]",2025-03-20,"[['Hanna', 'Michael', ''], ['Pezzelle', 'Sandro', ''], ['Belinkov', 'Yonatan', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2503.11367,Insu Jang,"Insu Jang and Runyu Lu and Nikhil Bansal and Ang Chen and Mosharaf
  Chowdhury",Cornstarch: Distributed Multimodal Training Must Be Multimodality-Aware,,,,,cs.DC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal large language models (MLLMs) extend the capabilities of large
language models (LLMs) by combining heterogeneous model architectures to handle
diverse modalities like images and audio. However, this inherent heterogeneity
in MLLM model structure and data types makes makeshift extensions to existing
LLM training frameworks unsuitable for efficient MLLM training.
  In this paper, we present Cornstarch, the first general-purpose distributed
MLLM training framework. Cornstarch facilitates modular MLLM construction,
enables composable parallelization of constituent models, and introduces
MLLM-specific optimizations to pipeline and context parallelism for efficient
distributed MLLM training. Our evaluation shows that Cornstarch outperforms
state-of-the-art solutions by up to $1.57\times$ in terms of training
throughput.
  Cornstarch is an open-source project available at
https://github.com/cornstarch-org/Cornstarch.
","[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 13:07:45 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 12:39:15 GMT'}]",2025-03-18,"[['Jang', 'Insu', ''], ['Lu', 'Runyu', ''], ['Bansal', 'Nikhil', ''], ['Chen', 'Ang', ''], ['Chowdhury', 'Mosharaf', '']]","[{'text': 'Multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'large\nlanguage models', 'label': 'Large Language Model'}, {'text': 'Cornstarch', 'label': 'Open-source LLMs'}, {'text': 'Cornstarch', 'label': 'Open-source LLMs'}, {'text': 'Cornstarch', 'label': 'Open-source LLMs'}, {'text': 'Cornstarch', 'label': 'Open-source LLMs'}, {'text': 'cornstarch-org', 'label': 'Open-source LLMs'}, {'text': 'Cornstarch', 'label': 'Open-source LLMs'}]",Large Language Model,"large
language models",0.9664971828460693
2503.11960,Jiawei Li,"Jiawei Li, David Farag\'o, Christian Petrov, Iftekhar Ahmed","Consider What Humans Consider: Optimizing Commit Message Leveraging
  Contexts Considered By Human",arXiv admin note: substantial text overlap with arXiv:2501.09861,,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Commit messages are crucial in software development, supporting maintenance
tasks and communication among developers. While Large Language Models (LLMs)
have advanced Commit Message Generation (CMG) using various software contexts,
some contexts developers consider to write high-quality commit messages are
often missed by CMG techniques and can't be easily retrieved or even retrieved
at all by automated tools. To address this, we propose Commit Message
Optimization (CMO), which enhances human-written messages by leveraging LLMs
and search-based optimization. CMO starts with human-written messages and
iteratively improves them by integrating key contexts and feedback from
external evaluators. Our extensive evaluation shows CMO generates commit
messages that are significantly more Rational, Comprehensive, and Expressive
while outperforming state-of-the-art CMG methods and human messages 40.3% to
78.4% of the time. Moreover, CMO can support existing CMG techniques to further
improve message quality and generate high-quality messages when the
human-written ones are left blank.
","[{'version': 'v1', 'created': 'Sat, 15 Mar 2025 02:10:02 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 06:15:33 GMT'}]",2025-03-19,"[['Li', 'Jiawei', ''], ['Faragó', 'David', ''], ['Petrov', 'Christian', ''], ['Ahmed', 'Iftekhar', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'software contexts', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.12052,Zhiyao Sun,"Zhiyao Sun, Yu-Hui Wen, Matthieu Lin, Ho-Jui Fang, Sheng Ye, Tian Lv,
  Yong-Jin Liu","Tailor: An Integrated Text-Driven CG-Ready Human and Garment Generation
  System",Project page: https://human-tailor.github.io,,,,cs.CV cs.GR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Creating detailed 3D human avatars with garments typically requires
specialized expertise and labor-intensive processes. Although recent advances
in generative AI have enabled text-to-3D human/clothing generation, current
methods fall short in offering accessible, integrated pipelines for producing
ready-to-use clothed avatars. To solve this, we introduce Tailor, an integrated
text-to-avatar system that generates high-fidelity, customizable 3D humans with
simulation-ready garments. Our system includes a three-stage pipeline. We first
employ a large language model to interpret textual descriptions into
parameterized body shapes and semantically matched garment templates. Next, we
develop topology-preserving deformation with novel geometric losses to adapt
garments precisely to body geometries. Furthermore, an enhanced texture
diffusion module with a symmetric local attention mechanism ensures both view
consistency and photorealistic details. Quantitative and qualitative
evaluations demonstrate that Tailor outperforms existing SoTA methods in terms
of fidelity, usability, and diversity. Code will be available for academic use.
","[{'version': 'v1', 'created': 'Sat, 15 Mar 2025 08:58:02 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 06:08:49 GMT'}]",2025-03-19,"[['Sun', 'Zhiyao', ''], ['Wen', 'Yu-Hui', ''], ['Lin', 'Matthieu', ''], ['Fang', 'Ho-Jui', ''], ['Ye', 'Sheng', ''], ['Lv', 'Tian', ''], ['Liu', 'Yong-Jin', '']]","[{'text': 'large language model', 'label': 'Large Language Model'}, {'text': 'symmetric local attention mechanism', 'label': 'Attention mechanism'}]",Large Language Model,large language model,1.0
2503.12374,Zhi Chen,"Zhi Chen, Wei Ma, Lingxiao Jiang","Unveiling Pitfalls: Understanding Why AI-driven Code Agents Fail at
  GitHub Issue Resolution",,,,,cs.SE cs.AI,http://creativecommons.org/licenses/by/4.0/,"  AI-driven software development has rapidly advanced with the emergence of
software development agents that leverage large language models (LLMs) to
tackle complex, repository-level software engineering tasks. These agents go
beyond just generation of final code; they engage in multi-step reasoning,
utilize various tools for code modification and debugging, and interact with
execution environments to diagnose and iteratively resolve issues. However,
most existing evaluations focus primarily on static analyses of final code
outputs, yielding limited insights into the agents' dynamic problem-solving
processes. To fill this gap, we conduct an in-depth empirical study on 3,977
solving-phase trajectories and 3,931 testing-phase logs from 8 top-ranked
agents evaluated on 500 GitHub issues in the SWE-Bench benchmark. Our
exploratory analysis shows that Python execution errors during the issue
resolution phase correlate with lower resolution rates and increased reasoning
overheads. We have identified the most prevalent errors -- such as
ModuleNotFoundError and TypeError -- and highlighted particularly challenging
errors like OSError and database-related issues (e.g., IntegrityError) that
demand significantly more debugging effort. Furthermore, we have discovered 3
bugs in the SWE-Bench platform that affect benchmark fairness and accuracy;
these issues have been reported to and confirmed by the maintainers. To promote
transparency and foster future research, we publicly share our datasets and
analysis scripts.
","[{'version': 'v1', 'created': 'Sun, 16 Mar 2025 06:24:51 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 10:08:16 GMT'}]",2025-03-20,"[['Chen', 'Zhi', ''], ['Ma', 'Wei', ''], ['Jiang', 'Lingxiao', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'benchmark fairness and accuracy', 'label': 'Model Bias and Fairness'}]",Large Language Model,large language models,0.9664971828460693
2503.12722,Jord Nguyen,"Kenneth J. K. Ong, Lye Jia Jun, Hieu Minh ""Jord"" Nguyen, Seong Hah
  Cho, Natalia P\'erez-Campanero Antol\'in","Identifying Cooperative Personalities in Multi-agent Contexts through
  Personality Steering with Representation Engineering","Poster, Technical AI Safety Conference 2025",,,,cs.AI cs.CL cs.GT cs.MA,http://creativecommons.org/licenses/by/4.0/,"  As Large Language Models (LLMs) gain autonomous capabilities, their
coordination in multi-agent settings becomes increasingly important. However,
they often struggle with cooperation, leading to suboptimal outcomes. Inspired
by Axelrod's Iterated Prisoner's Dilemma (IPD) tournaments, we explore how
personality traits influence LLM cooperation. Using representation engineering,
we steer Big Five traits (e.g., Agreeableness, Conscientiousness) in LLMs and
analyze their impact on IPD decision-making. Our results show that higher
Agreeableness and Conscientiousness improve cooperation but increase
susceptibility to exploitation, highlighting both the potential and limitations
of personality-based steering for aligning AI agents.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 01:21:54 GMT'}]",2025-03-18,"[['Ong', 'Kenneth J. K.', ''], ['Jun', 'Lye Jia', ''], ['Nguyen', 'Hieu Minh ""Jord""', ''], ['Cho', 'Seong Hah', ''], ['Antolín', 'Natalia Pérez-Campanero', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'Agreeableness', 'label': 'Model Bias and Fairness'}, {'text': 'Conscientiousness', 'label': 'Model Bias and Fairness'}, {'text': 'Agreeableness', 'label': 'Model Bias and Fairness'}, {'text': 'Conscientiousness', 'label': 'Model Bias and Fairness'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.12757,Christine Lee,"Christine Lee, Jihye Choi, Bilge Mutlu",MAP: Multi-user Personalization with Collaborative LLM-powered Agents,"In Extended Abstracts of the CHI Conference on Human Factors in
  Computing Systems (CHI EA '25), April 26-May 1, 2025, Yokohama, Japan",,10.1145/3706599.3719853,,cs.HC cs.AI cs.RO,http://creativecommons.org/licenses/by/4.0/,"  The widespread adoption of Large Language Models (LLMs) and LLM-powered
agents in multi-user settings underscores the need for reliable, usable methods
to accommodate diverse preferences and resolve conflicting directives. Drawing
on conflict resolution theory, we introduce a user-centered workflow for
multi-user personalization comprising three stages: Reflection, Analysis, and
Feedback. We then present MAP -- a \textbf{M}ulti-\textbf{A}gent system for
multi-user \textbf{P}ersonalization -- to operationalize this workflow. By
delegating subtasks to specialized agents, MAP (1) retrieves and reflects on
relevant user information, while enhancing reliability through agent-to-agent
interactions, (2) provides detailed analysis for improved transparency and
usability, and (3) integrates user feedback to iteratively refine results. Our
user study findings (n=12) highlight MAP's effectiveness and usability for
conflict resolution while emphasizing the importance of user involvement in
resolution verification and failure management. This work highlights the
potential of multi-agent systems to implement user-centered, multi-user
personalization workflows and concludes by offering insights for
personalization in multi-user contexts.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 02:52:10 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 19:15:44 GMT'}]",2025-03-20,"[['Lee', 'Christine', ''], ['Choi', 'Jihye', ''], ['Mutlu', 'Bilge', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.12797,Xinyu Ma,"Xinyu Ma, Ziyang Ding, Zhicong Luo, Chi Chen, Zonghao Guo, Derek F.
  Wong, Xiaoyi Feng, Maosong Sun","DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs
  for Knowledge-Intensive Visual Grounding",,,,,cs.CV cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Human experts excel at fine-grained visual discrimination by leveraging
domain knowledge to refine perceptual features, a capability that remains
underdeveloped in current Multimodal Large Language Models (MLLMs). Despite
possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning
into visual perception, often generating direct responses without deeper
analysis. To bridge this gap, we introduce knowledge-intensive visual grounding
(KVG), a novel visual grounding task that requires both fine-grained perception
and domain-specific knowledge integration. To address the challenges of KVG, we
propose DeepPerception, an MLLM enhanced with cognitive visual perception
capabilities. Our approach consists of (1) an automated data synthesis pipeline
that generates high-quality, knowledge-aligned training samples, and (2) a
two-stage training framework combining supervised fine-tuning for cognitive
reasoning scaffolding and reinforcement learning to optimize
perception-cognition synergy. To benchmark performance, we introduce KVG-Bench
a comprehensive dataset spanning 10 domains with 1.3K manually curated test
cases. Experimental results demonstrate that DeepPerception significantly
outperforms direct fine-tuning, achieving +8.08\% accuracy improvements on
KVG-Bench and exhibiting +4.60\% superior cross-domain generalization over
baseline approaches. Our findings highlight the importance of integrating
cognitive processes into MLLMs for human-like visual perception and open new
directions for multimodal reasoning research. The data, codes, and models are
released at https://github.com/thunlp/DeepPerception.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 04:06:34 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 05:06:22 GMT'}]",2025-03-19,"[['Ma', 'Xinyu', ''], ['Ding', 'Ziyang', ''], ['Luo', 'Zhicong', ''], ['Chen', 'Chi', ''], ['Guo', 'Zonghao', ''], ['Wong', 'Derek F.', ''], ['Feng', 'Xiaoyi', ''], ['Sun', 'Maosong', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'reinforcement learning', 'label': 'Few-shot Learning'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2503.12821,Mingyang Song,"Mingyang Song, Xiaoye Qu, Jiawei Zhou, Yu Cheng","From Head to Tail: Towards Balanced Representation in Large
  Vision-Language Models through Adaptive Data Calibration",Accepted by CVPR 2025,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large Vision-Language Models (LVLMs) have achieved significant progress in
combining visual comprehension with language generation. Despite this success,
the training data of LVLMs still suffers from Long-Tail (LT) problems, where
the data distribution is highly imbalanced. Previous works have mainly focused
on traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as
recognition and classification. Nevertheless, the exploration of LVLM (e.g.
LLaVA) and more general tasks (e.g. Visual Question Answering and Visual
Reasoning) remains under-explored. In this paper, we first conduct an in-depth
analysis of the LT issues in LVLMs and identify two core causes: the
overrepresentation of head concepts and the underrepresentation of tail
concepts. Based on the above observation, we propose an $\textbf{A}$daptive
$\textbf{D}$ata $\textbf{R}$efinement Framework ($\textbf{ADR}$), which
consists of two stages: $\textbf{D}$ata $\textbf{R}$ebalancing ($\textbf{DR}$)
and $\textbf{D}$ata $\textbf{S}$ynthesis ($\textbf{DS}$). In the DR stage, we
adaptively rebalance the redundant data based on entity distributions, while in
the DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and
scarce images to supplement underrepresented portions. Through comprehensive
evaluations across eleven benchmarks, our proposed ADR effectively mitigates
the long-tail problem in the training data, improving the average performance
of LLaVA 1.5 relatively by 4.36%, without increasing the training data volume.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 05:01:09 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 06:02:39 GMT'}]",2025-03-19,"[['Song', 'Mingyang', ''], ['Qu', 'Xiaoye', ''], ['Zhou', 'Jiawei', ''], ['Cheng', 'Yu', '']]","[{'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Vision-Language Models,0.7742220759391785
2503.12844,Junhyeok Kim,"Junhyeok Kim, Jaewoo Park, Junhee Park, Sangeyl Lee, Jiwan Chung,
  Jisung Kim, Ji Hoon Joung, Youngjae Yu","GuideDog: A Real-World Egocentric Multimodal Dataset for Blind and
  Low-Vision Accessibility-Aware Guidance",,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Mobility remains a significant challenge for the 2.2 billion people worldwide
affected by blindness and low vision (BLV), with 7% of visually impaired
individuals experiencing falls at least once a month. While recent advances in
Multimodal Large Language Models (MLLMs) offer promising opportunities for BLV
assistance, their development has been hindered by limited datasets. This
limitation stems from the fact that BLV-aware annotation requires specialized
domain knowledge and intensive labor. To address this gap, we introduce
GuideDog, a novel accessibility-aware guide dataset containing 22K
image-description pairs (including 2K human-annotated pairs) that capture
diverse real-world scenes from a pedestrian's viewpoint. Our approach shifts
the annotation burden from generation to verification through a collaborative
human-AI framework grounded in established accessibility standards,
significantly improving efficiency while maintaining high-quality annotations.
We also develop GuideDogQA, a subset of 818 samples featuring multiple-choice
questions designed to evaluate fine-grained visual perception capabilities,
specifically object recognition and relative depth perception. Our experimental
results highlight the importance of accurate spatial understanding for
effective BLV guidance. GuideDog and GuideDogQA will advance research in
MLLM-based assistive technologies for BLV individuals while contributing to
broader applications in understanding egocentric scenes for robotics and
augmented reality. The code and dataset will be publicly available.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 05:43:40 GMT'}]",2025-03-18,"[['Kim', 'Junhyeok', ''], ['Park', 'Jaewoo', ''], ['Park', 'Junhee', ''], ['Lee', 'Sangeyl', ''], ['Chung', 'Jiwan', ''], ['Kim', 'Jisung', ''], ['Joung', 'Ji Hoon', ''], ['Yu', 'Youngjae', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2503.12852,Aditi Tiwari,Aditi Tiwari and Klara Nahrstedt,"ACT360: An Efficient 360-Degree Action Detection and Summarization
  Framework for Mission-Critical Training and Debriefing","9 pages, 8 figures",,,,cs.CV cs.MM,http://creativecommons.org/licenses/by/4.0/,"  Effective training and debriefing are critical in high-stakes,
mission-critical environments such as disaster response, military simulations,
and industrial safety, where precision and minimizing errors are paramount. The
traditional post-training analysis relies on manually reviewing 2D videos, a
time-consuming process that lacks comprehensive situational awareness. To
address these limitations, we introduce ACT360, a system that leverages
360-degree videos and machine learning for automated action detection and
structured debriefing. ACT360 integrates 360YOWO, an enhanced You Only Watch
Once (YOWO) model with spatial attention and equirectangular-aware convolution
(EAC) to mitigate panoramic video distortions. To enable deployment in
resource-constrained environments, we apply quantization and model pruning,
reducing the model size by 74% while maintaining robust accuracy (mAP drop of
only 1.5%, from 0.865 to 0.850) and improving inference speed. We validate our
approach on a publicly available dataset of 55 labeled 360-degree videos
covering seven key operational actions, recorded across various real-world
training sessions and environmental conditions. Additionally, ACT360 integrates
360AIE (Action Insight Explorer), a web-based interface for automatic action
detection, retrieval, and textual summarization using large language models
(LLMs), significantly enhancing post-incident analysis efficiency. ACT360
serves as a generalized framework for mission-critical debriefing,
incorporating EAC, spatial attention, summarization, and model optimization.
These innovations apply to any training environment requiring lightweight
action detection and structured post-exercise analysis.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 06:12:36 GMT'}]",2025-03-18,"[['Tiwari', 'Aditi', ''], ['Nahrstedt', 'Klara', '']]","[{'text': 'spatial attention', 'label': 'Attention mechanism'}, {'text': 'equirectangular-aware convolution', 'label': 'Attention mechanism'}, {'text': 'quantization', 'label': 'quantisation'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'EAC', 'label': 'Attention mechanism'}, {'text': 'spatial attention', 'label': 'Attention mechanism'}]",Large Language Model,large language models,0.9664971828460693
2503.12854,Songjun Tu,"Songjun Tu, Jiahao Lin, Xiangyu Tian, Qichao Zhang, Linjing Li, Yuqian
  Fu, Nan Xu, Wei He, Xiangyuan Lan, Dongmei Jiang, Dongbin Zhao","Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical
  Investigation",,COLM 2025,,Submitted to COLM 2025,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Recent advancements in post-training methodologies for large language models
(LLMs) have highlighted reinforcement learning (RL) as a critical component for
enhancing reasoning. However, the substantial computational costs associated
with RL-based approaches have led to growing interest in alternative paradigms,
such as Direct Preference Optimization (DPO). In this study, we investigate the
effectiveness of DPO in facilitating self-improvement for LLMs through
iterative preference-based learning. We demonstrate that a single round of DPO
with coarse filtering significantly enhances mathematical reasoning
performance, particularly for strong base model. Furthermore, we design an
iterative enhancement framework for both the generator and the reward model
(RM), enabling their mutual improvement through online interaction across
multiple rounds of DPO. Finally, with simple verifiable rewards, our model
DPO-VP achieves RL-level performance with significantly lower computational
overhead. These findings highlight DPO as a scalable and cost-effective
alternative to RL, offering a practical solution for enhancing LLM reasoning in
resource-constrained situations.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 06:28:25 GMT'}]",2025-03-20,"[['Tu', 'Songjun', ''], ['Lin', 'Jiahao', ''], ['Tian', 'Xiangyu', ''], ['Zhang', 'Qichao', ''], ['Li', 'Linjing', ''], ['Fu', 'Yuqian', ''], ['Xu', 'Nan', ''], ['He', 'Wei', ''], ['Lan', 'Xiangyuan', ''], ['Jiang', 'Dongmei', ''], ['Zhao', 'Dongbin', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Direct Preference Optimization (DPO)', 'label': 'Few-shot Learning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'iterative preference-based learning', 'label': 'Few-shot Learning'}, {'text': 'DPO', 'label': 'Few-shot Learning'}, {'text': 'strong base model', 'label': 'Foundation Model'}, {'text': 'DPO', 'label': 'Few-shot Learning'}]",Large Language Model,large language models,0.9664971828460693
2503.12884,Kento Ueda,Kento Ueda and Atsushi Matsuo,"Optimizing Ansatz Design in Quantum Generative Adversarial Networks
  Using Large Language Models","8 pages, 2 figures",,,,quant-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a novel approach for improving the design of ansatzes in Quantum
Generative Adversarial Networks (qGANs) by leveraging Large Language Models
(LLMs). By combining the strengths of LLMs with qGANs, our approach iteratively
refines ansatz structures to improve accuracy while reducing circuit depth and
the number of parameters. This study paves the way for further exploration in
AI-driven quantum algorithm design. The flexibility of our proposed workflow
extends to other quantum variational algorithms, providing a general framework
for optimizing quantum circuits in a variety of quantum computing tasks.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 07:29:05 GMT'}]",2025-03-18,"[['Ueda', 'Kento', ''], ['Matsuo', 'Atsushi', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'qGANs', 'label': 'LLM'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.12908,Xinyan Jiang,"Xinyan Jiang, Hang Ye, Yongxin Zhu, Xiaoying Zheng, Zikang Chen, Jun
  Gong","HICD: Hallucination-Inducing via Attention Dispersion for Contrastive
  Decoding to Mitigate Hallucinations in Large Language Models",Under review at ARR - February 2025,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) often generate hallucinations, producing outputs
that are contextually inaccurate or factually incorrect. We introduce HICD, a
novel method designed to induce hallucinations for contrastive decoding to
mitigate hallucinations. Unlike existing contrastive decoding methods, HICD
selects attention heads crucial to the model's prediction as inducing heads,
then induces hallucinations by dispersing attention of these inducing heads and
compares the hallucinated outputs with the original outputs to obtain the final
result. Our approach significantly improves performance on tasks requiring
contextual faithfulness, such as context completion, reading comprehension, and
question answering. It also improves factuality in tasks requiring accurate
knowledge recall. We demonstrate that our inducing heads selection and
attention dispersion method leads to more ""contrast-effective"" hallucinations
for contrastive decoding, outperforming other hallucination-inducing methods.
Our findings provide a promising strategy for reducing hallucinations by
inducing hallucinations in a controlled manner, enhancing the performance of
LLMs in a wide range of tasks.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 08:17:28 GMT'}]",2025-03-18,"[['Jiang', 'Xinyan', ''], ['Ye', 'Hang', ''], ['Zhu', 'Yongxin', ''], ['Zheng', 'Xiaoying', ''], ['Chen', 'Zikang', ''], ['Gong', 'Jun', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'HICD', 'label': 'LLM'}, {'text': 'attention heads', 'label': 'Attention mechanism'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.12931,Rui Pu,"Rui Pu, Chaozhuo Li, Rui Ha, Litian Zhang, Lirong Qiu, Xi Zhang","MirrorGuard: Adaptive Defense Against Jailbreaks via Entropy-Guided
  Mirror Crafting",,,,,cs.CR cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Defending large language models (LLMs) against jailbreak attacks is crucial
for ensuring their safe deployment. Existing defense strategies generally rely
on predefined static criteria to differentiate between harmful and benign
prompts. However, such rigid rules are incapable of accommodating the inherent
complexity and dynamic nature of real jailbreak attacks. In this paper, we
propose a novel concept of ``mirror'' to enable dynamic and adaptive defense. A
mirror refers to a dynamically generated prompt that mirrors the syntactic
structure of the input while ensuring semantic safety. The personalized
discrepancies between the input prompts and their corresponding mirrors serve
as the guiding principles for defense. A new defense paradigm, MirrorGuard, is
further proposed to detect and calibrate risky inputs based on such mirrors. An
entropy-based detection metric, Relative Input Uncertainty (RIU), is integrated
into MirrorGuard to quantify the discrepancies between input prompts and
mirrors. MirrorGuard is evaluated on several popular datasets, demonstrating
state-of-the-art defense performance while maintaining general effectiveness.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 08:41:29 GMT'}]",2025-03-18,"[['Pu', 'Rui', ''], ['Li', 'Chaozhuo', ''], ['Ha', 'Rui', ''], ['Zhang', 'Litian', ''], ['Qiu', 'Lirong', ''], ['Zhang', 'Xi', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'input prompts', 'label': 'Prompting'}, {'text': 'input prompts', 'label': 'Prompting'}]",Large Language Model,large language models,0.9664971828460693
2503.12941,Haiyang Guo,"Haiyang Guo, Fanhu Zeng, Ziwei Xiang, Fei Zhu, Da-Han Wang, Xu-Yao
  Zhang, Cheng-Lin Liu","HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of
  Multimodal Large Language Model",Preprint,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Instruction tuning is widely used to improve a pre-trained Multimodal Large
Language Model (MLLM) by training it on curated task-specific datasets,
enabling better comprehension of human instructions. However, it is infeasible
to collect all possible instruction datasets simultaneously in real-world
scenarios. Thus, enabling MLLM with continual instruction tuning is essential
for maintaining their adaptability. However, existing methods often trade off
memory efficiency for performance gains, significantly compromising overall
efficiency. In this paper, we propose a task-specific expansion and
task-general fusion framework based on the variations in Centered Kernel
Alignment (CKA) similarity across different model layers when trained on
diverse datasets. Furthermore, we analyze the information leakage present in
the existing benchmark and propose a new and more challenging benchmark to
rationally evaluate the performance of different methods. Comprehensive
experiments showcase a significant performance improvement of our method
compared to existing state-of-the-art methods. Our code will be public
available.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 08:56:03 GMT'}]",2025-03-18,"[['Guo', 'Haiyang', ''], ['Zeng', 'Fanhu', ''], ['Xiang', 'Ziwei', ''], ['Zhu', 'Fei', ''], ['Wang', 'Da-Han', ''], ['Zhang', 'Xu-Yao', ''], ['Liu', 'Cheng-Lin', '']]","[{'text': 'Instruction tuning', 'label': 'Fine-tuning'}, {'text': 'Multimodal Large\nLanguage Model', 'label': 'Large Language Model'}, {'text': 'MLLM', 'label': 'Large Language Model'}, {'text': 'MLLM', 'label': 'Large Language Model'}, {'text': 'instruction tuning', 'label': 'Fine-tuning'}]",Large Language Model,"Multimodal Large
Language Model",0.7924776673316956
2503.12972,Junming Liu,"Junming Liu, Siyuan Meng, Yanting Gao, Song Mao, Pinlong Cai, Guohang
  Yan, Yirong Chen, Zilin Bian, Botian Shi, Ding Wang","Aligning Vision to Language: Text-Free Multimodal Knowledge Graph
  Construction for Enhanced LLMs Reasoning","14 pages, 7 figures, 6 tables",,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal reasoning in Large Language Models (LLMs) struggles with
incomplete knowledge and hallucination artifacts, challenges that textual
Knowledge Graphs (KGs) only partially mitigate due to their modality isolation.
While Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal
understanding, their practical construction is impeded by semantic narrowness
of manual text annotations and inherent noise in visual-semantic entity
linkages. In this paper, we propose Vision-align-to-Language integrated
Knowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances
LLMs reasoning through cross-modal information supplementation. Specifically,
we cascade pre-trained Vision-Language Models (VLMs) to align image features
with text, transforming them into descriptions that encapsulate image-specific
information. Furthermore, we developed a cross-modal similarity verification
mechanism to quantify semantic consistency, effectively filtering out noise
introduced during feature alignment. Even without manually annotated image
captions, the refined descriptions alone suffice to construct the MMKG.
Compared to conventional MMKGs construction paradigms, our approach achieves
substantial storage efficiency gains while maintaining direct entity-to-image
linkage capability. Experimental results on multimodal reasoning tasks
demonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art
models. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 09:31:14 GMT'}]",2025-03-18,"[['Liu', 'Junming', ''], ['Meng', 'Siyuan', ''], ['Gao', 'Yanting', ''], ['Mao', 'Song', ''], ['Cai', 'Pinlong', ''], ['Yan', 'Guohang', ''], ['Chen', 'Yirong', ''], ['Bian', 'Zilin', ''], ['Shi', 'Botian', ''], ['Wang', 'Ding', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.12989,Palakorn Achananuparp,"Palakorn Achananuparp, Ee-Peng Lim","A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation
  Classification Using Large Language Models",,,,,cs.CL cs.AI cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatically annotating job data with standardized occupations from
taxonomies, known as occupation classification, is crucial for labor market
analysis. However, this task is often hindered by data scarcity and the
challenges of manual annotations. While large language models (LLMs) hold
promise due to their extensive world knowledge and in-context learning
capabilities, their effectiveness depends on their knowledge of occupational
taxonomies, which remains unclear. In this study, we assess the ability of LLMs
to generate precise taxonomic entities from taxonomy, highlighting their
limitations. To address these challenges, we propose a multi-stage framework
consisting of inference, retrieval, and reranking stages, which integrates
taxonomy-guided reasoning examples to enhance performance by aligning outputs
with taxonomic knowledge. Evaluations on a large-scale dataset show significant
improvements in classification accuracy. Furthermore, we demonstrate the
framework's adaptability for multi-label skill classification. Our results
indicate that the framework outperforms existing LLM-based methods, offering a
practical and scalable solution for occupation classification and related tasks
across LLMs.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 09:44:50 GMT'}]",2025-03-18,"[['Achananuparp', 'Palakorn', ''], ['Lim', 'Ee-Peng', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2503.13038,Junjie Chen,"Junjie Chen, Haitao Li, Zhumin Chu, Yiqun Liu, Qingyao Ai",Overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) Task,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we provide an overview of the NTCIR-18 Automatic Evaluation of
LLMs (AEOLLM) task. As large language models (LLMs) grow popular in both
academia and industry, how to effectively evaluate the capacity of LLMs becomes
an increasingly critical but still challenging issue. Existing methods can be
divided into two types: manual evaluation, which is expensive, and automatic
evaluation, which faces many limitations including task format (the majority
belong to multiple-choice questions) and evaluation criteria (occupied by
reference-based metrics). To advance the innovation of automatic evaluation, we
propose the AEOLLM task which focuses on generative tasks and encourages
reference-free methods. Besides, we set up diverse subtasks such as dialogue
generation, text expansion, summary generation and non-factoid question
answering to comprehensively test different methods. This year, we received 48
runs from 4 teams in total. This paper will describe the background of the
task, the data set, the evaluation measures and the evaluation results,
respectively.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 10:42:34 GMT'}]",2025-03-18,"[['Chen', 'Junjie', ''], ['Li', 'Haitao', ''], ['Chu', 'Zhumin', ''], ['Liu', 'Yiqun', ''], ['Ai', 'Qingyao', '']]","[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'dialogue\ngeneration', 'label': 'ChatGPT'}, {'text': 'text expansion', 'label': 'ChatGPT'}, {'text': 'summary generation', 'label': 'ChatGPT'}]",Large Language Model,large language models,0.9664971828460693
2503.13081,Yasod Ginige,"Likai Tang, Niruth Bogahawatta, Yasod Ginige, Jiarui Xu, Shixuan Sun,
  Surangika Ranathunga, Suranga Seneviratne",A Framework to Assess Multilingual Vulnerabilities of LLMs,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) are acquiring a wider range of capabilities,
including understanding and responding in multiple languages. While they
undergo safety training to prevent them from answering illegal questions,
imbalances in training data and human evaluation resources can make these
models more susceptible to attacks in low-resource languages (LRL). This paper
proposes a framework to automatically assess the multilingual vulnerabilities
of commonly used LLMs. Using our framework, we evaluated six LLMs across eight
languages representing varying levels of resource availability. We validated
the assessments generated by our automated framework through human evaluation
in two languages, demonstrating that the framework's results align with human
judgments in most cases. Our findings reveal vulnerabilities in LRL; however,
these may pose minimal risk as they often stem from the model's poor
performance, resulting in incoherent responses.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 11:39:44 GMT'}]",2025-03-18,"[['Tang', 'Likai', ''], ['Bogahawatta', 'Niruth', ''], ['Ginige', 'Yasod', ''], ['Xu', 'Jiarui', ''], ['Sun', 'Shixuan', ''], ['Ranathunga', 'Surangika', ''], ['Seneviratne', 'Suranga', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.13107,Hao Yin,"Hao Yin, Guangzong Si, Zilei Wang","ClearSight: Visual Signal Enhancement for Object Hallucination
  Mitigation in Multimodal Large language Models",,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Contrastive decoding strategies are widely used to mitigate object
hallucinations in multimodal large language models (MLLMs). By reducing
over-reliance on language priors, these strategies ensure that generated
content remains closely grounded in visual inputs, producing contextually
accurate outputs. Since contrastive decoding requires no additional training or
external tools, it offers both computational efficiency and versatility, making
it highly attractive. However, these methods present two main limitations: (1)
bluntly suppressing language priors can compromise coherence and accuracy of
generated content, and (2) processing contrastive inputs adds computational
load, significantly slowing inference speed. To address these challenges, we
propose Visual Amplification Fusion (VAF), a plug-and-play technique that
enhances attention to visual signals within the model's middle layers, where
modality fusion predominantly occurs. This approach enables more effective
capture of visual features, reducing the model's bias toward language modality.
Experimental results demonstrate that VAF significantly reduces hallucinations
across various MLLMs without affecting inference speed, while maintaining
coherence and accuracy in generated outputs.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 12:30:40 GMT'}]",2025-03-18,"[['Yin', 'Hao', ''], ['Si', 'Guangzong', ''], ['Wang', 'Zilei', '']]","[{'text': 'multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'Visual Amplification Fusion', 'label': 'contextual Embedding'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'modality fusion', 'label': 'contextual Embedding'}, {'text': 'VAF', 'label': 'contextual Embedding'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,multimodal large language models,0.7649828195571899
2503.13108,Hao Yin,"Hao Yin, Guangzong Si, Zilei Wang","Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways
  to Faster Inference",,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Multimodal large language models (MLLMs) improve performance on
vision-language tasks by integrating visual features from pre-trained vision
encoders into large language models (LLMs). However, how MLLMs process and
utilize visual information remains unclear. In this paper, a shift in the
dominant flow of visual information is uncovered: (1) in shallow layers, strong
interactions are observed between image tokens and instruction tokens, where
most visual information is injected into instruction tokens to form cross-modal
semantic representations; (2) in deeper layers, image tokens primarily interact
with each other, aggregating the remaining visual information to optimize
semantic representations within visual modality. Based on these insights, we
propose Hierarchical Modality-Aware Pruning (HiMAP), a plug-and-play inference
acceleration method that dynamically prunes image tokens at specific layers,
reducing computational costs by approximately 65% without sacrificing
performance. Our findings offer a new understanding of visual information
processing in MLLMs and provide a state-of-the-art solution for efficient
inference.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 12:31:23 GMT'}]",2025-03-18,"[['Yin', 'Hao', ''], ['Si', 'Guangzong', ''], ['Wang', 'Zilei', '']]","[{'text': 'Multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'LLM'}]",Large Language Model,Multimodal large language models,0.7649828195571899
2503.13111,Erik Daxberger,"Erik Daxberger, Nina Wenzel, David Griffiths, Haiming Gang, Justin
  Lazarow, Gefen Kohavi, Kai Kang, Marcin Eichner, Yinfei Yang, Afshin Dehghan,
  Peter Grasch",MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs,,,,,cs.CV cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal large language models (MLLMs) excel at 2D visual understanding but
remain limited in their ability to reason about 3D space. In this work, we
leverage large-scale high-quality 3D scene data with open-set annotations to
introduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation
benchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data
covers diverse spatial tasks including spatial relationship prediction, metric
size and distance estimation, and 3D grounding. We show that CA-VQA enables us
to train MM-Spatial, a strong generalist MLLM that also achieves
state-of-the-art performance on 3D spatial understanding benchmarks, including
our own. We show how incorporating metric depth and multi-view inputs (provided
in CA-VQA) can further improve 3D understanding, and demonstrate that data
alone allows our model to achieve depth perception capabilities comparable to
dedicated monocular depth estimation models. We will publish our SFT dataset
and benchmark.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 12:34:22 GMT'}]",2025-03-18,"[['Daxberger', 'Erik', ''], ['Wenzel', 'Nina', ''], ['Griffiths', 'David', ''], ['Gang', 'Haiming', ''], ['Lazarow', 'Justin', ''], ['Kohavi', 'Gefen', ''], ['Kang', 'Kai', ''], ['Eichner', 'Marcin', ''], ['Yang', 'Yinfei', ''], ['Dehghan', 'Afshin', ''], ['Grasch', 'Peter', '']]","[{'text': 'Multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,Multimodal large language models,0.7649828195571899
2503.13116,Zeng Wang,"Zeng Wang, Minghao Shao, Mohammed Nabeel, Prithwish Basu Roy, Likhitha
  Mankali, Jitendra Bhandari, Ramesh Karri, Ozgur Sinanoglu, Muhammad Shafique,
  Johann Knechtel","VeriLeaky: Navigating IP Protection vs Utility in Fine-Tuning for
  LLM-Driven Verilog Coding",,,,,cs.CR cs.AR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) offer significant potential for coding, yet
fine-tuning (FT) with curated data is essential for niche languages like
Verilog. Using proprietary intellectual property (IP) for FT presents a serious
risk, as FT data can be leaked through LLM inference. This leads to a critical
dilemma for design houses: seeking to build externally accessible LLMs offering
competitive Verilog coding, how can they leverage in-house IP to enhance FT
utility while ensuring IP protection?
  For the first time in the literature, we study this dilemma. Using LLaMA
3.1-8B, we conduct in-house FT on a baseline Verilog dataset (RTLCoder)
supplemented with our own in-house IP, which is validated through multiple
tape-outs. To rigorously assess IP leakage, we quantify structural similarity
(AST/Dolos) and functional equivalence (Synopsys Formality) between generated
codes and our in-house IP. We show that our IP can indeed be leaked, confirming
the threat. As defense, we evaluate logic locking of Verilog codes (ASSURE).
This offers some level of protection, yet reduces the IP's utility for FT and
degrades the LLM's performance. Our study shows the need for novel strategies
that are both effective and minimally disruptive to FT, an essential effort for
enabling design houses to fully utilize their proprietary IP toward LLM-driven
Verilog coding.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 12:38:03 GMT'}]",2025-03-18,"[['Wang', 'Zeng', ''], ['Shao', 'Minghao', ''], ['Nabeel', 'Mohammed', ''], ['Roy', 'Prithwish Basu', ''], ['Mankali', 'Likhitha', ''], ['Bhandari', 'Jitendra', ''], ['Karri', 'Ramesh', ''], ['Sinanoglu', 'Ozgur', ''], ['Shafique', 'Muhammad', ''], ['Knechtel', 'Johann', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2503.13250,Zejia Zhang,"Zejia Zhang, Bo Yang, Xinxing Chen, Weizhuang Shi, Haoyuan Wang, Wei
  Luo and Jian Huang","MindEye-OmniAssist: A Gaze-Driven LLM-Enhanced Assistive Robot System
  for Implicit Intention Recognition and Task Execution",,,,,cs.RO cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A promising effective human-robot interaction in assistive robotic systems is
gaze-based control. However, current gaze-based assistive systems mainly help
users with basic grasping actions, offering limited support. Moreover, the
restricted intent recognition capability constrains the assistive system's
ability to provide diverse assistance functions. In this paper, we propose an
open implicit intention recognition framework powered by Large Language Model
(LLM) and Vision Foundation Model (VFM), which can process gaze input and
recognize user intents that are not confined to predefined or specific
scenarios. Furthermore, we implement a gaze-driven LLM-enhanced assistive robot
system (MindEye-OmniAssist) that recognizes user's intentions through gaze and
assists in completing task. To achieve this, the system utilizes open
vocabulary object detector, intention recognition network and LLM to infer
their full intentions. By integrating eye movement feedback and LLM, it
generates action sequences to assist the user in completing tasks. Real-world
experiments have been conducted for assistive tasks, and the system achieved an
overall success rate of 41/55 across various undefined tasks. Preliminary
results show that the proposed method holds the potential to provide a more
user-friendly human-computer interaction interface and significantly enhance
the versatility and effectiveness of assistive systems by supporting more
complex and diverse task.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:06:14 GMT'}]",2025-03-18,"[['Zhang', 'Zejia', ''], ['Yang', 'Bo', ''], ['Chen', 'Xinxing', ''], ['Shi', 'Weizhuang', ''], ['Wang', 'Haoyuan', ''], ['Luo', 'Wei', ''], ['Huang', 'Jian', '']]","[{'text': 'Large Language Model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'Vision Foundation Model', 'label': 'Foundation Model'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}]",Large Language Model,Large Language Model,1.0
2503.13299,Yijun Liu,"Yijun Liu, Jinzheng Yu, Yang Xu, Zhongyang Li, Qingfu Zhu",A Survey on Transformer Context Extension: Approaches and Evaluation,preprint,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-sa/4.0/,"  Large language models (LLMs) based on Transformer have been widely applied in
the filed of natural language processing (NLP), demonstrating strong
performance, particularly in handling short text tasks. However, when it comes
to long context scenarios, the performance of LLMs degrades due to some
challenges. To alleviate this phenomenon, there is a number of work proposed
recently. In this survey, we first list the challenges of applying pre-trained
LLMs to process long contexts. Then systematically review the approaches
related to long context and propose our taxonomy categorizing them into four
main types: positional encoding, context compression, retrieval augmented, and
attention pattern. In addition to the approaches, we focus on the evaluation of
long context, organizing relevant data, tasks, and metrics based on existing
long context benchmarks. Finally, we summarize unresolved issues in the long
context domain and put forward our views on future developments.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:44:09 GMT'}]",2025-03-18,"[['Liu', 'Yijun', ''], ['Yu', 'Jinzheng', ''], ['Xu', 'Yang', ''], ['Li', 'Zhongyang', ''], ['Zhu', 'Qingfu', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'positional encoding', 'label': 'contextual Embedding'}, {'text': 'context compression', 'label': 'contextual Embedding'}, {'text': 'retrieval augmented', 'label': 'contextual Embedding'}, {'text': 'attention pattern', 'label': 'Attention mechanism'}]",Large Language Model,Large language models,0.9664971828460693
2503.13305,Chi Han,"Chi Han, Heng Ji",Computation Mechanism Behind LLM Position Generalization,8 pages,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Most written natural languages are composed of sequences of words and
sentences. Similar to humans, large language models (LLMs) exhibit flexibility
in handling textual positions - a phenomenon we term position generalization.
They can understand texts with position perturbations and generalize to longer
texts than those encountered during training with the latest techniques. These
phenomena suggest that LLMs handle positions tolerantly, but how LLMs
computationally process positional relevance remains largely unexplored. This
work connects the linguistic phenomenon with LLMs' computational mechanisms. We
show how LLMs enforce certain computational mechanisms for the aforementioned
tolerance in position perturbations. Despite the complex design of the
self-attention mechanism, this work reveals that LLMs learn a counterintuitive
disentanglement of attention logits. Their values show a 0.959 linear
correlation with an approximation of the arithmetic sum of positional relevance
and semantic importance. Furthermore, we identify a prevalent pattern in
intermediate features, which we prove theoretically enables this effect. The
pattern, which is different from how randomly initialized parameters would
behave, suggests that it is a learned behavior rather than a natural result of
the model architecture. Based on these findings, we provide computational
explanations and criteria for LLMs' position flexibilities. This work takes a
pioneering step in linking position generalization with modern LLMs' internal
mechanisms.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:47:37 GMT'}]",2025-03-18,"[['Han', 'Chi', ''], ['Ji', 'Heng', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'self-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2503.13340,Xinyu Jessica Wang,"Xinyu Jessica Wang, Christine Lee, Bilge Mutlu","LearnMate: Enhancing Online Education with LLM-Powered Personalized
  Learning Plans and Support","In Extended Abstracts of the CHI Conference on Human Factors in
  Computing Systems (CHI EA '25), April 26-May 1, 2025, Yokohama, Japan",,10.1145/3706599.3719857,,cs.HC,http://creativecommons.org/licenses/by/4.0/,"  With the increasing prevalence of online learning, adapting education to
diverse learner needs remains a persistent challenge. Recent advancements in
artificial intelligence (AI), particularly large language models (LLMs),
promise powerful tools and capabilities to enhance personalized learning in
online educational environments. In this work, we explore how LLMs can improve
personalized learning experiences by catering to individual user needs toward
enhancing the overall quality of online education. We designed personalization
guidelines based on the growing literature on personalized learning to ground
LLMs in generating tailored learning plans. To operationalize these guidelines,
we implemented LearnMate, an LLM-based system that generates personalized
learning plans and provides users with real-time learning support. We discuss
the implications and future directions of this work, aiming to move beyond the
traditional one-size-fits-all approach by integrating LLM-based personalized
support into online learning environments.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 16:18:23 GMT'}]",2025-03-18,"[['Wang', 'Xinyu Jessica', ''], ['Lee', 'Christine', ''], ['Mutlu', 'Bilge', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LearnMate', 'label': 'LLM-based'}]",Large Language Model,large language models,0.9664971828460693
2503.13342,Ying Jiao,"Ying Jiao, Luc De Raedt, and Giuseppe Marra",Valid Text-to-SQL Generation with Unification-based DeepStochLog,,"In International Conference on Neural-Symbolic Learning and
  Reasoning (pp. 312-330). Cham: Springer Nature Switzerland (2024)",10.1007/978-3-031-71167-1_17,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models have been used to translate natural language questions
to SQL queries. Without hard constraints on syntax and database schema, they
occasionally produce invalid queries that are not executable. These failures
limit the usage of these systems in real-life scenarios. We propose a
neurosymbolic framework that imposes SQL syntax and schema constraints with
unification-based definite clause grammars and thus guarantees the generation
of valid queries. Our framework also builds a bi-directional interface to
language models to leverage their natural language understanding abilities. The
evaluation results on a subset of SQL grammars show that all our output queries
are valid. This work is the first step towards extending language models with
unification-based grammars. We demonstrate this extension enhances the
validity, execution accuracy, and ground truth alignment of the underlying
language model by a large margin. Our code is available at
https://github.com/ML-KULeuven/deepstochlog-lm.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 16:21:10 GMT'}]",2025-03-18,"[['Jiao', 'Ying', ''], ['De Raedt', 'Luc', ''], ['Marra', 'Giuseppe', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'unification-based definite clause grammars', 'label': 'Embedding'}, {'text': 'unification-based grammars', 'label': 'Embedding'}]",Large Language Model,Large language models,0.9664971828460693
2503.13401,Alexander Ku,"Alexander Ku, Declan Campbell, Xuechunzi Bai, Jiayi Geng, Ryan Liu,
  Raja Marjieh, R. Thomas McCoy, Andrew Nam, Ilia Sucholutsky, Veniamin
  Veselovsky, Liyi Zhang, Jian-Qiao Zhu, Thomas L. Griffiths","Using the Tools of Cognitive Science to Understand Large Language Models
  at Different Levels of Analysis",,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Modern artificial intelligence systems, such as large language models, are
increasingly powerful but also increasingly hard to understand. Recognizing
this problem as analogous to the historical difficulties in understanding the
human mind, we argue that methods developed in cognitive science can be useful
for understanding large language models. We propose a framework for applying
these methods based on Marr's three levels of analysis. By revisiting
established cognitive science techniques relevant to each level and
illustrating their potential to yield insights into the behavior and internal
organization of large language models, we aim to provide a toolkit for making
sense of these new kinds of minds.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:33:54 GMT'}]",2025-03-18,"[['Ku', 'Alexander', ''], ['Campbell', 'Declan', ''], ['Bai', 'Xuechunzi', ''], ['Geng', 'Jiayi', ''], ['Liu', 'Ryan', ''], ['Marjieh', 'Raja', ''], ['McCoy', 'R. Thomas', ''], ['Nam', 'Andrew', ''], ['Sucholutsky', 'Ilia', ''], ['Veselovsky', 'Veniamin', ''], ['Zhang', 'Liyi', ''], ['Zhu', 'Jian-Qiao', ''], ['Griffiths', 'Thomas L.', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2503.13413,Dengyun Peng,"Dengyun Peng, Yuhang Zhou, Qiguang Chen, Jinhao Liu, Jingjing Chen,
  Libo Qin","DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization
  Framework from a Deep-Learning Perspective",Preprint,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have achieved remarkable success across diverse
tasks, largely driven by well-designed prompts. However, crafting and selecting
such prompts often requires considerable human effort, significantly limiting
its scalability. To mitigate this, recent studies have explored automated
prompt optimization as a promising solution. Despite these efforts, existing
methods still face critical challenges in robustness, efficiency, and
generalization. To systematically address these challenges, we first conduct an
empirical analysis to identify the limitations of current reflection-based
prompt optimization paradigm. Building on these insights, we propose 7
innovative approaches inspired by traditional deep learning paradigms for
prompt optimization (DLPO), seamlessly integrating these concepts into
text-based gradient optimization. Through these advancements, we progressively
tackle the aforementioned challenges and validate our methods through extensive
experimentation. We hope our study not only provides valuable guidance for
future research but also offers a comprehensive understanding of the challenges
and potential solutions in prompt optimization. Our code is available at
https://github.com/sfasfaffa/DLPO.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:42:51 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 04:41:37 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 14:18:01 GMT'}]",2025-03-20,"[['Peng', 'Dengyun', ''], ['Zhou', 'Yuhang', ''], ['Chen', 'Qiguang', ''], ['Liu', 'Jinhao', ''], ['Chen', 'Jingjing', ''], ['Qin', 'Libo', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'scalability', 'label': 'Scaling law'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.13444,Ye Liu,"Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, Mike Zheng Shou",VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning,Project Page: https://videomind.github.io/,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Videos, with their unique temporal dimension, demand precise grounded
understanding, where answers are directly linked to visual, interpretable
evidence. Despite significant breakthroughs in reasoning capabilities within
Large Language Models, multi-modal reasoning - especially for videos - remains
unexplored. In this work, we introduce VideoMind, a novel video-language agent
designed for temporal-grounded video understanding. VideoMind incorporates two
key innovations: (i) We identify essential capabilities for video temporal
reasoning and develop a role-based agentic workflow, including a planner for
coordinating different roles, a grounder for temporal localization, a verifier
to assess temporal interval accuracy, and an answerer for question-answering.
(ii) To efficiently integrate these diverse roles, we propose a novel
Chain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA
adaptors while avoiding the overhead of multiple models, thus balancing
efficiency and flexibility. Extensive experiments on 14 public benchmarks
demonstrate that our agent achieves state-of-the-art performance on diverse
video understanding tasks, including 3 on grounded video question-answering, 6
on video temporal grounding, and 5 on general video question-answering,
underscoring its effectiveness in advancing video agent and long-form temporal
reasoning.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:59:33 GMT'}]",2025-03-18,"[['Liu', 'Ye', ''], ['Lin', 'Kevin Qinghong', ''], ['Chen', 'Chang Wen', ''], ['Shou', 'Mike Zheng', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.13575,Kai Tong,"Kai Tong, Kang Pan, Xiao Zhang, Erli Meng, Run He, Yawen Cui, Nuoyan
  Guo, Huiping Zhuang","Analytic Subspace Routing: How Recursive Least Squares Works in
  Continual Learning of Large Language Model","11 pages, 4 figures",,,,cs.LG cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) possess encompassing capabilities that can
process diverse language-related tasks. However, finetuning on LLMs will
diminish this general skills and continual finetuning will further cause severe
degradation on accumulated knowledge. Recently, Continual Learning (CL) in
Large Language Models (LLMs) arises which aims to continually adapt the LLMs to
new tasks while maintaining previously learned knowledge and inheriting general
skills. Existing techniques either leverage previous data to replay, leading to
extra computational costs, or utilize a single parameter-efficient module to
learn the downstream task, constraining new knowledge absorption with
interference between different tasks. Toward these issues, this paper proposes
Analytic Subspace Routing(ASR) to address these challenges. For each task, we
isolate the learning within a subspace of deep layers' features via low-rank
adaptation, eliminating knowledge interference between different tasks.
Additionally, we propose an analytic routing mechanism to properly utilize
knowledge learned in different subspaces. Our approach employs Recursive Least
Squares to train a multi-task router model, allowing the router to dynamically
adapt to incoming data without requiring access to historical data. Also, the
router effectively assigns the current task to an appropriate subspace and has
a non-forgetting property of previously learned tasks with a solid theoretical
guarantee. Experimental results demonstrate that our method achieves
near-perfect retention of prior knowledge while seamlessly integrating new
information, effectively overcoming the core limitations of existing methods.
Our code will be released after acceptance.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 13:40:46 GMT'}]",2025-03-19,"[['Tong', 'Kai', ''], ['Pan', 'Kang', ''], ['Zhang', 'Xiao', ''], ['Meng', 'Erli', ''], ['He', 'Run', ''], ['Cui', 'Yawen', ''], ['Guo', 'Nuoyan', ''], ['Zhuang', 'Huiping', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'finetuning', 'label': 'Fine-tuning'}, {'text': 'continual finetuning', 'label': 'Fine-tuning'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.13580,Sijia Gu,"Sijia Gu, Noor Nashid, Ali Mesbah",LLM Test Generation via Iterative Hybrid Program Analysis,,,,,cs.SE cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automating unit test generation remains a significant challenge, particularly
for complex methods in real-world projects. While Large Language Models (LLMs)
have made strides in code generation, they struggle to achieve high branch
coverage due to their limited ability to reason about intricate control flow
structures. To address this limitation, we introduce Panta, a technique that
emulates the iterative process human developers follow when analyzing code and
constructing test cases. Panta integrates static control flow analysis and
dynamic code coverage analysis to systematically guide LLMs in identifying
uncovered execution paths and generating better test cases. By incorporating an
iterative feedback-driven mechanism, our technique continuously refines test
generation based on static and dynamic path coverage insights, ensuring more
comprehensive and effective testing. Our empirical evaluation, conducted on
classes with high cyclomatic complexity from open-source projects, demonstrates
that Panta achieves 26% higher line coverage and 23% higher branch coverage
compared to the state-of-the-art.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 16:10:38 GMT'}]",2025-03-19,"[['Gu', 'Sijia', ''], ['Nashid', 'Noor', ''], ['Mesbah', 'Ali', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'open-source projects', 'label': 'Open-source LLMs'}, {'text': 'Panta', 'label': 'LLM'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.13646,Chiara Plizzari,"Chiara Plizzari, Alessio Tonioni, Yongqin Xian, Achin Kulshrestha,
  Federico Tombari","Omnia de EgoTempo: Benchmarking Temporal Understanding of Multi-Modal
  LLMs in Egocentric Videos","Accepted to CVPR 2025. Dataset and code are available at
  https://github.com/google-research-datasets/egotempo.git",,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Understanding fine-grained temporal dynamics is crucial in egocentric videos,
where continuous streams capture frequent, close-up interactions with objects.
In this work, we bring to light that current egocentric video
question-answering datasets often include questions that can be answered using
only few frames or commonsense reasoning, without being necessarily grounded in
the actual video. Our analysis shows that state-of-the-art Multi-Modal Large
Language Models (MLLMs) on these benchmarks achieve remarkably high performance
using just text or a single frame as input. To address these limitations, we
introduce EgoTempo, a dataset specifically designed to evaluate temporal
understanding in the egocentric domain. EgoTempo emphasizes tasks that require
integrating information across the entire video, ensuring that models would
need to rely on temporal patterns rather than static cues or pre-existing
knowledge. Extensive experiments on EgoTempo show that current MLLMs still fall
short in temporal reasoning on egocentric videos, and thus we hope EgoTempo
will catalyze new research in the field and inspire models that better capture
the complexity of temporal dynamics. Dataset and code are available at
https://github.com/google-research-datasets/egotempo.git.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 18:50:36 GMT'}]",2025-03-19,"[['Plizzari', 'Chiara', ''], ['Tonioni', 'Alessio', ''], ['Xian', 'Yongqin', ''], ['Kulshrestha', 'Achin', ''], ['Tombari', 'Federico', '']]","[{'text': 'Multi-Modal Large\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,"Multi-Modal Large
Language Models",0.7925285696983337
2503.13660,Qian Meng,"Qian Meng, Jin Peng Zhou, Kilian Q. Weinberger, and Hadas Kress-Gazit","INPROVF: Leveraging Large Language Models to Repair High-level Robot
  Controllers from Assumption Violations","To appear in ICLR 2025 Workshop: VerifAI: AI Verification in the
  Wild; in submission to 2025 IEEE 21th International Conference on Automation
  Science and Engineering (CASE), Los Angeles, CA, USA: IEEE, Aug. 2025",,,,cs.RO cs.AI cs.FL cs.SY eess.SY,http://creativecommons.org/licenses/by/4.0/,"  This paper presents INPROVF, an automatic framework that combines large
language models (LLMs) and formal methods to speed up the repair process of
high-level robot controllers. Previous approaches based solely on formal
methods are computationally expensive and cannot scale to large state spaces.
In contrast, INPROVF uses LLMs to generate repair candidates, and formal
methods to verify their correctness. To improve the quality of these
candidates, our framework first translates the symbolic representations of the
environment and controllers into natural language descriptions. If a candidate
fails the verification, INPROVF provides feedback on potential unsafe behaviors
or unsatisfied tasks, and iteratively prompts LLMs to generate improved
solutions. We demonstrate the effectiveness of INPROVF through 12 violations
with various workspaces, tasks, and state space sizes.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 19:08:36 GMT'}]",2025-03-19,"[['Meng', 'Qian', ''], ['Zhou', 'Jin Peng', ''], ['Weinberger', 'Kilian Q.', ''], ['Kress-Gazit', 'Hadas', '']]","[{'text': 'large\nlanguage models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompts', 'label': 'Prompting'}]",Large Language Model,"large
language models",0.9664971828460693
2503.13733,Dilshod Azizov,"Daniil Orel, Dilshod Azizov, Preslav Nakov","CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual,
  Multi-Generator and Multi-Domain Settings",,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have revolutionized code generation, automating
programming with remarkable efficiency. However, these advancements challenge
programming skills, ethics, and assessment integrity, making the detection of
LLM-generated code essential for maintaining accountability and standards.
While, there has been some research on this problem, it generally lacks domain
coverage and robustness, and only covers a small number of programming
languages. To this end, we propose a framework capable of distinguishing
between human- and LLM-written code across multiple programming languages, code
generators, and domains. We use a large-scale dataset from renowned platforms
and LLM-based code generators, alongside applying rigorous data quality checks,
feature engineering, and comparative analysis using evaluation of traditional
machine learning models, pre-trained language models (PLMs), and LLMs for code
detection. We perform an evaluation on out-of-domain scenarios, such as
detecting the authorship and hybrid authorship of generated code and
generalizing to unseen models, domains, and programming languages. Moreover,
our extensive experiments show that our framework effectively distinguishes
human- from LLM-written code and sets a new benchmark for this task.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 21:41:37 GMT'}]",2025-03-19,"[['Orel', 'Daniil', ''], ['Azizov', 'Dilshod', ''], ['Nakov', 'Preslav', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ethics', 'label': 'AI Ethics'}, {'text': 'renowned platforms', 'label': 'Open-source LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2503.13773,Tanmoy Sen,"Haiying Shen, Tanmoy Sen","Mitigating KV Cache Competition to Enhance User Experience in LLM
  Inference",,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes
high tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing
user experience, particularly in time-sensitive applications. However,
satisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To
address this, we propose a system, named CacheOPT for mitigating KV Cache
competition, based on key insights from our measurements, incorporating novel
components. First, it estimates a request's output length, bounding the
deviation with a high specified probability, adjusted based on the request
arrival rate. Second, it allocates the estimated KVC demand to a request, and
reuses other requests' allocated KVC to avoid preemptions while reducing
waiting time. Third, it proactively allocates KVC before instead of at the time
a request exhausts its allocation and reserves KVC globally to prevent
preemptions. Fourth, it chooses a request that has long TBT SLO, long job
remaining time and short preemption time to preempt. Fifth, it selects the
shortest-latency strategy between swapping and recomputation for preemptions.
Experiments show that CacheOPT achieves up to 3.29$\times$ and 2.83$\times$
lower tail TBT and tail TTFT, 47\% and 53\% higher TTFT and TBT SLO
attainments, and supports up to 1.58$\times$ higher request arrival rate than
the state-of-the-art methods.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 23:38:29 GMT'}]",2025-03-19,"[['Shen', 'Haiying', ''], ['Sen', 'Tanmoy', '']]","[{'text': 'Large Language Model', 'label': 'Large Language Model'}]",Large Language Model,Large Language Model,1.0
2503.13792,Xinyu Tian,"Xinyu Tian, Shu Zou, Zhaoyuan Yang, Jing Zhang","Identifying and Mitigating Position Bias of Multi-image Vision-Language
  Models",Accepted to CVPR2025,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  The evolution of Large Vision-Language Models (LVLMs) has progressed from
single to multi-image reasoning. Despite this advancement, our findings
indicate that LVLMs struggle to robustly utilize information across multiple
images, with predictions significantly affected by the alteration of image
positions. To further explore this issue, we introduce Position-wise Question
Answering (PQA), a meticulously designed task to quantify reasoning
capabilities at each position. Our analysis reveals a pronounced position bias
in LVLMs: open-source models excel in reasoning with images positioned later
but underperform with those in the middle or at the beginning, while
proprietary models show improved comprehension for images at the beginning and
end but struggle with those in the middle. Motivated by this, we propose SoFt
Attention (SoFA), a simple, training-free approach that mitigates this bias by
employing linear interpolation between inter-image causal attention and
bidirectional counterparts. Experimental results demonstrate that SoFA reduces
position bias and enhances the reasoning performance of existing LVLMs.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 00:45:02 GMT'}]",2025-03-19,"[['Tian', 'Xinyu', ''], ['Zou', 'Shu', ''], ['Yang', 'Zhaoyuan', ''], ['Zhang', 'Jing', '']]","[{'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'SoFt\nAttention', 'label': 'Attention mechanism'}, {'text': 'inter-image causal attention', 'label': 'Attention mechanism'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Vision-Language Models,0.7742220759391785
2503.13793,Dipin Khati,"Dipin Khati and Yijin Liu and David N. Palacio and Yixuan Zhang and
  Denys Poshyvanyk","Mapping the Trust Terrain: LLMs in Software Engineering -- Insights and
  Perspectives",,,,,cs.SE cs.AI,http://creativecommons.org/licenses/by-sa/4.0/,"  Applications of Large Language Models (LLMs) are rapidly growing in industry
and academia for various software engineering (SE) tasks. As these models
become more integral to critical processes, ensuring their reliability and
trustworthiness becomes essential. Consequently, the concept of trust in these
systems is becoming increasingly critical. Well-calibrated trust is important,
as excessive trust can lead to security vulnerabilities, and risks, while
insufficient trust can hinder innovation. However, the landscape of
trust-related concepts in LLMs in SE is relatively unclear, with concepts such
as trust, distrust, and trustworthiness lacking clear conceptualizations in the
SE community. To bring clarity to the current research status and identify
opportunities for future work, we conducted a comprehensive review of $88$
papers: a systematic literature review of $18$ papers focused on LLMs in SE,
complemented by an analysis of 70 papers from broader trust literature.
Additionally, we conducted a survey study with 25 domain experts to gain
insights into practitioners' understanding of trust and identify gaps between
existing literature and developers' perceptions. The result of our analysis
serves as a roadmap that covers trust-related concepts in LLMs in SE and
highlights areas for future exploration.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 00:49:43 GMT'}]",2025-03-19,"[['Khati', 'Dipin', ''], ['Liu', 'Yijin', ''], ['Palacio', 'David N.', ''], ['Zhang', 'Yixuan', ''], ['Poshyvanyk', 'Denys', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.13794,Yang Zhou,"Yang Zhou, Shiyu Zhao, Yuxiao Chen, Zhenting Wang, Dimitris N. Metaxas","LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated
  Data Generation",,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large foundation models trained on large-scale visual-text data can
significantly enhance Open Vocabulary Object Detection (OVD) through data
generation. However, this may lead to biased synthetic data and overfitting to
specific configurations. It can sidestep biases of manually curated data
generation by directly leveraging hidden states of Large Language Models
(LLMs), which is surprisingly rarely explored. This paper presents a systematic
method to enhance visual grounding by utilizing decoder layers of the LLM of a
MLLM. We introduce a zero-initialized cross-attention adapter to enable
efficient knowledge transfer from LLMs to object detectors, an new approach
called LED (LLM Enhanced Open-Vocabulary Object Detection). We demonstrate that
intermediate hidden states from early LLM layers retain strong spatial-semantic
correlations that are beneficial to grounding tasks. Experiments show that our
adaptation strategy significantly enhances the performance on complex free-form
text queries while remaining the same on plain categories. With our adaptation,
Qwen2-0.5B with Swin-T as the vision encoder improves GroundingDINO by 2.33% on
Omnilabel, at the overhead of 8.7% more GFLOPs. Qwen2-0.5B with a larger vision
encoder can further boost the performance by 6.22%. We further validate our
design by ablating on varied adapter architectures, sizes of LLMs, and which
layers to add adaptation.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 00:50:40 GMT'}]",2025-03-19,"[['Zhou', 'Yang', ''], ['Zhao', 'Shiyu', ''], ['Chen', 'Yuxiao', ''], ['Wang', 'Zhenting', ''], ['Metaxas', 'Dimitris N.', '']]","[{'text': 'Large foundation models', 'label': 'Foundation Model'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.13804,Kai Guo,"Kai Guo, Harry Shomer, Shenglai Zeng, Haoyu Han, Yu Wang, Jiliang Tang",Empowering GraphRAG with Knowledge Filtering and Integration,,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In recent years, large language models (LLMs) have revolutionized the field
of natural language processing. However, they often suffer from knowledge gaps
and hallucinations. Graph retrieval-augmented generation (GraphRAG) enhances
LLM reasoning by integrating structured knowledge from external graphs.
However, we identify two key challenges that plague GraphRAG:(1) Retrieving
noisy and irrelevant information can degrade performance and (2)Excessive
reliance on external knowledge suppresses the model's intrinsic reasoning. To
address these issues, we propose GraphRAG-FI (Filtering and Integration),
consisting of GraphRAG-Filtering and GraphRAG-Integration. GraphRAG-Filtering
employs a two-stage filtering mechanism to refine retrieved information.
GraphRAG-Integration employs a logits-based selection strategy to balance
external knowledge from GraphRAG with the LLM's intrinsic reasoning,reducing
over-reliance on retrievals. Experiments on knowledge graph QA tasks
demonstrate that GraphRAG-FI significantly improves reasoning performance
across multiple backbone models, establishing a more reliable and effective
GraphRAG framework.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 01:29:55 GMT'}]",2025-03-19,"[['Guo', 'Kai', ''], ['Shomer', 'Harry', ''], ['Zeng', 'Shenglai', ''], ['Han', 'Haoyu', ''], ['Wang', 'Yu', ''], ['Tang', 'Jiliang', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GraphRAG', 'label': 'RAG'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'GraphRAG', 'label': 'RAG'}, {'text': 'GraphRAG-FI', 'label': 'RAG'}, {'text': 'GraphRAG', 'label': 'RAG'}, {'text': 'GraphRAG', 'label': 'RAG'}]",Large Language Model,large language models,0.9664971828460693
2503.13856,Kai Chen Nj,"Kai Chen, Xinfeng Li, Tianpei Yang, Hewei Wang, Wei Dong, Yang Gao","MDTeamGPT: A Self-Evolving LLM-based Multi-Agent Framework for
  Multi-Disciplinary Team Medical Consultation",24 pages,,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have made significant progress in various
fields. However, challenges remain in Multi-Disciplinary Team (MDT) medical
consultations. Current research enhances reasoning through role assignment,
task decomposition, and accumulation of medical experience. Multi-role
collaboration in MDT consultations often results in excessively long dialogue
histories. This increases the model's cognitive burden and degrades both
efficiency and accuracy. Some methods only store treatment histories. They do
not extract effective experience or reflect on errors. This limits knowledge
generalization and system evolution. We propose a multi-agent MDT medical
consultation framework based on LLMs to address these issues. Our framework
uses consensus aggregation and a residual discussion structure for multi-round
consultations. It also employs a Correct Answer Knowledge Base (CorrectKB) and
a Chain-of-Thought Knowledge Base (ChainKB) to accumulate consultation
experience. These mechanisms enable the framework to evolve and continually
improve diagnosis rationality and accuracy. Experimental results on the MedQA
and PubMedQA datasets demonstrate that our framework achieves accuracies of
90.1% and 83.9%, respectively, and that the constructed knowledge bases
generalize effectively across test sets from both datasets.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 03:07:34 GMT'}]",2025-03-19,"[['Chen', 'Kai', ''], ['Li', 'Xinfeng', ''], ['Yang', 'Tianpei', ''], ['Wang', 'Hewei', ''], ['Dong', 'Wei', ''], ['Gao', 'Yang', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ChainKB', 'label': 'Chain of thought'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.13879,Wei Chen,"Wei Chen, Han Ding, Meng Yuan, Zhao Zhang, Deqing Wang, Fuzhen Zhuang","Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review
  Generation via Cognitive Alignment",23 pages,,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The rapid growth of scholarly submissions has overwhelmed traditional peer
review systems, driving the need for intelligent automation to preserve
scientific rigor. While large language models (LLMs) show promise in automating
manuscript critiques, their ability to synthesize high-stakes meta-reviews,
which require conflict-aware reasoning and consensus derivation, remains
underdeveloped. Existing methods fail to effectively handle conflicting
viewpoints within differing opinions, and often introduce additional cognitive
biases, such as anchoring effects and conformity bias.To overcome these
limitations, we propose the Cognitive Alignment Framework (CAF), a dual-process
architecture that transforms LLMs into adaptive scientific arbitrators. By
operationalizing Kahneman's dual-process theory, CAF introduces a three-step
cognitive pipeline: review initialization, incremental integration, and
cognitive alignment.Empirical validation shows that CAF outperforms existing
LLM-based methods, with sentiment consistency gains reaching up to 19.47\% and
content consistency improving by as much as 12.95\%.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 04:13:11 GMT'}]",2025-03-19,"[['Chen', 'Wei', ''], ['Ding', 'Han', ''], ['Yuan', 'Meng', ''], ['Zhang', 'Zhao', ''], ['Wang', 'Deqing', ''], ['Zhuang', 'Fuzhen', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'anchoring effects', 'label': 'Model Bias and Fairness'}, {'text': 'conformity bias', 'label': 'Model Bias and Fairness'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2503.13962,Chengze Jiang,"Chengze Jiang, Zhuangzhuang Wang, Minjing Dong, Jie Gui",Survey of Adversarial Robustness in Multimodal Large Language Models,9 pages,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal Large Language Models (MLLMs) have demonstrated exceptional
performance in artificial intelligence by facilitating integrated understanding
across diverse modalities, including text, images, video, audio, and speech.
However, their deployment in real-world applications raises significant
concerns about adversarial vulnerabilities that could compromise their safety
and reliability. Unlike unimodal models, MLLMs face unique challenges due to
the interdependencies among modalities, making them susceptible to
modality-specific threats and cross-modal adversarial manipulations. This paper
reviews the adversarial robustness of MLLMs, covering different modalities. We
begin with an overview of MLLMs and a taxonomy of adversarial attacks tailored
to each modality. Next, we review key datasets and evaluation metrics used to
assess the robustness of MLLMs. After that, we provide an in-depth review of
attacks targeting MLLMs across different modalities. Our survey also identifies
critical challenges and suggests promising future research directions.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 06:54:59 GMT'}]",2025-03-19,"[['Jiang', 'Chengze', ''], ['Wang', 'Zhuangzhuang', ''], ['Dong', 'Minjing', ''], ['Gui', 'Jie', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2503.13983,Jiankang Wang,"Jiankang Wang, Zhihan zhang, Zhihang Liu, Yang Li, Jiannan Ge, Hongtao
  Xie, Yongdong Zhang","SpaceVLLM: Endowing Multimodal Large Language Model with Spatio-Temporal
  Video Grounding Capability",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal large language models (MLLMs) have made remarkable progress in
either temporal or spatial localization. However, they struggle to perform
spatio-temporal video grounding. This limitation stems from two major
challenges. Firstly, it is difficult to extract accurate spatio-temporal
information of each frame in the video. Secondly, the substantial number of
visual tokens makes it challenging to precisely map visual tokens of each frame
to their corresponding spatial coordinates. To address these issues, we
introduce SpaceVLLM, a MLLM endowed with spatio-temporal video grounding
capability. Specifically, we adopt a set of interleaved Spatio-Temporal Aware
Queries to capture temporal perception and dynamic spatial information.
Moreover, we propose a Query-Guided Space Decoder to establish a corresponding
connection between the queries and spatial coordinates. Additionally, due to
the lack of spatio-temporal datasets, we construct the Unified Spatio-Temporal
Grounding (Uni-STG) dataset, comprising 480K instances across three tasks. This
dataset fully exploits the potential of MLLM to simultaneously facilitate
localization in both temporal and spatial dimensions. Extensive experiments
demonstrate that SpaceVLLM achieves the state-of-the-art performance across 11
benchmarks covering temporal, spatial, spatio-temporal and video understanding
tasks, highlighting the effectiveness of our approach. Our code, datasets and
model will be released.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 07:40:36 GMT'}]",2025-03-19,"[['Wang', 'Jiankang', ''], ['zhang', 'Zhihan', ''], ['Liu', 'Zhihang', ''], ['Li', 'Yang', ''], ['Ge', 'Jiannan', ''], ['Xie', 'Hongtao', ''], ['Zhang', 'Yongdong', '']]","[{'text': 'Multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,Multimodal large language models,0.7649828195571899
2503.14043,Guy Bar-Shalom,"Guy Bar-Shalom, Fabrizio Frasca, Derek Lim, Yoav Gelberg, Yftah Ziser,
  Ran El-Yaniv, Gal Chechik, Haggai Maron",Learning on LLM Output Signatures for gray-box LLM Behavior Analysis,,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have achieved widespread adoption, yet our
understanding of their behavior remains limited, particularly in detecting data
contamination and hallucinations. While recently proposed probing techniques
provide insights through activation analysis, they require ""white-box"" access
to model internals, often unavailable. Current ""gray-box"" approaches typically
analyze only the probability of the actual tokens in the sequence with simple
task-specific heuristics. Importantly, these methods overlook the rich
information contained in the full token distribution at each processing step.
To address these limitations, we propose that gray-box analysis should leverage
the complete observable output of LLMs, consisting of both the previously used
token probabilities as well as the complete token distribution sequences - a
unified data type we term LOS (LLM Output Signature). To this end, we develop a
transformer-based approach to process LOS that theoretically guarantees
approximation of existing techniques while enabling more nuanced analysis. Our
approach achieves superior performance on hallucination and data contamination
detection in gray-box settings, significantly outperforming existing baselines.
Furthermore, it demonstrates strong transfer capabilities across datasets and
LLMs, suggesting that LOS captures fundamental patterns in LLM behavior. Our
code is available at: https://github.com/BarSGuy/LLM-Output-Signatures-Network.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 09:04:37 GMT'}]",2025-03-19,"[['Bar-Shalom', 'Guy', ''], ['Frasca', 'Fabrizio', ''], ['Lim', 'Derek', ''], ['Gelberg', 'Yoav', ''], ['Ziser', 'Yftah', ''], ['El-Yaniv', 'Ran', ''], ['Chechik', 'Gal', ''], ['Maron', 'Haggai', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.14075,Zhou Yu,"Zhenwei Shao, Mingyang Wang, Zhou Yu, Wenwen Pan, Yan Yang, Tao Wei,
  Hongyuan Zhang, Ning Mao, Wei Chen, Jun Yu",Growing a Twig to Accelerate Large Vision-Language Models,"17 pages, 8 figures",,,,cs.CV cs.CL,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Large vision-language models (VLMs) have demonstrated remarkable capabilities
in open-world multimodal understanding, yet their high computational overheads
pose great challenges for practical deployment. Some recent works have proposed
methods to accelerate VLMs by pruning redundant visual tokens guided by the
attention maps of VLM's early layers. Despite the success of these token
pruning methods, they still suffer from two major shortcomings: (i)
considerable accuracy drop due to insensitive attention signals in early
layers, and (ii) limited speedup when generating long responses (e.g., 30
tokens). To address the limitations above, we present TwigVLM -- a simple and
general architecture by growing a lightweight twig upon an early layer of the
base VLM. Compared with most existing VLM acceleration methods purely based on
visual token pruning, our TwigVLM not only achieves better accuracy retention
by employing a twig-guided token pruning (TTP) strategy, but also yields higher
generation speed by utilizing a self-speculative decoding (SSD) strategy.
Taking LLaVA-1.5-7B as the base VLM, experimental results show that TwigVLM
preserves 96% of the original performance after pruning 88.9% of visual tokens
and achieves 154% speedup in generating long responses, delivering
significantly better performance in terms of both accuracy and speed over the
state-of-the-art VLM acceleration methods. Code will be made publicly
available.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 09:52:45 GMT'}]",2025-03-19,"[['Shao', 'Zhenwei', ''], ['Wang', 'Mingyang', ''], ['Yu', 'Zhou', ''], ['Pan', 'Wenwen', ''], ['Yang', 'Yan', ''], ['Wei', 'Tao', ''], ['Zhang', 'Hongyuan', ''], ['Mao', 'Ning', ''], ['Chen', 'Wei', ''], ['Yu', 'Jun', '']]","[{'text': 'Large vision-language models', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'attention maps', 'label': 'Attention mechanism'}, {'text': 'VLM', 'label': 'Large Language Model'}, {'text': 'VLM', 'label': 'Large Language Model'}, {'text': 'LLaVA-1.5-7B', 'label': 'Foundation Model'}, {'text': 'VLM', 'label': 'Large Language Model'}]",Large Language Model,Large vision-language models,0.7742220759391785
2503.14103,Jonas Oppenlaender,Jonas Oppenlaender,"DangerMaps: Personalized Safety Advice for Travel in Urban Environments
  using a Retrieval-Augmented Language Model","17 pages, 7 figures, 1 table",,,,cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Planning a trip into a potentially unsafe area is a difficult task. We
conducted a formative study on travelers' information needs, finding that most
of them turn to search engines for trip planning. Search engines, however, fail
to provide easily interpretable results adapted to the context and personal
information needs of a traveler. Large language models (LLMs) create new
possibilities for providing personalized travel safety advice. To explore this
idea, we developed DangerMaps, a mapping system that assists its users in
researching the safety of an urban travel destination, whether it is pre-travel
or on-location. DangerMaps plots safety ratings onto a map and provides
explanations on demand. This late breaking work specifically emphasizes the
challenges of designing real-world applications with large language models. We
provide a detailed description of our approach to prompt design and highlight
future areas of research.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 10:18:07 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 10:20:29 GMT'}]",2025-03-21,"[['Oppenlaender', 'Jonas', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'prompt design', 'label': 'Prompting'}]",Large Language Model,Large language models,0.9664971828460693
2503.14140,Zining Wang,"Zining Wang, Tongkun Guan, Pei Fu, Chen Duan, Qianyi Jiang, Zhentao
  Guo, Shan Guo, Junfeng Luo, Wei Shen, Xiaokang Yang","Marten: Visual Question Answering with Mask Generation for Multi-modal
  Document Understanding",Accepted by CVPR2025,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Multi-modal Large Language Models (MLLMs) have introduced a novel dimension
to document understanding, i.e., they endow large language models with visual
comprehension capabilities; however, how to design a suitable image-text
pre-training task for bridging the visual and language modality in
document-level MLLMs remains underexplored. In this study, we introduce a novel
visual-language alignment method that casts the key issue as a Visual Question
Answering with Mask generation (VQAMask) task, optimizing two tasks
simultaneously: VQA-based text parsing and mask generation. The former allows
the model to implicitly align images and text at the semantic level. The latter
introduces an additional mask generator (discarded during inference) to
explicitly ensure alignment between visual texts within images and their
corresponding image regions at a spatially-aware level. Together, they can
prevent model hallucinations when parsing visual text and effectively promote
spatially-aware feature representation learning. To support the proposed
VQAMask task, we construct a comprehensive image-mask generation pipeline and
provide a large-scale dataset with 6M data (MTMask6M). Subsequently, we
demonstrate that introducing the proposed mask generation task yields
competitive document-level understanding performance. Leveraging the proposed
VQAMask, we introduce Marten, a training-efficient MLLM tailored for
document-level understanding. Extensive experiments show that our Marten
consistently achieves significant improvements among 8B-MLLMs in
document-centric tasks. Code and datasets are available at
https://github.com/PriNing/Marten.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:07:14 GMT'}]",2025-03-19,"[['Wang', 'Zining', ''], ['Guan', 'Tongkun', ''], ['Fu', 'Pei', ''], ['Duan', 'Chen', ''], ['Jiang', 'Qianyi', ''], ['Guo', 'Zhentao', ''], ['Guo', 'Shan', ''], ['Luo', 'Junfeng', ''], ['Shen', 'Wei', ''], ['Yang', 'Xiaokang', '']]","[{'text': 'Multi-modal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'spatially-aware feature representation learning', 'label': 'Few-shot Learning'}, {'text': 'VQAMask', 'label': 'Few-shot Learning'}]",Large Language Model,Multi-modal Large Language Models,0.7925285696983337
2503.14153,Changran Xu,"Changran Xu, Yi Liu, Yunhao Zhou, Shan Huang, Ningyi Xu, Qiang Xu","Speculative Decoding for Verilog: Speed and Quality, All in One",Accepted by the 62nd Design Automation Conference (DAC 2025),,,,cs.LG cs.AR cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The rapid advancement of large language models (LLMs) has revolutionized code
generation tasks across various programming languages. However, the unique
characteristics of programming languages, particularly those like Verilog with
specific syntax and lower representation in training datasets, pose significant
challenges for conventional tokenization and decoding approaches. In this
paper, we introduce a novel application of speculative decoding for Verilog
code generation, showing that it can improve both inference speed and output
quality, effectively achieving speed and quality all in one. Unlike standard
LLM tokenization schemes, which often fragment meaningful code structures, our
approach aligns decoding stops with syntactically significant tokens, making it
easier for models to learn the token distribution. This refinement addresses
inherent tokenization issues and enhances the model's ability to capture
Verilog's logical constructs more effectively. Our experimental results show
that our method achieves up to a 5.05x speedup in Verilog code generation and
increases pass@10 functional accuracy on RTLLM by up to 17.19% compared to
conventional training strategies. These findings highlight speculative decoding
as a promising approach to bridge the quality gap in code generation for
specialized programming languages.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:21:53 GMT'}]",2025-03-19,"[['Xu', 'Changran', ''], ['Liu', 'Yi', ''], ['Zhou', 'Yunhao', ''], ['Huang', 'Shan', ''], ['Xu', 'Ningyi', ''], ['Xu', 'Qiang', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2503.14189,Yongqi Li,"Yongqi Li, Lu Yang, Jian Wang, Runyang You, Wenjie Li, Liqiang Nie","Towards Harmless Multimodal Assistants with Blind Preference
  Optimization",,,,,cs.CL cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities in multimodal understanding, reasoning, and interaction. Given the
extensive applications of MLLMs, the associated safety issues have become
increasingly critical. Due to the effectiveness of preference optimization in
aligning MLLMs with human preferences, there is an urgent need for
safety-related preference data for MLLMs. To address this, we construct the
MMSafe-PO preference dataset towards harmless multimodal assistants, featuring
multimodal instructions, the conversational format, and ranked paired responses
from human feedback. We also identify two insightful observations: modality
co-defense and modality cheating, which illustrate that MLLMs possess a certain
level of inherent defense while still presenting unique safety challenges.
Based on these observations, we propose the Blind Preference Optimization (BPO)
approach. Comprehensive experiments on three benchmarks show that BPO
effectively enhances the safety capabilities of MLLMs. Notably, BPO
significantly improves the safety rate of the base MLLM by 45.0%, outperforming
the DPO approach. Additionally, applying BPO to the MMSafe-PO dataset greatly
reduces the base MLLM's unsafe rate on other safety benchmarks (14.5% on
MM-SafetyBench and 82.9% on HarmEval, demonstrating the effectiveness and
robustness of both the dataset and the approach. We release code and data at
https://lu-yang666.github.io/MMsafe-PO-Web/.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 12:02:38 GMT'}]",2025-03-19,"[['Li', 'Yongqi', ''], ['Yang', 'Lu', ''], ['Wang', 'Jian', ''], ['You', 'Runyang', ''], ['Li', 'Wenjie', ''], ['Nie', 'Liqiang', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLM', 'label': 'Large Language Model'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2503.14217,Tennison Liu,"Tennison Liu, Nicolas Huynh, Mihaela van der Schaar",Decision Tree Induction Through LLMs via Semantically-Aware Evolution,"*Liu and Huynh contributed equally. Published as a conference paper
  at ICLR 2025",,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Decision trees are a crucial class of models offering robust predictive
performance and inherent interpretability across various domains, including
healthcare, finance, and logistics. However, current tree induction methods
often face limitations such as suboptimal solutions from greedy methods or
prohibitive computational costs and limited applicability of exact optimization
approaches. To address these challenges, we propose an evolutionary
optimization method for decision tree induction based on genetic programming
(GP). Our key innovation is the integration of semantic priors and
domain-specific knowledge about the search space into the optimization
algorithm. To this end, we introduce $\texttt{LLEGO}$, a framework that
incorporates semantic priors into genetic search operators through the use of
Large Language Models (LLMs), thereby enhancing search efficiency and targeting
regions of the search space that yield decision trees with superior
generalization performance. This is operationalized through novel genetic
operators that work with structured natural language prompts, effectively
utilizing LLMs as conditional generative models and sources of semantic
knowledge. Specifically, we introduce $\textit{fitness-guided}$ crossover to
exploit high-performing regions, and $\textit{diversity-guided}$ mutation for
efficient global exploration of the search space. These operators are
controlled by corresponding hyperparameters that enable a more nuanced balance
between exploration and exploitation across the search space. Empirically, we
demonstrate across various benchmarks that $\texttt{LLEGO}$ evolves
superior-performing trees compared to existing tree induction methods, and
exhibits significantly more efficient search performance compared to
conventional GP approaches.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 12:52:03 GMT'}]",2025-03-19,"[['Liu', 'Tennison', ''], ['Huynh', 'Nicolas', ''], ['van der Schaar', 'Mihaela', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'structured natural language prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'LLM'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.14232,Yuyang Xue,"Yuyang Xue, Edward Moroshko, Feng Chen, Steven McDonagh, Sotirios A.
  Tsaftaris","CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion
  Models",,,,,cs.CV cs.AI cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Text-to-Image diffusion models can produce undesirable content that
necessitates concept erasure techniques. However, existing methods struggle
with under-erasure, leaving residual traces of targeted concepts, or
over-erasure, mistakenly eliminating unrelated but visually similar concepts.
To address these limitations, we introduce CRCE, a novel concept erasure
framework that leverages Large Language Models to identify both semantically
related concepts that should be erased alongside the target and distinct
concepts that should be preserved. By explicitly modeling coreferential and
retained concepts semantically, CRCE enables more precise concept removal,
without unintended erasure. Experiments demonstrate that CRCE outperforms
existing methods on diverse erasure tasks.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 13:09:01 GMT'}]",2025-03-19,"[['Xue', 'Yuyang', ''], ['Moroshko', 'Edward', ''], ['Chen', 'Feng', ''], ['McDonagh', 'Steven', ''], ['Tsaftaris', 'Sotirios A.', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.14234,Ruiyi Yang,"Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D. Salim","KG-IRAG: A Knowledge Graph-Based Iterative Retrieval-Augmented
  Generation Framework for Temporal Reasoning","14 pages, 4 figures",,,,cs.AI cs.MA,http://creativecommons.org/licenses/by/4.0/,"  Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective
in enhancing the performance of Large Language Models (LLMs) on tasks that
require external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG
improves information retrieval for complex reasoning tasks, providing more
precise and comprehensive retrieval and generating more accurate responses to
QAs. However, most RAG methods fall short in addressing multi-step reasoning,
particularly when both information extraction and inference are necessary. To
address this limitation, this paper presents Knowledge Graph-Based Iterative
Retrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs
with iterative reasoning to improve LLMs' ability to handle queries involving
temporal and logical dependencies. Through iterative retrieval steps, KG-IRAG
incrementally gathers relevant data from external KGs, enabling step-by-step
reasoning. The proposed approach is particularly suited for scenarios where
reasoning is required alongside dynamic temporal data extraction, such as
determining optimal travel times based on weather conditions or traffic
patterns. Experimental results show that KG-IRAG improves accuracy in complex
reasoning tasks by effectively integrating external knowledge with iterative,
logic-based retrieval. Additionally, three new datasets: weatherQA-Irish,
weatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's
performance, demonstrating its potential beyond traditional RAG applications.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 13:11:43 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 04:49:29 GMT'}]",2025-03-20,"[['Yang', 'Ruiyi', ''], ['Xue', 'Hao', ''], ['Razzak', 'Imran', ''], ['Hacid', 'Hakim', ''], ['Salim', 'Flora D.', '']]","[{'text': 'GraphRAG', 'label': 'RAG'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Knowledge Graphs', 'label': 'LLMs'}, {'text': 'GraphRAG', 'label': 'RAG'}, {'text': 'KGs', 'label': 'LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'KG-IRAG', 'label': 'RAG'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.14257,Guang Dai,"Guang Dai, Pinhao Wang, Cheng Yao, Fangtian Ying",InnerSelf: Designing Self-Deepfaked Voice for Emotional Well-being,,,,,cs.HC,http://creativecommons.org/licenses/by/4.0/,"  One's own voice is one of the most frequently heard voices. Studies found
that hearing and talking to oneself have positive psychological effects.
However, the design and implementation of self-voice for emotional regulation
in HCI have yet to be explored. In this paper, we introduce InnerSelf, an
innovative voice system based on speech synthesis technologies and the Large
Language Model. It allows users to engage in supportive and empathic dialogue
with their deepfake voice. By manipulating positive self-talk, our system aims
to promote self-disclosure and regulation, reshaping negative thoughts and
improving emotional well-being.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 13:45:22 GMT'}]",2025-03-19,"[['Dai', 'Guang', ''], ['Wang', 'Pinhao', ''], ['Yao', 'Cheng', ''], ['Ying', 'Fangtian', '']]","[{'text': 'Large\nLanguage Model', 'label': 'Large Language Model'}]",Large Language Model,"Large
Language Model",1.0
2503.14269,Ojasv Kamal,"Vaibhav Aggarwal, Ojasv Kamal, Abhinav Japesh, Zhijing Jin, Bernhard
  Sch\""olkopf","DARS: Dynamic Action Re-Sampling to Enhance Coding Agent Performance by
  Adaptive Tree Traversal",,,,,cs.CL cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have revolutionized various domains, including
natural language processing, data analysis, and software development, by
enabling automation. In software engineering, LLM-powered coding agents have
garnered significant attention due to their potential to automate complex
development tasks, assist in debugging, and enhance productivity. However,
existing approaches often struggle with sub-optimal decision-making, requiring
either extensive manual intervention or inefficient compute scaling strategies.
To improve coding agent performance, we present Dynamic Action Re-Sampling
(DARS), a novel inference time compute scaling approach for coding agents, that
is faster and more effective at recovering from sub-optimal decisions compared
to baselines. While traditional agents either follow linear trajectories or
rely on random sampling for scaling compute, our approach DARS works by
branching out a trajectory at certain key decision points by taking an
alternative action given the history of the trajectory and execution feedback
of the previous attempt from that point. We evaluate our approach on SWE-Bench
Lite benchmark, demonstrating that this scaling strategy achieves a pass@k
score of 55% with Claude 3.5 Sonnet V2. Our framework achieves a pass@1 rate of
47%, outperforming state-of-the-art (SOTA) open-source frameworks.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 14:02:59 GMT'}]",2025-03-19,"[['Aggarwal', 'Vaibhav', ''], ['Kamal', 'Ojasv', ''], ['Japesh', 'Abhinav', ''], ['Jin', 'Zhijing', ''], ['Schölkopf', 'Bernhard', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLM-powered coding agents', 'label': 'LLM-powered'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.14324,Wei Song,"Wei Song, Yuran Wang, Zijia Song, Yadong Li, Haoze Sun, Weipeng Chen,
  Zenan Zhou, Jianhua Xu, Jiaqi Wang, Kaicheng Yu","DualToken: Towards Unifying Visual Understanding and Generation with
  Dual Visual Vocabularies",,,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The differing representation spaces required for visual understanding and
generation pose a challenge in unifying them within the autoregressive paradigm
of large language models. A vision tokenizer trained for reconstruction excels
at capturing low-level perceptual details, making it well-suited for visual
generation but lacking high-level semantic representations for understanding
tasks. Conversely, a vision encoder trained via contrastive learning aligns
well with language but struggles to decode back into the pixel space for
generation tasks. To bridge this gap, we propose DualToken, a method that
unifies representations for both understanding and generation within a single
tokenizer. However, directly integrating reconstruction and semantic objectives
in a single tokenizer creates conflicts, leading to degraded performance in
both reconstruction quality and semantic performance. Instead of forcing a
single codebook to handle both semantic and perceptual information, DualToken
disentangles them by introducing separate codebooks for high and low-level
features, effectively transforming their inherent conflict into a synergistic
relationship. As a result, DualToken achieves state-of-the-art performance in
both reconstruction and semantic tasks while demonstrating remarkable
effectiveness in downstream MLLM understanding and generation tasks. Notably,
we also show that DualToken, as a unified tokenizer, surpasses the naive
combination of two distinct types vision encoders, providing superior
performance within a unified MLLM.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 14:56:46 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 12:58:33 GMT'}]",2025-03-20,"[['Song', 'Wei', ''], ['Wang', 'Yuran', ''], ['Song', 'Zijia', ''], ['Li', 'Yadong', ''], ['Sun', 'Haoze', ''], ['Chen', 'Weipeng', ''], ['Zhou', 'Zenan', ''], ['Xu', 'Jianhua', ''], ['Wang', 'Jiaqi', ''], ['Yu', 'Kaicheng', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'contrastive learning', 'label': 'Few-shot Learning'}]",Large Language Model,large language models,0.9664971828460693
2503.14340,Yisen Xu,"Yisen Xu, Feng Lin, Jinqiu Yang, Tse-Hsun (Peter) Chen, Nikolaos
  Tsantalis","MANTRA: Enhancing Automated Method-Level Refactoring with Contextual RAG
  and Multi-Agent LLM Collaboration",10 pages,,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Maintaining and scaling software systems relies heavily on effective code
refactoring, yet this process remains labor-intensive, requiring developers to
carefully analyze existing codebases and prevent the introduction of new
defects. Although recent advancements have leveraged Large Language Models
(LLMs) to automate refactoring tasks, current solutions are constrained in
scope and lack mechanisms to guarantee code compilability and successful test
execution. In this work, we introduce MANTRA, a comprehensive LLM agent-based
framework that automates method-level refactoring. MANTRA integrates
Context-Aware Retrieval-Augmented Generation, coordinated Multi-Agent
Collaboration, and Verbal Reinforcement Learning to emulate human
decision-making during refactoring while preserving code correctness and
readability. Our empirical study, conducted on 703 instances of ""pure
refactorings"" (i.e., code changes exclusively involving structural
improvements), drawn from 10 representative Java projects, covers the six most
prevalent refactoring operations. Experimental results demonstrate that MANTRA
substantially surpasses a baseline LLM model (RawGPT ), achieving an 82.8%
success rate (582/703) in producing code that compiles and passes all tests,
compared to just 8.7% (61/703) with RawGPT. Moreover, in comparison to
IntelliJ's LLM-powered refactoring tool (EM-Assist), MANTRA exhibits a 50%
improvement in generating Extract Method transformations. A usability study
involving 37 professional developers further shows that refactorings performed
by MANTRA are perceived to be as readable and reusable as human-written code,
and in certain cases, even more favorable. These results highlight the
practical advantages of MANTRA and emphasize the growing potential of LLM-based
systems in advancing the automation of software refactoring tasks.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 15:16:51 GMT'}]",2025-03-19,"[['Xu', 'Yisen', '', 'Peter'], ['Lin', 'Feng', '', 'Peter'], ['Yang', 'Jinqiu', '', 'Peter'], ['Tse-Hsun', '', '', 'Peter'], ['Chen', '', ''], ['Tsantalis', 'Nikolaos', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Context-Aware Retrieval-Augmented Generation', 'label': 'Few-shot Learning'}, {'text': 'Verbal Reinforcement Learning', 'label': 'Few-shot Learning'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.14391,Deniz Yuret,Shadi Hamdan and Deniz Yuret,How much do LLMs learn from negative examples?,"8 pages, 6 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) undergo a three-phase training process:
unsupervised pre-training, supervised fine-tuning (SFT), and learning from
human feedback (RLHF/DPO). Notably, it is during the final phase that these
models are exposed to negative examples -- incorrect, rejected, or suboptimal
responses to queries. This paper delves into the role of negative examples in
the training of LLMs, using a likelihood-ratio (Likra) model on multiple-choice
question answering benchmarks to precisely manage the influence and the volume
of negative examples. Our findings reveal three key insights: (1) During a
critical phase in training, Likra with negative examples demonstrates a
significantly larger improvement per training example compared to SFT using
only positive examples. This leads to a sharp jump in the learning curve for
Likra unlike the smooth and gradual improvement of SFT; (2) negative examples
that are plausible but incorrect (near-misses) exert a greater influence; and
(3) while training with positive examples fails to significantly decrease the
likelihood of plausible but incorrect answers, training with negative examples
more accurately identifies them. These results indicate a potentially
significant role for negative examples in improving accuracy and reducing
hallucinations for LLMs.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:26:29 GMT'}]",2025-03-19,"[['Hamdan', 'Shadi', ''], ['Yuret', 'Deniz', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'SFT', 'label': 'Fine-tuning'}, {'text': 'SFT', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2503.14392,Qiantong Wang,Qiantong Wang,"From ""Hallucination"" to ""Suture"": Insights from Language Philosophy to
  Enhance Large Language Models",7 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper explores hallucination phenomena in large language models (LLMs)
through the lens of language philosophy and psychoanalysis. By incorporating
Lacan's concepts of the ""chain of signifiers"" and ""suture points,"" we propose
the Anchor-RAG framework as a novel approach to mitigate hallucinations. In
contrast to the predominant reliance on trial-and-error experiments, constant
adjustments of mathematical formulas, or resource-intensive methods that
emphasize quantity over quality, our approach returns to the fundamental
principles of linguistics to analyze the root causes of hallucinations in LLMs.
Drawing from robust theoretical foundations, we derive algorithms and models
that are not only effective in reducing hallucinations but also enhance LLM
performance and improve output quality. This paper seeks to establish a
comprehensive theoretical framework for understanding hallucinations in LLMs
and aims to challenge the prevalent ""guess-and-test"" approach and rat race
mentality in the field. We aspire to pave the way for a new era of
interpretable LLMs, offering deeper insights into the inner workings of
language-based AI systems.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:27:01 GMT'}]",2025-03-19,"[['Wang', 'Qiantong', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'chain of signifiers', 'label': 'Chain of thought'}, {'text': 'Anchor-RAG framework', 'label': 'RAG'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2503.14408,Parisa Ghanad Torshizi,"Parisa Ghanad Torshizi, Laura B. Hensel, Ari Shapiro, Stacy C.
  Marsella",Large Language Models for Virtual Human Gesture Selection,"9 pages, 6 figures, Accepted at the AAMAS 2025 conference",,,,cs.HC cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Co-speech gestures convey a wide variety of meanings and play an important
role in face-to-face human interactions. These gestures significantly influence
the addressee's engagement, recall, comprehension, and attitudes toward the
speaker. Similarly, they impact interactions between humans and embodied
virtual agents. The process of selecting and animating meaningful gestures has
thus become a key focus in the design of these agents. However, automating this
gesture selection process poses a significant challenge. Prior gesture
generation techniques have varied from fully automated, data-driven methods,
which often struggle to produce contextually meaningful gestures, to more
manual approaches that require crafting specific gesture expertise and are
time-consuming and lack generalizability. In this paper, we leverage the
semantic capabilities of Large Language Models to develop a gesture selection
approach that suggests meaningful, appropriate co-speech gestures. We first
describe how information on gestures is encoded into GPT-4. Then, we conduct a
study to evaluate alternative prompting approaches for their ability to select
meaningful, contextually relevant gestures and to align them appropriately with
the co-speech utterance. Finally, we detail and demonstrate how this approach
has been implemented within a virtual agent system, automating the selection
and subsequent animation of the selected gestures for enhanced human-agent
interactions.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:49:56 GMT'}]",2025-03-19,"[['Torshizi', 'Parisa Ghanad', ''], ['Hensel', 'Laura B.', ''], ['Shapiro', 'Ari', ''], ['Marsella', 'Stacy C.', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'alternative prompting approaches', 'label': 'Prompting'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.14411,Siwei Zhang,"Siwei Zhang, Yun Xiong, Yateng Tang, Xi Chen, Zian Jia, Zehao Gu,
  Jiarong Xu, Jiawei Zhang","Unifying Text Semantics and Graph Structures for Temporal
  Text-attributed Graphs with Large Language Models",Submit to ICML2025,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Temporal graph neural networks (TGNNs) have shown remarkable performance in
temporal graph modeling. However, real-world temporal graphs often possess rich
textual information, giving rise to temporal text-attributed graphs (TTAGs).
Such combination of dynamic text semantics and evolving graph structures
introduces heightened complexity. Existing TGNNs embed texts statically and
rely heavily on encoding mechanisms that biasedly prioritize structural
information, overlooking the temporal evolution of text semantics and the
essential interplay between semantics and structures for synergistic
reinforcement. To tackle these issues, we present \textbf{{Cross}}, a novel
framework that seamlessly extends existing TGNNs for TTAG modeling. The key
idea is to employ the advanced large language models (LLMs) to extract the
dynamic semantics in text space and then generate expressive representations
unifying both semantics and structures. Specifically, we propose a Temporal
Semantics Extractor in the {Cross} framework, which empowers the LLM to offer
the temporal semantic understanding of node's evolving contexts of textual
neighborhoods, facilitating semantic dynamics. Subsequently, we introduce the
Semantic-structural Co-encoder, which collaborates with the above Extractor for
synthesizing illuminating representations by jointly considering both semantic
and structural information while encouraging their mutual reinforcement.
Extensive experimental results on four public datasets and one practical
industrial dataset demonstrate {Cross}'s significant effectiveness and
robustness.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:50:10 GMT'}]",2025-03-19,"[['Zhang', 'Siwei', ''], ['Xiong', 'Yun', ''], ['Tang', 'Yateng', ''], ['Chen', 'Xi', ''], ['Jia', 'Zian', ''], ['Gu', 'Zehao', ''], ['Xu', 'Jiarong', ''], ['Zhang', 'Jiawei', '']]","[{'text': 'Temporal graph neural networks', 'label': 'Neural Language Model'}, {'text': 'TGNNs', 'label': 'Neural Language Model'}, {'text': 'TGNNs', 'label': 'Neural Language Model'}, {'text': 'advanced large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,advanced large language models,0.920845627784729
2503.14432,Wei Fang,"Wei Fang, Yang Zhang, Kaizhi Qian, James Glass, Yada Zhu","PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via
  Tool Play",,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) are increasingly integrated with specialized
external tools, yet many tasks demand zero-shot tool usage with minimal or
noisy documentation. Existing solutions rely on manual rewriting or labeled
data for validation, making them inapplicable in true zero-shot settings. To
address these challenges, we propose PLAY2PROMPT, an automated framework that
systematically ""plays"" with each tool to explore its input-output behaviors.
Through this iterative trial-and-error process, PLAY2PROMPT refines tool
documentation and generates usage examples without any labeled data. These
examples not only guide LLM inference but also serve as validation to further
enhance tool utilization. Extensive experiments on real-world tasks demonstrate
that PLAY2PROMPT significantly improves zero-shot tool performance across both
open and closed models, offering a scalable and effective solution for
domain-specific tool integration.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:09:57 GMT'}]",2025-03-19,"[['Fang', 'Wei', ''], ['Zhang', 'Yang', ''], ['Qian', 'Kaizhi', ''], ['Glass', 'James', ''], ['Zhu', 'Yada', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2503.14434,Nikhil Abhyankar,"Nikhil Abhyankar, Parshin Shojaee, Chandan K. Reddy","LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as
  Evolutionary Optimizers",,,,,cs.LG cs.AI cs.CL cs.NE,http://creativecommons.org/licenses/by/4.0/,"  Automated feature engineering plays a critical role in improving predictive
model performance for tabular learning tasks. Traditional automated feature
engineering methods are limited by their reliance on pre-defined
transformations within fixed, manually designed search spaces, often neglecting
domain knowledge. Recent advances using Large Language Models (LLMs) have
enabled the integration of domain knowledge into the feature engineering
process. However, existing LLM-based approaches use direct prompting or rely
solely on validation scores for feature selection, failing to leverage insights
from prior feature discovery experiments or establish meaningful reasoning
between feature generation and data-driven performance. To address these
challenges, we propose LLM-FE, a novel framework that combines evolutionary
search with the domain knowledge and reasoning capabilities of LLMs to
automatically discover effective features for tabular learning tasks. LLM-FE
formulates feature engineering as a program search problem, where LLMs propose
new feature transformation programs iteratively, and data-driven feedback
guides the search process. Our results demonstrate that LLM-FE consistently
outperforms state-of-the-art baselines, significantly enhancing the performance
of tabular prediction models across diverse classification and regression
benchmarks.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:11:24 GMT'}]",2025-03-19,"[['Abhyankar', 'Nikhil', ''], ['Shojaee', 'Parshin', ''], ['Reddy', 'Chandan K.', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'existing LLM-based approaches', 'label': 'LLM-based'}, {'text': 'direct prompting', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'tabular learning tasks', 'label': 'Few-shot Learning'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.14478,Xinyu Fang,"Xinyu Fang, Zhijian Chen, Kai Lan, Lixin Ma, Shengyuan Ding, Yingji
  Liang, Xiangyu Zhao, Farong Wen, Zicheng Zhang, Guofeng Zhang, Haodong Duan,
  Kai Chen, Dahua Lin",Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM,"Evaluation Code and dataset see
  https://github.com/open-compass/Creation-MMBench",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Creativity is a fundamental aspect of intelligence, involving the ability to
generate novel and appropriate solutions across diverse contexts. While Large
Language Models (LLMs) have been extensively evaluated for their creative
capabilities, the assessment of Multimodal Large Language Models (MLLMs) in
this domain remains largely unexplored. To address this gap, we introduce
Creation-MMBench, a multimodal benchmark specifically designed to evaluate the
creative capabilities of MLLMs in real-world, image-based tasks. The benchmark
comprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous
evaluation, we define instance-specific evaluation criteria for each test case,
guiding the assessment of both general response quality and factual consistency
with visual inputs. Experimental results reveal that current open-source MLLMs
significantly underperform compared to proprietary models in creative tasks.
Furthermore, our analysis demonstrates that visual fine-tuning can negatively
impact the base LLM's creative abilities. Creation-MMBench provides valuable
insights for advancing MLLM creativity and establishes a foundation for future
improvements in multimodal generative intelligence. Full data and evaluation
code is released on https://github.com/open-compass/Creation-MMBench.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:51:34 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 17:03:25 GMT'}]",2025-03-20,"[['Fang', 'Xinyu', ''], ['Chen', 'Zhijian', ''], ['Lan', 'Kai', ''], ['Ma', 'Lixin', ''], ['Ding', 'Shengyuan', ''], ['Liang', 'Yingji', ''], ['Zhao', 'Xiangyu', ''], ['Wen', 'Farong', ''], ['Zhang', 'Zicheng', ''], ['Zhang', 'Guofeng', ''], ['Duan', 'Haodong', ''], ['Chen', 'Kai', ''], ['Lin', 'Dahua', '']]","[{'text': 'Large\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'visual fine-tuning', 'label': 'Fine-tuning'}]",Large Language Model,"Large
Language Models",0.9664971828460693
2503.14504,Yi-Fan Zhang,"Tao Yu, Yi-Fan Zhang, Chaoyou Fu, Junkang Wu, Jinda Lu, Kun Wang,
  Xingyu Lu, Yunhang Shen, Guibin Zhang, Dingjie Song, Yibo Yan, Tianlong Xu,
  Qingsong Wen, Zhang Zhang, Yan Huang, Liang Wang, and Tieniu Tan",Aligning Multimodal LLM with Human Preference: A Survey,https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) can handle a wide variety of general tasks with
simple prompts, without the need for task-specific training. Multimodal Large
Language Models (MLLMs), built upon LLMs, have demonstrated impressive
potential in tackling complex tasks involving visual, auditory, and textual
data. However, critical issues related to truthfulness, safety, o1-like
reasoning, and alignment with human preference remain insufficiently addressed.
This gap has spurred the emergence of various alignment algorithms, each
targeting different application scenarios and optimization goals. Recent
studies have shown that alignment algorithms are a powerful approach to
resolving the aforementioned challenges. In this paper, we aim to provide a
comprehensive and systematic review of alignment algorithms for MLLMs.
Specifically, we explore four key aspects: (1) the application scenarios
covered by alignment algorithms, including general image understanding,
multi-image, video, and audio, and extended multimodal applications; (2) the
core factors in constructing alignment datasets, including data sources, model
responses, and preference annotations; (3) the benchmarks used to evaluate
alignment algorithms; and (4) a discussion of potential future directions for
the development of alignment algorithms. This work seeks to help researchers
organize current advancements in the field and inspire better alignment
methods. The project page of this paper is available at
https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:59:56 GMT'}]",2025-03-19,"[['Yu', 'Tao', ''], ['Zhang', 'Yi-Fan', ''], ['Fu', 'Chaoyou', ''], ['Wu', 'Junkang', ''], ['Lu', 'Jinda', ''], ['Wang', 'Kun', ''], ['Lu', 'Xingyu', ''], ['Shen', 'Yunhang', ''], ['Zhang', 'Guibin', ''], ['Song', 'Dingjie', ''], ['Yan', 'Yibo', ''], ['Xu', 'Tianlong', ''], ['Wen', 'Qingsong', ''], ['Zhang', 'Zhang', ''], ['Huang', 'Yan', ''], ['Wang', 'Liang', ''], ['Tan', 'Tieniu', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'simple prompts', 'label': 'Prompting'}, {'text': 'Multimodal Large\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'Awesome-Multimodal-Large-Language-Models', 'label': 'Open-source LLMs'}]",Large Language Model,Large language models,0.9664971828460693
2503.14530,Jiahui Geng,"Qing Li, Jiahui Geng, Derui Zhu, Fengyu Cai, Chenyang Lyu, Fakhri
  Karray","SAUCE: Selective Concept Unlearning in Vision-Language Models with
  Sparse Autoencoders",More comparative experiments are needed,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by-sa/4.0/,"  Unlearning methods for vision-language models (VLMs) have primarily adapted
techniques from large language models (LLMs), relying on weight updates that
demand extensive annotated forget sets. Moreover, these methods perform
unlearning at a coarse granularity, often leading to excessive forgetting and
reduced model utility. To address this issue, we introduce SAUCE, a novel
method that leverages sparse autoencoders (SAEs) for fine-grained and selective
concept unlearning in VLMs. Briefly, SAUCE first trains SAEs to capture
high-dimensional, semantically rich sparse features. It then identifies the
features most relevant to the target concept for unlearning. During inference,
it selectively modifies these features to suppress specific concepts while
preserving unrelated information. We evaluate SAUCE on two distinct VLMs,
LLaVA-v1.5-7B and LLaMA-3.2-11B-Vision-Instruct, across two types of tasks:
concrete concept unlearning (objects and sports scenes) and abstract concept
unlearning (emotions, colors, and materials), encompassing a total of 60
concepts. Extensive experiments demonstrate that SAUCE outperforms
state-of-the-art methods by 18.04% in unlearning quality while maintaining
comparable model utility. Furthermore, we investigate SAUCE's robustness
against widely used adversarial attacks, its transferability across models, and
its scalability in handling multiple simultaneous unlearning requests. Our
findings establish SAUCE as an effective and scalable solution for selective
concept unlearning in VLMs.
","[{'version': 'v1', 'created': 'Sun, 16 Mar 2025 17:32:23 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 05:47:10 GMT'}]",2025-03-21,"[['Li', 'Qing', ''], ['Geng', 'Jiahui', ''], ['Zhu', 'Derui', ''], ['Cai', 'Fengyu', ''], ['Lyu', 'Chenyang', ''], ['Karray', 'Fakhri', '']]","[{'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'concrete concept unlearning', 'label': 'Few-shot Learning'}, {'text': 'abstract concept\nunlearning', 'label': 'Few-shot Learning'}, {'text': 'VLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2503.14604,Sara Sarto,"Sara Sarto, Marcella Cornia, Rita Cucchiara","Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges
  and Future Perspectives","Repo GitHub:
  https://github.com/aimagelab/awesome-captioning-evaluation",,,,cs.CV cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The evaluation of machine-generated image captions is a complex and evolving
challenge. With the advent of Multimodal Large Language Models (MLLMs), image
captioning has become a core task, increasing the need for robust and reliable
evaluation metrics. This survey provides a comprehensive overview of
advancements in image captioning evaluation, analyzing the evolution,
strengths, and limitations of existing metrics. We assess these metrics across
multiple dimensions, including correlation with human judgment, ranking
accuracy, and sensitivity to hallucinations. Additionally, we explore the
challenges posed by the longer and more detailed captions generated by MLLMs
and examine the adaptability of current metrics to these stylistic variations.
Our analysis highlights some limitations of standard evaluation approaches and
suggests promising directions for future research in image captioning
assessment.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 18:03:56 GMT'}]",2025-03-20,"[['Sarto', 'Sara', ''], ['Cornia', 'Marcella', ''], ['Cucchiara', 'Rita', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2503.14674,Amirul Rahman,"Liu Jing, Amirul Rahman","Elevating Visual Question Answering through Implicitly Learned Reasoning
  Pathways in LVLMs",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Vision-Language Models (LVLMs) have shown remarkable progress in
various multimodal tasks, yet they often struggle with complex visual reasoning
that requires multi-step inference. To address this limitation, we propose
MF-SQ-LLaVA, a novel approach that enhances LVLMs by enabling implicit
self-questioning through end-to-end training. Our method involves augmenting
visual question answering datasets with reasoning chains consisting of
sub-question and answer pairs, and training the LVLM with a multi-task loss
that encourages the generation and answering of these intermediate steps, as
well as the prediction of the final answer. We conduct extensive experiments on
the ScienceQA and VQAv2 datasets, demonstrating that MF-SQ-LLaVA significantly
outperforms existing state-of-the-art models, including the base LLaVA and the
original SQ-LLaVA. Ablation studies further validate the contribution of each
component of our approach, and human evaluation confirms the improved accuracy
and coherence of the reasoning process enabled by our method.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 19:29:07 GMT'}]",2025-03-20,"[['Jing', 'Liu', ''], ['Rahman', 'Amirul', '']]","[{'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'reasoning chains', 'label': 'Chain of thought'}]",Large Language Model,Large Vision-Language Models,0.7742220759391785
2503.14724,Sebastian Zhao,"Sebastian Zhao, Alan Zhu, Hussein Mozannar, David Sontag, Ameet
  Talwalkar, Valerie Chen",CodingGenie: A Proactive LLM-Powered Programming Assistant,FSE Demo 2025,,,,cs.HC,http://creativecommons.org/licenses/by/4.0/,"  While developers increasingly adopt tools powered by large language models
(LLMs) in day-to-day workflows, these tools still require explicit user
invocation. To seamlessly integrate LLM capabilities to a developer's workflow,
we introduce CodingGenie, a proactive assistant integrated into the code
editor. CodingGenie autonomously provides suggestions, ranging from bug fixing
to unit testing, based on the current code context and allows users to
customize suggestions by providing a task description and selecting what
suggestions are shown. We demonstrate multiple use cases to show how proactive
suggestions from CodingGenie can improve developer experience, and also analyze
the cost of adding proactivity. We believe this open-source tool will enable
further research into proactive assistants. CodingGenie is open-sourced at
https://github.com/sebzhao/CodingGenie/ and video demos are available at
https://sebzhao.github.io/CodingGenie/.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 20:54:40 GMT'}]",2025-03-20,"[['Zhao', 'Sebastian', ''], ['Zhu', 'Alan', ''], ['Mozannar', 'Hussein', ''], ['Sontag', 'David', ''], ['Talwalkar', 'Ameet', ''], ['Chen', 'Valerie', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'CodingGenie', 'label': 'Open-source LLMs'}, {'text': 'CodingGenie', 'label': 'Open-source LLMs'}]",Large Language Model,large language models,0.9664971828460693
2503.14749,Sophia Hager,"Sophia Hager, David Mueller, Kevin Duh, and Nicholas Andrews","Uncertainty Distillation: Teaching Language Models to Express Semantic
  Confidence",,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by-sa/4.0/,"  As large language models (LLMs) are increasingly used for factual
question-answering, it becomes more important for LLMs to have the capability
to communicate the likelihood that their answer is correct. For these
verbalized expressions of uncertainty to be meaningful, they should reflect the
error rates at the expressed level of confidence. However, when prompted to
express confidence, the error rates of current LLMs are inconsistent with their
communicated confidences, highlighting the need for uncertainty quantification
methods. Many prior methods calculate lexical uncertainty, estimating a model's
confidence in the specific string it generated. In some cases, however, it may
be more useful to estimate semantic uncertainty, or the model's confidence in
the answer regardless of how it is verbalized. We propose a simple procedure,
uncertainty distillation, to teach an LLM to verbalize calibrated semantic
confidences. Using held-out data to map initial uncertainty estimates to
meaningful probabilities, we create examples annotated with verbalized
probabilities for supervised fine-tuning. We demonstrate our method yields
verbalized confidences that correlate with observed error rates with a small
fine-tuned language model as well as with larger instruction-tuned models, and
find that our semantic uncertainty correlates well with lexical uncertainty on
short answers.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 21:29:29 GMT'}]",2025-03-20,"[['Hager', 'Sophia', ''], ['Mueller', 'David', ''], ['Duh', 'Kevin', ''], ['Andrews', 'Nicholas', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompted', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'uncertainty distillation', 'label': 'Knowledge distillation'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}]",Large Language Model,large language models,0.9664971828460693
2503.14838,Chengran Yang,"Chengran Yang, Zhensu Sun, Hong Jin Kang, Jieke Shi, David Lo","Think Like Human Developers: Harnessing Community Knowledge for
  Structured Code Reasoning",,,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have significantly advanced automated code
generation, yet they struggle with complex coding tasks requiring multi-step
logical reasoning. High-quality reasoning data is crucial for improving LLMs'
reasoning capabilities, but such datasets remain scarce. Existing approaches
either rely on computationally expensive reinforcement learning (RL) or
error-prone reasoning chains synthesized by LLMs, posing challenges in
scalability and accuracy.
  To address this challenge, we propose SVRC (Structured and Validated
Reasoning Chains for Code Generation), a novel framework that mines,
restructures, and enriches reasoning chains from community-driven discussions
on software engineering platforms. SVRC refines unstructured and incomplete
discussions of coding problems by aligning them with Software Development Life
Cycle (SDLC) principles, ensuring that reasoning chains capture real-world
problem-solving strategies and support iterative refinement.
  To evaluate the effectiveness of SVRC, we introduce CodeThinker, an LLM
fine-tuned on 12,444 reasoning-augmented samples generated by SVRC. Experiments
on LiveCodeBench show that CodeThinker surpasses its base model by 42.86\% on
medium-level code problems in terms of pass@1 and outperforms GPT-4o-mini and
GPT-4o by 73.14\% and 115.86\%, respectively. Our ablation study further
highlights that each component of SVRC contributes to the reasoning
capabilities of CodeThinker.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 02:45:13 GMT'}]",2025-03-20,"[['Yang', 'Chengran', ''], ['Sun', 'Zhensu', ''], ['Kang', 'Hong Jin', ''], ['Shi', 'Jieke', ''], ['Lo', 'David', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'error-prone reasoning chains', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'scalability', 'label': 'Scaling law'}, {'text': 'reasoning chains', 'label': 'Chain of thought'}, {'text': 'reasoning chains', 'label': 'Chain of thought'}, {'text': 'iterative refinement', 'label': 'Fine-tuning'}, {'text': 'CodeThinker', 'label': 'LLM'}, {'text': 'CodeThinker', 'label': 'LLM'}, {'text': 'GPT-4o-mini', 'label': 'GPT'}, {'text': 'GPT-4o', 'label': 'GPT-4'}, {'text': 'CodeThinker', 'label': 'LLM'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.14883,Yu Hui Kellie Sim,"Kellie Yu Hui Sim, Kenny Tsu Wei Choo",Envisioning an AI-Enhanced Mental Health Ecosystem,"5 pages, 0 figures, accepted to the CHI'25 Envisioning the Future of
  Interactive Health Workshop, to be published in HAL",,,,cs.HC cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The rapid advancement of Large Language Models (LLMs), reasoning models, and
agentic AI approaches coincides with a growing global mental health crisis,
where increasing demand has not translated into adequate access to professional
support, particularly for underserved populations. This presents a unique
opportunity for AI to complement human-led interventions, offering scalable and
context-aware support while preserving human connection in this sensitive
domain. We explore various AI applications in peer support, self-help
interventions, proactive monitoring, and data-driven insights, using a
human-centred approach that ensures AI supports rather than replaces human
interaction. However, AI deployment in mental health fields presents challenges
such as ethical concerns, transparency, privacy risks, and risks of
over-reliance. We propose a hybrid ecosystem where where AI assists but does
not replace human providers, emphasising responsible deployment and evaluation.
We also present some of our early work and findings in several of these AI
applications. Finally, we outline future research directions for refining
AI-enhanced interventions while adhering to ethical and culturally sensitive
guidelines.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 04:21:38 GMT'}]",2025-03-20,"[['Sim', 'Kellie Yu Hui', ''], ['Choo', 'Kenny Tsu Wei', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'ethical concerns', 'label': 'AI Ethics'}, {'text': 'transparency', 'label': 'AI Ethics'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.14887,Hang Li,Hang Li and Xiao Wang and Bevan Koopman and Guido Zuccon,"Pseudo-Relevance Feedback Can Improve Zero-Shot LLM-Based Dense
  Retrieval",,,,,cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pseudo-relevance feedback (PRF) refines queries by leveraging initially
retrieved documents to improve retrieval effectiveness. In this paper, we
investigate how large language models (LLMs) can facilitate PRF for zero-shot
LLM-based dense retrieval, extending the recently proposed PromptReps method.
Specifically, our approach uses LLMs to extract salient passage features-such
as keywords and summaries-from top-ranked documents, which are then integrated
into PromptReps to produce enhanced query representations. Experiments on
passage retrieval benchmarks demonstrate that incorporating PRF significantly
boosts retrieval performance. Notably, smaller rankers with PRF can match the
effectiveness of larger rankers without PRF, highlighting PRF's potential to
improve LLM-driven search while maintaining an efficient balance between
effectiveness and resource usage.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 04:30:20 GMT'}]",2025-03-20,"[['Li', 'Hang', ''], ['Wang', 'Xiao', ''], ['Koopman', 'Bevan', ''], ['Zuccon', 'Guido', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'PRF', 'label': 'Few-shot Learning'}, {'text': 'PromptReps', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'PromptReps', 'label': 'Prompting'}, {'text': 'PRF', 'label': 'Few-shot Learning'}, {'text': 'PRF', 'label': 'Few-shot Learning'}]",Large Language Model,large language models,0.9664971828460693
2503.14891,Honglin Lin,"Honglin Lin, Zhuoshi Pan, Yu Li, Qizhi Pei, Xin Gao, Mengzhang Cai,
  Conghui He, Lijun Wu","MetaLadder: Ascending Mathematical Solution Quality via
  Analogical-Problem Reasoning Transfer",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have demonstrated promising capabilities in
solving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as
a vital component in guiding answer generation. Current paradigms typically
generate CoT and answers directly for a given problem, diverging from human
problem-solving strategies to some extent. Humans often solve problems by
recalling analogous cases and leveraging their solutions to reason about the
current task. Inspired by this cognitive process, we propose
\textbf{MetaLadder}, a novel framework that explicitly prompts LLMs to recall
and reflect on meta-problems, those structurally or semantically analogous
problems, alongside their CoT solutions before addressing the target problem.
Additionally, we introduce a problem-restating mechanism to enhance the model's
comprehension of the target problem by regenerating the original question,
which further improves reasoning accuracy. Therefore, the model can achieve
reasoning transfer from analogical problems, mimicking human-like ""learning
from examples"" and generalization abilities. Extensive experiments on
mathematical benchmarks demonstrate that our MetaLadder significantly boosts
LLMs' problem-solving accuracy, largely outperforming standard CoT-based
methods (\textbf{10.3\%} accuracy gain) and other methods. Our code and data
has been released at https://github.com/LHL3341/MetaLadder.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 04:36:35 GMT'}]",2025-03-20,"[['Lin', 'Honglin', ''], ['Pan', 'Zhuoshi', ''], ['Li', 'Yu', ''], ['Pei', 'Qizhi', ''], ['Gao', 'Xin', ''], ['Cai', 'Mengzhang', ''], ['He', 'Conghui', ''], ['Wu', 'Lijun', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Chain-of-Thought (CoT)', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.14895,Shuo Li,"Shuo Li, Jiajun Sun, Guodong Zheng, Xiaoran Fan, Yujiong Shen, Yi Lu,
  Zhiheng Xi, Yuming Yang, Wenming Tan, Tao Ji, Tao Gui, Qi Zhang, Xuanjing
  Huang","Mitigating Object Hallucinations in MLLMs via Multi-Frequency
  Perturbations",,,,,cs.CV cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Recently, multimodal large language models (MLLMs) have demonstrated
remarkable performance in visual-language tasks. However, the authenticity of
the responses generated by MLLMs is often compromised by object hallucinations.
We identify that a key cause of these hallucinations is the model's
over-susceptibility to specific image frequency features in detecting objects.
In this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,
cost-effective, and pluggable method that leverages both low-frequency and
high-frequency features of images to perturb visual feature representations and
explicitly suppress redundant frequency-domain features during inference,
thereby mitigating hallucinations. Experimental results demonstrate that our
method significantly mitigates object hallucinations across various model
architectures. Furthermore, as a training-time method, MFP can be combined with
inference-time methods to achieve state-of-the-art performance on the CHAIR
benchmark.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 04:39:45 GMT'}]",2025-03-20,"[['Li', 'Shuo', ''], ['Sun', 'Jiajun', ''], ['Zheng', 'Guodong', ''], ['Fan', 'Xiaoran', ''], ['Shen', 'Yujiong', ''], ['Lu', 'Yi', ''], ['Xi', 'Zhiheng', ''], ['Yang', 'Yuming', ''], ['Tan', 'Wenming', ''], ['Ji', 'Tao', ''], ['Gui', 'Tao', ''], ['Zhang', 'Qi', ''], ['Huang', 'Xuanjing', '']]","[{'text': 'multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,multimodal large language models,0.7649828195571899
2503.14900,Estrid He,"Estrid He, Tabinda Sarwar, Ibrahim Khalil, Xun Yi, and Ke Wang",Deep Contrastive Unlearning for Language Models,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The past a few years have witnessed the great success of large language
models, demonstrating powerful capabilities in comprehending textual data and
generating human-like languages. Large language models achieve success by being
trained on vast amounts of textual data, including online sources with
copyrighted content and user-generated knowledge. However, this comes at a
cost: the potential risk of exposing users' privacy and violating copyright
protections. Thus, to safeguard individuals' ""right to be forgotten"", there has
been increasing interests in machine unlearning -- the process of removing
information carried by particular training samples from a model while not
deteriorating its predictive quality. This is a challenging task due to the
black-box nature of language models. Most existing studies focus on mitigating
the impact of those forgot samples upon a model's outputs, and do not
explicitly consider the geometric distributions of samples in the latent space
of a model. To address this issue, we propose a machine unlearning framework,
named Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models.
Our proposed model achieves machine unlearning by directly optimizing the
latent space of a model. Comprehensive experiments on real-world datasets
demonstrate the effectiveness and efficiency of DeepCUT with consistent and
significant improvement over baseline methods.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 04:58:45 GMT'}]",2025-03-20,"[['He', 'Estrid', ''], ['Sarwar', 'Tabinda', ''], ['Khalil', 'Ibrahim', ''], ['Yi', 'Xun', ''], ['Wang', 'Ke', '']]","[{'text': 'large language\nmodels', 'label': 'Large Language Model'}, {'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'machine unlearning', 'label': 'Few-shot Learning'}, {'text': 'machine unlearning', 'label': 'Few-shot Learning'}]",Large Language Model,"large language
models",0.9664971828460693
2503.14908,Haoyu Chen,"Haoyu Chen, Xiaojie Xu, Wenbo Li, Jingjing Ren, Tian Ye, Songhua Liu,
  Ying-Cong Chen, Lei Zhu, Xinchao Wang",POSTA: A Go-to Framework for Customized Artistic Poster Generation,Accepted to CVPR 2025,,,,cs.GR cs.AI cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Poster design is a critical medium for visual communication. Prior work has
explored automatic poster design using deep learning techniques, but these
approaches lack text accuracy, user customization, and aesthetic appeal,
limiting their applicability in artistic domains such as movies and
exhibitions, where both clear content delivery and visual impact are essential.
To address these limitations, we present POSTA: a modular framework powered by
diffusion models and multimodal large language models (MLLMs) for customized
artistic poster generation. The framework consists of three modules. Background
Diffusion creates a themed background based on user input. Design MLLM then
generates layout and typography elements that align with and complement the
background style. Finally, to enhance the poster's aesthetic appeal, ArtText
Diffusion applies additional stylization to key text elements. The final result
is a visually cohesive and appealing poster, with a fully modular process that
allows for complete customization. To train our models, we develop the
PosterArt dataset, comprising high-quality artistic posters annotated with
layout, typography, and pixel-level stylized text segmentation. Our
comprehensive experimental analysis demonstrates POSTA's exceptional
controllability and design diversity, outperforming existing models in both
text accuracy and aesthetic quality.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 05:22:38 GMT'}]",2025-03-20,"[['Chen', 'Haoyu', ''], ['Xu', 'Xiaojie', ''], ['Li', 'Wenbo', ''], ['Ren', 'Jingjing', ''], ['Ye', 'Tian', ''], ['Liu', 'Songhua', ''], ['Chen', 'Ying-Cong', ''], ['Zhu', 'Lei', ''], ['Wang', 'Xinchao', '']]","[{'text': 'multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,multimodal large language models,0.7649828195571899
2503.14932,Ziyao Wang,"Ziyao Wang, Yexiao He, Zheyu Shen, Yu Li, Guoheng Sun, Myungjin Lee,
  Ang Li","Prada: Black-Box LLM Adaptation with Private Data on
  Resource-Constrained Devices",,,,,cs.CR cs.DC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In recent years, Large Language Models (LLMs) have demonstrated remarkable
abilities in various natural language processing tasks. However, adapting these
models to specialized domains using private datasets stored on
resource-constrained edge devices, such as smartphones and personal computers,
remains challenging due to significant privacy concerns and limited
computational resources. Existing model adaptation methods either compromise
data privacy by requiring data transmission or jeopardize model privacy by
exposing proprietary LLM parameters. To address these challenges, we propose
Prada, a novel privacy-preserving and efficient black-box LLM adaptation system
using private on-device datasets. Prada employs a lightweight proxy model
fine-tuned with Low-Rank Adaptation (LoRA) locally on user devices. During
inference, Prada leverages the logits offset, i.e., difference in outputs
between the base and adapted proxy models, to iteratively refine outputs from a
remote black-box LLM. This offset-based adaptation approach preserves both data
privacy and model privacy, as there is no need to share sensitive data or
proprietary model parameters. Furthermore, we incorporate speculative decoding
to further speed up the inference process of Prada, making the system
practically deployable on bandwidth-constrained edge devices, enabling a more
practical deployment of Prada. Extensive experiments on various downstream
tasks demonstrate that Prada achieves performance comparable to centralized
fine-tuning methods while significantly reducing computational overhead by up
to 60% and communication costs by up to 80%.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 06:38:51 GMT'}]",2025-03-20,"[['Wang', 'Ziyao', ''], ['He', 'Yexiao', ''], ['Shen', 'Zheyu', ''], ['Li', 'Yu', ''], ['Sun', 'Guoheng', ''], ['Lee', 'Myungjin', ''], ['Li', 'Ang', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.14935,Chongjun Tu,"Chongjun Tu, Lin Zhang, Pengtao Chen, Peng Ye, Xianfang Zeng, Wei
  Cheng, Gang Yu, Tao Chen","FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion
  Understanding",FAVOR-Bench project page: https://favor-bench.github.io/,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal Large Language Models (MLLMs) have shown remarkable capabilities
in video content understanding but still struggle with fine-grained motion
comprehension. To comprehensively assess the motion understanding ability of
existing MLLMs, we introduce FAVOR-Bench, comprising 1,776 videos with
structured manual annotations of various motions. Our benchmark includes both
close-ended and open-ended tasks. For close-ended evaluation, we carefully
design 8,184 multiple-choice question-answer pairs spanning six distinct
sub-tasks. For open-ended evaluation, we develop both a novel cost-efficient
LLM-free and a GPT-assisted caption assessment method, where the former can
enhance benchmarking interpretability and reproducibility. Comprehensive
experiments with 21 state-of-the-art MLLMs reveal significant limitations in
their ability to comprehend and describe detailed temporal dynamics in video
motions. To alleviate this limitation, we further build FAVOR-Train, a dataset
consisting of 17,152 videos with fine-grained motion annotations. The results
of finetuning Qwen2.5-VL on FAVOR-Train yield consistent improvements on
motion-related tasks of TVBench, MotionBench and our FAVOR-Bench. Comprehensive
assessment results demonstrate that the proposed FAVOR-Bench and FAVOR-Train
provide valuable tools to the community for developing more powerful video
understanding models. Project page:
\href{https://favor-bench.github.io/}{https://favor-bench.github.io/}.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 06:42:32 GMT'}]",2025-03-20,"[['Tu', 'Chongjun', ''], ['Zhang', 'Lin', ''], ['Chen', 'Pengtao', ''], ['Ye', 'Peng', ''], ['Zeng', 'Xianfang', ''], ['Cheng', 'Wei', ''], ['Yu', 'Gang', ''], ['Chen', 'Tao', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2503.14939,Tengjin Weng,"Tengjin Weng, Jingyi Wang, Wenhao Jiang and Zhong Ming",VisNumBench: Evaluating Number Sense of Multimodal Large Language Models,,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Can Multimodal Large Language Models (MLLMs) develop an intuitive number
sense similar to humans? Targeting this problem, we introduce Visual Number
Benchmark (VisNumBench) to evaluate the number sense abilities of MLLMs across
a wide range of visual numerical tasks. VisNumBench consists of about 1,900
multiple-choice question-answer pairs derived from both synthetic and
real-world visual data, covering seven visual numerical attributes and four
types of visual numerical estimation tasks. Our experiments on VisNumBench led
to the following key findings: (i) The 17 MLLMs we tested, including
open-source models such as Qwen2.5-VL and InternVL2.5, as well as proprietary
models like GPT-4o and Gemini 2.0 Flash, perform significantly below human
levels in number sense-related tasks. (ii) Multimodal mathematical models and
multimodal chain-of-thought (CoT) models did not exhibit significant
improvements in number sense abilities. (iii) Stronger MLLMs with larger
parameter sizes and broader general abilities demonstrate modest gains in
number sense abilities. We believe VisNumBench will serve as a valuable
resource for the research community, encouraging further advancements in
enhancing MLLMs' number sense abilities. All benchmark resources, including
code and datasets, will be publicly available at
https://wwwtttjjj.github.io/VisNumBench/.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 07:07:43 GMT'}]",2025-03-20,"[['Weng', 'Tengjin', ''], ['Wang', 'Jingyi', ''], ['Jiang', 'Wenhao', ''], ['Ming', 'Zhong', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2503.14941,Qihui Zhang,"Qihui Zhang, Munan Ning, Zheyuan Liu, Yanbo Wang, Jiayi Ye, Yue Huang,
  Shuo Yang, Xiao Chen, Yibing Song, Li Yuan","UPME: An Unsupervised Peer Review Framework for Multimodal Large
  Language Model Evaluation",Accepted by CVPR 2025,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal Large Language Models (MLLMs) have emerged to tackle the
challenges of Visual Question Answering (VQA), sparking a new research focus on
conducting objective evaluations of these models. Existing evaluation methods
face limitations due to the significant human workload required to design Q&A
pairs for visual images, which inherently restricts the scale and scope of
evaluations. Although automated MLLM-as-judge approaches attempt to reduce the
human workload through automatic evaluations, they often introduce biases. To
address these problems, we propose an Unsupervised Peer review MLLM Evaluation
framework. It utilizes only image data, allowing models to automatically
generate questions and conduct peer review assessments of answers from other
models, effectively alleviating the reliance on human workload. Additionally,
we introduce the vision-language scoring system to mitigate the bias issues,
which focuses on three aspects: (i) response correctness; (ii) visual
understanding and reasoning; and (iii) image-text correlation. Experimental
results demonstrate that UPME achieves a Pearson correlation of 0.944 with
human evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,
indicating that our framework closely aligns with human-designed benchmarks and
inherent human preferences.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 07:15:41 GMT'}]",2025-03-20,"[['Zhang', 'Qihui', ''], ['Ning', 'Munan', ''], ['Liu', 'Zheyuan', ''], ['Wang', 'Yanbo', ''], ['Ye', 'Jiayi', ''], ['Huang', 'Yue', ''], ['Yang', 'Shuo', ''], ['Chen', 'Xiao', ''], ['Song', 'Yibing', ''], ['Yuan', 'Li', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2503.14948,Hao Liang,"Hao Liang, Zhipeng Dong, Yi Yang, Mengyin Fu","ChatStitch: Visualizing Through Structures via Surround-View
  Unsupervised Deep Image Stitching with Collaborative LLM-Agents",,,,,cs.CV cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Collaborative perception has garnered significant attention for its ability
to enhance the perception capabilities of individual vehicles through the
exchange of information with surrounding vehicle-agents. However, existing
collaborative perception systems are limited by inefficiencies in user
interaction and the challenge of multi-camera photorealistic visualization. To
address these challenges, this paper introduces ChatStitch, the first
collaborative perception system capable of unveiling obscured blind spot
information through natural language commands integrated with external digital
assets. To adeptly handle complex or abstract commands, ChatStitch employs a
multi-agent collaborative framework based on Large Language Models. For
achieving the most intuitive perception for humans, ChatStitch proposes
SV-UDIS, the first surround-view unsupervised deep image stitching method under
the non-global-overlapping condition. We conducted extensive experiments on the
UDIS-D, MCOV-SLAM open datasets, and our real-world dataset. Specifically, our
SV-UDIS method achieves state-of-the-art performance on the UDIS-D dataset for
3, 4, and 5 image stitching tasks, with PSNR improvements of 9%, 17%, and 21%,
and SSIM improvements of 8%, 18%, and 26%, respectively.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 07:25:21 GMT'}]",2025-03-20,"[['Liang', 'Hao', ''], ['Dong', 'Zhipeng', ''], ['Yang', 'Yi', ''], ['Fu', 'Mengyin', '']]","[{'text': 'ChatStitch', 'label': 'ChatGPT'}, {'text': 'ChatStitch', 'label': 'ChatGPT'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'ChatStitch', 'label': 'ChatGPT'}, {'text': 'MCOV-SLAM', 'label': 'Open-source LLMs'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.14996,Francesco Maria Molfese,"Francesco Maria Molfese, Luca Moroni, Luca Gioffr\`e, Alessandro
  Scir\`e, Simone Conia and Roberto Navigli","Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM
  Evaluation in Multiple-Choice Question Answering","17 pages (9 main), 11 figures, 21 tables",,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  One of the most widely used tasks to evaluate Large Language Models (LLMs) is
Multiple-Choice Question Answering (MCQA). While open-ended question answering
tasks are more challenging to evaluate, MCQA tasks are, in principle, easier to
assess, as the model's answer is thought to be simple to extract and is
directly compared to a set of predefined choices. However, recent studies have
started to question the reliability of MCQA evaluation, showing that multiple
factors can significantly impact the reported performance of LLMs, especially
when the model generates free-form text before selecting one of the answer
choices. In this work, we shed light on the inconsistencies of MCQA evaluation
strategies, which can lead to inaccurate and misleading model comparisons. We
systematically analyze whether existing answer extraction methods are aligned
with human judgment, and how they are influenced by answer constraints in the
prompt across different domains. Our experiments demonstrate that traditional
evaluation strategies often underestimate LLM capabilities, while LLM-based
answer extractors are prone to systematic errors. Moreover, we reveal a
fundamental trade-off between including format constraints in the prompt to
simplify answer extraction and allowing models to generate free-form text to
improve reasoning. Our findings call for standardized evaluation methodologies
and highlight the need for more reliable and consistent MCQA evaluation
practices.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 08:45:03 GMT'}]",2025-03-20,"[['Molfese', 'Francesco Maria', ''], ['Moroni', 'Luca', ''], ['Gioffrè', 'Luca', ''], ['Scirè', 'Alessandro', ''], ['Conia', 'Simone', ''], ['Navigli', 'Roberto', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'LLM-based\nanswer extractors', 'label': 'LLM-based'}, {'text': 'prompt', 'label': 'Prompting'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15003,Amr Keleg,Amr Keleg,LLM Alignment for the Arabs: A Homogenous Culture or Diverse Ones?,Accepted to the C3NLP workshop (Co-located with NAACL 2025),,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have the potential of being useful tools that
can automate tasks and assist humans. However, these models are more fluent in
English and more aligned with Western cultures, norms, and values.
Arabic-specific LLMs are being developed to better capture the nuances of the
Arabic language, as well as the views of the Arabs. Yet, Arabs are sometimes
assumed to share the same culture. In this position paper, I discuss the
limitations of this assumption and provide preliminary thoughts for how to
build systems that can better represent the cultural diversity within the Arab
world. The invalidity of the cultural homogeneity assumption might seem
obvious, yet, it is widely adopted in developing multilingual and
Arabic-specific LLMs. I hope that this paper will encourage the NLP community
to be considerate of the cultural diversity within various communities speaking
the same language.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 08:52:59 GMT'}]",2025-03-20,"[['Keleg', 'Amr', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2503.15019,Shengqiong Wu,"Shengqiong Wu and Hao Fei and Jingkang Yang and Xiangtai Li and
  Juncheng Li and Hanwang Zhang and Tat-seng Chua",Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene,CVPR 2025,,,,cs.CV,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  The latest emerged 4D Panoptic Scene Graph (4D-PSG) provides an advanced-ever
representation for comprehensively modeling the dynamic 4D visual real world.
Unfortunately, current pioneering 4D-PSG research can primarily suffer from
data scarcity issues severely, as well as the resulting out-of-vocabulary
problems; also, the pipeline nature of the benchmark generation method can lead
to suboptimal performance. To address these challenges, this paper investigates
a novel framework for 4D-PSG generation that leverages rich 2D visual scene
annotations to enhance 4D scene learning. First, we introduce a 4D Large
Language Model (4D-LLM) integrated with a 3D mask decoder for end-to-end
generation of 4D-PSG. A chained SG inference mechanism is further designed to
exploit LLMs' open-vocabulary capabilities to infer accurate and comprehensive
object and relation labels iteratively. Most importantly, we propose a 2D-to-4D
visual scene transfer learning framework, where a spatial-temporal scene
transcending strategy effectively transfers dimension-invariant features from
abundant 2D SG annotations to 4D scenes, effectively compensating for data
scarcity in 4D-PSG. Extensive experiments on the benchmark data demonstrate
that we strikingly outperform baseline models by a large margin, highlighting
the effectiveness of our method.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 09:16:08 GMT'}]",2025-03-20,"[['Wu', 'Shengqiong', ''], ['Fei', 'Hao', ''], ['Yang', 'Jingkang', ''], ['Li', 'Xiangtai', ''], ['Li', 'Juncheng', ''], ['Zhang', 'Hanwang', ''], ['Chua', 'Tat-seng', '']]","[{'text': '4D Large\nLanguage Model', 'label': 'Large Language Model'}, {'text': '4D-LLM', 'label': 'Large Language Model'}]",Large Language Model,"4D Large
Language Model",0.8122194409370422
2503.15044,Haoyi Li,"Haoyi Li, Angela Yifei Yuan, Soyeon Caren Han, Christopher Leckie","SPADE: Systematic Prompt Framework for Automated Dialogue Expansion in
  Machine-Generated Text Detection",9 pages,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The increasing capability of large language models (LLMs) to generate
synthetic content has heightened concerns about their misuse, driving the
development of Machine-Generated Text (MGT) detection models. However, these
detectors face significant challenges due to the lack of systematically
generated, high-quality datasets for training. To address this issue, we
propose five novel data augmentation frameworks for synthetic user dialogue
generation through a structured prompting approach, reducing the costs
associated with traditional data collection methods. Our proposed method yields
14 new dialogue datasets, which we benchmark against seven MGT detection
models. The results demonstrate improved generalization performance when
utilizing a mixed dataset produced by our proposed augmentation framework.
Furthermore, considering that real-world agents lack knowledge of future
opponent utterances, we simulate online dialogue detection and examine the
relationship between chat history length and detection accuracy. We also
benchmark online detection performance with limited chat history on our
frameworks. Our open-source datasets can be downloaded from
https://github.com/AngieYYF/SPADE-customer-service-dialogue.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 09:32:52 GMT'}]",2025-03-20,"[['Li', 'Haoyi', ''], ['Yuan', 'Angela Yifei', ''], ['Han', 'Soyeon Caren', ''], ['Leckie', 'Christopher', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'structured prompting approach', 'label': 'Prompting'}]",Large Language Model,large language models,0.9664971828460693
2503.15055,Arina Razmyslovich,"Arina Razmyslovich, Kseniia Murasheva, Sofia Sedlova, Julien
  Capitaine, Eugene Dmitriev",ELTEX: A Framework for Domain-Driven Synthetic Data Generation,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework
for generating high-quality synthetic training data in specialized domains.
While Large Language Models (LLMs) have shown impressive general capabilities,
their performance in specialized domains like cybersecurity remains limited by
the scarcity of domain-specific training data. ELTEX addresses this challenge
by systematically integrating explicit domain indicator extraction with dynamic
prompting to preserve critical domain knowledge throughout the generation
process. We demonstrate ELTEX's effectiveness in the context of
blockchain-related cyberattack detection, where we fine-tune Gemma-2B using
various combinations of real and ELTEX-generated data. Our results show that
the ELTEX-enhanced model achieves performance competitive with GPT-4 across
both standard classification metrics and uncertainty calibration, while
requiring significantly fewer computational resources. We release a curated
synthetic dataset of social media texts for cyberattack detection in
blockchain. Our work demonstrates that domain-driven synthetic data generation
can effectively bridge the performance gap between resource-efficient models
and larger architectures in specialized domains.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 09:46:54 GMT'}]",2025-03-20,"[['Razmyslovich', 'Arina', ''], ['Murasheva', 'Kseniia', ''], ['Sedlova', 'Sofia', ''], ['Capitaine', 'Julien', ''], ['Dmitriev', 'Eugene', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'dynamic\nprompting', 'label': 'Prompting'}, {'text': 'GPT-4', 'label': 'GPT'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15091,Yao Cheng,"Yao Cheng, Zhe Han, Fengyang Jiang, Huaizhen Wang, Fengyu Zhou,
  Qingshan Yin, Lei Wei","Intelligent Spatial Perception by Building Hierarchical 3D Scene Graphs
  for Indoor Scenarios with the Help of LLMs",accepted by WRC SARA 2024,,,,cs.RO cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper addresses the high demand in advanced intelligent robot navigation
for a more holistic understanding of spatial environments, by introducing a
novel system that harnesses the capabilities of Large Language Models (LLMs) to
construct hierarchical 3D Scene Graphs (3DSGs) for indoor scenarios. The
proposed framework constructs 3DSGs consisting of a fundamental layer with rich
metric-semantic information, an object layer featuring precise point-cloud
representation of object nodes as well as visual descriptors, and higher layers
of room, floor, and building nodes. Thanks to the innovative application of
LLMs, not only object nodes but also nodes of higher layers, e.g., room nodes,
are annotated in an intelligent and accurate manner. A polling mechanism for
room classification using LLMs is proposed to enhance the accuracy and
reliability of the room node annotation. Thorough numerical experiments
demonstrate the system's ability to integrate semantic descriptions with
geometric data, creating an accurate and comprehensive representation of the
environment instrumental for context-aware navigation and task planning.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 10:40:28 GMT'}]",2025-03-20,"[['Cheng', 'Yao', ''], ['Han', 'Zhe', ''], ['Jiang', 'Fengyang', ''], ['Wang', 'Huaizhen', ''], ['Zhou', 'Fengyu', ''], ['Yin', 'Qingshan', ''], ['Wei', 'Lei', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15108,Mohamed Salim Aissi,"Mohamed Salim Aissi, Clemence Grislain, Mohamed Chetouani, Olivier
  Sigaud, Laure Soulier, Nicolas Thome","VIPER: Visual Perception and Explainable Reasoning for Sequential
  Decision-Making",,,,,cs.LG cs.AI cs.RO,http://creativecommons.org/licenses/by/4.0/,"  While Large Language Models (LLMs) excel at reasoning on text and
Vision-Language Models (VLMs) are highly effective for visual perception,
applying those models for visual instruction-based planning remains a widely
open problem. In this paper, we introduce VIPER, a novel framework for
multimodal instruction-based planning that integrates VLM-based perception with
LLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM
generates textual descriptions of image observations, which are then processed
by an LLM policy to predict actions based on the task goal. We fine-tune the
reasoning module using behavioral cloning and reinforcement learning, improving
our agent's decision-making capabilities. Experiments on the ALFWorld benchmark
show that VIPER significantly outperforms state-of-the-art visual
instruction-based planners while narrowing the gap with purely text-based
oracles. By leveraging text as an intermediate representation, VIPER also
enhances explainability, paving the way for a fine-grained analysis of
perception and reasoning components.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:05:42 GMT'}]",2025-03-20,"[['Aissi', 'Mohamed Salim', ''], ['Grislain', 'Clemence', ''], ['Chetouani', 'Mohamed', ''], ['Sigaud', 'Olivier', ''], ['Soulier', 'Laure', ''], ['Thome', 'Nicolas', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'behavioral cloning', 'label': 'Few-shot Learning'}, {'text': 'reinforcement learning', 'label': 'Few-shot Learning'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15112,Wenji Fang,"Shang Liu, Yao Lu, Wenji Fang, Mengming Li, Zhiyao Xie","OpenLLM-RTL: Open Dataset and Benchmark for LLM-Aided Design RTL
  Generation",ICCAD'24,,,,cs.AR,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  The automated generation of design RTL based on large language model (LLM)
and natural language instructions has demonstrated great potential in agile
circuit design. However, the lack of datasets and benchmarks in the public
domain prevents the development and fair evaluation of LLM solutions. This
paper highlights our latest advances in open datasets and benchmarks from three
perspectives: (1) RTLLM 2.0, an updated benchmark assessing LLM's capability in
design RTL generation. The benchmark is augmented to 50 hand-crafted designs.
Each design provides the design description, test cases, and a correct RTL
code. (2) AssertEval, an open-source benchmark assessing the LLM's assertion
generation capabilities for RTL verification. The benchmark includes 18
designs, each providing specification, signal definition, and correct RTL code.
(3) RTLCoder-Data, an extended open-source dataset with 80K instruction-code
data samples. Moreover, we propose a new verification-based method to verify
the functionality correctness of training data samples. Based on this
technique, we further release a dataset with 7K verified high-quality samples.
These three studies are integrated into one framework, providing off-the-shelf
support for the development and evaluation of LLMs for RTL code generation and
verification. Finally, extensive experiments indicate that LLM performance can
be boosted by enlarging the training dataset, improving data quality, and
improving the training scheme.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:12:53 GMT'}]",2025-03-20,"[['Liu', 'Shang', ''], ['Lu', 'Yao', ''], ['Fang', 'Wenji', ''], ['Li', 'Mengming', ''], ['Xie', 'Zhiyao', '']]","[{'text': 'large language model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]",Large Language Model,large language model,1.0
2503.15113,Benjamin Estermann,Benjamin Estermann and Roger Wattenhofer,Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs,Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have demonstrated remarkable text generation
capabilities, and recent advances in training paradigms have led to
breakthroughs in their reasoning performance. In this work, we investigate how
the reasoning effort of such models scales with problem complexity. We use the
infinitely scalable Tents puzzle, which has a known linear-time solution, to
analyze this scaling behavior. Our results show that reasoning effort scales
with problem size, but only up to a critical problem complexity. Beyond this
threshold, the reasoning effort does not continue to increase, and may even
decrease. This observation highlights a critical limitation in the logical
coherence of current LLMs as problem complexity increases, and underscores the
need for strategies to improve reasoning scalability. Furthermore, our results
reveal significant performance differences between current state-of-the-art
reasoning models when faced with increasingly complex logical puzzles.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:13:51 GMT'}]",2025-03-20,"[['Estermann', 'Benjamin', ''], ['Wattenhofer', 'Roger', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15117,Shichen Li,"Shichen Li, Zhongqing Wang, Zheyu Zhao, Yue Zhang, Peifeng Li","Exploring Model Editing for LLM-based Aspect-Based Sentiment
  Classification",AAAI2025,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Model editing aims at selectively updating a small subset of a neural model's
parameters with an interpretable strategy to achieve desired modifications. It
can significantly reduce computational costs to adapt to large language models
(LLMs). Given its ability to precisely target critical components within LLMs,
model editing shows great potential for efficient fine-tuning applications. In
this work, we investigate model editing to serve an efficient method for
adapting LLMs to solve aspect-based sentiment classification. Through causal
interventions, we trace and determine which neuron hidden states are essential
for the prediction of the model. By performing interventions and restorations
on each component of an LLM, we identify the importance of these components for
aspect-based sentiment classification. Our findings reveal that a distinct set
of mid-layer representations is essential for detecting the sentiment polarity
of given aspect words. Leveraging these insights, we develop a model editing
approach that focuses exclusively on these critical parts of the LLM, leading
to a more efficient method for adapting LLMs. Our in-domain and out-of-domain
experiments demonstrate that this approach achieves competitive results
compared to the currently strongest methods with significantly fewer trainable
parameters, highlighting a more efficient and interpretable fine-tuning
strategy.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:21:37 GMT'}]",2025-03-20,"[['Li', 'Shichen', ''], ['Wang', 'Zhongqing', ''], ['Zhao', 'Zheyu', ''], ['Zhang', 'Yue', ''], ['Li', 'Peifeng', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2503.15126,Haoyu Ji,"Haoyu Ji, Bowen Chen, Weihong Ren, Wenze Huang, Zhihao Yang, Zhiyong
  Wang, and Honghai Liu","Text-Derived Relational Graph-Enhanced Network for Skeleton-Based Action
  Segmentation",,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Skeleton-based Temporal Action Segmentation (STAS) aims to segment and
recognize various actions from long, untrimmed sequences of human skeletal
movements. Current STAS methods typically employ spatio-temporal modeling to
establish dependencies among joints as well as frames, and utilize one-hot
encoding with cross-entropy loss for frame-wise classification supervision.
However, these methods overlook the intrinsic correlations among joints and
actions within skeletal features, leading to a limited understanding of human
movements. To address this, we propose a Text-Derived Relational Graph-Enhanced
Network (TRG-Net) that leverages prior graphs generated by Large Language
Models (LLM) to enhance both modeling and supervision. For modeling, the
Dynamic Spatio-Temporal Fusion Modeling (DSFM) method incorporates Text-Derived
Joint Graphs (TJG) with channel- and frame-level dynamic adaptation to
effectively model spatial relations, while integrating spatio-temporal core
features during temporal modeling. For supervision, the Absolute-Relative
Inter-Class Supervision (ARIS) method employs contrastive learning between
action features and text embeddings to regularize the absolute class
distributions, and utilizes Text-Derived Action Graphs (TAG) to capture the
relative inter-class relationships among action features. Additionally, we
propose a Spatial-Aware Enhancement Processing (SAEP) method, which
incorporates random joint occlusion and axial rotation to enhance spatial
generalization. Performance evaluations on four public datasets demonstrate
that TRG-Net achieves state-of-the-art results.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:38:14 GMT'}]",2025-03-20,"[['Ji', 'Haoyu', ''], ['Chen', 'Bowen', ''], ['Ren', 'Weihong', ''], ['Huang', 'Wenze', ''], ['Yang', 'Zhihao', ''], ['Wang', 'Zhiyong', ''], ['Liu', 'Honghai', '']]","[{'text': 'Large Language\nModels', 'label': 'Large Language Model'}, {'text': 'Text-Derived\nJoint Graphs', 'label': 'Embedding'}, {'text': 'contrastive learning', 'label': 'Few-shot Learning'}, {'text': 'text embeddings', 'label': 'Embedding'}, {'text': 'Text-Derived Action Graphs', 'label': 'Embedding'}, {'text': 'four public datasets', 'label': 'Open-source LLMs'}]",Large Language Model,"Large Language
Models",0.9664971828460693
2503.15176,Navya Sonal Agarwal,Navya Sonal Agarwal and Sanjay Kumar Sonbhadra,A Review on Large Language Models for Visual Analytics,,,,,cs.HC cs.CL cs.CV,http://creativecommons.org/licenses/by/4.0/,"  This paper provides a comprehensive review of the integration of Large
Language Models (LLMs) with visual analytics, addressing their foundational
concepts, capabilities, and wide-ranging applications. It begins by outlining
the theoretical underpinnings of visual analytics and the transformative
potential of LLMs, specifically focusing on their roles in natural language
understanding, natural language generation, dialogue systems, and text-to-media
transformations. The review further investigates how the synergy between LLMs
and visual analytics enhances data interpretation, visualization techniques,
and interactive exploration capabilities. Key tools and platforms including
LIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized
multimodal models such as ChartLlama and CharXIV, are critically evaluated. The
paper discusses their functionalities, strengths, and limitations in supporting
data exploration, visualization enhancement, automated reporting, and insight
extraction. The taxonomy of LLM tasks, ranging from natural language
understanding (NLU), natural language generation (NLG), to dialogue systems and
text-to-media transformations, is systematically explored. This review provides
a SWOT analysis of integrating Large Language Models (LLMs) with visual
analytics, highlighting strengths like accessibility and flexibility,
weaknesses such as computational demands and biases, opportunities in
multimodal integration and user collaboration, and threats including privacy
concerns and skill degradation. It emphasizes addressing ethical considerations
and methodological improvements for effective integration.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 13:02:01 GMT'}]",2025-03-20,"[['Agarwal', 'Navya Sonal', ''], ['Sonbhadra', 'Sanjay Kumar', '']]","[{'text': 'Large\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LIDA', 'label': 'Open-source LLMs'}, {'text': 'Julius AI', 'label': 'Open-source LLMs'}, {'text': 'Zoho Analytics', 'label': 'Open-source LLMs'}, {'text': 'ChartLlama', 'label': 'Llama'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'privacy\nconcerns', 'label': 'AI Ethics'}, {'text': 'ethical considerations', 'label': 'AI Ethics'}]",Large Language Model,"Large
Language Models",0.9664971828460693
2503.15191,Hyunjun Kim He,"Sejong Kim, Hyunseo Song, Hyunwoo Seo, Hyunjun Kim","Optimizing Retrieval Strategies for Financial Question Answering
  Documents in Retrieval-Augmented Generation Systems","15 pages, 3 figures, 11 tables. Accepted at ICLR 2025 Workshop on
  Advances in Financial AI. Code available at
  https://github.com/seohyunwoo-0407/GAR",,,,cs.IR,http://creativecommons.org/licenses/by/4.0/,"  Retrieval-Augmented Generation (RAG) has emerged as a promising framework to
mitigate hallucinations in Large Language Models (LLMs), yet its overall
performance is dependent on the underlying retrieval system. In the finance
domain, documents such as 10-K reports pose distinct challenges due to
domain-specific vocabulary and multi-hierarchical tabular data. In this work,
we introduce an efficient, end-to-end RAG pipeline that enhances retrieval for
financial documents through a three-phase approach: pre-retrieval, retrieval,
and post-retrieval. In the pre-retrieval phase, various query and corpus
preprocessing techniques are employed to enrich input data. During the
retrieval phase, we fine-tuned state-of-the-art (SOTA) embedding models with
domain-specific knowledge and implemented a hybrid retrieval strategy that
combines dense and sparse representations. Finally, the post-retrieval phase
leverages Direct Preference Optimization (DPO) training and document selection
methods to further refine the results. Evaluations on seven financial question
answering datasets-FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA,
and MultiHiertt-demonstrate substantial improvements in retrieval performance,
leading to more accurate and contextually appropriate generation. These
findings highlight the critical role of tailored retrieval techniques in
advancing the effectiveness of RAG systems for financial applications. A fully
replicable pipeline is available on GitHub:
https://github.com/seohyunwoo-0407/GAR.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 13:21:49 GMT'}]",2025-03-20,"[['Kim', 'Sejong', ''], ['Song', 'Hyunseo', ''], ['Seo', 'Hyunwoo', ''], ['Kim', 'Hyunjun', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15195,Giorgia Crosilla,"Giorgia Crosilla, Lukas Klic and Giovanni Colavizza",Benchmarking Large Language Models for Handwritten Text Recognition,,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Traditional machine learning models for Handwritten Text Recognition (HTR)
rely on supervised training, requiring extensive manual annotations, and often
produce errors due to the separation between layout and text processing. In
contrast, Multimodal Large Language Models (MLLMs) offer a general approach to
recognizing diverse handwriting styles without the need for model-specific
training. The study benchmarks various proprietary and open-source LLMs against
Transkribus models, evaluating their performance on both modern and historical
datasets written in English, French, German, and Italian. In addition, emphasis
is placed on testing the models' ability to autonomously correct previously
generated outputs. Findings indicate that proprietary models, especially Claude
3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs
achieve excellent results in recognizing modern handwriting and exhibit a
preference for the English language due to their pre-training dataset
composition. Comparisons with Transkribus show no consistent advantage for
either approach. Moreover, LLMs demonstrate limited ability to autonomously
correct errors in zero-shot transcriptions.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 13:33:29 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 15:49:10 GMT'}]",2025-03-21,"[['Crosilla', 'Giorgia', ''], ['Klic', 'Lukas', ''], ['Colavizza', 'Giovanni', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'zero-shot settings', 'label': 'Few-shot Learning'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2503.15248,Nathalia Nascimento,"Jomar Thomas Almonte, Santhosh Anitha Boominathan, Nathalia Nascimento","Automated Non-Functional Requirements Generation in Software Engineering
  with Large Language Models: A Comparative Study",11 pages,,,,cs.SE cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neglecting non-functional requirements (NFRs) early in software development
can lead to critical challenges. Despite their importance, NFRs are often
overlooked or difficult to identify, impacting software quality. To support
requirements engineers in eliciting NFRs, we developed a framework that
leverages Large Language Models (LLMs) to derive quality-driven NFRs from
functional requirements (FRs). Using a custom prompting technique within a
Deno-based pipeline, the system identifies relevant quality attributes for each
functional requirement and generates corresponding NFRs, aiding systematic
integration. A crucial aspect is evaluating the quality and suitability of
these generated requirements. Can LLMs produce high-quality NFR suggestions?
Using 34 functional requirements - selected as a representative subset of 3,964
FRs-the LLMs inferred applicable attributes based on the ISO/IEC 25010:2023
standard, generating 1,593 NFRs. A horizontal evaluation covered three
dimensions: NFR validity, applicability of quality attributes, and
classification precision. Ten industry software quality evaluators, averaging
13 years of experience, assessed a subset for relevance and quality. The
evaluation showed strong alignment between LLM-generated NFRs and expert
assessments, with median validity and applicability scores of 5.0 (means: 4.63
and 4.59, respectively) on a 1-5 scale. In the classification task, 80.4% of
LLM-assigned attributes matched expert choices, with 8.3% near misses and 11.3%
mismatches. A comparative analysis of eight LLMs highlighted variations in
performance, with gemini-1.5-pro exhibiting the highest attribute accuracy,
while llama-3.3-70B achieved higher validity and applicability scores. These
findings provide insights into the feasibility of using LLMs for automated NFR
generation and lay the foundation for further exploration of AI-assisted
requirements engineering.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 14:23:22 GMT'}]",2025-03-20,"[['Almonte', 'Jomar Thomas', ''], ['Boominathan', 'Santhosh Anitha', ''], ['Nascimento', 'Nathalia', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'custom prompting technique', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15301,Huanyu Liu,"Jia Li, Hao Zhu, Huanyu Liu, Xianjie Shi, He Zong, Yihong Dong, Kechi
  Zhang, Siyuan Jiang, Zhi Jin, Ge Li","aiXcoder-7B-v2: Training LLMs to Fully Utilize the Long Context in
  Repository-level Code Completion",,,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Repository-level code completion aims to complete code based on the long
contexts of the repository. Existing studies extract long contexts from the
repository as inputs and leverage Large Language Models (LLMs) to generate
code. However, we reveal a severe limitation of LLMs, i.e., LLMs may ignore the
information within long contexts in code completion. In other words, even the
contexts contain useful information (e.g., relevant APIs or similar code), LLMs
may fail to utilize this information. We think this limitation is caused by an
inherent bias in LLMs, i.e., relying on nearby contexts and ignoring long-range
contexts. To address this, we propose a novel fine-tuning approach named CoLT.
The core idea of CoLT is to provide explicit supervision signals, which
emphasize that long-range contexts may hold relevant information. Specifically,
CoLT proposes a reinforcement learning-based training, which explicitly
encourages models to utilize the information within long contexts and punishes
models for ignoring long contexts. To support CoLT, we release CoLT-132K, a
large-scale dataset with 132k samples across four languages, each containing
long-context inputs. We apply CoLT to a popular LLM - aiXcoder-7B and release
aiXcoder-7B-v2. We conduct extensive experiments on CoLT-132K and a public
benchmark - CrossCodeEval. Our experiments yield the results: 1. Effectiveness.
CoLT substantially improves aiXcoder-7B. aiXcoder-7B-v2 outperforms aiXcoder-7B
by up to 44% in exact match. aiXcoder-7B-v2 becomes the state-of-the-art 7B
model in code completion and even surpasses larger models. 2. Generalizability.
The capability learned by CoLT can generalize to new languages. Besides, CoLT
is model-agnostic and effectively improves multiple LLMs. 3. Enhanced Context
Utilization Capability. CoLT significantly improves the capability of LLMs in
utilizing the relevant information within long contexts.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:22:58 GMT'}]",2025-03-20,"[['Li', 'Jia', ''], ['Zhu', 'Hao', ''], ['Liu', 'Huanyu', ''], ['Shi', 'Xianjie', ''], ['Zong', 'He', ''], ['Dong', 'Yihong', ''], ['Zhang', 'Kechi', ''], ['Jiang', 'Siyuan', ''], ['Jin', 'Zhi', ''], ['Li', 'Ge', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'CoLT', 'label': 'Fine-tuning'}, {'text': 'reinforcement learning-based training', 'label': 'Few-shot Learning'}, {'text': 'CoLT', 'label': 'Fine-tuning'}, {'text': 'CrossCodeEval', 'label': 'Open-source LLMs'}, {'text': 'CoLT', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'CoLT', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15338,Junyi Ao,"Junyi Ao, Dekun Chen, Xiaohai Tian, Wenjie Feng, Jun Zhang, Lu Lu,
  Yuxuan Wang, Haizhou Li, Zhizheng Wu",Solla: Towards a Speech-Oriented LLM That Hears Acoustic Context,,,,,eess.AS cs.CL cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have recently shown remarkable ability to
process not only text but also multimodal inputs such as speech and audio.
However, most existing models primarily focus on analyzing input signals using
text instructions, overlooking scenarios in which speech instructions and audio
are mixed and serve as inputs to the model. To address these challenges, we
introduce Solla, a novel framework designed to understand speech-based
questions and hear the acoustic context concurrently. Solla incorporates an
audio tagging module to effectively identify and represent audio events, as
well as an ASR-assisted prediction method to improve comprehension of spoken
content. To rigorously evaluate Solla and other publicly available models, we
propose a new benchmark dataset called SA-Eval, which includes three tasks:
audio event classification, audio captioning, and audio question answering.
SA-Eval has diverse speech instruction with various speaking styles,
encompassing two difficulty levels, easy and hard, to capture the range of
real-world acoustic conditions. Experimental results show that Solla performs
on par with or outperforms baseline models on both the easy and hard test sets,
underscoring its effectiveness in jointly understanding speech and audio.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:34:21 GMT'}]",2025-03-20,"[['Ao', 'Junyi', ''], ['Chen', 'Dekun', ''], ['Tian', 'Xiaohai', ''], ['Feng', 'Wenjie', ''], ['Zhang', 'Jun', ''], ['Lu', 'Lu', ''], ['Wang', 'Yuxuan', ''], ['Li', 'Haizhou', ''], ['Wu', 'Zhizheng', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Solla', 'label': 'Llama'}, {'text': 'Solla', 'label': 'Llama'}, {'text': 'Solla', 'label': 'Llama'}, {'text': 'Solla', 'label': 'Llama'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15341,Yuqi Zhu,"Yuqi Zhu, Ge Li, Xue Jiang, Jia Li, Hong Mei, Zhi Jin, Yihong Dong",Uncertainty-Guided Chain-of-Thought for Code Generation with LLMs,,,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Chain-of-Thought (CoT) reasoning has been demonstrated as an effective
technique for improving the problem-solving capabilities of large language
models (LLMs) in the context of code generation. However, existing CoT methods
often exhibit a tendency toward ""overthinking"", where the LLM consistently
applies reasoning strategies without adequately considering the task's
underlying complexity. This results in the LLMs allocating excessive
computational resources, in terms of tokens, to relatively simple tasks or
problems where the correct answer is already evident. Additionally, this
overthinking may lead LLMs down incorrect reasoning paths, resulting in
incorrect code generation. In this paper, we introduce UnCertainty-Aware
Chain-of-Thought (UnCert-CoT), an LLM-based approach designed to enhance code
generation by incorporating an uncertainty-aware CoT reasoning mechanism, which
focuses computational resources on targeting points where LLMs are more prone
to error. We propose two confidence-based uncertainty measures: Entropy-based
and Probability Differential-based methods. When uncertainty is high,
UnCert-CoT activates CoT-decoding to generate multiple reasoning paths and
selects the final code that exhibits the highest likelihood of correctness. In
contrast, LLM directly generates the code when uncertainty is low. This
uncertainty judgment mechanism allows LLMs to prioritize complex tasks and
avoid unnecessary steps in simpler cases, thereby improving overall efficiency
and accuracy in code generation. Our experimental results demonstrate that
UnCert-CoT significantly enhances code generation accuracy on challenging
benchmark MHPP(Mostly Hard Python Problems), it achieves improvements up to
6.1% on PassRate accuracy, particularly in situations where traditional LLMs
are prone to errors.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:40:45 GMT'}]",2025-03-20,"[['Zhu', 'Yuqi', ''], ['Li', 'Ge', ''], ['Jiang', 'Xue', ''], ['Li', 'Jia', ''], ['Mei', 'Hong', ''], ['Jin', 'Zhi', ''], ['Dong', 'Yihong', '']]","[{'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'large language\nmodels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'UnCert-CoT', 'label': 'LLM-based'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'UnCert-CoT', 'label': 'LLM-based'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,"large language
models",0.9664971828460693
2503.15358,Thomas Pickard,"Thomas Pickard, Aline Villavicencio, Maggie Mi, Wei He, Dylan Phelps,
  Carolina Scarton, Marco Idiart","SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity
  Representation",Preprint; SemEval-2025 proceedings to appear at ACL 2025,,,,cs.CL cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Idiomatic expressions present a unique challenge in NLP, as their meanings
are often not directly inferable from their constituent words. Despite recent
advancements in Large Language Models (LLMs), idiomaticity remains a
significant obstacle to robust semantic representation. We present datasets and
tasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity
Representation), which challenges the community to assess and improve models'
ability to interpret idiomatic expressions in multimodal contexts and in
multiple languages. Participants competed in two subtasks: ranking images based
on their alignment with idiomatic or literal meanings, and predicting the next
image in a sequence. The most effective methods achieved human-level
performance by leveraging pretrained LLMs and vision-language models in
mixture-of-experts settings, with multiple queries used to smooth over the
weaknesses in these models' representations of idiomaticity.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:58:46 GMT'}]",2025-03-20,"[['Pickard', 'Thomas', ''], ['Villavicencio', 'Aline', ''], ['Mi', 'Maggie', ''], ['He', 'Wei', ''], ['Phelps', 'Dylan', ''], ['Scarton', 'Carolina', ''], ['Idiart', 'Marco', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15369,Yinan Liang,"Yinan Liang, Ziwei Wang, Xiuwei Xu, Jie Zhou, Jiwen Lu","EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language
  Models",Accepted by CVPR 2025,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  While multimodal large language models demonstrate strong performance in
complex reasoning tasks, they pose significant challenges related to model
complexity during deployment, especially for resource-limited devices. In this
paper, we propose an automatic pruning method for large vision-language models
to enhance the efficiency of multimodal reasoning. Conventional methods rely on
the training data of the original model to select the proper pruning ratio for
different network components. However, these methods are impractical for large
vision-language models due to the unaffordable search costs caused by web-scale
training corpus. In contrast, our approach only leverages a small number of
samples to search for the desired pruning policy by maximizing its
generalization ability on unknown training data while maintaining the model
accuracy, which enables the achievement of an optimal trade-off between
accuracy and efficiency for large visual language models. Specifically, we
formulate the generalization gap of the pruning strategy using the structural
risk minimization principle. Based on both task performance and generalization
capability, we iteratively search for the optimal pruning policy within a given
search space and optimize the vision projector to evolve the search space with
higher upper bound of performance. We conduct extensive experiments on the
ScienceQA, Vizwiz, MM-vet, and LLaVA-Bench datasets for the task of visual
question answering. Using only 64 samples for pruning policy search,
EfficientLLaVA achieves an accuracy of 83.05% on ScienceQA, along with a
$\times$ 1.8 speedup compared to the dense LLaVA-v1.5-7B model.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:07:04 GMT'}]",2025-03-20,"[['Liang', 'Yinan', ''], ['Wang', 'Ziwei', ''], ['Xu', 'Xiuwei', ''], ['Zhou', 'Jie', ''], ['Lu', 'Jiwen', '']]","[{'text': 'large\nvision-language models', 'label': 'Large Language Model'}, {'text': 'ScienceQA', 'label': 'Large Language Model'}]",Large Language Model,"large
vision-language models",0.7742220759391785
2503.15426,Wei Tang,"Wei Tang, Yanpeng Sun, Qinying Gu, Zechao Li",Visual Position Prompt for MLLM based Visual Grounding,,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although Multimodal Large Language Models (MLLMs) excel at various
image-related tasks, they encounter challenges in precisely aligning
coordinates with spatial information within images, particularly in
position-aware tasks such as visual grounding. This limitation arises from two
key factors. First, MLLMs lack explicit spatial references, making it difficult
to associate textual descriptions with precise image locations. Second, their
feature extraction processes prioritize global context over fine-grained
spatial details, leading to weak localization capability. To address this
issue, we introduce VPP-LLaVA, an MLLM equipped with Visual Position Prompt
(VPP) to improve its grounding capability. VPP-LLaVA integrates two
complementary mechanisms. The global VPP overlays learnable, axis-like
embeddings onto the input image to provide structured spatial cues. The local
VPP focuses on fine-grained localization by incorporating position-aware
queries, which suggests probable object locations. We also introduce a VPP-SFT
dataset with 0.6M samples, consolidating high-quality visual grounding data
into a compact format for efficient model training. Training on this dataset
with VPP enhances the model's performance, achieving state-of-the-art results
on standard grounding benchmarks despite using fewer training samples compared
to other MLLMs like MiniGPT-v2, which rely on much larger datasets ($\sim$21M
samples). The code and VPP-SFT dataset will be available at
https://github.com/WayneTomas/VPP-LLaVA upon acceptance.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:08:13 GMT'}]",2025-03-20,"[['Tang', 'Wei', ''], ['Sun', 'Yanpeng', ''], ['Gu', 'Qinying', ''], ['Li', 'Zechao', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'Visual Position Prompt', 'label': 'Prompting'}, {'text': 'VPP', 'label': 'contextual Embedding'}, {'text': 'axis-like\nembeddings', 'label': 'contextual Embedding'}, {'text': 'VPP', 'label': 'Prompting'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2503.15463,Jianan Li,"Jia-Nan Li, Jian Guan, Songhao Wu, Wei Wu, Rui Yan","From 1,000,000 Users to Every User: Scaling Up Personalized Preference
  for User-level Alignment",,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Large language models (LLMs) have traditionally been aligned through
one-size-fits-all approaches that assume uniform human preferences,
fundamentally overlooking the diversity in user values and needs. This paper
introduces a comprehensive framework for scalable personalized alignment of
LLMs. We establish a systematic preference space characterizing psychological
and behavioral dimensions, alongside diverse persona representations for robust
preference inference in real-world scenarios. Building upon this foundation, we
introduce \textsc{AlignX}, a large-scale dataset of over 1.3 million
personalized preference examples, and develop two complementary alignment
approaches: \textit{in-context alignment} directly conditioning on persona
representations and \textit{preference-bridged alignment} modeling intermediate
preference distributions. Extensive experiments demonstrate substantial
improvements over existing methods, with an average 17.06\% accuracy gain
across four benchmarks while exhibiting a strong adaptation capability to novel
preferences, robustness to limited user data, and precise preference
controllability. These results validate our framework's effectiveness,
advancing toward truly user-adaptive AI systems.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:41:46 GMT'}]",2025-03-20,"[['Li', 'Jia-Nan', ''], ['Guan', 'Jian', ''], ['Wu', 'Songhao', ''], ['Wu', 'Wei', ''], ['Yan', 'Rui', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2503.15546,Shraddha Shah,Shraddha Pradipbhai Shah and Aditya Vilas Deshpande,"Enforcing Cybersecurity Constraints for LLM-driven Robot Agents for
  Online Transactions",,,,,cs.CR cs.AI cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The integration of Large Language Models (LLMs) into autonomous robotic
agents for conducting online transactions poses significant cybersecurity
challenges. This study aims to enforce robust cybersecurity constraints to
mitigate the risks associated with data breaches, transaction fraud, and system
manipulation. The background focuses on the rise of LLM-driven robotic systems
in e-commerce, finance, and service industries, alongside the vulnerabilities
they introduce. A novel security architecture combining blockchain technology
with multi-factor authentication (MFA) and real-time anomaly detection was
implemented to safeguard transactions. Key performance metrics such as
transaction integrity, response time, and breach detection accuracy were
evaluated, showing improved security and system performance. The results
highlight that the proposed architecture reduced fraudulent transactions by
90%, improved breach detection accuracy to 98%, and ensured secure transaction
validation within a latency of 0.05 seconds. These findings emphasize the
importance of cybersecurity in the deployment of LLM-driven robotic systems and
suggest a framework adaptable to various online platforms.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 01:01:10 GMT'}]",2025-03-21,"[['Shah', 'Shraddha Pradipbhai', ''], ['Deshpande', 'Aditya Vilas', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15547,Juhee Kim,"Juhee Kim, Woohyuk Choi, Byoungyoung Lee",Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents,,,,,cs.CR cs.AI cs.MA,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) are combined with plugins to create powerful LLM
agents that provide a wide range of services. Unlike traditional software, LLM
agent's behavior is determined at runtime by natural language prompts from
either user or plugin's data. This flexibility enables a new computing paradigm
with unlimited capabilities and programmability, but also introduces new
security risks, vulnerable to privilege escalation attacks. Moreover, user
prompt is prone to be interpreted in an insecure way by LLM agents, creating
non-deterministic behaviors that can be exploited by attackers. To address
these security risks, we propose Prompt Flow Integrity (PFI), a system
security-oriented solution to prevent privilege escalation in LLM agents.
Analyzing the architectural characteristics of LLM agents, PFI features three
mitigation techniques -- i.e., untrusted data identification, enforcing least
privilege on LLM agents, and validating unsafe data flows. Our evaluation
result shows that PFI effectively mitigates privilege escalation attacks while
successfully preserving the utility of LLM agents.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 05:27:57 GMT'}]",2025-03-21,"[['Kim', 'Juhee', ''], ['Choi', 'Woohyuk', ''], ['Lee', 'Byoungyoung', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'natural language prompts', 'label': 'Prompting'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15552,Tharindu Kumarage,"Tharindu Kumarage, Cameron Johnson, Jadie Adams, Lin Ai, Matthias
  Kirchner, Anthony Hoogs, Joshua Garland, Julia Hirschberg, Arslan Basharat,
  Huan Liu","Personalized Attacks of Social Engineering in Multi-turn Conversations
  -- LLM Agents for Simulation and Detection",,,,,cs.CR cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The rapid advancement of conversational agents, particularly chatbots powered
by Large Language Models (LLMs), poses a significant risk of social engineering
(SE) attacks on social media platforms. SE detection in multi-turn, chat-based
interactions is considerably more complex than single-instance detection due to
the dynamic nature of these conversations. A critical factor in mitigating this
threat is understanding the mechanisms through which SE attacks operate,
specifically how attackers exploit vulnerabilities and how victims' personality
traits contribute to their susceptibility. In this work, we propose an
LLM-agentic framework, SE-VSim, to simulate SE attack mechanisms by generating
multi-turn conversations. We model victim agents with varying personality
traits to assess how psychological profiles influence susceptibility to
manipulation. Using a dataset of over 1000 simulated conversations, we examine
attack scenarios in which adversaries, posing as recruiters, funding agencies,
and journalists, attempt to extract sensitive information. Based on this
analysis, we present a proof of concept, SE-OmniGuard, to offer personalized
protection to users by leveraging prior knowledge of the victims personality,
evaluating attack strategies, and monitoring information exchanges in
conversations to identify potential SE attempts.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 19:14:44 GMT'}]",2025-03-21,"[['Kumarage', 'Tharindu', ''], ['Johnson', 'Cameron', ''], ['Adams', 'Jadie', ''], ['Ai', 'Lin', ''], ['Kirchner', 'Matthias', ''], ['Hoogs', 'Anthony', ''], ['Garland', 'Joshua', ''], ['Hirschberg', 'Julia', ''], ['Basharat', 'Arslan', ''], ['Liu', 'Huan', '']]","[{'text': 'chatbots', 'label': 'ChatGPT'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15560,Prashant Kulkarni,"Prashant Kulkarni, Assaf Namer","Temporal Context Awareness: A Defense Framework Against Multi-turn
  Manipulation Attacks on Large Language Models","6 pages, 2 figures, IEEE CAI",,,,cs.CR cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Large Language Models (LLMs) are increasingly vulnerable to sophisticated
multi-turn manipulation attacks, where adversaries strategically build context
through seemingly benign conversational turns to circumvent safety measures and
elicit harmful or unauthorized responses. These attacks exploit the temporal
nature of dialogue to evade single-turn detection methods, representing a
critical security vulnerability with significant implications for real-world
deployments.
  This paper introduces the Temporal Context Awareness (TCA) framework, a novel
defense mechanism designed to address this challenge by continuously analyzing
semantic drift, cross-turn intention consistency and evolving conversational
patterns. The TCA framework integrates dynamic context embedding analysis,
cross-turn consistency verification, and progressive risk scoring to detect and
mitigate manipulation attempts effectively. Preliminary evaluations on
simulated adversarial scenarios demonstrate the framework's potential to
identify subtle manipulation patterns often missed by traditional detection
techniques, offering a much-needed layer of security for conversational AI
systems. In addition to outlining the design of TCA , we analyze diverse attack
vectors and their progression across multi-turn conversation, providing
valuable insights into adversarial tactics and their impact on LLM
vulnerabilities. Our findings underscore the pressing need for robust,
context-aware defenses in conversational AI systems and highlight TCA framework
as a promising direction for securing LLMs while preserving their utility in
legitimate applications. We make our implementation available to support
further research in this emerging area of AI security.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 22:30:17 GMT'}]",2025-03-21,"[['Kulkarni', 'Prashant', ''], ['Namer', 'Assaf', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'dynamic context embedding', 'label': 'contextual Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15564,Tung Sum Thomas Kwok,Tung Sum Thomas Kwok and Chi-Hua Wang and Guang Cheng,"GReaTER: Generate Realistic Tabular data after data Enhancement and
  Reduction","Accepted by Data Engineering Meets Large Language Models: Challenges
  and Opportunities Workshop@ICDE2025 Workshop at ICDE 2025",,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Tabular data synthesis involves not only multi-table synthesis but also
generating multi-modal data (e.g., strings and categories), which enables
diverse knowledge synthesis. However, separating numerical and categorical data
has limited the effectiveness of tabular data generation. The GReaT (Generate
Realistic Tabular Data) framework uses Large Language Models (LLMs) to encode
entire rows, eliminating the need to partition data types. Despite this, the
framework's performance is constrained by two issues: (1) tabular data entries
lack sufficient semantic meaning, limiting LLM's ability to leverage
pre-trained knowledge for in-context learning, and (2) complex multi-table
datasets struggle to establish effective relationships for collaboration. To
address these, we propose GReaTER (Generate Realistic Tabular Data after data
Enhancement and Reduction), which includes: (1) a data semantic enhancement
system that improves LLM's understanding of tabular data through mapping,
enabling better in-context learning, and (2) a cross-table connecting method to
establish efficient relationships across complex tables. Experimental results
show that GReaTER outperforms the GReaT framework.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 04:16:05 GMT'}]",2025-03-21,"[['Kwok', 'Tung Sum Thomas', ''], ['Wang', 'Chi-Hua', ''], ['Cheng', 'Guang', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15566,Shijing Chen,"Shijing Chen, Shoaib Jameel, Mohamed Reda Bouadjenek, Feilong Tang,
  Usman Naseem, Basem Suleiman, Hakim Hacid, Flora D. Salim, Imran Razzak","Enforcing Consistency and Fairness in Multi-level Hierarchical
  Classification with a Mask-based Output Layer","14 pages, 14 figures. arXiv admin note: text overlap with
  arXiv:2501.06827",,,,cs.LG,http://creativecommons.org/licenses/by-sa/4.0/,"  Traditional Multi-level Hierarchical Classification (MLHC) classifiers often
rely on backbone models with $n$ independent output layers. This structure
tends to overlook the hierarchical relationships between classes, leading to
inconsistent predictions that violate the underlying taxonomy. Additionally,
once a backbone architecture for an MLHC classifier is selected, adapting the
model to accommodate new tasks can be challenging. For example, incorporating
fairness to protect sensitive attributes within a hierarchical classifier
necessitates complex adjustments to maintain the class hierarchy while
enforcing fairness constraints. In this paper, we extend this concept to
hierarchical classification by introducing a fair, model-agnostic layer
designed to enforce taxonomy and optimize specific objectives, including
consistency, fairness, and exact match. Our evaluations demonstrate that the
proposed layer not only improves the fairness of predictions but also enforces
the taxonomy, resulting in consistent predictions and superior performance.
Compared to Large Language Models (LLMs) employing in-processing de-biasing
techniques and models without any bias correction, our approach achieves better
outcomes in both fairness and accuracy, making it particularly valuable in
sectors like e-commerce, healthcare, and education, where predictive
reliability is crucial.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 06:30:04 GMT'}]",2025-03-21,"[['Chen', 'Shijing', ''], ['Jameel', 'Shoaib', ''], ['Bouadjenek', 'Mohamed Reda', ''], ['Tang', 'Feilong', ''], ['Naseem', 'Usman', ''], ['Suleiman', 'Basem', ''], ['Hacid', 'Hakim', ''], ['Salim', 'Flora D.', ''], ['Razzak', 'Imran', '']]","[{'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15571,Pankaj Thorat,"Pankaj Thorat, Adnan Qidwai, Adrija Dhar, Aishwariya Chakraborty,
  Anand Eswaran, Hima Patel, Praveen Jayachandran","LLM-Aided Customizable Profiling of Code Data Based On Programming
  Language Concepts",21 pages,,,,cs.SE cs.ET cs.IR cs.LG cs.PL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Data profiling is critical in machine learning for generating descriptive
statistics, supporting both deeper understanding and downstream tasks like data
valuation and curation. This work addresses profiling specifically in the
context of code datasets for Large Language Models (code-LLMs), where data
quality directly influences tasks such as code generation and summarization.
Characterizing code datasets in terms of programming language concepts enables
better insights and targeted data curation. Our proposed methodology decomposes
code data profiling into two phases: (1) an offline phase where LLMs are
leveraged to derive and learn rules for extracting syntactic and semantic
concepts across various programming languages, including previously unseen or
low-resource languages, and (2) an online deterministic phase applying these
derived rules for efficient real-time analysis. This hybrid approach is
customizable, extensible to new syntactic and semantic constructs, and scalable
to multiple languages. Experimentally, our LLM-aided method achieves a mean
accuracy of 90.33% for syntactic extraction rules and semantic classification
accuracies averaging 80% and 77% across languages and semantic concepts,
respectively.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:01:00 GMT'}]",2025-03-21,"[['Thorat', 'Pankaj', ''], ['Qidwai', 'Adnan', ''], ['Dhar', 'Adrija', ''], ['Chakraborty', 'Aishwariya', ''], ['Eswaran', 'Anand', ''], ['Patel', 'Hima', ''], ['Jayachandran', 'Praveen', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15655,Zefeng Lin,"Zefeng Lin, Yi Xiao, Zhiqiang Mo, Qifan Zhang, Jie Wang, Jiayang Chen,
  Jiajing Zhang, Hui Zhang, Zhengyi Liu, Xianyong Fang, Xiaohua Xu","R$^2$: A LLM Based Novel-to-Screenplay Generation Framework with Causal
  Plot Graphs","16 pages, 6 figures",,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatically adapting novels into screenplays is important for the TV, film,
or opera industries to promote products with low costs. The strong performances
of large language models (LLMs) in long-text generation call us to propose a
LLM based framework Reader-Rewriter (R$^2$) for this task. However, there are
two fundamental challenges here. First, the LLM hallucinations may cause
inconsistent plot extraction and screenplay generation. Second, the
causality-embedded plot lines should be effectively extracted for coherent
rewriting. Therefore, two corresponding tactics are proposed: 1) A
hallucination-aware refinement method (HAR) to iteratively discover and
eliminate the affections of hallucinations; and 2) a causal plot-graph
construction method (CPC) based on a greedy cycle-breaking algorithm to
efficiently construct plot lines with event causalities. Recruiting those
efficient techniques, R$^2$ utilizes two modules to mimic the human screenplay
rewriting process: The Reader module adopts a sliding window and CPC to build
the causal plot graphs, while the Rewriter module generates first the scene
outlines based on the graphs and then the screenplays. HAR is integrated into
both modules for accurate inferences of LLMs. Experimental results demonstrate
the superiority of R$^2$, which substantially outperforms three existing
approaches (51.3%, 22.6%, and 57.1% absolute increases) in pairwise comparison
at the overall win rate for GPT-4o.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 19:09:40 GMT'}]",2025-03-21,"[['Lin', 'Zefeng', ''], ['Xiao', 'Yi', ''], ['Mo', 'Zhiqiang', ''], ['Zhang', 'Qifan', ''], ['Wang', 'Jie', ''], ['Chen', 'Jiayang', ''], ['Zhang', 'Jiajing', ''], ['Zhang', 'Hui', ''], ['Liu', 'Zhengyi', ''], ['Fang', 'Xianyong', ''], ['Xu', 'Xiaohua', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GPT-4o', 'label': 'GPT'}]",Large Language Model,large language models,0.9664971828460693
2503.15761,Mir Mohammad Khaleghi,"Mir Mohammad Khaleghi, Mehran Safayani, Abdolreza Mirzaei",GraPLUS: Graph-based Placement Using Semantics for Image Composition,"17 pages, 3 figures, 6 tables",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present GraPLUS (Graph-based Placement Using Semantics), a novel framework
for plausible object placement in images that leverages scene graphs and large
language models. Our approach uniquely combines graph-structured scene
representation with semantic understanding to determine contextually
appropriate object positions. The framework employs GPT-2 to transform
categorical node and edge labels into rich semantic embeddings that capture
both definitional characteristics and typical spatial contexts, enabling
nuanced understanding of object relationships and placement patterns. GraPLUS
achieves placement accuracy of 92.1% and an FID score of 28.83 on the OPA
dataset, outperforming state-of-the-art methods by 8.1% while maintaining
competitive visual quality. In human evaluation studies involving 964 samples
assessed by 19 participants, our method was preferred in 52.1% of cases,
significantly outperforming previous approaches. The framework's key
innovations include: (i) leveraging pre-trained scene graph models that
transfer knowledge from other domains, (ii) edge-aware graph neural networks
that process scene semantics through structured relationships, (iii) a
cross-modal attention mechanism that aligns categorical embeddings with
enhanced scene features, and (iv) a multiobjective training strategy
incorporating semantic consistency constraints.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 00:43:29 GMT'}]",2025-03-21,"[['Khaleghi', 'Mir Mohammad', ''], ['Safayani', 'Mehran', ''], ['Mirzaei', 'Abdolreza', '']]","[{'text': 'large\nlanguage models', 'label': 'Large Language Model'}, {'text': 'GPT-2', 'label': 'GPT'}, {'text': 'rich semantic embeddings', 'label': 'contextual Embedding'}, {'text': 'cross-modal attention mechanism', 'label': 'Attention mechanism'}, {'text': 'categorical embeddings', 'label': 'contextual Embedding'}]",Large Language Model,"large
language models",0.9664971828460693
2503.15772,Vishisht Rao,"Vishisht Rao, Aounon Kumar, Himabindu Lakkaraju, Nihar B. Shah",Detecting LLM-Written Peer Reviews,"26 pages, 1 figure",,,,cs.DL cs.AI cs.CR,http://creativecommons.org/licenses/by/4.0/,"  Editors of academic journals and program chairs of conferences require peer
reviewers to write their own reviews. However, there is growing concern about
the rise of lazy reviewing practices, where reviewers use large language models
(LLMs) to generate reviews instead of writing them independently. Existing
tools for detecting LLM-generated content are not designed to differentiate
between fully LLM-generated reviews and those merely polished by an LLM. In
this work, we employ a straightforward approach to identify LLM-generated
reviews - doing an indirect prompt injection via the paper PDF to ask the LLM
to embed a watermark. Our focus is on presenting watermarking schemes and
statistical tests that maintain a bounded family-wise error rate, when a venue
evaluates multiple reviews, with a higher power as compared to standard methods
like Bonferroni correction. These guarantees hold without relying on any
assumptions about human-written reviews. We also consider various methods for
prompt injection including font embedding and jailbreaking. We evaluate the
effectiveness and various tradeoffs of these methods, including different
reviewer defenses. We find a high success rate in the embedding of our
watermarks in LLM-generated reviews across models. We also find that our
approach is resilient to common reviewer defenses, and that the bounds on error
rates in our statistical tests hold in practice while having the power to flag
LLM-generated reviews, while Bonferroni correction is infeasible.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 01:11:35 GMT'}]",2025-03-21,"[['Rao', 'Vishisht', ''], ['Kumar', 'Aounon', ''], ['Lakkaraju', 'Himabindu', ''], ['Shah', 'Nihar B.', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'indirect prompt injection', 'label': 'Prompting'}, {'text': 'font embedding', 'label': 'Embedding'}, {'text': 'jailbreaking', 'label': 'Embedding'}]",Large Language Model,large language models,0.9664971828460693
2503.15783,Tsunehiko Tanaka,"Tsunehiko Tanaka, Edgar Simo-Serra","Grammar and Gameplay-aligned RL for Game Description Generation with
  LLMs",,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Game Description Generation (GDG) is the task of generating a game
description written in a Game Description Language (GDL) from natural language
text. Previous studies have explored generation methods leveraging the
contextual understanding capabilities of Large Language Models (LLMs); however,
accurately reproducing the game features of the game descriptions remains a
challenge. In this paper, we propose reinforcement learning-based fine-tuning
of LLMs for GDG (RLGDG). Our training method simultaneously improves
grammatical correctness and fidelity to game concepts by introducing both
grammar rewards and concept rewards. Furthermore, we adopt a two-stage training
strategy where Reinforcement Learning (RL) is applied following Supervised
Fine-Tuning (SFT). Experimental results demonstrate that our proposed method
significantly outperforms baseline methods using SFT alone.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 01:47:33 GMT'}]",2025-03-21,"[['Tanaka', 'Tsunehiko', ''], ['Simo-Serra', 'Edgar', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Supervised\nFine-Tuning (SFT)', 'label': 'Fine-tuning'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15807,Jiexiong Liu,"Cheng Li, Jiexiong Liu, Yixuan Chen, Yanqin Jia","Video-VoT-R1: An efficient video inference model integrating image
  packing and AoE architecture",18 pages,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the field of video-language pretraining, existing models face numerous
challenges in terms of inference efficiency and multimodal data processing.
This paper proposes a KunLunBaize-VoT-R1 video inference model based on a
long-sequence image encoder, along with its training and application methods.
By integrating image packing technology, the Autonomy-of-Experts (AoE)
architecture, and combining the video of Thought (VoT), a large language model
(LLM) trained with large-scale reinforcement learning, and multiple training
techniques, the efficiency and accuracy of the model in video inference tasks
are effectively improved. Experiments show that this model performs
outstandingly in multiple tests, providing a new solution for video-language
understanding.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 02:50:57 GMT'}]",2025-03-21,"[['Li', 'Cheng', ''], ['Liu', 'Jiexiong', ''], ['Chen', 'Yixuan', ''], ['Jia', 'Yanqin', '']]","[{'text': 'large language model', 'label': 'Large Language Model'}, {'text': 'large-scale reinforcement learning', 'label': 'Few-shot Learning'}]",Large Language Model,large language model,1.0
2503.15816,Abduljaleel Adejumo,"Abduljaleel Adejumo, Faegheh Yeganli, Clifford Broni-bediako, Aoran
  Xiao, Naoto Yokoya and Mennatullah Siam",A Vision Centric Remote Sensing Benchmark,"6 PAGES, 7 figures, CVPR",,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Multimodal Large Language Models (MLLMs) have achieved remarkable success in
vision-language tasks but their remote sensing (RS) counterpart are relatively
under explored. Unlike natural images, RS imagery presents unique challenges
that current MLLMs struggle to handle, particularly in visual grounding and
spatial reasoning. This study investigates the limitations of CLIP-based MLLMs
in RS, highlighting their failure to differentiate visually distinct yet
semantically similar RS images. To address this, we introduce a remote sensing
multimodal visual patterns (RSMMVP) benchmark. It is designed to evaluate MLLMs
in RS tasks by identifying the CLIP-blind pairs, where CLIP-based models
incorrectly assign high similarity scores to visually distinct RS images.
Through a visual question answering (VQA) evaluation, we analyze the
performance of state-of-the-art MLLMs, revealing significant limitations in RS
specific representation learning. The results provide valuable insights into
the weaknesses of CLIP-based visual encoding and offer a foundation for future
research to develop more effective MLLMs tailored for remote sensing
applications.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 03:03:46 GMT'}]",2025-03-21,"[['Adejumo', 'Abduljaleel', ''], ['Yeganli', 'Faegheh', ''], ['Broni-bediako', 'Clifford', ''], ['Xiao', 'Aoran', ''], ['Yokoya', 'Naoto', ''], ['Siam', 'Mennatullah', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2503.15837,Shangqing Zhao,"Shangqing Zhao, Yuhao Zhou, Yupei Ren, Zhe Chen, Chenghao Jia, Fang
  Zhe, Zhaogaung Long, Shu Liu and Man Lan","F\`ux\`i: A Benchmark for Evaluating Language Models on Ancient Chinese
  Text Understanding and Generation",working in progress,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Ancient Chinese text processing presents unique challenges for large language
models (LLMs) due to its distinct linguistic features, complex structural
constraints, and rich cultural context. While existing benchmarks have
primarily focused on evaluating comprehension through multiple-choice
questions, there remains a critical gap in assessing models' generative
capabilities in classical Chinese. We introduce F\`ux\`i, a comprehensive
benchmark that evaluates both understanding and generation capabilities across
21 diverse tasks. Our benchmark distinguishes itself through three key
contributions: (1) balanced coverage of both comprehension and generation
tasks, including novel tasks like poetry composition and couplet completion,
(2) specialized evaluation metrics designed specifically for classical Chinese
text generation, combining rule-based verification with fine-tuned LLM
evaluators, and (3) a systematic assessment framework that considers both
linguistic accuracy and cultural authenticity. Through extensive evaluation of
state-of-the-art LLMs, we reveal significant performance gaps between
understanding and generation tasks, with models achieving promising results in
comprehension but struggling considerably in generation tasks, particularly
those requiring deep cultural knowledge and adherence to classical formats. Our
findings highlight the current limitations in ancient Chinese text processing
and provide insights for future model development. The benchmark, evaluation
toolkit, and baseline results are publicly available to facilitate research in
this domain.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 04:26:40 GMT'}]",2025-03-21,"[['Zhao', 'Shangqing', ''], ['Zhou', 'Yuhao', ''], ['Ren', 'Yupei', ''], ['Chen', 'Zhe', ''], ['Jia', 'Chenghao', ''], ['Zhe', 'Fang', ''], ['Long', 'Zhaogaung', ''], ['Liu', 'Shu', ''], ['Lan', 'Man', '']]","[{'text': 'large language\nmodels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,"large language
models",0.9664971828460693
2503.15840,Junle Li,"Junle Li, Meiqi Tian, and Bingzhuo Zhong","Automatic Generation of Safety-compliant Linear Temporal Logic via Large
  Language Model: A Self-supervised Framework",,,,,cs.LO cs.FL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Ensuring safety in cyber-physical systems (CPS) poses a significant
challenge, especially when converting high-level tasks described by natural
language into formal specifications like Linear Temporal Logic (LTL). In
particular, the compliance of formal languages with respect to safety
restrictions imposed on CPS is crucial for system safety. In this paper, we
introduce AutoSafeLTL, a self-supervised framework that utilizes large language
models (LLMs) to automate the generation of safety-compliant LTL. Our approach
integrates a Language Inclusion check with an automated counterexample-guided
feedback and modification mechanism, establishing a pipeline that verifies the
safety-compliance of the resulting LTL while preserving its logical consistency
and semantic accuracy. To enhance the framework's understanding and correction
capabilities, we incorporate two additional Agent LLMs. Experimental results
demonstrate that AutoSafeLTL effectively guarantees safety-compliance for
generated LTL, achieving a 0% violation rate against imposed safety
constraints.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 04:40:29 GMT'}]",2025-03-21,"[['Li', 'Junle', ''], ['Tian', 'Meiqi', ''], ['Zhong', 'Bingzhuo', '']]","[{'text': 'large language\nmodels', 'label': 'Large Language Model'}]",Large Language Model,"large language
models",0.9664971828460693
2503.15846,Xuanming Cui,"Xuanming Cui, Jaiminkumar Ashokbhai Bhoi, Chionh Wei Peng, Adriel
  Kuek, Ser Nam Lim","What can Off-the-Shelves Large Multi-Modal Models do for Dynamic Scene
  Graph Generation?",,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Dynamic Scene Graph Generation (DSGG) for videos is a challenging task in
computer vision. While existing approaches often focus on sophisticated
architectural design and solely use recall during evaluation, we take a closer
look at their predicted scene graphs and discover three critical issues with
existing DSGG methods: severe precision-recall trade-off, lack of awareness on
triplet importance, and inappropriate evaluation protocols. On the other hand,
recent advances of Large Multimodal Models (LMMs) have shown great capabilities
in video understanding, yet they have not been tested on fine-grained,
frame-wise understanding tasks like DSGG. In this work, we conduct the first
systematic analysis of Video LMMs for performing DSGG. Without relying on
sophisticated architectural design, we show that LMMs with simple decoder-only
structure can be turned into State-of-the-Art scene graph generators that
effectively overcome the aforementioned issues, while requiring little
finetuning (5-10% training data).
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 04:58:53 GMT'}]",2025-03-21,"[['Cui', 'Xuanming', ''], ['Bhoi', 'Jaiminkumar Ashokbhai', ''], ['Peng', 'Chionh Wei', ''], ['Kuek', 'Adriel', ''], ['Lim', 'Ser Nam', '']]","[{'text': 'Large Multimodal Models', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'little\nfinetuning', 'label': 'Fine-tuning'}]",Large Language Model,Large Multimodal Models,0.573912501335144
2503.15850,Xiaoou Liu,"Xiaoou Liu, Tiejin Chen, Longchao Da, Chacha Chen, Zhen Lin, Hua Wei","Uncertainty Quantification and Confidence Calibration in Large Language
  Models: A Survey",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) excel in text generation, reasoning, and
decision-making, enabling their adoption in high-stakes domains such as
healthcare, law, and transportation. However, their reliability is a major
concern, as they often produce plausible but incorrect responses. Uncertainty
quantification (UQ) enhances trustworthiness by estimating confidence in
outputs, enabling risk mitigation and selective prediction. However,
traditional UQ methods struggle with LLMs due to computational constraints and
decoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources,
such as input ambiguity, reasoning path divergence, and decoding stochasticity,
that extend beyond classical aleatoric and epistemic uncertainty. To address
this, we introduce a new taxonomy that categorizes UQ methods based on
computational efficiency and uncertainty dimensions (input, reasoning,
parameter, and prediction uncertainty). We evaluate existing techniques, assess
their real-world applicability, and identify open challenges, emphasizing the
need for scalable, interpretable, and robust UQ approaches to enhance LLM
reliability.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 05:04:29 GMT'}]",2025-03-21,"[['Liu', 'Xiaoou', ''], ['Chen', 'Tiejin', ''], ['Da', 'Longchao', ''], ['Chen', 'Chacha', ''], ['Lin', 'Zhen', ''], ['Wei', 'Hua', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Uncertainty\nquantification', 'label': 'quantisation'}, {'text': 'UQ', 'label': 'quantisation'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15871,Kyungho Bae,"Kyungho Bae, Jinhyung Kim, Sihaeng Lee, Soonyoung Lee, Gunhee Lee, and
  Jinwoo Choi","MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through
  Disentangled Spatial-Temporal Representations",Accepted for CVPR 2025,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, we tackle action-scene hallucination in Video Large Language
Models (Video-LLMs), where models incorrectly predict actions based on the
scene context or scenes based on observed actions. We observe that existing
Video-LLMs often suffer from action-scene hallucination due to two main
factors. First, existing Video-LLMs intermingle spatial and temporal features
by applying an attention operation across all tokens. Second, they use the
standard Rotary Position Embedding (RoPE), which causes the text tokens to
overemphasize certain types of tokens depending on their sequential orders. To
address these issues, we introduce MASH-VLM, Mitigating Action-Scene
Hallucination in Video-LLMs through disentangled spatial-temporal
representations. Our approach includes two key innovations: (1) DST-attention,
a novel attention mechanism that disentangles the spatial and temporal tokens
within the LLM by using masked attention to restrict direct interactions
between the spatial and temporal tokens; (2) Harmonic-RoPE, which extends the
dimensionality of the positional IDs, allowing the spatial and temporal tokens
to maintain balanced positions relative to the text tokens. To evaluate the
action-scene hallucination in Video-LLMs, we introduce the UNSCENE benchmark
with 1,320 videos and 4,078 QA pairs. Extensive experiments demonstrate that
MASH-VLM achieves state-of-the-art results on the UNSCENE benchmark, as well as
on existing video understanding benchmarks.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 05:48:59 GMT'}]",2025-03-21,"[['Bae', 'Kyungho', ''], ['Kim', 'Jinhyung', ''], ['Lee', 'Sihaeng', ''], ['Lee', 'Soonyoung', ''], ['Lee', 'Gunhee', ''], ['Choi', 'Jinwoo', '']]","[{'text': 'Video Large Language\nModels', 'label': 'Large Language Model'}, {'text': 'Video-LLMs', 'label': 'Large Language Model'}, {'text': 'Video-LLMs', 'label': 'Large Language Model'}, {'text': 'Video-LLMs', 'label': 'Large Language Model'}, {'text': 'Rotary Position Embedding', 'label': 'contextual Embedding'}, {'text': 'Video-LLMs', 'label': 'Large Language Model'}, {'text': 'DST-attention', 'label': 'Attention mechanism'}, {'text': 'masked attention', 'label': 'Attention mechanism'}, {'text': 'Harmonic-RoPE', 'label': 'Attention mechanism'}, {'text': 'Video-LLMs', 'label': 'Large Language Model'}]",Large Language Model,"Video Large Language
Models",0.7616418600082397
2503.15876,Kai Chen,"Kai Chen, Zebing Sun","DeepPsy-Agent: A Stage-Aware and Deep-Thinking Emotional Support Agent
  System",,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper introduces DeepPsy-Agent, an innovative psychological support
system that combines the three-stage helping theory in psychology with deep
learning techniques. The system consists of two core components: (1) a
multi-stage response-capable dialogue model (\textit{deeppsy-chat}), which
enhances reasoning capabilities through stage-awareness and deep-thinking
analysis to generate high-quality responses; and (2) a real-time stage
transition detection model that identifies contextual shifts to guide the
dialogue towards more effective intervention stages. Based on 30,000 real
psychological hotline conversations, we employ AI-simulated dialogues and
expert re-annotation strategies to construct a high-quality multi-turn dialogue
dataset. Experimental results demonstrate that DeepPsy-Agent outperforms
general-purpose large language models (LLMs) in key metrics such as problem
exposure completeness, cognitive restructuring success rate, and action
adoption rate. Ablation studies further validate the effectiveness of
stage-awareness and deep-thinking modules, showing that stage information
contributes 42.3\% to performance, while the deep-thinking module increases
root-cause identification by 58.3\% and reduces ineffective suggestions by
72.1\%. This system addresses critical challenges in AI-based psychological
support through dynamic dialogue management and deep reasoning, advancing
intelligent mental health services.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 05:59:29 GMT'}]",2025-03-21,"[['Chen', 'Kai', ''], ['Sun', 'Zebing', '']]","[{'text': 'general-purpose large language models', 'label': 'Large Language Model'}]",Large Language Model,general-purpose large language models,0.8828719258308411
2503.15904,Hung-Hsuan Chen,"Evan Chen, Run-Jun Zhan, Yan-Bai Lin, Hung-Hsuan Chen","From Structured Prompts to Open Narratives: Measuring Gender Bias in
  LLMs Through Open-Ended Storytelling",,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have revolutionized natural language processing,
yet concerns persist regarding their tendency to reflect or amplify social
biases present in their training data. This study introduces a novel evaluation
framework to uncover gender biases in LLMs, focusing on their occupational
narratives. Unlike previous methods relying on structured scenarios or
carefully crafted prompts, our approach leverages free-form storytelling to
reveal biases embedded in the models. Systematic analyses show an
overrepresentation of female characters across occupations in six widely used
LLMs. Additionally, our findings reveal that LLM-generated occupational gender
rankings align more closely with human stereotypes than actual labor
statistics. These insights underscore the need for balanced mitigation
strategies to ensure fairness while avoiding the reinforcement of new
stereotypes.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 07:15:45 GMT'}]",2025-03-21,"[['Chen', 'Evan', ''], ['Zhan', 'Run-Jun', ''], ['Lin', 'Yan-Bai', ''], ['Chen', 'Hung-Hsuan', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'carefully crafted prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15921,Fahao Chen,"Fahao Chen, Peng Li, Tom H. Luan, Zhou Su, Jing Deng","SPIN: Accelerating Large Language Model Inference with Heterogeneous
  Speculative Models",Accepted by INFOCOM 2025,,,,cs.DC,http://creativecommons.org/licenses/by/4.0/,"  Speculative decoding has been shown as an effective way to accelerate Large
Language Model (LLM) inference by using a Small Speculative Model (SSM) to
generate candidate tokens in a so-called speculation phase, which are
subsequently verified by the LLM in a verification phase. However, current
state-of-the-art speculative decoding approaches have three key limitations:
handling requests with varying difficulty using homogeneous SSMs, lack of
robust support for batch processing, and insufficient holistic optimization for
both speculation and verification phases. In this paper, we introduce SPIN, an
efficient LLM inference serving system based on speculative decoding, designed
to address these challenges through three main innovations. First, SPIN
improves token speculation by using multiple heterogeneous SSMs, with a
learning-based algorithm for SSM selection that operates without prior
knowledge of request difficulty. Second, SPIN employs a request decomposition
method to minimize batching overhead during LLM verification. Finally, SPIN
orchestrates speculation and verification phases by pipelining their executions
on GPUs to achieve further acceleration. Experimental results demonstrate that
SPIN significantly outperforms state-of-the-art methods, achieving a
performance increase of approximately 2.28X.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 07:57:57 GMT'}]",2025-03-21,"[['Chen', 'Fahao', ''], ['Li', 'Peng', ''], ['Luan', 'Tom H.', ''], ['Su', 'Zhou', ''], ['Deng', 'Jing', '']]","[{'text': 'Large\nLanguage Model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]",Large Language Model,"Large
Language Model",1.0
2503.15937,Shiqi Jiang,"Gaole Dai, Shiqi Jiang, Ting Cao, Yuanchun Li, Yuqing Yang, Rui Tan,
  Mo Li, Lili Qiu","Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical
  Deployment","14 pages, 4 itertions",,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  We propose V-Droid, a mobile GUI task automation agent. Unlike previous
mobile agents that utilize Large Language Models (LLMs) as generators to
directly generate actions at each step, V-Droid employs LLMs as verifiers to
evaluate candidate actions before making final decisions. To realize this novel
paradigm, we introduce a comprehensive framework for constructing
verifier-driven mobile agents: the discretized action space construction
coupled with the prefilling-only workflow to accelerate the verification
process, the pair-wise progress preference training to significantly enhance
the verifier's decision-making capabilities, and the scalable human-agent joint
annotation scheme to efficiently collect the necessary data at scale. V-Droid
sets a new state-of-the-art task success rate across several public mobile task
automation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on
MobileAgentBench, surpassing existing agents by 9.5%, 2.1%, and 9%,
respectively. Furthermore, V-Droid achieves an impressively low latency of 0.7
seconds per step, making it the first mobile agent capable of delivering
near-real-time, effective decision-making capabilities.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:25:00 GMT'}]",2025-03-21,"[['Dai', 'Gaole', ''], ['Jiang', 'Shiqi', ''], ['Cao', 'Ting', ''], ['Li', 'Yuanchun', ''], ['Yang', 'Yuqing', ''], ['Tan', 'Rui', ''], ['Li', 'Mo', ''], ['Qiu', 'Lili', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'pair-wise progress preference training', 'label': 'Few-shot Learning'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.15944,Jinyi Liu,"Jinyi Liu, Yan Zheng, Rong Cheng, Qiyu Wu, Wei Guo, Fei Ni, Hebin
  Liang, Yifu Yuan, Hangyu Mao, Fuzheng Zhang, Jianye Hao","From Chaos to Order: The Atomic Reasoner Framework for Fine-grained
  Reasoning in Large Language Models",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advances in large language models (LLMs) have shown remarkable
progress, yet their capacity for logical ``slow-thinking'' reasoning persists
as a critical research frontier. Current inference scaling paradigms suffer
from two fundamental constraints: fragmented thought flows compromising logical
coherence, and intensively computational complexity that escalates with search
space dimensions. To overcome these limitations, we present \textbf{Atomic
Reasoner} (\textbf{AR}), a cognitive inference strategy that enables
fine-grained reasoning through systematic atomic-level operations. AR
decomposes the reasoning process into atomic cognitive units, employing a
cognitive routing mechanism to dynamically construct reasoning representations
and orchestrate inference pathways. This systematic methodology implements
stepwise, structured cognition, which ensures logical coherence while
significantly reducing cognitive load, effectively simulating the cognitive
patterns observed in human deep thinking processes. Extensive experimental
results demonstrate AR's superior reasoning capabilities without the
computational burden of exhaustive solution searches, particularly excelling in
linguistic logic puzzles. These findings substantiate AR's effectiveness in
enhancing LLMs' capacity for robust, long-sequence logical reasoning and
deliberation.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:34:53 GMT'}]",2025-03-21,"[['Liu', 'Jinyi', ''], ['Zheng', 'Yan', ''], ['Cheng', 'Rong', ''], ['Wu', 'Qiyu', ''], ['Guo', 'Wei', ''], ['Ni', 'Fei', ''], ['Liang', 'Hebin', ''], ['Yuan', 'Yifu', ''], ['Mao', 'Hangyu', ''], ['Zhang', 'Fuzheng', ''], ['Hao', 'Jianye', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Current inference scaling paradigms', 'label': 'Scaling law'}, {'text': 'fragmented thought flows', 'label': 'Chain of thought'}, {'text': 'logical\ncoherence', 'label': 'Chain of thought'}, {'text': 'logical coherence', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2503.15948,Vasily Konovalov,"Elisei Rykov, Kseniia Petrushina, Kseniia Titova, Alexander Panchenko,
  Vasily Konovalov","Don't Fight Hallucinations, Use Them: Estimating Image Realism using NLI
  over Atomic Facts","Proceedings of De-Factify 4: 4nd Workshop on Multimodal Fact Checking
  and Hate Speech Detection, co-located with AAAI-2025",,,,cs.CV cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Quantifying the realism of images remains a challenging problem in the field
of artificial intelligence. For example, an image of Albert Einstein holding a
smartphone violates common-sense because modern smartphone were invented after
Einstein's death. We introduce a novel method for assessing image realism using
Large Vision-Language Models (LVLMs) and Natural Language Inference (NLI). Our
approach is based on the premise that LVLMs may generate hallucinations when
confronted with images that defy common sense. Using LVLM to extract atomic
facts from these images, we obtain a mix of accurate facts and erroneous
hallucinations. We proceed by calculating pairwise entailment scores among
these facts, subsequently aggregating these values to yield a singular reality
score. This process serves to identify contradictions between genuine facts and
hallucinatory elements, signaling the presence of images that violate common
sense. Our approach has achieved a new state-of-the-art performance in
zero-shot mode on the WHOOPS! dataset.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:44:10 GMT'}]",2025-03-21,"[['Rykov', 'Elisei', ''], ['Petrushina', 'Kseniia', ''], ['Titova', 'Kseniia', ''], ['Panchenko', 'Alexander', ''], ['Konovalov', 'Vasily', '']]","[{'text': 'Albert Einstein', 'label': 'ALBERT'}, {'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'zero-shot mode', 'label': 'Zero-shot Learning'}]",Large Language Model,Large Vision-Language Models,0.7742220759391785
2503.15953,Mohammed Oualid Attaoui,Mohammed Attaoui and Fabrizio Pastore,GAN-enhanced Simulation-driven DNN Testing in Absence of Ground Truth,"15 pages, 8 figures, 13 tables",,,,cs.SE cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The generation of synthetic inputs via simulators driven by search algorithms
is essential for cost-effective testing of Deep Neural Network (DNN) components
for safety-critical systems. However, in many applications, simulators are
unable to produce the ground-truth data needed for automated test oracles and
to guide the search process.
  To tackle this issue, we propose an approach for the generation of inputs for
computer vision DNNs that integrates a generative network to ensure simulator
fidelity and employs heuristic-based search fitnesses that leverage
transformation consistency, noise resistance, surprise adequacy, and
uncertainty estimation. We compare the performance of our fitnesses with that
of a traditional fitness function leveraging ground truth; further, we assess
how the integration of a GAN not leveraging the ground truth impacts on test
and retraining effectiveness.
  Our results suggest that leveraging transformation consistency is the best
option to generate inputs for both DNN testing and retraining; it maximizes
input diversity, spots the inputs leading to worse DNN performance, and leads
to best DNN performance after retraining. Besides enabling simulator-based
testing in the absence of ground truth, our findings pave the way for testing
solutions that replace costly simulators with diffusion and large language
models, which might be more affordable than simulators, but cannot generate
ground-truth data.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:49:10 GMT'}]",2025-03-21,"[['Attaoui', 'Mohammed', ''], ['Pastore', 'Fabrizio', '']]","[{'text': 'large language\nmodels', 'label': 'Large Language Model'}]",Large Language Model,"large language
models",0.9664971828460693
2503.15990,Langming Liu,"Langming Liu, Haibin Chen, Yuhao Wang, Yujin Yuan, Shilei Liu, Wenbo
  Su, Xiangyu Zhao, Bo Zheng","ECKGBench: Benchmarking Large Language Models in E-commerce Leveraging
  Knowledge Graph",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have demonstrated their capabilities across
various NLP tasks. Their potential in e-commerce is also substantial, evidenced
by practical implementations such as platform search, personalized
recommendations, and customer service. One primary concern associated with LLMs
is their factuality (e.g., hallucination), which is urgent in e-commerce due to
its significant impact on user experience and revenue. Despite some methods
proposed to evaluate LLMs' factuality, issues such as lack of reliability, high
consumption, and lack of domain expertise leave a gap between effective
assessment in e-commerce. To bridge the evaluation gap, we propose ECKGBench, a
dataset specifically designed to evaluate the capacities of LLMs in e-commerce
knowledge. Specifically, we adopt a standardized workflow to automatically
generate questions based on a large-scale knowledge graph, guaranteeing
sufficient reliability. We employ the simple question-answering paradigm,
substantially improving the evaluation efficiency by the least input and output
tokens. Furthermore, we inject abundant e-commerce expertise in each evaluation
stage, including human annotation, prompt design, negative sampling, and
verification. Besides, we explore the LLMs' knowledge boundaries in e-commerce
from a novel perspective. Through comprehensive evaluations of several advanced
LLMs on ECKGBench, we provide meticulous analysis and insights into leveraging
LLMs for e-commerce.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 09:49:15 GMT'}]",2025-03-21,"[['Liu', 'Langming', ''], ['Chen', 'Haibin', ''], ['Wang', 'Yuhao', ''], ['Yuan', 'Yujin', ''], ['Liu', 'Shilei', ''], ['Su', 'Wenbo', ''], ['Zhao', 'Xiangyu', ''], ['Zheng', 'Bo', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ECKGBench', 'label': 'Open-source LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompt design', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ECKGBench', 'label': 'Open-source LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2503.16022,Mario Sanz-Guerrero,Mario Sanz-Guerrero and Katharina von der Wense,"Corrective In-Context Learning: Evaluating Self-Correction in Large
  Language Models","Accepted to the 6th Workshop on Insights from Negative Results in NLP
  at NAACL 2025",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  In-context learning (ICL) has transformed the use of large language models
(LLMs) for NLP tasks, enabling few-shot learning by conditioning on labeled
examples without finetuning. Despite its effectiveness, ICL is prone to errors,
especially for challenging examples. With the goal of improving the performance
of ICL, we propose corrective in-context learning (CICL), an approach that
incorporates a model's incorrect predictions alongside ground truth corrections
into the prompt, aiming to enhance classification accuracy through
self-correction. However, contrary to our hypothesis, extensive experiments on
text classification tasks demonstrate that CICL consistently underperforms
standard ICL, with performance degrading as the proportion of corrections in
the prompt increases. Our findings indicate that CICL introduces confusion by
disrupting the model's task understanding, rather than refining its
predictions. Additionally, we observe that presenting harder examples in
standard ICL does not improve performance, suggesting that example difficulty
alone may not be a reliable criterion for effective selection. By presenting
these negative results, we provide important insights into the limitations of
self-corrective mechanisms in LLMs and offer directions for future research.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 10:39:39 GMT'}]",2025-03-21,"[['Sanz-Guerrero', 'Mario', ''], ['von der Wense', 'Katharina', '']]","[{'text': 'In-context learning', 'label': 'Few-shot Learning'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'few-shot learning', 'label': 'Zero-shot Learning'}, {'text': 'finetuning', 'label': 'Fine-tuning'}, {'text': 'ICL', 'label': 'Zero-shot Learning'}, {'text': 'ICL', 'label': 'Few-shot Learning'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'CICL', 'label': 'Few-shot Learning'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'CICL', 'label': 'Few-shot Learning'}, {'text': 'ICL', 'label': 'contextual Embedding'}]",Large Language Model,large language models,0.9664971828460693
2503.16041,Akinyemi Sadeeq Akintola,"Bisola Faith Kayode, Akinyemi Sadeeq Akintola, Oluwole Fagbohun,
  Egonna Anaesiuba-Bristol, Onyekachukwu Ojumah, Oluwagbade Odimayo, Toyese
  Oloyede, Aniema Inyang, Teslim Kazeem, Habeeb Alli, Udodirim Ibem Offia,
  Prisca Chinazor Amajuoyi","GreenIQ: A Deep Search Platform for Comprehensive Carbon Market Analysis
  and Automated Report Generation","12 Pages, 1 figure",,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  This study introduces GreenIQ, an AI-powered deep search platform designed to
revolutionise carbon market intelligence through autonomous analysis and
automated report generation. Carbon markets operate across diverse regulatory
landscapes, generating vast amounts of heterogeneous data from policy
documents, industry reports, academic literature, and real-time trading
platforms. Traditional research approaches remain labour-intensive, slow, and
difficult to scale. GreenIQ addresses these limitations through a multi-agent
architecture powered by Large Language Models (LLMs), integrating five
specialised AI agents: a Main Researcher Agent for intelligent information
retrieval, a Report Writing Agent for structured synthesis, a Final Reviewer
Agent for accuracy verification, a Data Visualisation Agent for enhanced
interpretability, and a Translator Agent for multilingual adaptation. The
system achieves seamless integration of structured and unstructured information
with AI-driven citation verification, ensuring high transparency and
reliability. GreenIQ delivers a 99.2\% reduction in processing time and a
99.7\% cost reduction compared to traditional research methodologies. A novel
AI persona-based evaluation framework involving 16 domain-specific AI personas
highlights its superior cross-jurisdictional analytical capabilities and
regulatory insight generation. GreenIQ sets new standards in AI-driven research
synthesis, policy analysis, and sustainability finance by streamlining carbon
market research. It offers an efficient and scalable framework for
environmental and financial intelligence, enabling more accurate, timely, and
cost-effective decision-making in complex regulatory landscapes
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 11:19:43 GMT'}]",2025-03-21,"[['Kayode', 'Bisola Faith', ''], ['Akintola', 'Akinyemi Sadeeq', ''], ['Fagbohun', 'Oluwole', ''], ['Anaesiuba-Bristol', 'Egonna', ''], ['Ojumah', 'Onyekachukwu', ''], ['Odimayo', 'Oluwagbade', ''], ['Oloyede', 'Toyese', ''], ['Inyang', 'Aniema', ''], ['Kazeem', 'Teslim', ''], ['Alli', 'Habeeb', ''], ['Offia', 'Udodirim Ibem', ''], ['Amajuoyi', 'Prisca Chinazor', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.16094,Reem Masoud,"Reem I. Masoud, Martin Ferianc, Philip Treleaven, Miguel Rodrigues",Cultural Alignment in Large Language Models Using Soft Prompt Tuning,,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Model (LLM) alignment conventionally relies on supervised
fine-tuning or reinforcement learning based alignment frameworks. These methods
typically require labeled or preference datasets and involve updating model
weights to align the LLM with the training objective or reward model.
Meanwhile, in social sciences such as cross-cultural studies, factor analysis
is widely used to uncover underlying dimensions or latent variables that
explain observed patterns in survey data. The non-differentiable nature of
these measurements deriving from survey data renders the former alignment
methods infeasible for alignment with cultural dimensions. To overcome this, we
propose a parameter efficient strategy that combines soft prompt tuning, which
freezes the model parameters while modifying the input prompt embeddings, with
Differential Evolution (DE), a black-box optimization method for cases where a
differentiable objective is unattainable. This strategy ensures alignment
consistency without the need for preference data or model parameter updates,
significantly enhancing efficiency and mitigating overfitting. Our method
demonstrates significant improvements in LLama-3-8B-Instruct's cultural
dimensions across multiple regions, outperforming both the Naive LLM and the
In-context Learning (ICL) baseline, and effectively bridges computational
models with human cultural nuances.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 12:34:01 GMT'}]",2025-03-21,"[['Masoud', 'Reem I.', ''], ['Ferianc', 'Martin', ''], ['Treleaven', 'Philip', ''], ['Rodrigues', 'Miguel', '']]","[{'text': 'Large Language Model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'soft prompt tuning', 'label': 'Fine-tuning'}, {'text': 'input prompt embeddings', 'label': 'Embedding'}]",Large Language Model,Large Language Model,1.0
2503.16114,Chelse Swoopes,"Chelse Swoopes, Tyler Holloway, Elena L. Glassman","The Impact of Revealing Large Language Model Stochasticity on Trust,
  Reliability, and Anthropomorphization","Accepted and presented at Trust and Reliance in Evolving Human-AI
  Workflows (TREW) Workshop, CHI 2024",,,,cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Interfaces for interacting with large language models (LLMs) are often
designed to mimic human conversations, typically presenting a single response
to user queries. This design choice can obscure the probabilistic and
predictive nature of these models, potentially fostering undue trust and
over-anthropomorphization of the underlying model. In this paper, we
investigate (i) the effect of displaying multiple responses simultaneously as a
countermeasure to these issues, and (ii) how a cognitive support
mechanism-highlighting structural and semantic similarities across
responses-helps users deal with the increased cognitive load of that
intervention. We conducted a within-subjects study in which participants
inspected responses generated by an LLM under three conditions: one response,
ten responses with cognitive support, and ten responses without cognitive
support. Participants then answered questions about workload, trust and
reliance, and anthropomorphization. We conclude by reporting the results of
these studies and discussing future work and design opportunities for future
LLM interfaces.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:00:56 GMT'}]",2025-03-21,"[['Swoopes', 'Chelse', ''], ['Holloway', 'Tyler', ''], ['Glassman', 'Elena L.', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2503.16131,Yingjian Chen,"Feiyang Li, Yingjian Chen, Haoran Liu, Rui Yang, Han Yuan, Yuang
  Jiang, Tianxiao Li, Edison Marrese Taylor, Hossein Rouhizadeh, Yusuke
  Iwasawa, Douglas Teodoro, Yutaka Matsuo and Irene Li","MKG-Rank: Enhancing Large Language Models with Knowledge Graph for
  Multilingual Medical Question Answering",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have shown remarkable progress in medical
question answering (QA), yet their effectiveness remains predominantly limited
to English due to imbalanced multilingual training data and scarce medical
resources for low-resource languages. To address this critical language gap in
medical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking
(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric
LLMs to perform multilingual medical QA. Through a word-level translation
mechanism, our framework efficiently integrates comprehensive English-centric
medical knowledge graphs into LLM reasoning at a low cost, mitigating
cross-lingual semantic distortion and achieving precise medical QA across
language barriers. To enhance efficiency, we introduce caching and multi-angle
ranking strategies to optimize the retrieval process, significantly reducing
response times and prioritizing relevant medical knowledge. Extensive
evaluations on multilingual medical QA benchmarks across Chinese, Japanese,
Korean, and Swahili demonstrate that MKG-Rank consistently outperforms
zero-shot LLMs, achieving maximum 33.89% increase in accuracy, while
maintaining an average retrieval time of only 0.0009 seconds.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:25:03 GMT'}]",2025-03-21,"[['Li', 'Feiyang', ''], ['Chen', 'Yingjian', ''], ['Liu', 'Haoran', ''], ['Yang', 'Rui', ''], ['Yuan', 'Han', ''], ['Jiang', 'Yuang', ''], ['Li', 'Tianxiao', ''], ['Taylor', 'Edison Marrese', ''], ['Rouhizadeh', 'Hossein', ''], ['Iwasawa', 'Yusuke', ''], ['Teodoro', 'Douglas', ''], ['Matsuo', 'Yutaka', ''], ['Li', 'Irene', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.16167,Hong Yi Lin,"Hong Yi Lin, Chunhua Liu, Haoyu Gao, Patanamon Thongtanunam, Christoph
  Treude","CodeReviewQA: The Code Review Comprehension Assessment for Large
  Language Models",,,,,cs.SE cs.CL,http://creativecommons.org/licenses/by/4.0/,"  State-of-the-art large language models (LLMs) have demonstrated impressive
code generation capabilities but struggle with real-world software engineering
tasks, such as revising source code to address code reviews, hindering their
practical use. Code review comments are often implicit, ambiguous, and
colloquial, requiring models to grasp both code and human intent. This
challenge calls for evaluating large language models' ability to bridge both
technical and conversational contexts. While existing work has employed the
automated code refinement (ACR) task to resolve these comments, current
evaluation methods fall short, relying on text matching metrics that provide
limited insight into model failures and remain susceptible to training data
contamination. To address these limitations, we introduce a novel evaluation
benchmark, $\textbf{CodeReviewQA}$ that enables us to conduct fine-grained
assessment of model capabilities and mitigate data contamination risks. In
CodeReviewQA, we decompose the generation task of code refinement into
$\textbf{three essential reasoning steps}$: $\textit{change type recognition}$
(CTR), $\textit{change localisation}$ (CL), and $\textit{solution
identification}$ (SI). Each step is reformulated as multiple-choice questions
with varied difficulty levels, enabling precise assessment of model
capabilities, while mitigating data contamination risks. Our comprehensive
evaluation spans 72 recently released large language models on $\textbf{900
manually curated, high-quality examples}$ across nine programming languages.
Our results show that CodeReviewQA is able to expose specific model weaknesses
in code review comprehension, disentangled from their generative automated code
refinement results.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 14:07:31 GMT'}]",2025-03-21,"[['Lin', 'Hong Yi', ''], ['Liu', 'Chunhua', ''], ['Gao', 'Haoyu', ''], ['Thongtanunam', 'Patanamon', ''], ['Treude', 'Christoph', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2503.16191,Yinon Goldshtein,"Yinon Goldshtein, Gal Perelman, Assaf Schuster, Avi Ostfeld","Large Language Models for Water Distribution Systems Modeling and
  Decision-Making",Accepted to EWRI Congress 2025,,,,cs.AI cs.HC cs.LG,http://creativecommons.org/licenses/by/4.0/,"  The design, operations, and management of water distribution systems (WDS)
involve complex mathematical models. These models are continually improving due
to computational advancements, leading to better decision-making and more
efficient WDS management. However, the significant time and effort required for
modeling, programming, and analyzing results remain substantial challenges.
Another issue is the professional burden, which confines the interaction with
models, databases, and other sophisticated tools to a small group of experts,
thereby causing non-technical stakeholders to depend on these experts or make
decisions without modeling support. Furthermore, explaining model results is
challenging even for experts, as it is often unclear which conditions cause the
model to reach a certain state or recommend a specific policy. The recent
advancements in Large Language Models (LLMs) open doors for a new stage in
human-model interaction. This study proposes a framework of plain language
interactions with hydraulic and water quality models based on LLM-EPANET
architecture. This framework is tested with increasing levels of complexity of
queries to study the ability of LLMs to interact with WDS models, run complex
simulations, and report simulation results. The performance of the proposed
framework is evaluated across several categories of queries and hyper-parameter
configurations, demonstrating its potential to enhance decision-making
processes in WDS management.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 14:39:11 GMT'}]",2025-03-21,"[['Goldshtein', 'Yinon', ''], ['Perelman', 'Gal', ''], ['Schuster', 'Assaf', ''], ['Ostfeld', 'Avi', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLM-EPANET\narchitecture', 'label': 'LLM-based'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.16212,Qizhi Pei,"Qizhi Pei, Lijun Wu, Zhuoshi Pan, Yu Li, Honglin Lin, Chenlin Ming,
  Xin Gao, Conghui He, Rui Yan","MathFusion: Enhancing Mathematic Problem-solving of LLM through
  Instruction Fusion",Work in progress,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have shown impressive progress in mathematical
reasoning. While data augmentation is promising to enhance mathematical
problem-solving ability, current approaches are predominantly limited to
instance-level modifications-such as rephrasing or generating syntactic
variations-which fail to capture and leverage the intrinsic relational
structures inherent in mathematical knowledge. Inspired by human learning
processes, where mathematical proficiency develops through systematic exposure
to interconnected concepts, we introduce MathFusion, a novel framework that
enhances mathematical reasoning through cross-problem instruction synthesis.
MathFusion implements this through three fusion strategies: (1) sequential
fusion, which chains related problems to model solution dependencies; (2)
parallel fusion, which combines analogous problems to reinforce conceptual
understanding; and (3) conditional fusion, which creates context-aware
selective problems to enhance reasoning flexibility. By applying these
strategies, we generate a new dataset, \textbf{MathFusionQA}, followed by
fine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental
results demonstrate that MathFusion achieves substantial improvements in
mathematical reasoning while maintaining high data efficiency, boosting
performance by 18.0 points in accuracy across diverse benchmarks while
requiring only 45K additional synthetic instructions, representing a
substantial improvement over traditional single-instruction approaches. Our
datasets, models, and code are publicly available at
https://github.com/QizhiPei/mathfusion.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 15:00:41 GMT'}]",2025-03-21,"[['Pei', 'Qizhi', ''], ['Wu', 'Lijun', ''], ['Pan', 'Zhuoshi', ''], ['Li', 'Yu', ''], ['Lin', 'Honglin', ''], ['Ming', 'Chenlin', ''], ['Gao', 'Xin', ''], ['He', 'Conghui', ''], ['Yan', 'Rui', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'parallel fusion', 'label': 'contextual Embedding'}, {'text': 'conditional fusion', 'label': 'contextual Embedding'}, {'text': 'Mistral-7B', 'label': 'Llama'}, {'text': 'Llama3-8B', 'label': 'Llama'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.16257,Keda Tao,"Keda Tao, Haoxuan You, Yang Sui, Can Qin, Huan Wang","Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language
  Models",12 pages,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Video large language models (VideoLLMs) have demonstrated the capability to
process longer video inputs and enable complex reasoning and analysis. However,
due to the thousands of visual tokens from the video frames, key-value (KV)
cache can significantly increase memory requirements, becoming a bottleneck for
inference speed and memory usage. KV cache quantization is a widely used
approach to address this problem. In this paper, we find that 2-bit KV
quantization of VideoLLMs can hardly hurt the model performance, while the
limit of KV cache quantization in even lower bits has not been investigated. To
bridge this gap, we introduce VidKV, a plug-and-play KV cache quantization
method to compress the KV cache to lower than 2 bits. Specifically, (1) for
key, we propose a mixed-precision quantization strategy in the channel
dimension, where we perform 2-bit quantization for anomalous channels and 1-bit
quantization combined with FFT for normal channels; (2) for value, we implement
1.58-bit quantization while selectively filtering semantically salient visual
tokens for targeted preservation, for a better trade-off between precision and
model performance. Importantly, our findings suggest that the value cache of
VideoLLMs should be quantized in a per-channel fashion instead of the per-token
fashion proposed by prior KV cache quantization works for LLMs. Empirically,
extensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show
that VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit
precision with almost no performance drop compared to the FP16 counterparts.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 15:52:43 GMT'}]",2025-03-21,"[['Tao', 'Keda', ''], ['You', 'Haoxuan', ''], ['Sui', 'Yang', ''], ['Qin', 'Can', ''], ['Wang', 'Huan', '']]","[{'text': 'Video large language models', 'label': 'Large Language Model'}, {'text': 'VideoLLMs', 'label': 'Large Language Model'}, {'text': 'KV cache quantization', 'label': 'quantisation'}, {'text': '2-bit KV\nquantization', 'label': 'quantisation'}, {'text': 'VideoLLMs', 'label': 'Large Language Model'}, {'text': 'VidKV', 'label': 'quantisation'}, {'text': '2-bit quantization', 'label': 'quantisation'}, {'text': '1-bit\nquantization', 'label': 'quantisation'}, {'text': 'VideoLLMs', 'label': 'Large Language Model'}]",Large Language Model,Video large language models,0.7616418600082397
2503.16356,Ningyu Zhang,"Yunzhi Yao, Jizhan Fang, Jia-Chen Gu, Ningyu Zhang, Shumin Deng,
  Huajun Chen, Nanyun Peng",CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners,Work in progress,,,,cs.CL cs.AI cs.CV cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge Editing (KE) enables the modification of outdated or incorrect
information in large language models (LLMs). While existing KE methods can
update isolated facts, they struggle to generalize these updates to multi-hop
reasoning tasks that depend on the modified knowledge. Through an analysis of
reasoning circuits -- the neural pathways LLMs use for knowledge-based
inference, we observe that current layer-localized KE approaches, such as MEMIT
and WISE, which edit only single or a few model layers, struggle to effectively
incorporate updated information into these reasoning pathways. To address this
limitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method
that enables more effective integration of updated knowledge in LLMs. CaKE
leverages strategically curated data, guided by our circuits-based analysis,
that enforces the model to utilize the modified knowledge, stimulating the
model to develop appropriate reasoning circuits for newly integrated knowledge.
Experimental results show that CaKE enables more accurate and consistent use of
updated knowledge across related reasoning tasks, leading to an average of 20%
improvement in multi-hop reasoning accuracy on MQuAKE dataset compared to
existing KE methods. We release the code and data in
https://github.com/zjunlp/CaKE.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:14:34 GMT'}]",2025-03-21,"[['Yao', 'Yunzhi', ''], ['Fang', 'Jizhan', ''], ['Gu', 'Jia-Chen', ''], ['Zhang', 'Ningyu', ''], ['Deng', 'Shumin', ''], ['Chen', 'Huajun', ''], ['Peng', 'Nanyun', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'reasoning circuits', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'reasoning pathways', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2503.16376,Leyang Wang,Leyang Wang and Joice Lin,"LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial
  Images",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The success of modern machine learning, particularly in facial translation
networks, is highly dependent on the availability of high-quality, paired,
large-scale datasets. However, acquiring sufficient data is often challenging
and costly. Inspired by the recent success of diffusion models in high-quality
image synthesis and advancements in Large Language Models (LLMs), we propose a
novel framework called LLM-assisted Paired Image Generation (LaPIG). This
framework enables the construction of comprehensive, high-quality paired
visible and thermal images using captions generated by LLMs. Our method
encompasses three parts: visible image synthesis with ArcFace embedding,
thermal image translation using Latent Diffusion Models (LDMs), and caption
generation with LLMs. Our approach not only generates multi-view paired visible
and thermal images to increase data diversity but also produces high-quality
paired data while maintaining their identity information. We evaluate our
method on public datasets by comparing it with existing methods, demonstrating
the superiority of LaPIG.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:39:06 GMT'}]",2025-03-21,"[['Wang', 'Leyang', ''], ['Lin', 'Joice', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ArcFace embedding', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'public datasets', 'label': 'Open-source LLMs'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.16385,Yijia Luo,"Yijia Luo, Yulin Song, Xingyao Zhang, Jiaheng Liu, Weixun Wang, GengRu
  Chen, Wenbo Su, Bo Zheng","Deconstructing Long Chain-of-Thought: A Structured Reasoning
  Optimization Framework for Long CoT Distillation",,,,,cs.AI,http://creativecommons.org/publicdomain/zero/1.0/,"  Recent advancements in large language models (LLMs) have demonstrated
remarkable reasoning capabilities through long chain-of-thought (CoT)
reasoning. The R1 distillation scheme has emerged as a promising approach for
training cost-effective models with enhanced reasoning abilities. However, the
underlying mechanisms driving its effectiveness remain unclear. This study
examines the universality of distillation data and identifies key components
that enable the efficient transfer of long-chain reasoning capabilities in LLM
distillation. Our findings reveal that the effectiveness of long CoT reasoning
distillation from teacher models like Qwen-QwQ degrades significantly on
nonhomologous models, challenging the assumed universality of current
distillation methods. To gain deeper insights into the structure and patterns
of long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),
a distillation data enhancement framework. DLCoT consists of three key steps:
(1) data segmentation to decompose complex long CoT structures, (2)
simplification by eliminating unsolvable and redundant solutions, and (3)
optimization of intermediate error states. Our approach significantly improves
model performance and token efficiency, facilitating the development of
high-performance LLMs.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:46:38 GMT'}]",2025-03-21,"[['Luo', 'Yijia', ''], ['Song', 'Yulin', ''], ['Zhang', 'Xingyao', ''], ['Liu', 'Jiaheng', ''], ['Wang', 'Weixun', ''], ['Chen', 'GengRu', ''], ['Su', 'Wenbo', ''], ['Zheng', 'Bo', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'long chain-of-thought', 'label': 'Chain of thought'}, {'text': 'data segmentation', 'label': 'Knowledge distillation'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2503.16401,Guanyu Chen,"Guanyu Chen, Peiyang Wang, Tianren Zhang, Feng Chen","Exploring the Hidden Reasoning Process of Large Language Models by
  Misleading Them",,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) and Vision language models (VLMs) have been able
to perform various forms of reasoning tasks in a wide range of scenarios, but
are they truly engaging in task abstraction and rule-based reasoning beyond
mere memorization and pattern matching? To answer this question, we propose a
novel experimental approach, Misleading Fine-Tuning (MisFT), to examine whether
LLMs/VLMs perform abstract reasoning by altering their original understanding
of fundamental rules. In particular, by constructing a dataset with math
expressions that contradict correct operation principles, we fine-tune the
model to learn those contradictory rules and assess its generalization ability
on different test domains. Through a series of experiments, we find that
current LLMs/VLMs are capable of effectively applying contradictory rules to
solve practical math word problems and math expressions represented by images,
implying the presence of an internal mechanism that abstracts before reasoning.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:54:42 GMT'}]",2025-03-21,"[['Chen', 'Guanyu', ''], ['Wang', 'Peiyang', ''], ['Zhang', 'Tianren', ''], ['Chen', 'Feng', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Vision language models', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'Misleading Fine-Tuning', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2503.16419,Yang Sui,"Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang,
  Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen (Henry) Zhong, Hanjie Chen, Xia
  Hu","Stop Overthinking: A Survey on Efficient Reasoning for Large Language
  Models","Project Website:
  https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have demonstrated remarkable capabilities in
complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as
OpenAI o1 and DeepSeek-R1, have further improved performance in System-2
reasoning domains like mathematics and programming by harnessing supervised
fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the
Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences
improve performance, they also introduce significant computational overhead due
to verbose and redundant outputs, known as the ""overthinking phenomenon"". In
this paper, we provide the first structured survey to systematically
investigate and explore the current progress toward achieving efficient
reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we
categorize existing works into several key directions: (1) model-based
efficient reasoning, which considers optimizing full-length reasoning models
into more concise reasoning models or directly training efficient reasoning
models; (2) reasoning output-based efficient reasoning, which aims to
dynamically reduce reasoning steps and length during inference; (3) input
prompts-based efficient reasoning, which seeks to enhance reasoning efficiency
based on input prompt properties such as difficulty or length control.
Additionally, we introduce the use of efficient data for training reasoning
models, explore the reasoning capabilities of small language models, and
discuss evaluation methods and benchmarking.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:59:38 GMT'}]",2025-03-21,"[['Sui', 'Yang', '', 'Henry'], ['Chuang', 'Yu-Neng', '', 'Henry'], ['Wang', 'Guanchu', '', 'Henry'], ['Zhang', 'Jiamu', '', 'Henry'], ['Zhang', 'Tianyi', '', 'Henry'], ['Yuan', 'Jiayi', '', 'Henry'], ['Liu', 'Hongyi', '', 'Henry'], ['Wen', 'Andrew', '', 'Henry'], ['Shaochen', '', '', 'Henry'], ['Zhong', '', ''], ['Chen', 'Hanjie', ''], ['Hu', 'Xia', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'Large Reasoning Models', 'label': 'Large Language Model'}, {'text': 'OpenAI o1', 'label': 'Open-source LLMs'}, {'text': 'supervised\nfine-tuning (SFT)', 'label': 'Fine-tuning'}, {'text': 'Chain-of-Thought (CoT)', 'label': 'Chain of thought'}, {'text': 'input\nprompts-based', 'label': 'Prompting'}]",Large Language Model,Large Language Models,0.9664971828460693
