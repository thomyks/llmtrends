id,submitter,authors,title,comments,journal-ref,doi,report-no,categories,license,abstract,versions,update_date,authors_parsed,extracted_entities,assigned_concept,matched_keyword,similarity_score
2303.10440,Herv\'e Turlier,"Sacha Ichbiah, Anshuman Sinha, Fabrice Delbary, Herv\'e Turlier","Inverse 3D microscopy rendering for cell shape inference with active
  mesh","11 pages, 9 figures",,,,physics.bio-ph q-bio.QM,http://creativecommons.org/licenses/by-sa/4.0/,"  Traditional methods for biological shape inference, such as deep learning
(DL) and active contour models, face limitations in 3D. DL requires large
labeled datasets, which are difficult to obtain, while active contour models
rely on fine-tuned hyperparameters for intensity attraction and regularization.
We introduce deltaMic, a novel 3D differentiable renderer for fluorescence
microscopy. By leveraging differentiable Fourier-space convolution, deltaMic
accurately models the image formation process, integrating a parameterized
microscope point spread function and a mesh-based object representation. Unlike
DL-based segmentation, it directly optimizes shape and microscopy parameters to
fit real microscopy data, removing the need for large datasets or heuristic
priors. To enhance efficiency, we develop a GPU-accelerated Fourier transform
for triangle meshes, significantly improving speed. We demonstrate deltaMic's
ability to reconstruct cellular shapes from synthetic and real microscopy
images, providing a robust tool for 3D segmentation and biophysical modeling.
This work bridges physics-based rendering with modern optimization techniques,
offering a new paradigm for microscopy image analysis and inverse biophysical
modeling.
","[{'version': 'v1', 'created': 'Sat, 18 Mar 2023 15:45:10 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 21:54:17 GMT'}]",2025-03-19,"[['Ichbiah', 'Sacha', ''], ['Sinha', 'Anshuman', ''], ['Delbary', 'Fabrice', ''], ['Turlier', 'Herv√©', '']]","[{'text': 'fine-tuned hyperparameters', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuned hyperparameters,0.5314788818359375
2307.07748,Richard Lai Lee,"Richard Lee Lai, Jen-Cheng Hou, I-Chun Chern, Kuo-Hsuan Hung, Yi-Ting
  Chen, Mandar Gogate, Tughrul Arslan, Amir Hussain, and Yu Tsao","Audio-Visual Speech Enhancement Using Self-supervised Learning to
  Improve Speech Intelligibility in Cochlear Implant Simulations",,,,,eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Individuals with hearing impairments face challenges in their ability to
comprehend speech, particularly in noisy environments. The aim of this study is
to explore the effectiveness of audio-visual speech enhancement (AVSE) in
enhancing the intelligibility of vocoded speech in cochlear implant (CI)
simulations. Notably, the study focuses on a challenged scenario where there is
limited availability of training data for the AVSE task. To address this
problem, we propose a novel deep neural network framework termed
Self-Supervised Learning-based AVSE (SSL-AVSE). The proposed SSL-AVSE combines
visual cues, such as lip and mouth movements, from the target speakers with
corresponding audio signals. The contextually combined audio and visual data
are then fed into a Transformer-based SSL AV-HuBERT model to extract features,
which are further processed using a BLSTM-based SE model. The results
demonstrate several key findings. Firstly, SSL-AVSE successfully overcomes the
issue of limited data by leveraging the AV-HuBERT model. Secondly, by
fine-tuning the AV-HuBERT model parameters for the target SE task, significant
performance improvements are achieved. Specifically, there is a notable
enhancement in PESQ (Perceptual Evaluation of Speech Quality) from 1.43 to 1.67
and in STOI (Short-Time Objective Intelligibility) from 0.70 to 0.74.
Furthermore, the performance of the SSL-AVSE was evaluated using CI vocoded
speech to assess the intelligibility for CI users. Comparative experimental
outcomes reveal that in the presence of dynamic noises encountered during human
conversations, SSL-AVSE exhibits a substantial improvement. The NCM (Normal
Correlation Matrix) values indicate an increase of 26.5% to 87.2% compared to
the noisy baseline.
","[{'version': 'v1', 'created': 'Sat, 15 Jul 2023 09:05:57 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:04:06 GMT'}]",2025-03-20,"[['Lai', 'Richard Lee', ''], ['Hou', 'Jen-Cheng', ''], ['Chern', 'I-Chun', ''], ['Hung', 'Kuo-Hsuan', ''], ['Chen', 'Yi-Ting', ''], ['Gogate', 'Mandar', ''], ['Arslan', 'Tughrul', ''], ['Hussain', 'Amir', ''], ['Tsao', 'Yu', '']]","[{'text': 'SSL-AVSE', 'label': 'Transformer-based model'}, {'text': 'SSL-AVSE', 'label': 'Transformer-based model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'SSL-AVSE', 'label': 'Transformer-based model'}]",Fine-tuning,fine-tuning,1.0000001192092896
2308.11256,Linjian Meng,"Linjian Meng, Youzhi Zhang, Zhenxing Ge, Shangdong Yang, Tianyu Ding,
  Wenbin Li, Tianpei Yang, Bo An, Yang Gao",Efficient Last-iterate Convergence Algorithms in Solving Games,,,,,cs.GT cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  To establish last-iterate convergence for Counterfactual Regret Minimization
(CFR) algorithms in learning a Nash equilibrium (NE) of extensive-form games
(EFGs), recent studies reformulate learning an NE of the original EFG as
learning the NEs of a sequence of (perturbed) regularized EFGs. Consequently,
proving last-iterate convergence in solving the original EFG reduces to proving
last-iterate convergence in solving (perturbed) regularized EFGs. However, the
empirical convergence rates of the algorithms in these studies are suboptimal,
since they do not utilize Regret Matching (RM)-based CFR algorithms to solve
perturbed EFGs, which are known the exceptionally fast empirical convergence
rates. Additionally, since solving multiple perturbed regularized EFGs is
required, fine-tuning across all such games is infeasible, making
parameter-free algorithms highly desirable. In this paper, we prove that
CFR$^+$, a classical parameter-free RM-based CFR algorithm, achieves
last-iterate convergence in learning an NE of perturbed regularized EFGs.
Leveraging CFR$^+$ to solve perturbed regularized EFGs, we get Reward
Transformation CFR$^+$ (RTCFR$^+$). Importantly, we extend prior work on the
parameter-free property of CFR$^+$, enhancing its stability, which is crucial
for the empirical convergence of RTCFR$^+$. Experiments show that RTCFR$^+$
significantly outperforms existing algorithms with theoretical last-iterate
convergence guarantees.
","[{'version': 'v1', 'created': 'Tue, 22 Aug 2023 07:59:49 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 08:31:00 GMT'}]",2025-03-19,"[['Meng', 'Linjian', ''], ['Zhang', 'Youzhi', ''], ['Ge', 'Zhenxing', ''], ['Yang', 'Shangdong', ''], ['Ding', 'Tianyu', ''], ['Li', 'Wenbin', ''], ['Yang', 'Tianpei', ''], ['An', 'Bo', ''], ['Gao', 'Yang', '']]","[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2309.17211,Lukas Meiner,"Lukas Meiner, Jens Mehnert, Alexandru Paul Condurache",Data-Free Dynamic Compression of CNNs for Tractable Efficiency,Accepted at VISAPP 2025,,10.5220/0013301000003912,,cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  To reduce the computational cost of convolutional neural networks (CNNs) on
resource-constrained devices, structured pruning approaches have shown promise
in lowering floating-point operations (FLOPs) without substantial drops in
accuracy. However, most methods require fine-tuning or specific training
procedures to achieve a reasonable trade-off between retained accuracy and
reduction in FLOPs, adding computational overhead and requiring training data
to be available. To this end, we propose HASTE (Hashing for Tractable
Efficiency), a data-free, plug-and-play convolution module that instantly
reduces a network's test-time inference cost without training or fine-tuning.
Our approach utilizes locality-sensitive hashing (LSH) to detect redundancies
in the channel dimension of latent feature maps, compressing similar channels
to reduce input and filter depth simultaneously, resulting in cheaper
convolutions. We demonstrate our approach on the popular vision benchmarks
CIFAR-10 and ImageNet, where we achieve a 46.72% reduction in FLOPs with only a
1.25% loss in accuracy by swapping the convolution modules in a ResNet34 on
CIFAR-10 for our HASTE module.
","[{'version': 'v1', 'created': 'Fri, 29 Sep 2023 13:09:40 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:35:38 GMT'}]",2025-03-20,"[['Meiner', 'Lukas', ''], ['Mehnert', 'Jens', ''], ['Condurache', 'Alexandru Paul', '']]","[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2312.09853,Jeffrey Kuntz,"Jeffrey Kuntz, Andreas Trautner",Extra Dimensions Beyond the Horizon,"v2: discussions added, 29+5 pages, 2 figures",,,,hep-ph astro-ph.CO gr-qc hep-th,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We discuss an extra-dimensional braneworld with a 5th dimension compactified
on a circle. As a characteristic feature, the warp factor is hyperbolic and
separates the hidden and visible branes by a bulk horizon without a
singularity. The two most widely separated scales of 4D physics - the 4D Planck
mass and 4D cosmological constant - are determined by two physical scales in
the extra dimension, namely: $(i)$ the proper size of the extra dimension, $R$,
and, $(ii)$ the distance between the visible brane and the horizon, $R_0$. A
realistic scale hierarchy between 4D Planck mass and 4D cosmological constant
is obtained for $R/R_0\sim2.34$. The usual fine tuning is not reduced but
promoted to a fine tuning of two separate brane energy densities that must
approach the fundamental scale of the model with very high precision. Our
scenario is based on an exact solution to the 5D Einstein equations with a
strictly empty bulk and Friedmann-Lema\^itre-Robertson-Walker metric on the 4D
branes. This requires positive 4D brane energy densities and describes an
adiabatic runaway solution in agreement with the de Sitter swampland
conjecture. The Kaluza-Klein (KK) graviton states are solutions of a modified
P\""oschl-Teller potential which permits a discrete graviton spectrum of exactly
two modes. In addition to the usual massless graviton, our scenario predicts an
extra massive spin-2 graviton with a mass gap of
$m_1=\sqrt{2}H_0\approx2\times10^{-33}\,\mathrm{eV}$ which might be detectable
in the foreseeable future. A KK tower of gravitons, or a possible continuum of
massive graviton states, is prohibited by unitarity with respect to the
horizon. We discuss hurdles in turning this model into a realistic cosmology at
all times, which points us towards 4D brane tensions that that must be raising
towards the fundamental scale of the model, while the observable 4D expansion
rate is decreasing.
","[{'version': 'v1', 'created': 'Fri, 15 Dec 2023 15:00:46 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 10:13:23 GMT'}]",2025-03-20,"[['Kuntz', 'Jeffrey', ''], ['Trautner', 'Andreas', '']]","[{'text': 'fine tuning', 'label': 'Fine-tuning'}, {'text': 'fine tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine tuning,0.9617297649383545
2402.14598,Depin Liang,"Jianming Lv, Chengjun Wang, Depin Liang, Qianli Ma, Wei Chen, Xueqi
  Cheng","EMN: Brain-inspired Elastic Memory Network for Quick Domain Adaptive
  Feature Mapping","15 pages,15 figures",,,,cs.NE cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Utilizing unlabeled data in the target domain to perform continuous
optimization is critical to enhance the generalization ability of neural
networks. Most domain adaptation methods focus on time-consuming optimization
of deep feature extractors, which limits the deployment on lightweight edge
devices. Inspired by the memory mechanism and powerful generalization ability
of biological neural networks in human brains, we propose a novel gradient-free
Elastic Memory Network, namely EMN, to support quick fine-tuning of the mapping
between features and prediction without heavy optimization of deep features. In
particular, EMN adopts randomly connected neurons to memorize the association
of features and labels, where the signals in the network are propagated as
impulses, and the prediction is made by associating the memories stored on
neurons based on their confidence. More importantly, EMN supports reinforced
memorization of feature mapping based on unlabeled data to quickly adapt to a
new domain. Experiments based on four cross-domain real-world datasets show
that EMN can achieve up to 10% enhancement of performance while only needing
less than 1% timing cost of traditional domain adaptation methods.
","[{'version': 'v1', 'created': 'Sun, 4 Feb 2024 09:58:17 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 08:34:07 GMT'}]",2025-03-18,"[['Lv', 'Jianming', ''], ['Wang', 'Chengjun', ''], ['Liang', 'Depin', ''], ['Ma', 'Qianli', ''], ['Chen', 'Wei', ''], ['Cheng', 'Xueqi', '']]","[{'text': 'memory mechanism', 'label': 'Attention mechanism'}, {'text': 'quick fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,quick fine-tuning,0.9181714057922363
2402.15216,Yongzhi Huang,"Yongzhi Huang, Fengjun Xi, Liyun Tu, Jinxin Zhu, Haseeb Hassan,
  Liyilei Su, Yun Peng, Jingyu Li, Jun Ma, Bingding Huang",Label-efficient multi-organ segmentation with a diffusion model,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Accurate segmentation of multiple organs in Computed Tomography (CT) images
plays a vital role in computer-aided diagnosis systems. While various
supervised learning approaches have been proposed recently, these methods
heavily depend on a large amount of high-quality labeled data, which are
expensive to obtain in practice. To address this challenge, we propose a
label-efficient framework using knowledge transfer from a pre-trained diffusion
model for CT multi-organ segmentation. Specifically, we first pre-train a
denoising diffusion model on 207,029 unlabeled 2D CT slices to capture
anatomical patterns. Then, the model backbone is transferred to the downstream
multi-organ segmentation task, followed by fine-tuning with few labeled data.
In fine-tuning, two fine-tuning strategies, linear classification and
fine-tuning decoder, are employed to enhance segmentation performance while
preserving learned representations. Quantitative results show that the
pre-trained diffusion model is capable of generating diverse and realistic
256x256 CT images (Fr\'echet inception distance (FID): 11.32, spatial Fr\'echet
inception distance (sFID): 46.93, F1-score: 73.1%). Compared to
state-of-the-art methods for multi-organ segmentation, our method achieves
competitive performance on the FLARE 2022 dataset, particularly in limited
labeled data scenarios. After fine-tuning with 1% and 10% labeled data, our
method achieves dice similarity coefficients (DSCs) of 71.56% and 78.51%,
respectively. Remarkably, the method achieves a DSC score of 51.81% using only
four labeled CT slices. These results demonstrate the efficacy of our approach
in overcoming the limitations of supervised learning approaches that is highly
dependent on large-scale labeled data.
","[{'version': 'v1', 'created': 'Fri, 23 Feb 2024 09:25:57 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 02:42:26 GMT'}]",2025-03-21,"[['Huang', 'Yongzhi', ''], ['Xi', 'Fengjun', ''], ['Tu', 'Liyun', ''], ['Zhu', 'Jinxin', ''], ['Hassan', 'Haseeb', ''], ['Su', 'Liyilei', ''], ['Peng', 'Yun', ''], ['Li', 'Jingyu', ''], ['Ma', 'Jun', ''], ['Huang', 'Bingding', '']]","[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2403.04343,Yanqi Dai,"Yanqi Dai, Zebin You, Dong Jing, Yutian Luo, Nanyi Fei, Guoxing Yang,
  Zhiwu Lu","CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction
  Tuning",,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Visual instruction tuning is an important training stage for large multimodal
models. Nevertheless, when learning multiple visual tasks simultaneously, this
approach may lead to suboptimal and imbalanced overall performance due to
latent knowledge conflicts across tasks. To mitigate this issue, we introduce a
novel Comprehensive Task Balancing (CoTBal) algorithm tailored for multi-task
visual instruction tuning. To our knowledge, this is the first work to explore
multi-task optimization in visual instruction tuning. Specifically, we consider
two critical dimensions for task balancing: (1) Inter-Task Contribution, which
represents the phenomenon where learning one task could enhance the performance
on others owing to the overlapping knowledge domains across tasks, and (2)
Intra-Task Difficulty, which indicates the inherent learning difficulty of a
single task. Furthermore, by quantifying these with performance-based metrics,
comprehensive task balancing is thus achieved by assigning greater weight to
tasks that offer substantial contributions to others, receive minimal
contributions from others, and present high learning difficulties. Extensive
experiments on three benchmarks demonstrate that our CoTBal algorithm results
in superior and more balanced overall performance in multi-task visual
instruction tuning.
","[{'version': 'v1', 'created': 'Thu, 7 Mar 2024 09:11:16 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 03:24:54 GMT'}]",2025-03-19,"[['Dai', 'Yanqi', ''], ['You', 'Zebin', ''], ['Jing', 'Dong', ''], ['Luo', 'Yutian', ''], ['Fei', 'Nanyi', ''], ['Yang', 'Guoxing', ''], ['Lu', 'Zhiwu', '']]","[{'text': 'Visual instruction tuning', 'label': 'Fine-tuning'}, {'text': 'visual instruction tuning', 'label': 'Fine-tuning'}, {'text': 'visual instruction tuning', 'label': 'Fine-tuning'}, {'text': 'visual\ninstruction tuning', 'label': 'Fine-tuning'}]",Fine-tuning,Visual instruction tuning,0.530214786529541
2403.12029,Justin Kay,"Justin Kay, Timm Haucke, Suzanne Stathatos, Siqi Deng, Erik Young,
  Pietro Perona, Sara Beery, Grant Van Horn","Align and Distill: Unifying and Improving Domain Adaptive Object
  Detection","TMLR camera ready (Featured Certification). 33 pages, 15 figures",,,,cs.CV cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Object detectors often perform poorly on data that differs from their
training set. Domain adaptive object detection (DAOD) methods have recently
demonstrated strong results on addressing this challenge. Unfortunately, we
identify systemic benchmarking pitfalls that call past results into question
and hamper further progress: (a) Overestimation of performance due to
underpowered baselines, (b) Inconsistent implementation practices preventing
transparent comparisons of methods, and (c) Lack of generality due to outdated
backbones and lack of diversity in benchmarks. We address these problems by
introducing: (1) A unified benchmarking and implementation framework, Align and
Distill (ALDI), enabling comparison of DAOD methods and supporting future
development, (2) A fair and modern training and evaluation protocol for DAOD
that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset,
CFC-DAOD, enabling evaluation on diverse real-world data, and (4) A new method,
ALDI++, that achieves state-of-the-art results by a large margin. ALDI++
outperforms the previous state-of-the-art by +3.5 AP50 on Cityscapes to Foggy
Cityscapes, +5.7 AP50 on Sim10k to Cityscapes (where ours is the only method to
outperform a fair baseline), and +0.6 AP50 on CFC Kenai to Channel. ALDI and
ALDI++ are architecture-agnostic, setting a new state-of-the-art for YOLO and
DETR-based DAOD as well without additional hyperparameter tuning. Our
framework, dataset, and state-of-the-art method offer a critical reset for DAOD
and provide a strong foundation for future research. Code and data are
available: https://github.com/justinkay/aldi and
https://github.com/visipedia/caltech-fish-counting.
","[{'version': 'v1', 'created': 'Mon, 18 Mar 2024 17:58:02 GMT'}, {'version': 'v2', 'created': 'Sun, 25 Aug 2024 14:05:18 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 20:18:16 GMT'}]",2025-03-19,"[['Kay', 'Justin', ''], ['Haucke', 'Timm', ''], ['Stathatos', 'Suzanne', ''], ['Deng', 'Siqi', ''], ['Young', 'Erik', ''], ['Perona', 'Pietro', ''], ['Beery', 'Sara', ''], ['Van Horn', 'Grant', '']]","[{'text': 'hyperparameter tuning', 'label': 'Fine-tuning'}]",Fine-tuning,hyperparameter tuning,0.6193697452545166
2404.04858,Tony Lindeberg,Tony Lindeberg,"Do the receptive fields in the primary visual cortex span a variability
  over the degree of elongation of the receptive fields?","22 pages, 12 figures. Note: Companion paper regarding theoretical
  analysis in arXiv:2304.11920",,,,q-bio.NC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents results of combining (i) theoretical analysis regarding
connections between the orientation selectivity and the elongation of receptive
fields for the affine Gaussian derivative model with (ii) biological
measurements of orientation selectivity in the primary visual cortex, to
investigate if (iii) the receptive fields can be regarded as spanning a
variability in the degree of elongation.
  From an in-depth theoretical analysis of idealized models for the receptive
fields of simple and complex cells in the primary visual cortex, we have
established that the directional selectivity becomes more narrow with
increasing elongation of the receptive fields. By comparison with previously
established biological results, concerning broad vs. sharp orientation tuning
of visual neurons in the primary visual cortex, we demonstrate that those
underlying theoretical predictions, in combination with these biological
results, are consistent with a previously formulated biological hypothesis,
stating that the biological receptive field shapes should span the degrees of
freedom in affine image transformations, to support affine covariance over the
population of receptive fields in the primary visual cortex.
  Based on this possible indirect support for the working hypothesis concerning
affine covariance, we formulate a set of testable predictions that could be
used to, with neurophysiological experiments, judge if the receptive fields in
the primary visual cortex of higher mammals could be regarded as spanning a
variability over the eccentricity or the elongation of the receptive fields,
and, if so, then also characterize if such a variability would, in a structured
way, be related to the pinwheel structure in the visual cortex.
","[{'version': 'v1', 'created': 'Sun, 7 Apr 2024 08:06:12 GMT'}, {'version': 'v2', 'created': 'Tue, 9 Apr 2024 05:29:13 GMT'}, {'version': 'v3', 'created': 'Thu, 11 Apr 2024 10:44:55 GMT'}, {'version': 'v4', 'created': 'Fri, 3 May 2024 05:03:37 GMT'}, {'version': 'v5', 'created': 'Tue, 21 May 2024 14:15:04 GMT'}, {'version': 'v6', 'created': 'Mon, 10 Jun 2024 06:52:07 GMT'}, {'version': 'v7', 'created': 'Wed, 2 Oct 2024 14:43:28 GMT'}, {'version': 'v8', 'created': 'Tue, 22 Oct 2024 12:36:36 GMT'}, {'version': 'v9', 'created': 'Tue, 18 Mar 2025 15:14:32 GMT'}]",2025-03-19,"[['Lindeberg', 'Tony', '']]","[{'text': 'broad vs. sharp orientation tuning', 'label': 'Fine-tuning'}]",Fine-tuning,broad vs. sharp orientation tuning,0.6353417634963989
2404.07696,Rui Li,"Rui Li, Martin Trapp, Marcus Klasson, Arno Solin",Flatness Improves Backbone Generalisation in Few-shot Classification,,,,,cs.LG cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Deployment of deep neural networks in real-world settings typically requires
adaptation to new tasks with few examples. Few-shot classification (FSC)
provides a solution to this problem by leveraging pre-trained backbones for
fast adaptation to new classes. However, approaches for multi-domain FSC
typically result in complex pipelines aimed at information fusion and
task-specific adaptation without consideration of the importance of backbone
training. In this work, we introduce an effective strategy for backbone
training and selection in multi-domain FSC by utilizing flatness-aware training
and fine-tuning. Our work is theoretically grounded and empirically performs on
par or better than state-of-the-art methods despite being simpler. Further, our
results indicate that backbone training is crucial for good generalisation in
FSC across different adaptation methods.
","[{'version': 'v1', 'created': 'Thu, 11 Apr 2024 12:42:18 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 11:19:45 GMT'}]",2025-03-19,"[['Li', 'Rui', ''], ['Trapp', 'Martin', ''], ['Klasson', 'Marcus', ''], ['Solin', 'Arno', '']]","[{'text': 'Few-shot classification', 'label': 'Zero-shot Learning'}, {'text': 'FSC', 'label': 'Zero-shot Learning'}, {'text': 'FSC', 'label': 'Zero-shot Learning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'FSC', 'label': 'Zero-shot Learning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2404.15305,Hyungjun Yoon,"Hyungjun Yoon, Jaehyun Kwak, Biniyam Aschalew Tolera, Gaole Dai, Mo
  Li, Taesik Gong, Kimin Lee and Sung-Ju Lee","SelfReplay: Adapting Self-Supervised Sensory Models via Adaptive
  Meta-Task Replay","Accepted to the 23rd ACM Conference on Embedded Networked Sensor
  Systems (ACM SenSys 2025)",,,,eess.SP cs.LG,http://creativecommons.org/publicdomain/zero/1.0/,"  Self-supervised learning has emerged as a method for utilizing massive
unlabeled data for pre-training models, providing an effective feature
extractor for various mobile sensing applications. However, when deployed to
end-users, these models encounter significant domain shifts attributed to user
diversity. We investigate the performance degradation that occurs when
self-supervised models are fine-tuned in heterogeneous domains. To address the
issue, we propose SelfReplay, a few-shot domain adaptation framework for
personalizing self-supervised models. SelfReplay proposes self-supervised
meta-learning for initial model pre-training, followed by a user-side model
adaptation by replaying the self-supervision with user-specific data. This
allows models to adjust their pre-trained representations to the user with only
a few samples. Evaluation with four benchmarks demonstrates that SelfReplay
outperforms existing baselines by an average F1-score of 8.8%p. Our on-device
computational overhead analysis on a commodity off-the-shelf (COTS) smartphone
shows that SelfReplay completes adaptation within an unobtrusive latency (in
three minutes) with only a 9.54% memory consumption, demonstrating the
computational efficiency of the proposed method.
","[{'version': 'v1', 'created': 'Fri, 29 Mar 2024 08:48:07 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 11:56:18 GMT'}]",2025-03-21,"[['Yoon', 'Hyungjun', ''], ['Kwak', 'Jaehyun', ''], ['Tolera', 'Biniyam Aschalew', ''], ['Dai', 'Gaole', ''], ['Li', 'Mo', ''], ['Gong', 'Taesik', ''], ['Lee', 'Kimin', ''], ['Lee', 'Sung-Ju', '']]","[{'text': 'Self-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuned', 'label': 'Fine-tuning'}, {'text': 'self-supervised\nmeta-learning', 'label': 'Few-shot Learning'}]",Fine-tuning,fine-tuned,0.870777428150177
2405.13637,Radu Tudor Ionescu,"Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Nicu Sebe,
  Mubarak Shah","Curriculum Direct Preference Optimization for Diffusion and Consistency
  Models",Accepted at CVPR 2025,,,,cs.CV cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Direct Preference Optimization (DPO) has been proposed as an effective and
efficient alternative to reinforcement learning from human feedback (RLHF). In
this paper, we propose a novel and enhanced version of DPO based on curriculum
learning for text-to-image generation. Our method is divided into two training
stages. First, a ranking of the examples generated for each prompt is obtained
by employing a reward model. Then, increasingly difficult pairs of examples are
sampled and provided to a text-to-image generative (diffusion or consistency)
model. Generated samples that are far apart in the ranking are considered to
form easy pairs, while those that are close in the ranking form hard pairs. In
other words, we use the rank difference between samples as a measure of
difficulty. The sampled pairs are split into batches according to their
difficulty levels, which are gradually used to train the generative model. Our
approach, Curriculum DPO, is compared against state-of-the-art fine-tuning
approaches on nine benchmarks, outperforming the competing methods in terms of
text alignment, aesthetics and human preference. Our code is available at
https://github.com/CroitoruAlin/Curriculum-DPO.
","[{'version': 'v1', 'created': 'Wed, 22 May 2024 13:36:48 GMT'}, {'version': 'v2', 'created': 'Fri, 24 May 2024 13:14:40 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 16:44:48 GMT'}, {'version': 'v4', 'created': 'Tue, 18 Mar 2025 13:03:48 GMT'}]",2025-03-19,"[['Croitoru', 'Florinel-Alin', ''], ['Hondru', 'Vlad', ''], ['Ionescu', 'Radu Tudor', ''], ['Sebe', 'Nicu', ''], ['Shah', 'Mubarak', '']]","[{'text': 'prompt', 'label': 'Prompting'}, {'text': 'state-of-the-art fine-tuning\napproaches', 'label': 'Fine-tuning'}]",Fine-tuning,"state-of-the-art fine-tuning
approaches",0.7898973226547241
2406.01445,Yifei Bai,Yifei Bai and David M. Weld,"Tunably-polarized driving light controls the phase diagram of 1D
  quasicrystals and 2D quantum Hall matter",Accepted version (PRB). Comments are welcome!,,,,physics.atom-ph cond-mat.dis-nn cond-mat.quant-gas,http://creativecommons.org/licenses/by/4.0/,"  The well-known mapping between 1D quasiperiodic systems and 2D integer
quantum Hall matter can also be applied in the presence of driving. Here we
explore the effect of time-varying electric fields on the transport properties
and phase diagram of Harper-Hofstadter materials. We consider light of
arbitrary polarization illuminating a 2D electron gas at high magnetic field;
this system maps to a 1D quasicrystal subjected to simultaneous phasonic and
dipolar driving. We show that this generalized driving generates a tessellated
phase diagram featuring a nested duality-protected pattern of metal-insulator
transitions. Circularly or elliptically polarized light can create an extended
critical phase, opening up a new route to achieving wavefunction
multifractality without fine-tuning to a critical point. We describe in detail
a path to experimental realization of these phenomena using lattice-trapped
ultracold atoms.
","[{'version': 'v1', 'created': 'Mon, 3 Jun 2024 15:36:05 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Jun 2024 17:40:27 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 19:02:47 GMT'}]",2025-03-20,"[['Bai', 'Yifei', ''], ['Weld', 'David M.', '']]","[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2407.01509,Yusu Qian,"Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei
  Yang, Zhe Gan","MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal
  LLMs",Accepted at ICLR 2025,,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce MIA-Bench, a new benchmark designed to evaluate multimodal large
language models (MLLMs) on their ability to strictly adhere to complex
instructions. Our benchmark comprises a diverse set of 400 image-prompt pairs,
each crafted to challenge the models' compliance with layered instructions in
generating accurate responses that satisfy specific requested patterns.
Evaluation results from a wide array of state-of-the-art MLLMs reveal
significant variations in performance, highlighting areas for improvement in
instruction fidelity. Additionally, we create extra training data and explore
supervised fine-tuning to enhance the models' ability to strictly follow
instructions without compromising performance on other tasks. We hope this
benchmark not only serves as a tool for measuring MLLM adherence to
instructions, but also guides future developments in MLLM training methods.
","[{'version': 'v1', 'created': 'Mon, 1 Jul 2024 17:53:35 GMT'}, {'version': 'v2', 'created': 'Wed, 3 Jul 2024 18:11:45 GMT'}, {'version': 'v3', 'created': 'Thu, 25 Jul 2024 19:50:32 GMT'}, {'version': 'v4', 'created': 'Fri, 21 Feb 2025 03:49:13 GMT'}, {'version': 'v5', 'created': 'Thu, 20 Mar 2025 02:49:09 GMT'}]",2025-03-21,"[['Qian', 'Yusu', ''], ['Ye', 'Hanrong', ''], ['Fauconnier', 'Jean-Philippe', ''], ['Grasch', 'Peter', ''], ['Yang', 'Yinfei', ''], ['Gan', 'Zhe', '']]","[{'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,supervised fine-tuning,0.7449287176132202
2407.04194,Weihao Li,"Weihao Li, Dongming Huang",Regularization Using Synthetic Data in High-Dimensional Models,"98 pages, 12 figures",,,,math.ST stat.TH,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  To address the challenges of reliable statistical inference in
high-dimensional models, we introduce the Synthetic-data Regularized Estimator
(SRE). Unlike traditional regularization methods, the SRE regularizes the
complex target model via a weighted likelihood based on synthetic data
generated from a simpler, more stable model. This method provides a
theoretically sound and practically effective alternative to parameter
penalization. We establish key theoretical properties of the SRE in generalized
linear models, including existence, stability, consistency, and minimax rate
optimality. Applying the Convex Gaussian Min-Max Theorem, we derive a precise
asymptotic characterization in the high-dimensional linear regime. To deal with
the non-separable regularization, we introduce a novel decomposition in our
analysis. Building upon these results, we develop practical methodologies for
tuning parameter selection, confidence interval construction, and calibrated
variable selection in high-dimensional inference. The effectiveness of the SRE
is demonstrated through simulation studies and real-data applications.
","[{'version': 'v1', 'created': 'Fri, 5 Jul 2024 00:40:03 GMT'}, {'version': 'v2', 'created': 'Tue, 21 Jan 2025 07:41:54 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Feb 2025 09:19:17 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 14:33:10 GMT'}]",2025-03-18,"[['Li', 'Weihao', ''], ['Huang', 'Dongming', '']]","[{'text': 'tuning parameter selection', 'label': 'Fine-tuning'}]",Fine-tuning,tuning parameter selection,0.6054506301879883
2407.07066,Fardin Jalil Piran,"Fardin Jalil Piran and Prathyush P. Poduval and Hamza Errahmouni
  Barkam and Mohsen Imani and Farhad Imani","Explainable Differential Privacy-Hyperdimensional Computing for
  Balancing Privacy and Transparency in Additive Manufacturing Monitoring","30 pages, 14 figures",,10.1016/j.engappai.2025.110282,,cs.LG cs.CR cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine Learning (ML) models integrated with in-situ sensing offer
transformative solutions for defect detection in Additive Manufacturing (AM),
but this integration brings critical challenges in safeguarding sensitive data,
such as part designs and material compositions. Differential Privacy (DP),
which introduces mathematically controlled noise, provides a balance between
data utility and privacy. However, black-box Artificial Intelligence (AI)
models often obscure how this noise impacts model accuracy, complicating the
optimization of privacy-accuracy trade-offs. This study introduces the
Differential Privacy-Hyperdimensional Computing (DP-HD) framework, a novel
approach combining Explainable AI (XAI) and vector symbolic paradigms to
quantify and predict noise effects on accuracy using a Signal-to-Noise Ratio
(SNR) metric. DP-HD enables precise tuning of DP noise levels, ensuring an
optimal balance between privacy and performance. The framework has been
validated using real-world AM data, demonstrating its applicability to
industrial environments. Experimental results demonstrate DP-HD's capability to
achieve state-of-the-art accuracy (94.43%) with robust privacy protections in
anomaly detection for AM, even under significant noise conditions. Beyond AM,
DP-HD holds substantial promise for broader applications in privacy-sensitive
domains such as healthcare, financial services, and government data management,
where securing sensitive data while maintaining high ML performance is
paramount.
","[{'version': 'v1', 'created': 'Tue, 9 Jul 2024 17:42:26 GMT'}, {'version': 'v2', 'created': 'Wed, 10 Jul 2024 01:37:05 GMT'}, {'version': 'v3', 'created': 'Thu, 14 Nov 2024 20:13:19 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 21:17:59 GMT'}]",2025-03-19,"[['Piran', 'Fardin Jalil', ''], ['Poduval', 'Prathyush P.', ''], ['Barkam', 'Hamza Errahmouni', ''], ['Imani', 'Mohsen', ''], ['Imani', 'Farhad', '']]","[{'text': 'precise tuning', 'label': 'Fine-tuning'}]",Fine-tuning,precise tuning,0.7851148247718811
2407.20642,Basura Fernando,"Dhruv Verma, Debaditya Roy, Basura Fernando","Effectively Leveraging CLIP for Generating Situational Summaries of
  Images and Videos","38 pages, 12 figures. arXiv admin note: text overlap with
  arXiv:2307.00586",,,,cs.CV,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Situation recognition refers to the ability of an agent to identify and
understand various situations or contexts based on available information and
sensory inputs. It involves the cognitive process of interpreting data from the
environment to determine what is happening, what factors are involved, and what
actions caused those situations. This interpretation of situations is
formulated as a semantic role labeling problem in computer vision-based
situation recognition. Situations depicted in images and videos hold pivotal
information, essential for various applications like image and video
captioning, multimedia retrieval, autonomous systems and event monitoring.
However, existing methods often struggle with ambiguity and lack of context in
generating meaningful and accurate predictions. Leveraging multimodal models
such as CLIP, we propose ClipSitu, which sidesteps the need for full
fine-tuning and achieves state-of-the-art results in situation recognition and
localization tasks. ClipSitu harnesses CLIP-based image, verb, and role
embeddings to predict nouns fulfilling all the roles associated with a verb,
providing a comprehensive understanding of depicted scenarios. Through a
cross-attention Transformer, ClipSitu XTF enhances the connection between
semantic role queries and visual token representations, leading to superior
performance in situation recognition. We also propose a verb-wise role
prediction model with near-perfect accuracy to create an end-to-end framework
for producing situational summaries for out-of-domain images. We show that
situational summaries empower our ClipSitu models to produce structured
descriptions with reduced ambiguity compared to generic captions. Finally, we
extend ClipSitu to video situation recognition to showcase its versatility and
produce comparable performance to state-of-the-art methods.
","[{'version': 'v1', 'created': 'Tue, 30 Jul 2024 08:39:20 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 04:14:55 GMT'}]",2025-03-19,"[['Verma', 'Dhruv', ''], ['Roy', 'Debaditya', ''], ['Fernando', 'Basura', '']]","[{'text': 'full\nfine-tuning', 'label': 'Fine-tuning'}, {'text': 'role\nembeddings', 'label': 'contextual Embedding'}]",Fine-tuning,"full
fine-tuning",0.956924319267273
2408.03394,Zhaoxin Li,"Zhaoxin Li, Xiaoke Wang, Letian Chen, Rohan Paleja, Subramanya
  Nageshrao, Matthew Gombolay","Faster Model Predictive Control via Self-Supervised Initialization
  Learning",,,,,cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Optimization for robot control tasks, spanning various methodologies,
includes Model Predictive Control (MPC). However, the complexity of the system,
such as non-convex and non-differentiable cost functions and prolonged planning
horizons often drastically increases the computation time, limiting MPC's
real-world applicability. Prior works in speeding up the optimization have
limitations on optimizing MPC running time directly and generalizing to hold
out domains. To overcome this challenge, we develop a novel framework aiming at
expediting optimization processes directly. In our framework, we combine
offline self-supervised learning and online fine-tuning to improve the control
performance and reduce optimization time. We demonstrate the success of our
method on a novel and challenging Formula 1 track driving task. Comparing to
single-phase training, our approach achieves a 19.4\% reduction in optimization
time and a 6.3\% improvement in tracking accuracy on zero-shot tracks.
","[{'version': 'v1', 'created': 'Tue, 6 Aug 2024 18:41:57 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:36:18 GMT'}]",2025-03-20,"[['Li', 'Zhaoxin', ''], ['Wang', 'Xiaoke', ''], ['Chen', 'Letian', ''], ['Paleja', 'Rohan', ''], ['Nageshrao', 'Subramanya', ''], ['Gombolay', 'Matthew', '']]","[{'text': 'offline self-supervised learning', 'label': 'Zero-shot Learning'}, {'text': 'online fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,online fine-tuning,0.7261751890182495
2408.06663,Kaiser Sun,"Kaiser Sun, Mark Dredze","Amuro and Char: Analyzing the Relationship between Pre-Training and
  Fine-Tuning of Large Language Models",Rep4NLP Camera Ready,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The development of large language models leads to the formation of a
pre-train-then-align paradigm, in which the model is typically pre-trained on a
large text corpus and undergoes a tuning stage to align the model with human
preference or downstream tasks. In this work, we investigate the relationship
between pre-training and fine-tuning by fine-tuning multiple intermediate
pre-trained model checkpoints. Our results on 18 datasets suggest that i)
continual pre-training improves the model in a latent way that unveils after
fine-tuning; ii) with extra fine-tuning, the datasets that the model does not
demonstrate capability gain much more than those that the model performs well
during the pre-training stage; iii) although model benefits significantly
through supervised fine-tuning, it may forget previously known domain knowledge
and the tasks that are not seen during fine-tuning; iv) the model resembles
high sensitivity to evaluation prompts after supervised fine-tuning, but this
sensitivity can be alleviated by more pre-training.
","[{'version': 'v1', 'created': 'Tue, 13 Aug 2024 06:28:43 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Aug 2024 15:23:38 GMT'}, {'version': 'v3', 'created': 'Sun, 2 Feb 2025 22:07:55 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Feb 2025 16:57:29 GMT'}, {'version': 'v5', 'created': 'Tue, 18 Mar 2025 16:21:04 GMT'}]",2025-03-19,"[['Sun', 'Kaiser', ''], ['Dredze', 'Mark', '']]","[{'text': 'model', 'label': 'Neural Language Model'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'evaluation prompts', 'label': 'Prompting'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2408.16750,Bruno Bertini,Alessandro Foligno and Bruno Bertini,"Entanglement of Disjoint Intervals in Dual-Unitary Circuits: Exact
  Results","6+5 pages, 4 figures; v2 11 pages, 4 figures presentation improved;
  v3 version accepted by Quantum",,,,cond-mat.stat-mech hep-th math-ph math.MP quant-ph,http://creativecommons.org/licenses/by/4.0/,"  The growth of the entanglement between two disjoint intervals and its
complement after a quantum quench is regarded as a dynamical chaos indicator.
Namely, it is expected to show qualitatively different behaviours depending on
whether the underlying microscopic dynamics is chaotic or integrable. So far,
however, this could only be verified in the context of conformal field
theories. Here we present an exact confirmation of this expectation in a class
of interacting microscopic Floquet systems on the lattice, i.e., dual-unitary
circuits. These systems can either have zero or a super extensive number of
conserved charges: the latter case is achieved via fine-tuning. We show that,
for almost all dual unitary circuits on qubits and for a large family of
dual-unitary circuits on qudits the asymptotic entanglement dynamics agrees
with what is expected for chaotic systems. On the other hand, if we require the
systems to have conserved charges, we find that the entanglement displays the
qualitatively different behaviour expected for integrable systems.
Interestingly, despite having many conserved charges, charge-conserving
dual-unitary circuits are in general not Yang-Baxter integrable.
","[{'version': 'v1', 'created': 'Thu, 29 Aug 2024 17:45:27 GMT'}, {'version': 'v2', 'created': 'Tue, 19 Nov 2024 14:41:31 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 18:11:09 GMT'}]",2025-03-21,"[['Foligno', 'Alessandro', ''], ['Bertini', 'Bruno', '']]","[{'text': 'entanglement', 'label': 'quantisation'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'entanglement', 'label': 'quantisation'}]",Fine-tuning,fine-tuning,1.0000001192092896
2409.01586,Tiansheng Huang,"Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu","Booster: Tackling Harmful Fine-tuning for Large Language Models via
  Attenuating Harmful Perturbation",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Harmful fine-tuning attack poses serious safety concerns for large language
models' fine-tuning-as-a-service. While existing defenses have been proposed to
mitigate the issue, their performances are still far away from satisfactory,
and the root cause of the problem has not been fully recovered. To this end, we
in this paper show that harmful perturbation over the model weights could be a
probable cause of alignment-broken. In order to attenuate the negative impact
of harmful perturbation, we propose an alignment-stage solution, dubbed
Booster. Technically, along with the original alignment loss, we append a loss
regularizer in the alignment stage's optimization. The regularizer ensures that
the model's harmful loss reduction after the simulated harmful perturbation is
attenuated, thereby mitigating the subsequent fine-tuning risk. Empirical
results show that Booster can effectively reduce the harmful score of the
fine-tuned models while maintaining the performance of downstream tasks. Our
code is available at https://github.com/git-disl/Booster.
","[{'version': 'v1', 'created': 'Tue, 3 Sep 2024 03:59:22 GMT'}, {'version': 'v2', 'created': 'Wed, 4 Sep 2024 19:30:59 GMT'}, {'version': 'v3', 'created': 'Wed, 18 Sep 2024 19:03:30 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 17:17:16 GMT'}]",2025-03-18,"[['Huang', 'Tiansheng', ''], ['Hu', 'Sihao', ''], ['Ilhan', 'Fatih', ''], ['Tekin', 'Selim Furkan', ''], ['Liu', 'Ling', '']]","[{'text': 'fine-tuning-as-a-service', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning-as-a-service,0.6439697742462158
2409.02606,Rafael Pastrana,"Rafael Pastrana, Eder Medina, Isabel M. de Oliveira, Sigrid
  Adriaenssens, Ryan P. Adams","Real-time design of architectural structures with differentiable
  mechanics and neural networks",International Conference on Learning Representations (ICLR) 2025,,,,cs.CE,http://creativecommons.org/licenses/by/4.0/,"  Designing mechanically efficient geometry for architectural structures like
shells, towers, and bridges, is an expensive iterative process. Existing
techniques for solving such inverse problems rely on traditional optimization
methods, which are slow and computationally expensive, limiting iteration speed
and design exploration. Neural networks would seem to offer a solution via
data-driven amortized optimization, but they often require extensive
fine-tuning and cannot ensure that important design criteria, such as
mechanical integrity, are met. In this work, we combine neural networks with a
differentiable mechanics simulator to develop a model that accelerates the
solution of shape approximation problems for architectural structures
represented as bar systems. This model explicitly guarantees compliance with
mechanical constraints while generating designs that closely match target
geometries. We validate our approach in two tasks, the design of masonry shells
and cable-net towers. Our model achieves better accuracy and generalization
than fully neural alternatives, and comparable accuracy to direct optimization
but in real time, enabling fast and reliable design exploration. We further
demonstrate its advantages by integrating it into 3D modeling software and
fabricating a physical prototype. Our work opens up new opportunities for
accelerated mechanical design enhanced by neural networks for the built
environment.
","[{'version': 'v1', 'created': 'Wed, 4 Sep 2024 10:41:50 GMT'}, {'version': 'v2', 'created': 'Thu, 3 Oct 2024 20:02:47 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 15:37:40 GMT'}]",2025-03-18,"[['Pastrana', 'Rafael', ''], ['Medina', 'Eder', ''], ['de Oliveira', 'Isabel M.', ''], ['Adriaenssens', 'Sigrid', ''], ['Adams', 'Ryan P.', '']]","[{'text': 'extensive\nfine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,"extensive
fine-tuning",0.9066202044487
2409.11355,Karim Abou Zeid,"Gonzalo Martin Garcia, Karim Abou Zeid, Christian Schmidt, Daan de
  Geus, Alexander Hermans, Bastian Leibe",Fine-Tuning Image-Conditional Diffusion Models is Easier than You Think,"WACV 2025 Oral. Project page at
  https://vision.rwth-aachen.de/diffusion-e2e-ft",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent work showed that large diffusion models can be reused as highly
precise monocular depth estimators by casting depth estimation as an
image-conditional image generation task. While the proposed model achieved
state-of-the-art results, high computational demands due to multi-step
inference limited its use in many scenarios. In this paper, we show that the
perceived inefficiency was caused by a flaw in the inference pipeline that has
so far gone unnoticed. The fixed model performs comparably to the best
previously reported configuration while being more than 200$\times$ faster. To
optimize for downstream task performance, we perform end-to-end fine-tuning on
top of the single-step model with task-specific losses and get a deterministic
model that outperforms all other diffusion-based depth and normal estimation
models on common zero-shot benchmarks. We surprisingly find that this
fine-tuning protocol also works directly on Stable Diffusion and achieves
comparable performance to current state-of-the-art diffusion-based depth and
normal estimation models, calling into question some of the conclusions drawn
from prior works.
","[{'version': 'v1', 'created': 'Tue, 17 Sep 2024 16:58:52 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 21:59:31 GMT'}]",2025-03-21,"[['Garcia', 'Gonzalo Martin', ''], ['Zeid', 'Karim Abou', ''], ['Schmidt', 'Christian', ''], ['de Geus', 'Daan', ''], ['Hermans', 'Alexander', ''], ['Leibe', 'Bastian', '']]","[{'text': 'end-to-end fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,end-to-end fine-tuning,0.852120041847229
2409.13052,Hamed Rahimi Nohooji Dr,"Hamed Rahimi Nohooji, Holger Voos","Adaptive Trajectory Optimization for Task-Specific Human-Robot
  Collaboration","7 pages, 6 figures, 1 table",,,,cs.RO,http://creativecommons.org/licenses/by/4.0/,"  This paper proposes a task-specific trajectory optimization framework for
human-robot collaboration, enabling adaptive motion planning based on human
interaction dynamics. Unlike conventional approaches that rely on predefined
desired trajectories, the proposed framework optimizes the collaborative motion
dynamically using the inverse differential Riccati equation, ensuring
adaptability to task variations and human input. The generated trajectory
serves as the reference for a neuro-adaptive PID controller, which leverages a
neural network to adjust control gains in real time, addressing system
uncertainties while maintaining low computational complexity. The combination
of trajectory planning and the adaptive control law ensures stability and
accurate joint-space tracking without requiring extensive parameter tuning.
Numerical simulations validate the proposed approach.
","[{'version': 'v1', 'created': 'Thu, 19 Sep 2024 19:02:01 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 19:41:12 GMT'}]",2025-03-20,"[['Nohooji', 'Hamed Rahimi', ''], ['Voos', 'Holger', '']]","[{'text': 'inverse differential Riccati equation', 'label': 'Scaling law'}, {'text': 'adaptive control law', 'label': 'Scaling law'}, {'text': 'parameter tuning', 'label': 'Fine-tuning'}]",Fine-tuning,parameter tuning,0.6959539651870728
2409.13878,Amirmohammad Mohammadi,"Amirmohammad Mohammadi, Tejashri Kelhe, Davelle Carreiro, Alexandra
  Van Dine, Joshua Peeples","Cross-Domain Knowledge Transfer for Underwater Acoustic Classification
  Using Pre-trained Models","6 pages, 4 figures, This work has been submitted to the IEEE for
  possible publication. Added Grad-CAM analysis. Title changed. This work has
  been accepted to IEEE OCEANS 2025",,,,cs.SD cs.LG eess.AS,http://creativecommons.org/licenses/by/4.0/,"  Transfer learning is commonly employed to leverage large, pre-trained models
and perform fine-tuning for downstream tasks. The most prevalent pre-trained
models are initially trained using ImageNet. However, their ability to
generalize can vary across different data modalities. This study compares
pre-trained Audio Neural Networks (PANNs) and ImageNet pre-trained models
within the context of underwater acoustic target recognition (UATR). It was
observed that the ImageNet pre-trained models slightly out-perform pre-trained
audio models in passive sonar classification. We also analyzed the impact of
audio sampling rates for model pre-training and fine-tuning. This study
contributes to transfer learning applications of UATR, illustrating the
potential of pre-trained models to address limitations caused by scarce,
labeled data in the UATR domain.
","[{'version': 'v1', 'created': 'Fri, 20 Sep 2024 20:13:45 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 19:33:19 GMT'}]",2025-03-19,"[['Mohammadi', 'Amirmohammad', ''], ['Kelhe', 'Tejashri', ''], ['Carreiro', 'Davelle', ''], ['Van Dine', 'Alexandra', ''], ['Peeples', 'Joshua', '']]","[{'text': 'Transfer learning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'transfer learning', 'label': 'Few-shot Learning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2409.15771,Yuanzhao Zhang,Yuanzhao Zhang and William Gilpin,Zero-shot forecasting of chaotic systems,13th International Conference on Learning Representations (ICLR 2025),,,,cs.LG nlin.CD physics.comp-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Time-series forecasting is a challenging problem that traditionally requires
specialized models custom-trained for the specific task at hand. Recently,
inspired by the success of large language models, foundation models pre-trained
on vast amounts of time-series data from diverse domains have emerged as a
promising candidate for general-purpose time-series forecasting. The defining
characteristic of these foundation models is their ability to perform zero-shot
learning, that is, forecasting a new system from limited context data without
explicit re-training or fine-tuning. Here, we evaluate whether the zero-shot
learning paradigm extends to the challenging task of forecasting chaotic
systems. Across 135 distinct chaotic dynamical systems and $10^8$ timepoints,
we find that foundation models produce competitive forecasts compared to
custom-trained models (including NBEATS, TiDE, etc.), particularly when
training data is limited. Interestingly, even after point forecasts fail, large
foundation models are able to preserve the geometric and statistical properties
of the chaotic attractors. We attribute this success to foundation models'
ability to perform in-context learning and identify context parroting as a
simple mechanism used by these models to capture the long-term behavior of
chaotic dynamical systems. Our results highlight the potential of foundation
models as a tool for probing nonlinear and complex systems.
","[{'version': 'v1', 'created': 'Tue, 24 Sep 2024 05:56:58 GMT'}, {'version': 'v2', 'created': 'Tue, 3 Dec 2024 03:41:01 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 18:24:12 GMT'}]",2025-03-20,"[['Zhang', 'Yuanzhao', ''], ['Gilpin', 'William', '']]","[{'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'zero-shot\nlearning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'zero-shot\nlearning', 'label': 'Few-shot Learning'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'in-context learning', 'label': 'Few-shot Learning'}, {'text': 'context parroting', 'label': 'contextual Embedding'}, {'text': 'foundation\nmodels', 'label': 'Foundation Model'}]",Fine-tuning,fine-tuning,1.0000001192092896
2409.18584,Jiaming Zhou,"Jiaming Zhou, Shiyao Wang, Shiwan Zhao, Jiabei He, Haoqin Sun, Hui
  Wang, Cheng Liu, Aobo Kong, Yujie Guo, Xi Yang, Yequan Wang, Yonghua Lin and
  Yong Qin","ChildMandarin: A Comprehensive Mandarin Speech Dataset for Young
  Children Aged 3-5",,,,,cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automatic speech recognition (ASR) systems have advanced significantly with
models like Whisper, Conformer, and self-supervised frameworks such as Wav2vec
2.0 and HuBERT. However, developing robust ASR models for young children's
speech remains challenging due to differences in pronunciation, tone, and pace
compared to adult speech. In this paper, we introduce a new Mandarin speech
dataset focused on children aged 3 to 5, addressing the scarcity of resources
in this area. The dataset comprises 41.25 hours of speech with carefully
crafted manual transcriptions, collected from 397 speakers across various
provinces in China, with balanced gender representation. We provide a
comprehensive analysis of speaker demographics, speech duration distribution
and geographic coverage. Additionally, we evaluate ASR performance on models
trained from scratch, such as Conformer, as well as fine-tuned pre-trained
models like HuBERT and Whisper, where fine-tuning demonstrates significant
performance improvements. Furthermore, we assess speaker verification (SV) on
our dataset, showing that, despite the challenges posed by the unique vocal
characteristics of young children, the dataset effectively supports both ASR
and SV tasks. This dataset is a valuable contribution to Mandarin child speech
research. The dataset is now open-source and freely available for all academic
purposes on https://github.com/flageval-baai/ChildMandarin.
","[{'version': 'v1', 'created': 'Fri, 27 Sep 2024 09:42:27 GMT'}, {'version': 'v2', 'created': 'Mon, 30 Sep 2024 12:49:04 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 12:06:13 GMT'}]",2025-03-20,"[['Zhou', 'Jiaming', ''], ['Wang', 'Shiyao', ''], ['Zhao', 'Shiwan', ''], ['He', 'Jiabei', ''], ['Sun', 'Haoqin', ''], ['Wang', 'Hui', ''], ['Liu', 'Cheng', ''], ['Kong', 'Aobo', ''], ['Guo', 'Yujie', ''], ['Yang', 'Xi', ''], ['Wang', 'Yequan', ''], ['Lin', 'Yonghua', ''], ['Qin', 'Yong', '']]","[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2410.01949,Anji Liu,"Anji Liu, Oliver Broadrick, Mathias Niepert, and Guy Van den Broeck",Discrete Copula Diffusion,,,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Discrete diffusion models have recently shown significant progress in
modeling complex data, such as natural languages and DNA sequences. However,
unlike diffusion models for continuous data, which can generate high-quality
samples in just a few denoising steps, modern discrete diffusion models still
require hundreds or even thousands of denoising steps to perform well. In this
paper, we identify a fundamental limitation that prevents discrete diffusion
models from achieving strong performance with fewer steps -- they fail to
capture dependencies between output variables at each denoising step. To
address this issue, we provide a formal explanation and introduce a general
approach to supplement the missing dependency information by incorporating
another deep generative model, termed the copula model. Our method does not
require fine-tuning either the diffusion model or the copula model, yet it
enables high-quality sample generation with significantly fewer denoising
steps. When we apply this approach to autoregressive copula models, the
combined model outperforms both models individually in unconditional and
conditional text generation. Specifically, the hybrid model achieves better
(un)conditional text generation using 8 to 32 times fewer denoising steps than
the diffusion model alone. In addition to presenting an effective discrete
diffusion generation algorithm, this paper emphasizes the importance of
modeling inter-variable dependencies in discrete diffusion.
","[{'version': 'v1', 'created': 'Wed, 2 Oct 2024 18:51:38 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 08:34:29 GMT'}]",2025-03-20,"[['Liu', 'Anji', ''], ['Broadrick', 'Oliver', ''], ['Niepert', 'Mathias', ''], ['Broeck', 'Guy Van den', '']]","[{'text': 'discrete diffusion\nmodels', 'label': 'Neural Language Model'}, {'text': 'copula model', 'label': 'AI model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'diffusion model', 'label': 'AI model'}]",Fine-tuning,fine-tuning,1.0000001192092896
2410.05270,Mohammad Fahes,"Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick P\'erez, Raoul de
  Charette",CLIP's Visual Embedding Projector is a Few-shot Cornucopia,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider the problem of adapting a contrastively pretrained
vision-language model like CLIP (Radford et al., 2021) for few-shot
classification. The literature addresses this problem by learning a linear
classifier of the frozen visual features, optimizing word embeddings, or
learning external feature adapters. We introduce an alternative way for
few-shot CLIP adaptation without adding ''external'' parameters to optimize. We
find that simply fine-tuning the embedding projection matrix of the vision
encoder leads to better performance than all baselines. Furthermore, we show
that regularizing training with the distance between the fine-tuned and
pretrained matrices adds reliability for adapting CLIP, making the results
stable across different learning rates in the ''validation-free'' setting. This
simple approach, coined ProLIP, yields state-of-the-art performance on 11
few-shot classification benchmarks, few-shot cross-dataset transfer, domain
generalization, and base-to-new class generalization. We also show that ProLIP
significantly outperforms prompt tuning when extended to another task of
test-time adaptation, while being one order of magnitude faster to train. Code
will be made available at: https://github.com/astra-vision/ProLIP .
","[{'version': 'v1', 'created': 'Mon, 7 Oct 2024 17:59:59 GMT'}, {'version': 'v2', 'created': 'Fri, 6 Dec 2024 16:07:47 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 17:52:55 GMT'}]",2025-03-18,"[['Fahes', 'Mohammad', ''], ['Vu', 'Tuan-Hung', ''], ['Bursuc', 'Andrei', ''], ['P√©rez', 'Patrick', ''], ['de Charette', 'Raoul', '']]","[{'text': 'few-shot\nclassification', 'label': 'Zero-shot Learning'}, {'text': 'word embeddings', 'label': 'Embedding'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'few-shot cross-dataset transfer', 'label': 'Few-shot Learning'}, {'text': 'domain\ngeneralization', 'label': 'Few-shot Learning'}, {'text': 'base-to-new class generalization', 'label': 'Few-shot Learning'}, {'text': 'prompt tuning', 'label': 'Prompting'}]",Fine-tuning,fine-tuning,1.0000001192092896
2410.07933,Carolin Schmidt,"Carolin Schmidt, Daniele Gammelli, James Harrison, Marco Pavone,
  Filipe Rodrigues",Offline Hierarchical Reinforcement Learning via Inverse Optimization,,,,,cs.LG cs.SY eess.SY math.OC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Hierarchical policies enable strong performance in many sequential
decision-making problems, such as those with high-dimensional action spaces,
those requiring long-horizon planning, and settings with sparse rewards.
However, learning hierarchical policies from static offline datasets presents a
significant challenge. Crucially, actions taken by higher-level policies may
not be directly observable within hierarchical controllers, and the offline
dataset might have been generated using a different policy structure, hindering
the use of standard offline learning algorithms. In this work, we propose OHIO:
a framework for offline reinforcement learning (RL) of hierarchical policies.
Our framework leverages knowledge of the policy structure to solve the
\textit{inverse problem}, recovering the unobservable high-level actions that
likely generated the observed data under our hierarchical policy. This approach
constructs a dataset suitable for off-the-shelf offline training. We
demonstrate our framework on robotic and network optimization problems and show
that it substantially outperforms end-to-end RL methods and improves
robustness. We investigate a variety of instantiations of our framework, both
in direct deployment of policies trained offline and when online fine-tuning is
performed. Code and data are available at
https://ohio-offline-hierarchical-rl.github.io
","[{'version': 'v1', 'created': 'Thu, 10 Oct 2024 14:00:21 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 15:30:08 GMT'}]",2025-03-19,"[['Schmidt', 'Carolin', ''], ['Gammelli', 'Daniele', ''], ['Harrison', 'James', ''], ['Pavone', 'Marco', ''], ['Rodrigues', 'Filipe', '']]","[{'text': 'online fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,online fine-tuning,0.7261751890182495
2410.08621,Jeppe H. Surrow,"Jeppe H. Surrow, Simon T. Thomsen, Rakesh R. Kumar, M\'onica Far
  Brusatori, Maria Paula Montes, Ahan S. Palsole, Chris Hoede, Holger N. Klein,
  Nicolas Volet","Ultra-narrow linewidth laser across the C-band using
  polarization-controlled dual-cavity feedback",,"Opt. Express 33, 11863-11875 (2025)",10.1364/OE.544372,,physics.optics,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  A standard method to reduce the linewidth of semiconductor lasers involves
the use of external optical feedback (EOF). However, feedback powers less than
1 % usually trigger coherence collapse (CC), leading to chaotic laser dynamics
and linewidth broadening. This paper explores a method to mitigate CC through
precise tuning of the feedback polarization depending on the feedback power. We
report a semiconductor laser with a sub-100 Hz intrinsic linewidth, achieved
via EOF. The laser features a U-shaped cavity with two sampled grating
distributed Bragg reflectors (SG-DBRs), enabling broad tunability across a 42
nm wavelength range (1513-1555 nm). By injecting optical feedback into both
sides of the laser cavity via an external fiber-based cavity, we reduce the
intrinsic linewidth by more than three orders of magnitude, from MHz to sub-kHz
across the laser's tuning range. By dynamically tuning the polarization, we
demonstrate sub-100 Hz intrinsic linewidths at feedback powers up to 10 %,
marking an improvement over prior studies where CC limited performance.
","[{'version': 'v1', 'created': 'Fri, 11 Oct 2024 08:36:50 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 13:38:46 GMT'}]",2025-03-18,"[['Surrow', 'Jeppe H.', ''], ['Thomsen', 'Simon T.', ''], ['Kumar', 'Rakesh R.', ''], ['Brusatori', 'M√≥nica Far', ''], ['Montes', 'Maria Paula', ''], ['Palsole', 'Ahan S.', ''], ['Hoede', 'Chris', ''], ['Klein', 'Holger N.', ''], ['Volet', 'Nicolas', '']]","[{'text': 'feedback powers', 'label': 'LLM-powered'}, {'text': 'precise tuning', 'label': 'Fine-tuning'}, {'text': 'feedback powers', 'label': 'LLM-powered'}]",Fine-tuning,precise tuning,0.7851148247718811
2410.10880,Hengxiang Zhang,"Hengxiang Zhang, Songxin Zhang, Bingyi Jing, Hongxin Wei",Fine-tuning can Help Detect Pretraining Data from Large Language Models,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the era of large language models (LLMs), detecting pretraining data has
been increasingly important due to concerns about fair evaluation and ethical
risks. Current methods differentiate members and non-members by designing
scoring functions, like Perplexity and Min-k%. However, the diversity and
complexity of training data magnifies the difficulty of distinguishing, leading
to suboptimal performance in detecting pretraining data. In this paper, we
first explore the benefits of unseen data, which can be easily collected after
the release of the LLM. We find that the perplexities of LLMs shift differently
for members and non-members, after fine-tuning with a small amount of
previously unseen data. In light of this, we introduce a novel and effective
method termed Fine-tuned Score Deviation(FSD), which improves the performance
of current scoring functions for pretraining data detection. In particular, we
propose to measure the deviation distance of current scores after fine-tuning
on a small amount of unseen data within the same domain. In effect, using a few
unseen data can largely decrease the scores of all non-members, leading to a
larger deviation distance than members. Extensive experiments demonstrate the
effectiveness of our method, significantly improving the AUC score on common
benchmark datasets across various models.
","[{'version': 'v1', 'created': 'Wed, 9 Oct 2024 15:36:42 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 12:29:05 GMT'}]",2025-03-18,"[['Zhang', 'Hengxiang', ''], ['Zhang', 'Songxin', ''], ['Jing', 'Bingyi', ''], ['Wei', 'Hongxin', '']]","[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'fair evaluation', 'label': 'AI Ethics'}, {'text': 'ethical\nrisks', 'label': 'AI Ethics'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Fine-tuned Score Deviation(FSD)', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2410.11236,Guiyu Zhang,"Guiyu Zhang, Huan-ang Gao, Zijian Jiang, Hao Zhao, Zhedong Zheng","Ctrl-U: Robust Conditional Image Generation via Uncertainty-aware Reward
  Modeling",ICLR 2025,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we focus on the task of conditional image generation, where an
image is synthesized according to user instructions. The critical challenge
underpinning this task is ensuring both the fidelity of the generated images
and their semantic alignment with the provided conditions. To tackle this
issue, previous studies have employed supervised perceptual losses derived from
pre-trained models, i.e., reward models, to enforce alignment between the
condition and the generated result. However, we observe one inherent
shortcoming: considering the diversity of synthesized images, the reward model
usually provides inaccurate feedback when encountering newly generated data,
which can undermine the training process. To address this limitation, we
propose an uncertainty-aware reward modeling, called Ctrl-U, including
uncertainty estimation and uncertainty-aware regularization, designed to reduce
the adverse effects of imprecise feedback from the reward model. Given the
inherent cognitive uncertainty within reward models, even images generated
under identical conditions often result in a relatively large discrepancy in
reward loss. Inspired by the observation, we explicitly leverage such
prediction variance as an uncertainty indicator. Based on the uncertainty
estimation, we regularize the model training by adaptively rectifying the
reward. In particular, rewards with lower uncertainty receive higher loss
weights, while those with higher uncertainty are given reduced weights to allow
for larger variability. The proposed uncertainty regularization facilitates
reward fine-tuning through consistency construction. Extensive experiments
validate the effectiveness of our methodology in improving the controllability
and generation quality, as well as its scalability across diverse conditional
scenarios. Codes are publicly available at
https://grenoble-zhang.github.io/Ctrl-U-Page/.
","[{'version': 'v1', 'created': 'Tue, 15 Oct 2024 03:43:51 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Feb 2025 17:41:03 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 14:41:25 GMT'}]",2025-03-20,"[['Zhang', 'Guiyu', ''], ['Gao', 'Huan-ang', ''], ['Jiang', 'Zijian', ''], ['Zhao', 'Hao', ''], ['Zheng', 'Zhedong', '']]","[{'text': 'reward fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,reward fine-tuning,0.5952739715576172
2410.13850,Bruno Mlodozeniec,"Bruno Mlodozeniec, Runa Eschenhagen, Juhan Bae, Alexander Immer, David
  Krueger, Richard Turner",Influence Functions for Scalable Data Attribution in Diffusion Models,,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Diffusion models have led to significant advancements in generative
modelling. Yet their widespread adoption poses challenges regarding data
attribution and interpretability. In this paper, we aim to help address such
challenges in diffusion models by developing an influence functions framework.
Influence function-based data attribution methods approximate how a model's
output would have changed if some training data were removed. In supervised
learning, this is usually used for predicting how the loss on a particular
example would change. For diffusion models, we focus on predicting the change
in the probability of generating a particular example via several proxy
measurements. We show how to formulate influence functions for such quantities
and how previously proposed methods can be interpreted as particular design
choices in our framework. To ensure scalability of the Hessian computations in
influence functions, we systematically develop K-FAC approximations based on
generalised Gauss-Newton matrices specifically tailored to diffusion models. We
recast previously proposed methods as specific design choices in our framework
and show that our recommended method outperforms previous data attribution
approaches on common evaluations, such as the Linear Data-modelling Score (LDS)
or retraining without top influences, without the need for method-specific
hyperparameter tuning.
","[{'version': 'v1', 'created': 'Thu, 17 Oct 2024 17:59:02 GMT'}, {'version': 'v2', 'created': 'Thu, 24 Oct 2024 17:43:00 GMT'}, {'version': 'v3', 'created': 'Tue, 7 Jan 2025 15:28:09 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 13:47:39 GMT'}]",2025-03-18,"[['Mlodozeniec', 'Bruno', ''], ['Eschenhagen', 'Runa', ''], ['Bae', 'Juhan', ''], ['Immer', 'Alexander', ''], ['Krueger', 'David', ''], ['Turner', 'Richard', '']]","[{'text': 'supervised\nlearning', 'label': 'Few-shot Learning'}, {'text': 'method-specific\nhyperparameter tuning', 'label': 'Fine-tuning'}]",Fine-tuning,"method-specific
hyperparameter tuning",0.5459896922111511
2410.16713,Joshua Kazdan,"Joshua Kazdan, Rylan Schaeffer, Apratim Dey, Matthias Gerstgrasser,
  Rafael Rafailov, David L. Donoho, Sanmi Koyejo","Collapse or Thrive? Perils and Promises of Synthetic Data in a
  Self-Generating World","Accepted at NeurIPS 2024 Workshops: Mathematics of Modern Machine
  Learning (M3L) and Attributing Model Behavior at Scale (ATTRIB)",,,,cs.LG cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  What happens when generative machine learning models are pretrained on
web-scale datasets containing data generated by earlier models? Some prior work
warns of ""model collapse"" as the web is overwhelmed by synthetic data; other
work suggests the problem can be contained (i.e. collapse can be avoided) by
managing how available data are used in pretraining. In this paper, we report
experiments on three ways of using data (training-workflows), across three
generative model task-settings (multivariate Gaussian estimation, kernel
density estimation, and language-model fine-tuning) to further confirm the
possibility of containment: (a) we confirm that the training-workflow of {\it
replacing} all real data by successive generations of purely synthetic data
indeed suffers model collapse in all task-settings studied; (b) we consider the
training-workflow of {\it accumulating} synthetic data alongside real data and
training on all data combined and confirming that, although the proportion of
real data eventually becomes zero, models remain stable and their test losses
do not diverge under this training-workflow; (c) we consider a
training-workflow where real and synthetic data accumulate together but
successive generations of pretraining are constrained to use fixed-size data
subsets each generation. In this workflow, we observe slow and gradual rather
than explosive degradation of test loss performance across generations. Our
insights are particularly important when forecasting whether future frontier
generative models will collapse or thrive, and our results open avenues for
empirically and mathematically studying the context-dependent value of
synthetic data.
","[{'version': 'v1', 'created': 'Tue, 22 Oct 2024 05:49:24 GMT'}, {'version': 'v2', 'created': 'Mon, 16 Dec 2024 06:37:01 GMT'}, {'version': 'v3', 'created': 'Thu, 6 Feb 2025 00:43:54 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 21:14:46 GMT'}]",2025-03-19,"[['Kazdan', 'Joshua', ''], ['Schaeffer', 'Rylan', ''], ['Dey', 'Apratim', ''], ['Gerstgrasser', 'Matthias', ''], ['Rafailov', 'Rafael', ''], ['Donoho', 'David L.', ''], ['Koyejo', 'Sanmi', '']]","[{'text': 'multivariate Gaussian estimation', 'label': 'Zero-shot Learning'}, {'text': 'kernel\ndensity estimation', 'label': 'Zero-shot Learning'}, {'text': 'language-model fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,language-model fine-tuning,0.6101994514465332
2411.01667,Jonathan Pirnay,"Jonathan Pirnay, Jan G. Rittig, Alexander B. Wolf, Martin Grohe, Jakob
  Burger, Alexander Mitsos, Dominik G. Grimm",GraphXForm: Graph transformer for computer-aided molecular design,"Published in Digital Discovery, 2025",,10.1039/D4DD00339J,,cs.LG physics.chem-ph q-bio.BM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generative deep learning has become pivotal in molecular design for drug
discovery, materials science, and chemical engineering. A widely used paradigm
is to pretrain neural networks on string representations of molecules and
fine-tune them using reinforcement learning on specific objectives. However,
string-based models face challenges in ensuring chemical validity and enforcing
structural constraints like the presence of specific substructures. We propose
to instead combine graph-based molecular representations, which can naturally
ensure chemical validity, with transformer architectures, which are highly
expressive and capable of modeling long-range dependencies between atoms. Our
approach iteratively modifies a molecular graph by adding atoms and bonds,
which ensures chemical validity and facilitates the incorporation of structural
constraints. We present GraphXForm, a decoder-only graph transformer
architecture, which is pretrained on existing compounds and then fine-tuned
using a new training algorithm that combines elements of the deep cross-entropy
method and self-improvement learning. We evaluate GraphXForm on various drug
design tasks, demonstrating superior objective scores compared to
state-of-the-art molecular design approaches. Furthermore, we apply GraphXForm
to two solvent design tasks for liquid-liquid extraction, again outperforming
alternative methods while flexibly enforcing structural constraints or
initiating design from existing molecular structures.
","[{'version': 'v1', 'created': 'Sun, 3 Nov 2024 19:45:15 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 12:01:38 GMT'}]",2025-03-21,"[['Pirnay', 'Jonathan', ''], ['Rittig', 'Jan G.', ''], ['Wolf', 'Alexander B.', ''], ['Grohe', 'Martin', ''], ['Burger', 'Jakob', ''], ['Mitsos', 'Alexander', ''], ['Grimm', 'Dominik G.', '']]","[{'text': 'reinforcement learning', 'label': 'Few-shot Learning'}, {'text': 'transformer architectures', 'label': 'Transformers'}, {'text': 'GraphXForm', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'fine-tuned', 'label': 'Fine-tuning'}, {'text': 'GraphXForm', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'GraphXForm', 'label': 'Generative Pre-trained Transformer (GPT)'}]",Fine-tuning,fine-tuned,0.870777428150177
2411.04425,Ishika Agarwal,"Ishika Agarwal, Krishnateja Killamsetty, Lucian Popa, Marina
  Danilevksy",DELIFT: Data Efficient Language model Instruction Fine Tuning,,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Fine-tuning large language models (LLMs) is essential for enhancing their
performance on specific tasks but is often resource-intensive due to redundant
or uninformative data. To address this inefficiency, we introduce DELIFT (Data
Efficient Language model Instruction Fine-Tuning), a novel algorithm that
systematically optimizes data selection across the three key stages of
fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g.,
reasoning, question-answering), and (3) continual fine-tuning (e.g.,
incorporating new data versions). Unlike existing methods that focus on
single-stage optimization or rely on computationally intensive gradient
calculations, DELIFT operates efficiently across all stages. Central to our
approach is a pairwise utility metric that quantifies how beneficial a data
sample is for improving the model's responses to other samples, effectively
measuring the informational value relative to the model's current capabilities.
By leveraging different submodular functions applied to this metric, DELIFT
selects diverse and optimal subsets that are useful across all stages of
fine-tuning. Experiments across various tasks and model scales demonstrate that
DELIFT can reduce the fine-tuning data size by up to 70% without compromising
performance, offering significant computational savings and outperforming
existing methods in both efficiency and efficacy.
","[{'version': 'v1', 'created': 'Thu, 7 Nov 2024 04:38:29 GMT'}, {'version': 'v2', 'created': 'Sun, 10 Nov 2024 05:24:33 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 02:52:47 GMT'}]",2025-03-21,"[['Agarwal', 'Ishika', ''], ['Killamsetty', 'Krishnateja', ''], ['Popa', 'Lucian', ''], ['Danilevksy', 'Marina', '']]","[{'text': 'instruction tuning', 'label': 'Fine-tuning'}, {'text': 'task-specific fine-tuning', 'label': 'Fine-tuning'}, {'text': 'continual fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,continual fine-tuning,0.7702570557594299
2411.14793,Jooyoung Choi,"Jooyoung Choi, Chaehun Shin, Yeongtak Oh, Heeseung Kim, Jungbeom Lee,
  Sungroh Yoon",Style-Friendly SNR Sampler for Style-Driven Generation,Project page: https://stylefriendly.github.io/,,,,cs.CV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Recent text-to-image diffusion models generate high-quality images but
struggle to learn new, personalized styles, which limits the creation of unique
style templates. In style-driven generation, users typically supply reference
images exemplifying the desired style, together with text prompts that specify
desired stylistic attributes. Previous approaches popularly rely on
fine-tuning, yet it often blindly utilizes objectives and noise level
distributions from pre-training without adaptation. We discover that stylistic
features predominantly emerge at higher noise levels, leading current
fine-tuning methods to exhibit suboptimal style alignment. We propose the
Style-friendly SNR sampler, which aggressively shifts the signal-to-noise ratio
(SNR) distribution toward higher noise levels during fine-tuning to focus on
noise levels where stylistic features emerge. This enhances models' ability to
capture novel styles indicated by reference images and text prompts. We
demonstrate improved generation of novel styles that cannot be adequately
described solely with a text prompt, enabling the creation of new style
templates for personalized content creation.
","[{'version': 'v1', 'created': 'Fri, 22 Nov 2024 08:29:25 GMT'}, {'version': 'v2', 'created': 'Thu, 5 Dec 2024 04:19:59 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 05:25:16 GMT'}]",2025-03-21,"[['Choi', 'Jooyoung', ''], ['Shin', 'Chaehun', ''], ['Oh', 'Yeongtak', ''], ['Kim', 'Heeseung', ''], ['Lee', 'Jungbeom', ''], ['Yoon', 'Sungroh', '']]","[{'text': 'text prompts', 'label': 'Prompting'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'text prompts', 'label': 'Prompting'}]",Fine-tuning,fine-tuning,1.0000001192092896
2411.17385,Duolikun Danier,"Duolikun Danier, Mehmet Ayg\""un, Changjian Li, Hakan Bilen, Oisin Mac
  Aodha",DepthCues: Evaluating Monocular Depth Perception in Large Vision Models,"Accepted to CVPR 2025. Project page:
  https://danier97.github.io/depthcues/",,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Large-scale pre-trained vision models are becoming increasingly prevalent,
offering expressive and generalizable visual representations that benefit
various downstream tasks. Recent studies on the emergent properties of these
models have revealed their high-level geometric understanding, in particular in
the context of depth perception. However, it remains unclear how depth
perception arises in these models without explicit depth supervision provided
during pre-training. To investigate this, we examine whether the monocular
depth cues, similar to those used by the human visual system, emerge in these
models. We introduce a new benchmark, DepthCues, designed to evaluate depth cue
understanding, and present findings across 20 diverse and representative
pre-trained vision models. Our analysis shows that human-like depth cues emerge
in more recent larger models. We also explore enhancing depth perception in
large vision models by fine-tuning on DepthCues, and find that even without
dense depth supervision, this improves depth estimation. To support further
research, our benchmark and evaluation code will be made publicly available for
studying depth perception in vision models.
","[{'version': 'v1', 'created': 'Tue, 26 Nov 2024 12:44:17 GMT'}, {'version': 'v2', 'created': 'Fri, 7 Mar 2025 17:21:06 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 11:00:12 GMT'}]",2025-03-20,"[['Danier', 'Duolikun', ''], ['Ayg√ºn', 'Mehmet', ''], ['Li', 'Changjian', ''], ['Bilen', 'Hakan', ''], ['Mac Aodha', 'Oisin', '']]","[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2412.02114,Haodong Chen,"Harold Haodong Chen, Harry Yang, Ser-Nam Lim","Beyond Generation: Unlocking Universal Editing via Self-Supervised
  Fine-Tuning",Project: https://haroldchen19.github.io/UES-Page/,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advances in video generation have outpaced progress in video editing,
which remains constrained by several limiting factors, namely: (a) the task's
dependency on supervision severely limits generality, (b) an unnecessary
artificial separation between the generation and editing task, and (c) the high
computational costs of training a video model. In this work, we propose UES
(Unlocking Universal Editing via Self-Supervision), a lightweight
self-supervised fine-tuning strategy that transforms generation models into
unified generation-editing systems through self-supervised semantic alignment.
Our approach establishes a dual-conditioning mechanism where original
video-text pairs jointly provide visual and textual semantics, enabling
structured learning of intrinsic spatiotemporal correspondences. Key advantages
include: (i) Universality through supervision-free adaptation to diverse
editing tasks, (ii) Unification of generation and editing applicable to most
text(+image)-to-video model, and (iii) Efficiency via lightweight fine-tune
that reduces tunable parameters by 92.67%. To enable systematic evaluation, we
introduce OmniBench-99, a comprehensive benchmark spanning 99 videos across
humans/animals, environments, and objects, comprising 4 editing types and 8
scenarios. Extensive experiments show UES enables models without inherent
editing capability to perform powerful and universal editing while preserving
or even enhancing their original generation performance.
","[{'version': 'v1', 'created': 'Tue, 3 Dec 2024 03:10:19 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 10:51:59 GMT'}]",2025-03-19,"[['Chen', 'Harold Haodong', ''], ['Yang', 'Harry', ''], ['Lim', 'Ser-Nam', '']]","[{'text': 'lightweight fine-tune', 'label': 'Fine-tuning'}]",Fine-tuning,lightweight fine-tune,0.7465032935142517
2412.04445,Yi Chen,"Yi Chen, Yuying Ge, Weiliang Tang, Yizhuo Li, Yixiao Ge, Mingyu Ding,
  Ying Shan, Xihui Liu","Moto: Latent Motion Token as the Bridging Language for Learning Robot
  Manipulation from Videos","Project released at: https://chenyi99.github.io/moto/ Update: Added
  content related to real-world robot experiments and learning from human
  videos",,,,cs.RO cs.AI cs.CL cs.CV cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Recent developments in Large Language Models pre-trained on extensive corpora
have shown significant success in various natural language processing tasks
with minimal fine-tuning. This success offers new promise for robotics, which
has long been constrained by the high cost of action-labeled data. We ask:
given the abundant video data containing interaction-related knowledge
available as a rich ""corpus"", can a similar generative pre-training approach be
effectively applied to enhance robot learning? The key challenge is to identify
an effective representation for autoregressive pre-training that benefits robot
manipulation tasks. Inspired by the way humans learn new skills through
observing dynamic environments, we propose that effective robotic learning
should emphasize motion-related knowledge, which is closely tied to low-level
actions and is hardware-agnostic, facilitating the transfer of learned motions
to actual robot actions. To this end, we introduce Moto, which converts video
content into latent Motion Token sequences by a Latent Motion Tokenizer,
learning a bridging ""language"" of motion from videos in an unsupervised manner.
We pre-train Moto-GPT through motion token autoregression, enabling it to
capture diverse visual motion knowledge. After pre-training, Moto-GPT
demonstrates the promising ability to produce semantically interpretable motion
tokens, predict plausible motion trajectories, and assess trajectory
rationality through output likelihood. To transfer learned motion priors to
real robot actions, we implement a co-fine-tuning strategy that seamlessly
bridges latent motion token prediction and real robot control. Extensive
experiments show that the fine-tuned Moto-GPT exhibits superior robustness and
efficiency on robot manipulation benchmarks, underscoring its effectiveness in
transferring knowledge from video data to downstream visual manipulation tasks.
","[{'version': 'v1', 'created': 'Thu, 5 Dec 2024 18:57:04 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 02:50:55 GMT'}]",2025-03-21,"[['Chen', 'Yi', ''], ['Ge', 'Yuying', ''], ['Tang', 'Weiliang', ''], ['Li', 'Yizhuo', ''], ['Ge', 'Yixiao', ''], ['Ding', 'Mingyu', ''], ['Shan', 'Ying', ''], ['Liu', 'Xihui', '']]","[{'text': 'minimal fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Moto', 'label': 'Generative Pre-trained Transformer (GPT)'}]",Fine-tuning,minimal fine-tuning,0.848320722579956
2412.08908,Boxun Liu,"Boxun Liu, Shijian Gao, Xuanyu Liu, Xiang Cheng, Liuqing Yang",WiFo: Wireless Foundation Model for Channel Prediction,,,,,eess.SP,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Channel prediction permits to acquire channel state information (CSI) without
signaling overhead. However, almost all existing channel prediction methods
necessitate the deployment of a dedicated model to accommodate a specific
configuration. Leveraging the powerful modeling and multi-task learning
capabilities of foundation models, we propose the first space-time-frequency
(STF) wireless foundation model (WiFo) to address time-frequency channel
prediction tasks in a one-for-all manner. Specifically, WiFo is initially
pre-trained over massive and extensive diverse CSI datasets. Then, the model
will be instantly used for channel prediction under various CSI configurations
without any fine-tuning. We propose a masked autoencoder (MAE)-based network
structure for WiFo to handle heterogeneous STF CSI data, and design several
mask reconstruction tasks for self-supervised pre-training to capture the
inherent 3D variations of CSI. To fully unleash its predictive power, we build
a large-scale heterogeneous simulated CSI dataset consisting of 160K CSI
samples for pre-training. Simulations validate its superior unified learning
performance across multiple datasets and demonstrate its state-of-the-art
(SOTA) zero-shot generalization performance via comparisons with other
full-shot baselines.
","[{'version': 'v1', 'created': 'Thu, 12 Dec 2024 03:44:17 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 06:07:23 GMT'}]",2025-03-20,"[['Liu', 'Boxun', ''], ['Gao', 'Shijian', ''], ['Liu', 'Xuanyu', ''], ['Cheng', 'Xiang', ''], ['Yang', 'Liuqing', '']]","[{'text': 'WiFo', 'label': 'Foundation Model'}, {'text': 'WiFo', 'label': 'Foundation Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'WiFo', 'label': 'Foundation Model'}]",Fine-tuning,fine-tuning,1.0000001192092896
2412.16780,Changchang Sun,"Changchang Sun and Ren Wang and Yihua Zhang and Jinghan Jia and
  Jiancheng Liu and Gaowen Liu and Yan Yan and Sijia Liu","Forget Vectors at Play: Universal Input Perturbations Driving Machine
  Unlearning in Image Classification",,,,,cs.LG cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine unlearning (MU), which seeks to erase the influence of specific
unwanted data from already-trained models, is becoming increasingly vital in
model editing, particularly to comply with evolving data regulations like the
``right to be forgotten''. Conventional approaches are predominantly
model-based, typically requiring retraining or fine-tuning the model's weights
to meet unlearning requirements. In this work, we approach the MU problem from
a novel input perturbation-based perspective, where the model weights remain
intact throughout the unlearning process. We demonstrate the existence of a
proactive input-based unlearning strategy, referred to forget vector, which can
be generated as an input-agnostic data perturbation and remains as effective as
model-based approximate unlearning approaches. We also explore forget vector
arithmetic, whereby multiple class-specific forget vectors are combined through
simple operations (e.g., linear combinations) to generate new forget vectors
for unseen unlearning tasks, such as forgetting arbitrary subsets across
classes. Extensive experiments validate the effectiveness and adaptability of
the forget vector, showcasing its competitive performance relative to
state-of-the-art model-based methods. Codes are available at
https://github.com/Changchangsun/Forget-Vector.
","[{'version': 'v1', 'created': 'Sat, 21 Dec 2024 21:27:22 GMT'}, {'version': 'v2', 'created': 'Wed, 8 Jan 2025 17:00:18 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 01:25:27 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 01:46:48 GMT'}]",2025-03-18,"[['Sun', 'Changchang', ''], ['Wang', 'Ren', ''], ['Zhang', 'Yihua', ''], ['Jia', 'Jinghan', ''], ['Liu', 'Jiancheng', ''], ['Liu', 'Gaowen', ''], ['Yan', 'Yan', ''], ['Liu', 'Sijia', '']]","[{'text': 'Machine unlearning', 'label': 'Zero-shot Learning'}, {'text': 'evolving data regulations', 'label': 'AI Ethics'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2412.18860,Liang Wang,"Liang Wang, Nan Yang, Xingxing Zhang, Xiaolong Huang, Furu Wei",Bootstrap Your Own Context Length,19 pages,,,,cs.CL cs.IR,http://creativecommons.org/licenses/by/4.0/,"  We introduce a bootstrapping approach to train long-context language models
by exploiting their short-context capabilities only. Our method utilizes a
simple agent workflow to synthesize diverse long-context instruction tuning
data, thereby eliminating the necessity for manual data collection and
annotation. The proposed data synthesis workflow requires only a short-context
language model, a text retriever, and a document collection, all of which are
readily accessible within the open-source ecosystem. Subsequently, language
models are fine-tuned using the synthesized data to extend their context
lengths. In this manner, we effectively transfer the short-context capabilities
of language models to long-context scenarios through a bootstrapping process.
We conduct experiments with the open-source Llama-3 family of models and
demonstrate that our method can successfully extend the context length to up to
1M tokens, achieving superior performance across various benchmarks.
","[{'version': 'v1', 'created': 'Wed, 25 Dec 2024 10:08:54 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 02:46:34 GMT'}]",2025-03-20,"[['Wang', 'Liang', ''], ['Yang', 'Nan', ''], ['Zhang', 'Xingxing', ''], ['Huang', 'Xiaolong', ''], ['Wei', 'Furu', '']]","[{'text': 'fine-tuned', 'label': 'Fine-tuning'}, {'text': 'Llama-3', 'label': 'Llama'}]",Fine-tuning,fine-tuned,0.870777428150177
2412.20506,Haorui Ji,"Haorui Ji, Taojun Lin, Hongdong Li",DPBridge: Latent Diffusion Bridge for Dense Prediction,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Diffusion models have shown remarkable capabilities in modeling complex data
distributions by transforming noise into structured data through stochastic
processes. However, when applied to dense prediction tasks whose goal is to
capture per-pixel relationships between RGB images and dense signal maps,
starting the sampling process from an uninformative Gaussian noise often leads
to inefficient sampling and long latency. To overcome these challenges, we
propose DPBridge, a generative framework that establishes direct mapping
between input RGB images and dense signal maps based on a tractable bridge
process. Furthermore, we introduce finetuning strategies to leverage a
pretrained large-scale image diffusion backbone, enjoying its rich visual prior
knowledge to enable both efficient training and robust generalization.
Experiments show that DPBridge achieves competitive performance compared to
both feed-forward and diffusion-based approaches across various benchmarks,
validating its effectiveness and adaptability.
","[{'version': 'v1', 'created': 'Sun, 29 Dec 2024 15:50:34 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 06:20:07 GMT'}]",2025-03-20,"[['Ji', 'Haorui', ''], ['Lin', 'Taojun', ''], ['Li', 'Hongdong', '']]","[{'text': 'DPBridge', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'finetuning strategies', 'label': 'Fine-tuning'}, {'text': 'DPBridge', 'label': 'Generative Pre-trained Transformer (GPT)'}]",Fine-tuning,finetuning strategies,0.5682560205459595
2501.00513,Yifan Xu,"Yifan Xu, Xinhao Li, Yichun Yang, Desen Meng, Rui Huang, Limin Wang",CaReBench: A Fine-Grained Benchmark for Video Captioning and Retrieval,,,,,cs.CV cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Video understanding, including video captioning and retrieval, is still a
great challenge for video-language models (VLMs). The existing video retrieval
and caption benchmarks only include short descriptions, limits their ability of
detailed video understanding evaluation. To address this problem, we present
CaReBench, a testing benchmark for fine-grained video captioning and retrieval
with 1,000 high-quality pairs of videos and human-annotated detailed captions.
Uniquely, it provides manually separated spatial annotations and temporal
annotations for each video. Based on this design, we introduce two evaluation
metrics, ReBias and CapST, specifically tailored for video retrieval and video
captioning tasks, respectively. These metrics enable a comprehensive
investigation into the spatial and temporal biases inherent in VLMs. In
addition, to handle both video retrieval and video captioning tasks in a
unified framework, we develop a simple baseline based on a Multimodal Language
Model (MLLM). By implementing a two-stage Supervised Fine-Tuning (SFT), we
fully unlock the potential of MLLM, enabling it not only to generate detailed
video descriptions but also to extract video features. Surprisingly,
experimental results demonstrate that, compared to the CLIP-based models
designed for retrieval and the popular MLLMs skilled in video captioning, our
baseline shows competitive performance in both fine-grained video retrieval and
video detailed captioning.
","[{'version': 'v1', 'created': 'Tue, 31 Dec 2024 15:53:50 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 16:01:24 GMT'}]",2025-03-19,"[['Xu', 'Yifan', ''], ['Li', 'Xinhao', ''], ['Yang', 'Yichun', ''], ['Meng', 'Desen', ''], ['Huang', 'Rui', ''], ['Wang', 'Limin', '']]","[{'text': 'two-stage Supervised Fine-Tuning (SFT)', 'label': 'Fine-tuning'}]",Fine-tuning,two-stage Supervised Fine-Tuning (SFT),0.6180204153060913
2501.08669,Girolamo Macaluso,"Carlo Romeo, Girolamo Macaluso, Alessandro Sestini, Andrew D. Bagdanov","SPEQ: Offline Stabilization Phases for Efficient Q-Learning in High
  Update-To-Data Ratio Reinforcement Learning",,,,,cs.LG cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  High update-to-data (UTD) ratio algorithms in reinforcement learning (RL)
improve sample efficiency but incur high computational costs, limiting
real-world scalability. We propose Offline Stabilization Phases for Efficient
Q-Learning (SPEQ), an RL algorithm that combines low-UTD online training with
periodic offline stabilization phases. During these phases, Q-functions are
fine-tuned with high UTD ratios on a fixed replay buffer, reducing redundant
updates on suboptimal data. This structured training schedule optimally
balances computational and sample efficiency, addressing the limitations of
both high and low UTD ratio approaches. We empirically demonstrate that SPEQ
requires from 40% to 99% fewer gradient updates and 27% to 78% less training
time compared to state-of-the-art high UTD ratio methods while maintaining or
surpassing their performance on the MuJoCo continuous control benchmark. Our
findings highlight the potential of periodic stabilization phases as an
effective alternative to conventional training schedules, paving the way for
more scalable reinforcement learning solutions in real-world applications where
computational resources are constrained.
","[{'version': 'v1', 'created': 'Wed, 15 Jan 2025 09:04:19 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 12:54:07 GMT'}]",2025-03-19,"[['Romeo', 'Carlo', ''], ['Macaluso', 'Girolamo', ''], ['Sestini', 'Alessandro', ''], ['Bagdanov', 'Andrew D.', '']]","[{'text': 'fine-tuned', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuned,0.870777428150177
2502.02257,Tao Zhang,"Tao Zhang, Jinyong Wen, Zhen Chen, Kun Ding, Shiming Xiang, Chunhong
  Pan","UNIP: Rethinking Pre-trained Attention Patterns for Infrared Semantic
  Segmentation","ICLR 2025. 27 pages, 13 figures, 21 tables",,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Pre-training techniques significantly enhance the performance of semantic
segmentation tasks with limited training data. However, the efficacy under a
large domain gap between pre-training (e.g. RGB) and fine-tuning (e.g.
infrared) remains underexplored. In this study, we first benchmark the infrared
semantic segmentation performance of various pre-training methods and reveal
several phenomena distinct from the RGB domain. Next, our layerwise analysis of
pre-trained attention maps uncovers that: (1) There are three typical attention
patterns (local, hybrid, and global); (2) Pre-training tasks notably influence
the pattern distribution across layers; (3) The hybrid pattern is crucial for
semantic segmentation as it attends to both nearby and foreground elements; (4)
The texture bias impedes model generalization in infrared tasks. Building on
these insights, we propose UNIP, a UNified Infrared Pre-training framework, to
enhance the pre-trained model performance. This framework uses the
hybrid-attention distillation NMI-HAD as the pre-training target, a large-scale
mixed dataset InfMix for pre-training, and a last-layer feature pyramid network
LL-FPN for fine-tuning. Experimental results show that UNIP outperforms various
pre-training methods by up to 13.5\% in average mIoU on three infrared
segmentation tasks, evaluated using fine-tuning and linear probing metrics.
UNIP-S achieves performance on par with MAE-L while requiring only 1/10 of the
computational cost. Furthermore, UNIP significantly surpasses state-of-the-art
(SOTA) infrared or RGB segmentation methods and demonstrates broad potential
for application in other modalities, such as RGB and depth. Our code is
available at https://github.com/casiatao/UNIP.
","[{'version': 'v1', 'created': 'Tue, 4 Feb 2025 12:08:20 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 13:55:08 GMT'}]",2025-03-21,"[['Zhang', 'Tao', ''], ['Wen', 'Jinyong', ''], ['Chen', 'Zhen', ''], ['Ding', 'Kun', ''], ['Xiang', 'Shiming', ''], ['Pan', 'Chunhong', '']]","[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'hybrid', 'label': 'Attention mechanism'}, {'text': 'texture bias', 'label': 'Attention mechanism'}, {'text': 'hybrid-attention distillation', 'label': 'Knowledge distillation'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2502.17212,Xander Haijen,Xander Haijen and Bikram Koirala and Xuanwen Tao and Paul Scheunders,"A Two-step Linear Mixing Model for Unmixing under Hyperspectral
  Variability","13 pages, 10 figures, 5 tables. This work has been submitted to the
  IEEE for possible publication",,,,eess.IV cs.CV,http://creativecommons.org/licenses/by-sa/4.0/,"  Spectral unmixing is an important task in the research field of hyperspectral
image processing. It can be thought of as a regression problem, where the
observed variable (i.e., an image pixel) is to be found as a function of the
response variables (i.e., the pure materials in a scene, called endmembers).
The Linear Mixing Model (LMM) has received a great deal of attention, due to
its simplicity and ease of use in, e.g., optimization problems. Its biggest
flaw is that it assumes that any pure material can be characterized by one
unique spectrum throughout the entire scene. In many cases this is incorrect:
the endmembers face a significant amount of spectral variability caused by,
e.g., illumination conditions, atmospheric effects, or intrinsic variability.
Researchers have suggested several generalizations of the LMM to mitigate this
effect. However, most models lead to ill-posed and highly non-convex
optimization problems, which are hard to solve and have hyperparameters that
are difficult to tune. In this paper, we propose a two-step LMM that bridges
the gap between model complexity and computational tractability. We show that
this model leads to only a mildly non-convex optimization problem, which we
solve with an interior-point solver. This method requires virtually no
hyperparameter tuning, and can therefore be used easily and quickly in a wide
range of unmixing tasks. We show that the model is competitive and in some
cases superior to existing and well-established unmixing methods and
algorithms. We do this through several experiments on synthetic data, real-life
satellite data, and hybrid synthetic-real data.
","[{'version': 'v1', 'created': 'Mon, 24 Feb 2025 14:44:40 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 08:05:10 GMT'}]",2025-03-19,"[['Haijen', 'Xander', ''], ['Koirala', 'Bikram', ''], ['Tao', 'Xuanwen', ''], ['Scheunders', 'Paul', '']]","[{'text': 'hyperparameter tuning', 'label': 'Fine-tuning'}]",Fine-tuning,hyperparameter tuning,0.6193697452545166
2502.19634,Jiazhen Pan,"Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran
  Li, Chen Chen, Cheng Ouyang, Daniel Rueckert","MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language
  Models (VLMs) via Reinforcement Learning",,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Reasoning is a critical frontier for advancing medical image analysis, where
transparency and trustworthiness play a central role in both clinician trust
and regulatory approval. Although Medical Visual Language Models (VLMs) show
promise for radiological tasks, most existing VLMs merely produce final answers
without revealing the underlying reasoning. To address this gap, we introduce
MedVLM-R1, a medical VLM that explicitly generates natural language reasoning
to enhance transparency and trustworthiness. Instead of relying on supervised
fine-tuning (SFT), which often suffers from overfitting to training
distributions and fails to foster genuine reasoning, MedVLM-R1 employs a
reinforcement learning framework that incentivizes the model to discover
human-interpretable reasoning paths without using any reasoning references.
Despite limited training data (600 visual question answering samples) and model
parameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI,
CT, and X-ray benchmarks, outperforming larger models trained on over a million
samples. It also demonstrates robust domain generalization under
out-of-distribution tasks. By unifying medical image analysis with explicit
reasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable
AI in clinical practice. Inference model is available at:
https://huggingface.co/JZPeterPan/MedVLM-R1.
","[{'version': 'v1', 'created': 'Wed, 26 Feb 2025 23:57:34 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 13:55:33 GMT'}]",2025-03-20,"[['Pan', 'Jiazhen', ''], ['Liu', 'Che', ''], ['Wu', 'Junde', ''], ['Liu', 'Fenglin', ''], ['Zhu', 'Jiayuan', ''], ['Li', 'Hongwei Bran', ''], ['Chen', 'Chen', ''], ['Ouyang', 'Cheng', ''], ['Rueckert', 'Daniel', '']]","[{'text': 'regulatory approval', 'label': 'AI Ethics'}, {'text': 'Medical Visual Language Models', 'label': 'Large Language Model'}, {'text': 'supervised\nfine-tuning', 'label': 'Fine-tuning'}, {'text': 'reasoning', 'label': 'Chain of thought'}, {'text': 'reasoning', 'label': 'Chain of thought'}]",Fine-tuning,"supervised
fine-tuning",0.7449287176132202
2503.01196,Alain Moise Dikande Pr.,Alain M. Dikand\'e,"On a hyperbolic Duffing oscillator with linear damping and periodic
  forcing","18 pages, 34 figures",,,,nlin.CD math-ph math.MP physics.comp-ph,http://creativecommons.org/licenses/by/4.0/,"  The Duffing oscillator describes the dynamics of a mass suspended on a spring
with position-dependent stiffness. The mass is assumed to experience a linear
damping and a time-dependent external forcing. The model has been instrumental
in theoretical investigations of dynamical properties of systems with
parity-conserving symmetry, where a double-well substrate connects two
metastable states separated by a barrier. Physical systems of interest include
nonlinear feedback-controlled mass-spring-damper oscillators, active hysteresis
circuits (e.g. memristors), protein chains prone to hydrogen bond-mediated
conformational transitions, centro-symmetric crystals and so on. In this work
we consider a Duffing-type oscillator with a double-well potential represented
by a hyperbolic function of mass position. The hyperbolic double-well potential
has two degenerate minima that can be smoothly tuned by varying a deformability
parameter, leaving unchanged the barrier height. We investigate solutions of
the equation of motion in the absence and presence of damping and forcing. In
the absence of perturbations numerical solutions lead to a periodic train of
anharmonic oscillations featuring a crystal of pulse solitons of sech types.
However, when the hyperbolic double-well potential is inverted, analytical
solutions can be obtained which turn out to be kink-soliton crystals described
by Jacobi elliptic functions. When damping and forcing are taken into
consideration, the system dynamics can transit from periodic to chaotic phases
or vice-versa via period-doubling or period-halving bifurcations, by simply
varying the deformability parameter. The Poincar\'e map of the proposed model
carries the well-known characteristic signatures of chaos presursors of the
standard Duffing model, which happens to be just a particular case of the
bistable oscillator model with the hyperbolic double-well potential.
","[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 05:47:46 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 16:59:24 GMT'}]",2025-03-21,"[['Dikand√©', 'Alain M.', '']]","[{'text': 'smoothly tuned', 'label': 'Fine-tuning'}]",Fine-tuning,smoothly tuned,0.7274996638298035
2503.03644,Shuo Li,"Xiaojun Bi, Shuo Li, Ziyue Wang, Fuwen Luo, Weizheng Qiao, Lu Han,
  Ziwei Sun, Peng Li, Yang Liu","DongbaMIE: A Multimodal Information Extraction Dataset for Evaluating
  Semantic Understanding of Dongba Pictograms","Our dataset can be obtained from:
  https://github.com/thinklis/DongbaMIE",,,,cs.CV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Dongba pictographs are the only pictographs still in use in the world. They
have pictorial ideographic features, and their symbols carry rich cultural and
contextual information. Due to the lack of relevant datasets, existing research
has difficulty in advancing the study of semantic understanding of Dongba
pictographs. To this end, we propose \textbf{DongbaMIE}, the first multimodal
dataset for semantic understanding and extraction of Dongba pictographs,
consisting of Dongba pictograph images and corresponding Chinese semantic
annotations. DongbaMIE contains 23,530 sentence-level and 2,539 paragraph-level
images, covering four semantic dimensions: objects, actions, relations, and
attributes. We systematically evaluate multimodal large language models
(MLLMs), such as GPT-4o, Gemini-2.0, and Qwen2-VL. Experimental results show
that best F1 scores of proprietary models, GPT-4o and Gemini, for object
extraction task are only 3.16 and 3.11 respectively. For the open-source model
Qwen2-VL, it achieves only 11.49 after supervised fine-tuning. These suggest
that current MLLMs still face significant challenges in accurately recognizing
diverse semantic information in Dongba pictographs.
","[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 16:20:53 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Mar 2025 11:36:33 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 12:16:23 GMT'}]",2025-03-21,"[['Bi', 'Xiaojun', ''], ['Li', 'Shuo', ''], ['Wang', 'Ziyue', ''], ['Luo', 'Fuwen', ''], ['Qiao', 'Weizheng', ''], ['Han', 'Lu', ''], ['Sun', 'Ziwei', ''], ['Li', 'Peng', ''], ['Liu', 'Yang', '']]","[{'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,supervised fine-tuning,0.7449287176132202
2503.04833,Liming Lu,"Liming Lu, Shuchao Pang, Siyuan Liang, Haotian Zhu, Xiyu Zeng, Aishan
  Liu, Yunhuai Liu, Yongbin Zhou","Adversarial Training for Multimodal Large Language Models against
  Jailbreak Attacks",,,,,cs.CV cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multimodal large language models (MLLMs) have made remarkable strides in
cross-modal comprehension and generation tasks. However, they remain vulnerable
to jailbreak attacks, where crafted perturbations bypass security guardrails
and elicit harmful outputs. In this paper, we present the first adversarial
training (AT) paradigm tailored to defend against jailbreak attacks during the
MLLM training phase. Extending traditional AT to this domain poses two critical
challenges: efficiently tuning massive parameters and ensuring robustness
against attacks across multiple modalities. To address these challenges, we
introduce Projection Layer Against Adversarial Training (ProEAT), an end-to-end
AT framework. ProEAT incorporates a projector-based adversarial training
architecture that efficiently handles large-scale parameters while maintaining
computational feasibility by focusing adversarial training on a lightweight
projector layer instead of the entire model; additionally, we design a dynamic
weight adjustment mechanism that optimizes the loss function's weight
allocation based on task demands, streamlining the tuning process. To enhance
defense performance, we propose a joint optimization strategy across visual and
textual modalities, ensuring robust resistance to jailbreak attacks originating
from either modality. Extensive experiments conducted on five major jailbreak
attack methods across three mainstream MLLMs demonstrate the effectiveness of
our approach. ProEAT achieves state-of-the-art defense performance,
outperforming existing baselines by an average margin of +34% across text and
image modalities, while incurring only a 1% reduction in clean accuracy.
Furthermore, evaluations on real-world embodied intelligent systems highlight
the practical applicability of our framework, paving the way for the
development of more secure and reliable multimodal systems.
","[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 14:13:35 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 07:01:13 GMT'}]",2025-03-19,"[['Lu', 'Liming', ''], ['Pang', 'Shuchao', ''], ['Liang', 'Siyuan', ''], ['Zhu', 'Haotian', ''], ['Zeng', 'Xiyu', ''], ['Liu', 'Aishan', ''], ['Liu', 'Yunhuai', ''], ['Zhou', 'Yongbin', '']]","[{'text': 'Multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'efficiently tuning', 'label': 'Fine-tuning'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Fine-tuning,efficiently tuning,0.7900304794311523
2503.05403,"Verena H\""aberle","Verena H\""aberle, Xiuqiang He, Linbin Huang, Florian D\""orfler, Steven
  Low","Quantitative Decentralized Stability Certificates for Grid-Forming
  Converter Control","12 pages, 13 figures",,,,eess.SY cs.SY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a decentralized framework for guaranteeing the small-signal
stability of future power systems with grid-forming converters. Our approach
leverages dynamic loop-shifting techniques to compensate for the lack of
passivity in the network dynamics and establishes decentralized parametric
stability certificates, depending on the local device-level controls and
incorporating the effects of the network dynamics. By following practical
tuning rules, we are able to ensure plug-and-play operation without centralized
coordination. Unlike prior works, our approach accommodates coupled frequency
and voltage dynamics, incorporates network dynamics, and does not rely on
specific network configurations or operating points, offering a general and
scalable solution for the integration of power-electronics-based devices into
future power systems. We validate our theoretical stability results through
numerical case studies in a high-fidelity simulation model.
","[{'version': 'v1', 'created': 'Fri, 7 Mar 2025 13:26:55 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 13:51:36 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 14:54:56 GMT'}]",2025-03-18,"[['H√§berle', 'Verena', ''], ['He', 'Xiuqiang', ''], ['Huang', 'Linbin', ''], ['D√∂rfler', 'Florian', ''], ['Low', 'Steven', '']]","[{'text': 'practical\ntuning rules', 'label': 'Fine-tuning'}]",Fine-tuning,"practical
tuning rules",0.6915782690048218
2503.06166,Li Li,"Shawn Li, Peilin Cai, Yuxiao Zhou, Zhiyu Ni, Renjie Liang, You Qin, Yi
  Nian, Zhengzhong Tu, Xiyang Hu, Yue Zhao",Secure On-Device Video OOD Detection Without Backpropagation,,,,,cs.CR cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Out-of-Distribution (OOD) detection is critical for ensuring the reliability
of machine learning models in safety-critical applications such as autonomous
driving and medical diagnosis. While deploying personalized OOD detection
directly on edge devices is desirable, it remains challenging due to large
model sizes and the computational infeasibility of on-device training.
Federated learning partially addresses this but still requires gradient
computation and backpropagation, exceeding the capabilities of many edge
devices. To overcome these challenges, we propose SecDOOD, a secure
cloud-device collaboration framework for efficient on-device OOD detection
without requiring device-side backpropagation. SecDOOD utilizes cloud resources
for model training while ensuring user data privacy by retaining sensitive
information on-device. Central to SecDOOD is a HyperNetwork-based personalized
parameter generation module, which adapts cloud-trained models to
device-specific distributions by dynamically generating local weight
adjustments, effectively combining central and local information without local
fine-tuning. Additionally, our dynamic feature sampling and encryption strategy
selectively encrypts only the most informative feature channels, largely
reducing encryption overhead without compromising detection performance.
Extensive experiments across multiple datasets and OOD scenarios demonstrate
that SecDOOD achieves performance comparable to fully fine-tuned models,
enabling secure, efficient, and personalized OOD detection on resource-limited
edge devices. To enhance accessibility and reproducibility, our code is
publicly available at https://github.com/Dystopians/SecDOOD.
","[{'version': 'v1', 'created': 'Sat, 8 Mar 2025 11:03:21 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 07:44:00 GMT'}]",2025-03-18,"[['Li', 'Shawn', ''], ['Cai', 'Peilin', ''], ['Zhou', 'Yuxiao', ''], ['Ni', 'Zhiyu', ''], ['Liang', 'Renjie', ''], ['Qin', 'You', ''], ['Nian', 'Yi', ''], ['Tu', 'Zhengzhong', ''], ['Hu', 'Xiyang', ''], ['Zhao', 'Yue', '']]","[{'text': 'Federated learning', 'label': 'Few-shot Learning'}, {'text': 'local\nfine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,"local
fine-tuning",0.8377403020858765
2503.09020,Yuan Jiang,"Yuan Jiang, Yujian Zhang, Liang Lu, Christoph Treude, Xiaohong Su,
  Shan Huang and Tiantian Wang","Enhancing High-Quality Code Generation in Large Language Models with
  Comparative Prefix-Tuning",,,,,cs.SE cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have been widely adopted in commercial code
completion engines, significantly enhancing coding efficiency and productivity.
However, LLMs may generate code with quality issues that violate coding
standards and best practices, such as poor code style and maintainability, even
when the code is functionally correct. This necessitates additional effort from
developers to improve the code, potentially negating the efficiency gains
provided by LLMs. To address this problem, we propose a novel comparative
prefix-tuning method for controllable high-quality code generation. Our method
introduces a single, property-specific prefix that is prepended to the
activations of the LLM, serving as a lightweight alternative to fine-tuning.
Unlike existing methods that require training multiple prefixes, our approach
trains only one prefix and leverages pairs of high-quality and low-quality code
samples, introducing a sequence-level ranking loss to guide the model's
training. This comparative approach enables the model to better understand the
differences between high-quality and low-quality code, focusing on aspects that
impact code quality. Additionally, we design a data construction pipeline to
collect and annotate pairs of high-quality and low-quality code, facilitating
effective training. Extensive experiments on the Code Llama 7B model
demonstrate that our method improves code quality by over 100% in certain task
categories, while maintaining functional correctness. We also conduct ablation
studies and generalization experiments, confirming the effectiveness of our
method's components and its strong generalization capability.
","[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 03:15:46 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 07:24:48 GMT'}]",2025-03-20,"[['Jiang', 'Yuan', ''], ['Zhang', 'Yujian', ''], ['Lu', 'Liang', ''], ['Treude', 'Christoph', ''], ['Su', 'Xiaohong', ''], ['Huang', 'Shan', ''], ['Wang', 'Tiantian', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.09151,Jong Chul Ye,"Hyeonho Jeong, Suhyeon Lee, Jong Chul Ye",Reangle-A-Video: 4D Video Generation as Video-to-Video Translation,Project page: https://hyeonho99.github.io/reangle-a-video/,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  We introduce Reangle-A-Video, a unified framework for generating synchronized
multi-view videos from a single input video. Unlike mainstream approaches that
train multi-view video diffusion models on large-scale 4D datasets, our method
reframes the multi-view video generation task as video-to-videos translation,
leveraging publicly available image and video diffusion priors. In essence,
Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An
image-to-video diffusion transformer is synchronously fine-tuned in a
self-supervised manner to distill view-invariant motion from a set of warped
videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame
of the input video is warped and inpainted into various camera perspectives
under an inference-time cross-view consistency guidance using DUSt3R,
generating multi-view consistent starting images. Extensive experiments on
static view transport and dynamic camera control show that Reangle-A-Video
surpasses existing methods, establishing a new solution for multi-view video
generation. We will publicly release our code and data. Project page:
https://hyeonho99.github.io/reangle-a-video/
","[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 08:26:15 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 13:01:59 GMT'}]",2025-03-18,"[['Jeong', 'Hyeonho', ''], ['Lee', 'Suhyeon', ''], ['Ye', 'Jong Chul', '']]","[{'text': 'Multi-View Motion Learning', 'label': 'Few-shot Learning'}, {'text': 'synchronously fine-tuned', 'label': 'Fine-tuning'}]",Fine-tuning,synchronously fine-tuned,0.5768219232559204
2503.09315,Yihong Huang,"Yihong Huang, Chen Chu, Fan Zhang, Fei Chen, Yu Lin, Ruiduan Li,
  Zhihao Li","ShuffleGate: An Efficient and Self-Polarizing Feature Selection Method
  for Large-Scale Deep Models in Industry",,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep models in industrial applications rely on thousands of features for
accurate predictions, such as deep recommendation systems. While new features
are introduced to capture evolving user behavior, outdated or redundant
features often remain, significantly increasing storage and computational
costs. To address this issue, feature selection methods are widely adopted to
identify and remove less important features. However, existing approaches face
two major challenges: (1) they often require complex hyperparameter (Hp)
tuning, making them difficult to employ in practice, and (2) they fail to
produce well-separated feature importance scores, which complicates
straightforward feature removal. Moreover, the impact of removing unimportant
features can only be evaluated through retraining the model, a time-consuming
and resource-intensive process that severely hinders efficient feature
selection.
  To solve these challenges, we propose a novel feature selection approach,
ShuffleGate. In particular, it shuffles all feature values across instances
simultaneously and uses a gating mechanism that allows the model to dynamically
learn the weights for combining the original and shuffled inputs. Notably, it
can generate well-separated feature importance scores and estimate the
performance without retraining the model, while introducing only a single Hp.
Experiments on four public datasets show that our approach outperforms
state-of-the-art methods in feature selection for model retraining. Moreover,
it has been successfully integrated into the daily iteration of Bilibili's
search models across various scenarios, where it significantly reduces feature
set size (up to 60%+) and computational resource usage (up to 20%+), while
maintaining comparable performance.
","[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 12:05:03 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 12:35:52 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 05:06:43 GMT'}]",2025-03-19,"[['Huang', 'Yihong', ''], ['Chu', 'Chen', ''], ['Zhang', 'Fan', ''], ['Chen', 'Fei', ''], ['Lin', 'Yu', ''], ['Li', 'Ruiduan', ''], ['Li', 'Zhihao', '']]","[{'text': 'complex hyperparameter (Hp)\ntuning', 'label': 'Fine-tuning'}]",Fine-tuning,"complex hyperparameter (Hp)
tuning",0.5364035964012146
2503.10615,Yang Yi,"Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao
  Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, Wei Chen","R1-Onevision: Advancing Generalized Multimodal Reasoning through
  Cross-Modal Formalization",Code and Model: https://github.com/Fancy-MLLM/R1-onevision,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models have demonstrated remarkable reasoning capability in
complex textual tasks. However, multimodal reasoning, which requires
integrating visual and textual information, remains a significant challenge.
Existing visual-language models often struggle to effectively analyze and
reason visual content, resulting in suboptimal performance on complex reasoning
tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate
assessment of multimodal reasoning capabilities. In this paper, we introduce
R1-Onevision, a multimodal reasoning model designed to bridge the gap between
visual perception and deep reasoning. To achieve this, we propose a cross-modal
reasoning pipeline that transforms images into formal textural representations,
enabling precise language-based reasoning. Leveraging this pipeline, we
construct the R1-Onevision dataset which provides detailed, step-by-step
multimodal reasoning annotations across diverse domains. We further develop the
R1-Onevision model through supervised fine-tuning and reinforcement learning to
cultivate advanced reasoning and robust generalization abilities. To
comprehensively evaluate multimodal reasoning performance across different
grades, we introduce R1-Onevision-Bench, a benchmark aligned with human
educational stages, covering exams from junior high school to university and
beyond. Experimental results show that R1-Onevision achieves state-of-the-art
performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple
challenging multimodal reasoning benchmarks.
","[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:56:05 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 08:52:34 GMT'}]",2025-03-19,"[['Yang', 'Yi', ''], ['He', 'Xiaoxuan', ''], ['Pan', 'Hongkun', ''], ['Jiang', 'Xiyan', ''], ['Deng', 'Yan', ''], ['Yang', 'Xingtao', ''], ['Lu', 'Haoyu', ''], ['Yin', 'Dacheng', ''], ['Rao', 'Fengyun', ''], ['Zhu', 'Minfeng', ''], ['Zhang', 'Bo', ''], ['Chen', 'Wei', '']]","[{'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'reinforcement learning', 'label': 'Few-shot Learning'}]",Fine-tuning,supervised fine-tuning,0.7449287176132202
2503.11071,Chao Shuai,"Zhenguang Liu, Chao Shuai, Shaojing Fan, Ziping Dong, Jinwu Hu,
  Zhongjie Ba, Kui Ren","Harnessing Frequency Spectrum Insights for Image Copyright Protection
  Against Diffusion Models","Received by CVPR 2025 (10 pages, 11 figures)",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Diffusion models have achieved remarkable success in novel view synthesis,
but their reliance on large, diverse, and often untraceable Web datasets has
raised pressing concerns about image copyright protection. Current methods fall
short in reliably identifying unauthorized image use, as they struggle to
generalize across varied generation tasks and fail when the training dataset
includes images from multiple sources with few identifiable (watermarked or
poisoned) samples. In this paper, we present novel evidence that
diffusion-generated images faithfully preserve the statistical properties of
their training data, particularly reflected in their spectral features.
Leveraging this insight, we introduce \emph{CoprGuard}, a robust frequency
domain watermarking framework to safeguard against unauthorized image usage in
diffusion model training and fine-tuning. CoprGuard demonstrates remarkable
effectiveness against a wide range of models, from naive diffusion models to
sophisticated text-to-image models, and is robust even when watermarked images
comprise a mere 1\% of the training dataset. This robust and versatile approach
empowers content owners to protect their intellectual property in the era of
AI-driven image generation.
","[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 04:27:50 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 06:58:14 GMT'}]",2025-03-18,"[['Liu', 'Zhenguang', ''], ['Shuai', 'Chao', ''], ['Fan', 'Shaojing', ''], ['Dong', 'Ziping', ''], ['Hu', 'Jinwu', ''], ['Ba', 'Zhongjie', ''], ['Ren', 'Kui', '']]","[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.11771,Yizhuo Xiao,"Yizhuo Xiao, Mustafa Suphi Erden, Cheng Wang",Controllable Latent Diffusion for Traffic Simulation,"7 pages,2 figures, submitted to IROS conference",,,,cs.RO cs.MA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The validation of autonomous driving systems benefits greatly from the
ability to generate scenarios that are both realistic and precisely
controllable. Conventional approaches, such as real-world test drives, are not
only expensive but also lack the flexibility to capture targeted edge cases for
thorough evaluation. To address these challenges, we propose a controllable
latent diffusion that guides the training of diffusion models via reinforcement
learning to automatically generate a diverse and controllable set of driving
scenarios for virtual testing. Our approach removes the reliance on large-scale
real-world data by generating complex scenarios whose properties can be finely
tuned to challenge and assess autonomous vehicle systems. Experimental results
show that our approach has the lowest collision rate of $0.098$ and lowest
off-road rate of $0.096$, demonstrating superiority over existing baselines.
The proposed approach significantly improves the realism, stability and
controllability of the generated scenarios, enabling more nuanced safety
evaluation of autonomous vehicles.
","[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 18:04:41 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 17:28:43 GMT'}]",2025-03-20,"[['Xiao', 'Yizhuo', ''], ['Erden', 'Mustafa Suphi', ''], ['Wang', 'Cheng', '']]","[{'text': 'reinforcement\nlearning', 'label': 'Few-shot Learning'}, {'text': 'finely\ntuned', 'label': 'Fine-tuning'}]",Fine-tuning,"finely
tuned",0.7587447762489319
2503.12813,Mohammad Hossein Samaei,"Mousa Alizadeh, Mohammad Hossein Samaei, Azam Seilsepour, Mohammad TH
  Beheshti","Epidemic Forecasting with a Hybrid Deep Learning Method Using CNN-LSTM
  With WOA-GWO Parameter Optimization: Global COVID-19 Case Study",,,,,eess.IV cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Effective epidemic modeling is essential for managing public health crises,
requiring robust methods to predict disease spread and optimize resource
allocation. This study introduces a novel deep learning framework that advances
time series forecasting for infectious diseases, with its application to COVID
19 data as a critical case study. Our hybrid approach integrates Convolutional
Neural Networks (CNNs) and Long Short Term Memory (LSTM) models to capture
spatial and temporal dynamics of disease transmission across diverse regions.
The CNN extracts spatial features from raw epidemiological data, while the LSTM
models temporal patterns, yielding precise and adaptable predictions. To
maximize performance, we employ a hybrid optimization strategy combining the
Whale Optimization Algorithm (WOA) and Gray Wolf Optimization (GWO) to fine
tune hyperparameters, such as learning rates, batch sizes, and training epochs
enhancing model efficiency and accuracy. Applied to COVID 19 case data from 24
countries across six continents, our method outperforms established benchmarks,
including ARIMA and standalone LSTM models, with statistically significant
gains in predictive accuracy (e.g., reduced RMSE). This framework demonstrates
its potential as a versatile method for forecasting epidemic trends, offering
insights for resource planning and decision making in both historical contexts,
like the COVID 19 pandemic, and future outbreaks.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 04:41:26 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 03:10:14 GMT'}]",2025-03-19,"[['Alizadeh', 'Mousa', ''], ['Samaei', 'Mohammad Hossein', ''], ['Seilsepour', 'Azam', ''], ['Beheshti', 'Mohammad TH', '']]","[{'text': 'fine\ntune hyperparameters', 'label': 'Fine-tuning'}]",Fine-tuning,"fine
tune hyperparameters",0.5845230221748352
2503.12822,Mehdi Makni,"Mehdi Makni, Kayhan Behdin, Gabriel Afriat, Zheng Xu, Sergei
  Vassilvitskii, Natalia Ponomareva, Hussein Hazimeh, Rahul Mazumder",An Optimization Framework for Differentially Private Sparse Fine-Tuning,,,,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Differentially private stochastic gradient descent (DP-SGD) is broadly
considered to be the gold standard for training and fine-tuning neural networks
under differential privacy (DP). With the increasing availability of
high-quality pre-trained model checkpoints (e.g., vision and language models),
fine-tuning has become a popular strategy. However, despite recent progress in
understanding and applying DP-SGD for private transfer learning tasks,
significant challenges remain -- most notably, the performance gap between
models fine-tuned with DP-SGD and their non-private counterparts. Sparse
fine-tuning on private data has emerged as an alternative to full-model
fine-tuning; recent work has shown that privately fine-tuning only a small
subset of model weights and keeping the rest of the weights fixed can lead to
better performance. In this work, we propose a new approach for sparse
fine-tuning of neural networks under DP. Existing work on private sparse
finetuning often used fixed choice of trainable weights (e.g., updating only
the last layer), or relied on public model's weights to choose the subset of
weights to modify. Such choice of weights remains suboptimal. In contrast, we
explore an optimization-based approach, where our selection method makes use of
the private gradient information, while using off the shelf privacy accounting
techniques. Our numerical experiments on several computer vision models and
datasets show that our selection method leads to better prediction accuracy,
compared to full-model private fine-tuning or existing private sparse
fine-tuning approaches.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 05:05:05 GMT'}]",2025-03-18,"[['Makni', 'Mehdi', ''], ['Behdin', 'Kayhan', ''], ['Afriat', 'Gabriel', ''], ['Xu', 'Zheng', ''], ['Vassilvitskii', 'Sergei', ''], ['Ponomareva', 'Natalia', ''], ['Hazimeh', 'Hussein', ''], ['Mazumder', 'Rahul', '']]","[{'text': 'Sparse\nfine-tuning', 'label': 'Fine-tuning'}, {'text': 'full-model\nfine-tuning', 'label': 'Fine-tuning'}, {'text': 'sparse\nfine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,"full-model
fine-tuning",0.7202852964401245
2503.12858,Duke Nguyen,"Duke Nguyen, Aditya Joshi, Flora Salim","Harnessing Test-time Adaptation for NLU tasks Involving Dialects of
  English",,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Test-time adaptation (TTA) is an excellent method which helps generalize
models across domains, tasks, and distributions without the use of labeled
datasets. Thus, TTA is very useful in natural language processing (NLP) in the
dialectal setting, since oftentimes, models are trained on Standard American
English (SAE), evaluated on Indian English or Nigerian English, of which
distribution differs significantly from the former. This is especially useful
since dialectal datasets are scarce. In this paper, we explore one of the most
famous TTA techniques, SHOT, in dialectal NLP. We finetune and evaluate SHOT on
different combinations of dialectal GLUE. Our findings show that SHOT is a
viable technique when labeled datasets are unavailable. We also theoretically
propose the concept of dialectal gap and show that it has a positive
correlation with the effectiveness of SHOT. We also find that in many cases,
finetuning on SAE yields higher performance than finetuning on dialectal data.
Our code is available at https://github.com/dukenguyenxyz/dialect-adaptation
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 06:40:06 GMT'}]",2025-03-18,"[['Nguyen', 'Duke', ''], ['Joshi', 'Aditya', ''], ['Salim', 'Flora', '']]","[{'text': 'finetune', 'label': 'Fine-tuning'}, {'text': 'finetuning', 'label': 'Fine-tuning'}]",Fine-tuning,finetuning,0.5753726363182068
2503.12881,Taiju Tanii,Nobuhiro Maekawa and Taiju Tanii,"Does finetuning make $D$-term contributions smaller in natural grand
  unified theories with spontaneous supersymmetry breaking?",,,,,hep-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we explore the natural grand unified theory (GUT) with
spontaneous supersymmetry (SUSY) breaking, focusing on the contribution to
sfermion and Higgs masses. Natural GUTs solve various problems in SUSY GUTs
with the natural assumption that all terms allowed by $SO(10)\times U(1)_A$
symmetry are introduced with $O(1)$ coefficients. %, solve various the problems
in SUSY GUTs. It is also possible to introduce spontaneous SUSY breaking in the
natural GUT. This scenario predicts $D$-term contribution to sfermion and Higgs
masses dominates the $F$-term contribution, that potentially leads to the SUSY
flavor problem. Fortunately, it also predicts high-scale SUSY, which avoids the
SUSY flavor problem but leads to the instability of the %need for fine-tuning
to obtain the electroweak scale. The $D$-term domination results in the
superheavy Higgsino unless the $D$-term contribution becomes comparable with
the $F$-term contribution. We discuss whether it is possible to suppress the
$D$-term contribution while maintaining the $F$-term contribution through the
tuning of $O(1)$ coefficients in the natural GUT with spontaneous SUSY
breaking. Our results indicate that it is impossible. %the D-term is
proportional to the F-term and its derivatives, making it difficult to suppress
the D-term independently. This also suggests that the mass spectrum of the
sfermions can be predicted by the $D$-term contributions, and that the Higgsino
is not a candidate for dark matter in the scenario.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 07:24:32 GMT'}]",2025-03-18,"[['Maekawa', 'Nobuhiro', ''], ['Tanii', 'Taiju', '']]","[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.12897,Haiyang Guo,"Haiyang Guo, Fanhu Zeng, Fei Zhu, Wenzhuo Liu, Da-Han Wang, Jian Xu,
  Xu-Yao Zhang, Cheng-Lin Liu",Federated Continual Instruction Tuning,Preprint,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A vast amount of instruction tuning data is crucial for the impressive
performance of Large Multimodal Models (LMMs), but the associated computational
costs and data collection demands during supervised fine-tuning make it
impractical for most researchers. Federated learning (FL) has the potential to
leverage all distributed data and training resources to reduce the overhead of
joint training. However, most existing methods assume a fixed number of tasks,
while in real-world scenarios, clients continuously encounter new knowledge and
often struggle to retain old tasks due to memory constraints. In this work, we
introduce the Federated Continual Instruction Tuning (FCIT) benchmark to model
this real-world challenge. Our benchmark includes two realistic scenarios,
encompassing four different settings and twelve carefully curated instruction
tuning datasets. To address the challenges posed by FCIT, we propose dynamic
knowledge organization to effectively integrate updates from different tasks
during training and subspace selective activation to allocate task-specific
output during inference. Extensive experimental results demonstrate that our
proposed method significantly enhances model performance across varying levels
of data heterogeneity and catastrophic forgetting. Our source code and dataset
will be made publicly available.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 07:58:06 GMT'}]",2025-03-18,"[['Guo', 'Haiyang', ''], ['Zeng', 'Fanhu', ''], ['Zhu', 'Fei', ''], ['Liu', 'Wenzhuo', ''], ['Wang', 'Da-Han', ''], ['Xu', 'Jian', ''], ['Zhang', 'Xu-Yao', ''], ['Liu', 'Cheng-Lin', '']]","[{'text': 'Large Multimodal Models', 'label': 'Large Language Model'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Federated learning', 'label': 'Few-shot Learning'}, {'text': 'publicly available', 'label': 'Open-source LLMs'}]",Fine-tuning,supervised fine-tuning,0.7449287176132202
2503.12927,Yifei Chen,"Huangwei Chen, Yifei Chen, Zhenyu Yan, Mingyang Ding, Chenlei Li, Zhu
  Zhu, Feiwei Qin","MMLNB: Multi-Modal Learning for Neuroblastoma Subtyping Classification
  Assisted with Textual Description Generation","25 pages, 7 figures",,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neuroblastoma (NB), a leading cause of childhood cancer mortality, exhibits
significant histopathological variability, necessitating precise subtyping for
accurate prognosis and treatment. Traditional diagnostic methods rely on
subjective evaluations that are time-consuming and inconsistent. To address
these challenges, we introduce MMLNB, a multi-modal learning (MML) model that
integrates pathological images with generated textual descriptions to improve
classification accuracy and interpretability. The approach follows a two-stage
process. First, we fine-tune a Vision-Language Model (VLM) to enhance
pathology-aware text generation. Second, the fine-tuned VLM generates textual
descriptions, using a dual-branch architecture to independently extract visual
and textual features. These features are fused via Progressive Robust
Multi-Modal Fusion (PRMF) Block for stable training. Experimental results show
that the MMLNB model is more accurate than the single modal model. Ablation
studies demonstrate the importance of multi-modal fusion, fine-tuning, and the
PRMF mechanism. This research creates a scalable AI-driven framework for
digital pathology, enhancing reliability and interpretability in NB subtyping
classification. Our source code is available at
https://github.com/HovChen/MMLNB.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 08:38:46 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 09:27:16 GMT'}]",2025-03-20,"[['Chen', 'Huangwei', ''], ['Chen', 'Yifei', ''], ['Yan', 'Zhenyu', ''], ['Ding', 'Mingyang', ''], ['Li', 'Chenlei', ''], ['Zhu', 'Zhu', ''], ['Qin', 'Feiwei', '']]","[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.12968,Guanhua Ding,"Guanhua Ding, Yuxuan Xia, Runwei Guan, Qinchen Wu, Tao Huang, Weiping
  Ding, Jinping Sun, and Guoqiang Mao","OptiPMB: Enhancing 3D Multi-Object Tracking with Optimized Poisson
  Multi-Bernoulli Filtering",,,,,cs.CV cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Accurate 3D multi-object tracking (MOT) is crucial for autonomous driving, as
it enables robust perception, navigation, and planning in complex environments.
While deep learning-based solutions have demonstrated impressive 3D MOT
performance, model-based approaches remain appealing for their simplicity,
interpretability, and data efficiency. Conventional model-based trackers
typically rely on random vector-based Bayesian filters within the
tracking-by-detection (TBD) framework but face limitations due to heuristic
data association and track management schemes. In contrast, random finite set
(RFS)-based Bayesian filtering handles object birth, survival, and death in a
theoretically sound manner, facilitating interpretability and parameter tuning.
In this paper, we present OptiPMB, a novel RFS-based 3D MOT method that employs
an optimized Poisson multi-Bernoulli (PMB) filter while incorporating several
key innovative designs within the TBD framework. Specifically, we propose a
measurement-driven hybrid adaptive birth model for improved track
initialization, employ adaptive detection probability parameters to effectively
maintain tracks for occluded objects, and optimize density pruning and track
extraction modules to further enhance overall tracking performance. Extensive
evaluations on nuScenes and KITTI datasets show that OptiPMB achieves superior
tracking accuracy compared with state-of-the-art methods, thereby establishing
a new benchmark for model-based 3D MOT and offering valuable insights for
future research on RFS-based trackers in autonomous driving.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 09:24:26 GMT'}]",2025-03-18,"[['Ding', 'Guanhua', ''], ['Xia', 'Yuxuan', ''], ['Guan', 'Runwei', ''], ['Wu', 'Qinchen', ''], ['Huang', 'Tao', ''], ['Ding', 'Weiping', ''], ['Sun', 'Jinping', ''], ['Mao', 'Guoqiang', '']]","[{'text': 'parameter tuning', 'label': 'Fine-tuning'}]",Fine-tuning,parameter tuning,0.6959539651870728
2503.13021,Omri Suissa,"Omri Suissa, Muhiim Ali, Ariana Azarbal, Hui Shen, Shekhar Pradhan",Dynamic Relation Inference via Verb Embeddings,,,,,cs.CL cs.CV,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  CLIP has demonstrated exceptional image-text matching capabilities due to its
training on contrastive learning tasks. Past research has suggested that
whereas CLIP effectively matches text to images when the matching can be
achieved just by matching the text with the objects in the image, CLIP
struggles when the matching depends on representing the relationship among the
objects in the images (i.e., inferring relations). Previous attempts to address
this limitation by training CLIP on relation detection datasets with only
linguistic supervision have met with limited success. In this paper, we offer
insights and practical methods to advance the field of relation inference from
images. This paper approaches the task of creating a model that effectively
detects relations among the objects in images by producing text and image
embeddings that capture relationships through linguistic supervision. To this
end, we propose Dynamic Relation Inference via Verb Embeddings (DRIVE), which
augments the COCO dataset, fine-tunes CLIP with hard negatives
subject-relation-object triples and corresponding images, and introduces a
novel loss function to improve relation detection. Evaluated on multiple
CLIP-based models, our method significantly improves zero-shot relation
inference accuracy in both frozen and fine-tuned settings, significantly
outperforming CLIP and state-of-the-art models while generalizing well on
unseen data.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 10:24:27 GMT'}]",2025-03-18,"[['Suissa', 'Omri', ''], ['Ali', 'Muhiim', ''], ['Azarbal', 'Ariana', ''], ['Shen', 'Hui', ''], ['Pradhan', 'Shekhar', '']]","[{'text': 'text and image\nembeddings', 'label': 'Embedding'}, {'text': 'Verb Embeddings', 'label': 'Embedding'}, {'text': 'fine-tuned settings', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuned settings,0.789893388748169
2503.13029,Adrian Bekasiewicz,"Adrian Bekasiewicz, Mariusz Dzwonkowski, Tom Dhaene, and Ivo Couckuyt","Specification-Oriented Automatic Design of Topologically Agnostic
  Antenna Structure",,,10.1007/978-3-031-63759-9_2,,math.NA cs.NA eess.SP,http://creativecommons.org/licenses/by/4.0/,"  Design of antennas for modern applications is a challenging task that
combines cognition-driven development of topology intertwined with tuning of
its parameters using rigorous numerical optimization. However, the process can
be streamlined by neglecting the engineering insight in favor of automatic
de-termination of structure geometry. In this work, a specification-oriented
design of topologically agnostic antenna is considered. The radiator is
developed using a bi-stage algorithm that involves min-max classification of
randomly-generated topologies followed by local tuning of the promising designs
using a trust-region optimization applied to a feature-based representation of
the structure frequency response. The automatically generated antenna is
characterized by -10 dB bandwidth of over 600 MHz w.r.t. the center frequency
of 6.5 GHz and a dual-lobe radiation pattern. The obtained performance figures
make the radiator of use for in-door positioning applications. The design
method has been favorably compared against the frequency-based trust-region
optimization.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 10:31:28 GMT'}]",2025-03-18,"[['Bekasiewicz', 'Adrian', ''], ['Dzwonkowski', 'Mariusz', ''], ['Dhaene', 'Tom', ''], ['Couckuyt', 'Ivo', '']]","[{'text': 'local tuning', 'label': 'Fine-tuning'}]",Fine-tuning,local tuning,0.6783174872398376
2503.13049,Pietro Tierno Dr,"Andris P. Stikuts, Seemant Mishra, Artem Ryabov, Philipp Maass, Pietro
  Tierno",Engineering tunable fractional Shapiro steps in colloidal transport,,,,,cond-mat.soft,http://creativecommons.org/licenses/by/4.0/,"  Shapiro steps are quantized plateaus in the velocity-force or velocity-torque
curve of a driven system, when its speed remains constant despite an increase
in the driving force. For microscopic particles driven across a sinusoidal
potential, integer Shapiro steps have been observed. By driving a single
colloidal particle across a time-modulated, non-sinusoidal periodic optical
landscape, we here demonstrate that fractional Shapiro steps emerge in addition
to integer ones. Measuring the particle position via individual particle
tracking, we reveal the underlying microscopic mechanisms that produce integer
and fractional steps and demonstrate how these steps can be controlled by
tuning the shape and driving protocol of the optical potential. The flexibility
offered by optical engineering allows us to generate wide ranges of potential
shapes and to study, at the single-particle level, synchronization behavior in
driven soft condensed matter systems.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 10:54:02 GMT'}]",2025-03-18,"[['Stikuts', 'Andris P.', ''], ['Mishra', 'Seemant', ''], ['Ryabov', 'Artem', ''], ['Maass', 'Philipp', ''], ['Tierno', 'Pietro', '']]","[{'text': 'tuning', 'label': 'Fine-tuning'}]",Fine-tuning,tuning,0.844900906085968
2503.13054,Tiago Castro,"A. Fumagalli, T. Castro, S. Borgani, and M. Valentini",On the Robustness of Cluster Clustering Covariance Calibration,"12 pages, 8 figures, comments welcome :)",,,,astro-ph.CO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Ongoing and upcoming wide-field surveys at different wavelengths will measure
the distribution of galaxy clusters with unprecedented precision, demanding
accurate models for the two-point correlation function (2PCF) covariance. In
this work, we assess a semi-analytical framework for the cluster 2PCF
covariance that employs three nuisance parameters to account for non-Poissonian
shot noise, residual uncertainties in the halo bias model, and subleading noise
terms. We calibrate these parameters on a suite of fast approximate simulations
generated by PINOCCHIO as well as full $N$-body simulations from OpenGADGET3.
We demonstrate that PINOCCHIO can reproduce the 2PCF covariance measured in
OpenGADGET3 at the few percent level, provided the mass functions are carefully
rescaled. Resolution tests confirm that high particle counts are necessary to
capture shot-noise corrections, especially at high redshifts. We perform the
parameter calibration across multiple cosmological models, showing that one of
the nuisance parameters, the non-Poissonian shot-noise correction $\alpha$,
depends mildly on the amplitude of matter fluctuations $\sigma_8$. In contrast,
the remaining two parameters, $\beta$ controlling the bias correction and
$\gamma$ controlling the secondary shot-noise correction, exhibit more
significant variation with redshift and halo mass. Overall, our results
underscore the importance of calibrating covariance models on realistic mock
catalogs that replicate the selection function of forthcoming surveys and
highlight that approximate methods, when properly tuned, can effectively
complement full $N$-body simulations for precision cluster cosmology.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 10:59:19 GMT'}]",2025-03-18,"[['Fumagalli', 'A.', ''], ['Castro', 'T.', ''], ['Borgani', 'S.', ''], ['Valentini', 'M.', '']]","[{'text': 'PINOCCHIO', 'label': 'AI model'}, {'text': 'PINOCCHIO', 'label': 'AI model'}, {'text': 'properly tuned', 'label': 'Fine-tuning'}]",Fine-tuning,properly tuned,0.798178493976593
2503.13203,Corentin Sautier,"Corentin Sautier, Gilles Puy, Alexandre Boulch, Renaud Marlet, Vincent
  Lepetit","Clustering is back: Reaching state-of-the-art LiDAR instance
  segmentation without training",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Panoptic segmentation of LiDAR point clouds is fundamental to outdoor scene
understanding, with autonomous driving being a primary application. While
state-of-the-art approaches typically rely on end-to-end deep learning
architectures and extensive manual annotations of instances, the significant
cost and time investment required for labeling large-scale point cloud datasets
remains a major bottleneck in this field. In this work, we demonstrate that
competitive panoptic segmentation can be achieved using only semantic labels,
with instances predicted without any training or annotations. Our method
achieves performance comparable to current state-of-the-art supervised methods
on standard benchmarks including SemanticKITTI and nuScenes, and outperforms
every publicly available method on SemanticKITTI as a drop-in instance head
replacement, while running in real-time on a single-threaded CPU and requiring
no instance labels. Our method is fully explainable, and requires no learning
or parameter tuning. Code is available at https://github.com/valeoai/Alpine/
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 14:12:08 GMT'}]",2025-03-18,"[['Sautier', 'Corentin', ''], ['Puy', 'Gilles', ''], ['Boulch', 'Alexandre', ''], ['Marlet', 'Renaud', ''], ['Lepetit', 'Vincent', '']]","[{'text': 'parameter tuning', 'label': 'Fine-tuning'}]",Fine-tuning,parameter tuning,0.6959539651870728
2503.13208,Fan Sinan,"Sinan Fan, Liang Xie, Chen Shen, Ge Teng, Xiaosong Yuan, Xiaofeng
  Zhang, Chenxi Huang, Wenxiao Wang, Xiaofei He, Jieping Ye","Improving Complex Reasoning with Dynamic Prompt Corruption: A soft
  prompt Optimization Approach",Accepted by ICLR 2025,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Prompt-tuning (PT) for large language models (LLMs) can facilitate the
performance on various conventional NLP tasks with significantly fewer
trainable parameters. However, our investigation reveals that PT provides
limited improvement and may even degrade the primitive performance of LLMs on
complex reasoning tasks. Such a phenomenon suggests that soft prompts can
positively impact certain instances while negatively affecting others,
particularly during the later phases of reasoning. To address these challenges,
We first identify an information accumulation within the soft prompts. Through
detailed analysis, we demonstrate that this phenomenon is often accompanied by
erroneous information flow patterns in the deeper layers of the model, which
ultimately lead to incorrect reasoning outcomes. we propose a novel method
called \textbf{D}ynamic \textbf{P}rompt \textbf{C}orruption (DPC) to take
better advantage of soft prompts in complex reasoning tasks, which dynamically
adjusts the influence of soft prompts based on their impact on the reasoning
process. Specifically, DPC consists of two stages: Dynamic Trigger and Dynamic
Corruption. First, Dynamic Trigger measures the impact of soft prompts,
identifying whether beneficial or detrimental. Then, Dynamic Corruption
mitigates the negative effects of soft prompts by selectively masking key
tokens that interfere with the reasoning process. We validate the proposed
approach through extensive experiments on various LLMs and reasoning tasks,
including GSM8K, MATH, and AQuA. Experimental results demonstrate that DPC can
consistently enhance the performance of PT, achieving 4\%-8\% accuracy gains
compared to vanilla prompt tuning, highlighting the effectiveness of our
approach and its potential to enhance complex reasoning in LLMs.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 14:20:48 GMT'}]",2025-03-18,"[['Fan', 'Sinan', ''], ['Xie', 'Liang', ''], ['Shen', 'Chen', ''], ['Teng', 'Ge', ''], ['Yuan', 'Xiaosong', ''], ['Zhang', 'Xiaofeng', ''], ['Huang', 'Chenxi', ''], ['Wang', 'Wenxiao', ''], ['He', 'Xiaofei', ''], ['Ye', 'Jieping', '']]","[{'text': 'Prompt-tuning', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'PT', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Dynamic Trigger', 'label': 'Fine-tuning'}, {'text': 'Dynamic\nCorruption', 'label': 'Fine-tuning'}, {'text': 'Dynamic Trigger', 'label': 'Prompting'}, {'text': 'Dynamic Corruption', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'MATH', 'label': 'Large Language Model'}, {'text': 'PT', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Fine-tuning,Prompt-tuning,0.556545078754425
2503.13281,Xiaodi Li,"Xiaodi Li, Shaika Chowdhury, Chung Il Wi, Maria Vassilaki, Ken Liu,
  Terence T Sio, Owen Garrick, Young J Juhn, James R Cerhan, Cui Tao, and Nansu
  Zong","LLM-Match: An Open-Sourced Patient Matching Model Based on Large
  Language Models and Retrieval-Augmented Generation","10 pages, 1 figure",,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Patient matching is the process of linking patients to appropriate clinical
trials by accurately identifying and matching their medical records with trial
eligibility criteria. We propose LLM-Match, a novel framework for patient
matching leveraging fine-tuned open-source large language models. Our approach
consists of four key components. First, a retrieval-augmented generation (RAG)
module extracts relevant patient context from a vast pool of electronic health
records (EHRs). Second, a prompt generation module constructs input prompts by
integrating trial eligibility criteria (both inclusion and exclusion criteria),
patient context, and system instructions. Third, a fine-tuning module with a
classification head optimizes the model parameters using structured prompts and
ground-truth labels. Fourth, an evaluation module assesses the fine-tuned
model's performance on the testing datasets. We evaluated LLM-Match on four
open datasets - n2c2, SIGIR, TREC 2021, and TREC 2022 - using open-source
models, comparing it against TrialGPT, Zero-Shot, and GPT-4-based closed
models. LLM-Match outperformed all baselines.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:31:55 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 14:56:41 GMT'}]",2025-03-19,"[['Li', 'Xiaodi', ''], ['Chowdhury', 'Shaika', ''], ['Wi', 'Chung Il', ''], ['Vassilaki', 'Maria', ''], ['Liu', 'Ken', ''], ['Sio', 'Terence T', ''], ['Garrick', 'Owen', ''], ['Juhn', 'Young J', ''], ['Cerhan', 'James R', ''], ['Tao', 'Cui', ''], ['Zong', 'Nansu', '']]","[{'text': 'input prompts', 'label': 'Prompting'}, {'text': 'fine-tuning module', 'label': 'Fine-tuning'}, {'text': 'structured prompts', 'label': 'Prompting'}, {'text': 'n2c2', 'label': 'Large Language Model'}, {'text': 'TrialGPT', 'label': 'ChatGPT'}, {'text': 'Zero-Shot', 'label': 'ChatGPT'}, {'text': 'GPT-4-based', 'label': 'GPT'}]",Fine-tuning,fine-tuning module,0.7772243022918701
2503.13369,Wan Ju Kang,"Wan Ju Kang, Eunki Kim, Na Min An, Sangryul Kim, Haemin Choi, Ki Hoon
  Kwak, and James Thorne","Sightation Counts: Leveraging Sighted User Feedback in Building a
  BLV-aligned Dataset of Diagram Descriptions","37 pages, 10 figures, 21 tables",,,,cs.AI cs.CV cs.HC,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Often, the needs and visual abilities differ between the annotator group and
the end user group. Generating detailed diagram descriptions for blind and
low-vision (BLV) users is one such challenging domain. Sighted annotators could
describe visuals with ease, but existing studies have shown that direct
generations by them are costly, bias-prone, and somewhat lacking by BLV
standards. In this study, we ask sighted individuals to assess -- rather than
produce -- diagram descriptions generated by vision-language models (VLM) that
have been guided with latent supervision via a multi-pass inference. The
sighted assessments prove effective and useful to professional educators who
are themselves BLV and teach visually impaired learners. We release Sightation,
a collection of diagram description datasets spanning 5k diagrams and 137k
samples for completion, preference, retrieval, question answering, and
reasoning training purposes and demonstrate their fine-tuning potential in
various downstream tasks.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 16:52:46 GMT'}]",2025-03-18,"[['Kang', 'Wan Ju', ''], ['Kim', 'Eunki', ''], ['An', 'Na Min', ''], ['Kim', 'Sangryul', ''], ['Choi', 'Haemin', ''], ['Kwak', 'Ki Hoon', ''], ['Thorne', 'James', '']]","[{'text': 'fine-tuning potential', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning potential,0.6934475898742676
2503.13443,Haoyang Li Mr.,"Haoyang Li, Liang Wang, Chao Wang, Jing Jiang, Yan Peng, Guodong Long",DPC: Dual-Prompt Collaboration for Tuning Vision-Language Models,"Accepted by the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition 2025 (CVPR 2025)",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Base-New Trade-off (BNT) problem universally exists during the
optimization of CLIP-based prompt tuning, where continuous fine-tuning on base
(target) classes leads to a simultaneous decrease of generalization ability on
new (unseen) classes. Existing approaches attempt to regulate the prompt tuning
process to balance BNT by appending constraints. However, imposed on the same
target prompt, these constraints fail to fully avert the mutual exclusivity
between the optimization directions for base and new. As a novel solution to
this challenge, we propose the plug-and-play Dual-Prompt Collaboration (DPC)
framework, the first that decoupling the optimization processes of base and new
tasks at the prompt level. Specifically, we clone a learnable parallel prompt
based on the backbone prompt, and introduce a variable Weighting-Decoupling
framework to independently control the optimization directions of dual prompts
specific to base or new tasks, thus avoiding the conflict in generalization.
Meanwhile, we propose a Dynamic Hard Negative Optimizer, utilizing dual prompts
to construct a more challenging optimization task on base classes for
enhancement. For interpretability, we prove the feature channel invariance of
the prompt vector during the optimization process, providing theoretical
support for the Weighting-Decoupling of DPC. Extensive experiments on multiple
backbones demonstrate that DPC can significantly improve base performance
without introducing any external knowledge beyond the base classes, while
maintaining generalization to new classes. Code is available at:
https://github.com/JREion/DPC.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:59:27 GMT'}]",2025-03-18,"[['Li', 'Haoyang', ''], ['Wang', 'Liang', ''], ['Wang', 'Chao', ''], ['Jiang', 'Jing', ''], ['Peng', 'Yan', ''], ['Long', 'Guodong', '']]","[{'text': 'BNT', 'label': 'BERT'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'continuous fine-tuning', 'label': 'Fine-tuning'}, {'text': 'base', 'label': 'Foundation Model'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'BNT', 'label': 'BERT'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'prompt', 'label': 'Prompting'}]",Fine-tuning,continuous fine-tuning,0.8377360105514526
2503.13551,Teng Wang,"Teng Wang, Zhangyi Jiang, Zhenqi He, Wenhan Yang, Yanan Zheng, Zeyu
  Li, Zifan He, Shenyang Tong, Hailei Gong","Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in
  Large Language Models",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent studies show that Large Language Models (LLMs) achieve strong
reasoning capabilities through supervised fine-tuning or reinforcement
learning. However, a key approach, the Process Reward Model (PRM), suffers from
reward hacking, making it unreliable in identifying the best intermediate
steps. In this paper, we propose a novel reward model approach, Hierarchical
Reward Model (HRM), which evaluates both individual and consecutive reasoning
steps from fine-grained and coarse-grained level. HRM performs better in
assessing reasoning coherence and self-reflection, particularly when the
previous reasoning step is incorrect. Furthermore, to address the inefficiency
of autonomous generating PRM training data via Monte Carlo Tree Search (MCTS),
we introduce a lightweight and effective data augmentation strategy called
Hierarchical Node Compression (HNC) based on node merging (combining two
consecutive reasoning steps into one step) in the tree structure. This approach
diversifies MCTS results for HRM with negligible computational overhead,
enhancing label robustness by introducing noise. Empirical results on the
PRM800K dataset demonstrate that HRM, in conjunction with HNC, achieves
superior stability and reliability in evaluation compared to PRM. Furthermore,
cross-domain evaluations on MATH500 and GSM8K confirm HRM's superior
generalization and robustness across diverse reasoning tasks. The code for all
experiments will be released at https:
//github.com/tengwang0318/hierarchial_reward_model.
","[{'version': 'v1', 'created': 'Sun, 16 Mar 2025 15:18:40 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:43:56 GMT'}]",2025-03-20,"[['Wang', 'Teng', ''], ['Jiang', 'Zhangyi', ''], ['He', 'Zhenqi', ''], ['Yang', 'Wenhan', ''], ['Zheng', 'Yanan', ''], ['Li', 'Zeyu', ''], ['He', 'Zifan', ''], ['Tong', 'Shenyang', ''], ['Gong', 'Hailei', '']]","[{'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'reinforcement\nlearning', 'label': 'Few-shot Learning'}]",Fine-tuning,supervised fine-tuning,0.7449287176132202
2503.13661,Huy Hoang Ha,Huy Hoang Ha,"Pensez: Less Data, Better Reasoning -- Rethinking French LLM",,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) have demonstrated remarkable capabilities in
various natural language processing tasks. However, achieving strong
performance in specialized domains like mathematical reasoning and non-English
languages often requires extensive training on massive datasets. This paper
investigates a contrasting approach: strategic fine-tuning on a small,
high-quality, bilingual (English-French) dataset to enhance both the reasoning
capabilities and French language proficiency of a large language model. Rather
than relying on scale, we explore the hypothesis that targeted data curation
and optimized training can achieve competitive, or even superior, performance.
We demonstrate, through targeted supervised fine-tuning (SFT) on only 2,000
carefully selected samples, significant improvements in mathematical reasoning.
Specifically, Pensez 7B exhibits an increase in accuracy of the base model up
to 20% on the AIME25 and a 12% increase on a French MATH level 5 benchmark.
These results challenge the prevailing assumption that massive datasets are
aprerequisite for strong reasoning performance in LLMs, highlighting the
potential of strategic data curation and optimized fine-tuning for enhancing
both specialized skills and multilingual capabilities. Our findings have
implications for the efficient development of high-performing, multilingual
LLMs, especially in resource-constrained scenarios.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 19:09:11 GMT'}]",2025-03-19,"[['Ha', 'Huy Hoang', '']]","[{'text': 'strategic fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,strategic fine-tuning,0.6777284145355225
2503.13819,Xiaopei Chen,"Xiaopei Chen, Wen Wu, Zuguang Li, Liang Li, Fei Ji","LLM-Empowered IoT for 6G Networks: Architecture, Challenges, and
  Solutions",,,,,cs.ET,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Internet of Things (IoT) in the sixth generation (6G) era is envisioned
to evolve towards intelligence, ubiquity, and self-optimization. Large language
models (LLMs) have demonstrated remarkable generalization capabilities across
diverse domains, including natural language processing (NLP), computer vision
(CV), and beyond. In this article, we propose an LLM-empowered IoT architecture
for 6G networks to achieve intelligent autonomy while supporting advanced IoT
applications. LLMs are pushed to the edge of the 6G network to support the
synergy of LLMs and IoT. LLM solutions are tailored to both IoT application
requirements and IoT management needs, i.e., LLM for IoT. On the other hand,
edge inference and edge fine-tuning are discussed to support the deployment of
LLMs, i.e., LLM on IoT. Furthermore, we propose a memory-efficient split
federated learning (SFL) framework for LLM fine-tuning on heterogeneous IoT
devices that alleviates memory pressures on both IoT devices and the edge
server while achieving comparable performance and convergence time. Finally, a
case study is presented, followed by a discussion about open issues of
LLM-empowered IoT for 6G networks.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 01:53:42 GMT'}]",2025-03-19,"[['Chen', 'Xiaopei', ''], ['Wu', 'Wen', ''], ['Li', 'Zuguang', ''], ['Li', 'Liang', ''], ['Ji', 'Fei', '']]","[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'edge fine-tuning', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Fine-tuning,edge fine-tuning,0.6748901605606079
2503.13836,Seokhyeon Hong,"Seokhyeon Hong, Chaelin Kim, Serin Yoon, Junghyun Nam, Sihun Cha,
  Junyong Noh","SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation
  and Editing","CVPR 2025; Project page
  https://seokhyeonhong.github.io/projects/salad/",,,,cs.CV cs.AI cs.GR cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Text-driven motion generation has advanced significantly with the rise of
denoising diffusion models. However, previous methods often oversimplify
representations for the skeletal joints, temporal frames, and textual words,
limiting their ability to fully capture the information within each modality
and their interactions. Moreover, when using pre-trained models for downstream
tasks, such as editing, they typically require additional efforts, including
manual interventions, optimization, or fine-tuning. In this paper, we introduce
a skeleton-aware latent diffusion (SALAD), a model that explicitly captures the
intricate inter-relationships between joints, frames, and words. Furthermore,
by leveraging cross-attention maps produced during the generation process, we
enable attention-based zero-shot text-driven motion editing using a pre-trained
SALAD model, requiring no additional user input beyond text prompts. Our
approach significantly outperforms previous methods in terms of text-motion
alignment without compromising generation quality, and demonstrates practical
versatility by providing diverse editing capabilities beyond generation. Code
is available at project page.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 02:20:11 GMT'}]",2025-03-19,"[['Hong', 'Seokhyeon', ''], ['Kim', 'Chaelin', ''], ['Yoon', 'Serin', ''], ['Nam', 'Junghyun', ''], ['Cha', 'Sihun', ''], ['Noh', 'Junyong', '']]","[{'text': 'optimization', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'attention-based zero-shot text-driven motion editing', 'label': 'Few-shot Learning'}, {'text': 'text prompts', 'label': 'Prompting'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.13847,Monika Shah,"Monika Shah, Somdeb Sarkhel, Deepak Venugopal","Disentangling Fine-Tuning from Pre-Training in Visual Captioning with
  Hybrid Markov Logic","2024 IEEE International Conference on Big Data (BigData), 10 pages",,10.1109/BigData62323.2024.10825003,,cs.CV cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Multimodal systems have highly complex processing pipelines and are
pretrained over large datasets before being fine-tuned for specific tasks such
as visual captioning. However, it becomes hard to disentangle what the model
learns during the fine-tuning process from what it already knows due to its
pretraining. In this work, we learn a probabilistic model using Hybrid Markov
Logic Networks (HMLNs) over the training examples by relating symbolic
knowledge (extracted from the caption) with visual features (extracted from the
image). For a generated caption, we quantify the influence of training examples
based on the HMLN distribution using probabilistic inference. We evaluate two
types of inference procedures on the MSCOCO dataset for different types of
captioning models. Our results show that for BLIP2 (a model that uses a LLM),
the fine-tuning may have smaller influence on the knowledge the model has
acquired since it may have more general knowledge to perform visual captioning
as compared to models that do not use a LLM
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 02:39:26 GMT'}]",2025-03-19,"[['Shah', 'Monika', ''], ['Sarkhel', 'Somdeb', ''], ['Venugopal', 'Deepak', '']]","[{'text': 'fine-tuned', 'label': 'Fine-tuning'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'LLM', 'label': 'LLM'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.13939,Yuxiang Lai,"Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, Xiaofeng Yang","Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in
  Vision-Language Models",,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Vision-language models (VLMs) have advanced reasoning in natural scenes, but
their role in medical imaging remains underexplored. Medical reasoning tasks
demand robust image analysis and well-justified answers, posing challenges due
to the complexity of medical images. Transparency and trustworthiness are
essential for clinical adoption and regulatory compliance. We introduce Med-R1,
a framework exploring reinforcement learning (RL) to enhance VLMs'
generalizability and trustworthiness in medical reasoning. Leveraging the
DeepSeek strategy, we employ Group Relative Policy Optimization (GRPO) to guide
reasoning paths via reward signals. Unlike supervised fine-tuning (SFT), which
often overfits and lacks generalization, RL fosters robust and diverse
reasoning. Med-R1 is evaluated across eight medical imaging modalities: CT,
MRI, Ultrasound, Dermoscopy, Fundus Photography, Optical Coherence Tomography
(OCT), Microscopy, and X-ray Imaging. Compared to its base model, Qwen2-VL-2B,
Med-R1 achieves a 29.94% accuracy improvement and outperforms Qwen2-VL-72B,
which has 36 times more parameters. Testing across five question types-modality
recognition, anatomy identification, disease diagnosis, lesion grading, and
biological attribute analysis Med-R1 demonstrates superior generalization,
exceeding Qwen2-VL-2B by 32.06% and surpassing Qwen2-VL-72B in question-type
generalization. These findings show that RL improves medical reasoning and
enables parameter-efficient models to outperform significantly larger ones.
With interpretable reasoning outputs, Med-R1 represents a promising step toward
generalizable, trustworthy, and clinically viable medical VLMs.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 06:12:38 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 20:58:55 GMT'}]",2025-03-21,"[['Lai', 'Yuxiang', ''], ['Zhong', 'Jike', ''], ['Li', 'Ming', ''], ['Zhao', 'Shitian', ''], ['Yang', 'Xiaofeng', '']]","[{'text': 'regulatory compliance', 'label': 'AI Ethics'}, {'text': 'supervised fine-tuning (SFT)', 'label': 'Fine-tuning'}, {'text': 'Med-R1', 'label': 'Foundation Model'}]",Fine-tuning,supervised fine-tuning (SFT),0.6747050285339355
2503.13940,Hang Zhao,"Hang Zhao, Hongru Li, Dongfang Xu, Shenghui Song, and Khaled B.
  Letaief",Multi-Modal Self-Supervised Semantic Communication,,,,,cs.CV eess.SP,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Semantic communication is emerging as a promising paradigm that focuses on
the extraction and transmission of semantic meanings using deep learning
techniques. While current research primarily addresses the reduction of
semantic communication overhead, it often overlooks the training phase, which
can incur significant communication costs in dynamic wireless environments. To
address this challenge, we propose a multi-modal semantic communication system
that leverages multi-modal self-supervised learning to enhance task-agnostic
feature extraction. The proposed approach employs self-supervised learning
during the pre-training phase to extract task-agnostic semantic features,
followed by supervised fine-tuning for downstream tasks. This dual-phase
strategy effectively captures both modality-invariant and modality-specific
features while minimizing training-related communication overhead. Experimental
results on the NYU Depth V2 dataset demonstrate that the proposed method
significantly reduces training-related communication overhead while maintaining
or exceeding the performance of existing supervised learning approaches. The
findings underscore the advantages of multi-modal self-supervised learning in
semantic communication, paving the way for more efficient and scalable edge
inference systems.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 06:13:02 GMT'}]",2025-03-19,"[['Zhao', 'Hang', ''], ['Li', 'Hongru', ''], ['Xu', 'Dongfang', ''], ['Song', 'Shenghui', ''], ['Letaief', 'Khaled B.', '']]","[{'text': 'multi-modal self-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'self-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'multi-modal self-supervised learning', 'label': 'Few-shot Learning'}]",Fine-tuning,supervised fine-tuning,0.7449287176132202
2503.14036,Ina Kodrasi,Mingchi Hou and Ina Kodrasi,Variational Autoencoder for Personalized Pathological Speech Enhancement,Submitted to EUSIPCO 2025,,,,eess.AS cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The generalizability of speech enhancement (SE) models across speaker
conditions remains largely unexplored, despite its critical importance for
broader applicability. This paper investigates the performance of the hybrid
variational autoencoder (VAE)-non-negative matrix factorization (NMF) model for
SE, focusing primarily on its generalizability to pathological speakers with
Parkinson's disease. We show that VAE models trained on large neurotypical
datasets perform poorly on pathological speech. While fine-tuning these
pre-trained models with pathological speech improves performance, a performance
gap remains between neurotypical and pathological speakers. To address this
gap, we propose using personalized SE models derived from fine-tuning
pre-trained models with only a few seconds of clean data from each speaker. Our
results demonstrate that personalized models considerably enhance performance
for all speakers, achieving comparable results for both neurotypical and
pathological speakers.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 08:54:00 GMT'}]",2025-03-19,"[['Hou', 'Mingchi', ''], ['Kodrasi', 'Ina', '']]","[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.14118,Michele Ceriotti,"Arslan Mazitov, Filippo Bigi, Matthias Kellner, Paolo Pegolo, Davide
  Tisi, Guillaume Fraux, Sergey Pozdnyakov, Philip Loche, and Michele Ceriotti","PET-MAD, a universal interatomic potential for advanced materials
  modeling",,,,,cond-mat.mtrl-sci cs.LG physics.chem-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine-learning interatomic potentials (MLIPs) have greatly extended the
reach of atomic-scale simulations, offering the accuracy of first-principles
calculations at a fraction of the effort. Leveraging large quantum mechanical
databases and expressive architectures, recent ""universal"" models deliver
qualitative accuracy across the periodic table but are often biased toward
low-energy configurations. We introduce PET-MAD, a generally applicable MLIP
trained on a dataset combining stable inorganic and organic solids,
systematically modified to enhance atomic diversity. Using a moderate but
highly-consistent level of electronic-structure theory, we assess PET-MAD's
accuracy on established benchmarks and advanced simulations of six materials.
PET-MAD rivals state-of-the-art MLIPs for inorganic solids, while also being
reliable for molecules, organic materials, and surfaces. It is stable and fast,
enabling, out-of-the-box, the near-quantitative study of thermal and quantum
mechanical fluctuations, functional properties, and phase transitions. It can
be efficiently fine-tuned to deliver full quantum mechanical accuracy with a
minimal number of targeted calculations.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 10:35:30 GMT'}]",2025-03-19,"[['Mazitov', 'Arslan', ''], ['Bigi', 'Filippo', ''], ['Kellner', 'Matthias', ''], ['Pegolo', 'Paolo', ''], ['Tisi', 'Davide', ''], ['Fraux', 'Guillaume', ''], ['Pozdnyakov', 'Sergey', ''], ['Loche', 'Philip', ''], ['Ceriotti', 'Michele', '']]","[{'text': 'MLIPs', 'label': 'LLMs'}, {'text': 'MLIPs', 'label': 'LLMs'}, {'text': 'efficiently fine-tuned', 'label': 'Fine-tuning'}]",Fine-tuning,efficiently fine-tuned,0.7684534192085266
2503.14155,Jie Luo Dr.,"Muxuan Yang, Dongyang Yan, Lei Gao, Wei Liu, Yun Lai, Yadong Xu, Zhi
  Hong Hang, Jie Luo",Electromagnetic Duality Symmetry-Protected Dirac-Like Cones,,,,,physics.optics physics.class-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Dirac-like cones, featuring conical linear dispersions intersecting with flat
bands, typically arise from accidental degeneracy of multiple modes that
requires precise tuning of material and structural parameters, inherently
limiting their robustness and applications. In this work, by introducing
electromagnetic duality symmetry into photonic crystals, we demonstrate the
emergence of intrinsically robust deterministic Dirac-like cones. We show that
such symmetry (achieved through either self-dual particles or non-self-dual
particle clusters with duality-glide symmetry) enforces double degeneracies for
band structures of photonic crystals. Furthermore, by harnessing the joint
duality-structural symmetry, multiple deterministic Dirac-like cones exhibiting
exceptional resilience to lattice size variations can be obtained. Our
introduction of an extra symmetry into photonic crystals establishes a profound
connection between duality symmetry and Dirac physics, providing a robust
platform for advanced photonic band engineering.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:29:08 GMT'}]",2025-03-19,"[['Yang', 'Muxuan', ''], ['Yan', 'Dongyang', ''], ['Gao', 'Lei', ''], ['Liu', 'Wei', ''], ['Lai', 'Yun', ''], ['Xu', 'Yadong', ''], ['Hang', 'Zhi Hong', ''], ['Luo', 'Jie', '']]","[{'text': 'precise tuning', 'label': 'Fine-tuning'}]",Fine-tuning,precise tuning,0.7851148247718811
2503.14173,Raul Quijada,"Guillem Cadevall Ferreres, Marc Serrano Sanz, Marc Bardeli G\'amez,
  Pol Gerdt Basullas, Francesc Tarres Ruiz, Raul Quijada Ferrero",NERCat: Fine-Tuning for Enhanced Named Entity Recognition in Catalan,"7 pages, 1 table",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Named Entity Recognition (NER) is a critical component of Natural Language
Processing (NLP) for extracting structured information from unstructured text.
However, for low-resource languages like Catalan, the performance of NER
systems often suffers due to the lack of high-quality annotated datasets. This
paper introduces NERCat, a fine-tuned version of the GLiNER[1] model, designed
to improve NER performance specifically for Catalan text. We used a dataset of
manually annotated Catalan television transcriptions to train and fine-tune the
model, focusing on domains such as politics, sports, and culture. The
evaluation results show significant improvements in precision, recall, and
F1-score, particularly for underrepresented named entity categories such as
Law, Product, and Facility. This study demonstrates the effectiveness of
domain-specific fine-tuning in low-resource languages and highlights the
potential for enhancing Catalan NLP applications through manual annotation and
high-quality datasets.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:44:19 GMT'}]",2025-03-19,"[['Ferreres', 'Guillem Cadevall', ''], ['Sanz', 'Marc Serrano', ''], ['G√°mez', 'Marc Bardeli', ''], ['Basullas', 'Pol Gerdt', ''], ['Ruiz', 'Francesc Tarres', ''], ['Ferrero', 'Raul Quijada', '']]","[{'text': 'domain-specific fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,domain-specific fine-tuning,0.6912822723388672
2503.14258,Weihang Su,"Weihang Su, Baoqing Yue, Qingyao Ai, Yiran Hu, Jiaqi Li, Changyue
  Wang, Kaiyuan Zhang, Yueyue Wu, Yiqun Liu","JuDGE: Benchmarking Judgment Document Generation for Chinese Legal
  System",,,,,cs.CL cs.AI cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper introduces JuDGE (Judgment Document Generation Evaluation), a
novel benchmark for evaluating the performance of judgment document generation
in the Chinese legal system. We define the task as generating a complete legal
judgment document from the given factual description of the case. To facilitate
this benchmark, we construct a comprehensive dataset consisting of factual
descriptions from real legal cases, paired with their corresponding full
judgment documents, which serve as the ground truth for evaluating the quality
of generated documents. This dataset is further augmented by two external legal
corpora that provide additional legal knowledge for the task: one comprising
statutes and regulations, and the other consisting of a large collection of
past judgment documents. In collaboration with legal professionals, we
establish a comprehensive automated evaluation framework to assess the quality
of generated judgment documents across various dimensions. We evaluate various
baseline approaches, including few-shot in-context learning, fine-tuning, and a
multi-source retrieval-augmented generation (RAG) approach, using both general
and legal-domain LLMs. The experimental results demonstrate that, while RAG
approaches can effectively improve performance in this task, there is still
substantial room for further improvement. All the codes and datasets are
available at: https://github.com/oneal2000/JuDGE.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 13:48:18 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 15:09:51 GMT'}]",2025-03-21,"[['Su', 'Weihang', ''], ['Yue', 'Baoqing', ''], ['Ai', 'Qingyao', ''], ['Hu', 'Yiran', ''], ['Li', 'Jiaqi', ''], ['Wang', 'Changyue', ''], ['Zhang', 'Kaiyuan', ''], ['Wu', 'Yueyue', ''], ['Liu', 'Yiqun', '']]","[{'text': 'few-shot in-context learning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'RAG', 'label': 'RAG'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.14287,Enrico Tosi,"Enrico Tosi, Panwei Hu, Aleksandar Ichkov, Marina Petrova, Ljiljana
  Simi\'c","Cross-Environment Transfer Learning for Location-Aided Beam Prediction
  in 5G and Beyond Millimeter-Wave Networks",,,,,eess.SP cs.SY eess.SY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Millimeter-wave (mm-wave) communications requirebeamforming and consequent
precise beam alignmentbetween the gNodeB (gNB) and the user equipment (UE)
toovercome high propagation losses. This beam alignment needs tobe constantly
updated for different UE locations based on beamsweepingradio frequency
measurements, leading to significantbeam management overhead. One potential
solution involvesusing machine learning (ML) beam prediction algorithms
thatleverage UE position information to select the serving beamwithout the
overhead of beam sweeping. However, the highlysite-specific nature of mm-wave
propagation means that MLmodels require training from scratch for each
scenario, whichis inefficient in practice. In this paper, we propose a
robustcross-environment transfer learning solution for location-aidedbeam
prediction, whereby the ML model trained on a referencegNB is transferred to a
target gNB by fine-tuning with a limiteddataset. Extensive simulation results
based on ray-tracing in twourban environments show the effectiveness of our
solution forboth inter- and intra-city model transfer. Our results show thatby
training the model on a reference gNB and transferring themodel by fine-tuning
with only 5% of the target gNB dataset,we can achieve 80% accuracy in
predicting the best beamfor the target gNB. Importantly, our approach improves
thepoor generalization accuracy of transferring the model to newenvironments
without fine-tuning by around 75 percentage points.This demonstrates that
transfer learning enables high predictionaccuracy while reducing the
computational and training datasetcollection burden of ML-based beam
prediction, making itpractical for 5G-and-beyond deployments.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 14:24:50 GMT'}]",2025-03-19,"[['Tosi', 'Enrico', ''], ['Hu', 'Panwei', ''], ['Ichkov', 'Aleksandar', ''], ['Petrova', 'Marina', ''], ['Simiƒá', 'Ljiljana', '']]","[{'text': 'transfer learning', 'label': 'Few-shot Learning'}, {'text': 'model', 'label': 'AI model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'model', 'label': 'AI model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'transfer learning', 'label': 'Few-shot Learning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.14350,Shoubin Yu,"Shoubin Yu, Difan Liu, Ziqiao Ma, Yicong Hong, Yang Zhou, Hao Tan,
  Joyce Chai, Mohit Bansal","VEGGIE: Instructional Editing and Reasoning of Video Concepts with
  Grounded Generation","First three authors contributed equally. Project page:
  https://veggie-gen.github.io/",,,,cs.CV cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent video diffusion models have enhanced video editing, but it remains
challenging to handle instructional editing and diverse tasks (e.g., adding,
removing, changing) within a unified framework. In this paper, we introduce
VEGGIE, a Video Editor with Grounded Generation from Instructions, a simple
end-to-end framework that unifies video concept editing, grounding, and
reasoning based on diverse user instructions. Specifically, given a video and
text query, VEGGIE first utilizes an MLLM to interpret user intentions in
instructions and ground them to the video contexts, generating frame-specific
grounded task queries for pixel-space responses. A diffusion model then renders
these plans and generates edited videos that align with user intent. To support
diverse tasks and complex instructions, we employ a curriculum learning
strategy: first aligning the MLLM and video diffusion model with large-scale
instructional image editing data, followed by end-to-end fine-tuning on
high-quality multitask video data. Additionally, we introduce a novel data
synthesis pipeline to generate paired instructional video editing data for
model training. It transforms static image data into diverse, high-quality
video editing samples by leveraging Image-to-Video models to inject dynamics.
VEGGIE shows strong performance in instructional video editing with different
editing skills, outperforming the best instructional baseline as a versatile
model, while other models struggle with multi-tasking. VEGGIE also excels in
video object grounding and reasoning segmentation, where other baselines fail.
We further reveal how the multiple tasks help each other and highlight
promising applications like zero-shot multimodal instructional and in-context
video editing.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 15:31:12 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 20:33:40 GMT'}]",2025-03-21,"[['Yu', 'Shoubin', ''], ['Liu', 'Difan', ''], ['Ma', 'Ziqiao', ''], ['Hong', 'Yicong', ''], ['Zhou', 'Yang', ''], ['Tan', 'Hao', ''], ['Chai', 'Joyce', ''], ['Bansal', 'Mohit', '']]","[{'text': 'end-to-end fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,end-to-end fine-tuning,0.852120041847229
2503.14374,Patrick Breheny,Tabitha K. Peter and Patrick J. Breheny,"Cross-Validation in Penalized Linear Mixed Models: Addressing Common
  Implementation Pitfalls",,,,,stat.ME,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we develop an implementation of cross-validation for penalized
linear mixed models. While these models have been proposed for correlated
high-dimensional data, the current literature implicitly assumes that tuning
parameter selection procedures developed for independent data will also work
well in this context. We argue that such naive assumptions make analysis prone
to pitfalls, several of which we will describe. Here we present a correct
implementation of cross-validation for penalized linear mixed models,
addressing these common pitfalls. We support our methods with mathematical
proof, simulation study, and real data analysis.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:07:20 GMT'}]",2025-03-19,"[['Peter', 'Tabitha K.', ''], ['Breheny', 'Patrick J.', '']]","[{'text': 'tuning\nparameter selection procedures', 'label': 'Fine-tuning'}]",Fine-tuning,"tuning
parameter selection procedures",0.5813818573951721
2503.14421,Radu Tudor Ionescu,"Vlad Hondru, Eduard Hogea, Darian Onchis, Radu Tudor Ionescu",ExDDV: A New Dataset for Explainable Deepfake Detection in Video,,,,,cs.CV cs.AI cs.CL cs.LG cs.MM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The ever growing realism and quality of generated videos makes it
increasingly harder for humans to spot deepfake content, who need to rely more
and more on automatic deepfake detectors. However, deepfake detectors are also
prone to errors, and their decisions are not explainable, leaving humans
vulnerable to deepfake-based fraud and misinformation. To this end, we
introduce ExDDV, the first dataset and benchmark for Explainable Deepfake
Detection in Video. ExDDV comprises around 5.4K real and deepfake videos that
are manually annotated with text descriptions (to explain the artifacts) and
clicks (to point out the artifacts). We evaluate a number of vision-language
models on ExDDV, performing experiments with various fine-tuning and in-context
learning strategies. Our results show that text and click supervision are both
required to develop robust explainable models for deepfake videos, which are
able to localize and describe the observed artifacts. Our novel dataset and
code to reproduce the results are available at
https://github.com/vladhondru25/ExDDV.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:55:07 GMT'}]",2025-03-19,"[['Hondru', 'Vlad', ''], ['Hogea', 'Eduard', ''], ['Onchis', 'Darian', ''], ['Ionescu', 'Radu Tudor', '']]","[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.14481,Jacob Eisenstein,"Jacob Eisenstein and Reza Aghajani and Adam Fisch and Dheeru Dua and
  Fantine Huot and Mirella Lapata and Vicky Zayats and Jonathan Berant","Don't lie to your friends: Learning what you know from collaborative
  self-play",,,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  To be helpful assistants, AI agents must be aware of their own capabilities
and limitations. This includes knowing when to answer from parametric knowledge
versus using tools, when to trust tool outputs, and when to abstain or hedge.
Such capabilities are hard to teach through supervised fine-tuning because they
require constructing examples that reflect the agent's specific capabilities.
We therefore propose a radically new approach to teaching agents what they
know: \emph{collaborative self-play}. We construct multi-agent collaborations
in which the group is rewarded for collectively arriving at correct answers.
The desired meta-knowledge emerges from the incentives built into the structure
of the interaction. We focus on small societies of agents that have access to
heterogeneous tools (corpus-specific retrieval), and therefore must collaborate
to maximize their success while minimizing their effort. Experiments show that
group-level rewards for multi-agent communities can induce policies that
\emph{transfer} to improve tool use and selective prediction in settings where
individual agents are deployed in isolation.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:53:20 GMT'}]",2025-03-19,"[['Eisenstein', 'Jacob', ''], ['Aghajani', 'Reza', ''], ['Fisch', 'Adam', ''], ['Dua', 'Dheeru', ''], ['Huot', 'Fantine', ''], ['Lapata', 'Mirella', ''], ['Zayats', 'Vicky', ''], ['Berant', 'Jonathan', '']]","[{'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,supervised fine-tuning,0.7449287176132202
2503.14523,Xinyuan Song,"Siyi Wu, Leyi Zhao, Haotian Ma, Xinyuan Song","SDF-TopoNet: A Two-Stage Framework for Tubular Structure Segmentation
  via SDF Pre-training and Topology-Aware Fine-Tuning",,,,,eess.IV cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Accurate segmentation of tubular and curvilinear structures, such as blood
vessels, neurons, and road networks, is crucial in various applications. A key
challenge is ensuring topological correctness while maintaining computational
efficiency. Existing approaches often employ topological loss functions based
on persistent homology, such as Betti error, to enforce structural consistency.
However, these methods suffer from high computational costs and are insensitive
to pixel-level accuracy, often requiring additional loss terms like Dice or MSE
to compensate. To address these limitations, we propose \textbf{SDF-TopoNet},
an improved topology-aware segmentation framework that enhances both
segmentation accuracy and training efficiency. Our approach introduces a novel
two-stage training strategy. In the pre-training phase, we utilize the signed
distance function (SDF) as an auxiliary learning target, allowing the model to
encode topological information without directly relying on computationally
expensive topological loss functions. In the fine-tuning phase, we incorporate
a dynamic adapter alongside a refined topological loss to ensure topological
correctness while mitigating overfitting and computational overhead. We
evaluate our method on five benchmark datasets. Experimental results
demonstrate that SDF-TopoNet outperforms existing methods in both topological
accuracy and quantitative segmentation metrics, while significantly reducing
training complexity.
","[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 23:54:38 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 01:43:59 GMT'}]",2025-03-21,"[['Wu', 'Siyi', ''], ['Zhao', 'Leyi', ''], ['Ma', 'Haotian', ''], ['Song', 'Xinyuan', '']]","[{'text': 'Betti error', 'label': 'BERT'}, {'text': 'fine-tuning phase', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning phase,0.7284630537033081
2503.14536,Anandakumar D,"Praveen Shastry, Sowmya Chowdary Muthulur, Naveen Kumarasami,
  Anandakumar D, Mounigasri M, Keerthana R, Kishore Prasath Venkatesh, Bargava
  Subramanian, Kalyan Sivasailam, Revathi Ezhumalai, Abitha Marimuthu","Advancing Chronic Tuberculosis Diagnostics Using Vision-Language Models:
  A Multi modal Framework for Precision Analysis","10 pages , 3 figures",,,,eess.IV cs.AI cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Background This study proposes a Vision-Language Model (VLM) leveraging the
SIGLIP encoder and Gemma-3b transformer decoder to enhance automated chronic
tuberculosis (TB) screening. By integrating chest X-ray images with clinical
data, the model addresses the challenges of manual interpretation, improving
diagnostic consistency and accessibility, particularly in resource-constrained
settings.
  Methods The VLM architecture combines a Vision Transformer (ViT) for visual
encoding and a transformer-based text encoder to process clinical context, such
as patient histories and treatment records. Cross-modal attention mechanisms
align radiographic features with textual information, while the Gemma-3b
decoder generates comprehensive diagnostic reports. The model was pre-trained
on 5 million paired medical images and texts and fine-tuned using 100,000
chronic TB-specific chest X-rays.
  Results The model demonstrated high precision (94 percent) and recall (94
percent) for detecting key chronic TB pathologies, including fibrosis,
calcified granulomas, and bronchiectasis. Area Under the Curve (AUC) scores
exceeded 0.93, and Intersection over Union (IoU) values were above 0.91,
validating its effectiveness in detecting and localizing TB-related
abnormalities.
  Conclusion The VLM offers a robust and scalable solution for automated
chronic TB diagnosis, integrating radiographic and clinical data to deliver
actionable and context-aware insights. Future work will address subtle
pathologies and dataset biases to enhance the model's generalizability,
ensuring equitable performance across diverse populations and healthcare
settings.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 13:49:29 GMT'}]",2025-03-20,"[['Shastry', 'Praveen', ''], ['Muthulur', 'Sowmya Chowdary', ''], ['Kumarasami', 'Naveen', ''], ['D', 'Anandakumar', ''], ['M', 'Mounigasri', ''], ['R', 'Keerthana', ''], ['Venkatesh', 'Kishore Prasath', ''], ['Subramanian', 'Bargava', ''], ['Sivasailam', 'Kalyan', ''], ['Ezhumalai', 'Revathi', ''], ['Marimuthu', 'Abitha', '']]","[{'text': 'Cross-modal attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'fine-tuned', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuned,0.870777428150177
2503.14637,Alexander Mathis,Merkourios Simos and Alberto Silvio Chiappa and Alexander Mathis,"Reinforcement learning-based motion imitation for physiologically
  plausible musculoskeletal motor control",,,,,cs.RO cs.AI cs.CV cs.LG q-bio.NC,http://creativecommons.org/licenses/by-sa/4.0/,"  How do humans move? The quest to understand human motion has broad
applications in numerous fields, ranging from computer animation and motion
synthesis to neuroscience, human prosthetics and rehabilitation. Although
advances in reinforcement learning (RL) have produced impressive results in
capturing human motion using simplified humanoids, controlling physiologically
accurate models of the body remains an open challenge. In this work, we present
a model-free motion imitation framework (KINESIS) to advance the understanding
of muscle-based motor control. Using a musculoskeletal model of the lower body
with 80 muscle actuators and 20 DoF, we demonstrate that KINESIS achieves
strong imitation performance on 1.9 hours of motion capture data, is
controllable by natural language through pre-trained text-to-motion generative
models, and can be fine-tuned to carry out high-level tasks such as target goal
reaching. Importantly, KINESIS generates muscle activity patterns that
correlate well with human EMG activity. The physiological plausibility makes
KINESIS a promising model for tackling challenging problems in human motor
control theory, which we highlight by investigating Bernstein's redundancy
problem in the context of locomotion. Code, videos and benchmarks will be
available at https://github.com/amathislab/Kinesis.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 18:37:49 GMT'}]",2025-03-20,"[['Simos', 'Merkourios', ''], ['Chiappa', 'Alberto Silvio', ''], ['Mathis', 'Alexander', '']]","[{'text': 'pre-trained text-to-motion generative\nmodels', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'fine-tuned', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuned,0.870777428150177
2503.14701,Podshara Chanrungmaneekul,"Podshara Chanrungmaneekul, Yiting Chen, Joshua T. Grace, Aaron M.
  Dollar, Kaiyu Hang","ARC-Calib: Autonomous Markerless Camera-to-Robot Calibration via
  Exploratory Robot Motions","8 pages, 9 figures",,,,cs.RO cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Camera-to-robot (also known as eye-to-hand) calibration is a critical
component of vision-based robot manipulation. Traditional marker-based methods
often require human intervention for system setup. Furthermore, existing
autonomous markerless calibration methods typically rely on pre-trained robot
tracking models that impede their application on edge devices and require
fine-tuning for novel robot embodiments. To address these limitations, this
paper proposes a model-based markerless camera-to-robot calibration framework,
ARC-Calib, that is fully autonomous and generalizable across diverse robots and
scenarios without requiring extensive data collection or learning. First,
exploratory robot motions are introduced to generate easily trackable
trajectory-based visual patterns in the camera's image frames. Then, a
geometric optimization framework is proposed to exploit the coplanarity and
collinearity constraints from the observed motions to iteratively refine the
estimated calibration result. Our approach eliminates the need for extra effort
in either environmental marker setup or data collection and model training,
rendering it highly adaptable across a wide range of real-world autonomous
systems. Extensive experiments are conducted in both simulation and the real
world to validate its robustness and generalizability.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 20:03:32 GMT'}]",2025-03-20,"[['Chanrungmaneekul', 'Podshara', ''], ['Chen', 'Yiting', ''], ['Grace', 'Joshua T.', ''], ['Dollar', 'Aaron M.', ''], ['Hang', 'Kaiyu', '']]","[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.14718,Hakyung Sung,"Hakyung Sung, Gyu-Ho Shin","Second language Korean Universal Dependency treebank v1.2: Focus on data
  augmentation and annotation scheme refinement",,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  We expand the second language (L2) Korean Universal Dependencies (UD)
treebank with 5,454 manually annotated sentences. The annotation guidelines are
also revised to better align with the UD framework. Using this enhanced
treebank, we fine-tune three Korean language models and evaluate their
performance on in-domain and out-of-domain L2-Korean datasets. The results show
that fine-tuning significantly improves their performance across various
metrics, thus highlighting the importance of using well-tailored L2 datasets
for fine-tuning first-language-based, general-purpose language models for the
morphosyntactic analysis of L2 data.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 20:42:42 GMT'}]",2025-03-20,"[['Sung', 'Hakyung', ''], ['Shin', 'Gyu-Ho', '']]","[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.14755,Omar Rakha,"Omar E. Rakha, Hazem M. Abbas","Language Independent Named Entity Recognition via Orthogonal
  Transformation of Word Vectors",Paper was initially released in 2017 but was never published,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Word embeddings have been a key building block for NLP in which models relied
heavily on word embeddings in many different tasks. In this paper, a model is
proposed based on using Bidirectional LSTM/CRF with word embeddings to perform
named entity recognition for any language. This is done by training a model on
a source language (English) and transforming word embeddings from the target
language into word embeddings of the source language by using an orthogonal
linear transformation matrix. Evaluation of the model shows that by training a
model on an English dataset the model was capable of detecting named entities
in an Arabic dataset without neither training or fine tuning the model on an
Arabic language dataset.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 21:57:58 GMT'}]",2025-03-20,"[['Rakha', 'Omar E.', ''], ['Abbas', 'Hazem M.', '']]","[{'text': 'Word embeddings', 'label': 'Embedding'}, {'text': 'word embeddings', 'label': 'Embedding'}, {'text': 'word embeddings', 'label': 'Embedding'}, {'text': 'word embeddings', 'label': 'Embedding'}, {'text': 'word embeddings', 'label': 'Embedding'}, {'text': 'fine tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine tuning,0.9617297649383545
2503.14813,Chunhui Zhang,"Zhongyu Ouyang, Chunhui Zhang, Yaning Jia and Soroush Vosoughi",Scaled Supervision is an Implicit Lipschitz Regularizer,"Accepted to the International AAAI Conference on Web and Social Media
  (ICWSM 2025)",,,,cs.LG cs.IR,http://creativecommons.org/licenses/by/4.0/,"  In modern social media, recommender systems (RecSys) rely on the
click-through rate (CTR) as the standard metric to evaluate user engagement.
CTR prediction is traditionally framed as a binary classification task to
predict whether a user will interact with a given item. However, this approach
overlooks the complexity of real-world social modeling, where the user, item,
and their interactive features change dynamically in fast-paced online
environments. This dynamic nature often leads to model instability, reflected
in overfitting short-term fluctuations rather than higher-level interactive
patterns. While overfitting calls for more scaled and refined supervisions,
current solutions often rely on binary labels that overly simplify fine-grained
user preferences through the thresholding process, which significantly reduces
the richness of the supervision. Therefore, we aim to alleviate the overfitting
problem by increasing the supervision bandwidth in CTR training. Specifically,
(i) theoretically, we formulate the impact of fine-grained preferences on model
stability as a Lipschitz constrain; (ii) empirically, we discover that scaling
the supervision bandwidth can act as an implicit Lipschitz regularizer, stably
optimizing existing CTR models to achieve better generalizability. Extensive
experiments show that this scaled supervision significantly and consistently
improves the optimization process and the performance of existing CTR models,
even without the need for additional hyperparameter tuning.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 01:01:28 GMT'}]",2025-03-20,"[['Ouyang', 'Zhongyu', ''], ['Zhang', 'Chunhui', ''], ['Jia', 'Yaning', ''], ['Vosoughi', 'Soroush', '']]","[{'text': 'hyperparameter tuning', 'label': 'Fine-tuning'}]",Fine-tuning,hyperparameter tuning,0.6193697452545166
2503.14815,Hua Pei,"Yu Wang, Hua Pei","Study of event and particle selection effects on elliptic flow
  background at the isobar experiments based on AMPT model",,,,,nucl-th nucl-ex,http://creativecommons.org/licenses/by/4.0/,"  Measurement of the Chiral Magnetic Effect (CME) has been a popular topic of
high-energy nuclear physics in the last decade. The flow correlation $\gamma$
between charged hadron pairs of the same and opposite charges and their
difference $\Delta \gamma$ were measured to separate the CME-driven signal from
the collective flow background especially second-order elliptic $v_{2}$. The
STAR experiment have stepped further to the isobar experiment to compare
$\gamma$ and $\Delta \gamma$ between Ru+Ru and Zr+Zr
~\cite{PhysRevC.105.014901}, which were theoretically expected to produce the
same elliptic flow background but different CME signals. However, the measured
flow backgrounds also differ between Ru+Ru and Zr+Zr, indicating more
fine-tuning of RP and centrality definition necessary.
  This analysis applied the AMPT model~\cite{PhysRevC.72.064901} to simulate
the same collision system and energy as the STAR isobar experiment. Since the
AMPT model does not include magnetic field effects, we expect comparing its
output between Ru+Ru and Zr+Zr collision systems can provide an insight of the
possible bias of flow background definition, and help improve the measurement
of CME signal in real experiments. Multiple combinations of centrality and flow
definition were chosen to study how the $v_2$ and their difference would be
affected, especially by varying the particles selection of charge versus
neutral properties and broadening (pseudo-)rapidity regions, while STAR CME
work relied on charged-only particles at central rapidity.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 01:08:40 GMT'}]",2025-03-20,"[['Wang', 'Yu', ''], ['Pei', 'Hua', '']]","[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'AMPT model', 'label': 'AI model'}, {'text': 'AMPT model', 'label': 'AI model'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.14836,Kunyang Li,"Kunyang Li, Jean-Charles Noirot Ferrand, Ryan Sheatsley, Blaine Hoak,
  Yohan Beugin, Eric Pauley, Patrick McDaniel",On the Robustness Tradeoff in Fine-Tuning,,,,,cs.LG cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Fine-tuning has become the standard practice for adapting pre-trained
(upstream) models to downstream tasks. However, the impact on model robustness
is not well understood. In this work, we characterize the robustness-accuracy
trade-off in fine-tuning. We evaluate the robustness and accuracy of fine-tuned
models over 6 benchmark datasets and 7 different fine-tuning strategies. We
observe a consistent trade-off between adversarial robustness and accuracy.
Peripheral updates such as BitFit are more effective for simple tasks--over 75%
above the average measured with area under the Pareto frontiers on CIFAR-10 and
CIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention
layers via Compacter, achieves a better Pareto frontier on more complex
tasks--57.5% and 34.6% above the average on Caltech-256 and CUB-200,
respectively. Lastly, we observe that robustness of fine-tuning against
out-of-distribution data closely tracks accuracy. These insights emphasize the
need for robustness-aware fine-tuning to ensure reliable real-world
deployments.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 02:35:01 GMT'}]",2025-03-20,"[['Li', 'Kunyang', ''], ['Ferrand', 'Jean-Charles Noirot', ''], ['Sheatsley', 'Ryan', ''], ['Hoak', 'Blaine', ''], ['Beugin', 'Yohan', ''], ['Pauley', 'Eric', ''], ['McDaniel', 'Patrick', '']]","[{'text': 'Fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'attention\nlayers', 'label': 'Attention mechanism'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,Fine-tuning,1.0000001192092896
2503.14849,Zhuoyi Yang,Zhuoyi Yang and Ian G. Harris,LogLLaMA: Transformer-based log anomaly detection with LLaMA,"8 pages, 5 figures",,,,cs.LG cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Log anomaly detection refers to the task that distinguishes the anomalous log
messages from normal log messages. Transformer-based large language models
(LLMs) are becoming popular for log anomaly detection because of their superb
ability to understand complex and long language patterns. In this paper, we
propose LogLLaMA, a novel framework that leverages LLaMA2. LogLLaMA is first
finetuned on normal log messages from three large-scale datasets to learn their
patterns. After finetuning, the model is capable of generating successive log
messages given previous log messages. Our generative model is further trained
to identify anomalous log messages using reinforcement learning (RL). The
experimental results show that LogLLaMA outperforms the state-of-the-art
approaches for anomaly detection on BGL, Thunderbird, and HDFS datasets.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 03:13:37 GMT'}]",2025-03-20,"[['Yang', 'Zhuoyi', ''], ['Harris', 'Ian G.', '']]","[{'text': 'finetuned', 'label': 'Fine-tuning'}, {'text': 'finetuning', 'label': 'Fine-tuning'}]",Fine-tuning,finetuning,0.5753726363182068
2503.14878,Murtaza Zohair,"Murtaza Zohair, Vidushi Sharma, Eduardo A. Soares, Khanh Nguyen,
  Maxwell Giammona, Linda Sundberg, Andy Tek, Emilio A. V. Vital, Young-Hye La","Chemical Foundation Model Guided Design of High Ionic Conductivity
  Electrolyte Formulations",,,,,cond-mat.mtrl-sci physics.chem-ph,http://creativecommons.org/licenses/by/4.0/,"  Designing optimal formulations is a major challenge in developing
electrolytes for the next generation of rechargeable batteries due to the vast
combinatorial design space and complex interplay between multiple constituents.
Machine learning (ML) offers a powerful tool to uncover underlying chemical
design rules and accelerate the process of formulation discovery. In this work,
we present an approach to design new formulations that can achieve target
performance, using a generalizable chemical foundation model. The chemical
foundation model is fine-tuned on an experimental dataset of 13,666 ionic
conductivity values curated from the lithium-ion battery literature. The
fine-tuned model is used to discover 7 novel high conductivity electrolyte
formulations through generative screening, improving the conductivity of LiFSI
and LiDFOB based electrolytes by 82% and 172%, respectively. These findings
highlight a generalizable workflow that is highly adaptable to the discovery of
chemical mixtures with tailored properties to address challenges in energy
storage and beyond.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 04:14:19 GMT'}]",2025-03-20,"[['Zohair', 'Murtaza', ''], ['Sharma', 'Vidushi', ''], ['Soares', 'Eduardo A.', ''], ['Nguyen', 'Khanh', ''], ['Giammona', 'Maxwell', ''], ['Sundberg', 'Linda', ''], ['Tek', 'Andy', ''], ['Vital', 'Emilio A. V.', ''], ['La', 'Young-Hye', '']]","[{'text': 'chemical foundation model', 'label': 'Foundation Model'}, {'text': 'chemical\nfoundation model', 'label': 'Foundation Model'}, {'text': 'fine-tuned', 'label': 'Fine-tuning'}, {'text': 'generative screening', 'label': 'Few-shot Learning'}]",Fine-tuning,fine-tuned,0.870777428150177
2503.14897,Vaibhav Rathore,"Vaibhav Rathore, Shubhranil B, Saikat Dutta, Sarthak Mehrotra, Zsolt
  Kira, Biplab Banerjee","When Domain Generalization meets Generalized Category Discovery: An
  Adaptive Task-Arithmetic Driven Approach",,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Generalized Class Discovery (GCD) clusters base and novel classes in a target
domain using supervision from a source domain with only base classes. Current
methods often falter with distribution shifts and typically require access to
target data during training, which can sometimes be impractical. To address
this issue, we introduce the novel paradigm of Domain Generalization in GCD
(DG-GCD), where only source data is available for training, while the target
domain, with a distinct data distribution, remains unseen until inference. To
this end, our solution, DG2CD-Net, aims to construct a domain-independent,
discriminative embedding space for GCD. The core innovation is an episodic
training strategy that enhances cross-domain generalization by adapting a base
model on tasks derived from source and synthetic domains generated by a
foundation model. Each episode focuses on a cross-domain GCD task, diversifying
task setups over episodes and combining open-set domain adaptation with a novel
margin loss and representation learning for optimizing the feature space
progressively. To capture the effects of fine-tuning on the base model, we
extend task arithmetic by adaptively weighting the local task vectors
concerning the fine-tuned models based on their GCD performance on a validation
distribution. This episodic update mechanism boosts the adaptability of the
base model to unseen targets. Experiments across three datasets confirm that
DG2CD-Net outperforms existing GCD methods customized for DG-GCD.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 04:48:16 GMT'}]",2025-03-20,"[['Rathore', 'Vaibhav', ''], ['B', 'Shubhranil', ''], ['Dutta', 'Saikat', ''], ['Mehrotra', 'Sarthak', ''], ['Kira', 'Zsolt', ''], ['Banerjee', 'Biplab', '']]","[{'text': 'DG2CD-Net', 'label': 'Foundation Model'}, {'text': 'foundation model', 'label': 'Foundation Model'}, {'text': 'representation learning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'base model', 'label': 'Foundation Model'}, {'text': 'base model', 'label': 'Foundation Model'}, {'text': 'DG2CD-Net', 'label': 'Foundation Model'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.14953,Yang Liu,"Yang Liu, Wentao Feng, Zhuoyao Liu, Shudong Huang, Jiancheng Lv","Aligning Information Capacity Between Vision and Language via
  Dense-to-Sparse Feature Distillation for Image-Text Matching",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Enabling Visual Semantic Models to effectively handle multi-view description
matching has been a longstanding challenge. Existing methods typically learn a
set of embeddings to find the optimal match for each view's text and compute
similarity. However, the visual and text embeddings learned through these
approaches have limited information capacity and are prone to interference from
locally similar negative samples. To address this issue, we argue that the
information capacity of embeddings is crucial and propose Dense-to-Sparse
Feature Distilled Visual Semantic Embedding (D2S-VSE), which enhances the
information capacity of sparse text by leveraging dense text distillation.
Specifically, D2S-VSE is a two-stage framework. In the pre-training stage, we
align images with dense text to enhance the information capacity of visual
semantic embeddings. In the fine-tuning stage, we optimize two tasks
simultaneously, distilling dense text embeddings to sparse text embeddings
while aligning images and sparse texts, enhancing the information capacity of
sparse text embeddings. Our proposed D2S-VSE model is extensively evaluated on
the large-scale MS-COCO and Flickr30K datasets, demonstrating its superiority
over recent state-of-the-art methods.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 07:42:24 GMT'}]",2025-03-20,"[['Liu', 'Yang', ''], ['Feng', 'Wentao', ''], ['Liu', 'Zhuoyao', ''], ['Huang', 'Shudong', ''], ['Lv', 'Jiancheng', '']]","[{'text': 'Visual Semantic Embedding', 'label': 'Embedding'}, {'text': 'D2S-VSE', 'label': 'Embedding'}, {'text': 'dense text distillation', 'label': 'Knowledge distillation'}, {'text': 'visual\nsemantic embeddings', 'label': 'Embedding'}, {'text': 'fine-tuning stage', 'label': 'Fine-tuning'}, {'text': 'sparse text embeddings', 'label': 'Embedding'}, {'text': 'sparse text embeddings', 'label': 'Embedding'}]",Fine-tuning,fine-tuning stage,0.7817779779434204
2503.14998,Marta Hasny,"Marta Hasny, Maxime Di Folco, Keno Bressem, Julia Schnabel",TGV: Tabular Data-Guided Learning of Visual Cardiac Representations,,,,,cs.CV,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Contrastive learning methods in computer vision typically rely on different
views of the same image to form pairs. However, in medical imaging, we often
seek to compare entire patients with different phenotypes rather than just
multiple augmentations of one scan. We propose harnessing clinically relevant
tabular data to identify distinct patient phenotypes and form more meaningful
pairs in a contrastive learning framework. Our method uses tabular attributes
to guide the training of visual representations, without requiring a joint
embedding space. We demonstrate its strength using short-axis cardiac MR images
and clinical attributes from the UK Biobank, where tabular data helps to more
effectively distinguish between patient subgroups. Evaluation on downstream
tasks, including fine-tuning and zero-shot prediction of cardiovascular artery
diseases and cardiac phenotypes, shows that incorporating tabular data yields
stronger visual representations than conventional methods that rely solely on
image augmentations or combined image-tabular embeddings. Furthermore, we
demonstrate that image encoders trained with tabular guidance are capable of
embedding demographic information in their representations, allowing them to
use insights from tabular data for unimodal predictions, making them
well-suited to real-world medical settings where extensive clinical annotations
may not be routinely available at inference time. The code will be available on
GitHub.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 08:49:55 GMT'}]",2025-03-20,"[['Hasny', 'Marta', ''], ['Di Folco', 'Maxime', ''], ['Bressem', 'Keno', ''], ['Schnabel', 'Julia', '']]","[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'zero-shot prediction', 'label': 'Zero-shot Learning'}, {'text': 'combined image-tabular embeddings', 'label': 'Embedding'}, {'text': 'GitHub', 'label': 'Open-source LLMs'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.15027,Takuma Akimoto,"Soma Shiraki, Eli Barkai, Takuma Akimoto",Tunable Anomalous Diffusion in Subrecoil-Laser-Cooled Atoms,"9 pages, 4 figures",,,,cond-mat.stat-mech,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The control of atomic motion through laser cooling has revolutionized quantum
technologies, enabling applications ranging from quantum computing to precision
metrology. However, the spatial spreading of subrecoil-laser-cooled atoms --
crucial for understanding cooling mechanisms and atomic confinement -- remains
largely unexplored. Here, we analyze anomalous diffusion in
subrecoil-laser-cooled atoms, where a velocity-dependent fluorescence rate
$R(v) \propto |v|^{\alpha}$ governs transport properties. By tuning $\alpha$,
we uncover transitions between normal, subdiffusive, and superdiffusive
regimes. Notably, at $\alpha = 3/2$, diffusion is minimized, leading to optimal
atomic confinement. These findings advance the understanding of anomalous
transport in laser-cooled systems and offer new avenues for precise control of
atomic motion.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 09:22:47 GMT'}]",2025-03-20,"[['Shiraki', 'Soma', ''], ['Barkai', 'Eli', ''], ['Akimoto', 'Takuma', '']]","[{'text': 'tuning', 'label': 'Fine-tuning'}]",Fine-tuning,tuning,0.844900906085968
2503.15197,Feifei Li,"Feifei Li, Mi Zhang, Yiming Sun and Min Yang","Detect-and-Guide: Self-regulation of Diffusion Models for Safe
  Text-to-Image Generation via Guideline Token Optimization",CVPR25,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Text-to-image diffusion models have achieved state-of-the-art results in
synthesis tasks; however, there is a growing concern about their potential
misuse in creating harmful content. To mitigate these risks, post-hoc model
intervention techniques, such as concept unlearning and safety guidance, have
been developed. However, fine-tuning model weights or adapting the hidden
states of the diffusion model operates in an uninterpretable way, making it
unclear which part of the intermediate variables is responsible for unsafe
generation. These interventions severely affect the sampling trajectory when
erasing harmful concepts from complex, multi-concept prompts, thus hindering
their practical use in real-world settings. In this work, we propose the safe
generation framework Detect-and-Guide (DAG), leveraging the internal knowledge
of diffusion models to perform self-diagnosis and fine-grained self-regulation
during the sampling process. DAG first detects harmful concepts from noisy
latents using refined cross-attention maps of optimized tokens, then applies
safety guidance with adaptive strength and editing regions to negate unsafe
generation. The optimization only requires a small annotated dataset and can
provide precise detection maps with generalizability and concept specificity.
Moreover, DAG does not require fine-tuning of diffusion models, and therefore
introduces no loss to their generation diversity. Experiments on erasing sexual
content show that DAG achieves state-of-the-art safe generation performance,
balancing harmfulness mitigation and text-following performance on
multi-concept real-world prompts.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 13:37:52 GMT'}]",2025-03-20,"[['Li', 'Feifei', ''], ['Zhang', 'Mi', ''], ['Sun', 'Yiming', ''], ['Yang', 'Min', '']]","[{'text': 'concept unlearning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuning model weights', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning model weights,0.67231285572052
2503.15212,Sarah Matta,"Lucie Berger, Mathieu Lamard, Philippe Zhang, Laurent Borderie,
  Alexandre Le Guilcher, Pascale Massin, B\'eatrice Cochener, Gwenol\'e Quellec
  and Sarah Matta","Context-Aware Vision Language Foundation Models for Ocular Disease
  Screening in Retinal Images",4 pages,,,,eess.IV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Foundation models are large-scale versatile systems trained on vast
quantities of diverse data to learn generalizable representations. Their
adaptability with minimal fine-tuning makes them particularly promising for
medical imaging, where data variability and domain shifts are major challenges.
Currently, two types of foundation models dominate the literature:
self-supervised models and more recent vision-language models. In this study,
we advance the application of vision-language foundation (VLF) models for
ocular disease screening using the OPHDIAT dataset, which includes nearly
700,000 fundus photographs from a French diabetic retinopathy (DR) screening
network. This dataset provides extensive clinical data (patient-specific
information such as diabetic health conditions, and treatments), labeled
diagnostics, ophthalmologists text-based findings, and multiple retinal images
for each examination. Building on the FLAIR model $\unicode{x2013}$ a VLF model
for retinal pathology classification $\unicode{x2013}$ we propose novel
context-aware VLF models (e.g jointly analyzing multiple images from the same
visit or taking advantage of past diagnoses and contextual data) to fully
leverage the richness of the OPHDIAT dataset and enhance robustness to domain
shifts. Our approaches were evaluated on both in-domain (a testing subset of
OPHDIAT) and out-of-domain data (public datasets) to assess their
generalization performance. Our model demonstrated improved in-domain
performance for DR grading, achieving an area under the curve (AUC) ranging
from 0.851 to 0.9999, and generalized well to ocular disease detection on
out-of-domain data (AUC: 0.631-0.913).
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 13:52:33 GMT'}]",2025-03-20,"[['Berger', 'Lucie', ''], ['Lamard', 'Mathieu', ''], ['Zhang', 'Philippe', ''], ['Borderie', 'Laurent', ''], ['Guilcher', 'Alexandre Le', ''], ['Massin', 'Pascale', ''], ['Cochener', 'B√©atrice', ''], ['Quellec', 'Gwenol√©', ''], ['Matta', 'Sarah', '']]","[{'text': 'minimal fine-tuning', 'label': 'Fine-tuning'}, {'text': 'self-supervised models', 'label': 'Foundation Model'}, {'text': 'vision-language models', 'label': 'Foundation Model'}, {'text': 'FLAIR model', 'label': 'Foundation Model'}]",Fine-tuning,minimal fine-tuning,0.848320722579956
2503.15221,Josu\'e P\'erez Sabater,"Rodrigo Oliver, Josu\'e P\'erez-Sabater, Leire Paz-Arbaizar, Alejandro
  Lancho, Antonio Art\'es, Pablo M. Olmos",A Foundation Model for Patient Behavior Monitoring and Suicide Detection,"10 pages (31 with appendices), 6 figures (13 with appendices);
  submitted to UAI 2025",,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Foundation models (FMs) have achieved remarkable success across various
domains, yet their adoption in healthcare remains limited. While significant
advances have been made in medical imaging, genetic biomarkers, and time series
from electronic health records, the potential of FMs for patient behavior
monitoring through wearable devices remains underexplored. These datasets are
inherently heterogeneous, multisource, and often exhibit high rates of missing
data, posing unique challenges. This paper introduces a novel FM based on a
modified vector quantized variational autoencoder (VQ-VAE), specifically
designed to process real-world data from wearable devices. We demonstrate that
our pretrained FM, trained on a broad cohort of psychiatric patients, performs
downstream tasks via its latent representation without fine-tuning on a
held-out cohort of suicidal patients. To illustrate this, we develop a
probabilistic change-point detection algorithm for suicide detection and
demonstrate the FM's effectiveness in predicting emotional states. Our results
show that the discrete latent structure of the VQ-VAE outperforms a
state-of-the-art Informer architecture in unsupervised suicide detection, while
matching its performance in supervised emotion prediction when the latent
dimensionality is increased, though at the cost of reduced unsupervised
accuracy. This trade-off highlights the need for future FMs to integrate hybrid
discrete-continuous structures for balanced performance across tasks.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 14:01:16 GMT'}]",2025-03-20,"[['Oliver', 'Rodrigo', ''], ['P√©rez-Sabater', 'Josu√©', ''], ['Paz-Arbaizar', 'Leire', ''], ['Lancho', 'Alejandro', ''], ['Art√©s', 'Antonio', ''], ['Olmos', 'Pablo M.', '']]","[{'text': 'FMs', 'label': 'Foundation Model'}, {'text': 'FMs', 'label': 'Foundation Model'}, {'text': 'VQ-VAE', 'label': 'Foundation Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'VQ-VAE', 'label': 'Foundation Model'}, {'text': 'FMs', 'label': 'Foundation Model'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.15250,Quentin Nater,"Quentin Nater, Mourad Khayati, Jacques Pasquier",ImputeGAP: A Comprehensive Library for Time Series Imputation,,,,,cs.LG cs.DB,http://creativecommons.org/licenses/by/4.0/,"  With the prevalence of sensor failures, imputation--the process of estimating
missing values--has emerged as the cornerstone of time series data preparation.
While numerous imputation algorithms have been developed to address these data
gaps, existing libraries provide limited support. Furthermore, they often lack
the ability to simulate realistic patterns of time series missing data and fail
to account for the impact of imputation on subsequent downstream analysis.
  This paper introduces ImputeGAP, a comprehensive library for time series
imputation that supports a diverse range of imputation methods and modular
missing data simulation catering to datasets with varying characteristics. The
library includes extensive customization options, such as automated
hyperparameter tuning, benchmarking, explainability, downstream evaluation, and
compatibility with popular time series frameworks.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 14:24:20 GMT'}]",2025-03-20,"[['Nater', 'Quentin', ''], ['Khayati', 'Mourad', ''], ['Pasquier', 'Jacques', '']]","[{'text': 'automated\nhyperparameter tuning', 'label': 'Fine-tuning'}, {'text': 'benchmarking', 'label': 'Fine-tuning'}]",Fine-tuning,"automated
hyperparameter tuning",0.5582404136657715
2503.15310,Mike Walmsley,"Euclid Collaboration: M. Walmsley, M. Huertas-Company, L. Quilley, K.
  L. Masters, S. Kruk, K. A. Remmelgas, J. J. Popp, E. Romelli, D. O'Ryan, H.
  J. Dickinson, C. J. Lintott, S. Serjeant, R. J. Smethurst, B. Simmons, J.
  Shingirai Makechemu, I. L. Garland, H. Roberts, K. Mantha, L. F. Fortson, T.
  G\'eron, W. Keel, E. M. Baeten, C. Macmillan, J. Bovy, S. Casas, C. De Leo,
  H. Dom\'inguez S\'anchez, J. Katona, A. Kov\'acs, N. Aghanim, B. Altieri, A.
  Amara, S. Andreon, N. Auricchio, H. Aussel, C. Baccigalupi, M. Baldi, A.
  Balestra, S. Bardelli, A. Basset, P. Battaglia, R. Bender, A. Biviano, A.
  Bonchi, E. Branchini, M. Brescia, J. Brinchmann, S. Camera, G.
  Ca\~nas-Herrera, V. Capobianco, C. Carbone, J. Carretero, F. J. Castander, M.
  Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C.
  Colodro-Conde, G. Congedo, C. J. Conselice, L. Conversi, Y. Copin, F.
  Courbin, H. M. Courtois, M. Cropper, A. Da Silva, H. Degaudenzi, G. De Lucia,
  A. M. Di Giorgio, C. Dolding, H. Dole, F. Dubath, C. A. J. Duncan, X. Dupac,
  S. Dusini, A. Ealet, S. Escoffier, M. Fabricius, M. Farina, R. Farinelli, F.
  Faustini, F. Finelli, P. Fosalba, S. Fotopoulou, M. Frailis, E. Franceschi,
  S. Galeotta, K. George, B. Gillis, C. Giocoli, P. G\'omez-Alvarez, J.
  Gracia-Carpio, B. R. Granett, A. Grazian, F. Grupp, S. Gwyn, S. V. H. Haugan,
  H. Hoekstra, W. Holmes, I. M. Hook, F. Hormuth, A. Hornstrup, P. Hudelot, K.
  Jahnke, M. Jhabvala, B. Joachimi, E. Keih\""anen, S. Kermiche, A. Kiessling,
  R. Kohley, B. Kubik, K. Kuijken, M. K\""ummel, M. Kunz, H. Kurki-Suonio, O.
  Lahav, Q. Le Boulc'h, A. M. C. Le Brun, D. Le Mignant, P. Liebing, S. Ligori,
  P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, E. Maiorano, O.
  Mansutti, S. Marcin, O. Marggraf, M. Martinelli, N. Martinet, F. Marulli, R.
  Massey, S. Maurogordato, H. J. McCracken, E. Medinaceli, S. Mei, M. Melchior,
  Y. Mellier, M. Meneghetti, E. Merlin, G. Meylan, A. Mora, M. Moresco, L.
  Moscardini, R. Nakajima, C. Neissner, R. C. Nichol, S.-M. Niemi, J. W.
  Nightingale, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, W. J. Percival,
  V. Pettorino, S. Pires, G. Polenta, M. Poncet, L. A. Popa, L. Pozzetti, F.
  Raison, R. Rebolo, A. Renzi, J. Rhodes, G. Riccio, M. Roncarelli, B.
  Rusholme, R. Saglia, Z. Sakr, A. G. S\'anchez, D. Sapone, B. Sartoris, J. A.
  Schewtschenko, P. Schneider, T. Schrabback, M. Scodeggio, A. Secroun, G.
  Seidel, M. Seiffert, S. Serrano, P. Simon, C. Sirignano, G. Sirri, L. Stanco,
  J. Steinwagner, P. Tallada-Cresp\'i, D. Tavagnacco, A. N. Taylor, H. I.
  Teplitz, I. Tereno, N. Tessore, S. Toft, R. Toledo-Moreo, F. Torradeflot, I.
  Tutusaus, E. A. Valentijn, L. Valenziano, J. Valiviita, T. Vassallo, G.
  Verdoes Kleijn, A. Veropalumbo, Y. Wang, J. Weller, A. Zacchei, G. Zamorani,
  F. M. Zerbi, I. A. Zinchenko, E. Zucca, V. Allevato, M. Ballardini, M.
  Bolzonella, E. Bozzo, C. Burigana, R. Cabanac, A. Cappi, D. Di Ferdinando, J.
  A. Escartin Vigo, L. Gabarra, J. Mart\'in-Fleitas, S. Matthew, N. Mauri, R.
  B. Metcalf, A. Pezzotta, M. P\""ontinen, C. Porciani, I. Risso, V. Scottez, M.
  Sereno, M. Tenti, M. Viel, M. Wiesmann, Y. Akrami, I. T. Andika, S. Anselmi,
  M. Archidiacono, F. Atrio-Barandela, C. Benoist, K. Benson, D. Bertacca, M.
  Bethermin, L. Bisigello, A. Blanchard, L. Blot, H. B\""ohringer, M. L. Brown,
  S. Bruton, F. Buitrago, A. Calabro, B. Camacho Quevedo, F. Caro, C. S.
  Carvalho, T. Castro, F. Cogato, A. R. Cooray, O. Cucciati, S. Davini, F. De
  Paolis, G. Desprez, A. D\'iaz-S\'anchez, J. J. Diaz, S. Di Domizio, J. M.
  Diego, P.-A. Duc, A. Enia, Y. Fang, A. G. Ferrari, A. Finoguenov, A. Fontana,
  A. Franco, K. Ganga, J. Garc\'ia-Bellido, T. Gasparetto, V. Gautard, E.
  Gaztanaga, F. Giacomini, G. Gozaliasl, M. Guidi, C. M. Gutierrez, A. Hall, W.
  G. Hartley, S. Hemmati, C. Hern\'andez-Monteagudo, H. Hildebrandt, J. Hjorth,
  J. J. E. Kajava, Y. Kang, V. Kansal, D. Karagiannis, K. Kiiveri, C. C.
  Kirkpatrick, J. Le Graet, L. Legrand, M. Lembo, F. Lepori, G. Leroy, G. F.
  Lesci, J. Lesgourgues, L. Leuzzi, T. I. Liaudat, A. Loureiro, J.
  Macias-Perez, G. Maggio, M. Magliocchetti, F. Mannucci, R. Maoli, C. J. A. P.
  Martins, L. Maurin, M. Miluzio, P. Monaco, C. Moretti, G. Morgante, C.
  Murray, S. Nadathur, K. Naidoo, A. Navarro-Alsina, S. Nesseris, F.
  Passalacqua, K. Paterson, L. Patrizii, A. Pisani, D. Potter, S. Quai, M.
  Radovich, P.-F. Rocci, G. Rodighiero, S. Sacquegna, M. Sahl\'en, D. B.
  Sanders, E. Sarpa, C. Scarlata, J. Schaye, A. Schneider, M. Schultheis, D.
  Sciotti, E. Sellentin, F. Shankar, L. C. Smith, K. Tanidis, G. Testera, R.
  Teyssier, S. Tosi, A. Troja, M. Tucci, C. Valieri, A. Venhola, D. Vergani, G.
  Verza, P. Vielzeuf, N. A. Walton, E. Soubrie, D. Scott",Euclid Quick Data Release (Q1): First visual morphology catalogue,"Data: https://doi.org/10.5281/zenodo.15002907. Paper submitted as
  part of the A&A Special Issue `Euclid Quick Data Release (Q1)'. 16 pages, 15
  figures, plus appendices",,,,astro-ph.GA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a detailed visual morphology catalogue for Euclid's Quick Release
1 (Q1). Our catalogue includes galaxy features such as bars, spiral arms, and
ongoing mergers, for the 378000 bright ($I_E < 20.5$) or extended (area $\geq
700\,$pixels) galaxies in Q1. The catalogue was created by finetuning the
Zoobot galaxy foundation models on annotations from an intensive one month
campaign by Galaxy Zoo volunteers. Our measurements are fully automated and
hence fully scaleable. This catalogue is the first 0.4% of the approximately
100 million galaxies where Euclid will ultimately resolve detailed morphology.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:27:05 GMT'}]",2025-03-20,"[['Euclid Collaboration', '', ''], ['Walmsley', 'M.', ''], ['Huertas-Company', 'M.', ''], ['Quilley', 'L.', ''], ['Masters', 'K. L.', ''], ['Kruk', 'S.', ''], ['Remmelgas', 'K. A.', ''], ['Popp', 'J. J.', ''], ['Romelli', 'E.', ''], [""O'Ryan"", 'D.', ''], ['Dickinson', 'H. J.', ''], ['Lintott', 'C. J.', ''], ['Serjeant', 'S.', ''], ['Smethurst', 'R. J.', ''], ['Simmons', 'B.', ''], ['Makechemu', 'J. Shingirai', ''], ['Garland', 'I. L.', ''], ['Roberts', 'H.', ''], ['Mantha', 'K.', ''], ['Fortson', 'L. F.', ''], ['G√©ron', 'T.', ''], ['Keel', 'W.', ''], ['Baeten', 'E. M.', ''], ['Macmillan', 'C.', ''], ['Bovy', 'J.', ''], ['Casas', 'S.', ''], ['De Leo', 'C.', ''], ['S√°nchez', 'H. Dom√≠nguez', ''], ['Katona', 'J.', ''], ['Kov√°cs', 'A.', ''], ['Aghanim', 'N.', ''], ['Altieri', 'B.', ''], ['Amara', 'A.', ''], ['Andreon', 'S.', ''], ['Auricchio', 'N.', ''], ['Aussel', 'H.', ''], ['Baccigalupi', 'C.', ''], ['Baldi', 'M.', ''], ['Balestra', 'A.', ''], ['Bardelli', 'S.', ''], ['Basset', 'A.', ''], ['Battaglia', 'P.', ''], ['Bender', 'R.', ''], ['Biviano', 'A.', ''], ['Bonchi', 'A.', ''], ['Branchini', 'E.', ''], ['Brescia', 'M.', ''], ['Brinchmann', 'J.', ''], ['Camera', 'S.', ''], ['Ca√±as-Herrera', 'G.', ''], ['Capobianco', 'V.', ''], ['Carbone', 'C.', ''], ['Carretero', 'J.', ''], ['Castander', 'F. J.', ''], ['Castellano', 'M.', ''], ['Castignani', 'G.', ''], ['Cavuoti', 'S.', ''], ['Chambers', 'K. C.', ''], ['Cimatti', 'A.', ''], ['Colodro-Conde', 'C.', ''], ['Congedo', 'G.', ''], ['Conselice', 'C. J.', ''], ['Conversi', 'L.', ''], ['Copin', 'Y.', ''], ['Courbin', 'F.', ''], ['Courtois', 'H. M.', ''], ['Cropper', 'M.', ''], ['Da Silva', 'A.', ''], ['Degaudenzi', 'H.', ''], ['De Lucia', 'G.', ''], ['Di Giorgio', 'A. M.', ''], ['Dolding', 'C.', ''], ['Dole', 'H.', ''], ['Dubath', 'F.', ''], ['Duncan', 'C. A. J.', ''], ['Dupac', 'X.', ''], ['Dusini', 'S.', ''], ['Ealet', 'A.', ''], ['Escoffier', 'S.', ''], ['Fabricius', 'M.', ''], ['Farina', 'M.', ''], ['Farinelli', 'R.', ''], ['Faustini', 'F.', ''], ['Finelli', 'F.', ''], ['Fosalba', 'P.', ''], ['Fotopoulou', 'S.', ''], ['Frailis', 'M.', ''], ['Franceschi', 'E.', ''], ['Galeotta', 'S.', ''], ['George', 'K.', ''], ['Gillis', 'B.', ''], ['Giocoli', 'C.', ''], ['G√≥mez-Alvarez', 'P.', ''], ['Gracia-Carpio', 'J.', ''], ['Granett', 'B. R.', ''], ['Grazian', 'A.', ''], ['Grupp', 'F.', ''], ['Gwyn', 'S.', ''], ['Haugan', 'S. V. H.', ''], ['Hoekstra', 'H.', ''], ['Holmes', 'W.', ''], ['Hook', 'I. M.', ''], ['Hormuth', 'F.', ''], ['Hornstrup', 'A.', ''], ['Hudelot', 'P.', ''], ['Jahnke', 'K.', ''], ['Jhabvala', 'M.', ''], ['Joachimi', 'B.', ''], ['Keih√§nen', 'E.', ''], ['Kermiche', 'S.', ''], ['Kiessling', 'A.', ''], ['Kohley', 'R.', ''], ['Kubik', 'B.', ''], ['Kuijken', 'K.', ''], ['K√ºmmel', 'M.', ''], ['Kunz', 'M.', ''], ['Kurki-Suonio', 'H.', ''], ['Lahav', 'O.', ''], [""Boulc'h"", 'Q. Le', ''], ['Brun', 'A. M. C. Le', ''], ['Mignant', 'D. Le', ''], ['Liebing', 'P.', ''], ['Ligori', 'S.', ''], ['Lilje', 'P. B.', ''], ['Lindholm', 'V.', ''], ['Lloro', 'I.', ''], ['Mainetti', 'G.', ''], ['Maino', 'D.', ''], ['Maiorano', 'E.', ''], ['Mansutti', 'O.', ''], ['Marcin', 'S.', ''], ['Marggraf', 'O.', ''], ['Martinelli', 'M.', ''], ['Martinet', 'N.', ''], ['Marulli', 'F.', ''], ['Massey', 'R.', ''], ['Maurogordato', 'S.', ''], ['McCracken', 'H. J.', ''], ['Medinaceli', 'E.', ''], ['Mei', 'S.', ''], ['Melchior', 'M.', ''], ['Mellier', 'Y.', ''], ['Meneghetti', 'M.', ''], ['Merlin', 'E.', ''], ['Meylan', 'G.', ''], ['Mora', 'A.', ''], ['Moresco', 'M.', ''], ['Moscardini', 'L.', ''], ['Nakajima', 'R.', ''], ['Neissner', 'C.', ''], ['Nichol', 'R. C.', ''], ['Niemi', 'S. -M.', ''], ['Nightingale', 'J. W.', ''], ['Padilla', 'C.', ''], ['Paltani', 'S.', ''], ['Pasian', 'F.', ''], ['Pedersen', 'K.', ''], ['Percival', 'W. J.', ''], ['Pettorino', 'V.', ''], ['Pires', 'S.', ''], ['Polenta', 'G.', ''], ['Poncet', 'M.', ''], ['Popa', 'L. A.', ''], ['Pozzetti', 'L.', ''], ['Raison', 'F.', ''], ['Rebolo', 'R.', ''], ['Renzi', 'A.', ''], ['Rhodes', 'J.', ''], ['Riccio', 'G.', ''], ['Roncarelli', 'M.', ''], ['Rusholme', 'B.', ''], ['Saglia', 'R.', ''], ['Sakr', 'Z.', ''], ['S√°nchez', 'A. G.', ''], ['Sapone', 'D.', ''], ['Sartoris', 'B.', ''], ['Schewtschenko', 'J. A.', ''], ['Schneider', 'P.', ''], ['Schrabback', 'T.', ''], ['Scodeggio', 'M.', ''], ['Secroun', 'A.', ''], ['Seidel', 'G.', ''], ['Seiffert', 'M.', ''], ['Serrano', 'S.', ''], ['Simon', 'P.', ''], ['Sirignano', 'C.', ''], ['Sirri', 'G.', ''], ['Stanco', 'L.', ''], ['Steinwagner', 'J.', ''], ['Tallada-Cresp√≠', 'P.', ''], ['Tavagnacco', 'D.', ''], ['Taylor', 'A. N.', ''], ['Teplitz', 'H. I.', ''], ['Tereno', 'I.', ''], ['Tessore', 'N.', ''], ['Toft', 'S.', ''], ['Toledo-Moreo', 'R.', ''], ['Torradeflot', 'F.', ''], ['Tutusaus', 'I.', ''], ['Valentijn', 'E. A.', ''], ['Valenziano', 'L.', ''], ['Valiviita', 'J.', ''], ['Vassallo', 'T.', ''], ['Kleijn', 'G. Verdoes', ''], ['Veropalumbo', 'A.', ''], ['Wang', 'Y.', ''], ['Weller', 'J.', ''], ['Zacchei', 'A.', ''], ['Zamorani', 'G.', ''], ['Zerbi', 'F. M.', ''], ['Zinchenko', 'I. A.', ''], ['Zucca', 'E.', ''], ['Allevato', 'V.', ''], ['Ballardini', 'M.', ''], ['Bolzonella', 'M.', ''], ['Bozzo', 'E.', ''], ['Burigana', 'C.', ''], ['Cabanac', 'R.', ''], ['Cappi', 'A.', ''], ['Di Ferdinando', 'D.', ''], ['Vigo', 'J. A. Escartin', ''], ['Gabarra', 'L.', ''], ['Mart√≠n-Fleitas', 'J.', ''], ['Matthew', 'S.', ''], ['Mauri', 'N.', ''], ['Metcalf', 'R. B.', ''], ['Pezzotta', 'A.', ''], ['P√∂ntinen', 'M.', ''], ['Porciani', 'C.', ''], ['Risso', 'I.', ''], ['Scottez', 'V.', ''], ['Sereno', 'M.', ''], ['Tenti', 'M.', ''], ['Viel', 'M.', ''], ['Wiesmann', 'M.', ''], ['Akrami', 'Y.', ''], ['Andika', 'I. T.', ''], ['Anselmi', 'S.', ''], ['Archidiacono', 'M.', ''], ['Atrio-Barandela', 'F.', ''], ['Benoist', 'C.', ''], ['Benson', 'K.', ''], ['Bertacca', 'D.', ''], ['Bethermin', 'M.', ''], ['Bisigello', 'L.', ''], ['Blanchard', 'A.', ''], ['Blot', 'L.', ''], ['B√∂hringer', 'H.', ''], ['Brown', 'M. L.', ''], ['Bruton', 'S.', ''], ['Buitrago', 'F.', ''], ['Calabro', 'A.', ''], ['Quevedo', 'B. Camacho', ''], ['Caro', 'F.', ''], ['Carvalho', 'C. S.', ''], ['Castro', 'T.', ''], ['Cogato', 'F.', ''], ['Cooray', 'A. R.', ''], ['Cucciati', 'O.', ''], ['Davini', 'S.', ''], ['De Paolis', 'F.', ''], ['Desprez', 'G.', ''], ['D√≠az-S√°nchez', 'A.', ''], ['Diaz', 'J. J.', ''], ['Di Domizio', 'S.', ''], ['Diego', 'J. M.', ''], ['Duc', 'P. -A.', ''], ['Enia', 'A.', ''], ['Fang', 'Y.', ''], ['Ferrari', 'A. G.', ''], ['Finoguenov', 'A.', ''], ['Fontana', 'A.', ''], ['Franco', 'A.', ''], ['Ganga', 'K.', ''], ['Garc√≠a-Bellido', 'J.', ''], ['Gasparetto', 'T.', ''], ['Gautard', 'V.', ''], ['Gaztanaga', 'E.', ''], ['Giacomini', 'F.', ''], ['Gozaliasl', 'G.', ''], ['Guidi', 'M.', ''], ['Gutierrez', 'C. M.', ''], ['Hall', 'A.', ''], ['Hartley', 'W. G.', ''], ['Hemmati', 'S.', ''], ['Hern√°ndez-Monteagudo', 'C.', ''], ['Hildebrandt', 'H.', ''], ['Hjorth', 'J.', ''], ['Kajava', 'J. J. E.', ''], ['Kang', 'Y.', ''], ['Kansal', 'V.', ''], ['Karagiannis', 'D.', ''], ['Kiiveri', 'K.', ''], ['Kirkpatrick', 'C. C.', ''], ['Graet', 'J. Le', ''], ['Legrand', 'L.', ''], ['Lembo', 'M.', ''], ['Lepori', 'F.', ''], ['Leroy', 'G.', ''], ['Lesci', 'G. F.', ''], ['Lesgourgues', 'J.', ''], ['Leuzzi', 'L.', ''], ['Liaudat', 'T. I.', ''], ['Loureiro', 'A.', ''], ['Macias-Perez', 'J.', ''], ['Maggio', 'G.', ''], ['Magliocchetti', 'M.', ''], ['Mannucci', 'F.', ''], ['Maoli', 'R.', ''], ['Martins', 'C. J. A. P.', ''], ['Maurin', 'L.', ''], ['Miluzio', 'M.', ''], ['Monaco', 'P.', ''], ['Moretti', 'C.', ''], ['Morgante', 'G.', ''], ['Murray', 'C.', ''], ['Nadathur', 'S.', ''], ['Naidoo', 'K.', ''], ['Navarro-Alsina', 'A.', ''], ['Nesseris', 'S.', ''], ['Passalacqua', 'F.', ''], ['Paterson', 'K.', ''], ['Patrizii', 'L.', ''], ['Pisani', 'A.', ''], ['Potter', 'D.', ''], ['Quai', 'S.', ''], ['Radovich', 'M.', ''], ['Rocci', 'P. -F.', ''], ['Rodighiero', 'G.', ''], ['Sacquegna', 'S.', ''], ['Sahl√©n', 'M.', ''], ['Sanders', 'D. B.', ''], ['Sarpa', 'E.', ''], ['Scarlata', 'C.', ''], ['Schaye', 'J.', ''], ['Schneider', 'A.', ''], ['Schultheis', 'M.', ''], ['Sciotti', 'D.', ''], ['Sellentin', 'E.', ''], ['Shankar', 'F.', ''], ['Smith', 'L. C.', ''], ['Tanidis', 'K.', ''], ['Testera', 'G.', ''], ['Teyssier', 'R.', ''], ['Tosi', 'S.', ''], ['Troja', 'A.', ''], ['Tucci', 'M.', ''], ['Valieri', 'C.', ''], ['Venhola', 'A.', ''], ['Vergani', 'D.', ''], ['Verza', 'G.', ''], ['Vielzeuf', 'P.', ''], ['Walton', 'N. A.', ''], ['Soubrie', 'E.', ''], ['Scott', 'D.', '']]","[{'text': 'finetuning', 'label': 'Fine-tuning'}, {'text': 'Zoobot galaxy foundation models', 'label': 'Foundation Model'}]",Fine-tuning,finetuning,0.5753726363182068
2503.15352,Abhi Kamboj,"Abhi Kamboj, Minh N. Do","Leveraging Perfect Multimodal Alignment and Gaussian Assumptions for
  Cross-modal Transfer",,,,,cs.LG cs.AI cs.CV eess.SP,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Multimodal alignment aims to construct a joint latent vector space where two
modalities representing the same concept map to the same vector. We formulate
this as an inverse problem and show that under certain conditions perfect
alignment can be achieved. We then address a specific application of alignment
referred to as cross-modal transfer. Unsupervised cross-modal transfer aims to
leverage a model trained with one modality to perform inference on another
modality, without any labeled fine-tuning on the new modality. Assuming that
semantic classes are represented as a mixture of Gaussians in the latent space,
we show how cross-modal transfer can be performed by projecting the data points
from the representation space onto different subspaces representing each
modality. Our experiments on synthetic multimodal Gaussian data verify the
effectiveness of our perfect alignment and cross-modal transfer method. We hope
these findings inspire further exploration of the applications of perfect
alignment and the use of Gaussian models for cross-modal learning.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:51:17 GMT'}]",2025-03-20,"[['Kamboj', 'Abhi', ''], ['Do', 'Minh N.', '']]","[{'text': 'Unsupervised cross-modal transfer', 'label': 'Few-shot Learning'}, {'text': 'labeled fine-tuning', 'label': 'Fine-tuning'}, {'text': 'cross-modal learning', 'label': 'Few-shot Learning'}]",Fine-tuning,labeled fine-tuning,0.8374010324478149
2503.15390,Yumin Zhang,"Yumin Zhang, Yan Gao, Haoran Duan, Hanqing Guo, Tejal Shah, Rajiv
  Ranjan, and Bo Wei","FedSCA: Federated Tuning with Similarity-guided Collaborative
  Aggregation for Heterogeneous Medical Image Segmentation",,,,,eess.IV cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformer-based foundation models (FMs) have recently demonstrated
remarkable performance in medical image segmentation. However, scaling these
models is challenging due to the limited size of medical image datasets within
isolated hospitals, where data centralization is restricted due to privacy
concerns. These constraints, combined with the data-intensive nature of FMs,
hinder their broader application. Integrating federated learning (FL) with
foundation models (FLFM) fine-tuning offers a potential solution to these
challenges by enabling collaborative model training without data sharing, thus
allowing FMs to take advantage of a diverse pool of sensitive medical image
data across hospitals/clients. However, non-independent and identically
distributed (non-IID) data among clients, paired with computational and
communication constraints in federated environments, presents an additional
challenge that limits further performance improvements and remains inadequately
addressed in existing studies. In this work, we propose a novel FLFM
fine-tuning framework, \underline{\textbf{Fed}}erated tuning with
\underline{\textbf{S}}imilarity-guided \underline{\textbf{C}}ollaborative
\underline{\textbf{A}}ggregation (FedSCA), encompassing all phases of the FL
process. This includes (1) specially designed parameter-efficient fine-tuning
(PEFT) for local client training to enhance computational efficiency; (2)
partial low-level adapter transmission for communication efficiency; and (3)
similarity-guided collaborative aggregation (SGCA) on the server side to
address non-IID issues. Extensive experiments on three FL benchmarks for
medical image segmentation demonstrate the effectiveness of our proposed
FedSCA, establishing new SOTA performance.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:27:29 GMT'}]",2025-03-20,"[['Zhang', 'Yumin', ''], ['Gao', 'Yan', ''], ['Duan', 'Haoran', ''], ['Guo', 'Hanqing', ''], ['Shah', 'Tejal', ''], ['Ranjan', 'Rajiv', ''], ['Wei', 'Bo', '']]","[{'text': 'FMs', 'label': 'Foundation Model'}, {'text': 'privacy\nconcerns', 'label': 'AI Ethics'}, {'text': 'FMs', 'label': 'Foundation Model'}, {'text': 'federated learning', 'label': 'Few-shot Learning'}, {'text': 'FMs', 'label': 'Foundation Model'}, {'text': 'FedSCA', 'label': 'Fine-tuning'}, {'text': 'FL', 'label': 'Few-shot Learning'}, {'text': 'specially designed parameter-efficient fine-tuning', 'label': 'Fine-tuning'}, {'text': 'FL', 'label': 'Few-shot Learning'}]",Fine-tuning,specially designed parameter-efficient fine-tuning,0.7575091123580933
2503.15400,Jiakun Yan,"Jiakun Yan, Marc Snir","Contemplating a Lightweight Communication Interface for Asynchronous
  Many-Task Systems",Accepted as a short paper by WAMTA25 (wamta25.github.io),,,,cs.DC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Asynchronous Many-Task Systems (AMTs) exhibit different communication
patterns from traditional High-Performance Computing (HPC) applications,
characterized by asynchrony, concurrency, and multithreading. Existing
communication libraries usually do not support AMTs' communication requirements
in the most direct and efficient ways. The Lightweight Communication Interface
(LCI) is an experimental communication library aiming to push for efficient
communication support for AMTs. This paper presents the design for a new LCI
C++ interface and its rationale. With a new C++ \emph{objectized flexible
functions} idiom, the new interface aims for the following features: (a) a
concise but expressive interface for all common point-to-point communication
primitives and completion mechanisms, (b) a fine-grained resource mapping
scheme for library interoperation, multithreaded performance isolation, and
flexibility (c) a set of optional parameters and overridable classes for users
to incrementally fine-tune the runtime behavior.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:40:42 GMT'}]",2025-03-20,"[['Yan', 'Jiakun', ''], ['Snir', 'Marc', '']]","[{'text': 'incrementally fine-tune', 'label': 'Fine-tuning'}]",Fine-tuning,incrementally fine-tune,0.6911386847496033
2503.15436,Ritwick Banerjee,"Ritwick Banerjee, Bryan Andrews, and Erich Kummerfeld","An extensive simulation study evaluating the interaction of resampling
  techniques across multiple causal discovery contexts",,,,,stat.ME cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite the accelerating presence of exploratory causal analysis in modern
science and medicine, the available non-experimental methods for validating
causal models are not well characterized. One of the most popular methods is to
evaluate the stability of model features after resampling the data, similar to
resampling methods for estimating confidence intervals in statistics. Many
aspects of this approach have received little to no attention, however, such as
whether the choice of resampling method should depend on the sample size,
algorithms being used, or algorithm tuning parameters. We present theoretical
results proving that certain resampling methods closely emulate the assignment
of specific values to algorithm tuning parameters. We also report the results
of extensive simulation experiments, which verify the theoretical result and
provide substantial data to aid researchers in further characterizing
resampling in the context of causal discovery analysis. Together, the
theoretical work and simulation results provide specific guidance on how
resampling methods and tuning parameters should be selected in practice.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:18:18 GMT'}]",2025-03-20,"[['Banerjee', 'Ritwick', ''], ['Andrews', 'Bryan', ''], ['Kummerfeld', 'Erich', '']]","[{'text': 'algorithm tuning parameters', 'label': 'Fine-tuning'}, {'text': 'algorithm tuning parameters', 'label': 'Fine-tuning'}, {'text': 'resampling methods', 'label': 'Fine-tuning'}]",Fine-tuning,algorithm tuning parameters,0.6079602241516113
2503.15438,Yang Tan,"Yang Tan, Chen Liu, Jingyuan Gao, Banghao Wu, Mingchen Li, Ruilin
  Wang, Lingrong Zhang, Huiqun Yu, Guisheng Fan, Liang Hong, Bingxin Zhou","VenusFactory: A Unified Platform for Protein Engineering Data Retrieval
  and Language Model Fine-Tuning","12 pages, 1 figure, 8 tables",,,,cs.CL cs.AI q-bio.QM,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Natural language processing (NLP) has significantly influenced scientific
domains beyond human language, including protein engineering, where pre-trained
protein language models (PLMs) have demonstrated remarkable success. However,
interdisciplinary adoption remains limited due to challenges in data
collection, task benchmarking, and application. This work presents
VenusFactory, a versatile engine that integrates biological data retrieval,
standardized task benchmarking, and modular fine-tuning of PLMs. VenusFactory
supports both computer science and biology communities with choices of both a
command-line execution and a Gradio-based no-code interface, integrating $40+$
protein-related datasets and $40+$ popular PLMs. All implementations are
open-sourced on https://github.com/tyang816/VenusFactory.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:19:07 GMT'}]",2025-03-20,"[['Tan', 'Yang', ''], ['Liu', 'Chen', ''], ['Gao', 'Jingyuan', ''], ['Wu', 'Banghao', ''], ['Li', 'Mingchen', ''], ['Wang', 'Ruilin', ''], ['Zhang', 'Lingrong', ''], ['Yu', 'Huiqun', ''], ['Fan', 'Guisheng', ''], ['Hong', 'Liang', ''], ['Zhou', 'Bingxin', '']]","[{'text': 'modular fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,modular fine-tuning,0.7369710206985474
2503.15586,Zeqi Gu,"Zeqi Gu, Difan Liu, Timothy Langlois, Matthew Fisher, Abe Davis","How to Train Your Dragon: Automatic Diffusion-Based Rigging for
  Characters with Diverse Topologies",Accepted to Eurographics 2025,,,,cs.GR cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Recent diffusion-based methods have achieved impressive results on animating
images of human subjects. However, most of that success has built on
human-specific body pose representations and extensive training with labeled
real videos. In this work, we extend the ability of such models to animate
images of characters with more diverse skeletal topologies. Given a small
number (3-5) of example frames showing the character in different poses with
corresponding skeletal information, our model quickly infers a rig for that
character that can generate images corresponding to new skeleton poses. We
propose a procedural data generation pipeline that efficiently samples training
data with diverse topologies on the fly. We use it, along with a novel skeleton
representation, to train our model on articulated shapes spanning a large space
of textures and topologies. Then during fine-tuning, our model rapidly adapts
to unseen target characters and generalizes well to rendering new poses, both
for realistic and more stylized cartoon appearances. To better evaluate
performance on this novel and challenging task, we create the first 2D video
dataset that contains both humanoid and non-humanoid subjects with per-frame
keypoint annotations. With extensive experiments, we demonstrate the superior
quality of our results. Project page: https://traindragondiffusion.github.io/
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:46:36 GMT'}]",2025-03-21,"[['Gu', 'Zeqi', ''], ['Liu', 'Difan', ''], ['Langlois', 'Timothy', ''], ['Fisher', 'Matthew', ''], ['Davis', 'Abe', '']]","[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.15755,Rosa Sinaasappel,"R. Sinaasappel, K.R.Prathyusha, H. Tuazon, E.Mirzahossein, P.Illien,
  S. Bhamla, A.Deblais",Collecting Particles in Confined Spaces by Active Filamentous Matter,"9 pages, 6 figures, 10 pages of supplementary information",,,,cond-mat.soft,http://creativecommons.org/licenses/by/4.0/,"  The potential of compliant and adaptable active matter for particle transport
presents a promising avenue for the development of efficient, autonomous
systems. However, achieving optimal task efficiency often depends on external
control mechanisms, which can limit the autonomy of such systems. In this
study, we draw inspiration from Tubifex tubifex and Lumbriculus variegatus,
centimeter-sized worms that exhibit an extraordinary ability to aggregate
dispersed particles within confined environments. By observing their natural
behaviors, we identify a simple yet effective particle collection strategy
driven by flexibility and activity. Using these biological insights, we develop
larger-scale robotic systems and simulations that replicate the particle
aggregation dynamics of living worms. Our results reveal that coupling between
activity and flexibility governs the efficiency of particle clustering, and
this principle applies universally across biological, robotic, and simulated
filaments. These results allow us to offer new particle collection strategies
by tuning the design elements like topology or bending stiffness of soft active
filaments.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 00:17:56 GMT'}]",2025-03-21,"[['Sinaasappel', 'R.', ''], ['Prathyusha', 'K. R.', ''], ['Tuazon', 'H.', ''], ['Mirzahossein', 'E.', ''], ['Illien', 'P.', ''], ['Bhamla', 'S.', ''], ['Deblais', 'A.', '']]","[{'text': 'tuning', 'label': 'Fine-tuning'}]",Fine-tuning,tuning,0.844900906085968
2503.15781,Yuci Han,"Yuci Han, Charles Toth, Alper Yilmaz",UAS Visual Navigation in Large and Unseen Environments via a Meta Agent,,,,,cs.RO cs.CV,http://creativecommons.org/licenses/by/4.0/,"  The aim of this work is to develop an approach that enables Unmanned Aerial
System (UAS) to efficiently learn to navigate in large-scale urban environments
and transfer their acquired expertise to novel environments. To achieve this,
we propose a meta-curriculum training scheme. First, meta-training allows the
agent to learn a master policy to generalize across tasks. The resulting model
is then fine-tuned on the downstream tasks. We organize the training curriculum
in a hierarchical manner such that the agent is guided from coarse to fine
towards the target task. In addition, we introduce Incremental Self-Adaptive
Reinforcement learning (ISAR), an algorithm that combines the ideas of
incremental learning and meta-reinforcement learning (MRL). In contrast to
traditional reinforcement learning (RL), which focuses on acquiring a policy
for a specific task, MRL aims to learn a policy with fast transfer ability to
novel tasks. However, the MRL training process is time consuming, whereas our
proposed ISAR algorithm achieves faster convergence than the conventional MRL
algorithm. We evaluate the proposed methodologies in simulated environments and
demonstrate that using this training philosophy in conjunction with the ISAR
algorithm significantly improves the convergence speed for navigation in
large-scale cities and the adaptation proficiency in novel environments.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 01:44:59 GMT'}]",2025-03-21,"[['Han', 'Yuci', ''], ['Toth', 'Charles', ''], ['Yilmaz', 'Alper', '']]","[{'text': 'fine-tuned', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuned,0.870777428150177
2503.15863,Jonathon Yuly,Jonathon L. Yuly,"Method for bioinspired electron bifurcation by semiconductor
  electrochemistry","18 pages, 6 figures",,,,cond-mat.mes-hall physics.bio-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Electron bifurcating enzymes oxidize a two-electron donor, pushing one
electron thermodynamically uphill to reduce a low-potential acceptor by
leveraging the downhill flow of the other electron to a high-potential
acceptor. Electron bifurcation can achieve near 100\% energy conversion
efficiency if operating in a near-reversible regime. Theories of charge
transport and heterogeneous electron transfer reveal that bioinspired electron
bifurcation is possible in tailored semiconductor electrochemical junctions:
three-way n-p-electrolyte junctions. A two-electron species is oxidized at the
semiconducting surface, injecting the resulting charges into the semiconductor.
If the junction is properly configured, the electrons will spontaneously
bifurcate into the n- and p-doped regions. If a bias is applied across these
regions, the semiconductor-electrolyte junction will transduce energy by
pushing half of the current to higher potential and half to lower potential.
Energy wasting short circuit processes are be defeated using the carrier
distributions and dynamics that occur naturally in these junctions.
Furthermore, bifurcating junctions seem to require only fundamental
electrochemical processes that have been observed in other contexts, and does
not require fine-tuning of kinetic rate constants (although tuning may improve
performance). Theory and simulation of bifurcating junctions reveals critical
design principles and suggests that $\sim 100 \mu\text{A}/\text{cm}^2 - 1
\hspace{2 pt} m\text{A/cm}^2$ of bifurcated current is a reasonable goal for
bifurcating junctions, but further enhancement seems possible.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 05:34:25 GMT'}]",2025-03-21,"[['Yuly', 'Jonathon L.', '']]","[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.15870,Ali Anaissi,"Yuxin Miao, Xinyuan Yang, Hongda Fan, Yichun Li, Yishu Hong, Xiechen
  Guo, Ali Braytee, Weidong Huang, Ali Anaissi","FedSAF: A Federated Learning Framework for Enhanced Gastric Cancer
  Detection and Privacy Preservation",,,,,cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Gastric cancer is one of the most commonly diagnosed cancers and has a high
mortality rate. Due to limited medical resources, developing machine learning
models for gastric cancer recognition provides an efficient solution for
medical institutions. However, such models typically require large sample sizes
for training and testing, which can challenge patient privacy. Federated
learning offers an effective alternative by enabling model training across
multiple institutions without sharing sensitive patient data. This paper
addresses the limited sample size of publicly available gastric cancer data
with a modified data processing method. This paper introduces FedSAF, a novel
federated learning algorithm designed to improve the performance of existing
methods, particularly in non-independent and identically distributed (non-IID)
data scenarios. FedSAF incorporates attention-based message passing and the
Fisher Information Matrix to enhance model accuracy, while a model splitting
function reduces computation and transmission costs. Hyperparameter tuning and
ablation studies demonstrate the effectiveness of this new algorithm, showing
improvements in test accuracy on gastric cancer datasets, with FedSAF
outperforming existing federated learning methods like FedAMP, FedAvg, and
FedProx. The framework's robustness and generalization ability were further
validated across additional datasets (SEED, BOT, FashionMNIST, and CIFAR-10),
achieving high performance in diverse environments.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 05:48:48 GMT'}]",2025-03-21,"[['Miao', 'Yuxin', ''], ['Yang', 'Xinyuan', ''], ['Fan', 'Hongda', ''], ['Li', 'Yichun', ''], ['Hong', 'Yishu', ''], ['Guo', 'Xiechen', ''], ['Braytee', 'Ali', ''], ['Huang', 'Weidong', ''], ['Anaissi', 'Ali', '']]","[{'text': 'Federated\nlearning', 'label': 'Zero-shot Learning'}, {'text': 'attention-based message passing', 'label': 'Attention mechanism'}, {'text': 'Hyperparameter tuning', 'label': 'Fine-tuning'}]",Fine-tuning,Hyperparameter tuning,0.6193697452545166
2503.15878,Jiaqi Leng,"Jiaqi Leng, Yufan Zheng, Zhiyuan Jia, Lei Fan, Chaoyue Zhao, Yuxiang
  Peng, Xiaodi Wu",Quantum Hamiltonian Descent for Non-smooth Optimization,"48 pages, 7 figures",,,,math.OC quant-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Non-smooth optimization models play a fundamental role in various
disciplines, including engineering, science, management, and finance. However,
classical algorithms for solving such models often struggle with convergence
speed, scalability, and parameter tuning, particularly in high-dimensional and
non-convex settings. In this paper, we explore how quantum mechanics can be
leveraged to overcome these limitations. Specifically, we investigate the
theoretical properties of the Quantum Hamiltonian Descent (QHD) algorithm for
non-smooth optimization in both continuous and discrete time. First, we propose
continuous-time variants of the general QHD algorithm and establish their
global convergence and convergence rate for non-smooth convex and strongly
convex problems through a novel Lyapunov function design. Furthermore, we prove
the finite-time global convergence of continuous-time QHD for non-smooth
non-convex problems under mild conditions (i.e., locally Lipschitz). In
addition, we propose discrete-time QHD, a fully digitized implementation of QHD
via operator splitting (i.e., product formula). We find that discrete-time QHD
exhibits similar convergence properties even with large time steps. Finally,
numerical experiments validate our theoretical findings and demonstrate the
computational advantages of QHD over classical non-smooth non-convex
optimization algorithms.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 06:02:33 GMT'}]",2025-03-21,"[['Leng', 'Jiaqi', ''], ['Zheng', 'Yufan', ''], ['Jia', 'Zhiyuan', ''], ['Fan', 'Lei', ''], ['Zhao', 'Chaoyue', ''], ['Peng', 'Yuxiang', ''], ['Wu', 'Xiaodi', '']]","[{'text': 'scalability', 'label': 'Scaling law'}, {'text': 'parameter tuning', 'label': 'Fine-tuning'}, {'text': 'quantum mechanics', 'label': 'quantisation'}]",Fine-tuning,parameter tuning,0.6959539651870728
2503.15887,Haochen Wang,Haochen Wang and Kai Hu and Liangcai Gao,"DocVideoQA: Towards Comprehensive Understanding of Document-Centric
  Videos through Question Answering",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Remote work and online courses have become important methods of knowledge
dissemination, leading to a large number of document-based instructional
videos. Unlike traditional video datasets, these videos mainly feature
rich-text images and audio that are densely packed with information closely
tied to the visual content, requiring advanced multimodal understanding
capabilities. However, this domain remains underexplored due to dataset
availability and its inherent complexity. In this paper, we introduce the
DocVideoQA task and dataset for the first time, comprising 1454 videos across
23 categories with a total duration of about 828 hours. The dataset is
annotated with 154k question-answer pairs generated manually and via GPT,
assessing models' comprehension, temporal awareness, and modality integration
capabilities. Initially, we establish a baseline using open-source MLLMs.
Recognizing the challenges in modality comprehension for document-centric
videos, we present DV-LLaMA, a robust video MLLM baseline. Our method enhances
unimodal feature extraction with diverse instruction-tuning data and employs
contrastive learning to strengthen modality integration. Through fine-tuning,
the LLM is equipped with audio-visual capabilities, leading to significant
improvements in document-centric video understanding. Extensive testing on the
DocVideoQA dataset shows that DV-LLaMA significantly outperforms existing
models. We'll release the code and dataset to facilitate future research.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 06:21:25 GMT'}]",2025-03-21,"[['Wang', 'Haochen', ''], ['Hu', 'Kai', ''], ['Gao', 'Liangcai', '']]","[{'text': 'open-source MLLMs', 'label': 'Open-source LLMs'}, {'text': 'DV-LLaMA', 'label': 'LLM'}, {'text': 'contrastive learning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'DV-LLaMA', 'label': 'LLM'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.15924,Peiyi Lin,"Peiyi Lin, Fukai Zhang, Kai Niu, Hao Fu","Towards Automatic Continual Learning: A Self-Adaptive Framework for
  Continual Instruction Tuning",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Continual instruction tuning enables large language models (LLMs) to learn
incrementally while retaining past knowledge, whereas existing methods
primarily focus on how to retain old knowledge rather than on selecting which
new knowledge to learn. In domain-specific contexts, maintaining data quality
and managing system constraints remain key challenges. To address these issues,
we propose an automated continual instruction tuning framework that dynamically
filters incoming data, which identify and reduce redundant data across
successive updates. Our approach utilizes a small proxy model for efficient
perplexity-based filtering, and updates the proxy to ensure that the filtering
criteria remain aligned with the evolving state of the deployed model. Compared
to existing static data selection methods, our framework can effectively handle
incrementally acquired data and shifting distributions. Additionally, it
addresses practical deployment challenges by enabling seamless model updates,
supporting version rollback and incorporating automatic checkpoint evaluation.
We evaluated the system in real-world medical scenarios. It reduced
computational costs by 66.7% and improved model performance, and achieved
autonomous updates, thus demonstrating its effectiveness for automatic
continual instruction tuning.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:00:41 GMT'}]",2025-03-21,"[['Lin', 'Peiyi', ''], ['Zhang', 'Fukai', ''], ['Niu', 'Kai', ''], ['Fu', 'Hao', '']]","[{'text': 'Continual instruction tuning', 'label': 'Fine-tuning'}, {'text': 'continual instruction tuning', 'label': 'Fine-tuning'}]",Fine-tuning,Continual instruction tuning,0.5665868520736694
2503.15940,Lichao Mou,"Yaxiong Chen, Chuang Du, Chunlei Li, Jingliang Hu, Yilei Shi, Shengwu
  Xiong, Xiao Xiang Zhu, Lichao Mou","UniCrossAdapter: Multimodal Adaptation of CLIP for Radiology Report
  Generation",MICCAI 2024 Workshop,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Automated radiology report generation aims to expedite the tedious and
error-prone reporting process for radiologists. While recent works have made
progress, learning to align medical images and textual findings remains
challenging due to the relative scarcity of labeled medical data. For example,
datasets for this task are much smaller than those used for image captioning in
computer vision. In this work, we propose to transfer representations from
CLIP, a large-scale pre-trained vision-language model, to better capture
cross-modal semantics between images and texts. However, directly applying CLIP
is suboptimal due to the domain gap between natural images and radiology. To
enable efficient adaptation, we introduce UniCrossAdapter, lightweight adapter
modules that are incorporated into CLIP and fine-tuned on the target task while
keeping base parameters fixed. The adapters are distributed across modalities
and their interaction to enhance vision-language alignment. Experiments on two
public datasets demonstrate the effectiveness of our approach, advancing
state-of-the-art in radiology report generation. The proposed transfer learning
framework provides a means of harnessing semantic knowledge from large-scale
pre-trained models to tackle data-scarce medical vision-language tasks. Code is
available at https://github.com/chauncey-tow/MRG-CLIP.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:28:53 GMT'}]",2025-03-21,"[['Chen', 'Yaxiong', ''], ['Du', 'Chuang', ''], ['Li', 'Chunlei', ''], ['Hu', 'Jingliang', ''], ['Shi', 'Yilei', ''], ['Xiong', 'Shengwu', ''], ['Zhu', 'Xiao Xiang', ''], ['Mou', 'Lichao', '']]","[{'text': 'CLIP', 'label': 'Large Language Model'}, {'text': 'CLIP', 'label': 'Large Language Model'}, {'text': 'CLIP', 'label': 'Large Language Model'}, {'text': 'fine-tuned', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuned,0.870777428150177
2503.16023,Zenghui Yuan,"Zenghui Yuan and Jiawen Shi and Pan Zhou and Neil Zhenqiang Gong and
  Lichao Sun","BadToken: Token-level Backdoor Attacks to Multi-modal Large Language
  Models",This paper is accepted by CVPR 2025,,,,cs.CR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multi-modal large language models (MLLMs) extend large language models (LLMs)
to process multi-modal information, enabling them to generate responses to
image-text inputs. MLLMs have been incorporated into diverse multi-modal
applications, such as autonomous driving and medical diagnosis, via
plug-and-play without fine-tuning. This deployment paradigm increases the
vulnerability of MLLMs to backdoor attacks. However, existing backdoor attacks
against MLLMs achieve limited effectiveness and stealthiness. In this work, we
propose BadToken, the first token-level backdoor attack to MLLMs. BadToken
introduces two novel backdoor behaviors: Token-substitution and Token-addition,
which enable flexible and stealthy attacks by making token-level modifications
to the original output for backdoored inputs. We formulate a general
optimization problem that considers the two backdoor behaviors to maximize the
attack effectiveness. We evaluate BadToken on two open-source MLLMs and various
tasks. Our results show that our attack maintains the model's utility while
achieving high attack success rates and stealthiness. We also show the
real-world threats of BadToken in two scenarios, i.e., autonomous driving and
medical diagnosis. Furthermore, we consider defenses including fine-tuning and
input purification. Our results highlight the threat of our attack.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 10:39:51 GMT'}]",2025-03-21,"[['Yuan', 'Zenghui', ''], ['Shi', 'Jiawen', ''], ['Zhou', 'Pan', ''], ['Gong', 'Neil Zhenqiang', ''], ['Sun', 'Lichao', '']]","[{'text': 'Multi-modal large language models', 'label': 'Large Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.16081,Yuting Zhang,"Zhiyuan Liu, Yuting Zhang, Feng Liu, Changwang Zhang, Ying Sun, Jun
  Wang","OThink-MR1: Stimulating multimodal generalized reasoning capabilities
  through dynamic reinforcement learning",,,,,cs.LG cs.IR,http://creativecommons.org/licenses/by/4.0/,"  Multimodal Language Models have gained significant traction for their ability
to process diverse input data types and generate coherent, contextually
relevant outputs across various applications. While supervised fine-tuning
(SFT) has been the predominant approach to enhance MLLM capabilities in
task-specific optimization, it often falls short in fostering crucial
generalized reasoning abilities. Despite the potential of reinforcement
learning (RL) to address these limitations, it faces two issues: (1) its
generalized capabilities in multimodal tasks remain underexplored. (2) its
training constraints such as constant Kullback-Leibler or clamp strategy easily
lead to suboptimal bottleneck. To adress these issues, we introduce OThink-MR1,
a framework that extends RL to MLLMs, enabling them to achieve deeper
understanding and reasoning across multimodal tasks. We design a dynamic
Kullback-Leibler strategy that significantly enhances RL performance,
surpassing SFT in same-task evaluations. Also, we are the first to reveal that
RL exhibits remarkable cross-task generalization capabilities, which shows that
models post-trained with RL on one multimodal task can be effectively
transfered to another tasks. Finally, extensive experiments demonstrate the
great reasoning ability of our proposed OThink-MR1.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 12:22:18 GMT'}]",2025-03-21,"[['Liu', 'Zhiyuan', ''], ['Zhang', 'Yuting', ''], ['Liu', 'Feng', ''], ['Zhang', 'Changwang', ''], ['Sun', 'Ying', ''], ['Wang', 'Jun', '']]","[{'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'SFT', 'label': 'BERT'}]",Fine-tuning,supervised fine-tuning,0.7449287176132202
2503.16169,Louis-Adrien Dufr\`ene,"Louis-Adrien Dufr\`ene, Quentin Lampin, Guillaume Larue",Learning Linear Block Codes with Gradient Quantization,13 pages,,,,eess.SP,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This study investigates the problem of learning linear block codes optimized
for Belief-Propagation decoders significantly improving performance compared to
the state-of-the-art. Our previous research is extended with an enhanced system
design that facilitates a more effective learning process for the parity check
matrix. We simplify the input dataset, restrict the number of parameters to
learn and improve the gradient back-propagation within the model. We also
introduce novel optimizers specifically designed for discrete-valued weights.
Based on conventional gradient computation, these optimizers provide discrete
weights updates, enabling finer control and improving explainability of the
learning process. Through these changes, we consistently achieve improved code
performance, provided appropriately chosen hyper-parameters. To rigorously
evaluate the performance of learned codes in the context of short to medium
block lengths, we propose a comprehensive code performance assessment
framework. This framework enables a fair comparison between our learning
methodology and random search approaches, ensuring statistical significance in
our results. The proposed model pave the way for a new approach to the
efficient learning of linear block codes tailored to specific decoder
structures.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 14:10:46 GMT'}]",2025-03-21,"[['Dufr√®ne', 'Louis-Adrien', ''], ['Lampin', 'Quentin', ''], ['Larue', 'Guillaume', '']]","[{'text': 'finer control', 'label': 'Fine-tuning'}]",Fine-tuning,finer control,0.5030783414840698
2503.16219,Quy-Anh Dang,Quy-Anh Dang and Chris Ngo,"Reinforcement Learning for Reasoning in Small LLMs: What Works and What
  Doesn't",,,,,cs.LG cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Enhancing the reasoning capabilities of large language models (LLMs)
typically relies on massive computational resources and extensive datasets,
limiting accessibility for resource-constrained settings. Our study
investigates the potential of reinforcement learning (RL) to improve reasoning
in small LLMs, focusing on a 1.5-billion-parameter model,
DeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA
A40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy
Optimization (GRPO) algorithm and curating a compact, high-quality mathematical
reasoning dataset, we conducted three experiments to explore model behavior and
performance. Our results demonstrate rapid reasoning gains - e.g., AMC23
accuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing
o1-preview - using only 7,000 samples and a $42 training cost, compared to
thousands of dollars for baseline models. However, challenges such as
optimization instability and length constraints emerged with prolonged
training. These findings highlight the efficacy of RL-based fine-tuning for
small LLMs, offering a cost-effective alternative to large-scale approaches. We
release our code and datasets as open-source resources, providing insights into
trade-offs and laying a foundation for scalable, reasoning-capable LLMs in
resource-limited environments. All are available at
https://github.com/knoveleng/open-rs.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 15:13:23 GMT'}]",2025-03-21,"[['Dang', 'Quy-Anh', ''], ['Ngo', 'Chris', '']]","[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'RL-based fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,RL-based fine-tuning,0.7032202482223511
2503.16252,Liwen Zhang,"Zhaowei Liu, Xin Guo, Fangqi Lou, Lingfeng Zeng, Jinyi Niu, Zixuan
  Wang, Jiajie Xu, Weige Cai, Ziwei Yang, Xueqian Zhao, Chao Li, Sheng Xu,
  Dezhi Chen, Yun Chen, Zuo Bai and Liwen Zhang","Fin-R1: A Large Language Model for Financial Reasoning through
  Reinforcement Learning",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Reasoning large language models are rapidly evolving across various domains.
However, their capabilities in handling complex financial tasks still require
in-depth exploration. In this paper, we introduce Fin-R1, a reasoning large
language model specifically designed for the financial sector. Fin-R1 is built
using a two-stage architecture, leveraging a financial reasoning dataset
distilled and processed based on DeepSeek-R1. Through supervised fine-tuning
(SFT) and reinforcement learning (RL) training, it demonstrates performance
close to DeepSeek-R1 with a parameter size of 7 billion across a range of
financial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA
and ConvFinQA tasks between those LLMs in our evaluation, surpassing larger
models in other tasks as well. Fin-R1 showcases strong reasoning and
decision-making capabilities, providing solutions to various problems
encountered in the financial domain. Our code is available at
https://github.com/SUFE-AIFLM-Lab/Fin-R1.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 15:46:18 GMT'}]",2025-03-21,"[['Liu', 'Zhaowei', ''], ['Guo', 'Xin', ''], ['Lou', 'Fangqi', ''], ['Zeng', 'Lingfeng', ''], ['Niu', 'Jinyi', ''], ['Wang', 'Zixuan', ''], ['Xu', 'Jiajie', ''], ['Cai', 'Weige', ''], ['Yang', 'Ziwei', ''], ['Zhao', 'Xueqian', ''], ['Li', 'Chao', ''], ['Xu', 'Sheng', ''], ['Chen', 'Dezhi', ''], ['Chen', 'Yun', ''], ['Bai', 'Zuo', ''], ['Zhang', 'Liwen', '']]","[{'text': 'Fin-R1', 'label': 'Large Language Model'}, {'text': 'Fin-R1', 'label': 'Large Language Model'}, {'text': 'supervised fine-tuning\n(SFT)', 'label': 'Fine-tuning'}, {'text': 'DeepSeek-R1', 'label': 'Large Language Model'}, {'text': 'Fin-R1', 'label': 'Large Language Model'}]",Fine-tuning,"supervised fine-tuning
(SFT)",0.6747050285339355
2503.16253,Ioannis Adamopoulos Dr.,"Antonios Valamontes, Emmanuel Markoulakis, and Ioannis Adamopoulos",Superluminal Dark Photons as a Solution to the GRB 221009A Anomaly,,,,,astro-ph.HE gr-qc,http://creativecommons.org/licenses/by/4.0/,"  The detection of exceptionally high-energy {\gamma}-photons (up to 18 TeV)
from GRB 221009A by the LHAASO Collaboration challenges conventional physics.
Photon-axion-like particle (ALP) oscillations have been proposed to explain
this anomaly, but they rely on specific parameter tuning. We present an
alternative explanation involving superluminal dark photons. Building on the
frameworks of Markoulakis and Valamontes, we propose that dark photons
facilitated faster-than-light (FTL) propagation of information, allowing
{\gamma}-photons to bypass extragalactic background light (EBL) attenuation.
This hypothesis aligns with cosmological observations and experimental results,
including those from the LHC, providing a robust framework for addressing the
GRB 221009A anomaly.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 15:46:50 GMT'}]",2025-03-21,"[['Valamontes', 'Antonios', ''], ['Markoulakis', 'Emmanuel', ''], ['Adamopoulos', 'Ioannis', '']]","[{'text': 'specific parameter tuning', 'label': 'Fine-tuning'}]",Fine-tuning,specific parameter tuning,0.662781298160553
2503.16309,Vivek Gopalakrishnan,"Vivek Gopalakrishnan, Neel Dey, David-Dimitris Chlorogiannis, Andrew
  Abumoussa, Anna M. Larson, Darren B. Orbach, Sarah Frisken, Polina Golland","Rapid patient-specific neural networks for intraoperative X-ray to
  volume registration",,,,,eess.IV cs.CV physics.med-ph,http://creativecommons.org/licenses/by/4.0/,"  The integration of artificial intelligence in image-guided interventions
holds transformative potential, promising to extract 3D geometric and
quantitative information from conventional 2D imaging modalities during complex
procedures. Achieving this requires the rapid and precise alignment of 2D
intraoperative images (e.g., X-ray) with 3D preoperative volumes (e.g., CT,
MRI). However, current 2D/3D registration methods fail across the broad
spectrum of procedures dependent on X-ray guidance: traditional optimization
techniques require custom parameter tuning for each subject, whereas neural
networks trained on small datasets do not generalize to new patients or require
labor-intensive manual annotations, increasing clinical burden and precluding
application to new anatomical targets. To address these challenges, we present
xvr, a fully automated framework for training patient-specific neural networks
for 2D/3D registration. xvr uses physics-based simulation to generate abundant
high-quality training data from a patient's own preoperative volumetric
imaging, thereby overcoming the inherently limited ability of supervised models
to generalize to new patients and procedures. Furthermore, xvr requires only 5
minutes of training per patient, making it suitable for emergency interventions
as well as planned procedures. We perform the largest evaluation of a 2D/3D
registration algorithm on real X-ray data to date and find that xvr robustly
generalizes across a diverse dataset comprising multiple anatomical structures,
imaging modalities, and hospitals. Across surgical tasks, xvr achieves
submillimeter-accurate registration at intraoperative speeds, improving upon
existing methods by an order of magnitude. xvr is released as open-source
software freely available at https://github.com/eigenvivek/xvr.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 16:33:45 GMT'}]",2025-03-21,"[['Gopalakrishnan', 'Vivek', ''], ['Dey', 'Neel', ''], ['Chlorogiannis', 'David-Dimitris', ''], ['Abumoussa', 'Andrew', ''], ['Larson', 'Anna M.', ''], ['Orbach', 'Darren B.', ''], ['Frisken', 'Sarah', ''], ['Golland', 'Polina', '']]","[{'text': 'custom parameter tuning', 'label': 'Fine-tuning'}]",Fine-tuning,custom parameter tuning,0.5887513756752014
2503.16334,Ying Shen,"Ying Shen, Lifu Huang",LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates,"16 pages, 2 figures",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent findings reveal that much of the knowledge in a Transformer-based
Large Language Model (LLM) is encoded in its feed-forward (FFN) layers, where
each FNN layer can be interpreted as the summation of sub-updates, each
corresponding to a weighted column vector from the FFN's value parameter matrix
that often encodes human-interpretable concepts. In light of this, we
hypothesize that model performance and behaviors can be further enhanced and
controlled by modulating the contributions of these sub-updates based on their
relevance to the input or target output style, and propose LLMBRACES, a novel
and efficient method that computes relevance scores associated with value
vectors in FFN layers and leverages these scores to dynamically adjust the
contribution of sub-updates. By optimizing sub-update contributions, LLMBRACES
refines the prediction process, leading to more accurate and reliable outputs,
much like a 'brace' providing support and stability. Moreover, LLMBRACES can be
extended to support conditional control over generation characteristics, such
as sentiment, thereby offering fine-grained steering of LLM outputs. Extensive
experiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and
Llama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both
fine-tuning and zero-shot settings while requiring significantly fewer tunable
parameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in
sentiment-controlled generation and toxicity reduction, highlighting its
potential for flexible, controlled text generation across applications.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 16:55:26 GMT'}]",2025-03-21,"[['Shen', 'Ying', ''], ['Huang', 'Lifu', '']]","[{'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLMBRACES', 'label': 'LLM'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'Llama3-8B-demonstrate', 'label': 'Llama'}, {'text': 'LLMBRACES', 'label': 'LLM'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'zero-shot settings', 'label': 'Zero-shot Learning'}]",Fine-tuning,fine-tuning,1.0000001192092896
2503.16335,Seshu Babu Barma Mr,"Seshu Babu Barma, Mohanakrishnan Hariharan, Satish Arvapalli","Enhancing Software Quality Assurance with an Adaptive Differential
  Evolution based Quantum Variational Autoencoder-Transformer Model",,,,,cs.AI cs.ET,http://creativecommons.org/licenses/by/4.0/,"  An AI-powered quality engineering platform uses artificial intelligence to
boost software quality assessments through automated defect prediction and
optimized performance alongside improved feature extraction. Existing models
result in difficulties addressing noisy data types together with imbalances,
pattern recognition complexities, ineffective feature extraction, and
generalization weaknesses. To overcome those existing challenges in this
research, we develop a new model Adaptive Differential Evolution based Quantum
Variational Autoencoder-Transformer Model (ADE-QVAET), that combines a Quantum
Variational Autoencoder-Transformer (QVAET) to obtain high-dimensional latent
features and maintain sequential dependencies together with contextual
relationships, resulting in superior defect prediction accuracy. Adaptive
Differential Evolution (ADE) Optimization utilizes an adaptive parameter tuning
method that enhances model convergence and predictive performance. ADE-QVAET
integrates advanced AI techniques to create a robust solution for scalable and
accurate software defect prediction that represents a top-level AI-driven
technology for quality engineering applications. The proposed ADE-QVAET model
attains high accuracy, precision, recall, and f1-score during the training
percentage (TP) 90 of 98.08%, 92.45%, 94.67%, and 98.12%.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 16:55:38 GMT'}]",2025-03-21,"[['Barma', 'Seshu Babu', ''], ['Hariharan', 'Mohanakrishnan', ''], ['Arvapalli', 'Satish', '']]","[{'text': 'adaptive parameter tuning\nmethod', 'label': 'Fine-tuning'}]",Fine-tuning,"adaptive parameter tuning
method",0.5371303558349609
2503.16418,Liming Jiang,"Liming Jiang, Qing Yan, Yumin Jia, Zichuan Liu, Hao Kang, Xin Lu",InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity,"Project page: https://bytedance.github.io/InfiniteYou/ Code and
  model: https://github.com/bytedance/InfiniteYou",,,,cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Achieving flexible and high-fidelity identity-preserved image generation
remains formidable, particularly with advanced Diffusion Transformers (DiTs)
like FLUX. We introduce InfiniteYou (InfU), one of the earliest robust
frameworks leveraging DiTs for this task. InfU addresses significant issues of
existing methods, such as insufficient identity similarity, poor text-image
alignment, and low generation quality and aesthetics. Central to InfU is
InfuseNet, a component that injects identity features into the DiT base model
via residual connections, enhancing identity similarity while maintaining
generation capabilities. A multi-stage training strategy, including pretraining
and supervised fine-tuning (SFT) with synthetic single-person-multiple-sample
(SPMS) data, further improves text-image alignment, ameliorates image quality,
and alleviates face copy-pasting. Extensive experiments demonstrate that InfU
achieves state-of-the-art performance, surpassing existing baselines. In
addition, the plug-and-play design of InfU ensures compatibility with various
existing methods, offering a valuable contribution to the broader community.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:59:34 GMT'}]",2025-03-21,"[['Jiang', 'Liming', ''], ['Yan', 'Qing', ''], ['Jia', 'Yumin', ''], ['Liu', 'Zichuan', ''], ['Kang', 'Hao', ''], ['Lu', 'Xin', '']]","[{'text': 'FLUX', 'label': 'Transformers'}, {'text': 'DiTs', 'label': 'Transformers'}, {'text': 'pretraining', 'label': 'Fine-tuning'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,supervised fine-tuning,0.7449287176132202
2503.16429,Xiaoyang Wu,"Xiaoyang Wu, Daniel DeTone, Duncan Frost, Tianwei Shen, Chris Xie, Nan
  Yang, Jakob Engel, Richard Newcombe, Hengshuang Zhao, Julian Straub",Sonata: Self-Supervised Learning of Reliable Point Representations,"CVPR 2025, produced by Pointcept x Meta, project page:
  https://xywu.me/sonata/",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we question whether we have a reliable self-supervised point
cloud model that can be used for diverse 3D tasks via simple linear probing,
even with limited data and minimal computation. We find that existing 3D
self-supervised learning approaches fall short when evaluated on representation
quality through linear probing. We hypothesize that this is due to what we term
the ""geometric shortcut"", which causes representations to collapse to low-level
spatial features. This challenge is unique to 3D and arises from the sparse
nature of point cloud data. We address it through two key strategies: obscuring
spatial information and enhancing the reliance on input features, ultimately
composing a Sonata of 140k point clouds through self-distillation. Sonata is
simple and intuitive, yet its learned representations are strong and reliable:
zero-shot visualizations demonstrate semantic grouping, alongside strong
spatial reasoning through nearest-neighbor relationships. Sonata demonstrates
exceptional parameter and data efficiency, tripling linear probing accuracy
(from 21.8% to 72.5%) on ScanNet and nearly doubling performance with only 1%
of the data compared to previous approaches. Full fine-tuning further advances
SOTA across both 3D indoor and outdoor perception tasks.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:59:59 GMT'}]",2025-03-21,"[['Wu', 'Xiaoyang', ''], ['DeTone', 'Daniel', ''], ['Frost', 'Duncan', ''], ['Shen', 'Tianwei', ''], ['Xie', 'Chris', ''], ['Yang', 'Nan', ''], ['Engel', 'Jakob', ''], ['Newcombe', 'Richard', ''], ['Zhao', 'Hengshuang', ''], ['Straub', 'Julian', '']]","[{'text': 'self-distillation', 'label': 'Knowledge distillation'}, {'text': 'zero-shot visualizations', 'label': 'Few-shot Learning'}, {'text': 'Full fine-tuning', 'label': 'Fine-tuning'}]",Fine-tuning,Full fine-tuning,0.956924319267273
