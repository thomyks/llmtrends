id,submitter,authors,title,comments,journal-ref,doi,report-no,categories,license,abstract,versions,update_date,authors_parsed,extracted_entities,assigned_concept,matched_keyword,similarity_score
2011.03043,Nolan Dey,"Nolan Dey and Eric Taylor and Alexander Wong and Bryan Tripp and
  Graham W. Taylor","Neuron-based explanations of neural networks sacrifice completeness and
  interpretability",TMLR 2025,,,,cs.LG cs.AI cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  High quality explanations of neural networks (NNs) should exhibit two key
properties. Completeness ensures that they accurately reflect a network's
function and interpretability makes them understandable to humans. Many
existing methods provide explanations of individual neurons within a network.
In this work we provide evidence that for AlexNet pretrained on ImageNet,
neuron-based explanation methods sacrifice both completeness and
interpretability compared to activation principal components. Neurons are a
poor basis for AlexNet embeddings because they don't account for the
distributed nature of these representations. By examining two quantitative
measures of completeness and conducting a user study to measure
interpretability, we show the most important principal components provide more
complete and interpretable explanations than the most important neurons. Much
of the activation variance may be explained by examining relatively few
high-variance PCs, as opposed to studying every neuron. These principal
components also strongly affect network function, and are significantly more
interpretable than neurons. Our findings suggest that explanation methods for
networks like AlexNet should avoid using neurons as a basis for embeddings and
instead choose a basis, such as principal components, which accounts for the
high dimensional and distributed nature of a network's internal
representations. Interactive demo and code available at
https://ndey96.github.io/neuron-explanations-sacrifice.
","[{'version': 'v1', 'created': 'Thu, 5 Nov 2020 21:26:03 GMT'}, {'version': 'v2', 'created': 'Tue, 8 Dec 2020 00:01:04 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 16:17:02 GMT'}]",2025-03-20,"[['Dey', 'Nolan', ''], ['Taylor', 'Eric', ''], ['Wong', 'Alexander', ''], ['Tripp', 'Bryan', ''], ['Taylor', 'Graham W.', '']]","[{'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}]",Embedding,embeddings,0.963064432144165
2204.05831,Hanul Jeon,Hanul Jeon and Richard Matthews,Very large set axioms over constructive set theories,"51 pages, Final version","Bull. Symb. Logic 30 (2024) no.4, 455-535",10.1017/bsl.2024.8,,math.LO,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  We investigate large set axioms defined in terms of elementary embeddings
over constructive set theories, focusing on $\mathsf{IKP}$ and $\mathsf{CZF}$.
Most previously studied large set axioms, notably the constructive analogues of
large cardinals below $0^\sharp$, have proof-theoretic strength weaker than
full Second-order Arithmetic. On the other hand, the situation is dramatically
different for those defined via elementary embeddings. We show that by adding
to $\mathsf{IKP}$ the basic properties of an elementary embedding $j\colon V\to
M$ for $\Delta_0$-formulas, which we will denote by
$\mathsf{\Delta_0\text{-}BTEE}_M$, we obtain the consistency of $\mathsf{ZFC}$
and more. We will also see that the consistency strength of a Reinhardt set
exceeds that of $\mathsf{ZF+WA}$. Furthermore, we will define super Reinhardt
sets and $\mathsf{TR}$, which is a constructive analogue of $V$ being totally
Reinhardt, and prove that their proof-theoretic strength exceeds that of
$\mathsf{ZF}$ with choiceless large cardinals.
","[{'version': 'v1', 'created': 'Tue, 12 Apr 2022 14:18:11 GMT'}, {'version': 'v2', 'created': 'Thu, 4 May 2023 02:57:00 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 13:05:40 GMT'}]",2025-03-20,"[['Jeon', 'Hanul', ''], ['Matthews', 'Richard', '']]","[{'text': 'elementary embeddings', 'label': 'Embedding'}, {'text': 'elementary embeddings', 'label': 'Embedding'}, {'text': 'elementary embedding', 'label': 'Embedding'}]",Embedding,elementary embedding,0.8491398096084595
2307.00637,Kailai Li,Kailai Li,On Embedding B-Splines in Recursive State Estimation,9 pages,,,,eess.SY cs.SY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a principled study on establishing a probabilistic framework for
continuous-time state estimation. B-splines are embedded into state-space
modeling as a continuous-time intermediate, linking the state of recurrent
control points with asynchronous sensor measurements. Based thereon, the
spline-embedded recursive estimation scheme is established w.r.t. common sensor
fusion tasks, and corresponding technique for modeling uncertain motion
estimates is introduced. We evaluate the proposed estimation scheme using
real-world-based synthesized data in a range-inertial setting. Numerical
results demonstrate several advantages of spline embedding in recursive state
estimation compared to classical discrete-time filtering approaches.
","[{'version': 'v1', 'created': 'Sun, 2 Jul 2023 19:16:29 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 17:32:54 GMT'}]",2025-03-19,"[['Li', 'Kailai', '']]","[{'text': 'B-splines', 'label': 'BERT'}, {'text': 'spline embedding', 'label': 'Embedding'}]",Embedding,spline embedding,0.6114894151687622
2308.13342,Iain Moffatt,"Criel Merino, Iain Moffatt and Steven Noble",The critical group of a combinatorial map,,,,,math.CO,http://creativecommons.org/licenses/by/4.0/,"  Motivated by the appearance of embeddings in the theory of chip firing and
the critical group of a graph, we introduce a version of the critical group (or
sandpile group) for combinatorial maps, that is, for graphs embedded in
orientable surfaces. We provide several definitions of our critical group, by
approaching it through analogues of the cycle-cocycle matrix, the Laplacian
matrix, and as the group of critical states of a chip firing game (or sandpile
model) on the edges of a map.
  Our group can be regarded as a perturbation of the classical critical group
of its underlying graph by topological information, and it agrees with the
classical critical group in the plane case. Its cardinality is equal to the
number of spanning quasi-trees in a connected map, just as the cardinality of
the classical critical group is equal to the number of spanning trees of a
connected graph.
  Our approach exploits the properties of principally unimodular matrices and
the methods of delta-matroid theory.
","[{'version': 'v1', 'created': 'Fri, 25 Aug 2023 12:26:00 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 11:34:48 GMT'}]",2025-03-19,"[['Merino', 'Criel', ''], ['Moffatt', 'Iain', ''], ['Noble', 'Steven', '']]","[{'text': 'embeddings', 'label': 'Embedding'}]",Embedding,embeddings,0.963064432144165
2312.16644,"Marc Kesseb\""ohmer","Marc Kesseb\""ohmer, Aljoscha Niemann",Exact asymptotic order for generalised adaptive approximations,"19 pages, 3 figures. arXiv admin note: text overlap with
  arXiv:2202.05247. Accepted for publication in Journal of Approximation Theory
  2025",,,,math.OC cs.IT math.FA math.IT math.PR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this note, we present an abstract approach to study asymptotic orders for
adaptive approximations with respect to a monotone set function $\mathfrak{J}$
defined on dyadic cubes. We determine the exact upper order in terms of the
critical value of the corresponding $\mathfrak{J}$-partition function, and we
are able to provide upper and lower bounds in term of fractal-geometric
quantities. With properly chosen $\mathfrak{J}$, our new approach has
applications in many different areas of mathematics, including the spectral
theory of Krein-Feller operators, quantization dimensions of compactly
supported probability measures, and the exact asymptotic order for Kolmogorov,
Gelfand and linear widths for Sobolev embeddings into $L_{\mu}^p$-spaces.
","[{'version': 'v1', 'created': 'Wed, 27 Dec 2023 17:17:35 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 07:59:21 GMT'}]",2025-03-18,"[['Kesseböhmer', 'Marc', ''], ['Niemann', 'Aljoscha', '']]","[{'text': 'Sobolev embeddings', 'label': 'Embedding'}]",Embedding,Sobolev embeddings,0.5912158489227295
2401.09995,Zhihao Wang,Zhihao Wang,"Stated $SL_n$-skein modules, roots of unity, and TQFT","29 pages, the update for the accepted version, to appear in Israel
  Journal of Mathematics",,,,math.QA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  For a pb surface $\Sigma$, two positive integers $m,n$ with $m\mid n$, and
two invertible elements $v,\epsilon$ in a commutative domain $R$ with
$\epsilon^{2m} = 1$, we construct an $R$-linear isomorphism between the stated
$SL_n$-skein algebras $S_n(\Sigma,v)$ and $S_n(\Sigma,\epsilon v)$, which
restricts to an algebraic ismorphism between subalgebras of $S_n(\Sigma,v)$ and
$S_n(\Sigma,\epsilon v)$. Using this linear isomorphism, we prove the splitting
map $\Theta_{c}:S_n(\Sigma,v)\rightarrow S_n(\text{Cut}_c(\Sigma),v)$ for the
pb surface $\Sigma$ and the ideal arc $c$ is injective when $v^{2m} = 1$ and
$m\mid n$.
  We generalize Barrett's work to the $SL_n$-skein space and stated
$SL_n$-skein space. As an application, we prove the splitting map for the
marked 3-manifolds is always injective when the quantum parameter $v=-1$.
  Let $(M,\mathcal{N})$ be a connected marked 3-manifold with
$\mathcal{N}\neq\emptyset$, and let $(M,\mathcal{N}')$ be obtained from
$(M,\mathcal{N})$ by adding one extra marking. When $v^4 =1$, we prove the
$R$-linear map from $S_n(M,\mathcal{N},v)$ to $S_n(M,\mathcal{N}',v)$ induced
by the embedding $(M,\mathcal{N})\rightarrow (M,\mathcal{N}')$ is injective and
$S_n(M.\mathcal{N}',v) = S_n(M,\mathcal{N},v)\otimes_{R}O_{q_v}(SL_n)$, where
$O_{q_v}(SL_n)$ is the quantization of the regular function ring of $SL_n$.
This shows the splitting map for $S_n(M,\mathcal{N},v)$ is always injective.
  We formulate the stated $SL_n$-TQFT theory, which generalizes the Costantino
and L\^e's stated $SL_2$-TQFT theory.
","[{'version': 'v1', 'created': 'Thu, 18 Jan 2024 14:11:39 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 10:20:33 GMT'}]",2025-03-21,"[['Wang', 'Zhihao', '']]","[{'text': 'embedding', 'label': 'Embedding'}]",Embedding,embedding,1.0
2402.05889,Shoubin Yu,"Shoubin Yu, Jaehong Yoon, Mohit Bansal","CREMA: Generalizable and Efficient Video-Language Reasoning via
  Multimodal Modular Fusion","ICLR 2025; first two authors contributed equally. Project page:
  https://CREMA-VideoLLM.github.io/",,,,cs.CV cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite impressive advancements in recent multimodal reasoning approaches,
they are still limited in flexibility and efficiency, as these models typically
process only a few fixed modality inputs and require updates to numerous
parameters. This paper tackles these critical challenges and proposes CREMA, a
generalizable, highly efficient, and modular modality-fusion framework that can
incorporate any new modality to enhance video reasoning. We first augment
multiple informative modalities (such as optical flow, 3D point cloud, audio,
thermal heatmap, and touch map) from given videos without extra human
annotation by leveraging sensors or existing pre-trained models. Next, we
introduce a query transformer with multiple parameter-efficient modules
associated with each accessible modality. It projects diverse modality features
to the LLM token embedding space, allowing the model to integrate different
data types for response generation. Furthermore, we propose a novel progressive
multimodal fusion design supported by a lightweight fusion module and
modality-sequential training strategy. It helps compress information across
various assisting modalities, maintaining computational efficiency in the LLM
while improving performance. We validate our method on 7 video-language
reasoning tasks assisted by diverse modalities, including conventional VideoQA
and Video-Audio/3D/Touch/Thermal QA, and achieve better/equivalent performance
against strong multimodal LLMs, including OneLLM, BLIP-2, and SeViLA while
reducing over 90% trainable parameters. We provide extensive analyses of CREMA,
including the impact of each modality on reasoning domains, the design of the
fusion module, and example visualizations.
","[{'version': 'v1', 'created': 'Thu, 8 Feb 2024 18:27:22 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Jun 2024 21:03:33 GMT'}, {'version': 'v3', 'created': 'Thu, 5 Dec 2024 04:16:54 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 02:27:50 GMT'}]",2025-03-21,"[['Yu', 'Shoubin', ''], ['Yoon', 'Jaehong', ''], ['Bansal', 'Mohit', '']]","[{'text': 'LLM token embedding space', 'label': 'Embedding'}]",Embedding,LLM token embedding space,0.5332349538803101
2402.16773,Adrian Dawid,Adrian Dawid,Hofer geometry of $A_3$-configurations,"35 pages, 4 figures. v2: to appear in Journal of Symplectic Geometry;
  revised based on the referee's comments",,,,math.SG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Let $L_0,L_1,L_2 \subset M$ be exact Lagrangian spheres in a Liouville domain
$M$ with $2c_1(M)=0$. If $L_0,L_1,L_2$ form an $A_3$-configuration, we show
that $\mathscr{L}(L_0)$ and $\mathscr{L}(L_2)$ endowed with the Hofer metric
contain quasi-isometric embeddings of $(\mathbb{R}^\infty, \|\cdot\|_\infty)$,
i.e. infinite-dimensional quasi-flats. A corollary of the proof presented here
establishes that $\text{Ham}_c(M)$ itself contains an infinite-dimensional
quasi-flat. We also show that for a Dehn twist $\tau: M \to M$ along $L_1$ the
boundary depth of $CF(\tau^{2\ell}(L_0), L')$ is unbounded in $L' \in
\mathscr{L}(L_2)$ for any $\ell \in \mathbb{N}_0$.
","[{'version': 'v1', 'created': 'Mon, 26 Feb 2024 17:44:20 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 16:35:13 GMT'}]",2025-03-19,"[['Dawid', 'Adrian', '']]","[{'text': 'quasi-isometric embeddings', 'label': 'Embedding'}]",Embedding,quasi-isometric embeddings,0.6314345598220825
2403.07493,Fernando Diaz-Diaz,Fernando Diaz-Diaz and Ernesto Estrada,Signed graphs in data sciences via communicability geometry,,Information Sciences 122096 (2025),10.1016/j.ins.2025.122096,,math.MG cs.DM cs.LG math.CO physics.soc-ph,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Signed graphs are an emergent way of representing data in a variety of
contexts where antagonistic interactions exist. These include data from
biological, ecological, and social systems. Here we propose the concept of
communicability for signed graphs and explore in depth its mathematical
properties. We also prove that the communicability induces a hyperspherical
geometric embedding of the signed network, and derive communicability-based
metrics that satisfy the axioms of a distance even in the presence of negative
edges. We then apply these metrics to solve several problems in the data
analysis of signed graphs within a unified framework. These include the
partitioning of signed graphs, dimensionality reduction, finding hierarchies of
alliances in signed networks, and quantifying the degree of polarization
between the existing factions in social systems represented by these types of
graphs.
","[{'version': 'v1', 'created': 'Tue, 12 Mar 2024 10:32:35 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 19:37:08 GMT'}]",2025-03-20,"[['Diaz-Diaz', 'Fernando', ''], ['Estrada', 'Ernesto', '']]","[{'text': 'hyperspherical\ngeometric embedding', 'label': 'Embedding'}]",Embedding,"hyperspherical
geometric embedding",0.6308190822601318
2403.20298,YoonHyuk Choi,"Yoonhyuk Choi, Jiho Choi, Taewook Ko, Chong-Kwon Kim",Review-Based Hyperbolic Cross-Domain Recommendation,WSDM '25,,10.1145/3701551.3703486,,cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The issue of data sparsity poses a significant challenge to recommender
systems. In response to this, algorithms that leverage side information such as
review texts have been proposed. Furthermore, Cross-Domain Recommendation
(CDR), which captures domain-shareable knowledge and transfers it from a richer
domain (source) to a sparser one (target), has received notable attention.
Nevertheless, the majority of existing methodologies assume a Euclidean
embedding space, encountering difficulties in accurately representing richer
text information and managing complex interactions between users and items.
This paper advocates a hyperbolic CDR approach based on review texts for
modeling user-item relationships. We first emphasize that conventional
distance-based domain alignment techniques may cause problems because small
modifications in hyperbolic geometry result in magnified perturbations,
ultimately leading to the collapse of hierarchical structures. To address this
challenge, we propose hierarchy-aware embedding and domain alignment schemes
that adjust the scale to extract domain-shareable information without
disrupting structural forms. The process involves the initial embedding of
review texts in hyperbolic space, followed by feature extraction incorporating
degree-based normalization and structure alignment. We conducted extensive
experiments to substantiate the efficiency, robustness, and scalability of our
proposed model in comparison to state-of-the-art baselines.
","[{'version': 'v1', 'created': 'Fri, 29 Mar 2024 17:15:21 GMT'}, {'version': 'v2', 'created': 'Thu, 23 Jan 2025 05:48:39 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 07:11:37 GMT'}]",2025-03-20,"[['Choi', 'Yoonhyuk', ''], ['Choi', 'Jiho', ''], ['Ko', 'Taewook', ''], ['Kim', 'Chong-Kwon', '']]","[{'text': 'hierarchy-aware embedding', 'label': 'Embedding'}]",Embedding,hierarchy-aware embedding,0.6870903372764587
2406.03044,Geeling Chau,"Geeling Chau, Christopher Wang, Sabera Talukder, Vighnesh Subramaniam,
  Saraswati Soedarmadji, Yisong Yue, Boris Katz, and Andrei Barbu","Population Transformer: Learning Population-level Representations of
  Neural Activity","22 pages, 17 figures, ICLR 2025",,,,cs.LG q-bio.NC,http://creativecommons.org/licenses/by/4.0/,"  We present a self-supervised framework that learns population-level codes for
arbitrary ensembles of neural recordings at scale. We address key challenges in
scaling models with neural time-series data, namely, sparse and variable
electrode distribution across subjects and datasets. The Population Transformer
(PopT) stacks on top of pretrained temporal embeddings and enhances downstream
decoding by enabling learned aggregation of multiple spatially-sparse data
channels. The pretrained PopT lowers the amount of data required for downstream
decoding experiments, while increasing accuracy, even on held-out subjects and
tasks. Compared to end-to-end methods, this approach is computationally
lightweight, while achieving similar or better decoding performance. We further
show how our framework is generalizable to multiple time-series embeddings and
neural data modalities. Beyond decoding, we interpret the pretrained and
fine-tuned PopT models to show how they can be used to extract neuroscience
insights from large amounts of data. We release our code as well as a
pretrained PopT to enable off-the-shelf improvements in multi-channel
intracranial data decoding and interpretability. Code is available at
https://github.com/czlwang/PopulationTransformer.
","[{'version': 'v1', 'created': 'Wed, 5 Jun 2024 08:15:09 GMT'}, {'version': 'v2', 'created': 'Wed, 9 Oct 2024 17:07:27 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 17:58:10 GMT'}]",2025-03-18,"[['Chau', 'Geeling', ''], ['Wang', 'Christopher', ''], ['Talukder', 'Sabera', ''], ['Subramaniam', 'Vighnesh', ''], ['Soedarmadji', 'Saraswati', ''], ['Yue', 'Yisong', ''], ['Katz', 'Boris', ''], ['Barbu', 'Andrei', '']]","[{'text': 'pretrained temporal embeddings', 'label': 'Embedding'}, {'text': 'PopT', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'time-series embeddings', 'label': 'Embedding'}, {'text': 'PopT', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'PopT', 'label': 'Generative Pre-trained Transformer (GPT)'}]",Embedding,time-series embeddings,0.6168131828308105
2406.09188,Jaeseok Byun,"Jaeseok Byun, Seokhyeon Jeong, Wonjae Kim, Sanghyuk Chun, Taesup Moon","An Efficient Post-hoc Framework for Reducing Task Discrepancy of Text
  Encoders for Composed Image Retrieval",22 pages,,,,cs.CV cs.IR,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Composed Image Retrieval (CIR) aims to retrieve a target image based on a
reference image and conditioning text, enabling controllable image searches.
The mainstream Zero-Shot (ZS) CIR methods bypass the need for expensive
training CIR triplets by projecting image embeddings into the text token
embedding space, forming a composed query for retrieval. However, we highlight
an inherent limitation in these projection-based CIR: a task discrepancy of
text encoders between the original pre-training task of the encoders (text
$\leftrightarrow$ image) and the target CIR task (image + text
$\leftrightarrow$ image), which potentially negatively impacts CIR performance.
To reduce such a discrepancy, a naive solution would be to train both image and
text encoders with CIR triplets in a supervised manner. Instead, we introduce
Reducing Task Discrepancy of Text Encoders (RTD), an efficient text-only
post-hoc framework that complements projection-based CIR methods. We devise a
novel target-anchored text contrastive learning designed to enhance the
capability of the text encoder for CIR. We also propose two key enhancements:
(1) a hard negative-based refined batch sampling strategy and (2) a refined
concatenation scheme to further mitigate training-inference discrepancy.
Integrating RTD into state-of-the-art projection-based methods achieves
performance comparable to, or even surpassing, resource-intensive
state-of-the-art synthetic CIR triplet-based approaches only with 23 minutes of
additional training on 4 A100 GPUs (up to $100\times$ faster in training). Our
code will be available upon acceptance.
","[{'version': 'v1', 'created': 'Thu, 13 Jun 2024 14:49:28 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 04:06:55 GMT'}]",2025-03-19,"[['Byun', 'Jaeseok', ''], ['Jeong', 'Seokhyeon', ''], ['Kim', 'Wonjae', ''], ['Chun', 'Sanghyuk', ''], ['Moon', 'Taesup', '']]","[{'text': 'image embeddings', 'label': 'Embedding'}]",Embedding,image embeddings,0.8342078924179077
2406.19761,Tiago Cerqueira,"Tiago F. T. Cerqueira, Haichen Wang, Silvana Botti, Miguel A. L.
  Marques",A non-orthogonal representation of the chemical space,,,,,cond-mat.mtrl-sci,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a novel approach to generate a fingerprint for crystalline
materials that balances efficiency for machine processing and human
interpretability, allowing its application in both machine learning inference
and understanding of structure-property relationships. Our proposed material
encoding has two components: one representing the crystal structure and the
other characterizing the chemical composition, that we call Pettifor embedding.
For the latter we construct a non-orthogonal space where each axis represents a
chemical element and where the angle between the axes quantifies a measure of
the similarity between them. The chemical composition is then defined by the
point on the unit sphere in this non-orthogonal space. We show that the
Pettifor embeddings systematically outperform other commonly used elemental
embeddings in compositional machine learning models. Using the Pettifor
embeddings to define a distance metric and applying dimension reduction
techniques, we construct a two-dimensional global map of the space of
thermodynamically stable crystalline compounds. Despite their simplicity, such
maps succeed in providing a physical separation of material classes according
to basic physical properties.
","[{'version': 'v1', 'created': 'Fri, 28 Jun 2024 09:04:01 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 12:53:47 GMT'}]",2025-03-21,"[['Cerqueira', 'Tiago F. T.', ''], ['Wang', 'Haichen', ''], ['Botti', 'Silvana', ''], ['Marques', 'Miguel A. L.', '']]","[{'text': 'Pettifor embedding', 'label': 'Embedding'}, {'text': 'Pettifor embeddings', 'label': 'Embedding'}, {'text': 'Pettifor\nembeddings', 'label': 'Embedding'}]",Embedding,Pettifor embedding,0.694644570350647
2407.09364,Andrea Tagarelli,"Lucio La Cava, Davide Costa, Andrea Tagarelli","Is Contrasting All You Need? Contrastive Learning for the Detection and
  Attribution of AI-generated Text","Accepted for publication at the 27th European Conference on
  Artificial Intelligence (ECAI-2024), Volume 392, Pages 3179 - 3186, October
  2024",,10.3233/FAIA240862,,cs.CL cs.AI cs.CY cs.HC physics.soc-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The significant progress in the development of Large Language Models has
contributed to blurring the distinction between human and AI-generated text.
The increasing pervasiveness of AI-generated text and the difficulty in
detecting it poses new challenges for our society. In this paper, we tackle the
problem of detecting and attributing AI-generated text by proposing WhosAI, a
triplet-network contrastive learning framework designed to predict whether a
given input text has been generated by humans or AI and to unveil the
authorship of the text. Unlike most existing approaches, our proposed framework
is conceived to learn semantic similarity representations from multiple
generators at once, thus equally handling both detection and attribution tasks.
Furthermore, WhosAI is model-agnostic and scalable to the release of new AI
text-generation models by incorporating their generated instances into the
embedding space learned by our framework. Experimental results on the
TuringBench benchmark of 200K news articles show that our proposed framework
achieves outstanding results in both the Turing Test and Authorship Attribution
tasks, outperforming all the methods listed in the TuringBench benchmark
leaderboards.
","[{'version': 'v1', 'created': 'Fri, 12 Jul 2024 15:44:56 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 09:19:05 GMT'}]",2025-03-18,"[['La Cava', 'Lucio', ''], ['Costa', 'Davide', ''], ['Tagarelli', 'Andrea', '']]","[{'text': 'embedding space', 'label': 'Embedding'}]",Embedding,embedding space,0.8514168858528137
2407.12778,Girish Vishwa,Jos\'e Figueroa-O'Farrill and Girish S Vishwa,The BRST quantisation of chiral BMS-like field theories,v2: 28 pages. Fixed typos and added some references,,,EMPG-24-3,hep-th math-ph math.MP,http://creativecommons.org/licenses/by/4.0/,"  The BMS$_3$ Lie algebra belongs to a one-parameter family of Lie algebras
obtained by centrally extending abelian extensions of the Witt algebra by a
tensor density representation. In this paper we call such Lie algebras
$\hat{\mathfrak{g}}_\lambda$, with BMS$_3$ corresponding to the universal
central extension of $\lambda = -1$. We construct the BRST complex for
$\hat{\mathfrak{g}}_\lambda$ in two different ways: one in the language of
semi-infinite cohomology and the other using the formalism of vertex operator
algebras. We pay particular attention to the case of BMS$_3$ and discuss some
natural field-theoretical realisations. We prove two theorems about the BRST
cohomology of $\hat{\mathfrak{g}}_\lambda$. The first is the construction of a
quasi-isomorphic embedding of the chiral sector of any Virasoro string as a
$\hat{\mathfrak{g}}_\lambda$ string. The second is the isomorphism (as
Batalin-Vilkovisky algebras) of any $\hat{\mathfrak{g}}_\lambda$ BRST
cohomology and the chiral ring of a topologically twisted $N{=}2$
superconformal field theory.
","[{'version': 'v1', 'created': 'Wed, 17 Jul 2024 17:56:42 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:35:37 GMT'}]",2025-03-20,"[[""Figueroa-O'Farrill"", 'José', ''], ['Vishwa', 'Girish S', '']]","[{'text': 'quasi-isomorphic embedding', 'label': 'Embedding'}]",Embedding,quasi-isomorphic embedding,0.6196544170379639
2407.18865,Melih Can Zerin,"Melih Can Zerin, Elif Vural and Ali \""Ozg\""ur Y{\i}lmaz","Downlink Channel Covariance Matrix Estimation via Representation
  Learning with Graph Regularization",,,,,cs.LG eess.SP,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we propose an algorithm for downlink (DL) channel covariance
matrix (CCM) estimation for frequency division duplexing (FDD) massive
multiple-input multiple-output (MIMO) communication systems with base station
(BS) possessing a uniform linear array (ULA) antenna structure. We consider a
setting where the UL CCM is mapped to DL CCM by a mapping function. We first
present a theoretical error analysis of learning a nonlinear embedding by
constructing a mapping function, which points to the importance of the
Lipschitz regularity of the mapping function for achieving high estimation
performance. Then, based on the theoretical ground, we propose a representation
learning algorithm as a solution for the estimation problem, where Gaussian RBF
kernel interpolators are chosen to map UL CCMs to their DL counterparts. The
proposed algorithm is based on the optimization of an objective function that
fits a regression model between the DL CCM and UL CCM samples in the training
dataset and preserves the local geometric structure of the data in the UL CCM
space, while explicitly regulating the Lipschitz continuity of the mapping
function in light of our theoretical findings. The proposed algorithm surpasses
benchmark methods in terms of three error metrics as shown by simulations.
","[{'version': 'v1', 'created': 'Fri, 26 Jul 2024 16:52:30 GMT'}, {'version': 'v2', 'created': 'Sun, 1 Sep 2024 06:39:14 GMT'}, {'version': 'v3', 'created': 'Sun, 16 Feb 2025 17:33:51 GMT'}, {'version': 'v4', 'created': 'Tue, 18 Mar 2025 21:48:14 GMT'}]",2025-03-20,"[['Zerin', 'Melih Can', ''], ['Vural', 'Elif', ''], ['Yılmaz', 'Ali Özgür', '']]","[{'text': 'nonlinear embedding', 'label': 'Embedding'}, {'text': 'UL CCM', 'label': 'LLM'}]",Embedding,nonlinear embedding,0.8063837289810181
2408.09921,Urs Schreiber,"Grigorios Giotopoulos, Hisham Sati, Urs Schreiber",Holographic M-Brane Super-Embeddings,"28 pages; v2: radial prefactors fixed in (48) and (94), spurious
  critical radius removed",,,,hep-th gr-qc math-ph math.DG math.MP,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Over a decade before the modern formulation of AdS/CFT duality, Duff et al.
had observed a candidate microscopic explanation by identifying the CFT fields
with fluctuations of probe p-branes stretched out in parallel near the horizon
of their own black brane incarnation. A profound way to characterize these and
more general probe p-brane configurations, especially for M5-branes, is
expected to be as ""super-embeddings"" of their super-worldvolumes into target
super-spacetime - but no concrete example of these had appeared in the
literature. Here we fill this gap by constructing the explicit holographic
super-embeddings of probe M5-branes and M2-branes into their corresponding
super-AdS backgrounds.
","[{'version': 'v1', 'created': 'Mon, 19 Aug 2024 11:55:57 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 17:43:46 GMT'}]",2025-03-19,"[['Giotopoulos', 'Grigorios', ''], ['Sati', 'Hisham', ''], ['Schreiber', 'Urs', '']]","[{'text': 'super-embeddings', 'label': 'Embedding'}]",Embedding,super-embeddings,0.7325682044029236
2409.04532,Alvaro Gonzalez-Hernandez,Alvaro Gonzalez-Hernandez,"Explicit desingularisation of Kummer surfaces in characteristic two via
  specialisation","34 pages. Code added as an ancillary file. Fixed minor errors and
  corrected sections 2.2 and 5.3. Comments are welcome!",,,,math.AG math.NT,http://creativecommons.org/licenses/by/4.0/,"  We study the birational geometry of the Kummer surfaces associated to the
Jacobian varieties of genus two curves, with a particular focus on fields of
characteristic two. In order to do so, we explicitly compute a projective
embedding of the Jacobian of a general genus two curve and, from this, we
construct its associated Kummer surface. This explicit construction produces a
model for desingularised Kummer surfaces over any field of characteristic not
two, and specialising these equations to characteristic two provides a model of
a partial desingularisation. Adapting the classic description of the Picard
lattice in terms of tropes, we also describe how to explicitly find completely
desingularised models of Kummer surfaces whenever the $p$-rank is not zero. In
the final section of this paper, we compute an example of a Kummer surface with
everywhere good reduction over a quadratic number field, and draw connections
between the models we computed and a criterion that determines when a Kummer
surface has good reduction at two.
","[{'version': 'v1', 'created': 'Fri, 6 Sep 2024 18:00:26 GMT'}, {'version': 'v2', 'created': 'Wed, 9 Oct 2024 11:28:02 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 19:00:17 GMT'}]",2025-03-20,"[['Gonzalez-Hernandez', 'Alvaro', '']]","[{'text': 'projective\nembedding', 'label': 'Embedding'}]",Embedding,"projective
embedding",0.713232159614563
2409.07725,Quanjun Li,"Kaizhe Fan, Quanjun Li","GRE^2-MDCL: Graph Representation Embedding Enhanced via Multidimensional
  Contrastive Learning","I am requesting the withdrawal of my paper due to errors identified
  in the methodology and experimental results. Specifically, there are
  inaccuracies in the analysis section that may lead to misleading conclusions",,,,cs.LG cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Graph representation learning has emerged as a powerful tool for preserving
graph topology when mapping nodes to vector representations, enabling various
downstream tasks such as node classification and community detection. However,
most current graph neural network models face the challenge of requiring
extensive labeled data, which limits their practical applicability in
real-world scenarios where labeled data is scarce. To address this challenge,
researchers have explored Graph Contrastive Learning (GCL), which leverages
enhanced graph data and contrastive learning techniques. While promising,
existing GCL methods often struggle with effectively capturing both local and
global graph structures, and balancing the trade-off between nodelevel and
graph-level representations. In this work, we propose Graph Representation
Embedding Enhanced via Multidimensional Contrastive Learning (GRE2-MDCL). Our
model introduces a novel triple network architecture with a multi-head
attention GNN as the core. GRE2-MDCL first globally and locally augments the
input graph using SVD and LAGNN techniques. It then constructs a
multidimensional contrastive loss, incorporating cross-network, cross-view, and
neighbor contrast, to optimize the model. Extensive experiments on benchmark
datasets Cora, Citeseer, and PubMed demonstrate that GRE2-MDCL achieves
state-of-the-art performance, with average accuracies of 82.5%, 72.5%, and
81.6% respectively. Visualizations further show tighter intra-cluster
aggregation and clearer inter-cluster boundaries, highlighting the
effectiveness of our framework in improving upon baseline GCL models.
","[{'version': 'v1', 'created': 'Thu, 12 Sep 2024 03:09:05 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 02:10:52 GMT'}]",2025-03-21,"[['Fan', 'Kaizhe', ''], ['Li', 'Quanjun', '']]","[{'text': 'Graph representation learning', 'label': 'Embedding'}, {'text': 'Graph Contrastive Learning', 'label': 'Embedding'}, {'text': 'Graph Representation\nEmbedding', 'label': 'Embedding'}]",Embedding,"Graph Representation
Embedding",0.681243896484375
2409.08984,Osmin Lacombe,"Osmin Lacombe, Lorenzo Paoloni, Francisco G. Pedro",Higher-derivative supersymmetric effective field theories,"41 pages, comments are welcome. v2: published version, minor typos
  corrected",,,,hep-th hep-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper we study higher-derivative supersymmetric effective field
theories focusing on the systematic procedure for the elimination of ghosts
from the spectrum. Particular attention is paid to the auxiliary fields, for
which the higher-derivative terms induce non-algebraic equations of motion. By
employing field redefinitions or the reduction of order procedure (both in
component and superfield language) we show that the auxiliary fields remain
non-dynamical in the EFT and that on shell they give rise to both derivative
and non-derivative corrections to the scalar action. These methods are applied
to the search for a SUSY embedding of the DBI action and to the dimensional
reduction of HD terms for the K\""ahler moduli in type IIB string
compactifications.
","[{'version': 'v1', 'created': 'Fri, 13 Sep 2024 16:57:50 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 11:14:21 GMT'}]",2025-03-18,"[['Lacombe', 'Osmin', ''], ['Paoloni', 'Lorenzo', ''], ['Pedro', 'Francisco G.', '']]","[{'text': 'SUSY embedding', 'label': 'Embedding'}]",Embedding,SUSY embedding,0.5564827919006348
2410.15959,Zhi Hou,"Zhi Hou, Tianyi Zhang, Yuwen Xiong, Haonan Duan, Hengjun Pu, Ronglei
  Tong, Chengyang Zhao, Xizhou Zhu, Yu Qiao, Jifeng Dai, Yuntao Chen","Dita: Scaling Diffusion Transformer for Generalist
  Vision-Language-Action Policy","I want to withdraw the recent replacement (v4), given that the author
  is different, the title is also different and the content is totally
  different",,,,cs.RO cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While recent vision-language-action models trained on diverse robot datasets
exhibit promising generalization capabilities with limited in-domain data,
their reliance on compact action heads to predict discretized or continuous
actions constrains adaptability to heterogeneous action spaces. We present
Dita, a scalable framework that leverages Transformer architectures to directly
denoise continuous action sequences through a unified multimodal diffusion
process. Departing from prior methods that condition denoising on fused
embeddings via shallow networks, Dita employs in-context conditioning --
enabling fine-grained alignment between denoised actions and raw visual tokens
from historical observations. This design explicitly models action deltas and
environmental nuances. By scaling the diffusion action denoiser alongside the
Transformer's scalability, Dita effectively integrates cross-embodiment
datasets across diverse camera perspectives, observation scenes, tasks, and
action spaces. Such synergy enhances robustness against various variances and
facilitates the successful execution of long-horizon tasks. Evaluations across
extensive benchmarks demonstrate state-of-the-art or comparative performance in
simulation. Notably, Dita achieves robust real-world adaptation to
environmental variances and complex long-horizon tasks through 10-shot
finetuning, using only third-person camera inputs. The architecture establishes
a versatile, lightweight and open-source baseline for generalist robot policy
learning. Project Page: https://robodita.github.io/
","[{'version': 'v1', 'created': 'Mon, 21 Oct 2024 12:43:54 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Feb 2025 07:20:30 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Feb 2025 15:38:06 GMT'}, {'version': 'v4', 'created': 'Fri, 14 Mar 2025 15:30:07 GMT'}, {'version': 'v5', 'created': 'Mon, 17 Mar 2025 11:45:52 GMT'}]",2025-03-18,"[['Hou', 'Zhi', ''], ['Zhang', 'Tianyi', ''], ['Xiong', 'Yuwen', ''], ['Duan', 'Haonan', ''], ['Pu', 'Hengjun', ''], ['Tong', 'Ronglei', ''], ['Zhao', 'Chengyang', ''], ['Zhu', 'Xizhou', ''], ['Qiao', 'Yu', ''], ['Dai', 'Jifeng', ''], ['Chen', 'Yuntao', '']]","[{'text': 'fused\nembeddings', 'label': 'Embedding'}, {'text': '10-shot\nfinetuning', 'label': 'Fine-tuning'}]",Embedding,"fused
embeddings",0.7370766401290894
2410.17069,Guo Lingzhen,"Lingzhen Guo, Tangyou Huang and Lei Du",Engineering Bosonic Codes with Quantum Lattice Gates,"24 pages, 10 figures; some figures are updated in the 3rd version",,,,quant-ph cond-mat.mes-hall cond-mat.quant-gas physics.app-ph physics.optics,http://creativecommons.org/licenses/by/4.0/,"  Bosonic codes offer a hardware-efficient approach to encoding and protecting
quantum information with a single continuous-variable bosonic system. In this
paper, we introduce a new universal quantum gate set composed of only one type
of gate element, which we call the quantum lattice gate, to engineer bosonic
code states for fault-tolerant quantum computing. We develop a systematic
framework for code state engineering based on the Floquet Hamiltonian
engineering, where the target Hamiltonian is constructed directly from the
given target state(s). We apply our method to three basic code state
engineering processes, including single code state preparation, code space
embedding and code space transformation. We explore the application of our
method to automatic quantum error correction against single-photon loss with
four-legged cat codes. Our proposal is particularly well-suited for
superconducting circuit architectures with Josephson junctions, where the full
nonlinearity of Josephson junction potential is harnessed as a quantum resource
and the quantum lattice gate can be implemented on a sub-nanosecond timescale.
","[{'version': 'v1', 'created': 'Tue, 22 Oct 2024 14:47:44 GMT'}, {'version': 'v2', 'created': 'Thu, 7 Nov 2024 17:18:38 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 03:01:33 GMT'}]",2025-03-18,"[['Guo', 'Lingzhen', ''], ['Huang', 'Tangyou', ''], ['Du', 'Lei', '']]","[{'text': 'code space\nembedding', 'label': 'Embedding'}, {'text': 'code space transformation', 'label': 'Embedding'}]",Embedding,"code space
embedding",0.6430759429931641
2410.24160,Fu Feng,"Fu Feng, Yucheng Xie, Xu Yang, Jing Wang, Xin Geng","Redefining <Creative> in Dictionary: Towards an Enhanced Semantic
  Understanding of Creative Generation",,,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  ``Creative'' remains an inherently abstract concept for both humans and
diffusion models. While text-to-image (T2I) diffusion models can easily
generate out-of-distribution concepts like ``a blue banana'', they struggle
with generating combinatorial objects such as ``a creative mixture that
resembles a lettuce and a mantis'', due to difficulties in understanding the
semantic depth of ``creative''. Current methods rely heavily on synthesizing
reference prompts or images to achieve a creative effect, typically requiring
retraining for each unique creative output-a process that is computationally
intensive and limits practical applications. To address this, we introduce
CreTok, which brings meta-creativity to diffusion models by redefining
``creative'' as a new token, \texttt{<CreTok>}, thus enhancing models' semantic
understanding for combinatorial creativity. CreTok achieves such redefinition
by iteratively sampling diverse text pairs from our proposed CangJie dataset to
form adaptive prompts and restrictive prompts, and then optimizing the
similarity between their respective text embeddings. Extensive experiments
demonstrate that <CreTok> enables the universal and direct generation of
combinatorial creativity across diverse concepts without additional training,
achieving state-of-the-art performance with improved text-image alignment and
higher human preference ratings. Code will be made available at
https://github.com/fu-feng/CreTok.
","[{'version': 'v1', 'created': 'Thu, 31 Oct 2024 17:19:03 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Nov 2024 10:22:59 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 06:33:07 GMT'}]",2025-03-18,"[['Feng', 'Fu', ''], ['Xie', 'Yucheng', ''], ['Yang', 'Xu', ''], ['Wang', 'Jing', ''], ['Geng', 'Xin', '']]","[{'text': 'CreTok', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'CreTok', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'adaptive prompts', 'label': 'Prompting'}, {'text': 'restrictive prompts', 'label': 'Prompting'}, {'text': 'text embeddings', 'label': 'Embedding'}]",Embedding,text embeddings,0.8121178150177002
2411.04667,Ryoichiro Agata,"Ryoichiro Agata, Satoru Baba, Ayako Nakanishi, Yasuyuki Nakamura","HypoNet Nankai: Rapid hypocenter determination tool for the Nankai
  Trough subduction zone using physics-informed neural networks",,,,,physics.geo-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Accurate hypocenter determination in the Nankai Trough subduction zone is
essential for hazard assessment and advancing our understanding of seismic
activity in the region. A handy hypocenter determination tool incorporating a
realistic 3D velocity structure, accessible to the scientific community, is
beneficial. In this study, we developed HypoNet Nankai, a rapid hypocenter
determination tool based on a physics-informed neural network (PINN) emulator
(surrogate model) for travel time calculations. This tool leverages a PINN
trained to predict P-wave travel times between arbitrary underground sources
and surface receivers with a realistic 3D P-wave velocity structure model of
the Nankai Trough subduction zone that incorporates marine seismic survey data.
The PINN embeds physical laws, namely, the Eikonal equation, directly into the
loss function of training and circumvents the need for labeled training data.
To address the training challenges posed by small-scale features in the
velocity model, we employed a simple domain decomposition approach and Fourier
feature embedding. Once trained, the PINN immediately infers the P-wave travel
time, enabling rapid hypocenter determination. The data size required to store
NNs for travel time calculations is significantly smaller than those of
conventional travel-time tables. HypoNet Nankai provides high flexibility for
addition of new observation points. We verified HypoNet Nankai by comparing its
performance with a widely used grid-based numerical method for forward travel
time calculations and synthetic hypocenter determination. In both tests,
HypoNet Nankai provided results consistent with those for the conventional
method. HypoNet Nankai offers a rapid, accurate, and easy-to-use hypocenter
determination method for the Nankai Trough subduction zone, with greater data
efficiency and extendibility compared to conventional approaches.
","[{'version': 'v1', 'created': 'Thu, 7 Nov 2024 12:53:33 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 07:02:38 GMT'}]",2025-03-19,"[['Agata', 'Ryoichiro', ''], ['Baba', 'Satoru', ''], ['Nakanishi', 'Ayako', ''], ['Nakamura', 'Yasuyuki', '']]","[{'text': 'HypoNet Nankai', 'label': 'Neural Language Model'}, {'text': 'PINN', 'label': 'Neural Language Model'}, {'text': 'Fourier\nfeature embedding', 'label': 'Embedding'}, {'text': 'PINN', 'label': 'Neural Language Model'}, {'text': 'HypoNet Nankai', 'label': 'Neural Language Model'}, {'text': 'HypoNet Nankai', 'label': 'Neural Language Model'}, {'text': 'HypoNet Nankai', 'label': 'Neural Language Model'}]",Embedding,"Fourier
feature embedding",0.584425687789917
2411.04713,Sijie Zhu,"Xin Gu, Ming Li, Libo Zhang, Fan Chen, Longyin Wen, Tiejian Luo, Sijie
  Zhu",Multi-Reward as Condition for Instruction-based Image Editing,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  High-quality training triplets (instruction, original image, edited image)
are essential for instruction-based image editing. Predominant training
datasets (e.g., InsPix2Pix) are created using text-to-image generative models
(e.g., Stable Diffusion, DALL-E) which are not trained for image editing.
Accordingly, these datasets suffer from inaccurate instruction following, poor
detail preserving, and generation artifacts. In this paper, we propose to
address the training data quality issue with multi-perspective reward data
instead of refining the ground-truth image quality. 1) we first design a
quantitative metric system based on best-in-class LVLM (Large Vision Language
Model), i.e., GPT-4o in our case, to evaluate the generation quality from 3
perspectives, namely, instruction following, detail preserving, and generation
quality. For each perspective, we collected quantitative score in $0\sim 5$ and
text descriptive feedback on the specific failure points in ground-truth edited
images, resulting in a high-quality editing reward dataset, i.e.,
RewardEdit20K. 2) We further proposed a novel training framework to seamlessly
integrate the metric output, regarded as multi-reward, into editing models to
learn from the imperfect training triplets. During training, the reward scores
and text descriptions are encoded as embeddings and fed into both the latent
space and the U-Net of the editing models as auxiliary conditions. 3) We also
build a challenging evaluation benchmark with real-world images/photos and
diverse editing instructions, named Real-Edit. Experiments indicate that our
multi-reward conditioned model outperforms its no-reward counterpart on two
popular editing pipelines, i.e., InsPix2Pix and SmartEdit. Code is released at
https://github.com/bytedance/Multi-Reward-Editing.
","[{'version': 'v1', 'created': 'Wed, 6 Nov 2024 05:02:29 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 00:04:47 GMT'}]",2025-03-21,"[['Gu', 'Xin', ''], ['Li', 'Ming', ''], ['Zhang', 'Libo', ''], ['Chen', 'Fan', ''], ['Wen', 'Longyin', ''], ['Luo', 'Tiejian', ''], ['Zhu', 'Sijie', '']]","[{'text': 'embeddings', 'label': 'Embedding'}]",Embedding,embeddings,0.963064432144165
2411.10867,Aarush Sinha,"Vipula Rawte, Sarthak Jain, Aarush Sinha, Garv Kaushik, Aman Bansal,
  Prathiksha Rumale Vishwanath, Samyak Rajesh Jain, Aishwarya Naresh Reganti,
  Vinija Jain, Aman Chadha, Amit P. Sheth, Amitava Das","ViBe: A Text-to-Video Benchmark for Evaluating Hallucination in Large
  Multimodal Models",,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Recent advances in Large Multimodal Models (LMMs) have expanded their
capabilities to video understanding, with Text-to-Video (T2V) models excelling
in generating videos from textual prompts. However, they still frequently
produce hallucinated content, revealing AI-generated inconsistencies. We
introduce ViBe (https://vibe-t2v-bench.github.io/): a large-scale dataset of
hallucinated videos from open-source T2V models. We identify five major
hallucination types: Vanishing Subject, Omission Error, Numeric Variability,
Subject Dysmorphia, and Visual Incongruity. Using ten T2V models, we generated
and manually annotated 3,782 videos from 837 diverse MS COCO captions. Our
proposed benchmark includes a dataset of hallucinated videos and a
classification framework using video embeddings. ViBe serves as a critical
resource for evaluating T2V reliability and advancing hallucination detection.
We establish classification as a baseline, with the TimeSFormer + CNN ensemble
achieving the best performance (0.345 accuracy, 0.342 F1 score). While initial
baselines proposed achieve modest accuracy, this highlights the difficulty of
automated hallucination detection and the need for improved methods. Our
research aims to drive the development of more robust T2V models and evaluate
their outputs based on user preferences.
","[{'version': 'v1', 'created': 'Sat, 16 Nov 2024 19:23:12 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 18:53:09 GMT'}]",2025-03-21,"[['Rawte', 'Vipula', ''], ['Jain', 'Sarthak', ''], ['Sinha', 'Aarush', ''], ['Kaushik', 'Garv', ''], ['Bansal', 'Aman', ''], ['Vishwanath', 'Prathiksha Rumale', ''], ['Jain', 'Samyak Rajesh', ''], ['Reganti', 'Aishwarya Naresh', ''], ['Jain', 'Vinija', ''], ['Chadha', 'Aman', ''], ['Sheth', 'Amit P.', ''], ['Das', 'Amitava', '']]","[{'text': 'Large Multimodal Models', 'label': 'Large Language Model'}, {'text': 'textual prompts', 'label': 'Prompting'}, {'text': 'video embeddings', 'label': 'Embedding'}]",Embedding,video embeddings,0.7205303907394409
2411.11092,Ilja Gogi\'c,Ilja Gogi\'c and Mateo Toma\v{s}evi\'c,"An extension of Petek-\v{S}emrl preserver theorems for Jordan embeddings
  of structural matrix algebras","22 pages, to appear in J. Math. Anal. Appl",,10.1016/j.jmaa.2025.129497,,math.RA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Let $M_n$ be the algebra of $n \times n$ complex matrices and $\mathcal{T}_n
\subseteq M_n$ the corresponding upper-triangular subalgebra. In their
influential work, Petek and \v{S}emrl characterize Jordan automorphisms of
$M_n$ and $\mathcal{T}_n$, when $n \geq 3$, as (injective in the case of
$\mathcal{T}_n$) continuous commutativity and spectrum preserving maps $\phi :
M_n \to M_n$ and $\phi : \mathcal{T}_n \to \mathcal{T}_n$. Recently, in a joint
work with Petek, the authors extended this characterization to the maps $\phi :
\mathcal{A} \to M_n$, where $\mathcal{A}$ is an arbitrary subalgebra of $M_n$
that contains $\mathcal{T}_n$. In particular, any such map $\phi$ is a Jordan
embedding and hence of the form $\phi(X)=TXT^{-1}$ or $\phi(X)=TX^tT^{-1}$, for
some invertible matrix $T\in M_n$. In this paper we further extend the
aforementioned results in the context of structural matrix algebras (SMAs),
i.e. subalgebras $\mathcal{A}$ of $M_n$ that contain all diagonal matrices.
More precisely, we provide both a necessary and sufficient condition for an SMA
$\mathcal{A}\subseteq M_n$ such that any injective continuous commutativity and
spectrum preserving map $\phi: \mathcal{A} \to M_n$ is necessarily a Jordan
embedding. In contrast to the previous cases, such maps $\phi$ no longer need
to be multiplicative/antimultiplicative, nor rank-one preservers.
","[{'version': 'v1', 'created': 'Sun, 17 Nov 2024 14:48:20 GMT'}, {'version': 'v2', 'created': 'Sun, 26 Jan 2025 16:53:39 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 13:04:42 GMT'}]",2025-03-19,"[['Gogić', 'Ilja', ''], ['Tomašević', 'Mateo', '']]","[{'text': 'Jordan\nembedding', 'label': 'Embedding'}, {'text': 'Jordan\nembedding', 'label': 'Embedding'}]",Embedding,"Jordan
embedding",0.6536723375320435
2411.16154,Sizai Hou,"Sizai Hou, Songze Li and Duanyi Yao",DeDe: Detecting Backdoor Samples for SSL Encoders via Decoders,To appear on CVPR 2025,,,,cs.LG cs.CR,http://creativecommons.org/licenses/by/4.0/,"  Self-supervised learning (SSL) is pervasively exploited in training
high-quality upstream encoders with a large amount of unlabeled data. However,
it is found to be susceptible to backdoor attacks merely via polluting a small
portion of training data. The victim encoders associate triggered inputs with
target embeddings, e.g., mapping a triggered cat image to an airplane
embedding, such that the downstream tasks inherit unintended behaviors when the
trigger is activated. Emerging backdoor attacks have shown great threats across
different SSL paradigms such as contrastive learning and CLIP, yet limited
research is devoted to defending against such attacks, and existing defenses
fall short in detecting advanced stealthy backdoors. To address the
limitations, we propose a novel detection mechanism, DeDe, which detects the
activation of backdoor mappings caused by triggered inputs on victim encoders.
Specifically, DeDe trains a decoder for any given SSL encoder using an
auxiliary dataset (which can be out-of-distribution or even slightly poisoned),
so that for any triggered input that misleads the encoder into the target
embedding, the decoder generates an output image significantly different from
the input. DeDe leverages the discrepancy between the input and the decoded
output to identify potential backdoor misbehavior during inference. We
empirically evaluate DeDe on both contrastive learning and CLIP models against
various types of backdoor attacks. Our results demonstrate promising detection
effectiveness over various advanced attacks and superior performance compared
over state-of-the-art detection methods.
","[{'version': 'v1', 'created': 'Mon, 25 Nov 2024 07:26:22 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 07:05:27 GMT'}]",2025-03-21,"[['Hou', 'Sizai', ''], ['Li', 'Songze', ''], ['Yao', 'Duanyi', '']]","[{'text': 'Self-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'target embeddings', 'label': 'Embedding'}, {'text': 'airplane\nembedding', 'label': 'Embedding'}, {'text': 'contrastive learning', 'label': 'Few-shot Learning'}, {'text': 'CLIP', 'label': 'Few-shot Learning'}, {'text': 'target\nembedding', 'label': 'Embedding'}, {'text': 'contrastive learning', 'label': 'Few-shot Learning'}, {'text': 'CLIP', 'label': 'Few-shot Learning'}]",Embedding,"target
embedding",0.7403842806816101
2411.19108,Feng Liu,"Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong
  Zhao, Yingya Zhang, Qixiang Ye, Fang Wan",Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model,Accepted in CVPR 2025. Project: https://liewfeng.github.io/TeaCache,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As a fundamental backbone for video generation, diffusion models are
challenged by low inference speed due to the sequential nature of denoising.
Previous methods speed up the models by caching and reusing model outputs at
uniformly selected timesteps. However, such a strategy neglects the fact that
differences among model outputs are not uniform across timesteps, which hinders
selecting the appropriate model outputs to cache, leading to a poor balance
between inference efficiency and visual quality. In this study, we introduce
Timestep Embedding Aware Cache (TeaCache), a training-free caching approach
that estimates and leverages the fluctuating differences among model outputs
across timesteps. Rather than directly using the time-consuming model outputs,
TeaCache focuses on model inputs, which have a strong correlation with the
modeloutputs while incurring negligible computational cost. TeaCache first
modulates the noisy inputs using the timestep embeddings to ensure their
differences better approximating those of model outputs. TeaCache then
introduces a rescaling strategy to refine the estimated differences and
utilizes them to indicate output caching. Experiments show that TeaCache
achieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%
Vbench score) degradation of visual quality.
","[{'version': 'v1', 'created': 'Thu, 28 Nov 2024 12:50:05 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 04:49:23 GMT'}]",2025-03-19,"[['Liu', 'Feng', ''], ['Zhang', 'Shiwei', ''], ['Wang', 'Xiaofeng', ''], ['Wei', 'Yujie', ''], ['Qiu', 'Haonan', ''], ['Zhao', 'Yuzhong', ''], ['Zhang', 'Yingya', ''], ['Ye', 'Qixiang', ''], ['Wan', 'Fang', '']]","[{'text': 'timestep embeddings', 'label': 'Embedding'}]",Embedding,timestep embeddings,0.6185715198516846
2411.19895,Zixuan Chen,"Zixuan Chen, Guangcong Wang, Jiahao Zhu, Jianhuang Lai, Xiaohua Xie",GuardSplat: Efficient and Robust Watermarking for 3D Gaussian Splatting,"This paper is accepted by the IEEE/CVF International Conference on
  Computer Vision and Pattern Recognition (CVPR), 2025",,,,cs.CV cs.CR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  3D Gaussian Splatting (3DGS) has recently created impressive 3D assets for
various applications. However, considering security, capacity, invisibility,
and training efficiency, the copyright of 3DGS assets is not well protected as
existing watermarking methods are unsuited for its rendering pipeline. In this
paper, we propose GuardSplat, an innovative and efficient framework for
watermarking 3DGS assets. Specifically, 1) We propose a CLIP-guided pipeline
for optimizing the message decoder with minimal costs. The key objective is to
achieve high-accuracy extraction by leveraging CLIP's aligning capability and
rich representations, demonstrating exceptional capacity and efficiency. 2) We
tailor a Spherical-Harmonic-aware (SH-aware) Message Embedding module for 3DGS,
seamlessly embedding messages into the SH features of each 3D Gaussian while
preserving the original 3D structure. This enables watermarking 3DGS assets
with minimal fidelity trade-offs and prevents malicious users from removing the
watermarks from the model files, meeting the demands for invisibility and
security. 3) We present an Anti-distortion Message Extraction module to improve
robustness against various distortions. Experiments demonstrate that GuardSplat
outperforms state-of-the-art and achieves fast optimization speed. Project page
is at https://narcissusex.github.io/GuardSplat, and Code is at
https://github.com/NarcissusEx/GuardSplat.
","[{'version': 'v1', 'created': 'Fri, 29 Nov 2024 17:59:03 GMT'}, {'version': 'v2', 'created': 'Mon, 2 Dec 2024 17:44:52 GMT'}, {'version': 'v3', 'created': 'Wed, 5 Mar 2025 21:10:52 GMT'}, {'version': 'v4', 'created': 'Fri, 14 Mar 2025 05:13:01 GMT'}, {'version': 'v5', 'created': 'Mon, 17 Mar 2025 16:33:17 GMT'}]",2025-03-18,"[['Chen', 'Zixuan', ''], ['Wang', 'Guangcong', ''], ['Zhu', 'Jiahao', ''], ['Lai', 'Jianhuang', ''], ['Xie', 'Xiaohua', '']]","[{'text': 'Message Embedding', 'label': 'Embedding'}]",Embedding,Message Embedding,0.6955314874649048
2412.05994,Namgyu Kang,"Namgyu Kang, Jaemin Oh, Youngjoon Hong, Eunbyung Park","PIG: Physics-Informed Gaussians as Adaptive Parametric Mesh
  Representations","Accepted by ICLR 2025. Project page:
  https://namgyukang.github.io/Physics-Informed-Gaussians/",,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The numerical approximation of partial differential equations (PDEs) using
neural networks has seen significant advancements through Physics-Informed
Neural Networks (PINNs). Despite their straightforward optimization framework
and flexibility in implementing various PDEs, PINNs often suffer from limited
accuracy due to the spectral bias of Multi-Layer Perceptrons (MLPs), which
struggle to effectively learn high-frequency and nonlinear components.
Recently, parametric mesh representations in combination with neural networks
have been investigated as a promising approach to eliminate the inductive bias
of MLPs. However, they usually require high-resolution grids and a large number
of collocation points to achieve high accuracy while avoiding overfitting. In
addition, the fixed positions of the mesh parameters restrict their
flexibility, making accurate approximation of complex PDEs challenging. To
overcome these limitations, we propose Physics-Informed Gaussians (PIGs), which
combine feature embeddings using Gaussian functions with a lightweight neural
network. Our approach uses trainable parameters for the mean and variance of
each Gaussian, allowing for dynamic adjustment of their positions and shapes
during training. This adaptability enables our model to optimally approximate
PDE solutions, unlike models with fixed parameter positions. Furthermore, the
proposed approach maintains the same optimization framework used in PINNs,
allowing us to benefit from their excellent properties. Experimental results
show the competitive performance of our model across various PDEs,
demonstrating its potential as a robust tool for solving complex PDEs. Our
project page is available at
https://namgyukang.github.io/Physics-Informed-Gaussians/
","[{'version': 'v1', 'created': 'Sun, 8 Dec 2024 16:58:29 GMT'}, {'version': 'v2', 'created': 'Sun, 2 Mar 2025 12:21:49 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 14:17:32 GMT'}]",2025-03-19,"[['Kang', 'Namgyu', ''], ['Oh', 'Jaemin', ''], ['Hong', 'Youngjoon', ''], ['Park', 'Eunbyung', '']]","[{'text': 'feature embeddings', 'label': 'Embedding'}]",Embedding,feature embeddings,0.7862301468849182
2412.06646,Francesco Ortu,"Alessandro Serra, Francesco Ortu, Emanuele Panizon, Lucrezia
  Valeriani, Lorenzo Basile, Alessio Ansuini, Diego Doimo, Alberto Cazzaniga","The Narrow Gate: Localized Image-Text Communication in Vision-Language
  Models",,,,,cs.CV cs.LG,http://creativecommons.org/licenses/by-sa/4.0/,"  Recent advances in multimodal training have significantly improved the
integration of image understanding and generation within a unified model. This
study investigates how vision-language models (VLMs) handle image-understanding
tasks, specifically focusing on how visual information is processed and
transferred to the textual domain. We compare VLMs that generate both images
and text with those that output only text, highlighting key differences in
information flow. We find that in models with multimodal outputs, image and
text embeddings are more separated within the residual stream. Additionally,
models vary in how information is exchanged from visual to textual tokens. VLMs
that only output text exhibit a distributed communication pattern, where
information is exchanged through multiple image tokens. In contrast, models
trained for image and text generation tend to rely on a single token that acts
as a narrow gate for visual information. We demonstrate that ablating this
single token significantly deteriorates performance on image understanding
tasks. Furthermore, modifying this token enables effective steering of the
image semantics, showing that targeted, local interventions can reliably
control the model's global behavior.
","[{'version': 'v1', 'created': 'Mon, 9 Dec 2024 16:39:40 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 10:59:29 GMT'}]",2025-03-18,"[['Serra', 'Alessandro', ''], ['Ortu', 'Francesco', ''], ['Panizon', 'Emanuele', ''], ['Valeriani', 'Lucrezia', ''], ['Basile', 'Lorenzo', ''], ['Ansuini', 'Alessio', ''], ['Doimo', 'Diego', ''], ['Cazzaniga', 'Alberto', '']]","[{'text': 'image and\ntext embeddings', 'label': 'Embedding'}]",Embedding,"image and
text embeddings",0.7801387906074524
2412.09165,Zhijie Nie,"Zhijie Nie, Zhangchi Feng, Mingxin Li, Cunwang Zhang, Yanzhao Zhang,
  Dingkun Long, Richong Zhang",When Text Embedding Meets Large Language Model: A Comprehensive Survey,"Version 3: We added some latest works of LLM-based Embedders and
  MLLM-based Embedders",,,,cs.CL cs.AI cs.IR,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Text embedding has become a foundational technology in natural language
processing (NLP) during the deep learning era, driving advancements across a
wide array of downstream tasks. While many natural language understanding
challenges can now be modeled using generative paradigms and leverage the
robust generative and comprehension capabilities of large language models
(LLMs), numerous practical applications - such as semantic matching,
clustering, and information retrieval - continue to rely on text embeddings for
their efficiency and effectiveness. Therefore, integrating LLMs with text
embeddings has become a major research focus in recent years. In this survey,
we categorize the interplay between LLMs and text embeddings into three
overarching themes: (1) LLM-augmented text embedding, enhancing traditional
embedding methods with LLMs; (2) LLMs as text embedders, adapting their innate
capabilities for high-quality embedding; and (3) Text embedding understanding
with LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing
recent works based on interaction patterns rather than specific downstream
applications, we offer a novel and systematic overview of contributions from
various research and application domains in the era of LLMs. Furthermore, we
highlight the unresolved challenges that persisted in the pre-LLM era with
pre-trained language models (PLMs) and explore the emerging obstacles brought
forth by LLMs. Building on this analysis, we outline prospective directions for
the evolution of text embedding, addressing both theoretical and practical
opportunities in the rapidly advancing landscape of NLP.
","[{'version': 'v1', 'created': 'Thu, 12 Dec 2024 10:50:26 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 16:11:43 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 16:15:29 GMT'}]",2025-03-21,"[['Nie', 'Zhijie', ''], ['Feng', 'Zhangchi', ''], ['Li', 'Mingxin', ''], ['Zhang', 'Cunwang', ''], ['Zhang', 'Yanzhao', ''], ['Long', 'Dingkun', ''], ['Zhang', 'Richong', '']]","[{'text': 'Text embedding', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'text embeddings', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'text\nembeddings', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'text embeddings', 'label': 'Embedding'}, {'text': 'text embedding', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Text embedding', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'text embedding', 'label': 'Embedding'}]",Embedding,Text embedding,0.8247289657592773
2412.09468,Yilei Zhao,"Yilei Zhao, Wentao Zhang, Tingran Yang, Yong Jiang, Fei Huang, and Wei
  Yang Bryan Lim","STORM: A Spatio-Temporal Factor Model Based on Dual Vector Quantized
  Variational Autoencoders for Financial Trading",,,,,cs.LG cs.AI,http://creativecommons.org/licenses/by/4.0/,"  In financial trading, factor models are widely used to price assets and
capture excess returns from mispricing. Recently, we have witnessed the rise of
variational autoencoder-based latent factor models, which learn latent factors
self-adaptively. While these models focus on modeling overall market
conditions, they often fail to effectively capture the temporal patterns of
individual stocks. Additionally, representing multiple factors as single values
simplifies the model but limits its ability to capture complex relationships
and dependencies. As a result, the learned factors are of low quality and lack
diversity, reducing their effectiveness and robustness across different trading
periods. To address these issues, we propose a Spatio-Temporal factOR Model
based on dual vector quantized variational autoencoders, named STORM, which
extracts features of stocks from temporal and spatial perspectives, then fuses
and aligns these features at the fine-grained and semantic level, and
represents the factors as multi-dimensional embeddings. The discrete codebooks
cluster similar factor embeddings, ensuring orthogonality and diversity, which
helps distinguish between different factors and enables factor selection in
financial trading. To show the performance of the proposed factor model, we
apply it to two downstream experiments: portfolio management on two stock
datasets and individual trading tasks on six specific stocks. The extensive
experiments demonstrate STORM's flexibility in adapting to downstream tasks and
superior performance over baseline models.
","[{'version': 'v1', 'created': 'Thu, 12 Dec 2024 17:15:49 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Jan 2025 05:25:35 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 04:30:03 GMT'}]",2025-03-18,"[['Zhao', 'Yilei', ''], ['Zhang', 'Wentao', ''], ['Yang', 'Tingran', ''], ['Jiang', 'Yong', ''], ['Huang', 'Fei', ''], ['Lim', 'Wei Yang Bryan', '']]","[{'text': 'multi-dimensional embeddings', 'label': 'Embedding'}, {'text': 'factor embeddings', 'label': 'Embedding'}]",Embedding,multi-dimensional embeddings,0.7590426206588745
2501.11870,Haipeng Liu,"Yang Wang, Haipeng Liu, Zeqian Yi, Biao Qian, Meng Wang",Coarse-to-Fine Lightweight Meta-Embedding for ID-Based Recommendation,"16 pages, 6 figures, accepted to appear at Science China Information
  Sciences",,10.1007/s11432-024-4350-9,,cs.IR cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The state-of-the-art recommendation systems have shifted the attention to
efficient recommendation, e.g., on-device recommendation, under memory
constraints. To this end, the existing methods either focused on the
lightweight embeddings for both users and items, or involved on-device systems
enjoying the compact embeddings to enhance reusability and reduces space
complexity. However, they focus solely on the coarse granularity of embedding,
while overlook the fine-grained semantic nuances, to adversarially downgrade
the efficacy of meta-embeddings in capturing the intricate relationship over
both user and item, consequently resulting into the suboptimal recommendations.
In this paper, we aim to study how the meta-embedding can efficiently learn
varied grained semantics, together with how the fine-grained meta-embedding can
strengthen the representation of coarse-grained meta-embedding. To answer these
questions, we develop a novel graph neural networks (GNNs) based recommender
where each user and item serves as the node, linked directly to coarse-grained
virtual nodes and indirectly to fine-grained virtual nodes, ensuring different
grained semantic learning, while disclosing: 1) In contrast to coarse-grained
semantics, fine-grained semantics are well captured through sparse
meta-embeddings, which adaptively 2) balance the embedding uniqueness and
memory constraint. Additionally, the initialization method come up upon
SparsePCA, along with a soft thresholding activation function to render the
sparseness of the meta-embeddings. We propose a weight bridging update strategy
that focuses on matching each coarse-grained meta-embedding with several
fine-grained meta-embeddings based on the users/items' semantics. Extensive
experiments substantiate our method's superiority over existing baselines. Our
code is available at https://github.com/htyjers/C2F-MetaEmbed.
","[{'version': 'v1', 'created': 'Tue, 21 Jan 2025 03:56:23 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 01:12:41 GMT'}]",2025-03-20,"[['Wang', 'Yang', ''], ['Liu', 'Haipeng', ''], ['Yi', 'Zeqian', ''], ['Qian', 'Biao', ''], ['Wang', 'Meng', '']]","[{'text': 'lightweight embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'meta-embeddings', 'label': 'Embedding'}, {'text': 'meta-embedding', 'label': 'Embedding'}, {'text': 'meta-embedding', 'label': 'Embedding'}, {'text': 'meta-embedding', 'label': 'Embedding'}, {'text': 'meta-embeddings', 'label': 'Embedding'}, {'text': 'meta-embeddings', 'label': 'Embedding'}, {'text': 'meta-embedding', 'label': 'Embedding'}, {'text': 'meta-embeddings', 'label': 'Embedding'}]",Embedding,embeddings,0.963064432144165
2501.14670,Mark Cannon,Johannes Buerger and Mark Cannon,Safe adaptive NMPC using ellipsoidal tubes,,,,,math.OC,http://creativecommons.org/licenses/by/4.0/,"  A computationally efficient nonlinear Model Predictive Control (NMPC)
algorithm is proposed for safe learning-based control with a system model
represented by an incompletely known affine combination of basis functions and
subject to additive set-bounded disturbances. The proposed algorithm employs
successive linearization around predicted trajectories and accounts for the
uncertain components of future states due to linearization, modelling errors
and disturbances using ellipsoidal sets centered on the predicted nominal state
trajectory. An ellipsoidal tube-based approach ensures satisfaction of
constraints on control variables and model states. Feasibility is ensured using
local bounds on linearization errors and a procedure based on a backtracking
line search. We combine the approach with a set membership parameter estimation
strategy in numerical simulations. We show that the ellipsoidal embedding of
the predicted uncertainty scales favourably with the problem size. The
resulting algorithm is recursively feasible and provides closed-loop stability
and performance guarantees.
","[{'version': 'v1', 'created': 'Fri, 24 Jan 2025 17:45:34 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 16:17:03 GMT'}]",2025-03-19,"[['Buerger', 'Johannes', ''], ['Cannon', 'Mark', '']]","[{'text': 'ellipsoidal embedding', 'label': 'Embedding'}]",Embedding,ellipsoidal embedding,0.6579141020774841
2501.16944,Maximilian Muschalik,"Maximilian Muschalik, Fabian Fumagalli, Paolo Frazzetto, Janine
  Strotherm, Luca Hermes, Alessandro Sperduti, Eyke H\""ullermeier, Barbara
  Hammer","Exact Computation of Any-Order Shapley Interactions for Graph Neural
  Networks",Preprint Version. Accepted at ICLR 2025,,,,cs.LG cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Albeit the ubiquitous use of Graph Neural Networks (GNNs) in machine learning
(ML) prediction tasks involving graph-structured data, their interpretability
remains challenging. In explainable artificial intelligence (XAI), the Shapley
Value (SV) is the predominant method to quantify contributions of individual
features to a ML model's output. Addressing the limitations of SVs in complex
prediction models, Shapley Interactions (SIs) extend the SV to groups of
features. In this work, we explain single graph predictions of GNNs with SIs
that quantify node contributions and interactions among multiple nodes. By
exploiting the GNN architecture, we show that the structure of interactions in
node embeddings are preserved for graph prediction. As a result, the
exponential complexity of SIs depends only on the receptive fields, i.e. the
message-passing ranges determined by the connectivity of the graph and the
number of convolutional layers. Based on our theoretical results, we introduce
GraphSHAP-IQ, an efficient approach to compute any-order SIs exactly.
GraphSHAP-IQ is applicable to popular message passing techniques in conjunction
with a linear global pooling and output layer. We showcase that GraphSHAP-IQ
substantially reduces the exponential complexity of computing exact SIs on
multiple benchmark datasets. Beyond exact computation, we evaluate
GraphSHAP-IQ's approximation of SIs on popular GNN architectures and compare
with existing baselines. Lastly, we visualize SIs of real-world water
distribution networks and molecule structures using a SI-Graph.
","[{'version': 'v1', 'created': 'Tue, 28 Jan 2025 13:37:44 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 09:46:45 GMT'}]",2025-03-18,"[['Muschalik', 'Maximilian', ''], ['Fumagalli', 'Fabian', ''], ['Frazzetto', 'Paolo', ''], ['Strotherm', 'Janine', ''], ['Hermes', 'Luca', ''], ['Sperduti', 'Alessandro', ''], ['Hüllermeier', 'Eyke', ''], ['Hammer', 'Barbara', '']]","[{'text': 'node embeddings', 'label': 'Embedding'}]",Embedding,node embeddings,0.7718001008033752
2502.01684,Srinitish Srinivasan,Srinitish Srinivasan and Omkumar CU,"Leveraging Joint Predictive Embedding and Bayesian Inference in Graph
  Self Supervised Learning",Preprit. Under Review,,,,cs.LG cs.AI cs.SI,http://creativecommons.org/licenses/by/4.0/,"  Graph representation learning has emerged as a cornerstone for tasks like
node classification and link prediction, yet prevailing self-supervised
learning (SSL) methods face challenges such as computational inefficiency,
reliance on contrastive objectives, and representation collapse. Existing
approaches often depend on feature reconstruction, negative sampling, or
complex decoders, which introduce training overhead and hinder generalization.
Further, current techniques which address such limitations fail to account for
the contribution of node embeddings to a certain prediction in the absence of
labeled nodes. To address these limitations, we propose a novel joint embedding
predictive framework for graph SSL that eliminates contrastive objectives and
negative sampling while preserving semantic and structural information.
Additionally, we introduce a semantic-aware objective term that incorporates
pseudo-labels derived from Gaussian Mixture Models (GMMs), enhancing node
discriminability by evaluating latent feature contributions. Extensive
experiments demonstrate that our framework outperforms state-of-the-art graph
SSL methods across benchmarks, achieving superior performance without
contrastive loss or complex decoders. Key innovations include (1) a
non-contrastive, view-invariant joint embedding predictive architecture, (2)
Leveraging single context and multiple targets relationship between subgraphs,
and (3) GMM-based pseudo-label scoring to capture semantic contributions. This
work advances graph SSL by offering a computationally efficient,
collapse-resistant paradigm that bridges spatial and semantic graph features
for downstream tasks. The code for our paper can be found at
https://github.com/Deceptrax123/JPEB-GSSL
","[{'version': 'v1', 'created': 'Sun, 2 Feb 2025 07:42:45 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 08:45:19 GMT'}]",2025-03-18,"[['Srinivasan', 'Srinitish', ''], ['CU', 'Omkumar', '']]","[{'text': 'Graph representation learning', 'label': 'Few-shot Learning'}, {'text': 'node embeddings', 'label': 'Embedding'}, {'text': 'joint embedding', 'label': 'contextual Embedding'}]",Embedding,node embeddings,0.7718001008033752
2502.07215,Osman Tursun,"Osman Tursun, Sinan Kalkan, Simon Denman, Clinton Fookes",PDV: Prompt Directional Vectors for Zero-shot Composed Image Retrieval,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Zero-shot composed image retrieval (ZS-CIR) enables image search using a
reference image and text prompt without requiring specialized text-image
composition networks trained on large-scale paired data. However, current
ZS-CIR approaches face three critical limitations in their reliance on composed
text embeddings: static query embedding representations, insufficient
utilization of image embeddings, and suboptimal performance when fusing text
and image embeddings. To address these challenges, we introduce the Prompt
Directional Vector (PDV), a simple yet effective training-free enhancement that
captures semantic modifications induced by user prompts. PDV enables three key
improvements: (1) dynamic composed text embeddings where prompt adjustments are
controllable via a scaling factor, (2) composed image embeddings through
semantic transfer from text prompts to image features, and (3) weighted fusion
of composed text and image embeddings that enhances retrieval by balancing
visual and semantic similarity. Our approach serves as a plug-and-play
enhancement for existing ZS-CIR methods with minimal computational overhead.
Extensive experiments across multiple benchmarks demonstrate that PDV
consistently improves retrieval performance when integrated with
state-of-the-art ZS-CIR approaches, particularly for methods that generate
accurate compositional embeddings. The code will be publicly available.
","[{'version': 'v1', 'created': 'Tue, 11 Feb 2025 03:20:21 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 01:26:06 GMT'}]",2025-03-18,"[['Tursun', 'Osman', ''], ['Kalkan', 'Sinan', ''], ['Denman', 'Simon', ''], ['Fookes', 'Clinton', '']]","[{'text': 'composed\ntext embeddings', 'label': 'Embedding'}, {'text': 'static query embedding representations', 'label': 'Embedding'}, {'text': 'image embeddings', 'label': 'Embedding'}, {'text': 'user prompts', 'label': 'Prompting'}, {'text': 'composed text embeddings', 'label': 'Embedding'}, {'text': 'scaling factor', 'label': 'Scaling law'}, {'text': 'composed image embeddings', 'label': 'Embedding'}, {'text': 'text prompts', 'label': 'Prompting'}, {'text': 'composed text and image embeddings', 'label': 'Embedding'}, {'text': 'publicly available', 'label': 'Open-source LLMs'}]",Embedding,image embeddings,0.8342078924179077
2503.00183,Jeffrey Adler,"Jeffrey D. Adler, Joshua M. Lansky, and Loren Spice",On smooth-group actions on reductive groups and spherical buildings,"With an appendix by Sean Cotner, Joshua M. Lansky, and Loren Spice.
  v2: revisions to appendix",,,,math.RT math.AG math.GR,http://creativecommons.org/licenses/by/4.0/,"  Let $k$ be a field, and suppose that $\Gamma$ is a smooth $k$-group that acts
on a connected, reductive $k$-group $\widetilde G$. Let $G$ denote the maximal
smooth, connected subgroup of the group of $\Gamma$-fixed points in $\widetilde
G$. Under fairly general conditions, we show that $G$ is a reductive $k$-group,
and that the image of the functorial embedding $\mathscr{S}(G) \longrightarrow
\mathscr{S}(\widetilde G)$ of spherical buildings is the set of
``$\Gamma$-fixed points in $\mathscr{S}(\widetilde G)$'', in a suitable sense.
In particular, we do not need to assume that $\Gamma$ has order relatively
prime to the characteristic of $k$ (nor even that $\Gamma$ is finite), nor that
the action of $\Gamma$ preserves a Borel-torus pair in $\widetilde G$.
","[{'version': 'v1', 'created': 'Fri, 28 Feb 2025 21:00:31 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 17:57:43 GMT'}]",2025-03-18,"[['Adler', 'Jeffrey D.', ''], ['Lansky', 'Joshua M.', ''], ['Spice', 'Loren', '']]","[{'text': 'functorial embedding', 'label': 'Embedding'}]",Embedding,functorial embedding,0.6446795463562012
2503.04310,Guillermo Garc\'ia-S\'aez,Jos\'e Carlos Bellido and Guillermo Garc\'ia-S\'aez,Bessel Potential Spaces and Complex Interpolation: Continuous embeddings,,,,,math.FA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Bessel potential spaces, introduced in the 1960s, are derived through complex
interpolation between Lebesgue and Sobolev spaces, making them intermediate
spaces of fractional differentiability order. Bessel potential spaces have
recently gained attention due to their identification with the Riesz fractional
gradient. This paper explores Bessel potential spaces as complex interpolation
spaces, providing original proofs of fundamental properties based on abstract
interpolation theory. Main results include a direct proof of norm equivalence,
continuous embeddings, and the relationship with Gagliardo spaces.
","[{'version': 'v1', 'created': 'Thu, 6 Mar 2025 10:53:14 GMT'}, {'version': 'v2', 'created': 'Fri, 7 Mar 2025 12:28:04 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 17:37:29 GMT'}]",2025-03-18,"[['Bellido', 'José Carlos', ''], ['García-Sáez', 'Guillermo', '']]","[{'text': 'Bessel potential spaces', 'label': 'BERT'}, {'text': 'Bessel potential spaces', 'label': 'BERT'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'Bessel potential spaces', 'label': 'BERT'}, {'text': 'continuous embeddings', 'label': 'Embedding'}]",Embedding,continuous embeddings,0.8004128336906433
2503.08049,Nadarasar Bahavan,"Nadarasar Bahavan, Sachith Seneviratne, Saman Halgamuge","SphOR: A Representation Learning Perspective on Open-set Recognition for
  Identifying Unknown Classes in Deep Learning Models",,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  The widespread use of deep learning classifiers necessitates Open-set
recognition (OSR), which enables the identification of input data not only from
classes known during training but also from unknown classes that might be
present in test data. Many existing OSR methods are computationally expensive
due to the reliance on complex generative models or suffer from high training
costs. We investigate OSR from a representation-learning perspective,
specifically through spherical embeddings. We introduce SphOR, a
computationally efficient representation learning method that models the
feature space as a mixture of von Mises-Fisher distributions. This approach
enables the use of semantically ambiguous samples during training, to improve
the detection of samples from unknown classes. We further explore the
relationship between OSR performance and key representation learning properties
which influence how well features are structured in high-dimensional space.
Extensive experiments on multiple OSR benchmarks demonstrate the effectiveness
of our method, producing state-of-the-art results, with improvements up-to 6%
that validate its performance. Code at
https://github.com/nadarasarbahavan/SpHOR
","[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 05:06:11 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 22:03:31 GMT'}]",2025-03-21,"[['Bahavan', 'Nadarasar', ''], ['Seneviratne', 'Sachith', ''], ['Halgamuge', 'Saman', '']]","[{'text': 'spherical embeddings', 'label': 'Embedding'}, {'text': 'SphOR', 'label': 'Generative Pre-trained Transformer (GPT)'}]",Embedding,spherical embeddings,0.6692852973937988
2503.09101,Mohammad Tariqul Islam,"Mohammad Tariqul Islam, Jason W. Fleischer","The Shape of Attraction in UMAP: Exploring the Embedding Forces in
  Dimensionality Reduction",9 page + appendix,,,,cs.LG cs.AI cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Uniform manifold approximation and projection (UMAP) is among the most
popular neighbor embedding methods. The method relies on attractive and
repulsive forces among high-dimensional data points to obtain a low-dimensional
embedding. In this paper, we analyze the forces to reveal their effects on
cluster formations and visualization. Repulsion emphasizes differences,
controlling cluster boundaries and inter-cluster distance. Attraction is more
subtle, as attractive tension between points can manifest simultaneously as
attraction and repulsion in the lower-dimensional mapping. This explains the
need for learning rate annealing and motivates the different treatments between
attractive and repulsive terms. Moreover, by modifying attraction, we improve
the consistency of cluster formation under random initialization. Overall, our
analysis makes UMAP and similar embedding methods more interpretable, more
robust, and more accurate.
","[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 06:37:43 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 15:48:38 GMT'}]",2025-03-19,"[['Islam', 'Mohammad Tariqul', ''], ['Fleischer', 'Jason W.', '']]","[{'text': 'Uniform manifold approximation and projection', 'label': 'Embedding'}, {'text': 'UMAP', 'label': 'Embedding'}, {'text': 'low-dimensional\nembedding', 'label': 'Embedding'}, {'text': 'UMAP', 'label': 'Embedding'}]",Embedding,"low-dimensional
embedding",0.7721446752548218
2503.09248,Lihua Zhou,"Lihua Zhou, Mao Ye, Shuaifeng Li, Nianxin Li, Xiatian Zhu, Lei Deng,
  Hongbin Liu, Zhen Lei",Bayesian Test-Time Adaptation for Vision-Language Models,Accepted to CVPR 2025,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Test-time adaptation with pre-trained vision-language models, such as CLIP,
aims to adapt the model to new, potentially out-of-distribution test data.
Existing methods calculate the similarity between visual embedding and
learnable class embeddings, which are initialized by text embeddings, for
zero-shot image classification. In this work, we first analyze this process
based on Bayes theorem, and observe that the core factors influencing the final
prediction are the likelihood and the prior. However, existing methods
essentially focus on adapting class embeddings to adapt likelihood, but they
often ignore the importance of prior. To address this gap, we propose a novel
approach, \textbf{B}ayesian \textbf{C}lass \textbf{A}daptation (BCA), which in
addition to continuously updating class embeddings to adapt likelihood, also
uses the posterior of incoming samples to continuously update the prior for
each class embedding. This dual updating mechanism allows the model to better
adapt to distribution shifts and achieve higher prediction accuracy. Our method
not only surpasses existing approaches in terms of performance metrics but also
maintains superior inference rates and memory usage, making it highly efficient
and practical for real-world applications.
","[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 10:42:11 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 06:59:16 GMT'}]",2025-03-18,"[['Zhou', 'Lihua', ''], ['Ye', 'Mao', ''], ['Li', 'Shuaifeng', ''], ['Li', 'Nianxin', ''], ['Zhu', 'Xiatian', ''], ['Deng', 'Lei', ''], ['Liu', 'Hongbin', ''], ['Lei', 'Zhen', '']]","[{'text': 'CLIP', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'visual embedding', 'label': 'Embedding'}, {'text': 'class embeddings', 'label': 'Embedding'}, {'text': 'text embeddings', 'label': 'Embedding'}, {'text': 'Bayes theorem', 'label': 'Few-shot Learning'}, {'text': 'class embeddings', 'label': 'Embedding'}, {'text': 'class embeddings', 'label': 'Embedding'}, {'text': 'class embedding', 'label': 'Embedding'}]",Embedding,visual embedding,0.8153178691864014
2503.09496,Junjie Zhou,"Junjie Zhou, Jiao Tang, Yingli Zuo, Peng Wan, Daoqiang Zhang, Wei Shao","Robust Multimodal Survival Prediction with the Latent Differentiation
  Conditional Variational AutoEncoder",Accepted by CVPR2025,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  The integrative analysis of histopathological images and genomic data has
received increasing attention for survival prediction of human cancers.
However, the existing studies always hold the assumption that full modalities
are available. As a matter of fact, the cost for collecting genomic data is
high, which sometimes makes genomic data unavailable in testing samples. A
common way of tackling such incompleteness is to generate the genomic
representations from the pathology images. Nevertheless, such strategy still
faces the following two challenges: (1) The gigapixel whole slide images (WSIs)
are huge and thus hard for representation. (2) It is difficult to generate the
genomic embeddings with diverse function categories in a unified generative
framework. To address the above challenges, we propose a Conditional Latent
Differentiation Variational AutoEncoder (LD-CVAE) for robust multimodal
survival prediction, even with missing genomic data. Specifically, a
Variational Information Bottleneck Transformer (VIB-Trans) module is proposed
to learn compressed pathological representations from the gigapixel WSIs. To
generate different functional genomic features, we develop a novel Latent
Differentiation Variational AutoEncoder (LD-VAE) to learn the common and
specific posteriors for the genomic embeddings with diverse functions. Finally,
we use the product-of-experts technique to integrate the genomic common
posterior and image posterior for the joint latent distribution estimation in
LD-CVAE. We test the effectiveness of our method on five different cancer
datasets, and the experimental results demonstrate its superiority in both
complete and missing modality scenarios.
","[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 15:58:37 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 07:15:08 GMT'}]",2025-03-19,"[['Zhou', 'Junjie', ''], ['Tang', 'Jiao', ''], ['Zuo', 'Yingli', ''], ['Wan', 'Peng', ''], ['Zhang', 'Daoqiang', ''], ['Shao', 'Wei', '']]","[{'text': 'genomic\nrepresentations', 'label': 'Embedding'}, {'text': 'genomic embeddings', 'label': 'Embedding'}, {'text': 'genomic embeddings', 'label': 'Embedding'}]",Embedding,genomic embeddings,0.6675938367843628
2503.10772,Ju He,"Ju He, Qihang Yu, Qihao Liu, Liang-Chieh Chen",FlowTok: Flowing Seamlessly Across Text and Image Tokens,Project page at https://tacju.github.io/projects/flowtok.html,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Bridging different modalities lies at the heart of cross-modality generation.
While conventional approaches treat the text modality as a conditioning signal
that gradually guides the denoising process from Gaussian noise to the target
image modality, we explore a much simpler paradigm-directly evolving between
text and image modalities through flow matching. This requires projecting both
modalities into a shared latent space, which poses a significant challenge due
to their inherently different representations: text is highly semantic and
encoded as 1D tokens, whereas images are spatially redundant and represented as
2D latent embeddings. To address this, we introduce FlowTok, a minimal
framework that seamlessly flows across text and images by encoding images into
a compact 1D token representation. Compared to prior methods, this design
reduces the latent space size by 3.3x at an image resolution of 256,
eliminating the need for complex conditioning mechanisms or noise scheduling.
Moreover, FlowTok naturally extends to image-to-text generation under the same
formulation. With its streamlined architecture centered around compact 1D
tokens, FlowTok is highly memory-efficient, requires significantly fewer
training resources, and achieves much faster sampling speeds-all while
delivering performance comparable to state-of-the-art models. Code will be
available at https://github.com/bytedance/1d-tokenizer.
","[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 18:06:13 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:39:37 GMT'}]",2025-03-20,"[['He', 'Ju', ''], ['Yu', 'Qihang', ''], ['Liu', 'Qihao', ''], ['Chen', 'Liang-Chieh', '']]","[{'text': '2D latent embeddings', 'label': 'Embedding'}]",Embedding,2D latent embeddings,0.7135133743286133
2503.11022,Hai Huang,"Hai Huang, Ziteng Xu, and Zhaoyu Zhang","Towards Efficient PCSEL Design: A Data-Driven Approach for Design
  Insights",,,,,physics.optics,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a data-driven design approach for photonic crystals to achieve
high efficiency in photonic crystal surface-emitting lasers (PCSELs). By
discretizing the photonic crystal structure into a grid, we enable the
generation of arbitrary lattice designs. Multiple fully connected layers
combined with a position embedding module extract essential features from the
photonic crystal designs, while coupled-wave theory (CWT) is used to evaluate
the efficiency (based on the ratio of surface-emitting to edge-emitting
resonant) and quality factor Q. We introduce the Neural Networks (NNs) model to
evaluate the structures, and to find a better performance design according to
the evaluation result. The model achieves high prediction accuracy, with
Pearson correlation coefficients of 0.780 for SEE and 0.887 for the
log-transformed Q. Additionally, we perform Shapley value analysis to identify
the most important Fourier coefficients, providing insights into the factors
that impact the performance of PCSEL designs. Our work speeds up the design
process and offers valuable guidance for optimizing high-performance PCSELs,
supporting the development of fully photonic design automation (PDA).
","[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 02:40:30 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 10:14:24 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 06:27:45 GMT'}]",2025-03-21,"[['Huang', 'Hai', ''], ['Xu', 'Ziteng', ''], ['Zhang', 'Zhaoyu', '']]","[{'text': 'position embedding module', 'label': 'Embedding'}]",Embedding,position embedding module,0.6141906976699829
2503.12358,In-Chang Baek,"In-Chang Baek, Sung-Hyun Kim, Seo-Young Lee, Dong-Hyeun Kim,
  Kyung-Joong Kim","IPCGRL: Language-Instructed Reinforcement Learning for Procedural Level
  Generation","9 pages, 9 figures, 3 tables",,,,cs.AI cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Recent research has highlighted the significance of natural language in
enhancing the controllability of generative models. While various efforts have
been made to leverage natural language for content generation, research on deep
reinforcement learning (DRL) agents utilizing text-based instructions for
procedural content generation remains limited. In this paper, we propose
IPCGRL, an instruction-based procedural content generation method via
reinforcement learning, which incorporates a sentence embedding model. IPCGRL
fine-tunes task-specific embedding representations to effectively compress
game-level conditions. We evaluate IPCGRL in a two-dimensional level generation
task and compare its performance with a general-purpose embedding method. The
results indicate that IPCGRL achieves up to a 21.4% improvement in
controllability and a 17.2% improvement in generalizability for unseen
instructions. Furthermore, the proposed method extends the modality of
conditional input, enabling a more flexible and expressive interaction
framework for procedural content generation.
","[{'version': 'v1', 'created': 'Sun, 16 Mar 2025 04:53:38 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 05:22:24 GMT'}]",2025-03-19,"[['Baek', 'In-Chang', ''], ['Kim', 'Sung-Hyun', ''], ['Lee', 'Seo-Young', ''], ['Kim', 'Dong-Hyeun', ''], ['Kim', 'Kyung-Joong', '']]","[{'text': 'task-specific embedding representations', 'label': 'Embedding'}]",Embedding,task-specific embedding representations,0.6056550741195679
2503.12713,Hanul Jeon,Hanul Jeon,Martin's measurable dilator,47 pages,,,,math.LO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Martin's remarking proof of $\mathbf{\Pi}^1_2$-determinacy from an iterable
rank-into-rank embedding highlighted the connection between large cardinals and
determinacy. In this paper, we isolate a large cardinal object called a
measurable dilator from Martin's proof of $\mathbf{\Pi}^1_2$-determinacy, which
captures the structural essence of Martin's proof of
$\mathbf{\Pi}^1_2$-determinacy.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 00:48:05 GMT'}]",2025-03-18,"[['Jeon', 'Hanul', '']]","[{'text': 'iterable\nrank-into-rank embedding', 'label': 'Embedding'}]",Embedding,"iterable
rank-into-rank embedding",0.5353941917419434
2503.12720,Feng Qiao,"Feng Qiao, Zhexiao Xiong, Eric Xing, Nathan Jacobs","GenStereo: Towards Open-World Generation of Stereo Images and
  Unsupervised Matching",Project page is available at https://qjizhi.github.io/genstereo,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Stereo images are fundamental to numerous applications, including extended
reality (XR) devices, autonomous driving, and robotics. Unfortunately,
acquiring high-quality stereo images remains challenging due to the precise
calibration requirements of dual-camera setups and the complexity of obtaining
accurate, dense disparity maps. Existing stereo image generation methods
typically focus on either visual quality for viewing or geometric accuracy for
matching, but not both. We introduce GenStereo, a diffusion-based approach, to
bridge this gap. The method includes two primary innovations (1) conditioning
the diffusion process on a disparity-aware coordinate embedding and a warped
input image, allowing for more precise stereo alignment than previous methods,
and (2) an adaptive fusion mechanism that intelligently combines the
diffusion-generated image with a warped image, improving both realism and
disparity consistency. Through extensive training on 11 diverse stereo
datasets, GenStereo demonstrates strong generalization ability. GenStereo
achieves state-of-the-art performance in both stereo image generation and
unsupervised stereo matching tasks. Our framework eliminates the need for
complex hardware setups while enabling high-quality stereo image generation,
making it valuable for both real-world applications and unsupervised learning
scenarios. Project page is available at https://qjizhi.github.io/genstereo
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 01:19:28 GMT'}]",2025-03-18,"[['Qiao', 'Feng', ''], ['Xiong', 'Zhexiao', ''], ['Xing', 'Eric', ''], ['Jacobs', 'Nathan', '']]","[{'text': 'disparity-aware coordinate embedding', 'label': 'Embedding'}]",Embedding,disparity-aware coordinate embedding,0.5408298373222351
2503.12739,Tianyu Zong,"Tianyu Zong, Bingkang Shi, Hongzhu Yi, Jungang Xu","TNCSE: Tensor's Norm Constraints for Unsupervised Contrastive Learning
  of Sentence Embeddings",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Unsupervised sentence embedding representation has become a hot research
topic in natural language processing. As a tensor, sentence embedding has two
critical properties: direction and norm. Existing works have been limited to
constraining only the orientation of the samples' representations while
ignoring the features of their module lengths. To address this issue, we
propose a new training objective that optimizes the training of unsupervised
contrastive learning by constraining the module length features between
positive samples. We combine the training objective of Tensor's Norm
Constraints with ensemble learning to propose a new Sentence Embedding
representation framework, TNCSE. We evaluate seven semantic text similarity
tasks, and the results show that TNCSE and derived models are the current
state-of-the-art approach; in addition, we conduct extensive zero-shot
evaluations, and the results show that TNCSE outperforms other baselines.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 02:14:42 GMT'}]",2025-03-18,"[['Zong', 'Tianyu', ''], ['Shi', 'Bingkang', ''], ['Yi', 'Hongzhu', ''], ['Xu', 'Jungang', '']]","[{'text': 'sentence embedding', 'label': 'Embedding'}, {'text': 'sentence embedding', 'label': 'Embedding'}, {'text': 'ensemble learning', 'label': 'Few-shot Learning'}, {'text': 'Sentence Embedding', 'label': 'Embedding'}]",Embedding,sentence embedding,0.7457362413406372
2503.12814,Jinseok Bae,"Jinseok Bae, Jungdam Won, Donggeun Lim, Inwoo Hwang, Young Min Kim","Versatile Physics-based Character Control with Hybrid Latent
  Representation",,,,,cs.GR cs.AI cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a versatile latent representation that enables physically
simulated character to efficiently utilize motion priors. To build a powerful
motion embedding that is shared across multiple tasks, the physics controller
should employ rich latent space that is easily explored and capable of
generating high-quality motion. We propose integrating continuous and discrete
latent representations to build a versatile motion prior that can be adapted to
a wide range of challenging control tasks. Specifically, we build a discrete
latent model to capture distinctive posterior distribution without collapse,
and simultaneously augment the sampled vector with the continuous residuals to
generate high-quality, smooth motion without jittering. We further incorporate
Residual Vector Quantization, which not only maximizes the capacity of the
discrete motion prior, but also efficiently abstracts the action space during
the task learning phase. We demonstrate that our agent can produce diverse yet
smooth motions simply by traversing the learned motion prior through
unconditional motion generation. Furthermore, our model robustly satisfies
sparse goal conditions with highly expressive natural motions, including
head-mounted device tracking and motion in-betweening at irregular intervals,
which could not be achieved with existing latent representations.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 04:45:51 GMT'}]",2025-03-18,"[['Bae', 'Jinseok', ''], ['Won', 'Jungdam', ''], ['Lim', 'Donggeun', ''], ['Hwang', 'Inwoo', ''], ['Kim', 'Young Min', '']]","[{'text': 'motion embedding', 'label': 'Embedding'}, {'text': 'Residual Vector Quantization', 'label': 'quantisation'}]",Embedding,motion embedding,0.670015811920166
2503.12834,Seunggwan Lee,"Seunggwan Lee, Hwanhee Jung, Byoungsoo Koh, Qixing Huang, Sangho Yoon,
  Sangpil Kim",PASTA: Part-Aware Sketch-to-3D Shape Generation with Text-Aligned Prior,"19 pages, 18 figures",,,,cs.CV cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  A fundamental challenge in conditional 3D shape generation is to minimize the
information loss and maximize the intention of user input. Existing approaches
have predominantly focused on two types of isolated conditional signals, i.e.,
user sketches and text descriptions, each of which does not offer flexible
control of the generated shape. In this paper, we introduce PASTA, the flexible
approach that seamlessly integrates a user sketch and a text description for 3D
shape generation. The key idea is to use text embeddings from a vision-language
model to enrich the semantic representation of sketches. Specifically, these
text-derived priors specify the part components of the object, compensating for
missing visual cues from ambiguous sketches. In addition, we introduce ISG-Net
which employs two types of graph convolutional networks: IndivGCN, which
processes fine-grained details, and PartGCN, which aggregates these details
into parts and refines the structure of objects. Extensive experiments
demonstrate that PASTA outperforms existing methods in part-level editing and
achieves state-of-the-art results in sketch-to-3D shape generation.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 05:31:09 GMT'}]",2025-03-18,"[['Lee', 'Seunggwan', ''], ['Jung', 'Hwanhee', ''], ['Koh', 'Byoungsoo', ''], ['Huang', 'Qixing', ''], ['Yoon', 'Sangho', ''], ['Kim', 'Sangpil', '']]","[{'text': 'text embeddings', 'label': 'Embedding'}]",Embedding,text embeddings,0.8121178150177002
2503.12836,Sumin In,"Sumin In, Youngdong Jang, Utae Jeong, MinHyuk Jang, Hyeongcheol Park,
  Eunbyung Park, Sangpil Kim",CompMarkGS: Robust Watermarking for Compression 3D Gaussian Splatting,"23 pages, 17 figures",,,,cs.CV cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  3D Gaussian Splatting (3DGS) enables rapid differentiable rendering for 3D
reconstruction and novel view synthesis, leading to its widespread commercial
use. Consequently, copyright protection via watermarking has become critical.
However, because 3DGS relies on millions of Gaussians, which require gigabytes
of storage, efficient transfer and storage require compression. Existing 3DGS
watermarking methods are vulnerable to quantization-based compression, often
resulting in the loss of the embedded watermark. To address this challenge, we
propose a novel watermarking method that ensures watermark robustness after
model compression while maintaining high rendering quality. In detail, we
incorporate a quantization distortion layer that simulates compression during
training, preserving the watermark under quantization-based compression. Also,
we propose a learnable watermark embedding feature that embeds the watermark
into the anchor feature, ensuring structural consistency and seamless
integration into the 3D scene. Furthermore, we present a frequency-aware anchor
growing mechanism to enhance image quality in high-frequency regions by
effectively identifying Guassians within these regions. Experimental results
confirm that our method preserves the watermark and maintains superior image
quality under high compression, validating it as a promising approach for a
secure 3DGS model.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 05:32:15 GMT'}]",2025-03-18,"[['In', 'Sumin', ''], ['Jang', 'Youngdong', ''], ['Jeong', 'Utae', ''], ['Jang', 'MinHyuk', ''], ['Park', 'Hyeongcheol', ''], ['Park', 'Eunbyung', ''], ['Kim', 'Sangpil', '']]","[{'text': 'compression', 'label': 'quantisation'}, {'text': 'quantization-based compression', 'label': 'quantisation'}, {'text': 'compression', 'label': 'quantisation'}, {'text': 'compression', 'label': 'quantisation'}, {'text': 'quantization-based compression', 'label': 'quantisation'}, {'text': 'learnable watermark embedding feature', 'label': 'Embedding'}, {'text': 'compression', 'label': 'quantisation'}]",Embedding,learnable watermark embedding feature,0.5608842372894287
2503.12893,Masanari Kimura,Masanari Kimura,Edgeworth Expansion for Semi-hard Triplet Loss,,,,,stat.ML cs.LG,http://creativecommons.org/licenses/by/4.0/,"  We develop a higher-order asymptotic analysis for the semi-hard triplet loss
using the Edgeworth expansion. It is known that this loss function enforces
that embeddings of similar samples are close while those of dissimilar samples
are separated by a specified margin. By refining the classical central limit
theorem, our approach quantifies the impact of the margin parameter and the
skewness of the underlying data distribution on the loss behavior. In
particular, we derive explicit Edgeworth expansions that reveal first-order
corrections in terms of the third cumulant, thereby characterizing non-Gaussian
effects present in the distribution of distance differences between
anchor-positive and anchor-negative pairs. Our findings provide detailed
insight into the sensitivity of the semi-hard triplet loss to its parameters
and offer guidance for choosing the margin to ensure training stability.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 07:46:10 GMT'}]",2025-03-18,"[['Kimura', 'Masanari', '']]","[{'text': 'embeddings', 'label': 'Embedding'}]",Embedding,embeddings,0.963064432144165
2503.12896,Shuaifan Jin,"Shuaifan Jin, Xiaoyi Pang, Zhibo Wang, He Wang, Jiacheng Du, Jiahui
  Hu, Kui Ren","Safeguarding LLM Embeddings in End-Cloud Collaboration via
  Entropy-Driven Perturbation",,,,,cs.CR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent studies improve on-device language model (LM) inference through
end-cloud collaboration, where the end device retrieves useful information from
cloud databases to enhance local processing, known as Retrieval-Augmented
Generation (RAG). Typically, to retrieve information from the cloud while
safeguarding privacy, the end device transforms original data into embeddings
with a local embedding model. However, the recently emerging Embedding
Inversion Attacks (EIAs) can still recover the original data from text
embeddings (e.g., training a recovery model to map embeddings back to original
texts), posing a significant threat to user privacy. To address this risk, we
propose EntroGuard, an entropy-driven perturbation-based embedding privacy
protection method, which can protect the privacy of text embeddings while
maintaining retrieval accuracy during the end-cloud collaboration.
Specifically, to defeat various EIAs, we perturb the embeddings to increase the
entropy of the recovered text in the common structure of recovery models, thus
steering the embeddings toward meaningless texts rather than original sensitive
texts during the recovery process. To maintain retrieval performance in the
cloud, we constrain the perturbations within a bound, applying the strategy of
reducing them where redundant and increasing them where sparse. Moreover,
EntroGuard can be directly integrated into end devices without requiring any
modifications to the embedding model. Extensive experimental results
demonstrate that EntroGuard can reduce the risk of privacy leakage by up to 8
times at most with negligible loss of retrieval performance compared to
existing privacy-preserving methods.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 07:58:05 GMT'}]",2025-03-18,"[['Jin', 'Shuaifan', ''], ['Pang', 'Xiaoyi', ''], ['Wang', 'Zhibo', ''], ['Wang', 'He', ''], ['Du', 'Jiacheng', ''], ['Hu', 'Jiahui', ''], ['Ren', 'Kui', '']]","[{'text': 'Retrieval-Augmented\nGeneration (RAG)', 'label': 'RAG'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}]",Embedding,embeddings,0.963064432144165
2503.12953,Zheyuan Liu,"Zheyuan Liu, Junyan Wang, Zicheng Duan, Cristian Rodriguez-Opazo,
  Anton van den Hengel","Frame-wise Conditioning Adaptation for Fine-Tuning Diffusion Models in
  Text-to-Video Prediction","20 pages, 15 figures",,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Text-video prediction (TVP) is a downstream video generation task that
requires a model to produce subsequent video frames given a series of initial
video frames and text describing the required motion. In practice TVP methods
focus on a particular category of videos depicting manipulations of objects
carried out by human beings or robot arms. Previous methods adapt models
pre-trained on text-to-image tasks, and thus tend to generate video that lacks
the required continuity. A natural progression would be to leverage more recent
pre-trained text-to-video (T2V) models. This approach is rendered more
challenging by the fact that the most common fine-tuning technique, low-rank
adaptation (LoRA), yields undesirable results. In this work, we propose an
adaptation-based strategy we label Frame-wise Conditioning Adaptation (FCA).
Within the module, we devise a sub-module that produces frame-wise text
embeddings from the input text, which acts as an additional text condition to
aid generation. We use FCA to fine-tune the T2V model, which incorporates the
initial frame(s) as an extra condition. We compare and discuss the more
effective strategy for injecting such embeddings into the T2V model. We conduct
extensive ablation studies on our design choices with quantitative and
qualitative performance analysis. Our approach establishes a new
state-of-the-art for the task of TVP. The project page is at
https://github.com/Cuberick-Orion/FCA .
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 09:06:21 GMT'}]",2025-03-18,"[['Liu', 'Zheyuan', ''], ['Wang', 'Junyan', ''], ['Duan', 'Zicheng', ''], ['Rodriguez-Opazo', 'Cristian', ''], ['Hengel', 'Anton van den', '']]","[{'text': 'frame-wise text\nembeddings', 'label': 'Embedding'}]",Embedding,"frame-wise text
embeddings",0.6638262271881104
2503.12994,Vincent Labatut,"No\'e Cecillon (LIA), Vincent Labatut (LIA), Richard Dufour (LS2N -
  \'equipe TALN)","Conversation-Based Multimodal Abuse Detection Through Text and Graph
  Embeddings",,"Computing, 2025",,,cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Abusive behavior is common on online social networks, and forces the hosts of
such platforms to find new solutions to address this problem. Various methods
have been proposed to automate this task in the past decade. Most of them rely
on the exchanged content, but ignore the structure and dynamics of the
conversation, which could provide some relevant information. In this article,
we propose to use representation learning methods to automatically produce
embeddings of this textual content and of the conversational graphs depicting
message exchanges. While the latter could be enhanced by including additional
information on top of the raw conversational structure, no method currently
exists to learn wholegraph representations using simultaneously edge
directions, weights, signs, and vertex attributes. We propose two such methods
to fill this gap in the literature. We experiment with 5 textual and 13 graph
embedding methods, and apply them to a dataset of online messages annotated for
abuse detection. Our best results achieve an F -measure of 81.02 using text
alone and 80.61 using graphs alone. We also combine both modalities of
information (text and graphs) through three fusion strategies, and show that
this strongly improves abuse detection performance, increasing the F -measure
to 87.06. Finally, we identify which specific engineered features are captured
by the embedding methods under consideration. These features have clear
interpretations and help explain what information the representation learning
methods deem discriminative.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 09:51:17 GMT'}]",2025-03-18,"[['Cecillon', 'Noé', '', 'LIA'], ['Labatut', 'Vincent', '', 'LIA'], ['Dufour', 'Richard', '', 'LS2N -\n  équipe TALN']]","[{'text': 'embeddings', 'label': 'Embedding'}]",Embedding,embeddings,0.963064432144165
2503.13012,Xingguo Lv,"Xingguo Lv, Xingbo Dong, Liwen Wang, Jiewen Yang, Lei Zhao, Bin Pu,
  Zhe Jin, Xuejun Li","Test-Time Domain Generalization via Universe Learning: A Multi-Graph
  Matching Approach for Medical Image Segmentation",,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite domain generalization (DG) has significantly addressed the
performance degradation of pre-trained models caused by domain shifts, it often
falls short in real-world deployment. Test-time adaptation (TTA), which adjusts
a learned model using unlabeled test data, presents a promising solution.
However, most existing TTA methods struggle to deliver strong performance in
medical image segmentation, primarily because they overlook the crucial prior
knowledge inherent to medical images. To address this challenge, we incorporate
morphological information and propose a framework based on multi-graph
matching. Specifically, we introduce learnable universe embeddings that
integrate morphological priors during multi-source training, along with novel
unsupervised test-time paradigms for domain adaptation. This approach
guarantees cycle-consistency in multi-matching while enabling the model to more
effectively capture the invariant priors of unseen data, significantly
mitigating the effects of domain shifts. Extensive experiments demonstrate that
our method outperforms other state-of-the-art approaches on two medical image
segmentation benchmarks for both multi-source and single-source domain
generalization tasks. The source code is available at
https://github.com/Yore0/TTDG-MGM.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 10:11:11 GMT'}]",2025-03-18,"[['Lv', 'Xingguo', ''], ['Dong', 'Xingbo', ''], ['Wang', 'Liwen', ''], ['Yang', 'Jiewen', ''], ['Zhao', 'Lei', ''], ['Pu', 'Bin', ''], ['Jin', 'Zhe', ''], ['Li', 'Xuejun', '']]","[{'text': 'multi-graph\nmatching', 'label': 'Embedding'}, {'text': 'learnable universe embeddings', 'label': 'Embedding'}, {'text': 'multi-source training', 'label': 'Few-shot Learning'}]",Embedding,learnable universe embeddings,0.6419995427131653
2503.13045,Gabriele Berton,"Gabriele Berton, Kevin Musgrave, Carlo Masone",All You Need to Know About Training Image Retrieval Models,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Image retrieval is the task of finding images in a database that are most
similar to a given query image. The performance of an image retrieval pipeline
depends on many training-time factors, including the embedding model
architecture, loss function, data sampler, mining function, learning rate(s),
and batch size. In this work, we run tens of thousands of training runs to
understand the effect each of these factors has on retrieval accuracy. We also
discover best practices that hold across multiple datasets. The code is
available at https://github.com/gmberton/image-retrieval
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 10:50:34 GMT'}]",2025-03-18,"[['Berton', 'Gabriele', ''], ['Musgrave', 'Kevin', ''], ['Masone', 'Carlo', '']]","[{'text': 'embedding model\narchitecture', 'label': 'Embedding'}]",Embedding,"embedding model
architecture",0.708124041557312
2503.13229,Yongkang Cheng,"Yongkang Cheng, Shaoli Huang","HoloGest: Decoupled Diffusion and Motion Priors for Generating
  Holisticly Expressive Co-speech Gestures",Accepted by 3DV 2025,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Animating virtual characters with holistic co-speech gestures is a
challenging but critical task. Previous systems have primarily focused on the
weak correlation between audio and gestures, leading to physically unnatural
outcomes that degrade the user experience. To address this problem, we
introduce HoleGest, a novel neural network framework based on decoupled
diffusion and motion priors for the automatic generation of high-quality,
expressive co-speech gestures. Our system leverages large-scale human motion
datasets to learn a robust prior with low audio dependency and high motion
reliance, enabling stable global motion and detailed finger movements. To
improve the generation efficiency of diffusion-based models, we integrate
implicit joint constraints with explicit geometric and conditional constraints,
capturing complex motion distributions between large strides. This integration
significantly enhances generation speed while maintaining high-quality motion.
Furthermore, we design a shared embedding space for gesture-transcription text
alignment, enabling the generation of semantically correct gesture actions.
Extensive experiments and user feedback demonstrate the effectiveness and
potential applications of our model, with our method achieving a level of
realism close to the ground truth, providing an immersive user experience. Our
code, model, and demo are are available at
https://cyk990422.github.io/HoloGest.github.io/.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 14:42:31 GMT'}]",2025-03-18,"[['Cheng', 'Yongkang', ''], ['Huang', 'Shaoli', '']]","[{'text': 'shared embedding space', 'label': 'Embedding'}]",Embedding,shared embedding space,0.7479425668716431
2503.13254,Jiangxia Cao,"Yu Liu, Hanbin Jiang, Lei Zhu, Yu Zhang, Yuqi Mao, Jiangxia Cao,
  Shuchao Pang","Federated Mixture-of-Expert for Non-Overlapped Cross-Domain Sequential
  Recommendation",Work in progress,,,,cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the real world, users always have multiple interests while surfing
different services to enrich their daily lives, e.g., watching hot short
videos/live streamings. To describe user interests precisely for a better user
experience, the recent literature proposes cross-domain techniques by
transferring the other related services (a.k.a. domain) knowledge to enhance
the accuracy of target service prediction. In practice, naive cross-domain
techniques typically require there exist some overlapped users, and sharing
overall information across domains, including user historical logs, user/item
embeddings, and model parameter checkpoints. Nevertheless, other domain's
user-side historical logs and embeddings are not always available in real-world
RecSys designing, since users may be totally non-overlapped across domains, or
the privacy-preserving policy limits the personalized information sharing
across domains. Thereby, a challenging but valuable problem is raised: How to
empower target domain prediction accuracy by utilizing the other domain model
parameters checkpoints only? To answer the question, we propose the FMoE-CDSR,
which explores the non-overlapped cross-domain sequential recommendation
scenario from the federated learning perspective.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:12:37 GMT'}]",2025-03-18,"[['Liu', 'Yu', ''], ['Jiang', 'Hanbin', ''], ['Zhu', 'Lei', ''], ['Zhang', 'Yu', ''], ['Mao', 'Yuqi', ''], ['Cao', 'Jiangxia', ''], ['Pang', 'Shuchao', '']]","[{'text': 'user/item\nembeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'privacy-preserving policy', 'label': 'AI Ethics'}]",Embedding,embeddings,0.963064432144165
2503.13383,Mengyao Lyu,"Mengyao Lyu, Yan Li, Huasong Zhong, Wenhao Yang, Hui Chen, Jungong
  Han, Guiguang Ding, Zhenheng Yang","Cream of the Crop: Harvesting Rich, Scalable and Transferable
  Multi-Modal Data for Instruction Fine-Tuning",update comparison with sota and analysis,,,,cs.CV cs.AI cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  The hypothesis that pretrained large language models (LLMs) necessitate only
minimal supervision during the fine-tuning (SFT) stage (Zhou et al., 2024) has
been substantiated by recent advancements in data curation and selection
research. However, their stability and generalizability are compromised due to
the vulnerability to experimental setups and validation protocols, falling
short of surpassing random sampling (Diddee & Ippolito, 2024; Xia et al.,
2024b). Built upon LLMs, multi-modal LLMs (MLLMs), combined with the sheer
token volume and heightened heterogeneity of data sources, amplify both the
significance and complexity of data selection.
  To harvest multi-modal instructional data in a robust and efficient manner,
we re-define the granularity of the quality metric by decomposing it into 14
vision-language-related capabilities, and introduce multi-modal rich scorers to
evaluate the capabilities of each data candidate. To promote diversity, in
light of the inherent objective of the alignment stage, we take interaction
style as diversity indicator and use a multi-modal rich styler to identify data
instruction patterns. In doing so, our multi-modal rich scorers and styler
(mmSSR) guarantee that high-scoring information is conveyed to users in
diversified forms. Free from embedding-based clustering or greedy sampling,
mmSSR efficiently scales to millions of data with varying budget constraints,
supports customization for general or specific capability acquisition, and
facilitates training-free generalization to new domains for curation. Across
10+ experimental settings, validated by 14 multi-modal benchmarks, we
demonstrate consistent improvements over random sampling, baseline strategies
and state-of-the-art selection methods, achieving 99.1% of full performance
with only 30% of the 2.6M data.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:11:22 GMT'}]",2025-03-18,"[['Lyu', 'Mengyao', ''], ['Li', 'Yan', ''], ['Zhong', 'Huasong', ''], ['Yang', 'Wenhao', ''], ['Chen', 'Hui', ''], ['Han', 'Jungong', ''], ['Ding', 'Guiguang', ''], ['Yang', 'Zhenheng', '']]","[{'text': 'embedding-based clustering', 'label': 'Embedding'}]",Embedding,embedding-based clustering,0.6901138424873352
2503.13409,Guillaume Lagarde,Gabriel Bathie and Guillaume Lagarde,"A $(1+\epsilon)$-Approximation for Ultrametric Embedding in Subquadratic
  Time",Extended version of AAAI 2025,,,,cs.DS,http://creativecommons.org/licenses/by/4.0/,"  Efficiently computing accurate representations of high-dimensional data is
essential for data analysis and unsupervised learning. Dendrograms, also known
as ultrametrics, are widely used representations that preserve hierarchical
relationships within the data. However, popular methods for computing them,
such as linkage algorithms, suffer from quadratic time and space complexity,
making them impractical for large datasets.
  The ""best ultrametric embedding"" (a.k.a. ""best ultrametric fit"") problem,
which aims to find the ultrametric that best preserves the distances between
points in the original data, is known to require at least quadratic time for an
exact solution.
  Recent work has focused on improving scalability by approximating optimal
solutions in subquadratic time, resulting in a $(\sqrt{2} +
\epsilon)$-approximation (Cohen-Addad, de Joannis de Verclos and Lagarde,
2021).
  In this paper, we present the first subquadratic algorithm that achieves
arbitrarily precise approximations of the optimal ultrametric embedding.
Specifically, we provide an algorithm that, for any $c \geq 1$, outputs a
$c$-approximation of the best ultrametric in time $\tilde{O}(n^{1 + 1/c})$. In
particular, for any fixed $\epsilon > 0$, the algorithm computes a
$(1+\epsilon)$-approximation in time $\tilde{O}(n^{2 - \epsilon +
o(\epsilon^2)})$.
  Experimental results show that our algorithm improves upon previous methods
in terms of approximation quality while maintaining comparable running times.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:38:37 GMT'}]",2025-03-18,"[['Bathie', 'Gabriel', ''], ['Lagarde', 'Guillaume', '']]","[{'text': 'Dendrograms', 'label': 'Embedding'}, {'text': 'ultrametric embedding', 'label': 'Embedding'}]",Embedding,ultrametric embedding,0.6919133067131042
2503.13557,Yifei Chen,Yifei Chen and Lambert Schomaker,"APF+: Boosting adaptive-potential function reinforcement learning
  methods with a W-shaped network for high-dimensional games",46 pages,,,,cs.LG cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Studies in reward shaping for reinforcement learning (RL) have flourished in
recent years due to its ability to speed up training. Our previous work
proposed an adaptive potential function (APF) and showed that APF can
accelerate the Q-learning with a Multi-layer Perceptron algorithm in the
low-dimensional domain. This paper proposes to extend APF with an encoder
(APF+) for RL state representation, allowing applying APF to the pixel-based
Atari games using a state-encoding method that projects high-dimensional game's
pixel frames to low-dimensional embeddings. We approach by designing the
state-representation encoder as a W-shaped network (W-Net), by using which we
are able to encode both the background as well as the moving entities in the
game frames. Specifically, the embeddings derived from the pre-trained W-Net
consist of two latent vectors: One represents the input state, and the other
represents the deviation of the input state's representation from itself. We
then incorporate W-Net into APF to train a downstream Dueling Deep Q-Network
(DDQN), obtain the APF-WNet-DDQN, and demonstrate its effectiveness in Atari
game-playing tasks. To evaluate the APF+W-Net module in such high-dimensional
tasks, we compare with two types of baseline methods: (i) the basic DDQN; and
(ii) two encoder-replaced APF-DDQN methods where we replace W-Net by (a) an
unsupervised state representation method called Spatiotemporal Deep Infomax
(ST-DIM) and (b) a ground truth state representation provided by the Atari
Annotated RAM Interface (ARI). The experiment results show that out of 20 Atari
games, APF-WNet-DDQN outperforms DDQN (14/20 games) and APF-STDIM-DDQN (13/20
games) significantly. In comparison against the APF-ARI-DDQN which employs
embeddings directly of the detailed game-internal state information, the
APF-WNet-DDQN achieves a comparable performance.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 01:53:26 GMT'}]",2025-03-19,"[['Chen', 'Yifei', ''], ['Schomaker', 'Lambert', '']]","[{'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}]",Embedding,embeddings,0.963064432144165
2503.13623,Tomojit Ghosh,"Sai Vijay Kumar Surineela, Prathyusha Kanakamalla, Harigovind
  Harikumar, and Tomojit Ghosh",A Convex formulation for linear discriminant analysis,"Total pages 29 including references, six figures, seven tables.
  Submitted to an Elsevier journal",,,,cs.LG cs.CV,http://creativecommons.org/licenses/by/4.0/,"  We present a supervised dimensionality reduction technique called Convex
Linear Discriminant Analysis (ConvexLDA). The proposed model optimizes a
multi-objective cost function by balancing two complementary terms. The first
term pulls the samples of a class towards its centroid by minimizing a sample's
distance from its class-centroid in low dimensional space. The second term
pushes the classes far apart by maximizing their hyperellipsoid scattering
volume via the logarithm of the determinant (\textit{log det}) of the outer
product matrix formed by the low-dimensional class-centroids. Using the
negative of the \textit{log det}, we pose the final cost as a minimization
problem, which balances the two terms using a hyper-parameter $\lambda$. We
demonstrate that the cost function is convex. Unlike Fisher LDA, the proposed
method doesn't require to compute the inverse of a matrix, hence avoiding any
ill-conditioned problem where data dimension is very high, e.g. RNA-seq data.
ConvexLDA doesn't require pair-wise distance calculation, making it faster and
more easily scalable. Moreover, the convex nature of the cost function ensures
global optimality, enhancing the reliability of the learned embedding. Our
experimental evaluation demonstrates that ConvexLDA outperforms several popular
linear discriminant analysis (LDA)-based methods on a range of high-dimensional
biological data, image data sets, etc.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 18:17:49 GMT'}]",2025-03-19,"[['Surineela', 'Sai Vijay Kumar', ''], ['Kanakamalla', 'Prathyusha', ''], ['Harikumar', 'Harigovind', ''], ['Ghosh', 'Tomojit', '']]","[{'text': 'learned embedding', 'label': 'Embedding'}]",Embedding,learned embedding,0.8268041610717773
2503.13641,Noah Snyder,Cain Edie-Michell and Noah Snyder,Interpolation categories for Conformal Embeddings,29 pages,,,,math.QA math.OA math.RT,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper we give a diagrammatic description of the categories of modules
coming from the conformal embeddings $\mathcal{V}(\mathfrak{sl}_N,N) \subset
\mathcal{V}(\mathfrak{so}_{N^2-1},1)$. A small variant on this construction
(morally corresponding to a conformal embedding of $\mathfrak{gl}_N$ level $N$
into $\mathfrak{o}_{N^2-1}$ level $1$) has uniform generators and relations
which are rational functions in $q = e^{2 \pi i/4N}$, which allows us to
construct a new continuous family of tensor categories at non-integer level
which interpolate between these categories. This is the second example of such
an interpolation category for families of conformal embeddings after Zhengwei
Liu's interpolation categories $\mathcal{V}(\mathfrak{sl}_N, N\pm 2) \subset
\mathcal{V}(\mathfrak{sl}_{N(N\pm 1)/2},1)$ which he constructed using his
classification Yang-Baxter planar algebras. Our approach is different from
Liu's, we build a two-color skein theory, with one strand coming from $X$ the
image of defining representation of $\mathfrak{sl}_N$ and the other strand
coming from an invertible object $g$ in the category of local modules, and a
trivalent vertex coming from a map $X \otimes X^* \rightarrow g$. We anticipate
small variations on our approach will yield interpolation categories for every
infinite discrete family of conformal embeddings.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 18:45:47 GMT'}]",2025-03-19,"[['Edie-Michell', 'Cain', ''], ['Snyder', 'Noah', '']]","[{'text': 'conformal embeddings', 'label': 'Embedding'}, {'text': 'conformal embedding', 'label': 'Embedding'}, {'text': 'conformal embeddings', 'label': 'Embedding'}, {'text': 'conformal embeddings', 'label': 'Embedding'}]",Embedding,conformal embedding,0.5596656799316406
2503.13777,Xuyang Fang,"Xuyang Fang, Sion Hannuna, Neill Campbell",8-Calves Image dataset,"11 pages, 5 figures",,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  We introduce the 8-Calves dataset, a benchmark for evaluating object
detection and identity classification in occlusion-rich, temporally consistent
environments. The dataset comprises a 1-hour video (67,760 frames) of eight
Holstein Friesian calves in a barn, with ground truth bounding boxes and
identities, alongside 900 static frames for detection tasks. Each calf exhibits
a unique coat pattern, enabling precise identity distinction.
  For cow detection, we fine-tuned 28 models (25 YOLO variants, 3 transformers)
on 600 frames, testing on the full video. Results reveal smaller YOLO models
(e.g., YOLOV9c) outperform larger counterparts despite potential bias from a
YOLOv8m-based labeling pipeline. For identity classification, embeddings from
23 pretrained vision models (ResNet, ConvNextV2, ViTs) were evaluated via
linear classifiers and KNN. Modern architectures like ConvNextV2 excelled,
while larger models frequently overfit, highlighting inefficiencies in scaling.
  Key findings include: (1) Minimal, targeted augmentations (e.g., rotation)
outperform complex strategies on simpler datasets; (2) Pretraining strategies
(e.g., BEiT, DinoV2) significantly boost identity recognition; (3) Temporal
continuity and natural motion patterns offer unique challenges absent in
synthetic or domain-specific benchmarks. The dataset's controlled design and
extended sequences (1 hour vs. prior 10-minute benchmarks) make it a pragmatic
tool for stress-testing occlusion handling, temporal consistency, and
efficiency.
  The link to the dataset is https://github.com/tonyFang04/8-calves.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 23:47:52 GMT'}]",2025-03-19,"[['Fang', 'Xuyang', ''], ['Hannuna', 'Sion', ''], ['Campbell', 'Neill', '']]","[{'text': 'embeddings', 'label': 'Embedding'}]",Embedding,embeddings,0.963064432144165
2503.13805,Xin Zhong,"Muhammad Ahtesham, Xin Zhong","Text-Guided Image Invariant Feature Learning for Robust Image
  Watermarking",,,,,cs.CV cs.LG cs.MM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Ensuring robustness in image watermarking is crucial for and maintaining
content integrity under diverse transformations. Recent self-supervised
learning (SSL) approaches, such as DINO, have been leveraged for watermarking
but primarily focus on general feature representation rather than explicitly
learning invariant features. In this work, we propose a novel text-guided
invariant feature learning framework for robust image watermarking. Our
approach leverages CLIP's multimodal capabilities, using text embeddings as
stable semantic anchors to enforce feature invariance under distortions. We
evaluate the proposed method across multiple datasets, demonstrating superior
robustness against various image transformations. Compared to state-of-the-art
SSL methods, our model achieves higher cosine similarity in feature consistency
tests and outperforms existing watermarking schemes in extraction accuracy
under severe distortions. These results highlight the efficacy of our method in
learning invariant representations tailored for robust deep learning-based
watermarking.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 01:32:38 GMT'}]",2025-03-19,"[['Ahtesham', 'Muhammad', ''], ['Zhong', 'Xin', '']]","[{'text': 'text embeddings', 'label': 'Embedding'}]",Embedding,text embeddings,0.8121178150177002
2503.13861,Yujin Wang Mr,"Yujin Wang, Quanfeng Liu, Zhengxin Jiang, Tianyi Wang, Junfeng Jiao,
  Hongqing Chu, Bingzhao Gao, Hong Chen","RAD: Retrieval-Augmented Decision-Making of Meta-Actions with
  Vision-Language Models in Autonomous Driving",,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Accurately understanding and deciding high-level meta-actions is essential
for ensuring reliable and safe autonomous driving systems. While
vision-language models (VLMs) have shown significant potential in various
autonomous driving tasks, they often suffer from limitations such as inadequate
spatial perception and hallucination, reducing their effectiveness in complex
autonomous driving scenarios. To address these challenges, we propose a
retrieval-augmented decision-making (RAD) framework, a novel architecture
designed to enhance VLMs' capabilities to reliably generate meta-actions in
autonomous driving scenes. RAD leverages a retrieval-augmented generation (RAG)
pipeline to dynamically improve decision accuracy through a three-stage process
consisting of the embedding flow, retrieving flow, and generating flow.
Additionally, we fine-tune VLMs on a specifically curated dataset derived from
the NuScenes dataset to enhance their spatial perception and bird's-eye view
image comprehension capabilities. Extensive experimental evaluations on the
curated NuScenes-based dataset demonstrate that RAD outperforms baseline
methods across key evaluation metrics, including match accuracy, and F1 score,
and self-defined overall score, highlighting its effectiveness in improving
meta-action decision-making for autonomous driving tasks.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 03:25:57 GMT'}]",2025-03-19,"[['Wang', 'Yujin', ''], ['Liu', 'Quanfeng', ''], ['Jiang', 'Zhengxin', ''], ['Wang', 'Tianyi', ''], ['Jiao', 'Junfeng', ''], ['Chu', 'Hongqing', ''], ['Gao', 'Bingzhao', ''], ['Chen', 'Hong', '']]","[{'text': 'embedding flow', 'label': 'Embedding'}]",Embedding,embedding flow,0.6818602085113525
2503.13925,Da Kuang,"Da Kuang, Guanwen Qiu, Junhyong Kim","Reconstructing Cell Lineage Trees from Phenotypic Features with Metric
  Learning",,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  How a single fertilized cell gives rise to a complex array of specialized
cell types in development is a central question in biology. The cells grow,
divide, and acquire differentiated characteristics through poorly understood
molecular processes. A key approach to studying developmental processes is to
infer the tree graph of cell lineage division and differentiation histories,
providing an analytical framework for dissecting individual cells' molecular
decisions during replication and differentiation. Although genetically
engineered lineage-tracing methods have advanced the field, they are either
infeasible or ethically constrained in many organisms. In contrast, modern
single-cell technologies can measure high-content molecular profiles (e.g.,
transcriptomes) in a wide range of biological systems.
  Here, we introduce CellTreeQM, a novel deep learning method based on
transformer architectures that learns an embedding space with geometric
properties optimized for tree-graph inference. By formulating lineage
reconstruction as a tree-metric learning problem, we have systematically
explored supervised, weakly supervised, and unsupervised training settings and
present a Lineage Reconstruction Benchmark to facilitate comprehensive
evaluation of our learning method. We benchmarked the method on (1) synthetic
data modeled via Brownian motion with independent noise and spurious signals
and (2) lineage-resolved single-cell RNA sequencing datasets. Experimental
results show that CellTreeQM recovers lineage structures with minimal
supervision and limited data, offering a scalable framework for uncovering cell
lineage relationships in challenging animal models. To our knowledge, this is
the first method to cast cell lineage inference explicitly as a metric learning
task, paving the way for future computational models aimed at uncovering the
molecular dynamics of cell lineage.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 05:41:03 GMT'}]",2025-03-19,"[['Kuang', 'Da', ''], ['Qiu', 'Guanwen', ''], ['Kim', 'Junhyong', '']]","[{'text': 'ethically constrained', 'label': 'AI Ethics'}, {'text': 'embedding space', 'label': 'Embedding'}]",Embedding,embedding space,0.8514168858528137
2503.13948,Mufan Liu,"Mufan Liu, Qi Yang, He Huang, Wenjie Huang, Zhenlong Yuan, Zhu Li,
  Yiling Xu","Light4GS: Lightweight Compact 4D Gaussian Splatting Generation via
  Context Model",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  3D Gaussian Splatting (3DGS) has emerged as an efficient and high-fidelity
paradigm for novel view synthesis. To adapt 3DGS for dynamic content,
deformable 3DGS incorporates temporally deformable primitives with learnable
latent embeddings to capture complex motions. Despite its impressive
performance, the high-dimensional embeddings and vast number of primitives lead
to substantial storage requirements. In this paper, we introduce a
\textbf{Light}weight \textbf{4}D\textbf{GS} framework, called Light4GS, that
employs significance pruning with a deep context model to provide a lightweight
storage-efficient dynamic 3DGS representation. The proposed Light4GS is based
on 4DGS that is a typical representation of deformable 3DGS. Specifically, our
framework is built upon two core components: (1) a spatio-temporal significance
pruning strategy that eliminates over 64\% of the deformable primitives,
followed by an entropy-constrained spherical harmonics compression applied to
the remainder; and (2) a deep context model that integrates intra- and
inter-prediction with hyperprior into a coarse-to-fine context structure to
enable efficient multiscale latent embedding compression. Our approach achieves
over 120x compression and increases rendering FPS up to 20\% compared to the
baseline 4DGS, and also superior to frame-wise state-of-the-art 3DGS
compression methods, revealing the effectiveness of our Light4GS in terms of
both intra- and inter-prediction methods without sacrificing rendering quality.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 06:28:13 GMT'}]",2025-03-19,"[['Liu', 'Mufan', ''], ['Yang', 'Qi', ''], ['Huang', 'He', ''], ['Huang', 'Wenjie', ''], ['Yuan', 'Zhenlong', ''], ['Li', 'Zhu', ''], ['Xu', 'Yiling', '']]","[{'text': 'latent embeddings', 'label': 'contextual Embedding'}, {'text': 'high-dimensional embeddings', 'label': 'Embedding'}, {'text': '3DGS', 'label': 'contextual Embedding'}, {'text': '3DGS', 'label': 'contextual Embedding'}]",Embedding,high-dimensional embeddings,0.7461893558502197
2503.13954,Ni Tianhao,"Tianhao Ni, Bingjie Li and Zhigang Yao","Enhanced High-Dimensional Data Visualization through Adaptive
  Multi-Scale Manifold Embedding",,,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  To address the dual challenges of the curse of dimensionality and the
difficulty in separating intra-cluster and inter-cluster structures in
high-dimensional manifold embedding, we proposes an Adaptive Multi-Scale
Manifold Embedding (AMSME) algorithm. By introducing ordinal distance to
replace traditional Euclidean distances, we theoretically demonstrate that
ordinal distance overcomes the constraints of the curse of dimensionality in
high-dimensional spaces, effectively distinguishing heterogeneous samples. We
design an adaptive neighborhood adjustment method to construct similarity
graphs that simultaneously balance intra-cluster compactness and inter-cluster
separability. Furthermore, we develop a two-stage embedding framework: the
first stage achieves preliminary cluster separation while preserving
connectivity between structurally similar clusters via the similarity graph,
and the second stage enhances inter-cluster separation through a label-driven
distance reweighting. Experimental results demonstrate that AMSME significantly
preserves intra-cluster topological structures and improves inter-cluster
separation on real-world datasets. Additionally, leveraging its
multi-resolution analysis capability, AMSME discovers novel neuronal subtypes
in the mouse lumbar dorsal root ganglion scRNA-seq dataset, with marker gene
analysis revealing their distinct biological roles.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 06:46:53 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 05:21:06 GMT'}]",2025-03-20,"[['Ni', 'Tianhao', ''], ['Li', 'Bingjie', ''], ['Yao', 'Zhigang', '']]","[{'text': 'high-dimensional manifold embedding', 'label': 'Embedding'}, {'text': 'Adaptive Multi-Scale\nManifold Embedding', 'label': 'Embedding'}, {'text': 'AMSME', 'label': 'Embedding'}, {'text': 'AMSME', 'label': 'Embedding'}, {'text': 'AMSME', 'label': 'Embedding'}]",Embedding,high-dimensional manifold embedding,0.6394561529159546
2503.13957,Liulei Li,"Mu Chen, Liulei Li, Wenguan Wang, Yi Yang",DIFFVSGG: Diffusion-Driven Online Video Scene Graph Generation,"CVPR 2025, Code: https://github.com/kagawa588/DiffVsgg",,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Top-leading solutions for Video Scene Graph Generation (VSGG) typically adopt
an offline pipeline. Though demonstrating promising performance, they remain
unable to handle real-time video streams and consume large GPU memory.
Moreover, these approaches fall short in temporal reasoning, merely aggregating
frame-level predictions over a temporal context. In response, we introduce
DIFFVSGG, an online VSGG solution that frames this task as an iterative scene
graph update problem. Drawing inspiration from Latent Diffusion Models (LDMs)
which generate images via denoising a latent feature embedding, we unify the
decoding of object classification, bounding box regression, and graph
generation three tasks using one shared feature embedding. Then, given an
embedding containing unified features of object pairs, we conduct a step-wise
Denoising on it within LDMs, so as to deliver a clean embedding which clearly
indicates the relationships between objects. This embedding then serves as the
input to task-specific heads for object classification, scene graph generation,
etc. DIFFVSGG further facilitates continuous temporal reasoning, where
predictions for subsequent frames leverage results of past frames as the
conditional inputs of LDMs, to guide the reverse diffusion process for current
frames. Extensive experiments on three setups of Action Genome demonstrate the
superiority of DIFFVSGG.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 06:49:51 GMT'}]",2025-03-19,"[['Chen', 'Mu', ''], ['Li', 'Liulei', ''], ['Wang', 'Wenguan', ''], ['Yang', 'Yi', '']]","[{'text': 'latent feature embedding', 'label': 'Embedding'}, {'text': 'embedding', 'label': 'Embedding'}, {'text': 'embedding', 'label': 'Embedding'}, {'text': 'embedding', 'label': 'Embedding'}, {'text': 'embedding', 'label': 'Embedding'}]",Embedding,embedding,1.0
2503.14002,Damian Boborzi,"Damian Boborzi and Phillip Mueller and Jonas Emrich and Dominik Schmid
  and Sebastian Mueller and Lars Mikelsons","MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific
  Generative Modeling",,,,,cs.CV cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Generative models have recently made remarkable progress in the field of 3D
objects. However, their practical application in fields like engineering
remains limited since they fail to deliver the accuracy, quality, and
controllability needed for domain-specific tasks. Fine-tuning large generative
models is a promising perspective for making these models available in these
fields. Creating high-quality, domain-specific 3D datasets is crucial for
fine-tuning large generative models, yet the data filtering and annotation
process remains a significant bottleneck. We present MeshFleet, a filtered and
annotated 3D vehicle dataset extracted from Objaverse-XL, the most extensive
publicly available collection of 3D objects. Our approach proposes a pipeline
for automated data filtering based on a quality classifier. This classifier is
trained on a manually labeled subset of Objaverse, incorporating DINOv2 and
SigLIP embeddings, refined through caption-based analysis and uncertainty
estimation. We demonstrate the efficacy of our filtering method through a
comparative analysis against caption and image aesthetic score-based techniques
and fine-tuning experiments with SV3D, highlighting the importance of targeted
data selection for domain-specific 3D generative modeling.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 08:09:24 GMT'}]",2025-03-19,"[['Boborzi', 'Damian', ''], ['Mueller', 'Phillip', ''], ['Emrich', 'Jonas', ''], ['Schmid', 'Dominik', ''], ['Mueller', 'Sebastian', ''], ['Mikelsons', 'Lars', '']]","[{'text': 'Objaverse-XL', 'label': 'Large Language Model'}, {'text': 'DINOv2', 'label': 'Embedding'}, {'text': 'SigLIP embeddings', 'label': 'Embedding'}]",Embedding,SigLIP embeddings,0.6315517425537109
2503.14040,Songen Gu,"Binjie Liu, Lina Liu, Sanyi Zhang, Songen Gu, Yihao Zhi, Tianyi Zhu,
  Lei Yang, Long Ye","MAG: Multi-Modal Aligned Autoregressive Co-Speech Gesture Generation
  without Vector Quantization",,,,,cs.GR cs.CV cs.SD,http://creativecommons.org/licenses/by/4.0/,"  This work focuses on full-body co-speech gesture generation. Existing methods
typically employ an autoregressive model accompanied by vector-quantized tokens
for gesture generation, which results in information loss and compromises the
realism of the generated gestures. To address this, inspired by the natural
continuity of real-world human motion, we propose MAG, a novel multi-modal
aligned framework for high-quality and diverse co-speech gesture synthesis
without relying on discrete tokenization. Specifically, (1) we introduce a
motion-text-audio-aligned variational autoencoder (MTA-VAE), which leverages
pre-trained WavCaps' text and audio embeddings to enhance both semantic and
rhythmic alignment with motion, ultimately producing more realistic gestures.
(2) Building on this, we propose a multimodal masked autoregressive model
(MMAG) that enables autoregressive modeling in continuous motion embeddings
through diffusion without vector quantization. To further ensure multi-modal
consistency, MMAG incorporates a hybrid granularity audio-text fusion block,
which serves as conditioning for diffusion process. Extensive experiments on
two benchmark datasets demonstrate that MAG achieves stateof-the-art
performance both quantitatively and qualitatively, producing highly realistic
and diverse co-speech gestures.The code will be released to facilitate future
research.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 09:02:02 GMT'}]",2025-03-19,"[['Liu', 'Binjie', ''], ['Liu', 'Lina', ''], ['Zhang', 'Sanyi', ''], ['Gu', 'Songen', ''], ['Zhi', 'Yihao', ''], ['Zhu', 'Tianyi', ''], ['Yang', 'Lei', ''], ['Ye', 'Long', '']]","[{'text': 'text and audio embeddings', 'label': 'Embedding'}, {'text': 'continuous motion embeddings', 'label': 'Embedding'}, {'text': 'vector quantization', 'label': 'quantisation'}]",Embedding,text and audio embeddings,0.6867680549621582
2503.14138,Siddharth Jaiswal,"Siddharth D Jaiswal, Sagnik Basu, Sandipan Sikdar, Animesh Mukherjee","Exploring Disparity-Accuracy Trade-offs in Face Recognition Systems: The
  Role of Datasets, Architectures, and Loss Functions",This work has been accepted for publication at AAAI ICWSM 2025,,,,cs.CV cs.AI cs.CY,http://creativecommons.org/licenses/by/4.0/,"  Automated Face Recognition Systems (FRSs), developed using deep learning
models, are deployed worldwide for identity verification and facial attribute
analysis. The performance of these models is determined by a complex
interdependence among the model architecture, optimization/loss function and
datasets. Although FRSs have surpassed human-level accuracy, they continue to
be disparate against certain demographics. Due to the ubiquity of applications,
it is extremely important to understand the impact of the three components --
model architecture, loss function and face image dataset on the
accuracy-disparity trade-off to design better, unbiased platforms. In this
work, we perform an in-depth analysis of three FRSs for the task of gender
prediction, with various architectural modifications resulting in ten
deep-learning models coupled with four loss functions and benchmark them on
seven face datasets across 266 evaluation configurations. Our results show that
all three components have an individual as well as a combined impact on both
accuracy and disparity. We identify that datasets have an inherent property
that causes them to perform similarly across models, independent of the choice
of loss functions. Moreover, the choice of dataset determines the model's
perceived bias -- the same model reports bias in opposite directions for three
gender-balanced datasets of ``in-the-wild'' face images of popular individuals.
Studying the facial embeddings shows that the models are unable to generalize a
uniform definition of what constitutes a ``female face'' as opposed to a ``male
face'', due to dataset diversity. We provide recommendations to model
developers on using our study as a blueprint for model development and
subsequent deployment.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:04:57 GMT'}]",2025-03-19,"[['Jaiswal', 'Siddharth D', ''], ['Basu', 'Sagnik', ''], ['Sikdar', 'Sandipan', ''], ['Mukherjee', 'Animesh', '']]","[{'text': 'facial embeddings', 'label': 'Embedding'}]",Embedding,facial embeddings,0.6875802278518677
2503.14185,Wuwei Huang,"Wuwei Huang, Dexin Wang, Deyi Xiong","AdaST: Dynamically Adapting Encoder States in the Decoder for End-to-End
  Speech-to-Text Translation",ACL 2021 Findings,,,,cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In end-to-end speech translation, acoustic representations learned by the
encoder are usually fixed and static, from the perspective of the decoder,
which is not desirable for dealing with the cross-modal and cross-lingual
challenge in speech translation. In this paper, we show the benefits of varying
acoustic states according to decoder hidden states and propose an adaptive
speech-to-text translation model that is able to dynamically adapt acoustic
states in the decoder. We concatenate the acoustic state and target word
embedding sequence and feed the concatenated sequence into subsequent blocks in
the decoder. In order to model the deep interaction between acoustic states and
target hidden states, a speech-text mixed attention sublayer is introduced to
replace the conventional cross-attention network. Experiment results on two
widely-used datasets show that the proposed method significantly outperforms
state-of-the-art neural speech translation models.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:59:27 GMT'}]",2025-03-19,"[['Huang', 'Wuwei', ''], ['Wang', 'Dexin', ''], ['Xiong', 'Deyi', '']]","[{'text': 'target word\nembedding sequence', 'label': 'Embedding'}]",Embedding,"target word
embedding sequence",0.5420953035354614
2503.14213,Ashraf Ghiye,"Ashraf Ghiye, Baptiste Barreau, Laurent Carlier, Michalis Vazirgiannis","Rolling Forward: Enhancing LightGCN with Causal Graph Convolution for
  Credit Bond Recommendation","8 pages, published in the international conference for AI in Finance
  (ACM ICAIF'24)",,10.1145/3677052.3698683,,cs.IR cs.LG q-fin.CP,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Graph Neural Networks have significantly advanced research in recommender
systems over the past few years. These methods typically capture global
interests using aggregated past interactions and rely on static embeddings of
users and items over extended periods of time. While effective in some domains,
these methods fall short in many real-world scenarios, especially in finance,
where user interests and item popularity evolve rapidly over time. To address
these challenges, we introduce a novel extension to Light Graph Convolutional
Network (LightGCN) designed to learn temporal node embeddings that capture
dynamic interests. Our approach employs causal convolution to maintain a
forward-looking model architecture. By preserving the chronological order of
user-item interactions and introducing a dynamic update mechanism for
embeddings through a sliding window, the proposed model generates well-timed
and contextually relevant recommendations. Extensive experiments on a
real-world dataset from BNP Paribas demonstrate that our approach significantly
enhances the performance of LightGCN while maintaining the simplicity and
efficiency of its architecture. Our findings provide new insights into
designing graph-based recommender systems in time-sensitive applications,
particularly for financial product recommendations.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 12:47:01 GMT'}]",2025-03-19,"[['Ghiye', 'Ashraf', ''], ['Barreau', 'Baptiste', ''], ['Carlier', 'Laurent', ''], ['Vazirgiannis', 'Michalis', '']]","[{'text': 'embeddings', 'label': 'contextual Embedding'}, {'text': 'temporal node embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}]",Embedding,embeddings,0.963064432144165
2503.14219,Yusuke Monno,"Yizhou Li, Yusuke Monno, Masatoshi Okutomi, Yuuichi Tanaka, Seiichi
  Kataoka, Teruaki Kosiba","Segmentation-Guided Neural Radiance Fields for Novel Street View
  Synthesis","Presented at VISAPP2025. Project page:
  http://www.ok.sc.e.titech.ac.jp/res/NVS/index.html",,,,cs.CV eess.IV,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Recent advances in Neural Radiance Fields (NeRF) have shown great potential
in 3D reconstruction and novel view synthesis, particularly for indoor and
small-scale scenes. However, extending NeRF to large-scale outdoor environments
presents challenges such as transient objects, sparse cameras and textures, and
varying lighting conditions. In this paper, we propose a segmentation-guided
enhancement to NeRF for outdoor street scenes, focusing on complex urban
environments. Our approach extends ZipNeRF and utilizes Grounded SAM for
segmentation mask generation, enabling effective handling of transient objects,
modeling of the sky, and regularization of the ground. We also introduce
appearance embeddings to adapt to inconsistent lighting across view sequences.
Experimental results demonstrate that our method outperforms the baseline
ZipNeRF, improving novel view synthesis quality with fewer artifacts and
sharper details.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 12:54:36 GMT'}]",2025-03-19,"[['Li', 'Yizhou', ''], ['Monno', 'Yusuke', ''], ['Okutomi', 'Masatoshi', ''], ['Tanaka', 'Yuuichi', ''], ['Kataoka', 'Seiichi', ''], ['Kosiba', 'Teruaki', '']]","[{'text': 'appearance embeddings', 'label': 'Embedding'}]",Embedding,appearance embeddings,0.6963491439819336
2503.14275,Jiang Qin,"Jiang Qin, Senmao Li, Alexandra Gomez-Villa, Shiqi Yang, Yaxing Wang,
  Kai Wang, Joost van de Weijer",Free-Lunch Color-Texture Disentanglement for Stylized Image Generation,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advances in Text-to-Image (T2I) diffusion models have transformed
image generation, enabling significant progress in stylized generation using
only a few style reference images. However, current diffusion-based methods
struggle with fine-grained style customization due to challenges in controlling
multiple style attributes, such as color and texture. This paper introduces the
first tuning-free approach to achieve free-lunch color-texture disentanglement
in stylized T2I generation, addressing the need for independently controlled
style elements for the Disentangled Stylized Image Generation (DisIG) problem.
Our approach leverages the Image-Prompt Additivity property in the CLIP image
embedding space to develop techniques for separating and extracting
Color-Texture Embeddings (CTE) from individual color and texture reference
images. To ensure that the color palette of the generated image aligns closely
with the color reference, we apply a whitening and coloring transformation to
enhance color consistency. Additionally, to prevent texture loss due to the
signal-leak bias inherent in diffusion training, we introduce a noise term that
preserves textural fidelity during the Regularized Whitening and Coloring
Transformation (RegWCT). Through these methods, our Style Attributes
Disentanglement approach (SADis) delivers a more precise and customizable
solution for stylized image generation. Experiments on images from the WikiArt
and StyleDrop datasets demonstrate that, both qualitatively and quantitatively,
SADis surpasses state-of-the-art stylization methods in the DisIG task.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 14:10:43 GMT'}]",2025-03-19,"[['Qin', 'Jiang', ''], ['Li', 'Senmao', ''], ['Gomez-Villa', 'Alexandra', ''], ['Yang', 'Shiqi', ''], ['Wang', 'Yaxing', ''], ['Wang', 'Kai', ''], ['van de Weijer', 'Joost', '']]","[{'text': 'Color-Texture Embeddings', 'label': 'Embedding'}, {'text': 'quantitatively', 'label': 'quantisation'}]",Embedding,Color-Texture Embeddings,0.6252241730690002
2503.14304,Yuheng Li,"Yuheng Li, Mingzhe Hu, Richard L.J. Qiu, Maria Thor, Andre Williams,
  Deborah Marshall and Xiaofeng Yang","RoMedFormer: A Rotary-Embedding Transformer Foundation Model for 3D
  Genito-Pelvic Structure Segmentation in MRI and CT",,,,,eess.IV cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Deep learning-based segmentation of genito-pelvic structures in MRI and CT is
crucial for applications such as radiation therapy, surgical planning, and
disease diagnosis. However, existing segmentation models often struggle with
generalizability across imaging modalities, and anatomical variations. In this
work, we propose RoMedFormer, a rotary-embedding transformer-based foundation
model designed for 3D female genito-pelvic structure segmentation in both MRI
and CT. RoMedFormer leverages self-supervised learning and rotary positional
embeddings to enhance spatial feature representation and capture long-range
dependencies in 3D medical data. We pre-train our model using a diverse dataset
of 3D MRI and CT scans and fine-tune it for downstream segmentation tasks.
Experimental results demonstrate that RoMedFormer achieves superior performance
segmenting genito-pelvic organs. Our findings highlight the potential of
transformer-based architectures in medical image segmentation and pave the way
for more transferable segmentation frameworks.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 14:45:05 GMT'}]",2025-03-19,"[['Li', 'Yuheng', ''], ['Hu', 'Mingzhe', ''], ['Qiu', 'Richard L. J.', ''], ['Thor', 'Maria', ''], ['Williams', 'Andre', ''], ['Marshall', 'Deborah', ''], ['Yang', 'Xiaofeng', '']]","[{'text': 'RoMedFormer', 'label': 'Foundation Model'}, {'text': 'RoMedFormer', 'label': 'Foundation Model'}, {'text': 'self-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'rotary positional\nembeddings', 'label': 'Embedding'}, {'text': 'RoMedFormer', 'label': 'Foundation Model'}]",Embedding,"rotary positional
embeddings",0.602608323097229
2503.14343,Yuanpeng He,"Yali Bi, Enyu Che, Yinan Chen, Yuanpeng He, Jingwei Qu","Multi-Prototype Embedding Refinement for Semi-Supervised Medical Image
  Segmentation",,,,,eess.IV cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Medical image segmentation aims to identify anatomical structures at the
voxel-level. Segmentation accuracy relies on distinguishing voxel differences.
Compared to advancements achieved in studies of the inter-class variance, the
intra-class variance receives less attention. Moreover, traditional linear
classifiers, limited by a single learnable weight per class, struggle to
capture this finer distinction. To address the above challenges, we propose a
Multi-Prototype-based Embedding Refinement method for semi-supervised medical
image segmentation. Specifically, we design a multi-prototype-based
classification strategy, rethinking the segmentation from the perspective of
structural relationships between voxel embeddings. The intra-class variations
are explored by clustering voxels along the distribution of multiple prototypes
in each class. Next, we introduce a consistency constraint to alleviate the
limitation of linear classifiers. This constraint integrates different
classification granularities from a linear classifier and the proposed
prototype-based classifier. In the thorough evaluation on two popular
benchmarks, our method achieves superior performance compared with
state-of-the-art methods. Code is available at
https://github.com/Briley-byl123/MPER.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 15:23:52 GMT'}]",2025-03-19,"[['Bi', 'Yali', ''], ['Che', 'Enyu', ''], ['Chen', 'Yinan', ''], ['He', 'Yuanpeng', ''], ['Qu', 'Jingwei', '']]","[{'text': 'voxel embeddings', 'label': 'Embedding'}]",Embedding,voxel embeddings,0.6481046080589294
2503.14446,Crislaine Kuster,Crislaine Kuster,Codimension one foliations on adjoint varieties,"28 pages, 0 figures, comments are welcome!",,,,math.AG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we classify codimension one foliations on adjoint varieties
with most positive anti-canonical class. We show that on adjoint varieties with
Picard number one, these foliations are always induced by a pencil of
hyperplane sections with respect to their minimal embedding. For adjoint
varieties of Picard number two, there is more than one component of such
foliations, and we describe each of them. As a tool for understanding these
foliations, we introduce the concept of the degree of a foliation with respect
to a family of rational curves, which may be of independent interest.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:24:26 GMT'}]",2025-03-19,"[['Kuster', 'Crislaine', '']]","[{'text': 'minimal embedding', 'label': 'Embedding'}]",Embedding,minimal embedding,0.8026180267333984
2503.14473,Jason Han,"Jason Han, Nicholas S. DiBrita, Younghyun Cho, Hengrui Luo, Tirthak
  Patel","EnQode: Fast Amplitude Embedding for Quantum Machine Learning Using
  Classical Data","EnQode will appear in the Proceedings of the Design Automation
  Conference (DAC), 2025",,,,quant-ph cs.ET cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Amplitude embedding (AE) is essential in quantum machine learning (QML) for
encoding classical data onto quantum circuits. However, conventional AE methods
suffer from deep, variable-length circuits that introduce high output error due
to extensive gate usage and variable error rates across samples, resulting in
noise-driven inconsistencies that degrade model accuracy. We introduce EnQode,
a fast AE technique based on symbolic representation that addresses these
limitations by clustering dataset samples and solving for cluster mean states
through a low-depth, machine-specific ansatz. Optimized to reduce physical
gates and SWAP operations, EnQode ensures all samples face consistent, low
noise levels by standardizing circuit depth and composition. With over 90%
fidelity in data mapping, EnQode enables robust, high-performance QML on noisy
intermediate-scale quantum (NISQ) devices. Our open-source solution provides a
scalable and efficient alternative for integrating classical data with quantum
models.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:48:03 GMT'}]",2025-03-19,"[['Han', 'Jason', ''], ['DiBrita', 'Nicholas S.', ''], ['Cho', 'Younghyun', ''], ['Luo', 'Hengrui', ''], ['Patel', 'Tirthak', '']]","[{'text': 'Amplitude embedding', 'label': 'Embedding'}, {'text': 'quantum machine learning', 'label': 'Few-shot Learning'}, {'text': 'EnQode', 'label': 'Embedding'}, {'text': 'open-source solution', 'label': 'Open-source LLMs'}]",Embedding,Amplitude embedding,0.6402260065078735
2503.14553,Kasra Borazjani,"Kasra Borazjani, Payam Abdisarabshali, Naji Khosravan, Seyyedali
  Hosseinalipour","Redefining non-IID Data in Federated Learning for Computer Vision Tasks:
  Migrating from Labels to Embeddings for Task-Specific Data Distributions","14 pages, 9 figures, 1 table, (implementations are included at our
  GitHub repository: https://github.com/KasraBorazjani/task-perspective-het)",,,,cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Federated Learning (FL) represents a paradigm shift in distributed machine
learning (ML), enabling clients to train models collaboratively while keeping
their raw data private. This paradigm shift from traditional centralized ML
introduces challenges due to the non-iid (non-independent and identically
distributed) nature of data across clients, significantly impacting FL's
performance. Existing literature, predominantly model data heterogeneity by
imposing label distribution skew across clients. In this paper, we show that
label distribution skew fails to fully capture the real-world data
heterogeneity among clients in computer vision tasks beyond classification.
Subsequently, we demonstrate that current approaches overestimate FL's
performance by relying on label/class distribution skew, exposing an overlooked
gap in the literature. By utilizing pre-trained deep neural networks to extract
task-specific data embeddings, we define task-specific data heterogeneity
through the lens of each vision task and introduce a new level of data
heterogeneity called embedding-based data heterogeneity. Our methodology
involves clustering data points based on embeddings and distributing them among
clients using the Dirichlet distribution. Through extensive experiments, we
evaluate the performance of different FL methods under our revamped notion of
data heterogeneity, introducing new benchmark performance measures to the
literature. We further unveil a series of open research directions that can be
pursued.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 22:16:53 GMT'}]",2025-03-20,"[['Borazjani', 'Kasra', ''], ['Abdisarabshali', 'Payam', ''], ['Khosravan', 'Naji', ''], ['Hosseinalipour', 'Seyyedali', '']]","[{'text': 'Federated Learning', 'label': 'Few-shot Learning'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}]",Embedding,embeddings,0.963064432144165
2503.14667,Pablo Ochoa Mr,Pablo Ochoa,"Elliptic systems in Orlicz-Sobolev spaces with critical sources in
  bounded domains",,,,,math.AP,http://creativecommons.org/publicdomain/zero/1.0/,"  In this paper, we show the existence of non-trivial solutions to very general
elliptic systems with critical non-linearities in the sense of embeddings in
Orlicz-Sobolev spaces. This allows to consider non-linearities which do not
have polynomial growth. To achieve the existence, we combine a Mountain Pass
Theorem without the Palais-Smale condition with the second Concentration
Compactness Principle of Lions in Orlicz-Sobolev spaces.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 19:14:51 GMT'}]",2025-03-20,"[['Ochoa', 'Pablo', '']]","[{'text': 'embeddings', 'label': 'Embedding'}]",Embedding,embeddings,0.963064432144165
2503.14736,Yilan Dong,"Yilan Dong, Haohe Liu, Qing Wang, Jiahao Yang, Wenqing Wang, Gregory
  Slabaugh, Shanxin Yuan","HandSplat: Embedding-Driven Gaussian Splatting for High-Fidelity Hand
  Rendering",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing 3D Gaussian Splatting (3DGS) methods for hand rendering rely on
rigid skeletal motion with an oversimplified non-rigid motion model, which
fails to capture fine geometric and appearance details. Additionally, they
perform densification based solely on per-point gradients and process poses
independently, ignoring spatial and temporal correlations. These limitations
lead to geometric detail loss, temporal instability, and inefficient point
distribution. To address these issues, we propose HandSplat, a novel Gaussian
Splatting-based framework that enhances both fidelity and stability for hand
rendering. To improve fidelity, we extend standard 3DGS attributes with
implicit geometry and appearance embeddings for finer non-rigid motion modeling
while preserving the static hand characteristic modeled by original 3DGS
attributes. Additionally, we introduce a local gradient-aware densification
strategy that dynamically refines Gaussian density in high-variation regions.
To improve stability, we incorporate pose-conditioned attribute regularization
to encourage attribute consistency across similar poses, mitigating temporal
artifacts. Extensive experiments on InterHand2.6M demonstrate that HandSplat
surpasses existing methods in fidelity and stability while achieving real-time
performance. We will release the code and pre-trained models upon acceptance.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 21:09:04 GMT'}]",2025-03-20,"[['Dong', 'Yilan', ''], ['Liu', 'Haohe', ''], ['Wang', 'Qing', ''], ['Yang', 'Jiahao', ''], ['Wang', 'Wenqing', ''], ['Slabaugh', 'Gregory', ''], ['Yuan', 'Shanxin', '']]","[{'text': 'implicit geometry and appearance embeddings', 'label': 'Embedding'}]",Embedding,implicit geometry and appearance embeddings,0.5625889897346497
2503.14769,Jonathan Beardsley,Jonathan Beardsley,Dynkin Systems and the One-Point Geometry,"19 pages, opacity in tikz diagrams does not compile correctly on
  arXiv",,,,math.CT math.AT math.CO math.PR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this note I demonstrate that the collection of Dynkin systems on finite
sets assembles into a Connes-Consani $\mathbb{F}_1$-module, with the collection
of partitions of finite sets as a sub-module. The underlying simplicial set of
this $\mathbb{F}_1$-module is shown to be isomorphic to the delooping of the
Krasner hyperfield $\mathbb{K}$, where $1+1=\{0,1\}$. The face and degeneracy
maps of the underlying simplicial set of the $\mathbb{F}_1$-module of
partitions correspond to merging partition blocks and introducing singleton
blocks, respectively. I also show that the $\mathbb{F}_1$-module of partitions
cannot correspond to a set with a binary operation (even partially defined or
multivalued) under the ``Eilenberg-MacLane'' embedding. These results imply
that the $n$-fold sum of the Dynkin $\mathbb{F}_1$-module with itself is
isomorphic to the $\mathbb{F}_1$-module of the discrete projective geometry on
$n$ points.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 22:43:06 GMT'}]",2025-03-20,"[['Beardsley', 'Jonathan', '']]","[{'text': 'Eilenberg-MacLane', 'label': 'Embedding'}, {'text': 'embedding', 'label': 'Embedding'}]",Embedding,embedding,1.0
2503.14824,Zikun Zhou,"Zikun Zhou, Yushuai Sun, Wenjie Pei, Xin Li, Yaowei Wang","Prototype Perturbation for Relaxing Alignment Constraints in
  Backward-Compatible Learning",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The traditional paradigm to update retrieval models requires re-computing the
embeddings of the gallery data, a time-consuming and computationally intensive
process known as backfilling. To circumvent backfilling, Backward-Compatible
Learning (BCL) has been widely explored, which aims to train a new model
compatible with the old one. Many previous works focus on effectively aligning
the embeddings of the new model with those of the old one to enhance the
backward-compatibility. Nevertheless, such strong alignment constraints would
compromise the discriminative ability of the new model, particularly when
different classes are closely clustered and hard to distinguish in the old
feature space. To address this issue, we propose to relax the constraints by
introducing perturbations to the old feature prototypes. This allows us to
align the new feature space with a pseudo-old feature space defined by these
perturbed prototypes, thereby preserving the discriminative ability of the new
model in backward-compatible learning. We have developed two approaches for
calculating the perturbations: Neighbor-Driven Prototype Perturbation (NDPP)
and Optimization-Driven Prototype Perturbation (ODPP). Particularly, they take
into account the feature distributions of not only the old but also the new
models to obtain proper perturbations along with new model updating. Extensive
experiments on the landmark and commodity datasets demonstrate that our
approaches perform favorably against state-of-the-art BCL algorithms.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 01:45:48 GMT'}]",2025-03-20,"[['Zhou', 'Zikun', ''], ['Sun', 'Yushuai', ''], ['Pei', 'Wenjie', ''], ['Li', 'Xin', ''], ['Wang', 'Yaowei', '']]","[{'text': 'embeddings', 'label': 'Embedding'}, {'text': 'Backward-Compatible\nLearning', 'label': 'Few-shot Learning'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'backward-compatible learning', 'label': 'Few-shot Learning'}]",Embedding,embeddings,0.963064432144165
2503.14868,Hoigi Seo,"Hoigi Seo, Wongi Jeong, Kyungryeol Lee, Se Young Chun","Efficient Personalization of Quantized Diffusion Model without
  Backpropagation",,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Diffusion models have shown remarkable performance in image synthesis, but
they demand extensive computational and memory resources for training,
fine-tuning and inference. Although advanced quantization techniques have
successfully minimized memory usage for inference, training and fine-tuning
these quantized models still require large memory possibly due to
dequantization for accurate computation of gradients and/or backpropagation for
gradient-based algorithms. However, memory-efficient fine-tuning is
particularly desirable for applications such as personalization that often must
be run on edge devices like mobile phones with private data. In this work, we
address this challenge by quantizing a diffusion model with personalization via
Textual Inversion and by leveraging a zeroth-order optimization on
personalization tokens without dequantization so that it does not require
gradient and activation storage for backpropagation that consumes considerable
memory. Since a gradient estimation using zeroth-order optimization is quite
noisy for a single or a few images in personalization, we propose to denoise
the estimated gradient by projecting it onto a subspace that is constructed
with the past history of the tokens, dubbed Subspace Gradient. In addition, we
investigated the influence of text embedding in image generation, leading to
our proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for
sampling with effective diffusion timesteps. Our method achieves comparable
performance to prior methods in image and text alignment scores for
personalizing Stable Diffusion with only forward passes while reducing training
memory demand up to $8.2\times$.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 03:45:37 GMT'}]",2025-03-20,"[['Seo', 'Hoigi', ''], ['Jeong', 'Wongi', ''], ['Lee', 'Kyungryeol', ''], ['Chun', 'Se Young', '']]","[{'text': 'dequantization', 'label': 'quantisation'}, {'text': 'memory-efficient fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Textual Inversion', 'label': 'contextual Embedding'}, {'text': 'dequantization', 'label': 'quantisation'}, {'text': 'zeroth-order optimization', 'label': 'Zero-shot Learning'}, {'text': 'text embedding', 'label': 'Embedding'}]",Embedding,text embedding,0.8247289657592773
2503.14925,Haoyu Lei,"Haoyu Lei, Shizhan Gong, Qi Dou, Farzan Farnia","pFedFair: Towards Optimal Group Fairness-Accuracy Trade-off in
  Heterogeneous Federated Learning",,,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Federated learning (FL) algorithms commonly aim to maximize clients' accuracy
by training a model on their collective data. However, in several FL
applications, the model's decisions should meet a group fairness constraint to
be independent of sensitive attributes such as gender or race. While such group
fairness constraints can be incorporated into the objective function of the FL
optimization problem, in this work, we show that such an approach would lead to
suboptimal classification accuracy in an FL setting with heterogeneous client
distributions. To achieve an optimal accuracy-group fairness trade-off, we
propose the Personalized Federated Learning for Client-Level Group Fairness
(pFedFair) framework, where clients locally impose their fairness constraints
over the distributed training process. Leveraging the image embedding models,
we extend the application of pFedFair to computer vision settings, where we
numerically show that pFedFair achieves an optimal group fairness-accuracy
trade-off in heterogeneous FL settings. We present the results of several
numerical experiments on benchmark and synthetic datasets, which highlight the
suboptimality of non-personalized FL algorithms and the improvements made by
the pFedFair method.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 06:15:31 GMT'}]",2025-03-20,"[['Lei', 'Haoyu', ''], ['Gong', 'Shizhan', ''], ['Dou', 'Qi', ''], ['Farnia', 'Farzan', '']]","[{'text': 'model', 'label': 'Neural Language Model'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'image embedding models', 'label': 'Embedding'}]",Embedding,image embedding models,0.7555274367332458
2503.14979,Lichao Mou,"Yaxiong Chen, Junjian Hu, Chunlei Li, Zixuan Zheng, Jingliang Hu,
  Yilei Shi, Shengwu Xiong, Xiao Xiang Zhu, Lichao Mou","One-Shot Medical Video Object Segmentation via Temporal Contrastive
  Memory Networks",MICCAI 2024 Workshop,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Video object segmentation is crucial for the efficient analysis of complex
medical video data, yet it faces significant challenges in data availability
and annotation. We introduce the task of one-shot medical video object
segmentation, which requires separating foreground and background pixels
throughout a video given only the mask annotation of the first frame. To
address this problem, we propose a temporal contrastive memory network
comprising image and mask encoders to learn feature representations, a temporal
contrastive memory bank that aligns embeddings from adjacent frames while
pushing apart distant ones to explicitly model inter-frame relationships and
stores these features, and a decoder that fuses encoded image features and
memory readouts for segmentation. We also collect a diverse, multi-source
medical video dataset spanning various modalities and anatomies to benchmark
this task. Extensive experiments demonstrate state-of-the-art performance in
segmenting both seen and unseen structures from a single exemplar, showing
ability to generalize from scarce labels. This highlights the potential to
alleviate annotation burdens for medical video analysis. Code is available at
https://github.com/MedAITech/TCMN.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 08:17:48 GMT'}]",2025-03-20,"[['Chen', 'Yaxiong', ''], ['Hu', 'Junjian', ''], ['Li', 'Chunlei', ''], ['Zheng', 'Zixuan', ''], ['Hu', 'Jingliang', ''], ['Shi', 'Yilei', ''], ['Xiong', 'Shengwu', ''], ['Zhu', 'Xiao Xiang', ''], ['Mou', 'Lichao', '']]","[{'text': 'feature representations', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}]",Embedding,embeddings,0.963064432144165
2503.15009,Etienne Menager,"Etienne M\'enager (WILLOW, DI-ENS), Tanguy Navez (DEFROST), Paul
  Chaillou (DEFROST, CRIStAL), Olivier Goury (INSERM, DEFROST), Alexandre
  Kruszewski (DEFROST, CRIStAL), Christian Duriez (DEFROST, CRIStAL)","Modeling, Embedded Control and Design of Soft Robots using a Learned
  Condensed FEM Model","IEEE Transactions on Robotics, In press",,,,cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Finite Element Method (FEM) is a powerful modeling tool for predicting
soft robots' behavior, but its computation time can limit practical
applications. In this paper, a learning-based approach based on condensation of
the FEM model is detailed. The proposed method handles several kinds of
actuators and contacts with the environment. We demonstrate that this compact
model can be learned as a unified model across several designs and remains very
efficient in terms of modeling since we can deduce the direct and inverse
kinematics of the robot. Building upon the intuition introduced in [11], the
learned model is presented as a general framework for modeling, controlling,
and designing soft manipulators. First, the method's adaptability and
versatility are illustrated through optimization based control problems
involving positioning and manipulation tasks with mechanical contact-based
coupling. Secondly, the low memory consumption and the high prediction speed of
the learned condensed model are leveraged for real-time embedding control
without relying on costly online FEM simulation. Finally, the ability of the
learned condensed FEM model to capture soft robot design variations and its
differentiability are leveraged in calibration and design optimization
applications.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 08:59:44 GMT'}]",2025-03-20,"[['Ménager', 'Etienne', '', 'WILLOW, DI-ENS'], ['Navez', 'Tanguy', '', 'DEFROST'], ['Chaillou', 'Paul', '', 'DEFROST, CRIStAL'], ['Goury', 'Olivier', '', 'INSERM, DEFROST'], ['Kruszewski', 'Alexandre', '', 'DEFROST, CRIStAL'], ['Duriez', 'Christian', '', 'DEFROST, CRIStAL']]","[{'text': 'real-time embedding', 'label': 'Embedding'}]",Embedding,real-time embedding,0.7154863476753235
2503.15029,Jianbo Zhao,"Jianbo Zhao, Taiyu Ban, Zhihao Liu, Hangning Zhou, Xiyang Wang, Qibin
  Zhou, Hailong Qin, Mu Yang, Lei Liu, Bin Li","DRoPE: Directional Rotary Position Embedding for Efficient Agent
  Interaction Modeling",,,,,cs.RO cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Accurate and efficient modeling of agent interactions is essential for
trajectory generation, the core of autonomous driving systems. Existing
methods, scene-centric, agent-centric, and query-centric frameworks, each
present distinct advantages and drawbacks, creating an impossible triangle
among accuracy, computational time, and memory efficiency. To break this
limitation, we propose Directional Rotary Position Embedding (DRoPE), a novel
adaptation of Rotary Position Embedding (RoPE), originally developed in natural
language processing. Unlike traditional relative position embedding (RPE),
which introduces significant space complexity, RoPE efficiently encodes
relative positions without explicitly increasing complexity but faces inherent
limitations in handling angular information due to periodicity. DRoPE overcomes
this limitation by introducing a uniform identity scalar into RoPE's 2D rotary
transformation, aligning rotation angles with realistic agent headings to
naturally encode relative angular information. We theoretically analyze DRoPE's
correctness and efficiency, demonstrating its capability to simultaneously
optimize trajectory generation accuracy, time complexity, and space complexity.
Empirical evaluations compared with various state-of-the-art trajectory
generation models, confirm DRoPE's good performance and significantly reduced
space complexity, indicating both theoretical soundness and practical
effectiveness. The video documentation is available at
https://drope-traj.github.io/.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 09:23:09 GMT'}]",2025-03-20,"[['Zhao', 'Jianbo', ''], ['Ban', 'Taiyu', ''], ['Liu', 'Zhihao', ''], ['Zhou', 'Hangning', ''], ['Wang', 'Xiyang', ''], ['Zhou', 'Qibin', ''], ['Qin', 'Hailong', ''], ['Yang', 'Mu', ''], ['Liu', 'Lei', ''], ['Li', 'Bin', '']]","[{'text': 'Directional Rotary Position Embedding', 'label': 'Embedding'}, {'text': 'DRoPE', 'label': 'Embedding'}, {'text': 'Rotary Position Embedding', 'label': 'Embedding'}, {'text': 'RoPE', 'label': 'Embedding'}, {'text': 'relative position embedding', 'label': 'Embedding'}, {'text': 'RoPE', 'label': 'Embedding'}, {'text': 'DRoPE', 'label': 'Embedding'}, {'text': 'RoPE', 'label': 'Embedding'}, {'text': 'DRoPE', 'label': 'Embedding'}, {'text': 'DRoPE', 'label': 'Embedding'}]",Embedding,relative position embedding,0.7441638708114624
2503.15057,Jaihyun Park,"Jaihyun Park, Ryan Cordell","A Data-driven Investigation of Euphemistic Language: Comparing the usage
  of ""slave"" and ""servant"" in 19th century US newspapers","The 5th International Conference on Natural Language Processing for
  Digital Humanities (NLP4DH)",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  This study investigates the usage of ""slave"" and ""servant"" in the 19th
century US newspapers using computational methods. While both terms were used
to refer to enslaved African Americans, they were used in distinct ways. In the
Chronicling America corpus, we included possible OCR errors by using FastText
embedding and excluded text reprints to consider text reprint culture in the
19th century. Word2vec embedding was used to find semantically close words to
""slave"" and ""servant"" and log-odds ratio was calculated to identify
over-represented discourse words in the Southern and Northern newspapers. We
found that ""slave"" is associated with socio-economic, legal, and administrative
words, however, ""servant"" is linked to religious words in the Northern
newspapers while Southern newspapers associated ""servant"" with domestic and
familial words. We further found that slave discourse words in Southern
newspapers are more prevalent in Northern newspapers while servant discourse
words from each side are prevalent in their own region. This study contributes
to the understanding of how newspapers created different discourses around
enslaved African Americans in the 19th century US.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 09:49:22 GMT'}]",2025-03-20,"[['Park', 'Jaihyun', ''], ['Cordell', 'Ryan', '']]","[{'text': 'FastText\nembedding', 'label': 'Embedding'}, {'text': 'Word2vec embedding', 'label': 'Embedding'}]",Embedding,"FastText
embedding",0.6925028562545776
2503.15137,Antonio Alarc\'on,Antonio Alarcon and Jorge Hidalgo,Holomorphic null curves in the special linear group,,,,,math.DG math.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper we develop the theory of approximation for holomorphic null
curves in the special linear group ${\rm SL}_2(\mathbb{C})$. In particular, we
establish Runge, Mergelyan, Mittag-Leffler, and Carleman type theorems for the
family of holomorphic null immersions $M\to{\rm SL}_2(\mathbb{C})$ from any
open Riemann surface $M$. Our results include jet interpolation of Weierstrass
type and approximation by embeddings, as well as global conditions on the
approximating curves. As application, we show that every open Riemann surface
admits a proper holomorphic null embedding into ${\rm SL}_2(\mathbb{C})$, and
hence also a proper conformal immersion of constant mean curvature $1$ into
hyperbolic 3-space. This settles a problem posed by Alarcon and Forstneric in
2015.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:55:40 GMT'}]",2025-03-20,"[['Alarcon', 'Antonio', ''], ['Hidalgo', 'Jorge', '']]","[{'text': 'embeddings', 'label': 'Embedding'}]",Embedding,embeddings,0.963064432144165
2503.15267,Marco Podda,"Alessio Micheli, Alejandro Moreo, Marco Podda, Fabrizio Sebastiani,
  William Simoni, Domenico Tortorella",Learning to quantify graph nodes,,,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Network Quantification is the problem of estimating the class proportions in
unlabeled subsets of graph nodes. When prior probability shift is at play, this
task cannot be effectively addressed by first classifying the nodes and then
counting the class predictions. In addition, unlike non-relational
quantification on i.i.d. datapoints, Network Quantification demands enhanced
flexibility to capture a broad range of connectivity patterns, resilience to
the challenge of heterophily, and efficiency to scale to larger networks. To
meet these stringent requirements we introduce XNQ, a novel method that
synergizes the flexibility and efficiency of the unsupervised node embeddings
computed by randomized recursive Graph Neural Networks, with an
Expectation-Maximization algorithm that provides a robust quantification-aware
adjustment to the output probabilities of a calibrated node classifier. We
validate the design choices underpinning our method through comprehensive
ablation experiments. In an extensive evaluation, we find that our approach
consistently and significantly improves on the best Network Quantification
methods to date, thereby setting the new state of the art for this challenging
task. Simultaneously, it provides a training speed-up of up to 10x-100x over
other graph learning based methods.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 14:43:12 GMT'}]",2025-03-20,"[['Micheli', 'Alessio', ''], ['Moreo', 'Alejandro', ''], ['Podda', 'Marco', ''], ['Sebastiani', 'Fabrizio', ''], ['Simoni', 'William', ''], ['Tortorella', 'Domenico', '']]","[{'text': 'unsupervised node embeddings', 'label': 'Embedding'}]",Embedding,unsupervised node embeddings,0.6525121927261353
2503.15312,Malgorzata Siudek Malgorzata Siudek,"Euclid Collaboration: M. Siudek, M. Huertas-Company, M. Smith, G.
  Martinez-Solaeche, F. Lanusse, S. Ho, E. Angeloudi, P. A. C. Cunha, H.
  Dom\'inguez S\'anchez, M. Dunn, Y. Fu, P. Iglesias-Navarro, J. Junais, J. H.
  Knapen, B. Laloux, M. Mezcua, W. Roster, G. Stevens, J. Vega-Ferrero, N.
  Aghanim, B. Altieri, A. Amara, S. Andreon, N. Auricchio, H. Aussel, C.
  Baccigalupi, M. Baldi, S. Bardelli, P. Battaglia, A. Biviano, A. Bonchi, E.
  Branchini, M. Brescia, J. Brinchmann, S. Camera, G. Ca\~nas-Herrera, V.
  Capobianco, C. Carbone, J. Carretero, S. Casas, F. J. Castander, M.
  Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C.
  Colodro-Conde, G. Congedo, C. J. Conselice, L. Conversi, Y. Copin, F.
  Courbin, H. M. Courtois, M. Cropper, A. Da Silva, H. Degaudenzi, G. De Lucia,
  A. M. Di Giorgio, J. Dinis, C. Dolding, H. Dole, F. Dubath, C. A. J. Duncan,
  X. Dupac, S. Dusini, S. Escoffier, M. Farina, R. Farinelli, F. Faustini, S.
  Ferriol, F. Finelli, S. Fotopoulou, M. Frailis, E. Franceschi, S. Galeotta,
  K. George, B. Gillis, C. Giocoli, J. Gracia-Carpio, B. R. Granett, A.
  Grazian, F. Grupp, S. Gwyn, S. V. H. Haugan, W. Holmes, I. M. Hook, F.
  Hormuth, A. Hornstrup, K. Jahnke, M. Jhabvala, E. Keih\""anen, S. Kermiche, A.
  Kiessling, B. Kubik, M. K\""ummel, M. Kunz, H. Kurki-Suonio, Q. Le Boulc'h, A.
  M. C. Le Brun, D. Le Mignant, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro,
  G. Mainetti, D. Maino, E. Maiorano, O. Mansutti, S. Marcin, O. Marggraf, M.
  Martinelli, N. Martinet, F. Marulli, R. Massey, S. Maurogordato, H. J.
  McCracken, E. Medinaceli, S. Mei, M. Melchior, Y. Mellier, M. Meneghetti, E.
  Merlin, G. Meylan, A. Mora, M. Moresco, L. Moscardini, R. Nakajima, C.
  Neissner, S.-M. Niemi, J. W. Nightingale, C. Padilla, S. Paltani, F. Pasian,
  K. Pedersen, W. J. Percival, V. Pettorino, S. Pires, G. Polenta, M. Poncet,
  L. A. Popa, L. Pozzetti, F. Raison, A. Renzi, J. Rhodes, G. Riccio, E.
  Romelli, M. Roncarelli, R. Saglia, Z. Sakr, A. G. S\'anchez, D. Sapone, B.
  Sartoris, J. A. Schewtschenko, P. Schneider, T. Schrabback, M. Scodeggio, A.
  Secroun, G. Seidel, M. Seiffert, S. Serrano, P. Simon, C. Sirignano, G.
  Sirri, L. Stanco, J. Steinwagner, P. Tallada-Cresp\'i, A. N. Taylor, I.
  Tereno, S. Toft, R. Toledo-Moreo, F. Torradeflot, I. Tutusaus, L. Valenziano,
  J. Valiviita, T. Vassallo, G. Verdoes Kleijn, A. Veropalumbo, Y. Wang, J.
  Weller, A. Zacchei, G. Zamorani, F. M. Zerbi, I. A. Zinchenko, E. Zucca, V.
  Allevato, M. Ballardini, M. Bolzonella, E. Bozzo, C. Burigana, R. Cabanac, A.
  Cappi, D. Di Ferdinando, J. A. Escartin Vigo, L. Gabarra, J.
  Mart\'in-Fleitas, S. Matthew, N. Mauri, R. B. Metcalf, A. Pezzotta, M.
  P\""ontinen, C. Porciani, I. Risso, V. Scottez, M. Sereno, M. Tenti, M. Viel,
  M. Wiesmann, Y. Akrami, I. T. Andika, S. Anselmi, M. Archidiacono, F.
  Atrio-Barandela, C. Benoist, K. Benson, D. Bertacca, M. Bethermin, L.
  Bisigello, A. Blanchard, L. Blot, M. L. Brown, S. Bruton, A. Calabro, B.
  Camacho Quevedo, F. Caro, C. S. Carvalho, T. Castro, Y. Charles, F. Cogato,
  A. R. Cooray, O. Cucciati, S. Davini, F. De Paolis, G. Desprez, A.
  D\'iaz-S\'anchez, J. J. Diaz, S. Di Domizio, J. M. Diego, P.-A. Duc, A. Enia,
  Y. Fang, A. G. Ferrari, P. G. Ferreira, A. Finoguenov, A. Fontana, A. Franco,
  K. Ganga, J. Garc\'ia-Bellido, T. Gasparetto, V. Gautard, E. Gaztanaga, F.
  Giacomini, F. Gianotti, G. Gozaliasl, M. Guidi, C. M. Gutierrez, A. Hall, W.
  G. Hartley, S. Hemmati, C. Hern\'andez-Monteagudo, H. Hildebrandt, J. Hjorth,
  J. J. E. Kajava, Y. Kang, V. Kansal, D. Karagiannis, K. Kiiveri, C. C.
  Kirkpatrick, S. Kruk, J. Le Graet, L. Legrand, M. Lembo, F. Lepori, G. Leroy,
  G. F. Lesci, J. Lesgourgues, L. Leuzzi, T. I. Liaudat, A. Loureiro, J.
  Macias-Perez, G. Maggio, M. Magliocchetti, E. A. Magnier, F. Mannucci, R.
  Maoli, C. J. A. P. Martins, L. Maurin, M. Miluzio, P. Monaco, C. Moretti, G.
  Morgante, C. Murray, K. Naidoo, A. Navarro-Alsina, S. Nesseris, F.
  Passalacqua, K. Paterson, L. Patrizii, A. Pisani, D. Potter, S. Quai, M.
  Radovich, S. Sacquegna, M. Sahl\'en, D. B. Sanders, E. Sarpa, A. Schneider,
  D. Sciotti, D. Scognamiglio, E. Sellentin, L. C. Smith, K. Tanidis, G.
  Testera, R. Teyssier, S. Tosi, A. Troja, M. Tucci, C. Valieri, A. Venhola, D.
  Vergani, G. Verza, P. Vielzeuf, N. A. Walton, J. G. Sorce","Euclid Quick Data Release (Q1) Exploring galaxy properties with a
  multi-modal foundation model","Paper submitted as part of the A&A Special Issue `Euclid Quick Data
  Release (Q1)', 31 pages, 17 figures",,,,astro-ph.GA,http://creativecommons.org/licenses/by/4.0/,"  Modern astronomical surveys, such as the Euclid mission, produce
high-dimensional, multi-modal data sets that include imaging and spectroscopic
information for millions of galaxies. These data serve as an ideal benchmark
for large, pre-trained multi-modal models, which can leverage vast amounts of
unlabelled data. In this work, we present the first exploration of Euclid data
with AstroPT, an autoregressive multi-modal foundation model trained on
approximately 300 000 optical and infrared Euclid images and spectral energy
distributions (SEDs) from the first Euclid Quick Data Release. We compare
self-supervised pre-training with baseline fully supervised training across
several tasks: galaxy morphology classification; redshift estimation;
similarity searches; and outlier detection. Our results show that: (a) AstroPT
embeddings are highly informative, correlating with morphology and effectively
isolating outliers; (b) including infrared data helps to isolate stars, but
degrades the identification of edge-on galaxies, which are better captured by
optical images; (c) simple fine-tuning of these embeddings for photometric
redshift and stellar mass estimation outperforms a fully supervised approach,
even when using only 1% of the training labels; and (d) incorporating SED data
into AstroPT via a straightforward multi-modal token-chaining method improves
photo-z predictions, and allow us to identify potentially more interesting
anomalies (such as ringed or interacting galaxies) compared to a model
pre-trained solely on imaging data.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:27:07 GMT'}]",2025-03-20,"[['Euclid Collaboration', '', ''], ['Siudek', 'M.', ''], ['Huertas-Company', 'M.', ''], ['Smith', 'M.', ''], ['Martinez-Solaeche', 'G.', ''], ['Lanusse', 'F.', ''], ['Ho', 'S.', ''], ['Angeloudi', 'E.', ''], ['Cunha', 'P. A. C.', ''], ['Sánchez', 'H. Domínguez', ''], ['Dunn', 'M.', ''], ['Fu', 'Y.', ''], ['Iglesias-Navarro', 'P.', ''], ['Junais', 'J.', ''], ['Knapen', 'J. H.', ''], ['Laloux', 'B.', ''], ['Mezcua', 'M.', ''], ['Roster', 'W.', ''], ['Stevens', 'G.', ''], ['Vega-Ferrero', 'J.', ''], ['Aghanim', 'N.', ''], ['Altieri', 'B.', ''], ['Amara', 'A.', ''], ['Andreon', 'S.', ''], ['Auricchio', 'N.', ''], ['Aussel', 'H.', ''], ['Baccigalupi', 'C.', ''], ['Baldi', 'M.', ''], ['Bardelli', 'S.', ''], ['Battaglia', 'P.', ''], ['Biviano', 'A.', ''], ['Bonchi', 'A.', ''], ['Branchini', 'E.', ''], ['Brescia', 'M.', ''], ['Brinchmann', 'J.', ''], ['Camera', 'S.', ''], ['Cañas-Herrera', 'G.', ''], ['Capobianco', 'V.', ''], ['Carbone', 'C.', ''], ['Carretero', 'J.', ''], ['Casas', 'S.', ''], ['Castander', 'F. J.', ''], ['Castellano', 'M.', ''], ['Castignani', 'G.', ''], ['Cavuoti', 'S.', ''], ['Chambers', 'K. C.', ''], ['Cimatti', 'A.', ''], ['Colodro-Conde', 'C.', ''], ['Congedo', 'G.', ''], ['Conselice', 'C. J.', ''], ['Conversi', 'L.', ''], ['Copin', 'Y.', ''], ['Courbin', 'F.', ''], ['Courtois', 'H. M.', ''], ['Cropper', 'M.', ''], ['Da Silva', 'A.', ''], ['Degaudenzi', 'H.', ''], ['De Lucia', 'G.', ''], ['Di Giorgio', 'A. M.', ''], ['Dinis', 'J.', ''], ['Dolding', 'C.', ''], ['Dole', 'H.', ''], ['Dubath', 'F.', ''], ['Duncan', 'C. A. J.', ''], ['Dupac', 'X.', ''], ['Dusini', 'S.', ''], ['Escoffier', 'S.', ''], ['Farina', 'M.', ''], ['Farinelli', 'R.', ''], ['Faustini', 'F.', ''], ['Ferriol', 'S.', ''], ['Finelli', 'F.', ''], ['Fotopoulou', 'S.', ''], ['Frailis', 'M.', ''], ['Franceschi', 'E.', ''], ['Galeotta', 'S.', ''], ['George', 'K.', ''], ['Gillis', 'B.', ''], ['Giocoli', 'C.', ''], ['Gracia-Carpio', 'J.', ''], ['Granett', 'B. R.', ''], ['Grazian', 'A.', ''], ['Grupp', 'F.', ''], ['Gwyn', 'S.', ''], ['Haugan', 'S. V. H.', ''], ['Holmes', 'W.', ''], ['Hook', 'I. M.', ''], ['Hormuth', 'F.', ''], ['Hornstrup', 'A.', ''], ['Jahnke', 'K.', ''], ['Jhabvala', 'M.', ''], ['Keihänen', 'E.', ''], ['Kermiche', 'S.', ''], ['Kiessling', 'A.', ''], ['Kubik', 'B.', ''], ['Kümmel', 'M.', ''], ['Kunz', 'M.', ''], ['Kurki-Suonio', 'H.', ''], [""Boulc'h"", 'Q. Le', ''], ['Brun', 'A. M. C. Le', ''], ['Mignant', 'D. Le', ''], ['Ligori', 'S.', ''], ['Lilje', 'P. B.', ''], ['Lindholm', 'V.', ''], ['Lloro', 'I.', ''], ['Mainetti', 'G.', ''], ['Maino', 'D.', ''], ['Maiorano', 'E.', ''], ['Mansutti', 'O.', ''], ['Marcin', 'S.', ''], ['Marggraf', 'O.', ''], ['Martinelli', 'M.', ''], ['Martinet', 'N.', ''], ['Marulli', 'F.', ''], ['Massey', 'R.', ''], ['Maurogordato', 'S.', ''], ['McCracken', 'H. J.', ''], ['Medinaceli', 'E.', ''], ['Mei', 'S.', ''], ['Melchior', 'M.', ''], ['Mellier', 'Y.', ''], ['Meneghetti', 'M.', ''], ['Merlin', 'E.', ''], ['Meylan', 'G.', ''], ['Mora', 'A.', ''], ['Moresco', 'M.', ''], ['Moscardini', 'L.', ''], ['Nakajima', 'R.', ''], ['Neissner', 'C.', ''], ['Niemi', 'S. -M.', ''], ['Nightingale', 'J. W.', ''], ['Padilla', 'C.', ''], ['Paltani', 'S.', ''], ['Pasian', 'F.', ''], ['Pedersen', 'K.', ''], ['Percival', 'W. J.', ''], ['Pettorino', 'V.', ''], ['Pires', 'S.', ''], ['Polenta', 'G.', ''], ['Poncet', 'M.', ''], ['Popa', 'L. A.', ''], ['Pozzetti', 'L.', ''], ['Raison', 'F.', ''], ['Renzi', 'A.', ''], ['Rhodes', 'J.', ''], ['Riccio', 'G.', ''], ['Romelli', 'E.', ''], ['Roncarelli', 'M.', ''], ['Saglia', 'R.', ''], ['Sakr', 'Z.', ''], ['Sánchez', 'A. G.', ''], ['Sapone', 'D.', ''], ['Sartoris', 'B.', ''], ['Schewtschenko', 'J. A.', ''], ['Schneider', 'P.', ''], ['Schrabback', 'T.', ''], ['Scodeggio', 'M.', ''], ['Secroun', 'A.', ''], ['Seidel', 'G.', ''], ['Seiffert', 'M.', ''], ['Serrano', 'S.', ''], ['Simon', 'P.', ''], ['Sirignano', 'C.', ''], ['Sirri', 'G.', ''], ['Stanco', 'L.', ''], ['Steinwagner', 'J.', ''], ['Tallada-Crespí', 'P.', ''], ['Taylor', 'A. N.', ''], ['Tereno', 'I.', ''], ['Toft', 'S.', ''], ['Toledo-Moreo', 'R.', ''], ['Torradeflot', 'F.', ''], ['Tutusaus', 'I.', ''], ['Valenziano', 'L.', ''], ['Valiviita', 'J.', ''], ['Vassallo', 'T.', ''], ['Kleijn', 'G. Verdoes', ''], ['Veropalumbo', 'A.', ''], ['Wang', 'Y.', ''], ['Weller', 'J.', ''], ['Zacchei', 'A.', ''], ['Zamorani', 'G.', ''], ['Zerbi', 'F. M.', ''], ['Zinchenko', 'I. A.', ''], ['Zucca', 'E.', ''], ['Allevato', 'V.', ''], ['Ballardini', 'M.', ''], ['Bolzonella', 'M.', ''], ['Bozzo', 'E.', ''], ['Burigana', 'C.', ''], ['Cabanac', 'R.', ''], ['Cappi', 'A.', ''], ['Di Ferdinando', 'D.', ''], ['Vigo', 'J. A. Escartin', ''], ['Gabarra', 'L.', ''], ['Martín-Fleitas', 'J.', ''], ['Matthew', 'S.', ''], ['Mauri', 'N.', ''], ['Metcalf', 'R. B.', ''], ['Pezzotta', 'A.', ''], ['Pöntinen', 'M.', ''], ['Porciani', 'C.', ''], ['Risso', 'I.', ''], ['Scottez', 'V.', ''], ['Sereno', 'M.', ''], ['Tenti', 'M.', ''], ['Viel', 'M.', ''], ['Wiesmann', 'M.', ''], ['Akrami', 'Y.', ''], ['Andika', 'I. T.', ''], ['Anselmi', 'S.', ''], ['Archidiacono', 'M.', ''], ['Atrio-Barandela', 'F.', ''], ['Benoist', 'C.', ''], ['Benson', 'K.', ''], ['Bertacca', 'D.', ''], ['Bethermin', 'M.', ''], ['Bisigello', 'L.', ''], ['Blanchard', 'A.', ''], ['Blot', 'L.', ''], ['Brown', 'M. L.', ''], ['Bruton', 'S.', ''], ['Calabro', 'A.', ''], ['Quevedo', 'B. Camacho', ''], ['Caro', 'F.', ''], ['Carvalho', 'C. S.', ''], ['Castro', 'T.', ''], ['Charles', 'Y.', ''], ['Cogato', 'F.', ''], ['Cooray', 'A. R.', ''], ['Cucciati', 'O.', ''], ['Davini', 'S.', ''], ['De Paolis', 'F.', ''], ['Desprez', 'G.', ''], ['Díaz-Sánchez', 'A.', ''], ['Diaz', 'J. J.', ''], ['Di Domizio', 'S.', ''], ['Diego', 'J. M.', ''], ['Duc', 'P. -A.', ''], ['Enia', 'A.', ''], ['Fang', 'Y.', ''], ['Ferrari', 'A. G.', ''], ['Ferreira', 'P. G.', ''], ['Finoguenov', 'A.', ''], ['Fontana', 'A.', ''], ['Franco', 'A.', ''], ['Ganga', 'K.', ''], ['García-Bellido', 'J.', ''], ['Gasparetto', 'T.', ''], ['Gautard', 'V.', ''], ['Gaztanaga', 'E.', ''], ['Giacomini', 'F.', ''], ['Gianotti', 'F.', ''], ['Gozaliasl', 'G.', ''], ['Guidi', 'M.', ''], ['Gutierrez', 'C. M.', ''], ['Hall', 'A.', ''], ['Hartley', 'W. G.', ''], ['Hemmati', 'S.', ''], ['Hernández-Monteagudo', 'C.', ''], ['Hildebrandt', 'H.', ''], ['Hjorth', 'J.', ''], ['Kajava', 'J. J. E.', ''], ['Kang', 'Y.', ''], ['Kansal', 'V.', ''], ['Karagiannis', 'D.', ''], ['Kiiveri', 'K.', ''], ['Kirkpatrick', 'C. C.', ''], ['Kruk', 'S.', ''], ['Graet', 'J. Le', ''], ['Legrand', 'L.', ''], ['Lembo', 'M.', ''], ['Lepori', 'F.', ''], ['Leroy', 'G.', ''], ['Lesci', 'G. F.', ''], ['Lesgourgues', 'J.', ''], ['Leuzzi', 'L.', ''], ['Liaudat', 'T. I.', ''], ['Loureiro', 'A.', ''], ['Macias-Perez', 'J.', ''], ['Maggio', 'G.', ''], ['Magliocchetti', 'M.', ''], ['Magnier', 'E. A.', ''], ['Mannucci', 'F.', ''], ['Maoli', 'R.', ''], ['Martins', 'C. J. A. P.', ''], ['Maurin', 'L.', ''], ['Miluzio', 'M.', ''], ['Monaco', 'P.', ''], ['Moretti', 'C.', ''], ['Morgante', 'G.', ''], ['Murray', 'C.', ''], ['Naidoo', 'K.', ''], ['Navarro-Alsina', 'A.', ''], ['Nesseris', 'S.', ''], ['Passalacqua', 'F.', ''], ['Paterson', 'K.', ''], ['Patrizii', 'L.', ''], ['Pisani', 'A.', ''], ['Potter', 'D.', ''], ['Quai', 'S.', ''], ['Radovich', 'M.', ''], ['Sacquegna', 'S.', ''], ['Sahlén', 'M.', ''], ['Sanders', 'D. B.', ''], ['Sarpa', 'E.', ''], ['Schneider', 'A.', ''], ['Sciotti', 'D.', ''], ['Scognamiglio', 'D.', ''], ['Sellentin', 'E.', ''], ['Smith', 'L. C.', ''], ['Tanidis', 'K.', ''], ['Testera', 'G.', ''], ['Teyssier', 'R.', ''], ['Tosi', 'S.', ''], ['Troja', 'A.', ''], ['Tucci', 'M.', ''], ['Valieri', 'C.', ''], ['Venhola', 'A.', ''], ['Vergani', 'D.', ''], ['Verza', 'G.', ''], ['Vielzeuf', 'P.', ''], ['Walton', 'N. A.', ''], ['Sorce', 'J. G.', '']]","[{'text': 'AstroPT', 'label': 'Foundation Model'}, {'text': 'AstroPT\nembeddings', 'label': 'Embedding'}, {'text': 'simple fine-tuning', 'label': 'Fine-tuning'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'AstroPT', 'label': 'Foundation Model'}]",Embedding,embeddings,0.963064432144165
2503.15374,Anatole Callies,"Anatole Callies (Inato), Quentin Bodinier (Inato), Philippe Ravaud
  (Inato, Universit\'e Paris Cit\'e and Universit\'e Sorbonne Paris Nord,
  INSERM, INRAE, Paris, France, Centre d'epid\'emiologie clinique, AP-HP,
  H\^opital H\^otel Dieu, Paris, France) and Kourosh Davarpanah (Inato)","Real-world validation of a multimodal LLM-powered pipeline for
  High-Accuracy Clinical Trial Patient Matching leveraging EHR data",,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Background: Patient recruitment in clinical trials is hindered by complex
eligibility criteria and labor-intensive chart reviews. Prior research using
text-only models have struggled to address this problem in a reliable and
scalable way due to (1) limited reasoning capabilities, (2) information loss
from converting visual records to text, and (3) lack of a generic EHR
integration to extract patient data.
  Methods: We introduce a broadly applicable, integration-free, LLM-powered
pipeline that automates patient-trial matching using unprocessed documents
extracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm,
enabling the assessment of even the most complex criteria, (2) visual
capabilities of latest LLMs to interpret medical records without lossy
image-to-text conversions, and (3) multimodal embeddings for efficient medical
record search. The pipeline was validated on the n2c2 2018 cohort selection
dataset (288 diabetic patients) and a real-world dataset composed of 485
patients from 30 different sites matched against 36 diverse trials.
  Results: On the n2c2 dataset, our method achieved a new state-of-the-art
criterion-level accuracy of 93\%. In real-world trials, the pipeline yielded an
accuracy of 87\%, undermined by the difficulty to replicate human
decision-making when medical records lack sufficient information. Nevertheless,
users were able to review overall eligibility in under 9 minutes per patient on
average, representing an 80\% improvement over traditional manual chart
reviews.
  Conclusion: This pipeline demonstrates robust performance in clinical trial
patient matching without requiring custom integration with site systems or
trial-specific tailoring, thereby enabling scalable deployment across sites
seeking to leverage AI for patient matching.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:12:11 GMT'}]",2025-03-20,"[['Callies', 'Anatole', '', 'Inato'], ['Bodinier', 'Quentin', '', 'Inato'], ['Ravaud', 'Philippe', '', ""Inato, Université Paris Cité and Université Sorbonne Paris Nord,\n  INSERM, INRAE, Paris, France, Centre d'epidémiologie clinique, AP-HP,\n  Hôpital Hôtel Dieu, Paris, France""], ['Davarpanah', 'Kourosh', '', 'Inato']]","[{'text': 'multimodal embeddings', 'label': 'Embedding'}]",Embedding,multimodal embeddings,0.6755873560905457
2503.15406,Jisu Nam,"Jisu Nam, Soowon Son, Zhan Xu, Jing Shi, Difan Liu, Feng Liu, Aashish
  Misraa, Seungryong Kim, Yang Zhou",Visual Persona: Foundation Model for Full-Body Human Customization,"CVPR 2025, Project page is available at
  https://cvlab-kaist.github.io/Visual-Persona",,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  We introduce Visual Persona, a foundation model for text-to-image full-body
human customization that, given a single in-the-wild human image, generates
diverse images of the individual guided by text descriptions. Unlike prior
methods that focus solely on preserving facial identity, our approach captures
detailed full-body appearance, aligning with text descriptions for body
structure and scene variations. Training this model requires large-scale paired
human data, consisting of multiple images per individual with consistent
full-body identities, which is notoriously difficult to obtain. To address
this, we propose a data curation pipeline leveraging vision-language models to
evaluate full-body appearance consistency, resulting in Visual Persona-500K, a
dataset of 580k paired human images across 100k unique identities. For precise
appearance transfer, we introduce a transformer encoder-decoder architecture
adapted to a pre-trained text-to-image diffusion model, which augments the
input image into distinct body regions, encodes these regions as local
appearance features, and projects them into dense identity embeddings
independently to condition the diffusion model for synthesizing customized
images. Visual Persona consistently surpasses existing approaches, generating
high-quality, customized images from in-the-wild inputs. Extensive ablation
studies validate design choices, and we demonstrate the versatility of Visual
Persona across various downstream tasks.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:45:47 GMT'}]",2025-03-20,"[['Nam', 'Jisu', ''], ['Son', 'Soowon', ''], ['Xu', 'Zhan', ''], ['Shi', 'Jing', ''], ['Liu', 'Difan', ''], ['Liu', 'Feng', ''], ['Misraa', 'Aashish', ''], ['Kim', 'Seungryong', ''], ['Zhou', 'Yang', '']]","[{'text': 'Visual Persona', 'label': 'Foundation Model'}, {'text': 'dense identity embeddings', 'label': 'Embedding'}, {'text': 'Visual Persona', 'label': 'Foundation Model'}, {'text': 'Visual\nPersona', 'label': 'Foundation Model'}]",Embedding,dense identity embeddings,0.6136041879653931
2503.15441,Te-Sheng Lin,Wei-Fan Hu and Te-Sheng Lin and Ming-Chih Lai,"A discontinuity-capturing neural network with categorical embedding and
  its application to anisotropic elliptic interface problems",,,,,math.NA cs.LG cs.NA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we propose a discontinuity-capturing shallow neural network
with categorical embedding to represent piecewise smooth functions. The network
comprises three hidden layers, a discontinuity-capturing layer, a categorical
embedding layer, and a fully-connected layer. Under such a design, we show that
a piecewise smooth function, even with a large number of pieces, can be
approximated by a single neural network with high prediction accuracy. We then
leverage the proposed network model to solve anisotropic elliptic interface
problems. The network is trained by minimizing the mean squared error loss of
the system. Our results show that, despite its simple and shallow structure,
the proposed neural network model exhibits comparable efficiency and accuracy
to traditional grid-based numerical methods.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:21:51 GMT'}]",2025-03-20,"[['Hu', 'Wei-Fan', ''], ['Lin', 'Te-Sheng', ''], ['Lai', 'Ming-Chih', '']]","[{'text': 'categorical embedding', 'label': 'Embedding'}, {'text': 'categorical\nembedding layer', 'label': 'Embedding'}]",Embedding,categorical embedding,0.7434220314025879
2503.15458,Gediminas Juska,"Gediminas Juska, Simone Varo, Nicola Maraviglia, John O'Hara, Salvador
  Medina, Luca Colavecchi, Francesco Mattana, Armando Trapala, Michael Schmidt,
  Agnieszka Gocalinska, Emanuele Pelucchi","Self-aligned pillar arrays embedding site-controlled single quantum dots
  for enhanced non-classical light emission",,,,,physics.optics cond-mat.other,http://creativecommons.org/licenses/by/4.0/,"  This work presents a foundational approach for fabricating arrays of
self-aligned micro- and nanopillar structures incorporating individual
site-controlled quantum dots (QDs) for enhanced light extraction. This method
leverages the non-planar surface morphology of pyramidal QD samples to define
dielectric masks self - aligned to the QD positions. The mask size, and
consequently the lateral dimensions of the pillars, is precisely controlled
through a chemical mechanical polishing step, obviating the need for any
additional lithography step for creating the pillar. This fabrication technique
offers several key advantages, including precise control over the pillar sites,
and fully deterministic embedding of QD structures. The functionality of the
structures was validated by integrating single In0.25Ga0.75As QDs - upon
two-photon excitation of the biexciton state, the emission of single and
polarization-entangled photon pairs was observed. Additionally, an extra
fabrication step to deposit dome-like structures atop the pillars was
demonstrated, effectively enhancing light extraction efficiency up to 12%.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:40:43 GMT'}]",2025-03-20,"[['Juska', 'Gediminas', ''], ['Varo', 'Simone', ''], ['Maraviglia', 'Nicola', ''], [""O'Hara"", 'John', ''], ['Medina', 'Salvador', ''], ['Colavecchi', 'Luca', ''], ['Mattana', 'Francesco', ''], ['Trapala', 'Armando', ''], ['Schmidt', 'Michael', ''], ['Gocalinska', 'Agnieszka', ''], ['Pelucchi', 'Emanuele', '']]","[{'text': 'embedding', 'label': 'Embedding'}]",Embedding,embedding,1.0
2503.15573,Da Ma,"Da Ma and Gonghu Shang and Zhi Chen and Libo Qin and Yijie Luo and Lei
  Pan and Shuai Fan and Lu Chen and Kai Yu","Neuronal Activation States as Sample Embeddings for Data Selection in
  Task-Specific Instruction Tuning",preprint,,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Task-specific instruction tuning enhances the performance of large language
models (LLMs) on specialized tasks, yet efficiently selecting relevant data for
this purpose remains a challenge. Inspired by neural coactivation in the human
brain, we propose a novel data selection method called NAS, which leverages
neuronal activation states as embeddings for samples in the feature space.
Extensive experiments show that NAS outperforms classical data selection
methods in terms of both effectiveness and robustness across different models,
datasets, and selection ratios.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:35:57 GMT'}]",2025-03-21,"[['Ma', 'Da', ''], ['Shang', 'Gonghu', ''], ['Chen', 'Zhi', ''], ['Qin', 'Libo', ''], ['Luo', 'Yijie', ''], ['Pan', 'Lei', ''], ['Fan', 'Shuai', ''], ['Chen', 'Lu', ''], ['Yu', 'Kai', '']]","[{'text': 'Task-specific instruction tuning', 'label': 'Fine-tuning'}, {'text': 'embeddings', 'label': 'Embedding'}]",Embedding,embeddings,0.963064432144165
2503.15576,Alba M\'arquez-Rodr\'iguez,"Alba M\'arquez-Rodr\'iguez, Miguel \'Angel Mohedano-Munoz, Manuel J.
  Mar\'in-Jim\'enez, Eduardo Santamar\'ia-Garc\'ia, Giulia Bastianelli, Pedro
  Jordano and Irene Mendoza","A Bird Song Detector for improving bird identification through Deep
  Learning: a case study from Do\~nana","20 pages, 13 images, for associated dataset see
  https://huggingface.co/datasets/GrunCrow/BIRDeep_AudioAnnotations , for
  associated code see
  https://github.com/GrunCrow/BIRDeep_BirdSongDetector_NeuralNetworks and
  https://github.com/GrunCrow/Bird-Song-Detector",,,,cs.SD cs.AI cs.CV cs.LG cs.NE,http://creativecommons.org/licenses/by-sa/4.0/,"  Passive Acoustic Monitoring with automatic recorders is essential for
ecosystem conservation but generates vast unsupervised audio data, posing
challenges for extracting meaningful information. Deep Learning techniques
offer a promising solution. BirdNET, a widely used model for bird
identification, has shown success in many study systems but is limited in some
regions due to biases in its training data. A key challenge in bird species
detection is that many recordings either lack target species or contain
overlapping vocalizations. To overcome these problems, we developed a
multi-stage pipeline for automatic bird vocalization identification in Do\~nana
National Park (SW Spain), a region facing significant conservation threats. Our
approach included a Bird Song Detector to isolate vocalizations and custom
classifiers trained with BirdNET embeddings. We manually annotated 461 minutes
of audio from three habitats across nine locations, yielding 3,749 annotations
for 34 classes. Spectrograms facilitated the use of image processing
techniques. Applying the Bird Song Detector before classification improved
species identification, as all classification models performed better when
analyzing only the segments where birds were detected. Specifically, the
combination of the Bird Song Detector and fine-tuned BirdNET compared to the
baseline without the Bird Song Detector. Our approach demonstrated the
effectiveness of integrating a Bird Song Detector with fine-tuned
classification models for bird identification at local soundscapes. These
findings highlight the need to adapt general-purpose tools for specific
ecological challenges, as demonstrated in Do\~nana. Automatically detecting
bird species serves for tracking the health status of this threatened
ecosystem, given the sensitivity of birds to environmental changes, and helps
in the design of conservation measures for reducing biodiversity loss
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 13:19:06 GMT'}]",2025-03-21,"[['Márquez-Rodríguez', 'Alba', ''], ['Mohedano-Munoz', 'Miguel Ángel', ''], ['Marín-Jiménez', 'Manuel J.', ''], ['Santamaría-García', 'Eduardo', ''], ['Bastianelli', 'Giulia', ''], ['Jordano', 'Pedro', ''], ['Mendoza', 'Irene', '']]","[{'text': 'BirdNET embeddings', 'label': 'Embedding'}]",Embedding,BirdNET embeddings,0.617822527885437
2503.15617,Masud Ahmed,"Masud Ahmed, Zahid Hasan, Syed Arefinul Haque, Abu Zaher Md Faridee,
  Sanjay Purushotham, Suya You, Nirmalya Roy","CAM-Seg: A Continuous-valued Embedding Approach for Semantic Image
  Generation",,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Traditional transformer-based semantic segmentation relies on quantized
embeddings. However, our analysis reveals that autoencoder accuracy on
segmentation mask using quantized embeddings (e.g. VQ-VAE) is 8% lower than
continuous-valued embeddings (e.g. KL-VAE). Motivated by this, we propose a
continuous-valued embedding framework for semantic segmentation. By
reformulating semantic mask generation as a continuous image-to-embedding
diffusion process, our approach eliminates the need for discrete latent
representations while preserving fine-grained spatial and semantic details. Our
key contribution includes a diffusion-guided autoregressive transformer that
learns a continuous semantic embedding space by modeling long-range
dependencies in image features. Our framework contains a unified architecture
combining a VAE encoder for continuous feature extraction, a diffusion-guided
transformer for conditioned embedding generation, and a VAE decoder for
semantic mask reconstruction. Our setting facilitates zero-shot domain
adaptation capabilities enabled by the continuity of the embedding space.
Experiments across diverse datasets (e.g., Cityscapes and domain-shifted
variants) demonstrate state-of-the-art robustness to distribution shifts,
including adverse weather (e.g., fog, snow) and viewpoint variations. Our model
also exhibits strong noise resilience, achieving robust performance ($\approx$
95% AP compared to baseline) under gaussian noise, moderate motion blur, and
moderate brightness/contrast variations, while experiencing only a moderate
impact ($\approx$ 90% AP compared to baseline) from 50% salt and pepper noise,
saturation and hue shifts. Code available:
https://github.com/mahmed10/CAMSS.git
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 18:06:54 GMT'}]",2025-03-21,"[['Ahmed', 'Masud', ''], ['Hasan', 'Zahid', ''], ['Haque', 'Syed Arefinul', ''], ['Faridee', 'Abu Zaher Md', ''], ['Purushotham', 'Sanjay', ''], ['You', 'Suya', ''], ['Roy', 'Nirmalya', '']]","[{'text': 'quantized\nembeddings', 'label': 'Embedding'}, {'text': 'quantized embeddings', 'label': 'Embedding'}, {'text': 'VQ-VAE', 'label': 'Embedding'}, {'text': 'continuous-valued embeddings', 'label': 'Embedding'}, {'text': 'KL-VAE', 'label': 'Embedding'}, {'text': 'continuous semantic embedding space', 'label': 'Embedding'}]",Embedding,continuous-valued embeddings,0.6811061501502991
2503.15712,Weiwen Hu,"Weiwen Hu, Niccol\`o Parodi, Marcus Zepp, Ingo Feldmann, Oliver
  Schreer, Peter Eisert",SPNeRF: Open Vocabulary 3D Neural Scene Segmentation with Superpoints,"In Proceedings of the 20th International Joint Conference on Computer
  Vision, Imaging and Computer Graphics Theory and Applications (2025)",,10.5220/0013255100003912,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Open-vocabulary segmentation, powered by large visual-language models like
CLIP, has expanded 2D segmentation capabilities beyond fixed classes predefined
by the dataset, enabling zero-shot understanding across diverse scenes.
Extending these capabilities to 3D segmentation introduces challenges, as
CLIP's image-based embeddings often lack the geometric detail necessary for 3D
scene segmentation. Recent methods tend to address this by introducing
additional segmentation models or replacing CLIP with variations trained on
segmentation data, which lead to redundancy or loss on CLIP's general language
capabilities. To overcome this limitation, we introduce SPNeRF, a NeRF based
zero-shot 3D segmentation approach that leverages geometric priors. We
integrate geometric primitives derived from the 3D scene into NeRF training to
produce primitive-wise CLIP features, avoiding the ambiguity of point-wise
features. Additionally, we propose a primitive-based merging mechanism enhanced
with affinity scores. Without relying on additional segmentation models, our
method further explores CLIP's capability for 3D segmentation and achieves
notable improvements over original LERF.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 21:45:59 GMT'}]",2025-03-21,"[['Hu', 'Weiwen', ''], ['Parodi', 'Niccolò', ''], ['Zepp', 'Marcus', ''], ['Feldmann', 'Ingo', ''], ['Schreer', 'Oliver', ''], ['Eisert', 'Peter', '']]","[{'text': 'CLIP', 'label': 'Large Language Model'}, {'text': 'CLIP', 'label': 'Large Language Model'}, {'text': 'image-based embeddings', 'label': 'Embedding'}, {'text': 'CLIP', 'label': 'Large Language Model'}, {'text': 'CLIP', 'label': 'Large Language Model'}, {'text': 'CLIP', 'label': 'Large Language Model'}, {'text': 'CLIP', 'label': 'Large Language Model'}]",Embedding,image-based embeddings,0.7884131073951721
2503.15798,Shibo Jie,"Shibo Jie, Yehui Tang, Kai Han, Yitong Li, Duyu Tang, Zhi-Hong Deng,
  Yunhe Wang",Mixture of Lookup Experts,,,,,cs.LG cs.CL,http://creativecommons.org/publicdomain/zero/1.0/,"  Mixture-of-Experts (MoE) activates only a subset of experts during inference,
allowing the model to maintain low inference FLOPs and latency even as the
parameter count scales up. However, since MoE dynamically selects the experts,
all the experts need to be loaded into VRAM. Their large parameter size still
limits deployment, and offloading, which load experts into VRAM only when
needed, significantly increase inference latency. To address this, we propose
Mixture of Lookup Experts (MoLE), a new MoE architecture that is efficient in
both communication and VRAM usage. In MoLE, the experts are Feed-Forward
Networks (FFNs) during training, taking the output of the embedding layer as
input. Before inference, these experts can be re-parameterized as lookup tables
(LUTs) that retrieves expert outputs based on input ids, and offloaded to
storage devices. Therefore, we do not need to perform expert computations
during inference. Instead, we directly retrieve the expert's computation
results based on input ids and load them into VRAM, and thus the resulting
communication overhead is negligible. Experiments show that, with the same
FLOPs and VRAM usage, MoLE achieves inference speeds comparable to dense models
and significantly faster than MoE with experts offloading, while maintaining
performance on par with MoE.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 02:31:57 GMT'}]",2025-03-21,"[['Jie', 'Shibo', ''], ['Tang', 'Yehui', ''], ['Han', 'Kai', ''], ['Li', 'Yitong', ''], ['Tang', 'Duyu', ''], ['Deng', 'Zhi-Hong', ''], ['Wang', 'Yunhe', '']]","[{'text': 'embedding layer', 'label': 'Embedding'}]",Embedding,embedding layer,0.7880257368087769
2503.15922,Fatima-Zahrae EL-BOUKKOURI,"Fatima-Zahrae El-Boukkouri (INSA Toulouse, IMT), Josselin Garnier
  (CMAP, ASCII), Olivier Roustant (INSA Toulouse, IMT, RT-UQ)","General reproducing properties in RKHS with application to derivative
  and integral operators",,,,,math.ST stat.ML stat.TH,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we generalize the reproducing property in Reproducing Kernel
Hilbert Spaces (RKHS). We establish a reproducing property for the closure of
the class of combinations of composition operators under minimal conditions. As
an application, we improve the existing sufficient conditions for the
reproducing property to hold for the derivative operator, as well as for the
existence of the mean embedding function. These results extend the scope of
applicability of the representer theorem for regularized learning algorithms
that involve data for function values, gradients, or any other operator from
the considered class.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 07:58:09 GMT'}]",2025-03-21,"[['El-Boukkouri', 'Fatima-Zahrae', '', 'INSA Toulouse, IMT'], ['Garnier', 'Josselin', '', 'CMAP, ASCII'], ['Roustant', 'Olivier', '', 'INSA Toulouse, IMT, RT-UQ']]","[{'text': 'mean embedding function', 'label': 'Embedding'}, {'text': 'regularized learning algorithms', 'label': 'Few-shot Learning'}]",Embedding,mean embedding function,0.6314435601234436
2503.15958,Thomas Peyrat,"Caroline Hillairet (CREST), Thomas Peyrat (CREST), Anthony R\'eveillac
  (INSA Toulouse, IMT, UT)",Multivariate Self-Exciting Processes with Dependencies,,,,,math.PR q-fin.RM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper introduces the class of multidimensional self-exciting processes
with dependencies (MSPD), which is a unifying writing for a large class of
processes: counting, loss, intensity, and also shifted processes. The framework
takes into account dynamic dependencies between the frequency and the severity
components of the risk, and therefore induces theoretical challenges in the
computations of risk valuations. We present a general method for calculating
different quantities related to these MSPDs, which combines the Poisson
imbedding, the pseudo-chaotic expansion and Malliavin calculus. The methodology
is illustrated for the computation of explicit general correlation formula.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:55:02 GMT'}]",2025-03-21,"[['Hillairet', 'Caroline', '', 'CREST'], ['Peyrat', 'Thomas', '', 'CREST'], ['Réveillac', 'Anthony', '', 'INSA Toulouse, IMT, UT']]","[{'text': 'Poisson\nimbedding', 'label': 'Embedding'}, {'text': 'pseudo-chaotic expansion', 'label': 'Embedding'}, {'text': 'Malliavin calculus', 'label': 'Embedding'}]",Embedding,"Poisson
imbedding",0.5439561605453491
2503.16133,Chao Li,"Lei Chen, Hao Li, Yuxin Zhang, Chao Li, and Kai Wen",Multi-Prompt Style Interpolation for Fine-Grained Artistic Control,,,,,cs.GR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text-driven image style transfer has seen remarkable progress with methods
leveraging cross-modal embeddings for fast, high-quality stylization. However,
most existing pipelines assume a \emph{single} textual style prompt, limiting
the range of artistic control and expressiveness. In this paper, we propose a
novel \emph{multi-prompt style interpolation} framework that extends the
recently introduced \textbf{StyleMamba} approach. Our method supports blending
or interpolating among multiple textual prompts (eg, ``cubism,''
``impressionism,'' and ``cartoon''), allowing the creation of nuanced or hybrid
artistic styles within a \emph{single} image. We introduce a
\textit{Multi-Prompt Embedding Mixer} combined with \textit{Adaptive Blending
Weights} to enable fine-grained control over the spatial and semantic influence
of each style. Further, we propose a \emph{Hierarchical Masked Directional
Loss} to refine region-specific style consistency. Experiments and user studies
confirm our approach outperforms single-prompt baselines and naive linear
combinations of styles, achieving superior style fidelity, text-image
alignment, and artistic flexibility, all while maintaining the computational
efficiency offered by the state-space formulation.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:29:32 GMT'}]",2025-03-21,"[['Chen', 'Lei', ''], ['Li', 'Hao', ''], ['Zhang', 'Yuxin', ''], ['Li', 'Chao', ''], ['Wen', 'Kai', '']]","[{'text': 'cross-modal embeddings', 'label': 'Embedding'}, {'text': 'textual prompts', 'label': 'Prompting'}, {'text': 'fine-grained control', 'label': 'Fine-tuning'}]",Embedding,cross-modal embeddings,0.6585479378700256
2503.16153,Tianyi Wei,"Tianyi Wei, Yifan Zhou, Dongdong Chen, Xingang Pan","FreeFlux: Understanding and Exploiting Layer-Specific Roles in
  RoPE-Based MMDiT for Versatile Image Editing",Project page: https://wtybest.github.io/projects/FreeFlux/,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  The integration of Rotary Position Embedding (RoPE) in Multimodal Diffusion
Transformer (MMDiT) has significantly enhanced text-to-image generation
quality. However, the fundamental reliance of self-attention layers on
positional embedding versus query-key similarity during generation remains an
intriguing question. We present the first mechanistic analysis of RoPE-based
MMDiT models (e.g., FLUX), introducing an automated probing strategy that
disentangles positional information versus content dependencies by
strategically manipulating RoPE during generation. Our analysis reveals
distinct dependency patterns that do not straightforwardly correlate with
depth, offering new insights into the layer-specific roles in RoPE-based MMDiT.
Based on these findings, we propose a training-free, task-specific image
editing framework that categorizes editing tasks into three types:
position-dependent editing (e.g., object addition), content
similarity-dependent editing (e.g., non-rigid editing), and region-preserved
editing (e.g., background replacement). For each type, we design tailored
key-value injection strategies based on the characteristics of the editing
task. Extensive qualitative and quantitative evaluations demonstrate that our
method outperforms state-of-the-art approaches, particularly in preserving
original semantic content and achieving seamless modifications.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:55:12 GMT'}]",2025-03-21,"[['Wei', 'Tianyi', ''], ['Zhou', 'Yifan', ''], ['Chen', 'Dongdong', ''], ['Pan', 'Xingang', '']]","[{'text': 'Rotary Position Embedding', 'label': 'contextual Embedding'}, {'text': 'RoPE', 'label': 'contextual Embedding'}, {'text': 'self-attention layers', 'label': 'Attention mechanism'}, {'text': 'positional embedding', 'label': 'Embedding'}, {'text': 'RoPE', 'label': 'RoBERTa'}]",Embedding,positional embedding,0.7962335348129272
2503.16159,Federico Berto,"Jiwoo Son, Zhikai Zhao, Federico Berto, Chuanbo Hua, Changhyun Kwon,
  Jinkyoo Park",Neural Combinatorial Optimization for Real-World Routing,,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Vehicle Routing Problems (VRPs) are a class of NP-hard problems ubiquitous in
several real-world logistics scenarios that pose significant challenges for
optimization. Neural Combinatorial Optimization (NCO) has emerged as a
promising alternative to classical approaches, as it can learn fast heuristics
to solve VRPs. However, most research works in NCO for VRPs focus on simplified
settings, which do not account for asymmetric distances and travel durations
that cannot be derived by simple Euclidean distances and unrealistic data
distributions, hindering real-world deployment. This work introduces RRNCO
(Real Routing NCO) to bridge the gap of NCO between synthetic and real-world
VRPs in the critical aspects of both data and modeling. First, we introduce a
new, openly available dataset with real-world data containing a diverse dataset
of locations, distances, and duration matrices from 100 cities, considering
realistic settings with actual routing distances and durations obtained from
Open Source Routing Machine (OSRM). Second, we propose a novel approach that
efficiently processes both node and edge features through contextual gating,
enabling the construction of more informed node embedding, and we finally
incorporate an Adaptation Attention Free Module (AAFM) with neural adaptive
bias mechanisms that effectively integrates not only distance matrices but also
angular relationships between nodes, allowing our model to capture rich
structural information. RRNCO achieves state-of-the-art results in real-world
VRPs among NCO methods. We make our dataset and code publicly available at
https://github.com/ai4co/real-routing-nco.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:57:33 GMT'}]",2025-03-21,"[['Son', 'Jiwoo', ''], ['Zhao', 'Zhikai', ''], ['Berto', 'Federico', ''], ['Hua', 'Chuanbo', ''], ['Kwon', 'Changhyun', ''], ['Park', 'Jinkyoo', '']]","[{'text': 'contextual gating', 'label': 'contextual Embedding'}, {'text': 'node embedding', 'label': 'Embedding'}]",Embedding,node embedding,0.7871551513671875
