id,submitter,authors,title,comments,journal-ref,doi,report-no,categories,license,abstract,versions,update_date,authors_parsed,extracted_entities,assigned_concept,matched_keyword,similarity_score
2106.05957,Andrew Ellis,Andrew Ellis and Heidi Christina Thysen,Subjective Causality in Choice,,,,,econ.TH,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Choices based on observational data depend on beliefs about which
correlations reflect causality. An agent predicts the consequence of available
actions using a dataset and her subjective beliefs about causality represented
by a directed acyclic graph (DAG). We identify her DAG from her random choice
rule. Her choices reveal the chains of causal reasoning that she undertakes and
the confounding variables she adjusts for, and these pin down her model. When
her choices generate the dataset used, her behavior affects her inferences,
which in turn affect her choices. We provide necessary and sufficient
conditions for testing whether her behavior is compatible with such a model.
","[{'version': 'v1', 'created': 'Thu, 10 Jun 2021 17:54:00 GMT'}, {'version': 'v2', 'created': 'Thu, 21 Oct 2021 09:00:14 GMT'}, {'version': 'v3', 'created': 'Fri, 9 Dec 2022 16:24:07 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 09:56:05 GMT'}]",2025-03-21,"[['Ellis', 'Andrew', ''], ['Thysen', 'Heidi Christina', '']]","[{'text': 'chains of causal reasoning', 'label': 'Chain of thought'}]",Chain of thought,chains of causal reasoning,0.5537554025650024
2401.05787,Md Rizwan Parvez,Md Rizwan Parvez,"Chain of Evidences and Evidence to Generate: Prompting for Context
  Grounded and Retrieval Augmented Reasoning",Accepted at NAACL KnowledgeNLP 2025,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  While chain-of-thoughts (CoT) prompting has revolutionized how LLMs perform
reasoning tasks, its current methods and variations (e.g, Self-consistency,
ReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR) etc.,)
suffer from limitations like limited context grounding,
hallucination/inconsistent output generation, and iterative sluggishness. To
overcome these challenges, we introduce a novel mono/dual-step zero-shot
prompting framework built upon two unique strategies Chain of Evidences (CoE)}
and Evidence to Generate (E2G). Instead of unverified reasoning claims, our
innovative approaches leverage the power of ""evidence for decision making"" by
first focusing exclusively on the thought sequences explicitly mentioned in the
context which then serve as extracted evidence, guiding the LLM's output
generation process with greater precision and efficiency. This simple yet
potent approach unlocks the full potential of chain-of-thoughts prompting,
facilitating faster, more reliable, and contextually aware reasoning in LLMs.
Our framework consistently achieves remarkable results across various
knowledge-intensive reasoning and generation tasks, surpassing baseline
approaches with state-of-the-art LLMs. For instance, (i) on the LogiQA
benchmark using GPT-4, CoE achieves a new state-of-the-art accuracy of 53.8%,
surpassing CoT by 18%, ToT by 11%, and CR by 9%; (ii) CoE with PaLM-2
outperforms the variable-shot performance of Gemini Ultra by 0.9 F1 points,
achieving an F1 score of 83.3 on DROP. We release our prompts and outputs on
these benchmarks as a new instruction tuning dataset for future research at
https://huggingface.co/datasets/kagnlp/Chain-of-Evidences/.
","[{'version': 'v1', 'created': 'Thu, 11 Jan 2024 09:49:15 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 10:35:11 GMT'}]",2025-03-18,"[['Parvez', 'Md Rizwan', '']]","[{'text': 'chain-of-thoughts', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'Chain of Evidences', 'label': 'Chain of thought'}, {'text': 'chain-of-thoughts prompting', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'ToT', 'label': 'Chain of thought'}]",Chain of thought,chain-of-thoughts,0.9114571809768677
2405.02536,James Murray Louw,"Lyudmila Grigoryeva, James Louw, and Juan-Pablo Ortega",Forecasting causal dynamics with universal reservoirs,"37 pages, 5 figures",,,,math.DS,http://creativecommons.org/licenses/by/4.0/,"  An iterated multistep forecasting scheme based on recurrent neural networks
(RNN) is proposed for the time series generated by causal chains with infinite
memory. This forecasting strategy contains, as a particular case, the iterative
prediction strategies for dynamical systems that are customary in reservoir
computing. Explicit error bounds are obtained as a function of the forecasting
horizon, functional and dynamical features of the specific RNN used, and the
approximation error committed by it. In particular, the growth rate of the
error is shown to be exponential and controlled by the top Lyapunov exponent of
the proxy system. The framework in the paper circumvents difficult-to-verify
embedding hypotheses that appear in previous references in the literature and
applies to new situations like the finite-dimensional observations of
functional differential equations or the deterministic parts of stochastic
processes to which standard embedding techniques do not necessarily apply.
","[{'version': 'v1', 'created': 'Sat, 4 May 2024 01:41:58 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 08:31:20 GMT'}]",2025-03-21,"[['Grigoryeva', 'Lyudmila', ''], ['Louw', 'James', ''], ['Ortega', 'Juan-Pablo', '']]","[{'text': 'causal chains', 'label': 'Chain of thought'}]",Chain of thought,causal chains,0.5337221622467041
2405.17418,Jiaming Liu,"Chenxuan Li, Jiaming Liu, Guanqun Wang, Xiaoqi Li, Sixiang Chen, Liang
  Heng, Chuyan Xiong, Jiaxin Ge, Renrui Zhang, Kaichen Zhou, Shanghang Zhang","A Self-Correcting Vision-Language-Action Model for Fast and Slow System
  Manipulation",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, some studies have integrated Multimodal Large Language Models into
robotic manipulation, constructing vision-language-action models (VLAs) to
interpret multimodal information and predict SE(3) poses. While VLAs have shown
promising progress, they may suffer from failures when faced with novel and
complex tasks. To emulate human-like reasoning for more robust manipulation, we
propose the self-corrected (SC-)VLA framework, which integrates fast system for
directly predicting actions and slow system for reflecting on failed actions
within a single VLA policy. For the fast system, we incorporate
parameter-efficient fine-tuning to equip the model with pose prediction
capabilities while preserving the inherent reasoning abilities of MLLMs. For
the slow system, we propose a Chain-of-Thought training strategy for failure
correction, designed to mimic human reflection after a manipulation failure.
Specifically, our model learns to identify the causes of action failures,
adaptively seek expert feedback, reflect on the current failure scenario, and
iteratively generate corrective actions, step by step. Furthermore, a
continuous policy learning method is designed based on successfully corrected
samples, enhancing the fast system's adaptability to the current configuration.
We compare SC-VLA with the previous SOTA VLA in both simulation and real-world
tasks, demonstrating an efficient correction process and improved manipulation
accuracy on both seen and unseen tasks.
","[{'version': 'v1', 'created': 'Mon, 27 May 2024 17:58:48 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 03:55:48 GMT'}]",2025-03-20,"[['Li', 'Chenxuan', ''], ['Liu', 'Jiaming', ''], ['Wang', 'Guanqun', ''], ['Li', 'Xiaoqi', ''], ['Chen', 'Sixiang', ''], ['Heng', 'Liang', ''], ['Xiong', 'Chuyan', ''], ['Ge', 'Jiaxin', ''], ['Zhang', 'Renrui', ''], ['Zhou', 'Kaichen', ''], ['Zhang', 'Shanghang', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'parameter-efficient fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Chain-of-Thought', 'label': 'Chain of thought'}]",Chain of thought,Chain-of-Thought,0.9539169669151306
2408.15045,Wenhui Liao,"Wenhui Liao, Jiapeng Wang, Hongliang Li, Chengyu Wang, Jun Huang,
  Lianwen Jin","DocLayLLM: An Efficient Multi-modal Extension of Large Language Models
  for Text-rich Document Understanding",CVPR2025,,,,cs.CV,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Text-rich document understanding (TDU) requires comprehensive analysis of
documents containing substantial textual content and complex layouts. While
Multimodal Large Language Models (MLLMs) have achieved fast progress in this
domain, existing approaches either demand significant computational resources
or struggle with effective multi-modal integration. In this paper, we introduce
DocLayLLM, an efficient multi-modal extension of LLMs specifically designed for
TDU. By lightly integrating visual patch tokens and 2D positional tokens into
LLMs' input and encoding the document content using the LLMs themselves, we
fully take advantage of the document comprehension capability of LLMs and
enhance their perception of OCR information. We have also deeply considered the
role of chain-of-thought (CoT) and innovatively proposed the techniques of CoT
Pre-training and CoT Annealing. Our DocLayLLM can achieve remarkable
performances with lightweight training settings, showcasing its efficiency and
effectiveness. Experimental results demonstrate that our DocLayLLM outperforms
existing OCR-dependent methods and OCR-free competitors. Code and model are
available at https://github.com/whlscut/DocLayLLM.
","[{'version': 'v1', 'created': 'Tue, 27 Aug 2024 13:13:38 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Aug 2024 08:32:44 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 10:05:04 GMT'}]",2025-03-20,"[['Liao', 'Wenhui', ''], ['Wang', 'Jiapeng', ''], ['Li', 'Hongliang', ''], ['Wang', 'Chengyu', ''], ['Huang', 'Jun', ''], ['Jin', 'Lianwen', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'chain-of-thought', 'label': 'Chain of thought'}]",Chain of thought,chain-of-thought,0.9539169669151306
2412.02172,Xueqing Wu,"Xueqing Wu, Yuheng Ding, Bingxuan Li, Pan Lu, Da Yin, Kai-Wei Chang,
  Nanyun Peng","VISCO: Benchmarking Fine-Grained Critique and Correction Towards
  Self-Improvement in Visual Reasoning",CVPR 2025. https://visco-benchmark.github.io/,,,,cs.CV cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  The ability of large vision-language models (LVLMs) to critique and correct
their reasoning is an essential building block towards their self-improvement.
However, a systematic analysis of such capabilities in LVLMs is still lacking.
We propose VISCO, the first benchmark to extensively analyze the fine-grained
critique and correction capabilities of LVLMs. Compared to existing work that
uses a single scalar value to critique the entire reasoning [4], VISCO features
dense and fine-grained critique, requiring LVLMs to evaluate the correctness of
each step in the chain-of-thought and provide natural language explanations to
support their judgments. Extensive evaluation of 24 LVLMs demonstrates that
human-written critiques significantly enhance the performance after correction,
showcasing the potential of the self-improvement strategy. However, the
model-generated critiques are less helpful and sometimes detrimental to the
performance, suggesting that critique is the crucial bottleneck. We identified
three common patterns in critique failures: failure to critique visual
perception, reluctance to ""say no"", and exaggerated assumption of error
propagation. To address these issues, we propose an effective LookBack strategy
that revisits the image to verify each piece of information in the initial
reasoning. LookBack significantly improves critique and correction performance
by up to 13.5%.
","[{'version': 'v1', 'created': 'Tue, 3 Dec 2024 05:04:49 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 08:02:22 GMT'}]",2025-03-19,"[['Wu', 'Xueqing', ''], ['Ding', 'Yuheng', ''], ['Li', 'Bingxuan', ''], ['Lu', 'Pan', ''], ['Yin', 'Da', ''], ['Chang', 'Kai-Wei', ''], ['Peng', 'Nanyun', '']]","[{'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'chain-of-thought', 'label': 'Chain of thought'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]",Chain of thought,chain-of-thought,0.9539169669151306
2503.08679,Iv\'an Arcuschin,"Iv\'an Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran
  Rajamanoharan, Neel Nanda, Arthur Conmy",Chain-of-Thought Reasoning In The Wild Is Not Always Faithful,"Accepted to the Reasoning and Planning for LLMs Workshop (ICLR 25),
  10 main paper pages, 39 appendix pages",,,,cs.AI cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art
AI capabilities. However, recent studies have shown that CoT reasoning is not
always faithful, i.e. CoT reasoning does not always reflect how models arrive
at conclusions. So far, most of these studies have focused on unfaithfulness in
unnatural contexts where an explicit bias has been introduced. In contrast, we
show that unfaithful CoT can occur on realistic prompts with no artificial
bias. Our results reveal non-negligible rates of several forms of unfaithful
reasoning in frontier models: Sonnet 3.7 (16.3%), DeepSeek R1 (5.3%) and
ChatGPT-4o (7.0%) all answer a notable proportion of question pairs
unfaithfully. Specifically, we find that models rationalize their implicit
biases in answers to binary questions (""implicit post-hoc rationalization"").
For example, when separately presented with the questions ""Is X bigger than Y?""
and ""Is Y bigger than X?"", models sometimes produce superficially coherent
arguments to justify answering Yes to both questions or No to both questions,
despite such responses being logically contradictory. We also investigate
restoration errors (Dziri et al., 2023), where models make and then silently
correct errors in their reasoning, and unfaithful shortcuts, where models use
clearly illogical reasoning to simplify solving problems in Putnam questions (a
hard benchmark). Our findings raise challenges for AI safety work that relies
on monitoring CoT to detect undesired behavior.
","[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 17:56:30 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 17:49:58 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 19:20:42 GMT'}]",2025-03-21,"[['Arcuschin', 'Iván', ''], ['Janiak', 'Jett', ''], ['Krzyzanowski', 'Robert', ''], ['Rajamanoharan', 'Senthooran', ''], ['Nanda', 'Neel', ''], ['Conmy', 'Arthur', '']]","[{'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'CoT reasoning', 'label': 'Chain of thought'}, {'text': 'CoT reasoning', 'label': 'Chain of thought'}, {'text': 'CoT', 'label': 'Chain of thought'}, {'text': 'realistic prompts', 'label': 'Prompting'}, {'text': 'ChatGPT-4o', 'label': 'ChatGPT'}, {'text': 'models', 'label': 'AI model'}, {'text': 'models', 'label': 'AI model'}, {'text': 'models', 'label': 'AI model'}, {'text': 'AI safety work', 'label': 'AI Ethics'}, {'text': 'CoT', 'label': 'Chain of thought'}]",Chain of thought,Chain-of-Thought,0.9539169669151306
2503.10177,Yirong Sun,"Yirong Sun, Yanjun Chen","PRISM: Preference Refinement via Implicit Scene Modeling for 3D
  Vision-Language Preference-Based Reinforcement Learning","I withdraw arXiv:2503.10177 due to critical computational errors
  invalidating its conclusions and the withdrawal of consent from co-author
  Yanjun Chen",,,,cs.CL cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose PRISM, a novel framework designed to overcome the limitations of
2D-based Preference-Based Reinforcement Learning (PBRL) by unifying 3D point
cloud modeling and future-aware preference refinement. At its core, PRISM
adopts a 3D Point Cloud-Language Model (3D-PC-LLM) to mitigate occlusion and
viewpoint biases, ensuring more stable and spatially consistent preference
signals. Additionally, PRISM leverages Chain-of-Thought (CoT) reasoning to
incorporate long-horizon considerations, thereby preventing the short-sighted
feedback often seen in static preference comparisons. In contrast to
conventional PBRL techniques, this integration of 3D perception and
future-oriented reasoning leads to significant gains in preference agreement
rates, faster policy convergence, and robust generalization across unseen
robotic environments. Our empirical results, spanning tasks such as robotic
manipulation and autonomous navigation, highlight PRISM's potential for
real-world applications where precise spatial understanding and reliable
long-term decision-making are critical. By bridging 3D geometric awareness with
CoT-driven preference modeling, PRISM establishes a comprehensive foundation
for scalable, human-aligned reinforcement learning.
","[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 08:58:10 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 06:22:21 GMT'}]",2025-03-20,"[['Sun', 'Yirong', ''], ['Chen', 'Yanjun', '']]","[{'text': 'PRISM', 'label': 'Foundation Model'}, {'text': 'PRISM', 'label': 'Foundation Model'}, {'text': 'PRISM', 'label': 'Foundation Model'}, {'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'PRISM', 'label': 'Foundation Model'}]",Chain of thought,Chain-of-Thought,0.9539169669151306
2503.11989,Dharani Chandra,Dharani Chandra,Applications of Large Language Model Reasoning in Feature Generation,I just updated the format of the references in the paper,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have revolutionized natural language processing
through their state of art reasoning capabilities. This paper explores the
convergence of LLM reasoning techniques and feature generation for machine
learning tasks. We examine four key reasoning approaches: Chain of Thought,
Tree of Thoughts, Retrieval-Augmented Generation, and Thought Space
Exploration. Our analysis reveals how these approaches can be used to identify
effective feature generation rules without having to manually specify search
spaces. The paper categorizes LLM-based feature generation methods across
various domains including finance, healthcare, and text analytics. LLMs can
extract key information from clinical notes and radiology reports in
healthcare, by enabling more efficient data utilization. In finance, LLMs
facilitate text generation, summarization, and entity extraction from complex
documents. We analyze evaluation methodologies for assessing feature quality
and downstream performance, with particular attention to OCTree's decision tree
reasoning approach that provides language-based feedback for iterative
improvements. Current challenges include hallucination, computational
efficiency, and domain adaptation. As of March 2025, emerging approaches
include inference-time compute scaling, reinforcement learning, and supervised
fine-tuning with model distillation. Future directions point toward multimodal
feature generation, self-improving systems, and neuro-symbolic approaches. This
paper provides a detailed overview of an emerging field that promises to
automate and enhance feature engineering through language model reasoning.
","[{'version': 'v1', 'created': 'Sat, 15 Mar 2025 04:18:01 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 02:18:33 GMT'}]",2025-03-21,"[['Chandra', 'Dharani', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Chain of Thought', 'label': 'Chain of thought'}, {'text': 'Tree of Thoughts', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'inference-time compute scaling', 'label': 'Scaling law'}, {'text': 'reinforcement learning', 'label': 'Knowledge distillation'}, {'text': 'supervised\nfine-tuning', 'label': 'Fine-tuning'}, {'text': 'model distillation', 'label': 'Knowledge distillation'}]",Chain of thought,Chain of Thought,0.9999998807907104
2503.12303,Xiaoying Zhang,"Xiaoying Zhang, Da Peng, Yipeng Zhang, Zonghao Guo, Chengyue Wu, Chi
  Chen, Wei Ke, Helen Meng, Maosong Sun","Towards Self-Improving Systematic Cognition for Next-Generation
  Foundation MLLMs","38 pages. Preprint, work in progress",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite their impressive capabilities, Multimodal Large Language Models
(MLLMs) face challenges with fine-grained perception and complex reasoning.
Prevalent multimodal pre-training approaches focus on enhancing perception by
training on high-quality image captions due to the extremely high cost of
collecting chain-of-thought (CoT) reasoning data for improving reasoning. While
leveraging advanced MLLMs for caption generation enhances scalability, the
outputs often lack comprehensiveness and accuracy. In this paper, we introduce
Self-Improving cognition (SIcog), a self-learning framework designed to
construct next-generation foundation MLLMs by enhancing their systematic
cognitive capabilities through multimodal pre-training with self-generated
data. Specifically, we propose Chain-of-Description, an approach that improves
an MLLM's systematic perception by enabling step-by-step visual understanding,
ensuring greater comprehensiveness and accuracy. Additionally, we adopt a
structured CoT reasoning technique to enable MLLMs to integrate in-depth
multimodal reasoning. To construct a next-generation foundation MLLM with
self-improved cognition, SIcog first equips an MLLM with systematic perception
and reasoning abilities using minimal external annotations. The enhanced models
then generate detailed captions and CoT reasoning data, which are further
curated through self-consistency. This curated data is ultimately used for
multimodal pre-training to develop next-generation foundation models. Extensive
experiments on both low- and high-resolution MLLMs across diverse benchmarks
demonstrate that, with merely 213K self-generated pre-training samples, SIcog
produces next-generation foundation MLLMs with significantly improved
cognition, achieving benchmark-leading performance compared to prevalent
pre-training approaches.
","[{'version': 'v1', 'created': 'Sun, 16 Mar 2025 00:25:13 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 13:42:31 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 12:22:00 GMT'}]",2025-03-20,"[['Zhang', 'Xiaoying', ''], ['Peng', 'Da', ''], ['Zhang', 'Yipeng', ''], ['Guo', 'Zonghao', ''], ['Wu', 'Chengyue', ''], ['Chen', 'Chi', ''], ['Ke', 'Wei', ''], ['Meng', 'Helen', ''], ['Sun', 'Maosong', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'chain-of-thought', 'label': 'Chain of thought'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'scalability', 'label': 'Scaling law'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLM', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Chain of thought,chain-of-thought,0.9539169669151306
2503.12524,Jinsik Lee,"LG AI Research, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley
  Jungkyu Choi, Yemuk Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Kijeong
  Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Hyosang Kim, Joonkee Kim,
  Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul
  Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee,
  Sangha Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, Sihyuk Yi, Hyeongu Yun",EXAONE Deep: Reasoning Enhanced Language Models,"arXiv admin note: substantial text overlap with arXiv:2412.04862,
  arXiv:2408.03541",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  We present EXAONE Deep series, which exhibits superior capabilities in
various reasoning tasks, including math and coding benchmarks. We train our
models mainly on the reasoning-specialized dataset that incorporates long
streams of thought processes. Evaluation results show that our smaller models,
EXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while
the largest model, EXAONE Deep 32B, demonstrates competitive performance
against leading open-weight models. All EXAONE Deep models are openly available
for research purposes and can be downloaded from
https://huggingface.co/LGAI-EXAONE
","[{'version': 'v1', 'created': 'Sun, 16 Mar 2025 14:39:33 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 07:09:24 GMT'}]",2025-03-20,"[['Research', 'LG AI', ''], ['Bae', 'Kyunghoon', ''], ['Choi', 'Eunbi', ''], ['Choi', 'Kibong', ''], ['Choi', 'Stanley Jungkyu', ''], ['Choi', 'Yemuk', ''], ['Hong', 'Seokhee', ''], ['Hwang', 'Junwon', ''], ['Jeon', 'Hyojin', ''], ['Jeon', 'Kijeong', ''], ['Jo', 'Gerrard Jeongwon', ''], ['Jo', 'Hyunjik', ''], ['Jung', 'Jiyeon', ''], ['Kim', 'Hyosang', ''], ['Kim', 'Joonkee', ''], ['Kim', 'Seonghwan', ''], ['Kim', 'Soyeon', ''], ['Kim', 'Sunkyoung', ''], ['Kim', 'Yireun', ''], ['Kim', 'Yongil', ''], ['Kim', 'Youchul', ''], ['Lee', 'Edward Hwayoung', ''], ['Lee', 'Haeju', ''], ['Lee', 'Honglak', ''], ['Lee', 'Jinsik', ''], ['Lee', 'Kyungmin', ''], ['Park', 'Sangha', ''], ['Park', 'Yongmin', ''], ['Yang', 'Sihoon', ''], ['Yeen', 'Heuiyeen', ''], ['Yi', 'Sihyuk', ''], ['Yun', 'Hyeongu', '']]","[{'text': 'long\nstreams of thought processes', 'label': 'Chain of thought'}]",Chain of thought,"long
streams of thought processes",0.5854611396789551
2503.12721,Luca Collini,"Luca Collini, Andrew Hennessee, Ramesh Karri, Siddharth Garg",Can Reasoning Models Reason about Hardware? An Agentic HLS Perspective,"7 pages, submitted for peer review",,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Recent Large Language Models (LLMs) such as OpenAI o3-mini and DeepSeek-R1
use enhanced reasoning through Chain-of-Thought (CoT). Their potential in
hardware design, which relies on expert-driven iterative optimization, remains
unexplored. This paper investigates whether reasoning LLMs can address
challenges in High-Level Synthesis (HLS) design space exploration and
optimization. During HLS, engineers manually define pragmas/directives to
balance performance and resource constraints. We propose an LLM-based
optimization agentic framework that automatically restructures code, inserts
pragmas, and identifies optimal design points via feedback from HLs tools and
access to integer-linear programming (ILP) solvers. Experiments compare
reasoning models against conventional LLMs on benchmarks using success rate,
efficiency, and design quality (area/latency) metrics, and provide the
first-ever glimpse into the CoTs produced by a powerful open-source reasoning
model like DeepSeek-R1.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 01:21:39 GMT'}]",2025-03-18,"[['Collini', 'Luca', ''], ['Hennessee', 'Andrew', ''], ['Karri', 'Ramesh', ''], ['Garg', 'Siddharth', '']]","[{'text': 'Chain-of-Thought', 'label': 'Chain of thought'}]",Chain of thought,Chain-of-Thought,0.9539169669151306
2503.12799,Qiong Wu,"Qiong Wu, Xiangcong Yang, Yiyi Zhou, Chenxin Fang, Baiyang Song,
  Xiaoshuai Sun, Rongrong Ji",Grounded Chain-of-Thought for Multimodal Large Language Models,,,,,cs.CV cs.MM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Despite great progress, existing multimodal large language models (MLLMs) are
prone to visual hallucination, greatly impeding their trustworthy applications.
In this paper, we study this problem from the perspective of visual-spatial
reasoning, and propose a new learning task for MLLMs, termed Grounded
Chain-of-Thought (GCoT). Different from recent visual CoT studies, which focus
more on visual knowledge reasoning, GCoT is keen to helping MLLMs to recognize
and ground the relevant visual cues step by step, thereby predicting the
correct answer with grounding coordinates as the intuitive basis. To facilitate
this task, we also carefully design and construct a dataset called multimodal
grounded chain-of-thought (MM-GCoT) consisting of 24,022 GCoT examples for
5,033 images. Besides, a comprehensive consistency evaluation system is also
introduced, including the metrics of answer accuracy, grounding accuracy and
answer-grounding consistency. We further design and conduct a bunch of
experiments on 12 advanced MLLMs, and reveal some notable findings: i. most
MLLMs performs poorly on the consistency evaluation, indicating obvious visual
hallucination; ii. visual hallucination is not directly related to the
parameter size and general multimodal performance, i.e., a larger and stronger
MLLM is not less affected by this issue. Lastly, we also demonstrate that the
proposed dataset can help existing MLLMs to well cultivate their GCoT
capability and reduce the inconsistent answering significantly. Moreover, their
GCoT can be also generalized to exiting multimodal tasks, such as open-world QA
and REC.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 04:07:47 GMT'}]",2025-03-18,"[['Wu', 'Qiong', ''], ['Yang', 'Xiangcong', ''], ['Zhou', 'Yiyi', ''], ['Fang', 'Chenxin', ''], ['Song', 'Baiyang', ''], ['Sun', 'Xiaoshuai', ''], ['Ji', 'Rongrong', '']]","[{'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'Grounded\nChain-of-Thought', 'label': 'Chain of thought'}, {'text': 'GCoT', 'label': 'Chain of thought'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'grounded chain-of-thought', 'label': 'Chain of thought'}, {'text': 'GCoT', 'label': 'Chain of thought'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'GCoT', 'label': 'Chain of thought'}]",Chain of thought,"Grounded
Chain-of-Thought",0.6417593955993652
2503.12937,Jingyi Zhang,"Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang,
  Shijian Lu, Dacheng Tao","R1-VL: Learning to Reason with Multimodal Large Language Models via
  Step-wise Group Relative Policy Optimization",,,,,cs.AI cs.CL cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent studies generally enhance MLLMs' reasoning capabilities via supervised
fine-tuning on high-quality chain-of-thought reasoning data, which often leads
models to merely imitate successful reasoning paths without understanding what
the wrong reasoning paths are. In this work, we aim to enhance the MLLMs'
reasoning ability beyond passively imitating positive reasoning paths. To this
end, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new
online reinforcement learning framework that enables MLLMs to self-improve
reasoning ability via simple, effective and dense step-wise rewarding.
Specifically, StepGRPO introduces two novel rule-based reasoning rewards:
Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity
Reward (StepRVR). StepRAR rewards the reasoning paths that contain necessary
intermediate reasoning steps via a soft key-step matching technique, while
StepRAR rewards reasoning paths that follow a well-structured and logically
consistent reasoning process through a reasoning completeness and logic
evaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series
of MLLMs with outstanding capabilities in step-by-step reasoning. Extensive
experiments over 8 benchmarks demonstrate the superiority of our methods.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 08:51:44 GMT'}]",2025-03-18,"[['Zhang', 'Jingyi', ''], ['Huang', 'Jiaxing', ''], ['Yao', 'Huanjin', ''], ['Liu', 'Shunyu', ''], ['Zhang', 'Xikun', ''], ['Lu', 'Shijian', ''], ['Tao', 'Dacheng', '']]","[{'text': 'supervised\nfine-tuning', 'label': 'Fine-tuning'}, {'text': 'chain-of-thought', 'label': 'Chain of thought'}]",Chain of thought,chain-of-thought,0.9539169669151306
2503.13055,Yu-Hong Shen,"Yu-Hong Shen, Chuan-Yu Wu, Yi-Ru Yang, Yen-Ling Tai, Yi-Ting Chen","Mitigating Cross-Modal Distraction and Ensuring Geometric Feasibility
  via Affordance-Guided, Self-Consistent MLLMs for Food Preparation Task
  Planning",,,,,cs.RO cs.AI,http://creativecommons.org/licenses/by/4.0/,"  We study Multimodal Large Language Models (MLLMs) with in-context learning
for food preparation task planning. In this context, we identify two key
challenges: cross-modal distraction and geometric feasibility. Cross-modal
distraction occurs when the inclusion of visual input degrades the reasoning
performance of a MLLM. Geometric feasibility refers to the ability of MLLMs to
ensure that the selected skills are physically executable in the environment.
To address these issues, we adapt Chain of Thought (CoT) with Self-Consistency
to mitigate reasoning loss from cross-modal distractions and use affordance
predictor as skill preconditions to guide MLLM on geometric feasibility. We
construct a dataset to evaluate the ability of MLLMs on quantity estimation,
reachability analysis, relative positioning and collision avoidance. We
conducted a detailed evaluation to identify issues among different baselines
and analyze the reasons for improvement, providing insights into each approach.
Our method reaches a success rate of 76.7% on the entire dataset, showing a
substantial improvement over the CoT baseline at 36.7%.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 11:01:02 GMT'}]",2025-03-18,"[['Shen', 'Yu-Hong', ''], ['Wu', 'Chuan-Yu', ''], ['Yang', 'Yi-Ru', ''], ['Tai', 'Yen-Ling', ''], ['Chen', 'Yi-Ting', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLM', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'Chain of Thought', 'label': 'Chain of thought'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Chain of thought,Chain of Thought,0.9999998807907104
2503.13184,Shihao Yuan,"Yuanze Li, Shihao Yuan, Haolin Wang, Qizhang Li, Ming Liu, Chen Xu,
  Guangming Shi, Wangmeng Zuo","Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided
  Visual Tokenizer and Manufacturing Process",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although recent methods have tried to introduce large multimodal models
(LMMs) into industrial anomaly detection (IAD), their generalization in the IAD
field is far inferior to that for general purposes. We summarize the main
reasons for this gap into two aspects. On one hand, general-purpose LMMs lack
cognition of defects in the visual modality, thereby failing to sufficiently
focus on defect areas. Therefore, we propose to modify the AnyRes structure of
the LLaVA model, providing the potential anomalous areas identified by existing
IAD models to the LMMs. On the other hand, existing methods mainly focus on
identifying defects by learning defect patterns or comparing with normal
samples, yet they fall short of understanding the causes of these defects.
Considering that the generation of defects is closely related to the
manufacturing process, we propose a manufacturing-driven IAD paradigm. An
instruction-tuning dataset for IAD (InstructIAD) and a data organization
approach for Chain-of-Thought with manufacturing (CoT-M) are designed to
leverage the manufacturing process for IAD. Based on the above two
modifications, we present Triad, a novel LMM-based method incorporating an
expert-guided region-of-interest tokenizer and manufacturing process for
industrial anomaly detection. Extensive experiments show that our Triad not
only demonstrates competitive performance against current LMMs but also
achieves further improved accuracy when equipped with manufacturing processes.
Source code, training data, and pre-trained models will be publicly available
at https://github.com/tzjtatata/Triad.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 13:56:57 GMT'}]",2025-03-18,"[['Li', 'Yuanze', ''], ['Yuan', 'Shihao', ''], ['Wang', 'Haolin', ''], ['Li', 'Qizhang', ''], ['Liu', 'Ming', ''], ['Xu', 'Chen', ''], ['Shi', 'Guangming', ''], ['Zuo', 'Wangmeng', '']]","[{'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'Chain-of-Thought with manufacturing', 'label': 'Chain of thought'}, {'text': 'LMMs', 'label': 'Large Language Model'}]",Chain of thought,Chain-of-Thought with manufacturing,0.6713610887527466
2503.13360,Hai-Long Sun,"Hai-Long Sun, Zhun Sun, Houwen Peng, Han-Jia Ye","Mitigating Visual Forgetting via Take-along Visual Conditioning for
  Multi-modal Long CoT Reasoning","The project page is available at
  https://sun-hailong.github.io/projects/TVC",,,,cs.CV cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advancements in Large Language Models (LLMs) have demonstrated
enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting
to advanced, product-oriented solutions like OpenAI o1. During our
re-implementation of this model, we noticed that in multimodal tasks requiring
visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to
maintain focus on the visual information, in other words, MLLMs suffer from a
gradual decline in attention to visual information as reasoning progresses,
causing text-over-relied outputs. To investigate this, we ablate image inputs
during long-chain reasoning. Concretely, we truncate the reasoning process
midway, then re-complete the reasoning process with the input image removed. We
observe only a ~2% accuracy drop on MathVista's test-hard subset, revealing the
model's textual outputs dominate the following reasoning process. Motivated by
this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts
image input to critical reasoning stages and compresses redundant visual tokens
via dynamic pruning. This methodology helps the model retain attention to the
visual components throughout the reasoning. Our approach achieves
state-of-the-art performance on average across five mathematical reasoning
benchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in
enhancing multimodal reasoning systems.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 16:45:12 GMT'}]",2025-03-18,"[['Sun', 'Hai-Long', ''], ['Sun', 'Zhun', ''], ['Peng', 'Houwen', ''], ['Ye', 'Han-Jia', '']]","[{'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'long-chain reasoning', 'label': 'Chain of thought'}, {'text': 'attention', 'label': 'Attention mechanism'}]",Chain of thought,Chain-of-Thought,0.9539169669151306
2503.13399,James Burgess,"James Burgess, Jeffrey J Nirschl, Laura Bravo-S\'anchez, Alejandro
  Lozano, Sanket Rajan Gupte, Jesus G. Galaz-Montoya, Yuhui Zhang, Yuchang Su,
  Disha Bhowmik, Zachary Coman, Sarina M. Hasan, Alexandra Johannesson, William
  D. Leineweber, Malvika G Nair, Ridhi Yarlagadda, Connor Zuraski, Wah Chiu,
  Sarah Cohen, Jan N. Hansen, Manuel D Leonetti, Chad Liu, Emma Lundberg,
  Serena Yeung-Levy","MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based
  Scientific Research","CVPR 2025 (Conference on Computer Vision and Pattern Recognition)
  Project page at https://jmhb0.github.io/microvqa Benchmark at
  https://huggingface.co/datasets/jmhb/microvqa",,,,cs.CV cs.AI cs.CL cs.LG q-bio.CB,http://creativecommons.org/licenses/by/4.0/,"  Scientific research demands sophisticated reasoning over multimodal data, a
challenge especially prevalent in biology. Despite recent advances in
multimodal large language models (MLLMs) for AI-assisted research, existing
multimodal reasoning benchmarks only target up to college-level difficulty,
while research-level benchmarks emphasize lower-level perception, falling short
of the complex multimodal reasoning needed for scientific discovery. To bridge
this gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark
designed to assess three reasoning capabilities vital in research workflows:
expert image understanding, hypothesis generation, and experiment proposal.
MicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology
experts across diverse microscopy modalities, ensuring VQA samples represent
real scientific practice. In constructing the benchmark, we find that standard
MCQ generation methods induce language shortcuts, motivating a new two-stage
pipeline: an optimized LLM prompt structures question-answer pairs into MCQs;
then, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking
on state-of-the-art MLLMs reveal a peak performance of 53\%; models with
smaller LLMs only slightly underperform top models, suggesting that
language-based reasoning is less challenging than multimodal reasoning; and
tuning with scientific articles enhances performance. Expert analysis of
chain-of-thought responses shows that perception errors are the most frequent,
followed by knowledge errors and then overgeneralization errors. These insights
highlight the challenges in multimodal scientific reasoning, showing MicroVQA
is a valuable resource advancing AI-driven biomedical research. MicroVQA is
available at https://huggingface.co/datasets/jmhb/microvqa, and project page at
https://jmhb0.github.io/microvqa.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:33:10 GMT'}]",2025-03-18,"[['Burgess', 'James', ''], ['Nirschl', 'Jeffrey J', ''], ['Bravo-Sánchez', 'Laura', ''], ['Lozano', 'Alejandro', ''], ['Gupte', 'Sanket Rajan', ''], ['Galaz-Montoya', 'Jesus G.', ''], ['Zhang', 'Yuhui', ''], ['Su', 'Yuchang', ''], ['Bhowmik', 'Disha', ''], ['Coman', 'Zachary', ''], ['Hasan', 'Sarina M.', ''], ['Johannesson', 'Alexandra', ''], ['Leineweber', 'William D.', ''], ['Nair', 'Malvika G', ''], ['Yarlagadda', 'Ridhi', ''], ['Zuraski', 'Connor', ''], ['Chiu', 'Wah', ''], ['Cohen', 'Sarah', ''], ['Hansen', 'Jan N.', ''], ['Leonetti', 'Manuel D', ''], ['Liu', 'Chad', ''], ['Lundberg', 'Emma', ''], ['Yeung-Levy', 'Serena', '']]","[{'text': 'multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'optimized LLM prompt', 'label': 'Prompting'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'chain-of-thought responses', 'label': 'Chain of thought'}]",Chain of thought,chain-of-thought responses,0.7817511558532715
2503.13988,Mykyta Syromiatnikov,"Mykyta Syromiatnikov, Victoria Ruvinskaya, Nataliia Komleva","Empowering Smaller Models: Tuning LLaMA and Gemma with Chain-of-Thought
  for Ukrainian Exam Tasks","12 pages, 6 tables, 2 figures",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Leading large language models have demonstrated impressive capabilities in
reasoning-intensive tasks, such as standardized educational testing. However,
they often require extensive training in low-resource settings with
inaccessible infrastructure. Small or compact models, though more efficient,
frequently lack sufficient support for underrepresented languages, leaving a
performance gap in critical domains. This work explores the potential of
parameter-efficient fine-tuning of compact open-weight language models to
handle reasoning-intensive tasks in the underrepresented Ukrainian language,
building on the findings of the ZNO-Eval benchmark. Parameter-efficient
fine-tuning of LLaMA 3.1 (8 billion parameters), LLaMA 3.2 (3 billion
parameters), and Gemma 2 (9 billion parameters) models on chain-of-thought
solutions resulted in a modest test score improvement of up to 17.4% on complex
matching tasks and 1.6% overall compared to tuning on answer letters alone,
offering enhanced interpretability and robustness. In addition, the proposed
tuning method with joint task topic and step-by-step solution generation
outperforms standard chain-of-thought tuning in matching tasks and provides a
5.4% gain over the best LLaMA 3.2 model due to guiding the model to recall and
apply domain-relevant information. Contrasting obtained results with zero-shot
evaluations of leading open-weight and proprietary models such as Qwen,
DeepSeek R1, OpenAI o1 and o3, Gemini, and Claude, highlight that fine-tuning
LLaMA and Gemma models with 2,032 step-by-step solutions and 20 to 50 million
trainable parameters on a single A100 GPU lets them outperform GPT-4o mini,
Mistral Large, and larger open-weight models. This research also evaluates how
merging the quantized adapter with the base model influences the generation
quality. Source code and tuned models are available at
https://github.com/NLPForUA/ZNO.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 07:44:49 GMT'}]",2025-03-19,"[['Syromiatnikov', 'Mykyta', ''], ['Ruvinskaya', 'Victoria', ''], ['Komleva', 'Nataliia', '']]","[{'text': 'parameter-efficient fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Parameter-efficient\nfine-tuning', 'label': 'Fine-tuning'}, {'text': 'LLaMA 3.2', 'label': 'Large Language Model'}, {'text': 'chain-of-thought\nsolutions', 'label': 'Chain of thought'}, {'text': 'chain-of-thought', 'label': 'Chain of thought'}, {'text': 'LLaMA 3.2', 'label': 'Large Language Model'}, {'text': 'Mistral Large', 'label': 'Mistral'}]",Chain of thought,chain-of-thought,0.9539169669151306
2503.14337,Chenxiao Yang,"Chenxiao Yang, Nathan Srebro, David McAllester, Zhiyuan Li",PENCIL: Long Thoughts with Short Memory,,,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While recent works (e.g. o1, DeepSeek R1) have demonstrated great promise of
using long Chain-of-Thought (CoT) to improve reasoning capabilities of language
models, scaling it up during test-time is challenging due to inefficient memory
usage -- intermediate computations accumulate indefinitely in context even no
longer needed for future thoughts. We propose PENCIL, which incorporates a
reduction mechanism into the autoregressive generation process, allowing the
model to recursively clean up intermediate thoughts based on patterns learned
from training. With this reduction mechanism, PENCIL significantly reduces the
maximal context length required during generation, and thus can generate longer
thoughts with limited memory, solving larger-scale problems given more thinking
time. For example, we demonstrate PENCIL achieves 97\% accuracy on the
challenging Einstein's puzzle -- a task even large models like GPT-4 struggle
with -- using only a small 25M-parameter transformer with 2048 context length.
Theoretically, we prove PENCIL can perform universal space-efficient
computation by simulating Turing machines with optimal time and space
complexity, and thus can solve arbitrary computational tasks that would
otherwise be intractable given context window constraints.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 15:14:14 GMT'}]",2025-03-19,"[['Yang', 'Chenxiao', ''], ['Srebro', 'Nathan', ''], ['McAllester', 'David', ''], ['Li', 'Zhiyuan', '']]","[{'text': 'long Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'PENCIL', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'PENCIL', 'label': 'Generative Pre-trained Transformer (GPT)'}]",Chain of thought,long Chain-of-Thought,0.8942457437515259
2503.15235,Chentian Wei,"Chentian Wei, Jiewei Chen, Jinzhu Xu",Exploring Large Language Models for Word Games:Who is the Spy?,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Word games hold significant research value for natural language processing
(NLP), game theory, and related fields due to their rule-based and situational
nature. This study explores how large language models (LLMs) can be effectively
involved in word games and proposes a training-free framework. ""Shei Shi Wo Di""
or ""Who is the Spy"" in English, is a classic word game. Using this game as an
example, we introduce a Chain-of-Thought (CoT)-based scheduling framework to
enable LLMs to achieve excellent performance in tasks such as inferring role
words and disguising their identities. We evaluate the framework's performance
based on game success rates and the accuracy of the LLM agents' analytical
results. Experimental results affirm the framework's effectiveness,
demonstrating notable improvements in LLM performance across multiple datasets.
This work highlights the potential of LLMs in mastering situational reasoning
and social interactions within structured game environments. Our code is
publicly available at https://github.com/ct-wei/Who-is-The-Spy.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 14:13:02 GMT'}]",2025-03-20,"[['Wei', 'Chentian', ''], ['Chen', 'Jiewei', ''], ['Xu', 'Jinzhu', '']]","[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Who is the Spy', 'label': 'Large Language Model'}, {'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Chain of thought,Chain-of-Thought,0.9539169669151306
2503.15268,Roberto Araya,Roberto Araya,"Do Chains-of-Thoughts of Large Language Models Suffer from
  Hallucinations, Cognitive Biases, or Phobias in Bayesian Reasoning?","24 pages, 3 figures",,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Learning to reason and carefully explain arguments is central to students'
cognitive, mathematical, and computational thinking development. This is
particularly challenging in problems under uncertainty and in Bayesian
reasoning. With the new generation of large language models (LLMs) capable of
reasoning using Chain-of-Thought (CoT), there is an excellent opportunity to
learn with them as they explain their reasoning through a dialogue with their
artificial internal voice. It is an engaging and excellent opportunity to learn
Bayesian reasoning. Furthermore, given that different LLMs sometimes arrive at
opposite solutions, CoT generates opportunities for deep learning by detailed
comparisons of reasonings. However, unlike humans, we found that they do not
autonomously explain using ecologically valid strategies like natural
frequencies, whole objects, and embodied heuristics. This is unfortunate, as
these strategies help humans avoid critical mistakes and have proven
pedagogical value in Bayesian reasoning. In order to overcome these biases and
aid understanding and learning, we included prompts that induce LLMs to use
these strategies. We found that LLMs with CoT incorporate them but not
consistently. They show persistent biases towards symbolic reasoning and
avoidance or phobia of ecologically valid strategies.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 14:44:02 GMT'}]",2025-03-20,"[['Araya', 'Roberto', '']]","[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'CoT', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'CoT', 'label': 'Chain of thought'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'CoT', 'label': 'Chain of thought'}]",Chain of thought,Chain-of-Thought,0.9539169669151306
2503.15558,Yin Cui,"NVIDIA: Alisson Azzolini, Hannah Brandon, Prithvijit Chattopadhyay,
  Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, Francesco Ferroni,
  Rama Govindaraju, Jinwei Gu, Siddharth Gururani, Imad El Hanafi, Zekun Hao,
  Jacob Huffman, Jingyi Jin, Brendan Johnson, Rizwan Khan, George Kurian, Elena
  Lantz, Nayeon Lee, Zhaoshuo Li, Xuan Li, Tsung-Yi Lin, Yen-Chen Lin, Ming-Yu
  Liu, Andrew Mathau, Yun Ni, Lindsey Pavao, Wei Ping, David W. Romero, Misha
  Smelyanskiy, Shuran Song, Lyne Tchapmi, Andrew Z. Wang, Boxin Wang, Haoxiang
  Wang, Fangyin Wei, Jiashu Xu, Yao Xu, Xiaodong Yang, Zhuolin Yang, Xiaohui
  Zeng, Zhe Zhang",Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning,,,,,cs.AI cs.CV cs.LG cs.RO,http://creativecommons.org/licenses/by/4.0/,"  Physical AI systems need to perceive, understand, and perform complex actions
in the physical world. In this paper, we present the Cosmos-Reason1 models that
can understand the physical world and generate appropriate embodied decisions
(e.g., next step action) in natural language through long chain-of-thought
reasoning processes. We begin by defining key capabilities for Physical AI
reasoning, with a focus on physical common sense and embodied reasoning. To
represent physical common sense, we use a hierarchical ontology that captures
fundamental knowledge about space, time, and physics. For embodied reasoning,
we rely on a two-dimensional ontology that generalizes across different
physical embodiments. Building on these capabilities, we develop two multimodal
large language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data
and train our models in four stages: vision pre-training, general supervised
fine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL)
as the post-training. To evaluate our models, we build comprehensive benchmarks
for physical common sense and embodied reasoning according to our ontologies.
Evaluation results show that Physical AI SFT and reinforcement learning bring
significant improvements. To facilitate the development of Physical AI, we will
make our code and pre-trained models available under the NVIDIA Open Model
License at https://github.com/nvidia-cosmos/cosmos-reason1.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 22:06:58 GMT'}]",2025-03-21,"[['NVIDIA', '', ''], [':', '', ''], ['Azzolini', 'Alisson', ''], ['Brandon', 'Hannah', ''], ['Chattopadhyay', 'Prithvijit', ''], ['Chen', 'Huayu', ''], ['Chu', 'Jinju', ''], ['Cui', 'Yin', ''], ['Diamond', 'Jenna', ''], ['Ding', 'Yifan', ''], ['Ferroni', 'Francesco', ''], ['Govindaraju', 'Rama', ''], ['Gu', 'Jinwei', ''], ['Gururani', 'Siddharth', ''], ['Hanafi', 'Imad El', ''], ['Hao', 'Zekun', ''], ['Huffman', 'Jacob', ''], ['Jin', 'Jingyi', ''], ['Johnson', 'Brendan', ''], ['Khan', 'Rizwan', ''], ['Kurian', 'George', ''], ['Lantz', 'Elena', ''], ['Lee', 'Nayeon', ''], ['Li', 'Zhaoshuo', ''], ['Li', 'Xuan', ''], ['Lin', 'Tsung-Yi', ''], ['Lin', 'Yen-Chen', ''], ['Liu', 'Ming-Yu', ''], ['Mathau', 'Andrew', ''], ['Ni', 'Yun', ''], ['Pavao', 'Lindsey', ''], ['Ping', 'Wei', ''], ['Romero', 'David W.', ''], ['Smelyanskiy', 'Misha', ''], ['Song', 'Shuran', ''], ['Tchapmi', 'Lyne', ''], ['Wang', 'Andrew Z.', ''], ['Wang', 'Boxin', ''], ['Wang', 'Haoxiang', ''], ['Wei', 'Fangyin', ''], ['Xu', 'Jiashu', ''], ['Xu', 'Yao', ''], ['Yang', 'Xiaodong', ''], ['Yang', 'Zhuolin', ''], ['Zeng', 'Xiaohui', ''], ['Zhang', 'Zhe', '']]","[{'text': 'long chain-of-thought', 'label': 'Chain of thought'}, {'text': 'physical common sense', 'label': 'Chain of thought'}, {'text': 'physical common sense', 'label': 'Chain of thought'}, {'text': 'vision pre-training', 'label': 'Fine-tuning'}, {'text': 'general supervised\nfine-tuning (SFT)', 'label': 'Fine-tuning'}, {'text': 'Physical AI SFT', 'label': 'Fine-tuning'}, {'text': 'Physical AI reinforcement learning (RL)', 'label': 'Few-shot Learning'}, {'text': 'physical common sense', 'label': 'Chain of thought'}, {'text': 'Physical AI SFT', 'label': 'Fine-tuning'}]",Chain of thought,long chain-of-thought,0.8942457437515259
2503.16013,Xiaomeng Chu,"Xiaomeng Chu, Jiajun Deng, Guoliang You, Wei Liu, Xingchen Li, Jianmin
  Ji, Yanyong Zhang","GraspCoT: Integrating Physical Property Reasoning for 6-DoF Grasping
  under Flexible Language Instructions",,,,,cs.RO cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Flexible instruction-guided 6-DoF grasping is a significant yet challenging
task for real-world robotic systems. Existing methods utilize the contextual
understanding capabilities of the large language models (LLMs) to establish
mappings between expressions and targets, allowing robots to comprehend users'
intentions in the instructions. However, the LLM's knowledge about objects'
physical properties remains underexplored despite its tight relevance to
grasping. In this work, we propose GraspCoT, a 6-DoF grasp detection framework
that integrates a Chain-of-Thought (CoT) reasoning mechanism oriented to
physical properties, guided by auxiliary question-answering (QA) tasks.
Particularly, we design a set of QA templates to enable hierarchical reasoning
that includes three stages: target parsing, physical property analysis, and
grasp action selection. Moreover, GraspCoT presents a unified multimodal LLM
architecture, which encodes multi-view observations of 3D scenes into 3D-aware
visual tokens, and then jointly embeds these visual tokens with CoT-derived
textual tokens within LLMs to generate grasp pose predictions. Furthermore, we
present IntentGrasp, a large-scale benchmark that fills the gap in public
datasets for multi-object grasp detection under diverse and indirect verbal
commands. Extensive experiments on IntentGrasp demonstrate the superiority of
our method, with additional validation in real-world robotic applications
confirming its practicality. Codes and data will be released.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 10:32:38 GMT'}]",2025-03-21,"[['Chu', 'Xiaomeng', ''], ['Deng', 'Jiajun', ''], ['You', 'Guoliang', ''], ['Liu', 'Wei', ''], ['Li', 'Xingchen', ''], ['Ji', 'Jianmin', ''], ['Zhang', 'Yanyong', '']]","[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'public\ndatasets', 'label': 'Open-source LLMs'}]",Chain of thought,Chain-of-Thought,0.9539169669151306
