id,submitter,authors,title,comments,journal-ref,doi,report-no,categories,license,abstract,versions,update_date,authors_parsed,extracted_entities,assigned_concept,matched_keyword,similarity_score
2303.11299,Takaharu Otsuka,"T. Otsuka, Y. Tsunoda, N. Shimizu, Y. Utsuno, T. Abe, H. Ueno","Prevailing Triaxial Shapes in Atomic Nuclei and a Quantum Theory of
  Rotation of Composite Objects","41 pages, 26 figures,revision from the v7 version. Additional
  discussions on practical conservation of K quantum numbers in strongly
  deformed nuclei",,,,nucl-th nucl-ex,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  In the traditional view, heavy deformed nuclei are like axially-symmetric
prolate ellipsoids, rotating about one of the short axes. In the present
picture, their shapes may be triaxial. The triaxial shape yields complex
rotations, which actually well reproduce experimental data, as confirmed by
state-of-the-art Configuration Interaction calculations. Two origins are
suggested for the triaxiality: (i) binding-energy gain by the symmetry
restoration for triaxial shapes, and (ii) another gain by specific components
of the nuclear force, like tensor force and high-multipole (e.g. hexadecupole)
central force. While the origin (i) produces basic smaller triaxiality for
virtually all deformed nuclei, the origin (ii) produces medium triaxiality for
a certain class of nuclei. An example of the former is 154Sm, a typical
showcase of axial symmetry but is now suggested to depict a modest yet finite
triaxiality. The latter, medium triaxiality, is discussed from various
viewpoints for some exemplified nuclei including 166Er, and experimental
findings. Many-body structures of the gamma band and the double-gamma band are
clarified. Regarding the general features of rotational states of deformed
many-body systems including triaxial ones, the well-known J(J+1) rule of
rotational excitation energies is discussed, within the quantum mechanical
many-body theory, without resorting to the quantization of a rotating classical
rigid body. The picture of prevailing triaxial shapes thus emerges, where the
empirically known rotational-band pattern appears with good K quantum number,
but the internal structure is dfferent from conventional picture a la A. Bohr.
The possible relations to Davydov's rigid-triaxial-rotor model are mentioned.
","[{'version': 'v1', 'created': 'Mon, 20 Mar 2023 17:38:41 GMT'}, {'version': 'v2', 'created': 'Fri, 19 May 2023 12:58:13 GMT'}, {'version': 'v3', 'created': 'Mon, 12 Jun 2023 23:52:27 GMT'}, {'version': 'v4', 'created': 'Tue, 9 Jul 2024 13:02:33 GMT'}, {'version': 'v5', 'created': 'Mon, 29 Jul 2024 09:43:56 GMT'}, {'version': 'v6', 'created': 'Mon, 30 Sep 2024 10:15:07 GMT'}, {'version': 'v7', 'created': 'Mon, 23 Dec 2024 16:48:46 GMT'}, {'version': 'v8', 'created': 'Thu, 20 Mar 2025 09:05:59 GMT'}]",2025-03-21,"[['Otsuka', 'T.', ''], ['Tsunoda', 'Y.', ''], ['Shimizu', 'N.', ''], ['Utsuno', 'Y.', ''], ['Abe', 'T.', ''], ['Ueno', 'H.', '']]","[{'text': 'quantization', 'label': 'quantisation'}]",quantisation,quantization,0.813445508480072
2306.17220,Iliya Esin,"Iliya Esin, \'Etienne Lantagne-Hurtubise, Frederik Nathan, Gil Refael","Quantum geometry and bounds on dissipation in slowly driven quantum
  systems","5 pages, 3 figures + supplementary material",,,,quant-ph cond-mat.mes-hall cond-mat.stat-mech,http://creativecommons.org/licenses/by/4.0/,"  We show that energy dissipation in slowly-driven, Markovian quantum systems
at low temperature is linked to the geometry of the driving protocol through
the quantum (or Fubini-Study) metric. Utilizing these findings, we establish
lower bounds on dissipation rates in two-tone protocols, such as those employed
for topological frequency conversion. Notably, in appropriate limits these
bounds are only determined by the topology of the protocol and an effective
quality factor of the system-bath coupling. Our results bridge topological and
geometric phenomena with energy dissipation in open quantum systems, and
further provide design principles for optimal driving protocols.
","[{'version': 'v1', 'created': 'Thu, 29 Jun 2023 18:00:03 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Jul 2023 21:00:22 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 14:37:16 GMT'}]",2025-03-21,"[['Esin', 'Iliya', ''], ['Lantagne-Hurtubise', 'Étienne', ''], ['Nathan', 'Frederik', ''], ['Refael', 'Gil', '']]","[{'text': 'quantum', 'label': 'quantisation'}, {'text': 'topological frequency conversion', 'label': 'quantisation'}]",quantisation,quantum,0.5509136915206909
2309.09545,Jiadong Liang,"Xiang Li, Jiadong Liang, Xinyun Chen and Zhihua Zhang","Convergence and Inference of Stream SGD, with Applications to Queueing
  Systems and Inventory Control",,,,,math.OC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Stream stochastic gradient descent (SGD) is a simple and efficient method for
solving online optimization problems in operations research (OR), where data is
generated by parameter-dependent Markov chains. Unlike traditional approaches
which require increasing batch sizes during iterations, stream SGD uses a
single sample per iteration, significantly improving sample efficiency. This
paper establishes a systematic framework for analyzing stream SGD, leveraging
the Poisson equation solution to address gradient bias and statistical
dependence. We prove optimal O(1/T) convergence rates and the state-of-the-art
O(log T) regret, while also introducing an online inference method for
uncertainty quantification and supporting it by a novel functional central
limit theorem. We propose a novel Wasserstein-type divergence to describe the
framework's conditions, which makes the assumptions in question directly
verified via coupling techniques tailored to underlying OR models. We consider
applications in queueing systems and inventory management, demonstrating the
practicality and broad relevance, as well as providing new insights into the
effectiveness of stream SGD in OR fields.
","[{'version': 'v1', 'created': 'Mon, 18 Sep 2023 07:42:47 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 02:39:34 GMT'}]",2025-03-19,"[['Li', 'Xiang', ''], ['Liang', 'Jiadong', ''], ['Chen', 'Xinyun', ''], ['Zhang', 'Zhihua', '']]","[{'text': 'uncertainty quantification', 'label': 'quantisation'}]",quantisation,uncertainty quantification,0.571454644203186
2402.01549,Meng Ruoyu,Ruoyu Meng and Aditya Ramamoorthy,"Quantum advantage in zero-error function computation with side
  information","Added about 16 more pages about discussion in variable-length
  classical codes (VLC): Problem setting, optimal rate, comparison with quantum
  code",,,,cs.IT math.CO math.IT quant-ph,http://creativecommons.org/licenses/by/4.0/,"  We consider the problem of zero-error function computation with side
information. Alice and Bob have correlated sources $X,Y$ with joint p.m.f.
$p_{XY}(\cdot, \cdot)$. Bob wants to calculate $f(X,Y)$ with zero error. Alice
encodes $m$-length blocks $(m \geq 1)$ of her observations to Bob over
error-free channels, which can be classical or quantum. We consider two
classical settings. (i) Alice communicates via a fixed length code (FLC), and
(ii) Alice communicates via a variable length code (VLC). In the FLC scenario,
the minimum communication rate depends on the asymptotic growth of the
chromatic number of an appropriately defined $m$-instance ``confusion graph''
$G^{(m)}$. In the VLC scenario, the corresponding rate is characterized by the
asymptotics of the chromatic entropy of $G^{(m)}$. %and has single-letter
characterization in terms of K\""orner's graph entropy if $G^{(m)}$ is $m$-times
graph OR product. In the quantum setting, we only consider fixed length codes;
the corresponding rate depends on the asymptotic growth of the orthogonal rank
of the complement of $G^{(m)}$. The behavior of the communication rates depends
critically on $G^{(m)}$, which is shown to be sandwiched between $G^{\boxtimes
m}$ ($m$-times strong product) and $G^{\lor m}$ ($m$-times OR product)
respectively. Our work presents necessary and sufficient conditions on the
function $f(\cdot, \cdot)$ and joint p.m.f. $p_{XY}(\cdot,\cdot)$ such that
$G^{(m)}$ equals either $G^{\boxtimes m}$ or $G^{\lor m}$. Our work explores
the multitude of possible behaviors of the quantum and classical (FLC/VLC)
rates in the single-instance case and the asymptotic (in $m$) case for several
classes of confusion graphs.
","[{'version': 'v1', 'created': 'Fri, 2 Feb 2024 16:41:36 GMT'}, {'version': 'v2', 'created': 'Mon, 4 Mar 2024 23:02:26 GMT'}, {'version': 'v3', 'created': 'Tue, 14 Jan 2025 23:45:09 GMT'}, {'version': 'v4', 'created': 'Wed, 19 Mar 2025 17:16:54 GMT'}]",2025-03-20,"[['Meng', 'Ruoyu', ''], ['Ramamoorthy', 'Aditya', '']]","[{'text': 'Alice', 'label': 'ALBERT'}, {'text': 'Bob', 'label': 'ALBERT'}, {'text': 'Alice', 'label': 'ALBERT'}, {'text': 'Bob', 'label': 'ALBERT'}, {'text': 'quantum', 'label': 'quantisation'}, {'text': 'Alice', 'label': 'ALBERT'}, {'text': 'Alice', 'label': 'ALBERT'}, {'text': 'quantum', 'label': 'quantisation'}]",quantisation,quantum,0.5509136915206909
2405.14304,Mojtaba Bemana,"Mojtaba Bemana, Thomas Leimk\""uhler, Karol Myszkowski, Hans-Peter
  Seidel, Tobias Ritschel",Bracket Diffusion: HDR Image Generation by Consistent LDR Denoising,"11 pages, 14 figures, Accepted to Eurographics 2025, see
  https://bracketdiffusion.mpi-inf.mpg.de",,,,cs.GR cs.CV eess.IV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We demonstrate generating HDR images using the concerted action of multiple
black-box, pre-trained LDR image diffusion models. Relying on a pre-trained LDR
generative diffusion models is vital as, first, there is no sufficiently large
HDR image dataset available to re-train them, and, second, even if it was,
re-training such models is impossible for most compute budgets. Instead, we
seek inspiration from the HDR image capture literature that traditionally fuses
sets of LDR images, called ""exposure brackets'', to produce a single HDR image.
We operate multiple denoising processes to generate multiple LDR brackets that
together form a valid HDR result. The key to making this work is to introduce a
consistency term into the diffusion process to couple the brackets such that
they agree across the exposure range they share while accounting for possible
differences due to the quantization error. We demonstrate state-of-the-art
unconditional and conditional or restoration-type (LDR2HDR) generative modeling
results, yet in HDR.
","[{'version': 'v1', 'created': 'Thu, 23 May 2024 08:24:22 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 14:54:28 GMT'}]",2025-03-19,"[['Bemana', 'Mojtaba', ''], ['Leimkühler', 'Thomas', ''], ['Myszkowski', 'Karol', ''], ['Seidel', 'Hans-Peter', ''], ['Ritschel', 'Tobias', '']]","[{'text': 'quantization error', 'label': 'quantisation'}]",quantisation,quantization error,0.6830846667289734
2405.15741,Travis Whyte,"Travis Whyte, David J. Wilson and Christopher E. Thomas","Near-threshold states in coupled $DD^{\ast}-D^{\ast}D^{\ast}$ scattering
  from lattice QCD","31 pages, 13 figures, 14 tables","Phys. Rev. D 111, 034511 (2025)",10.1103/PhysRevD.111.034511,,hep-lat hep-ph,http://creativecommons.org/licenses/by/4.0/,"  The first determination of doubly-charmed isospin-0 coupled-channel
$DD^\ast-D^\ast D^\ast$ scattering amplitudes from lattice QCD is presented.
The finite-volume spectrum is computed for three lattice volumes with a
light-quark mass corresponding to $m_\pi\approx 391$ MeV and is used to extract
the scattering amplitudes in $J^P = 1^+$ via the L\""{u}scher quantization
condition. By analytically continuing the scattering amplitudes to complex
energies, a $T_{cc}$ pole corresponding to a virtual bound state is found below
$DD^\ast$ threshold. We also find a second pole, $T_{cc}^\prime$, corresponding
to a resonance pole below the kinematically closed $D^\ast D^\ast$ channel, to
which it has a strong coupling. A non-zero coupling is robustly found between
the $S$-wave $D D^\ast$ and $D^\ast D^\ast$ channels producing a clear cusp in
the $D D^\ast$ amplitude at the $D^\ast D^\ast$ threshold energy. This suggests
that the experimental $T_{cc}^\prime$ should be observable in $D D^\ast$ and
$D^\ast D^\ast$ final states at ongoing experiments.
","[{'version': 'v1', 'created': 'Fri, 24 May 2024 17:36:34 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:47:32 GMT'}]",2025-03-20,"[['Whyte', 'Travis', ''], ['Wilson', 'David J.', ''], ['Thomas', 'Christopher E.', '']]","[{'text': 'lattice QCD', 'label': 'quantisation'}, {'text': 'L\\""{u}scher quantization\ncondition', 'label': 'quantisation'}]",quantisation,"L\""{u}scher quantization
condition",0.5030887126922607
2407.19690,Jun-Hui Zheng,"Xi-Yu Chen, Lijia Jiang, Wen-Kai Bai, Tao Yang, and Jun-Hui Zheng","Synthetic half-integer magnetic monopole and single-vortex dynamics in
  spherical Bose-Einstein condensates","11 pages, 4 figures, 1 table",,,,cond-mat.quant-gas nlin.PS quant-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Magnetic monopoles are crucial in explaining the quantization of electric
charges and quantum Hall effects, while artificially creating a minimal
magnetic monopole in experiments remains a challenge. Here, we come up with a
flexible way to simulate a half-integer-type monopole in Bose gases and
investigate the induced vortex dynamics on a sphere. We list the possible
experiment parameter settings for different isotopes and discuss their
experimental feasibility. With the assumption of a rigid monopole-vortex
structure, we analytically predict the vortex trajectory in an external
magnetic field. We then confirm the result by numerically solving the
Gross-Pitaevskii equation, which employs two gauges simultaneously (the Wu-Yang
approach) to prevent singularity in the one-gauge method when a monopole is
present. The study offers significant insight into the characteristics of
monopoles and vortices, facilitating avenues for experimental validation.
","[{'version': 'v1', 'created': 'Mon, 29 Jul 2024 04:20:24 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 01:50:25 GMT'}]",2025-03-21,"[['Chen', 'Xi-Yu', ''], ['Jiang', 'Lijia', ''], ['Bai', 'Wen-Kai', ''], ['Yang', 'Tao', ''], ['Zheng', 'Jun-Hui', '']]","[{'text': 'quantization', 'label': 'quantisation'}]",quantisation,quantization,0.813445508480072
2409.01402,Nikola Herceg,"Nikola Herceg, Tajron Juri\'c, A. Naveena Kumara, Andjelo Samsarov,
  Ivica Smoli\'c",Noncommutative quasinormal modes of Schwarzschild black hole,"38 pages, 6 figures, 23 tables; improved version, typos corrected,
  references added",,,"RBI-ThPhys-2024-16, ZTF-EP-24-07",gr-qc hep-th,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study gravitational perturbations of the Schwarzschild metric in the
context of noncommutative gravity. $r-\varphi$ and $r-t$ noncommutativity are
introduced through a Moyal twist of the Hopf algebra of diffeomorphisms.
Differential geometric structures such as curvature tensors are also twisted.
Noncommutative equations of motion are derived from the recently proposed NC
vacuum Einstein equation. Here, in addition to previously calculated axial NC
potential, we present the polar solution which generalizes the work done by
Zerilli. Quasinormal mode frequencies of the two potentials are calculated
using three methods: WKB, P\""oschl-Teller and Rosen-Morse. Notably, we apply
the WKB method up to the 13th order and determine the optimal order for each
noncommutative parameter value individually. Additionally, we provide
comprehensive error estimations for the higher-order WKB calculations, offering
insights into the accuracy of our results. By comparing the spectra, we
conclude that the classical isospectrality of axial and polar modes is broken
upon spacetime quantization. Isospectrality is restored in the eikonal limit.
","[{'version': 'v1', 'created': 'Mon, 2 Sep 2024 18:00:01 GMT'}, {'version': 'v2', 'created': 'Fri, 13 Sep 2024 20:47:18 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 22:14:59 GMT'}]",2025-03-20,"[['Herceg', 'Nikola', ''], ['Jurić', 'Tajron', ''], ['Kumara', 'A. Naveena', ''], ['Samsarov', 'Andjelo', ''], ['Smolić', 'Ivica', '']]","[{'text': 'spacetime quantization', 'label': 'quantisation'}]",quantisation,spacetime quantization,0.663964033126831
2412.10979,Ying Wang,Ying Wang and Jian Guo and Yanlong Zhao and Ji-feng Zhang,"Distributed Estimation with Quantized Measurements and Communication
  over Markovian Switching Topologies","17 pages, 7 figures, submitted to Automatica",,,,eess.SY cs.SY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper addresses distributed parameter estimation in stochastic dynamic
systems with quantized measurements, constrained by quantized communication and
Markovian switching directed topologies. To enable accurate recovery of the
original signal from quantized communication signal, a persistent
excitation-compliant linear compression encoding method is introduced.
Leveraging this encoding, this paper proposes an estimation-fusion type
quantized distributed identification algorithm under a stochastic approximation
framework. The algorithm operates in two phases: first, it estimates
neighboring estimates using quantized communication information, then it
creates a fusion estimate by combining these estimates through a
consensus-based distributed stochastic approximation approach. To tackle the
difficulty caused by the coupling between these two estimates, two combined
Lyapunov functions are constructed to analyze the convergence performance.
Specifically, the mean-square convergence of the estimates is established under
a conditional expectation-type cooperative excitation condition and the union
topology containing a spanning tree. Besides, the convergence rate is derived
to match the step size's order under suitable step-size coefficients.
Furthermore, the impact of communication uncertainties including stochastic
communication noise and Markov-switching rate is analyzed on the convergence
rate. A numerical example illustrates the theoretical findings and highlights
the joint effect of sensors under quantized communication.
","[{'version': 'v1', 'created': 'Sat, 14 Dec 2024 21:52:42 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 14:30:04 GMT'}]",2025-03-18,"[['Wang', 'Ying', ''], ['Guo', 'Jian', ''], ['Zhao', 'Yanlong', ''], ['Zhang', 'Ji-feng', '']]","[{'text': 'quantized communication', 'label': 'quantisation'}, {'text': 'quantized communication', 'label': 'quantisation'}]",quantisation,quantized communication,0.5989185571670532
2501.03318,Igor Shovkovy,Ritesh Ghosh and Igor A. Shovkovy,Neutrino energy and momentum emission from magnetized dense quark matter,"31 pages, 7 multi-panel figures, v2: added several discussions; to
  appear in JHEP",,,,hep-ph hep-th nucl-th,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Using first-principles field-theoretic methods, we investigate neutrino
emission from strongly magnetized dense quark matter under conditions relevant
to compact stars. We develop a customized approximation that fully accounts for
the Landau-level quantization of electron states while neglecting such
quantization for quarks. This approach is well-justified in dense quark matter,
where the chemical potentials of up and down quarks significantly exceed those
of electrons. Our analysis provides a detailed exploration of the influence of
strong magnetic fields on neutrino emission, including both the modification of
the total emission rate and the emergence of emission asymmetry relative to the
magnetic field direction. We further examine the role of temperature in
smoothing the oscillatory behavior of neutrino emission as a function of
magnetic field strength. Additionally, we study the interplay between the
Landau-level quantization of electrons and the Fermi-liquid effects of quarks
in modifying the phase space of relevant weak processes. Finally, we briefly
discuss the broader implications of magnetic fields on stellar cooling
processes and the potential contribution of asymmetric neutrino emission to
pulsar kicks.
","[{'version': 'v1', 'created': 'Mon, 6 Jan 2025 19:00:01 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 02:22:02 GMT'}]",2025-03-21,"[['Ghosh', 'Ritesh', ''], ['Shovkovy', 'Igor A.', '']]","[{'text': 'Landau-level quantization', 'label': 'quantisation'}, {'text': 'Landau-level quantization', 'label': 'quantisation'}]",quantisation,Landau-level quantization,0.5439216494560242
2501.06377,Ronald Caplan,"Ronald M. Caplan, Miko M. Stulajter, Jon A. Linker, Cooper Downs, Lisa
  A. Upton, Bibhuti Kumar Jha, Raphael Attie, Charles N. Arge, Carl J. Henney","Open-source Flux Transport (OFT). I. HipFT -- High-performance Flux
  Transport","32 pages, 16 figures",,,,astro-ph.SR physics.comp-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Global solar photospheric magnetic maps play a critical role in solar and
heliospheric physics research. Routine magnetograph measurements of the field
occur only along the Sun-Earth line, leaving the far-side of the Sun
unobserved. Surface Flux Transport (SFT) models attempt to mitigate this by
modeling the surface evolution of the field. While such models have long been
established in the community (with several releasing public full-Sun maps),
none are open source. The Open Source Flux Transport (OFT) model seeks to fill
this gap by providing an open and user-extensible SFT model that also builds on
the knowledge of previous models with updated numerical and data
acquisition/assimilation methods along with additional user-defined features.
In this first of a series of papers on OFT, we introduce its computational
core: the High-performance Flux Transport (HipFT) code
(github.com/predsci/hipft). HipFT implements advection, diffusion, and data
assimilation in a modular design that supports a variety of flow models and
options. It can compute multiple realizations in a single run across model
parameters to create ensembles of maps for uncertainty quantification and is
high-performance through the use of multi-CPU and multi-GPU parallelism. HipFT
is designed to enable users to easily write extensions, enhancing its
flexibility and adaptability. We describe HipFT's model features, validations
of its numerical methods, performance of its parallel and GPU-accelerated code
implementation, analysis/post-processing options, and example use cases.
","[{'version': 'v1', 'created': 'Fri, 10 Jan 2025 22:55:45 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 16:40:26 GMT'}]",2025-03-18,"[['Caplan', 'Ronald M.', ''], ['Stulajter', 'Miko M.', ''], ['Linker', 'Jon A.', ''], ['Downs', 'Cooper', ''], ['Upton', 'Lisa A.', ''], ['Jha', 'Bibhuti Kumar', ''], ['Attie', 'Raphael', ''], ['Arge', 'Charles N.', ''], ['Henney', 'Carl J.', '']]","[{'text': 'uncertainty quantification', 'label': 'quantisation'}]",quantisation,uncertainty quantification,0.571454644203186
2501.08238,Xuanjun Chen,"Xuanjun Chen, Jiawei Du, Haibin Wu, Lin Zhang, I-Ming Lin, I-Hsiang
  Chiu, Wenze Ren, Yuan Tseng, Yu Tsao, Jyh-Shing Roger Jang, Hung-yi Lee","CodecFake+: A Large-Scale Neural Audio Codec-Based Deepfake Speech
  Dataset",Work in Progress,,,,cs.SD eess.AS,http://creativecommons.org/licenses/by/4.0/,"  With the rapid advancement of neural audio codecs, codec-based speech
generation (CoSG) systems have become highly powerful. Unfortunately, CoSG also
enables the creation of highly realistic deepfake speech, making it easier to
mimic an individual's voice and spread misinformation. We refer to this
emerging deepfake speech generated by CoSG systems as CodecFake. Detecting such
CodecFake is an urgent challenge, yet most existing systems primarily focus on
detecting fake speech generated by traditional speech synthesis models. In this
paper, we introduce CodecFake+, a large-scale dataset designed to advance
CodecFake detection. To our knowledge, CodecFake+ is the largest dataset
encompassing the most diverse range of codec architectures. The training set is
generated through re-synthesis using 31 publicly available open-source codec
models, while the evaluation set includes web-sourced data from 17 advanced
CoSG models. We also propose a comprehensive taxonomy that categorizes codecs
by their root components: vector quantizer, auxiliary objectives, and decoder
types. Our proposed dataset and taxonomy enable detailed analysis at multiple
levels to discern the key factors for successful CodecFake detection. At the
individual codec level, we validate the effectiveness of using codec
re-synthesized speech (CoRS) as training data for large-scale CodecFake
detection. At the taxonomy level, we show that detection performance is
strongest when the re-synthesis model incorporates disentanglement auxiliary
objectives or a frequency-domain decoder. Furthermore, from the perspective of
using all the CoRS training data, we show that our proposed taxonomy can be
used to select better training data for improving detection performance.
Overall, we envision that CodecFake+ will be a valuable resource for both
general and fine-grained exploration to develop better anti-spoofing models
against CodecFake.
","[{'version': 'v1', 'created': 'Tue, 14 Jan 2025 16:26:14 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 22:22:05 GMT'}]",2025-03-19,"[['Chen', 'Xuanjun', ''], ['Du', 'Jiawei', ''], ['Wu', 'Haibin', ''], ['Zhang', 'Lin', ''], ['Lin', 'I-Ming', ''], ['Chiu', 'I-Hsiang', ''], ['Ren', 'Wenze', ''], ['Tseng', 'Yuan', ''], ['Tsao', 'Yu', ''], ['Jang', 'Jyh-Shing Roger', ''], ['Lee', 'Hung-yi', '']]","[{'text': 'vector quantizer', 'label': 'quantisation'}]",quantisation,vector quantizer,0.5825115442276001
2503.01639,Sueda Taner,"Sueda Taner, Ziyi Wang, and Christoph Studer",Cauchy-Schwarz Regularizers,Accepted to ICLR 2025,,,,math.OC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce a novel class of regularization functions, called Cauchy-Schwarz
(CS) regularizers, which can be designed to induce a wide range of properties
in solution vectors of optimization problems. To demonstrate the versatility of
CS regularizers, we derive regularization functions that promote
discrete-valued vectors, eigenvectors of a given matrix, and orthogonal
matrices. The resulting CS regularizers are simple, differentiable, and can be
free of spurious stationary points, making them suitable for gradient-based
solvers and large-scale optimization problems. In addition, CS regularizers
automatically adapt to the appropriate scale, which is, for example, beneficial
when discretizing the weights of neural networks. To demonstrate the efficacy
of CS regularizers, we provide results for solving underdetermined systems of
linear equations and weight quantization in neural networks. Furthermore, we
discuss specializations, variations, and generalizations, which lead to an even
broader class of new and possibly more powerful regularizers.
","[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 15:19:16 GMT'}, {'version': 'v2', 'created': 'Fri, 7 Mar 2025 14:31:52 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 10:01:57 GMT'}]",2025-03-18,"[['Taner', 'Sueda', ''], ['Wang', 'Ziyi', ''], ['Studer', 'Christoph', '']]","[{'text': 'CS regularizers', 'label': 'LLMs'}, {'text': 'weight quantization', 'label': 'quantisation'}]",quantisation,weight quantization,0.6230819225311279
2503.08154,Tian Jin,"Tian Jin, Enjun Du, Changwei Wang, Wenhao Xu, Ding Luo","Structure-Activation Synergy: A Dual Efficiency Framework for
  Parameter-Memory Optimized Transfer Learning",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While parameter-efficient transfer learning (PETL) successfully reduces
trainable parameters for adapting large pre-trained models, conventional
methods exhibit limited effectiveness in decreasing activation memory
consumption - a critical bottleneck for deployment on resource-constrained
devices. We present Structure-Activation Synergy (S2A), an innovative framework
achieving dual optimization of parameters and memory through two synergistic
mechanisms: (1) Structural activation modules (bias/prompt/side adaptations)
that strategically minimize both parametric complexity and intermediate feature
storage requirements, and (2) Derivative-aware 4-bit quantization for
non-parametric operators that maintains model fidelity through
gradient-informed precision allocation. Extensive evaluations across multiple
architectures (ViT, Swin, ResNet) and datasets (ImageNet-1K, CIFAR, DomainNet)
demonstrate S2A's superior efficiency, reducing GPU memory consumption by 75\%
(4.2 average reduction) while maintaining 98.7\% of full fine-tuning accuracy
with only 0.9\% tunable parameters. This hardware-aware paradigm establishes
new state-of-the-art in efficient model adaptation, offering practical
deployment advantages through simultaneous parameter and memory optimization
without compromising model capability
","[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 08:10:03 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 16:50:29 GMT'}]",2025-03-18,"[['Jin', 'Tian', ''], ['Du', 'Enjun', ''], ['Wang', 'Changwei', ''], ['Xu', 'Wenhao', ''], ['Luo', 'Ding', '']]","[{'text': 'Derivative-aware 4-bit quantization', 'label': 'quantisation'}]",quantisation,Derivative-aware 4-bit quantization,0.5734385848045349
2503.09615,Pablo Arnault,Pablo Arnault,"Canonical quantization of the complex scalar field without making use of
  its real and imaginary parts",38 pages,,,,physics.gen-ph,http://creativecommons.org/licenses/by-sa/4.0/,"  We proceed to the canonical quantization of the complex scalar field without
making use of its real and imaginary parts. Our motivation is to formally
connect, as tightly as possible, the quantum-field notions of particle and
antiparticle - most prominently represented, formally, by creation and
annihilation operators - to the initial classical field theory - whose main
formal object is the field amplitude at a given spacetime point. Our point of
view is that doing this via the use of the real and imaginary parts of the
field is not satisfying. The derivation demands to consider, just before
quantization, the field and its complex conjugate as independent fields, which
yields a system of two copies of independent complex scalar fields. One then
proceeds to quantization with these two copies, which leads to the introduction
of two families of creation and annihilation operators, corresponding to
particles on the one hand, and antiparticles on the other hand. One realizes
that having two such families is the only hope for being able to ""invert"" the
definitions of the creation and annihilation in terms of the Fourier quantized
fields, so as to obtain an expression of the direct-space fields in terms of
these creation and annihilation operators, because the real-field condition
used in the case of a real scalar field does not hold for a complex scalar
field. This hope is then met by introducing the complex-conjugate constraint at
the quantum level, that is, that the second independent field copy is actually
the complex conjugate of the first. All standard results are then recovered in
a rigorous and purely deductive way. While we reckon our derivation exists in
the literature, we have not found it.
","[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 01:30:16 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 16:39:19 GMT'}]",2025-03-18,"[['Arnault', 'Pablo', '']]","[{'text': 'canonical quantization', 'label': 'quantisation'}, {'text': 'quantization', 'label': 'quantisation'}, {'text': 'quantization', 'label': 'quantisation'}]",quantisation,quantization,0.813445508480072
2503.09975,Joonhyung Lee,"Joonhyung Lee, Shmulik Markovich-Golan, Daniel Ohayon, Yair Hanani,
  Gunho Park, Byeongwook Kim, Asaf Karnieli, Uri Livne, Haihao Shen, Tai Huang,
  Se Jung Kwon, Dongsoo Lee",Faster Inference of LLMs using FP8 on the Intel Gaudi,,,,,cs.AR,http://creativecommons.org/licenses/by/4.0/,"  Low-precision data types are essential in modern neural networks during both
training and inference as they enhance throughput and computational capacity by
better exploiting available hardware resources. Despite the incorporation of
FP8 in commercially available neural network accelerators, a comprehensive
exposition of its underlying mechanisms, along with rigorous performance and
accuracy evaluations, is still lacking. In this work, we contribute in three
significant ways. First, we analyze the implementation details and quantization
options associated with FP8 for inference on the Intel Gaudi AI accelerator.
Second, we empirically quantify the throughput improvements afforded by the use
of FP8 at both the operator level and in end-to-end scenarios. Third, we assess
the accuracy impact of various FP8 quantization methods. Our experimental
results indicate that the Intel Gaudi 2 accelerator consistently achieves high
computational unit utilization, frequently exceeding 90% MFU, while incurring
an accuracy degradation of less than 1%.
","[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 02:21:39 GMT'}, {'version': 'v2', 'created': 'Fri, 14 Mar 2025 01:57:02 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 03:04:50 GMT'}]",2025-03-18,"[['Lee', 'Joonhyung', ''], ['Markovich-Golan', 'Shmulik', ''], ['Ohayon', 'Daniel', ''], ['Hanani', 'Yair', ''], ['Park', 'Gunho', ''], ['Kim', 'Byeongwook', ''], ['Karnieli', 'Asaf', ''], ['Livne', 'Uri', ''], ['Shen', 'Haihao', ''], ['Huang', 'Tai', ''], ['Kwon', 'Se Jung', ''], ['Lee', 'Dongsoo', '']]","[{'text': 'quantization\noptions', 'label': 'quantisation'}, {'text': 'quantization methods', 'label': 'quantisation'}]",quantisation,"quantization
options",0.7135424613952637
2503.10199,Ruibiao Song,"Ruibiao Song, Liying Zhang","Optimal Estimation and Uncertainty Quantification for Stochastic Inverse
  Problems via Variational Bayesian Methods",,,,,math.NA cs.NA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Bayesian inversion method demonstrates significant potential for solving
inverse problems, enabling both point estimation and uncertainty
quantification. However, Bayesian maximum a posteriori (MAP) estimation may
become unstable when handling data from diverse distributions (e.g., solutions
of stochastic partial differential equations (SPDEs)). Additionally, Monte
Carlo sampling methods are computationally expensive. To address these
challenges, we propose a novel two-stage optimization method based on optimal
control theory and variational Bayesian methods. This method not only achieves
stable solutions for stochastic inverse problems but also efficiently
quantifies the uncertainty of the solutions. In the first stage, we introduce a
new weighting formulation to ensure the stability of the Bayesian MAP
estimation. In the second stage, we derive the necessary condition to
efficiently quantify the uncertainty of the solutions, by combining the new
weighting formula with variational inference. Furthermore, we establish an
error estimation theorem that relates the exact solution to the optimally
estimated solution under different amounts of observed data. Finally, the
efficiency of the proposed method is demonstrated through numerical examples.
","[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 09:34:33 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 02:20:35 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 08:23:28 GMT'}, {'version': 'v4', 'created': 'Wed, 19 Mar 2025 10:23:49 GMT'}]",2025-03-20,"[['Song', 'Ruibiao', ''], ['Zhang', 'Liying', '']]","[{'text': 'uncertainty\nquantification', 'label': 'quantisation'}]",quantisation,"uncertainty
quantification",0.571454644203186
2503.13089,Baohao Liao,"Baohao Liao and Christian Herold and Seyyed Hadi Hashemi and Stefan
  Vasilev and Shahram Khadivi and Christof Monz","ClusComp: A Simple Paradigm for Model Compression and Efficient
  Finetuning","26 pages, 11 figures, 18 tables",,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  As large language models (LLMs) scale, model compression is crucial for edge
deployment and accessibility. Weight-only quantization reduces model size but
suffers from performance degradation at lower bit widths. Moreover, standard
finetuning is incompatible with quantized models, and alternative methods often
fall short of full finetuning. In this paper, we propose ClusComp, a simple yet
effective compression paradigm that clusters weight matrices into codebooks and
finetunes them block-by-block. ClusComp (1) achieves superior performance in
2-4 bit quantization, (2) pushes compression to 1-bit while outperforming
ultra-low-bit methods with minimal finetuning, and (3) enables efficient
finetuning, even surpassing existing quantization-based approaches and rivaling
full FP16 finetuning. Notably, ClusComp supports compression and finetuning of
70B LLMs on a single A6000-48GB GPU.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 11:52:16 GMT'}]",2025-03-18,"[['Liao', 'Baohao', ''], ['Herold', 'Christian', ''], ['Hashemi', 'Seyyed Hadi', ''], ['Vasilev', 'Stefan', ''], ['Khadivi', 'Shahram', ''], ['Monz', 'Christof', '']]","[{'text': 'Weight-only quantization', 'label': 'quantisation'}, {'text': 'standard\nfinetuning', 'label': 'Fine-tuning'}, {'text': '2-4 bit quantization', 'label': 'quantisation'}, {'text': 'finetuning', 'label': 'Fine-tuning'}]",quantisation,2-4 bit quantization,0.6378128528594971
2503.13630,"Eren Volkan K\""u\c{c}\""uk","Eren Volkan K\""u\c{c}\""uk","The Birth of Quantum Mechanics: A Historical Study Through the Canonical
  Papers",,,,,physics.hist-ph quant-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper explores the historical development of the theory of quantum
mechanics between 1900 and 1927 by chronological examination of the
foundational papers and ideas. Beginning with Planck's introduction of energy
quantisation in blackbody radiation, we follow the emergence of Einstein's
light quanta hypothesis, Bohr's atomic model, and the statistical implications
of indistinguishable particles. Special emphasis is placed on the transition
from the Old Quantum Theory to modern quantum mechanics, particularly through
Heisenberg's matrix mechanics and Schr\""{o}dinger's wave mechanics. This study
aims to provide a structured historical account, offering insights into the
conceptual transformations that led to quantum mechanics while making the
development accessible to physicists, historians of science, and advanced
students interested in the origins of modern quantum theory.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 18:26:50 GMT'}]",2025-03-19,"[['Küçük', 'Eren Volkan', '']]","[{'text': 'energy\nquantisation', 'label': 'quantisation'}, {'text': ""Bohr's atomic model"", 'label': 'Foundation Model'}]",quantisation,"energy
quantisation",0.687409520149231
2503.13909,Pavia Bera,Pavia Bera and Sanjukta Bhanja,"Quantification of Uncertainties in Probabilistic Deep Neural Network by
  Implementing Boosting of Variational Inference",,,,,cs.LG stat.ML,http://creativecommons.org/licenses/by/4.0/,"  Modern neural network architectures have achieved remarkable accuracies but
remain highly dependent on their training data, often lacking interpretability
in their learned mappings. While effective on large datasets, they tend to
overfit on smaller ones. Probabilistic neural networks, such as those utilizing
variational inference, address this limitation by incorporating uncertainty
estimation through weight distributions rather than point estimates. However,
standard variational inference often relies on a single-density approximation,
which can lead to poor posterior estimates and hinder model performance. We
propose Boosted Bayesian Neural Networks (BBNN), a novel approach that enhances
neural network weight distribution approximations using Boosting Variational
Inference (BVI). By iteratively constructing a mixture of densities, BVI
expands the approximating family, enabling a more expressive posterior that
leads to improved generalization and uncertainty estimation. While this
approach increases computational complexity, it significantly enhances accuracy
an essential tradeoff, particularly in high-stakes applications such as medical
diagnostics, where false negatives can have severe consequences. Our
experimental results demonstrate that BBNN achieves ~5% higher accuracy
compared to conventional neural networks while providing superior uncertainty
quantification. This improvement highlights the effectiveness of leveraging a
mixture-based variational family to better approximate the posterior
distribution, ultimately advancing probabilistic deep learning.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 05:11:21 GMT'}]",2025-03-19,"[['Bera', 'Pavia', ''], ['Bhanja', 'Sanjukta', '']]","[{'text': 'uncertainty\nquantification', 'label': 'quantisation'}]",quantisation,"uncertainty
quantification",0.571454644203186
2503.13917,Yujia Tong,"Yujia Tong, Yuze Wang, Jingling Yuan, Chuang Hu","Robust Machine Unlearning for Quantized Neural Networks via Adaptive
  Gradient Reweighting with Similar Labels","15 pages, 4 figures",,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Model quantization enables efficient deployment of deep neural networks on
edge devices through low-bit parameter representation, yet raises critical
challenges for implementing machine unlearning (MU) under data privacy
regulations. Existing MU methods designed for full-precision models fail to
address two fundamental limitations in quantized networks: 1) Noise
amplification from label mismatch during data processing, and 2) Gradient
imbalance between forgotten and retained data during training. These issues are
exacerbated by quantized models' constrained parameter space and discrete
optimization. We propose Q-MUL, the first dedicated unlearning framework for
quantized models. Our method introduces two key innovations: 1) Similar Labels
assignment replaces random labels with semantically consistent alternatives to
minimize noise injection, and 2) Adaptive Gradient Reweighting dynamically
aligns parameter update contributions from forgotten and retained data. Through
systematic analysis of quantized model vulnerabilities, we establish
theoretical foundations for these mechanisms. Extensive evaluations on
benchmark datasets demonstrate Q-MUL's superiority over existing approaches.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 05:22:13 GMT'}]",2025-03-19,"[['Tong', 'Yujia', ''], ['Wang', 'Yuze', ''], ['Yuan', 'Jingling', ''], ['Hu', 'Chuang', '']]","[{'text': 'Model quantization', 'label': 'quantisation'}, {'text': 'data privacy\nregulations', 'label': 'AI Ethics'}, {'text': 'discrete\noptimization', 'label': 'Fine-tuning'}, {'text': 'Adaptive Gradient Reweighting', 'label': 'Fine-tuning'}]",quantisation,Model quantization,0.644131600856781
2503.13947,Sayak Nag,"Sayak Nag, Udita Ghosh, Sarosij Bose, Calvin-Khang Ta, Jiachen Li,
  Amit K Roy Chowdhury","Conformal Prediction and MLLM aided Uncertainty Quantification in Scene
  Graph Generation",Accepted at CVPR 2025,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Scene Graph Generation (SGG) aims to represent visual scenes by identifying
objects and their pairwise relationships, providing a structured understanding
of image content. However, inherent challenges like long-tailed class
distributions and prediction variability necessitate uncertainty quantification
in SGG for its practical viability. In this paper, we introduce a novel
Conformal Prediction (CP) based framework, adaptive to any existing SGG method,
for quantifying their predictive uncertainty by constructing well-calibrated
prediction sets over their generated scene graphs. These scene graph prediction
sets are designed to achieve statistically rigorous coverage guarantees.
Additionally, to ensure these prediction sets contain the most practically
interpretable scene graphs, we design an effective MLLM-based post-processing
strategy for selecting the most visually and semantically plausible scene
graphs within these prediction sets. We show that our proposed approach can
produce diverse possible scene graphs from an image, assess the reliability of
SGG methods, and improve overall SGG performance.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 06:27:57 GMT'}]",2025-03-19,"[['Nag', 'Sayak', ''], ['Ghosh', 'Udita', ''], ['Bose', 'Sarosij', ''], ['Ta', 'Calvin-Khang', ''], ['Li', 'Jiachen', ''], ['Chowdhury', 'Amit K Roy', '']]","[{'text': 'uncertainty quantification', 'label': 'quantisation'}]",quantisation,uncertainty quantification,0.571454644203186
2503.13978,Jorge Baeza-Ballesteros,"Jorge Baeza-Ballesteros, Pilar Hern\'andez, Fernando Romero-L\'opez",The $\pi\pi$ scattering amplitude at large $N_\text{c}$,"63 pages including 3 appendices, 18 figures, 18 tables. Full list of
  operators included as ancillary file",,,"DESY-25-040, MIT-CTP/5850",hep-lat hep-ph hep-th,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study the scaling of meson-meson scattering amplitudes with the number of
colors, $N_\text{c}$. We use lattice calculations in a theory with
$N_\text{f}=4$ degenerate flavors, with $N_\text{c}=3-6$ and pion mass
$M_\pi\approx 560$ MeV. We focus on three different scattering channels, two of
which have the same quantum numbers as some tetraquark candidates recently
found at LHCb: the $T_{cs0}^0(2900)$, $T_{c\bar{s}0}^{++}(2900)$,
$T_{c\bar{s}0}^0(2900)$ and $T_{c\bar{s}1}^0(2900)$ states. Finite-volume
energies are extracted using a large set of operators, containing two-particle
operators with the form of two pions or two vector mesons, and local tetraquark
operators. The resulting energy spectra is used to constrain the
infinite-volume scattering amplitude by means of L\""uscher's quantization
condition. We consider polynomial parametrizations of the phase shift, as well
as one-loop chiral perturbation theory (ChPT) predictions. We find that our
lattice results follow the expected $N_\text{c}$ scaling and are sensitive to
subleading $N_\text{c}$ corrections. In addition, we constrain the scaling of
different combinations of low-energy constants from matching to large
$N_\text{c}$ ChPT. The results for the channel corresponding to a $(\pi^+ D^+_s
- K^+ D^+)$ state show evidence of a virtual bound state with energy
$E_\text{virtual}=1.63(10)M_\pi$ for $N_\text{c}=3$, while this pole disappears
at $N_\text{c}>3$. This may be connected to the exotic states found in
experiment.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 07:27:05 GMT'}]",2025-03-19,"[['Baeza-Ballesteros', 'Jorge', ''], ['Hernández', 'Pilar', ''], ['Romero-López', 'Fernando', '']]","[{'text': 'L\\""uscher\'s quantization\ncondition', 'label': 'quantisation'}]",quantisation,"L\""uscher's quantization
condition",0.6183217763900757
2503.13986,Pengfei Tian,"Pengfei Tian, Fan Yang and Peng Ding","Stratified Permutational Berry--Esseen Bounds and Their Applications to
  Statistics",,,,,math.ST stat.TH,http://creativecommons.org/licenses/by/4.0/,"  The stratified linear permutation statistic arises in various statistics
problems, including stratified and post-stratified survey sampling, stratified
and post-stratified experiments, conditional permutation tests, etc. Although
we can derive the Berry--Esseen bounds for the stratified linear permutation
statistic based on existing bounds for the non-stratified statistics, those
bounds are not sharp, and moreover, this strategy does not work in general
settings with heterogeneous strata with varying sizes. We first use Stein's
method to obtain a unified stratified permutational Berry--Esseen bound that
can accommodate heterogeneous strata. We then apply the bound to various
statistics problems, leading to stronger theoretical quantifications and
thereby facilitating statistical inference in those problems.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 07:44:01 GMT'}]",2025-03-19,"[['Tian', 'Pengfei', ''], ['Yang', 'Fan', ''], ['Ding', 'Peng', '']]","[{'text': 'theoretical quantifications', 'label': 'quantisation'}]",quantisation,theoretical quantifications,0.6950218081474304
2503.14177,Mohamad Al Ahdab,"Mohamad Al Ahdab, Zheng-Hua Tan, John Leth","Distributions and Direct Parametrization for Stable Stochastic
  State-Space Models",,,,,stat.ME cs.SY eess.SY,http://creativecommons.org/licenses/by/4.0/,"  We present a direct parametrization for continuous-time stochastic
state-space models that ensures external stability via the stochastic
bounded-real lemma. Our formulation facilitates the construction of
probabilistic priors that enforce almost-sure stability which are suitable for
sampling-based Bayesian inference methods. We validate our work with a
simulation example and demonstrate its ability to yield stable predictions with
uncertainty quantification.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:51:11 GMT'}]",2025-03-19,"[['Ahdab', 'Mohamad Al', ''], ['Tan', 'Zheng-Hua', ''], ['Leth', 'John', '']]","[{'text': 'uncertainty quantification', 'label': 'quantisation'}]",quantisation,uncertainty quantification,0.571454644203186
2503.14259,Ziyad Sheebaelhamd,"Ziyad Sheebaelhamd, Michael Tschannen, Michael Muehlebach, Claire
  Vernade",Quantization-Free Autoregressive Action Transformer,,,,,cs.LG cs.RO,http://creativecommons.org/licenses/by/4.0/,"  Current transformer-based imitation learning approaches introduce discrete
action representations and train an autoregressive transformer decoder on the
resulting latent code. However, the initial quantization breaks the continuous
structure of the action space thereby limiting the capabilities of the
generative model. We propose a quantization-free method instead that leverages
Generative Infinite-Vocabulary Transformers (GIVT) as a direct, continuous
policy parametrization for autoregressive transformers. This simplifies the
imitation learning pipeline while achieving state-of-the-art performance on a
variety of popular simulated robotics tasks. We enhance our policy roll-outs by
carefully studying sampling algorithms, further improving the results.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 13:50:35 GMT'}]",2025-03-19,"[['Sheebaelhamd', 'Ziyad', ''], ['Tschannen', 'Michael', ''], ['Muehlebach', 'Michael', ''], ['Vernade', 'Claire', '']]","[{'text': 'initial quantization', 'label': 'quantisation'}, {'text': 'Generative Infinite-Vocabulary Transformers', 'label': 'Transformers'}, {'text': 'autoregressive transformers', 'label': 'Transformers'}]",quantisation,initial quantization,0.69083571434021
2503.14344,Mrinal Kanti Roychowdhury,"Shivam Dubey, Mrinal Kanti Roychowdhury, and Saurabh Verma",Quantization for a condensation system,,,,,math.DS math.PR,http://creativecommons.org/licenses/by/4.0/,"  For a given $r \in (0, +\infty)$, the quantization dimension of order $r$, if
it exists, denoted by $D_r(\mu)$, represents the rate at which the $n$th
quantization error of order $r$ approaches to zero as the number of elements
$n$ in an optimal set of $n$-means for $\mu$ tends to infinity. If $D_r(\mu)$
does not exist, we define $\underline{D}_r(\mu)$ and $\overline{D}_r(\mu)$ as
the lower and the upper quantization dimensions of $\mu$ of order $r$,
respectively. In this paper, we investigate the quantization dimension of the
condensation measure $\mu$ associated with a condensation system
$(\{S_j\}_{j=1}^N, (p_j)_{j=0}^N, \nu).$ We provide two examples: one where
$\nu$ is an infinite discrete distribution on $\mathbb{R}$, and one where $\nu$
is a uniform distribution on $\mathbb{R}$. For both the discrete and uniform
distributions $\nu$, we determine the optimal sets of $n$-means, and calculate
the quantization dimensions of condensation measures $\mu$, and show that the
$D_r(\mu)$-dimensional quantization coefficients do not exist. Moreover, we
demonstrate that the lower and upper quantization coefficients are finite and
positive.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 15:24:28 GMT'}]",2025-03-19,"[['Dubey', 'Shivam', ''], ['Roychowdhury', 'Mrinal Kanti', ''], ['Verma', 'Saurabh', '']]","[{'text': 'quantization dimensions', 'label': 'quantisation'}]",quantisation,quantization dimensions,0.6772695779800415
2503.14387,Mahmoud Alawashra,"Mahmoud Alawashra, Jan Ben\'a\v{c}ek, Martin Pohl, Mikhail Medvedev","Electromagnetic field solver for QED polarization in super-strong
  magnetic fields of magnetar and laser plasmas",Submitted to JCAP,,,,physics.plasm-ph astro-ph.HE,http://creativecommons.org/licenses/by/4.0/,"  Super-strongly magnetized plasmas play a crucial role in extreme environments
of magnetar and laboratory laser experiments, demanding comprehensive
understanding of how quantum electrodynamic (QED) effects influence plasma
behaviour. Earlier analytical and semi-analytical calculations have shown that
QED effects can significantly modify the plasma polarization mode behaviour
around magnetars using analytical and semi-analytical calculations. In this
work, we present the first electromagnetic field solver that is valid beyond
the Schwinger limit. QED vacuum polarization in super-strong magnetic fields
are modeled with nonlinear Maxwell equations. We show that electromagnetic
waves in simulations follow the analytical solutions well and reproduce the
birefringence effects of electromagnetic wave modes between the $O$ and $X$
polarizations of perpendicular electromagnetic waves and those between $L$ and
$R$ polarizations of parallel waves. This new framework can be applied to
kinetic as well as in other types of computer simulations. The solver's key
advantage lies in its versatility, allowing it to be used in gyro-motion,
gyro-center, and gyro-kinetic simulations, which do not resolve the cyclotron
motion, or in plasma studies with ground-level Landau quantization.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:22:33 GMT'}]",2025-03-19,"[['Alawashra', 'Mahmoud', ''], ['Benáček', 'Jan', ''], ['Pohl', 'Martin', ''], ['Medvedev', 'Mikhail', '']]","[{'text': 'Landau quantization', 'label': 'quantisation'}]",quantisation,Landau quantization,0.5282343626022339
2503.14598,Haoyang Gao,"Haoyang Gao, Leigh S. Martin, Lillian B. Hughes, Nathaniel T. Leitao,
  Piotr Put, Hengyun Zhou, Nazli U. Koyluoglu, Simon A. Meynell, Ania C.
  Bleszynski Jayich, Hongkun Park, Mikhail D. Lukin","Signal amplification in a solid-state quantum sensor via asymmetric
  time-reversal of many-body dynamics","25 pages, 15 digures",,,,quant-ph cond-mat.dis-nn,http://creativecommons.org/licenses/by/4.0/,"  Electronic spins of nitrogen vacancy (NV) centers in diamond constitute a
promising system for micro- and nano-scale magnetic sensing, due to their
operation under ambient conditions, ease of placement in close proximity to
sensing targets, and biological compatibility. At high densities, the
electronic spins interact through dipolar coupling, which typically limits but
can also potentially enhance sensing performance. Here we report the
experimental demonstration of many-body signal amplification in a solid-state,
room temperature quantum sensor. Our approach utilizes time-reversed
two-axis-twisting interactions, engineered through dynamical control of the
quantization axis and Floquet engineering in a two-dimensional ensemble of NV
centers. Strikingly, we observe that the optimal amplification occurs when the
backward evolution time equals twice the forward evolution time, in sharp
contrast to the conventional Loschmidt echo. These observations can be
understood as resulting from an underlying time-reversed mirror symmetry of the
microscopic dynamics, providing key insights into signal amplification and
opening the door towards entanglement-enhanced practical quantum sensing.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 18:01:25 GMT'}]",2025-03-20,"[['Gao', 'Haoyang', ''], ['Martin', 'Leigh S.', ''], ['Hughes', 'Lillian B.', ''], ['Leitao', 'Nathaniel T.', ''], ['Put', 'Piotr', ''], ['Zhou', 'Hengyun', ''], ['Koyluoglu', 'Nazli U.', ''], ['Meynell', 'Simon A.', ''], ['Jayich', 'Ania C. Bleszynski', ''], ['Park', 'Hongkun', ''], ['Lukin', 'Mikhail D.', '']]","[{'text': 'quantization axis', 'label': 'quantisation'}]",quantisation,quantization axis,0.6386812329292297
2503.14663,Anni Zhou,"Anni Zhou, Beyah Raheem, Rishikesan Kamaleswaran, Yao Xie","Sepsyn-OLCP: An Online Learning-based Framework for Early Sepsis
  Prediction with Uncertainty Quantification using Conformal Prediction",,,,,cs.LG,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Sepsis is a life-threatening syndrome with high morbidity and mortality in
hospitals. Early prediction of sepsis plays a crucial role in facilitating
early interventions for septic patients. However, early sepsis prediction
systems with uncertainty quantification and adaptive learning are scarce. This
paper proposes Sepsyn-OLCP, a novel online learning algorithm for early sepsis
prediction by integrating conformal prediction for uncertainty quantification
and Bayesian bandits for adaptive decision-making. By combining the robustness
of Bayesian models with the statistical uncertainty guarantees of conformal
prediction methodologies, this algorithm delivers accurate and trustworthy
predictions, addressing the critical need for reliable and adaptive systems in
high-stakes healthcare applications such as early sepsis prediction. We
evaluate the performance of Sepsyn-OLCP in terms of regret in stochastic bandit
setting, the area under the receiver operating characteristic curve (AUROC),
and F-measure. Our results show that Sepsyn-OLCP outperforms existing
individual models, increasing AUROC of a neural network from 0.64 to 0.73
without retraining and high computational costs. And the model selection policy
converges to the optimal strategy in the long run. We propose a novel
reinforcement learning-based framework integrated with conformal prediction
techniques to provide uncertainty quantification for early sepsis prediction.
The proposed methodology delivers accurate and trustworthy predictions,
addressing a critical need in high-stakes healthcare applications like early
sepsis prediction.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 19:10:34 GMT'}]",2025-03-20,"[['Zhou', 'Anni', ''], ['Raheem', 'Beyah', ''], ['Kamaleswaran', 'Rishikesan', ''], ['Xie', 'Yao', '']]","[{'text': 'uncertainty quantification', 'label': 'quantisation'}, {'text': 'adaptive learning', 'label': 'Few-shot Learning'}, {'text': 'uncertainty quantification', 'label': 'quantisation'}, {'text': 'uncertainty quantification', 'label': 'quantisation'}]",quantisation,uncertainty quantification,0.571454644203186
2503.14665,Parker Ewen,"Parker Ewen, Hao Chen, Seth Isaacson, Joey Wilson, Katherine A.
  Skinner, Ram Vasudevan","These Magic Moments: Differentiable Uncertainty Quantification of
  Radiance Field Models",,,,,cs.CV cs.RO,http://creativecommons.org/licenses/by/4.0/,"  This paper introduces a novel approach to uncertainty quantification for
radiance fields by leveraging higher-order moments of the rendering equation.
Uncertainty quantification is crucial for downstream tasks including view
planning and scene understanding, where safety and robustness are paramount.
However, the high dimensionality and complexity of radiance fields pose
significant challenges for uncertainty quantification, limiting the use of
these uncertainty quantification methods in high-speed decision-making. We
demonstrate that the probabilistic nature of the rendering process enables
efficient and differentiable computation of higher-order moments for radiance
field outputs, including color, depth, and semantic predictions. Our method
outperforms existing radiance field uncertainty estimation techniques while
offering a more direct, computationally efficient, and differentiable
formulation without the need for post-processing.Beyond uncertainty
quantification, we also illustrate the utility of our approach in downstream
applications such as next-best-view (NBV) selection and active ray sampling for
neural radiance field training. Extensive experiments on synthetic and
real-world scenes confirm the efficacy of our approach, which achieves
state-of-the-art performance while maintaining simplicity.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 19:12:02 GMT'}]",2025-03-20,"[['Ewen', 'Parker', ''], ['Chen', 'Hao', ''], ['Isaacson', 'Seth', ''], ['Wilson', 'Joey', ''], ['Skinner', 'Katherine A.', ''], ['Vasudevan', 'Ram', '']]","[{'text': 'uncertainty quantification', 'label': 'quantisation'}, {'text': 'Uncertainty quantification', 'label': 'quantisation'}, {'text': 'uncertainty quantification', 'label': 'quantisation'}, {'text': 'uncertainty\nquantification', 'label': 'quantisation'}]",quantisation,uncertainty quantification,0.571454644203186
2503.14668,Mykhaylo Khoma,Mykhaylo Khoma,"Elastic and charge transfer cross sections for low to ultralow
  $\rm{H}(1s) + \rm{H}^{+}$ collisions. Quantal and semiclassical calculations","15 pages, 4 figures",,,,physics.atom-ph,http://creativecommons.org/licenses/by/4.0/,"  The elastic scattering and resonant charge transfer integral cross sections
in $\rm{H}(1s) + \rm{H^+}$ collisions are computed for the center-of-mass
energy range of $10^{-10}-10$ eV. Fully quantal and semiclassical approaches
are utilized in these calculations. The reliability of the semiclassical
approximation for very low collision energies is discussed. The results are
compared with available data from the literature.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 19:16:30 GMT'}]",2025-03-20,"[['Khoma', 'Mykhaylo', '']]","[{'text': 'Fully quantal and semiclassical approaches', 'label': 'quantisation'}]",quantisation,Fully quantal and semiclassical approaches,0.604477047920227
2503.14697,Juan Sosa,Juan Sosa and Carlo Mart\'inez,"Bayesian Sociality Models: A Scalable and Flexible Alternative for
  Network Analysis","44 pages, 4 tables, 10 figures",,,,stat.ME stat.CO,http://creativecommons.org/licenses/by/4.0/,"  Bayesian sociality models provide a scalable and flexible alternative for
network analysis, capturing degree heterogeneity through actor-specific
parameters while mitigating the identifiability challenges of latent space
models. This paper develops a comprehensive Bayesian inference framework,
leveraging Markov chain Monte Carlo and variational inference to assess their
efficiency-accuracy trade-offs. Through empirical and simulation studies, we
demonstrate the model's robustness in goodness-of-fit, predictive performance,
clustering, and other key network analysis tasks. The Bayesian paradigm further
enhances uncertainty quantification and interpretability, positioning sociality
models as a powerful and generalizable tool for modern network science.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 20:00:46 GMT'}]",2025-03-20,"[['Sosa', 'Juan', ''], ['Martínez', 'Carlo', '']]","[{'text': 'uncertainty quantification', 'label': 'quantisation'}]",quantisation,uncertainty quantification,0.571454644203186
2503.14785,Nima Negarandeh,"Nima Negarandeh, Carlos Mora, Ramin Bostanabad","SEEK: Self-adaptive Explainable Kernel For Nonstationary Gaussian
  Processes",,,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Gaussian processes (GPs) are powerful probabilistic models that define
flexible priors over functions, offering strong interpretability and
uncertainty quantification. However, GP models often rely on simple, stationary
kernels which can lead to suboptimal predictions and miscalibrated uncertainty
estimates, especially in nonstationary real-world applications. In this paper,
we introduce SEEK, a novel class of learnable kernels to model complex,
nonstationary functions via GPs. Inspired by artificial neurons, SEEK is
derived from first principles to ensure symmetry and positive
semi-definiteness, key properties of valid kernels. The proposed method
achieves flexible and adaptive nonstationarity by learning a mapping from a set
of base kernels. Compared to existing techniques, our approach is more
interpretable and much less prone to overfitting. We conduct comprehensive
sensitivity analyses and comparative studies to demonstrate that our approach
is not robust to only many of its design choices, but also outperforms existing
stationary/nonstationary kernels in both mean prediction accuracy and
uncertainty quantification.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 23:30:02 GMT'}]",2025-03-20,"[['Negarandeh', 'Nima', ''], ['Mora', 'Carlos', ''], ['Bostanabad', 'Ramin', '']]","[{'text': 'uncertainty quantification', 'label': 'quantisation'}, {'text': 'uncertainty quantification', 'label': 'quantisation'}]",quantisation,uncertainty quantification,0.571454644203186
2503.14794,Dougal Davis,Dougal Davis and Lucas Mason-Brown,"Hodge theory, intertwining functors, and the Orbit Method for real
  reductive groups",64 pages. Comments welcome!,,,,math.RT math.AG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study the Hodge filtrations of Schmid and Vilonen on unipotent
representations of real reductive groups. We show that for various well-defined
classes of unipotent representations (including, for example, the oscillator
representations of metaplectic groups, the minimal representations of all
simple groups, and all unipotent representations of complex groups) the Hodge
filtration coincides with the quantization filtration predicted by the Orbit
Method. We deduce a number of longstanding conjectures about such
representations, including a proof that they are unitary and a description of
their $K$-types in terms of co-adjoint orbits. The proofs rely heavily on
certain good homological properties of the Hodge filtrations on weakly
unipotent representations, which are established using a Hodge-theoretic
upgrade of the Beilinson-Bernstein theory of intertwining functors for
$\mathcal{D}$-modules on the flag variety. The latter consists of an action of
the affine Hecke algebra on a category of filtered monodromic
$\mathcal{D}$-modules, which we use to compare Hodge filtrations coming from
different localizations of the same representation. As an application of the
same methods, we also prove a new cohomology vanishing theorem for mixed Hodge
modules on partial flag varieties.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 00:01:22 GMT'}]",2025-03-20,"[['Davis', 'Dougal', ''], ['Mason-Brown', 'Lucas', '']]","[{'text': 'quantization', 'label': 'quantisation'}]",quantisation,quantization,0.813445508480072
2503.15446,Yutaka Yoshida,Yutaka Yoshida,"Quantized Coulomb branch of 4d $\mathcal{N}=2$ $Sp(N)$ gauge theory and
  spherical DAHA of $(C_N^{\vee}, C_N)$-type",34 pages,,,,hep-th math-ph math.AG math.MP math.RT,http://creativecommons.org/licenses/by/4.0/,"  We study BPS loop operators in a 4d $\mathcal{N}=2$ $Sp(N)$ gauge theory with
four hypermultiplets in the fundamental representation and one hypermultiplet
in the anti-symmetric representation. The algebra of BPS loop operators in the
$\Omega$-background provides a deformation quantization of the Coulomb branch,
which is expected to coincide with the quantized K-theoretic Coulomb branch in
the mathematical literature. For the rank-one case, i.e., $Sp(1) \simeq SU(2)$,
we show that the quantization of the Coulomb branch, evaluated using the
supersymmetric localization formula, agrees with the polynomial representation
of the spherical part of the double affine Hecke algebra (spherical DAHA) of
$(C_1^{\vee}, C_1)$-type. For higher-rank cases, where $N \geq 2$, we
conjecture that the quantized Coulomb branch of the 4d $\mathcal{N}=2$ $Sp(N)$
gauge theory is isomorphic to the spherical DAHA of $(C_N^{\vee}, C_N)$-type .
As evidence for this conjecture, we demonstrate that the quantization of an 't
Hooft loop agrees with the Koornwinder operator in the polynomial
representation of the spherical DAHA.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:24:50 GMT'}]",2025-03-20,"[['Yoshida', 'Yutaka', '']]","[{'text': 'deformation quantization', 'label': 'quantisation'}, {'text': 'quantization', 'label': 'quantisation'}]",quantisation,quantization,0.813445508480072
2503.15465,Ruichen Chen,"Ruichen Chen, Keith G. Mills, Di Niu","FP4DiT: Towards Effective Floating Point Quantization for Diffusion
  Transformers",The code is available at https://github.com/cccrrrccc/FP4DiT,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Diffusion Models (DM) have revolutionized the text-to-image visual generation
process. However, the large computational cost and model footprint of DMs
hinders practical deployment, especially on edge devices. Post-training
quantization (PTQ) is a lightweight method to alleviate these burdens without
the need for training or fine-tuning. While recent DM PTQ methods achieve W4A8
on integer-based PTQ, two key limitations remain: First, while most existing DM
PTQ methods evaluate on classical DMs like Stable Diffusion XL, 1.5 or earlier,
which use convolutional U-Nets, newer Diffusion Transformer (DiT) models like
the PixArt series, Hunyuan and others adopt fundamentally different transformer
backbones to achieve superior image synthesis. Second, integer (INT)
quantization is prevailing in DM PTQ but doesn't align well with the network
weight and activation distribution, while Floating-Point Quantization (FPQ) is
still under-investigated, yet it holds the potential to better align the weight
and activation distributions in low-bit settings for DiT. In response, we
introduce FP4DiT, a PTQ method that leverages FPQ to achieve W4A6 quantization.
Specifically, we extend and generalize the Adaptive Rounding PTQ technique to
adequately calibrate weight quantization for FPQ and demonstrate that DiT
activations depend on input patch data, necessitating robust online activation
quantization techniques. Experimental results demonstrate that FP4DiT
outperforms integer-based PTQ at W4A6 and W4A8 precision and generates
convincing visual content on PixArt-$\alpha$, PixArt-$\Sigma$ and Hunyuan in
terms of several T2I metrics such as HPSv2 and CLIP.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:44:21 GMT'}]",2025-03-20,"[['Chen', 'Ruichen', ''], ['Mills', 'Keith G.', ''], ['Niu', 'Di', '']]","[{'text': 'Post-training\nquantization', 'label': 'quantisation'}, {'text': 'PTQ', 'label': 'quantisation'}, {'text': 'PTQ', 'label': 'quantisation'}, {'text': 'integer (INT)\nquantization', 'label': 'quantisation'}, {'text': 'Floating-Point Quantization', 'label': 'quantisation'}, {'text': 'FPQ', 'label': 'quantisation'}, {'text': 'FPQ', 'label': 'quantisation'}, {'text': 'PTQ', 'label': 'quantisation'}, {'text': 'FPQ', 'label': 'quantisation'}]",quantisation,"Post-training
quantization",0.6493542194366455
2503.15482,Richard Barney,"Richard Barney, Djamil Lakhdar-Hamina, Victor Galitski",Natural Quantization of Neural Networks,"7 pages, 8 figures, 1 table",,,,quant-ph cond-mat.dis-nn cs.LG,http://creativecommons.org/licenses/by/4.0/,"  We propose a natural quantization of a standard neural network, where the
neurons correspond to qubits and the activation functions are implemented via
quantum gates and measurements. The simplest quantized neural network
corresponds to applying single-qubit rotations, with the rotation angles being
dependent on the weights and measurement outcomes of the previous layer. This
realization has the advantage of being smoothly tunable from the purely
classical limit with no quantum uncertainty (thereby reproducing the classical
neural network exactly) to a quantum case, where superpositions introduce an
intrinsic uncertainty in the network. We benchmark this architecture on a
subset of the standard MNIST dataset and find a regime of ""quantum advantage,""
where the validation error rate in the quantum realization is smaller than that
in the classical model. We also consider another approach where quantumness is
introduced via weak measurements of ancilla qubits entangled with the neuron
qubits. This quantum neural network also allows for smooth tuning of the degree
of quantumness by controlling an entanglement angle, $g$, with $g=\frac\pi 2$
replicating the classical regime. We find that validation error is also
minimized within the quantum regime in this approach. We also observe a quantum
transition, with sharp loss of the quantum network's ability to learn at a
critical point $g_c$. The proposed quantum neural networks are readily
realizable in present-day quantum computers on commercial datasets.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:57:11 GMT'}]",2025-03-20,"[['Barney', 'Richard', ''], ['Lakhdar-Hamina', 'Djamil', ''], ['Galitski', 'Victor', '']]","[{'text': 'natural quantization', 'label': 'quantisation'}, {'text': 'smoothly tunable', 'label': 'Fine-tuning'}, {'text': 'quantum advantage', 'label': 'quantisation'}, {'text': 'smooth tuning', 'label': 'Fine-tuning'}]",quantisation,natural quantization,0.7380500435829163
2503.15591,Minsung Kim,"Hee-Cheol Kim, Minsung Kim, Sung-Soo Kim, Kimyeong Lee, Xin Wang","Probing Quantum Curves and Transitions in 5d SQFTs via Defects and
  Blowup Equations","72 pages, 22 figures",,,"KIAS-Q25003, USTC-ICTS/PCFT-25-09",hep-th,http://creativecommons.org/licenses/by/4.0/,"  We investigate codimension-2 defect partition functions and quantum
Seiberg-Witten curves in 5d rank-1 supersymmetric QFTs, including
non-Lagrangian and Kaluza-Klein theories. Using generalized blowup equations,
we compute defect partition functions in the $\Omega$-background and show that,
in the Nekrasov-Shatashvili limit, they satisfy certain difference equations
that encode the quantization of classical Seiberg-Witten curves. Furthermore,
we explore novel transitions in the defect partition functions and their
relation to coordinate transformations of quantum Seiberg-Witten curves, with a
focus on SL(2,$\mathbb{Z}$) transformations and Hanany-Witten transitions.
These findings provide new insights into the interplay between codimension-2
defects, quantum curves, and the geometric structure of 5d supersymmetric QFTs.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 18:00:00 GMT'}]",2025-03-21,"[['Kim', 'Hee-Cheol', ''], ['Kim', 'Minsung', ''], ['Kim', 'Sung-Soo', ''], ['Lee', 'Kimyeong', ''], ['Wang', 'Xin', '']]","[{'text': 'quantization', 'label': 'quantisation'}]",quantisation,quantization,0.813445508480072
2503.15642,Carlo Cepollaro,"Fatemeh Bibak, Carlo Cepollaro, Nicol\'as Medina S\'anchez, Borivoje
  Daki\'c, \v{C}aslav Brukner","The classical limit of quantum mechanics through coarse-grained
  measurements","Main text: 25 pages, Appendix: 2 pages, Figures: 5",,,,quant-ph physics.class-ph,http://creativecommons.org/licenses/by/4.0/,"  We address the classical limit of quantum mechanics, focusing on its
emergence through coarse-grained measurements when multiple outcomes are
conflated into slots. We rigorously derive effective classical kinematics under
such measurements, demonstrating that when the volume of the coarse-grained
slot in phase space significantly exceeds Planck's constant, quantum states can
be effectively described by classical probability distributions. Furthermore,
we show that the dynamics, derived under coarse-grained observations and the
linear approximation of the quantum Hamiltonian around its classical values
within the slots, is effectively described by a classical Hamiltonian following
Liouville dynamics. The classical Hamiltonian obtained through this process is
equivalent to the one from which the underlying quantum Hamiltonian is derived
via the (Dirac) quantization procedure, completing the quantization-classical
limit loop. The Ehrenfest time, marking the duration within which classical
behavior remains valid, is analyzed for various physical systems. The
implications of these findings are discussed in the context of both macroscopic
and microscopic systems, revealing the mechanisms behind their observed
classicality. This work provides a comprehensive framework for understanding
the quantum-to-classical transition and addresses foundational questions about
the consistency of the quantization-classical limit cycle.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 18:59:58 GMT'}]",2025-03-21,"[['Bibak', 'Fatemeh', ''], ['Cepollaro', 'Carlo', ''], ['Sánchez', 'Nicolás Medina', ''], ['Dakić', 'Borivoje', ''], ['Brukner', 'Časlav', '']]","[{'text': 'Dirac) quantization procedure', 'label': 'quantisation'}, {'text': 'quantization-classical\nlimit loop', 'label': 'quantisation'}]",quantisation,Dirac) quantization procedure,0.6254645586013794
2503.15889,Cynthia Dong,"Cynthia Dong, Hong Jia, Young D. Kwon, Georgios Rizos, Cecilia Mascolo","LeanTTA: A Backpropagation-Free and Stateless Approach to Quantized
  Test-Time Adaptation on Edge Devices","8 pages, 5 figures",,,,cs.LG cs.AI,http://creativecommons.org/licenses/by/4.0/,"  While there are many advantages to deploying machine learning models on edge
devices, the resource constraints of mobile platforms, the dynamic nature of
the environment, and differences between the distribution of training versus
in-the-wild data make such deployments challenging. Current test-time
adaptation methods are often memory-intensive and not designed to be
quantization-compatible or deployed on low-resource devices. To address these
challenges, we present LeanTTA, a novel backpropagation-free and stateless
framework for quantized test-time adaptation tailored to edge devices. Our
approach minimizes computational costs by dynamically updating normalization
statistics without backpropagation, which frees LeanTTA from the common pitfall
of relying on large batches and historical data, making our method robust to
realistic deployment scenarios. Our approach is the first to enable further
computational gains by combining partial adaptation with quantized module
fusion. We validate our framework across sensor modalities, demonstrating
significant improvements over state-of-the-art TTA methods, including a 15.7%
error reduction, peak memory usage of only 11.2MB for ResNet18, and fast
adaptation within an order-of-magnitude of normal inference speeds on-device.
LeanTTA provides a robust solution for achieving the right trade offs between
accuracy and system efficiency in edge deployments, addressing the unique
challenges posed by limited data and varied operational conditions.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 06:27:09 GMT'}]",2025-03-21,"[['Dong', 'Cynthia', ''], ['Jia', 'Hong', ''], ['Kwon', 'Young D.', ''], ['Rizos', 'Georgios', ''], ['Mascolo', 'Cecilia', '']]","[{'text': 'quantization-compatible', 'label': 'quantisation'}]",quantisation,quantization-compatible,0.7064218521118164
2503.16027,Yiming Yang,"Yiming Yang, Deyu Ming, Serge Guillas","Distribution of Deep Gaussian process Gradients and Sequential Design
  for Simulators with Sharp Variations",,,,,stat.CO stat.AP stat.ME,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep Gaussian Processes (DGPs), multi-layered extensions of GPs, better
emulate simulators with regime transitions or sharp changes than standard GPs.
Gradient information is crucial for tasks like sensitivity analysis and
dimension reduction. Although gradient posteriors are well-defined in GPs,
extending them to DGPs is challenging due to their hierarchical structure. We
propose a novel method to approximate the DGP emulator's gradient distribution,
enabling efficient gradient computation with uncertainty quantification (UQ).
Our approach derives an analytical gradient mean and the covariance. The
numerical results show that our method outperforms GP and DGP with finite
difference methods in gradient accuracy, offering the extra unique benefit of
UQ. Based on the gradient information, we further propose a sequential design
criterion to identify the sharp variation regions efficiently, with the
gradient norm as a key indicator whose distribution can be readily evaluated in
our framework. We evaluated the proposed sequential design using synthetic
examples and empirical applications, demonstrating its superior performance in
emulating functions with sharp changes compared to existing design methods. The
DGP gradient computation is seamlessly integrated into the advanced Python
package dgpsi for DGP emulation, along with the proposed sequential design
available at https://github.com/yyimingucl/DGP.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 10:48:56 GMT'}]",2025-03-21,"[['Yang', 'Yiming', ''], ['Ming', 'Deyu', ''], ['Guillas', 'Serge', '']]","[{'text': 'uncertainty quantification', 'label': 'quantisation'}]",quantisation,uncertainty quantification,0.571454644203186
2503.16163,Shibo Jie,"Shibo Jie, Yehui Tang, Kai Han, Zhi-Hong Deng, Jing Han",SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs,,,,,cs.CL,http://creativecommons.org/publicdomain/zero/1.0/,"  Transformer-based large language models (LLMs) have already achieved
remarkable results on long-text tasks, but the limited GPU memory (VRAM)
resources struggle to accommodate the linearly growing demand for key-value
(KV) cache as the sequence length increases, which has become a bottleneck for
the application of LLMs on long sequences. Existing KV cache compression
methods include eviction, merging, or quantization of the KV cache to reduce
its size. However, compression results in irreversible information forgetting,
potentially affecting the accuracy of subsequent decoding. In this paper, we
propose SpeCache, which takes full advantage of the large and easily expandable
CPU memory to offload the complete KV cache, and dynamically fetches KV pairs
back in each decoding step based on their importance measured by low-bit KV
cache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,
SpeCache speculatively predicts the KV pairs that the next token might attend
to, allowing us to prefetch them before the next decoding step which enables
parallelization of prefetching and computation. Experiments on LongBench and
Needle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM
usage while avoiding information forgetting for long sequences without
re-training, even with a 10x high KV cache compression ratio.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 14:01:56 GMT'}]",2025-03-21,"[['Jie', 'Shibo', ''], ['Tang', 'Yehui', ''], ['Han', 'Kai', ''], ['Deng', 'Zhi-Hong', ''], ['Han', 'Jing', '']]","[{'text': 'eviction', 'label': 'quantisation'}, {'text': 'merging', 'label': 'quantisation'}, {'text': 'quantization', 'label': 'quantisation'}]",quantisation,quantization,0.813445508480072
2503.16194,Ziyao Guo,"Ziyao Guo, Kaipeng Zhang, Michael Qizhe Shieh","Improving Autoregressive Image Generation through Coarse-to-Fine Token
  Prediction",Work in progress,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Autoregressive models have shown remarkable success in image generation by
adapting sequential prediction techniques from language modeling. However,
applying these approaches to images requires discretizing continuous pixel data
through vector quantization methods like VQ-VAE. To alleviate the quantization
errors that existed in VQ-VAE, recent works tend to use larger codebooks.
However, this will accordingly expand vocabulary size, complicating the
autoregressive modeling task. This paper aims to find a way to enjoy the
benefits of large codebooks without making autoregressive modeling more
difficult. Through empirical investigation, we discover that tokens with
similar codeword representations produce similar effects on the final generated
image, revealing significant redundancy in large codebooks. Based on this
insight, we propose to predict tokens from coarse to fine (CTF), realized by
assigning the same coarse label for similar tokens. Our framework consists of
two stages: (1) an autoregressive model that sequentially predicts coarse
labels for each token in the sequence, and (2) an auxiliary model that
simultaneously predicts fine-grained labels for all tokens conditioned on their
coarse labels. Experiments on ImageNet demonstrate our method's superior
performance, achieving an average improvement of 59 points in Inception Score
compared to baselines. Notably, despite adding an inference step, our approach
achieves faster sampling speeds.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 14:41:29 GMT'}]",2025-03-21,"[['Guo', 'Ziyao', ''], ['Zhang', 'Kaipeng', ''], ['Shieh', 'Michael Qizhe', '']]","[{'text': 'vector quantization methods', 'label': 'quantisation'}, {'text': 'VQ-VAE', 'label': 'quantisation'}]",quantisation,vector quantization methods,0.5234082341194153
2503.16198,Igor Pikovski,Vasileios Fragkos and Igor Pikovski,"Probing classical and quantum violations of the equivalence of active
  and passive gravitational mass","13 pages, 5 figures, 2 tables",,,,quant-ph gr-qc physics.atom-ph,http://creativecommons.org/licenses/by/4.0/,"  The equivalence of active and passive (EAP) gravitational mass is one of the
most fundamental principles of gravity. But in contrast to the usual
equivalence of inertial and (passive) gravitational mass, the EAP has not
received much attention. Here we revisit this principle and show how it can be
used to probe quantum gravity in laboratory-based experiments. We first examine
how the dynamics under EAP violations affects classical systems and show that
new laboratory tests can be performed, to improve over the current experimental
bounds and to test new manifestations of EAP violations. We then extend the
analysis to the quantum domain, where quantized energy contributes to mass and
the EAP principle can thus shed light on how quantum source masses would
gravitate. We show that experiments with cold polar molecules, and future
experiments with nuclear atomic clocks, can test the quantum EAP in a regime
where quantum gravity phenomenology could become relevant. Our results open new
opportunities for fundamental tests of gravity in high-precision laboratory
experiments that can shed light on foundational principles of gravity and its
interface with quantum theory.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 14:45:24 GMT'}]",2025-03-21,"[['Fragkos', 'Vasileios', ''], ['Pikovski', 'Igor', '']]","[{'text': 'quantized energy', 'label': 'quantisation'}]",quantisation,quantized energy,0.5685881972312927
2503.16222,Teresa Klatzer,"Teresa Klatzer and Savvas Melidonis and Marcelo Pereyra and
  Konstantinos C. Zygalakis","Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson
  Inverse Problems","31 pages, 17 figures",,,,stat.CO cs.CV cs.NA math.NA stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper introduces a novel plug-and-play (PnP) Langevin sampling
methodology for Bayesian inference in low-photon Poisson imaging problems, a
challenging class of problems with significant applications in astronomy,
medicine, and biology. PnP Langevin sampling algorithms offer a powerful
framework for Bayesian image restoration, enabling accurate point estimation as
well as advanced inference tasks, including uncertainty quantification and
visualization analyses, and empirical Bayesian inference for automatic model
parameter tuning. However, existing PnP Langevin algorithms are not well-suited
for low-photon Poisson imaging due to high solution uncertainty and poor
regularity properties, such as exploding gradients and non-negativity
constraints. To address these challenges, we propose two strategies for
extending Langevin PnP sampling to Poisson imaging models: (i) an accelerated
PnP Langevin method that incorporates boundary reflections and a Poisson
likelihood approximation and (ii) a mirror sampling algorithm that leverages a
Riemannian geometry to handle the constraints and the poor regularity of the
likelihood without approximations. The effectiveness of these approaches is
demonstrated through extensive numerical experiments and comparisons with
state-of-the-art methods.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 15:17:05 GMT'}]",2025-03-21,"[['Klatzer', 'Teresa', ''], ['Melidonis', 'Savvas', ''], ['Pereyra', 'Marcelo', ''], ['Zygalakis', 'Konstantinos C.', '']]","[{'text': 'uncertainty quantification', 'label': 'quantisation'}, {'text': 'automatic model\nparameter tuning', 'label': 'Fine-tuning'}]",quantisation,uncertainty quantification,0.571454644203186
2503.16364,Naeim Zarezadeh,"Z. Zarezadeh, N. Zarezadeh",Neural Networks: According to the Principles of Grassmann Algebra,,,,,cs.LG cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  In this paper, we explore the algebra of quantum idempotents and the
quantization of fermions which gives rise to a Hilbert space equal to the
Grassmann algebra associated with the Lie algebra. Since idempotents carry
representations of the algebra under consideration, they form algebraic
varieties and smooth manifolds in the natural topology. In addition to the
motivation of linking up mathematical physics with machine learning, it is also
shown that by using idempotents and invariant subspace of the corresponding
algebras, these representations encode and perhaps provide a probabilistic
interpretation of reasoning and relational paths in geometrical terms.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:21:23 GMT'}]",2025-03-21,"[['Zarezadeh', 'Z.', ''], ['Zarezadeh', 'N.', '']]","[{'text': 'quantization', 'label': 'quantisation'}]",quantisation,quantization,0.813445508480072
2503.16430,Yuqing Wang,"Yuqing Wang, Zhijie Lin, Yao Teng, Yuanzhi Zhu, Shuhuai Ren, Jiashi
  Feng, Xihui Liu","Bridging Continuous and Discrete Tokens for Autoregressive Visual
  Generation",Project page: https://yuqingwang1029.github.io/TokenBridge,,,,cs.CV,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Autoregressive visual generation models typically rely on tokenizers to
compress images into tokens that can be predicted sequentially. A fundamental
dilemma exists in token representation: discrete tokens enable straightforward
modeling with standard cross-entropy loss, but suffer from information loss and
tokenizer training instability; continuous tokens better preserve visual
details, but require complex distribution modeling, complicating the generation
pipeline. In this paper, we propose TokenBridge, which bridges this gap by
maintaining the strong representation capacity of continuous tokens while
preserving the modeling simplicity of discrete tokens. To achieve this, we
decouple discretization from the tokenizer training process through
post-training quantization that directly obtains discrete tokens from
continuous representations. Specifically, we introduce a dimension-wise
quantization strategy that independently discretizes each feature dimension,
paired with a lightweight autoregressive prediction mechanism that efficiently
model the resulting large token space. Extensive experiments show that our
approach achieves reconstruction and generation quality on par with continuous
methods while using standard categorical prediction. This work demonstrates
that bridging discrete and continuous paradigms can effectively harness the
strengths of both approaches, providing a promising direction for high-quality
visual generation with simple autoregressive modeling. Project page:
https://yuqingwang1029.github.io/TokenBridge.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:59:59 GMT'}]",2025-03-21,"[['Wang', 'Yuqing', ''], ['Lin', 'Zhijie', ''], ['Teng', 'Yao', ''], ['Zhu', 'Yuanzhi', ''], ['Ren', 'Shuhuai', ''], ['Feng', 'Jiashi', ''], ['Liu', 'Xihui', '']]","[{'text': 'continuous tokens', 'label': 'Large Language Model'}, {'text': 'continuous tokens', 'label': 'Large Language Model'}, {'text': 'post-training quantization', 'label': 'quantisation'}, {'text': 'dimension-wise\nquantization', 'label': 'quantisation'}]",quantisation,post-training quantization,0.6493542194366455
