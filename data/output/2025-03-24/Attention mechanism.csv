id,submitter,authors,title,comments,journal-ref,doi,report-no,categories,license,abstract,versions,update_date,authors_parsed,extracted_entities,assigned_concept,matched_keyword,similarity_score
2108.06062,Yike Xu,Yike Xu and Mark S. Andersland,Worst-Case Services and State-Based Scheduling,,,,,eess.SY cs.SY math.OC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we shed new light on a classical scheduling problem: given a
slot-timed, constant-capacity server, what short-run scheduling decisions must
be made to provide long-run service guarantees to competing flows of unit-sized
tasks? We model each flow's long-run guarantee as a worst-case service that
maps each queued arrival vector recording the flow's cumulative task arrivals,
including those initially queued, to a worst-case acceptable departure vector
lower-bounding its cumulative served tasks. We show that these maps are states
that can be updated as tasks arrive and are served, introduce state-based
scheduling, find the schedulability condition necessary and sufficient to
maintain all flows' long-run guarantees, and use this condition to identify all
short-run scheduling decisions that preserve schedulability. Our framework is
general but computationally complex. To reduce complexity, we consider three
specializations. First, we show that when satisfactory short-run scheduling
decisions exist, at least one can be efficiently identified by maximizing the
server's capacity slack, a generalization of the earliest-deadline-first rule.
Second, we show that a special class of worst-case services, min-plus services,
can be efficiently specified and updated using properties of the min-plus
algebra. Finally, we show that efficiency can be further improved by
restricting attention to a min-plus service subclass, dual-curve services. This
last specialization turns out to be a dynamic extension of service curves that
maintains all essential features of our framework while approaching near
practical viability.
","[{'version': 'v1', 'created': 'Fri, 13 Aug 2021 05:00:10 GMT'}, {'version': 'v2', 'created': 'Tue, 22 Nov 2022 08:15:59 GMT'}, {'version': 'v3', 'created': 'Sat, 29 Apr 2023 14:42:13 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 04:26:47 GMT'}]",2025-03-21,"[['Xu', 'Yike', ''], ['Andersland', 'Mark S.', '']]","[{'text': 'attention', 'label': 'Attention mechanism'}]",Attention mechanism,attention,0.7383304834365845
2209.12075,Jiamian Wang,"Jiamian Wang, Kunpeng Li, Yulun Zhang, Xin Yuan, Zhiqiang Tao",S^2-Transformer for Mask-Aware Hyperspectral Image Reconstruction,Accepted by TPAMI,,,,eess.IV cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Snapshot compressive imaging (SCI) surges as a novel way of capturing
hyperspectral images. It operates an optical encoder to compress the 3D data
into a 2D measurement and adopts a software decoder for the signal
reconstruction. Recently, a representative SCI set-up of coded aperture
snapshot compressive imager (CASSI) with Transformer reconstruction backend
remarks high-fidelity sensing performance. However, dominant spatial and
spectral attention designs show limitations in hyperspectral modeling. The
spatial attention values describe the inter-pixel correlation but overlook the
across-spectra variation within each pixel. The spectral attention size is
unscalable to the token spatial size and thus bottlenecks information
allocation. Besides, CASSI entangles the spatial and spectral information into
a 2D measurement, placing a barrier for information disentanglement and
modeling. In addition, CASSI blocks the light with a physical binary mask,
yielding the masked data loss. To tackle above challenges, we propose a
spatial-spectral (S2-) Transformer implemented by a paralleled attention design
and a mask-aware learning strategy. Firstly, we systematically explore pros and
cons of different spatial (-spectral) attention designs, based on which we find
performing both attentions in parallel well disentangles and models the blended
information. Secondly, the masked pixels induce higher prediction difficulty
and should be treated differently from unmasked ones. We adaptively prioritize
the loss penalty attributing to the mask structure by referring to the
mask-encoded prediction as an uncertainty estimator. We theoretically discuss
the distinct convergence tendencies between masked/unmasked regions of the
proposed learning strategy. Extensive experiments demonstrate that on average,
the results of the proposed method are superior over the state-of-the-art
method.
","[{'version': 'v1', 'created': 'Sat, 24 Sep 2022 19:26:46 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Dec 2022 15:41:22 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 23:57:52 GMT'}]",2025-03-21,"[['Wang', 'Jiamian', ''], ['Li', 'Kunpeng', ''], ['Zhang', 'Yulun', ''], ['Yuan', 'Xin', ''], ['Tao', 'Zhiqiang', '']]","[{'text': 'spectral attention', 'label': 'Attention mechanism'}, {'text': 'mask-aware learning strategy', 'label': 'Few-shot Learning'}]",Attention mechanism,spectral attention,0.6150141954421997
2312.01061,Huan Chen,"Huan Chen, Wangcai Zhao, Tingfa Xu, Shiyun Zhou, Peifu Liu and Jianan
  Li","Spectral-wise Implicit Neural Representation for Hyperspectral Image
  Reconstruction","Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology, has been published","Volume: 34, Issue: 5, May 2024",10.1109/TCSVT.2023.3318366,,eess.IV cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Coded Aperture Snapshot Spectral Imaging (CASSI) reconstruction aims to
recover the 3D spatial-spectral signal from 2D measurement. Existing methods
for reconstructing Hyperspectral Image (HSI) typically involve learning
mappings from a 2D compressed image to a predetermined set of discrete spectral
bands. However, this approach overlooks the inherent continuity of the spectral
information. In this study, we propose an innovative method called
Spectral-wise Implicit Neural Representation (SINR) as a pioneering step toward
addressing this limitation. SINR introduces a continuous spectral amplification
process for HSI reconstruction, enabling spectral super-resolution with
customizable magnification factors. To achieve this, we leverage the concept of
implicit neural representation. Specifically, our approach introduces a
spectral-wise attention mechanism that treats individual channels as distinct
tokens, thereby capturing global spectral dependencies. Additionally, our
approach incorporates two components, namely a Fourier coordinate encoder and a
spectral scale factor module. The Fourier coordinate encoder enhances the
SINR's ability to emphasize high-frequency components, while the spectral scale
factor module guides the SINR to adapt to the variable number of spectral
channels. Notably, the SINR framework enhances the flexibility of CASSI
reconstruction by accommodating an unlimited number of spectral bands in the
desired output. Extensive experiments demonstrate that our SINR outperforms
baseline methods. By enabling continuous reconstruction within the CASSI
framework, we take the initial stride toward integrating implicit neural
representation into the field.
","[{'version': 'v1', 'created': 'Sat, 2 Dec 2023 08:06:07 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 02:47:41 GMT'}]",2025-03-19,"[['Chen', 'Huan', ''], ['Zhao', 'Wangcai', ''], ['Xu', 'Tingfa', ''], ['Zhou', 'Shiyun', ''], ['Liu', 'Peifu', ''], ['Li', 'Jianan', '']]","[{'text': 'spectral-wise attention mechanism', 'label': 'Attention mechanism'}]",Attention mechanism,spectral-wise attention mechanism,0.8220318555831909
2403.05906,Jingyun Xue,"Jingyun Xue, Tao Wang, Pengwen Dai, Kaihao Zhang","Segmentation Guided Sparse Transformer for Under-Display Camera Image
  Restoration","13 pages, 10 figures, conference or other essential info",,,,eess.IV cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Under-Display Camera (UDC) is an emerging technology that achieves
full-screen display via hiding the camera under the display panel. However, the
current implementation of UDC causes serious degradation. The incident light
required for camera imaging undergoes attenuation and diffraction when passing
through the display panel, leading to various artifacts in UDC imaging.
Presently, the prevailing UDC image restoration methods predominantly utilize
convolutional neural network architectures, whereas Transformer-based methods
have exhibited superior performance in the majority of image restoration tasks.
This is attributed to the Transformer's capability to sample global features
for the local reconstruction of images, thereby achieving high-quality image
restoration. In this paper, we observe that when using the Vision Transformer
for UDC degraded image restoration, the global attention samples a large amount
of redundant information and noise. Furthermore, compared to the ordinary
Transformer employing dense attention, the Transformer utilizing sparse
attention can alleviate the adverse impact of redundant information and noise.
Building upon this discovery, we propose a Segmentation Guided Sparse
Transformer method (SGSFormer) for the task of restoring high-quality images
from UDC degraded images. Specifically, we utilize sparse self-attention to
filter out redundant information and noise, directing the model's attention to
focus on the features more relevant to the degraded regions in need of
reconstruction. Moreover, we integrate the instance segmentation map as prior
information to guide the sparse self-attention in filtering and focusing on the
correct regions.
","[{'version': 'v1', 'created': 'Sat, 9 Mar 2024 13:11:59 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 11:49:18 GMT'}]",2025-03-21,"[['Xue', 'Jingyun', ''], ['Wang', 'Tao', ''], ['Dai', 'Pengwen', ''], ['Zhang', 'Kaihao', '']]","[{'text': 'global attention', 'label': 'Attention mechanism'}, {'text': 'dense attention', 'label': 'Attention mechanism'}, {'text': 'sparse\nattention', 'label': 'Attention mechanism'}, {'text': 'sparse self-attention', 'label': 'Attention mechanism'}, {'text': 'sparse self-attention', 'label': 'Attention mechanism'}]",Attention mechanism,dense attention,0.7055617570877075
2405.04476,Wang Lijun,"Lijun Wang, Yixian Lu, Ziyan Gao, Kai Li, Jianqiang Huang, Yuntao
  Kong, Shogo Okada","BERP: A Blind Estimator of Room Parameters for Single-Channel Noisy
  Speech Signals","16-page with supplementary materials, Submitted to IEEE/ACM
  Transaction on Audio Speech and Language Processing (TASLP)",,,,eess.AS cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Room acoustical parameters (RAPs), room geometrical parameters (RGPs) and
instantaneous occupancy level are essential metrics for parameterizing the room
acoustical characteristics (RACs) of a sound field around a listener's local
environment, offering comprehensive indications for various applications.
Current blind estimation methods either fail to cover a broad range of
real-world acoustic environments in the context of real background noise or
estimate only a few RAPs and RGPs from noisy single-channel speech signals. In
addition, they are limited in their ability to estimate the instantaneous
occupancy level. In this paper, we propose a new universal blind estimation
framework called the blind estimator of room parameters (BERP) to estimate
RAPs, RGPs and occupancy level via a unified methodology. It consists of two
modules: a unified room feature encoder that combines attention mechanisms with
convolutional layers to learn common features across room parameters, and
multiple separate parametric predictors for continuous estimation of each
parameter in parallel. The combination of attention and convolutions enables
the model to capture acoustic features locally and globally from speech,
yielding more robust and multitask generalizable common features. Separate
predictors allow the model to independently optimize for each room parameter to
reduce task learning conflict and improve per-task performance. This estimation
framework enables universal and efficient estimation of room parameters while
maintaining satisfactory performance. To evaluate the effectiveness of the
proposed framework, we compile a task-specific dataset from several publicly
available datasets, including synthetic and real reverberant recordings. The
results reveal that BERP achieves state-of-the-art (SOTA) performance and
excellent adaptability to real-world scenarios. The code and weights are
available on GitHub.
","[{'version': 'v1', 'created': 'Tue, 7 May 2024 16:41:41 GMT'}, {'version': 'v2', 'created': 'Thu, 16 May 2024 10:17:12 GMT'}, {'version': 'v3', 'created': 'Sat, 19 Oct 2024 12:44:24 GMT'}, {'version': 'v4', 'created': 'Wed, 23 Oct 2024 11:01:59 GMT'}, {'version': 'v5', 'created': 'Thu, 24 Oct 2024 01:59:56 GMT'}, {'version': 'v6', 'created': 'Tue, 18 Mar 2025 15:08:12 GMT'}]",2025-03-19,"[['Wang', 'Lijun', ''], ['Lu', 'Yixian', ''], ['Gao', 'Ziyan', ''], ['Li', 'Kai', ''], ['Huang', 'Jianqiang', ''], ['Kong', 'Yuntao', ''], ['Okada', 'Shogo', '']]","[{'text': 'attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'GitHub', 'label': 'Open-source LLMs'}]",Attention mechanism,attention mechanisms,0.9558142423629761
2406.06652,Yubin Xiao,"Yubin Xiao, Di Wang, Xuan Wu, Yuesong Wu, Boyang Li, Wei Du, Liupu
  Wang, You Zhou","Improving Generalization of Neural Vehicle Routing Problem Solvers
  Through the Lens of Model Architecture",This work has been accepted by Neural Networks,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural models produce promising results when solving Vehicle Routing Problems
(VRPs), but often fall short in generalization. Recent attempts to enhance
model generalization often incur unnecessarily large training cost or cannot be
directly applied to other models solving different VRP variants. To address
these issues, we take a novel perspective on model architecture in this study.
Specifically, we propose a plug-and-play Entropy-based Scaling Factor (ESF) and
a Distribution-Specific (DS) decoder to enhance the size and distribution
generalization, respectively. ESF adjusts the attention weight pattern of the
model towards familiar ones discovered during training when solving VRPs of
varying sizes. The DS decoder explicitly models VRPs of multiple training
distribution patterns through multiple auxiliary light decoders, expanding the
model representation space to encompass a broader range of distributional
scenarios. We conduct extensive experiments on both synthetic and widely
recognized real-world benchmarking datasets and compare the performance with
seven baseline models. The results demonstrate the effectiveness of using ESF
and DS decoder to obtain a more generalizable model and showcase their
applicability to solve different VRP variants, i.e., travelling salesman
problem and capacitated VRP. Notably, our proposed generic components require
minimal computational resources, and can be effortlessly integrated into
conventional generalization strategies to further elevate model generalization.
","[{'version': 'v1', 'created': 'Mon, 10 Jun 2024 09:03:17 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Jun 2024 14:02:57 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 08:40:04 GMT'}]",2025-03-19,"[['Xiao', 'Yubin', ''], ['Wang', 'Di', ''], ['Wu', 'Xuan', ''], ['Wu', 'Yuesong', ''], ['Li', 'Boyang', ''], ['Du', 'Wei', ''], ['Wang', 'Liupu', ''], ['Zhou', 'You', '']]","[{'text': 'attention weight pattern', 'label': 'Attention mechanism'}]",Attention mechanism,attention weight pattern,0.7244582176208496
2406.11579,Yiming Zhang,"Han-Hung Lee, Yiming Zhang, Angel X. Chang",Duoduo CLIP: Efficient 3D Understanding with Multi-View Images,ICLR 2025,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  We introduce Duoduo CLIP, a model for 3D representation learning that learns
shape encodings from multi-view images instead of point clouds. The choice of
multi-view images allows us to leverage 2D priors from off-the-shelf CLIP
models to facilitate fine-tuning with 3D data. Our approach not only shows
better generalization compared to existing point cloud methods, but also
reduces GPU requirements and training time. In addition, the model is modified
with cross-view attention to leverage information across multiple frames of the
object which further boosts performance. Notably, our model is permutation
invariant to the order of multi-view images while being pose-free. Compared to
the current SOTA point cloud method that requires 480 A100 hours to train 1
billion model parameters we only require 57 A5000 hours and 87 million
parameters. Multi-view images also provide more flexibility including being
able to encode objects with a variable number of images, and performance scales
when more views are used. In contrast, point cloud based methods require an
entire scan or model of the object. We showcase this flexibility with
benchmarks from images of real-world objects. Our model also achieves better
performance in more fine-grained text to shape retrieval, demonstrating better
text-and-shape alignment than point cloud based models.
","[{'version': 'v1', 'created': 'Mon, 17 Jun 2024 14:16:12 GMT'}, {'version': 'v2', 'created': 'Thu, 17 Oct 2024 21:05:16 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 23:55:24 GMT'}]",2025-03-21,"[['Lee', 'Han-Hung', ''], ['Zhang', 'Yiming', ''], ['Chang', 'Angel X.', '']]","[{'text': 'cross-view attention', 'label': 'Attention mechanism'}]",Attention mechanism,cross-view attention,0.5597537755966187
2406.12179,Roman Beliy,"Roman Beliy, Navve Wasserman, Amit Zalcher, Michal Irani",The Wisdom of a Crowd of Brains: A Universal Brain Encoder,,,,,cs.CV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Image-to-fMRI encoding is important for both neuroscience research and
practical applications. However, such ""Brain-Encoders"" have been typically
trained per-subject and per fMRI-dataset, thus restricted to very limited
training data. In this paper we propose a Universal Brain-Encoder, which can be
trained jointly on data from many different subjects/datasets/machines. What
makes this possible is our new voxel-centric Encoder architecture, which learns
a unique ""voxel-embedding"" per brain-voxel. Our Encoder trains to predict the
response of each brain-voxel on every image, by directly computing the
cross-attention between the brain-voxel embedding and multi-level deep image
features. This voxel-centric architecture allows the functional role of each
brain-voxel to naturally emerge from the voxel-image cross-attention. We show
the power of this approach to (i) combine data from multiple different subjects
(a ""Crowd of Brains"") to improve each individual brain-encoding, (ii) quick &
effective Transfer-Learning across subjects, datasets, and machines (e.g.,
3-Tesla, 7-Tesla), with few training examples, and (iii) use the learned
voxel-embeddings as a powerful tool to explore brain functionality (e.g., what
is encoded where in the brain).
","[{'version': 'v1', 'created': 'Tue, 18 Jun 2024 01:17:07 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 23:24:48 GMT'}]",2025-03-21,"[['Beliy', 'Roman', ''], ['Wasserman', 'Navve', ''], ['Zalcher', 'Amit', ''], ['Irani', 'Michal', '']]","[{'text': 'Image-to-fMRI encoding', 'label': 'Embedding'}, {'text': 'voxel-embedding', 'label': 'Embedding'}, {'text': 'cross-attention', 'label': 'Attention mechanism'}, {'text': 'brain-voxel embedding', 'label': 'Embedding'}, {'text': 'voxel-image cross-attention', 'label': 'Attention mechanism'}, {'text': 'Transfer-Learning', 'label': 'Few-shot Learning'}, {'text': 'voxel-embeddings', 'label': 'Embedding'}]",Attention mechanism,cross-attention,0.6773566007614136
2407.10310,Dong Chen,"Dong Chen, Arman Hosseini, Arik Smith, Zeyang Zheng, David Xiang,
  Arsalan Heydarian, Omid Shoghli, Bradford Campbell","Impact of Road Infrastructure and Traffic Scenarios on E-scooterists'
  Riding and Gaze Behavior","12 pages, 10 figures","International Conference on Transportation & Development (ICTD
  2025)",,,cs.CY cs.SY eess.SY,http://creativecommons.org/licenses/by/4.0/,"  The growing adoption of e-scooters has raised significant safety concerns,
particularly due to a surge in injuries and fatalities. This study explores the
relationship between road infrastructure, traffic scenarios, and e-scooterists'
riding and gaze behaviors to improve road safety and user experience. A
naturalistic study was conducted using instrumented e-scooters, capturing gaze
patterns, fixation metrics, and head movement data across various road layouts
and traffic scenarios. Key findings reveal that bike lanes offer a stable
environment with reduced horizontal head movement and focused attention on the
road, while shared roads and sidewalks lead to more dispersed gaze and
increased head movement, indicating higher uncertainty and complexity.
Interactions with other road users, such as navigating intersections, passing
buses, riding near cars, and descending on downhill paths, demand greater
cognitive load. Intersections require heightened visual focus and spatial
awareness, reflected in increased horizontal eye and head movements.
Interactions with vehicles prioritize visual scanning over head movement to
maintain stability and avoid collisions, while high-speed and downhill riding
demand focused attention on obstacles and the road surface. The results provide
insights into e-scooter riders' behavior and physiological response analysis,
paving the way for safer riding experiences and improved understanding of their
needs.
","[{'version': 'v1', 'created': 'Sun, 5 May 2024 19:55:46 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 03:00:26 GMT'}]",2025-03-18,"[['Chen', 'Dong', ''], ['Hosseini', 'Arman', ''], ['Smith', 'Arik', ''], ['Zheng', 'Zeyang', ''], ['Xiang', 'David', ''], ['Heydarian', 'Arsalan', ''], ['Shoghli', 'Omid', ''], ['Campbell', 'Bradford', '']]","[{'text': 'gaze\npatterns', 'label': 'Attention mechanism'}, {'text': 'focused attention', 'label': 'Attention mechanism'}, {'text': 'focused attention', 'label': 'Attention mechanism'}]",Attention mechanism,focused attention,0.7952965497970581
2407.12331,Junseo Park,Junseo Park and Hyeryung Jang,"I2AM: Interpreting Image-to-Image Latent Diffusion Models via
  Bi-Attribution Maps",23 pages,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large-scale diffusion models have made significant advances in image
generation, particularly through cross-attention mechanisms. While
cross-attention has been well-studied in text-to-image tasks, their
interpretability in image-to-image (I2I) diffusion models remains
underexplored. This paper introduces Image-to-Image Attribution Maps (I2AM), a
method that enhances the interpretability of I2I models by visualizing
bidirectional attribution maps, from the reference image to the generated image
and vice versa. I2AM aggregates cross-attention scores across time steps,
attention heads, and layers, offering insights into how critical features are
transferred between images. We demonstrate the effectiveness of I2AM across
object detection, inpainting, and super-resolution tasks. Our results
demonstrate that I2AM successfully identifies key regions responsible for
generating the output, even in complex scenes. Additionally, we introduce the
Inpainting Mask Attention Consistency Score (IMACS) as a novel evaluation
metric to assess the alignment between attribution maps and inpainting masks,
which correlates strongly with existing performance metrics. Through extensive
experiments, we show that I2AM enables model debugging and refinement,
providing practical tools for improving I2I model's performance and
interpretability.
","[{'version': 'v1', 'created': 'Wed, 17 Jul 2024 06:15:05 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 08:27:10 GMT'}]",2025-03-21,"[['Park', 'Junseo', ''], ['Jang', 'Hyeryung', '']]","[{'text': 'cross-attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'cross-attention', 'label': 'Attention mechanism'}, {'text': 'cross-attention', 'label': 'Attention mechanism'}]",Attention mechanism,cross-attention mechanisms,0.8177332282066345
2407.15176,Xiaoran Liu,"Xiaoran Liu, Ruixiao Li, Qipeng Guo, Zhigeng Liu, Yuerong Song, Kai
  Lv, Hang Yan, Linlin Li, Qun Liu, Xipeng Qiu",ReAttention: Training-Free Infinite Context with Finite Attention Scope,"21 pages, 11 figures, Accepted by ICLR 2025",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The long-context capability of the Large Language Models (LLM) has made
significant breakthroughs, but the maximum supported context length in length
extrapolation remains a critical bottleneck limiting their practical
applications. The constraint of context length in LLMs arises from the
self-attention mechanism, which cannot effectively and efficiently capture the
semantic relationships within infinitely long contexts via the limited
pre-trained positional information and attention scope. In this work, we
propose ReAttention, a training-free approach enabling LLM based on the
self-attention mechanism to support an infinite context with a finite attention
scope under sufficient memory resources. ReAttention performs the
position-agnostic top-$k$ attention before the ordinary position-aware
self-attention, freeing LLMs from the length extrapolation issue. We validate
the performance of ReAttention on the LongBench, L-Eval, and InfiniteBench and
demonstrate that it is on par with traditional methods. Furthermore, we also
apply ReAttention on mainstream LLMs, including LLaMA3.1-8B and
Mistral-v0.3-7B, enabling them to support context lengths of at least 1M and
even expanding the context length of LLaMA3.2-3B-chat by 128$\times$ to 4M
without any further training in Needle-In-A-Haystack tests. We also improve the
efficiency of ReAttention with Triton and achieve an efficient extrapolation
without additional overhead. The code is available at
https://github.com/OpenMOSS/ReAttention.
","[{'version': 'v1', 'created': 'Sun, 21 Jul 2024 14:23:37 GMT'}, {'version': 'v2', 'created': 'Sat, 5 Oct 2024 02:09:26 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 12:15:10 GMT'}]",2025-03-20,"[['Liu', 'Xiaoran', ''], ['Li', 'Ruixiao', ''], ['Guo', 'Qipeng', ''], ['Liu', 'Zhigeng', ''], ['Song', 'Yuerong', ''], ['Lv', 'Kai', ''], ['Yan', 'Hang', ''], ['Li', 'Linlin', ''], ['Liu', 'Qun', ''], ['Qiu', 'Xipeng', '']]","[{'text': 'self-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'ReAttention', 'label': 'contextual Embedding'}, {'text': 'self-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'ReAttention', 'label': 'contextual Embedding'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'ReAttention', 'label': 'contextual Embedding'}, {'text': 'ReAttention', 'label': 'contextual Embedding'}, {'text': 'ReAttention', 'label': 'contextual Embedding'}, {'text': 'Triton', 'label': 'Mistral'}, {'text': 'ReAttention', 'label': 'contextual Embedding'}]",Attention mechanism,self-attention mechanism,0.8757837414741516
2408.12252,Haotian Zhang,"Haotian Zhang, Shijian Gao, Xiang Cheng, Liuqing Yang","Synesthesia of Machines (SoM)-Enhanced Wideband Multi-User CSI Learning
  With LiDAR Sensing","6 pages, 4 figures, 1 table",,,,eess.SP,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Light detection and ranging (LiDAR) has been utilized for optimizing wireless
communications due to its ability to detect the environment. This paper
explores the use of LiDAR in channel estimation for wideband multi-user
multiple-input-multiple-output orthogonal frequency division multiplexing
systems and introduces a LiDAR-enhanced Channel State Information (CSI)
learning network (LE-CLN). By utilizing user positioning information, LE-CLN
first calculates user-localized over-complete angular measurements. It then
investigates the correlation between LiDAR and CSI, transforming raw LiDAR data
into a low-complexity format embedded with signal propagation characteristics.
LE-CLN also adapts the use of LiDAR based on channel conditions through
attention mechanisms. Thanks to the unique wireless features offered by LiDAR,
LE-CLN achieves higher estimation accuracy and spectrum efficiency compared to
benchmarks, particularly in latency-sensitive applications where pilot
transmissions are expected to be reduced.
","[{'version': 'v1', 'created': 'Thu, 22 Aug 2024 09:44:15 GMT'}, {'version': 'v2', 'created': 'Fri, 6 Dec 2024 02:41:47 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 05:46:23 GMT'}]",2025-03-19,"[['Zhang', 'Haotian', ''], ['Gao', 'Shijian', ''], ['Cheng', 'Xiang', ''], ['Yang', 'Liuqing', '']]","[{'text': 'attention mechanisms', 'label': 'Attention mechanism'}]",Attention mechanism,attention mechanisms,0.9558142423629761
2408.15185,Ghazal Alinezhad Noghre,"Ghazal Alinezhad Noghre, Armin Danesh Pazho, Hamed Tabkhi","Human-Centric Video Anomaly Detection Through Spatio-Temporal Pose
  Tokenization and Transformer",,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Video Anomaly Detection (VAD) presents a significant challenge in computer
vision, particularly due to the unpredictable and infrequent nature of
anomalous events, coupled with the diverse and dynamic environments in which
they occur. Human-centric VAD, a specialized area within this domain, faces
additional complexities, including variations in human behavior, potential
biases in data, and substantial privacy concerns related to human subjects.
These issues complicate the development of models that are both robust and
generalizable. To address these challenges, recent advancements have focused on
pose-based VAD, which leverages human pose as a high-level feature to mitigate
privacy concerns, reduce appearance biases, and minimize background
interference. In this paper, we introduce SPARTA, a novel transformer-based
architecture designed specifically for human-centric pose-based VAD. SPARTA
introduces an innovative Spatio-Temporal Pose and Relative Pose (ST-PRP)
tokenization method that produces an enriched representation of human motion
over time. This approach ensures that the transformer's attention mechanism
captures both spatial and temporal patterns simultaneously, rather than
focusing on only one aspect. The addition of the relative pose further
emphasizes subtle deviations from normal human movements. The architecture's
core, a novel Unified Encoder Twin Decoders (UETD) transformer, significantly
improves the detection of anomalous behaviors in video data. Extensive
evaluations across multiple benchmark datasets demonstrate that SPARTA
consistently outperforms existing methods, establishing a new state-of-the-art
in pose-based VAD.
","[{'version': 'v1', 'created': 'Tue, 27 Aug 2024 16:40:14 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 14:05:49 GMT'}]",2025-03-18,"[['Noghre', 'Ghazal Alinezhad', ''], ['Pazho', 'Armin Danesh', ''], ['Tabkhi', 'Hamed', '']]","[{'text': 'privacy concerns', 'label': 'AI Ethics'}, {'text': 'privacy concerns', 'label': 'AI Ethics'}, {'text': 'attention mechanism', 'label': 'Attention mechanism'}]",Attention mechanism,attention mechanism,1.0
2409.17273,Shravan Venkatraman,"Shravan Venkatraman, Pandiyaraju V, Abeshek A, Aravintakshan S A,
  Pavan Kumar S, Kannan A, Madhan S","Targeted Neural Architectures in Multi-Objective Frameworks for Complete
  Glioma Characterization from Multimodal MRI","29 pages, 25 figures, 6 tables",,,,eess.IV cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Brain tumors result from abnormal cell growth in brain tissue. If
undiagnosed, they cause neurological deficits, including cognitive impairment,
motor dysfunction, and sensory loss. As tumors grow, intracranial pressure
increases, potentially leading to fatal complications such as brain herniation.
Early diagnosis and treatment are crucial to controlling these effects and
slowing tumor progression. Deep learning (DL) and artificial intelligence (AI)
are increasingly used to assist doctors in early diagnosis through magnetic
resonance imaging (MRI) scans. Our research proposes targeted neural
architectures within multi-objective frameworks that can localize, segment, and
classify the grade of these gliomas from multimodal MRI images to solve this
critical issue. Our localization framework utilizes a targeted architecture
that enhances the LinkNet framework with an encoder inspired by VGG19 for
better multimodal feature extraction from the tumor along with spatial and
graph attention mechanisms that sharpen feature focus and inter-feature
relationships. For the segmentation objective, we deployed a specialized
framework using the SeResNet101 CNN model as the encoder backbone integrated
into the LinkNet architecture, achieving an IoU Score of 96%. The
classification objective is addressed through a distinct framework implemented
by combining the SeResNet152 feature extractor with Adaptive Boosting
classifier, reaching an accuracy of 98.53%. Our multi-objective approach with
targeted neural architectures demonstrated promising results for complete
glioma characterization, with the potential to advance medical AI by enabling
early diagnosis and providing more accurate treatment options for patients.
","[{'version': 'v1', 'created': 'Wed, 25 Sep 2024 18:38:57 GMT'}, {'version': 'v2', 'created': 'Sat, 23 Nov 2024 07:55:26 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 15:56:39 GMT'}]",2025-03-19,"[['Venkatraman', 'Shravan', ''], ['V', 'Pandiyaraju', ''], ['A', 'Abeshek', ''], ['A', 'Aravintakshan S', ''], ['S', 'Pavan Kumar', ''], ['A', 'Kannan', ''], ['S', 'Madhan', '']]","[{'text': 'spatial and\ngraph attention mechanisms', 'label': 'Attention mechanism'}]",Attention mechanism,"spatial and
graph attention mechanisms",0.7406826615333557
2410.21967,Chengkai Huang,"Hongtao Huang, Chengkai Huang, Tong Yu, Xiaojun Chang, Wen Hu, Julian
  McAuley, Lina Yao",Dual Conditional Diffusion Models for Sequential Recommendation,,,,,cs.IR cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Recent advancements in diffusion models have shown promising results in
sequential recommendation (SR). Existing approaches predominantly rely on
implicit conditional diffusion models, which compress user behaviors into a
single representation during the forward diffusion process. While effective to
some extent, this oversimplification often leads to the loss of sequential and
contextual information, which is critical for understanding user behavior.
Moreover, explicit information, such as user-item interactions or sequential
patterns, remains underutilized, despite its potential to directly guide the
recommendation process and improve precision. However, combining implicit and
explicit information is non-trivial, as it requires dynamically integrating
these complementary signals while avoiding noise and irrelevant patterns within
user behaviors. To address these challenges, we propose Dual Conditional
Diffusion Models for Sequential Recommendation (DCRec), which effectively
integrates implicit and explicit information by embedding dual conditions into
both the forward and reverse diffusion processes. This allows the model to
retain valuable sequential and contextual information while leveraging explicit
user-item interactions to guide the recommendation process. Specifically, we
introduce the Dual Conditional Diffusion Transformer (DCDT), which employs a
cross-attention mechanism to dynamically integrate explicit signals throughout
the diffusion stages, ensuring contextual understanding and minimizing the
influence of irrelevant patterns. This design enables precise and contextually
relevant recommendations. Extensive experiments on public benchmark datasets
demonstrate that DCRec significantly outperforms state-of-the-art methods in
both accuracy and computational efficiency.
","[{'version': 'v1', 'created': 'Tue, 29 Oct 2024 11:51:06 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 04:42:54 GMT'}]",2025-03-19,"[['Huang', 'Hongtao', ''], ['Huang', 'Chengkai', ''], ['Yu', 'Tong', ''], ['Chang', 'Xiaojun', ''], ['Hu', 'Wen', ''], ['McAuley', 'Julian', ''], ['Yao', 'Lina', '']]","[{'text': 'cross-attention mechanism', 'label': 'Attention mechanism'}]",Attention mechanism,cross-attention mechanism,0.8302809000015259
2411.02767,Uday Kiran Reddy Tadipatri,"Uday Kiran Reddy Tadipatri, Benjamin D. Haeffele, Joshua Agterberg,
  Ren\'e Vidal","A Convex Relaxation Approach to Generalization Analysis for Parallel
  Positively Homogeneous Networks",Accepted at AISTATS 2025,,,,cs.LG eess.SP stat.ML,http://creativecommons.org/licenses/by/4.0/,"  We propose a general framework for deriving generalization bounds for
parallel positively homogeneous neural networks--a class of neural networks
whose input-output map decomposes as the sum of positively homogeneous maps.
Examples of such networks include matrix factorization and sensing,
single-layer multi-head attention mechanisms, tensor factorization, deep linear
and ReLU networks, and more. Our general framework is based on linking the
non-convex empirical risk minimization (ERM) problem to a closely related
convex optimization problem over prediction functions, which provides a global,
achievable lower-bound to the ERM problem. We exploit this convex lower-bound
to perform generalization analysis in the convex space while controlling the
discrepancy between the convex model and its non-convex counterpart. We apply
our general framework to a wide variety of models ranging from low-rank matrix
sensing, to structured matrix sensing, two-layer linear networks, two-layer
ReLU networks, and single-layer multi-head attention mechanisms, achieving
generalization bounds with a sample complexity that scales almost linearly with
the network width.
","[{'version': 'v1', 'created': 'Tue, 5 Nov 2024 03:24:34 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 21:12:45 GMT'}]",2025-03-20,"[['Tadipatri', 'Uday Kiran Reddy', ''], ['Haeffele', 'Benjamin D.', ''], ['Agterberg', 'Joshua', ''], ['Vidal', 'Ren√©', '']]","[{'text': 'single-layer multi-head attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'single-layer multi-head attention mechanisms', 'label': 'Attention mechanism'}]",Attention mechanism,single-layer multi-head attention mechanisms,0.7406455874443054
2411.08205,Ricardo Felipe Ferreira,"Ricardo F. Ferreira, Matheus E. Pacola, Vitor G. Schiavone and Rodrigo
  F. O. Pena","Consistent model selection for estimating functional interactions among
  stochastic neurons with variable-length memory","51 pages, 2 figures",,,,stat.AP,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We address the problem of identifying functional interactions among
stochastic neurons with variable-length memory from their spiking activity. The
neuronal network is modeled by a stochastic system of interacting point
processes with variable-length memory. Each chain describes the activity of a
single neuron, indicating whether it spikes at a given time. One neuron's
influence on another can be either excitatory or inhibitory. To identify the
existence and nature of an interaction between a neuron and its postsynaptic
counterpart, we propose a model selection procedure based on the observation of
the spike activity of a finite set of neurons over a finite time. The proposed
procedure is also based on the maximum likelihood estimator for the synaptic
weight matrix of the network neuronal model. In this sense, we prove the
consistency of the maximum likelihood estimator {followed} by a proof of the
consistency of the neighborhood interaction estimation procedure. The
effectiveness of the proposed model selection procedure is demonstrated using
simulated data, which validates the underlying theory. The method is also
applied to analyze spike train data recorded from hippocampal neurons in rats
during a visual attention task, where a computational model reconstructs the
spiking activity and the results reveal interesting and biologically relevant
information.
","[{'version': 'v1', 'created': 'Tue, 12 Nov 2024 21:52:51 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 23:26:46 GMT'}]",2025-03-21,"[['Ferreira', 'Ricardo F.', ''], ['Pacola', 'Matheus E.', ''], ['Schiavone', 'Vitor G.', ''], ['Pena', 'Rodrigo F. O.', '']]","[{'text': 'visual attention task', 'label': 'Attention mechanism'}]",Attention mechanism,visual attention task,0.7174323797225952
2411.10411,Onay Urfalioglu,"Markus Karmann, Onay Urfalioglu","Repurposing Stable Diffusion Attention for Training-Free Unsupervised
  Interactive Segmentation",Accepted by CVPR 2025,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent progress in interactive point prompt based Image Segmentation allows
to significantly reduce the manual effort to obtain high quality semantic
labels. State-of-the-art unsupervised methods use self-supervised pre-trained
models to obtain pseudo-labels which are used in training a prompt-based
segmentation model. In this paper, we propose a novel unsupervised and
training-free approach based solely on the self-attention of Stable Diffusion.
We interpret the self-attention tensor as a Markov transition operator, which
enables us to iteratively construct a Markov chain. Pixel-wise counting of the
required number of iterations along the Markov chain to reach a relative
probability threshold yields a Markov-iteration-map, which we simply call a
Markov-map. Compared to the raw attention maps, we show that our proposed
Markov-map has less noise, sharper semantic boundaries and more uniform values
within semantically similar regions. We integrate the Markov-map in a simple
yet effective truncated nearest neighbor framework to obtain interactive point
prompt based segmentation. Despite being training-free, we experimentally show
that our approach yields excellent results in terms of Number of Clicks (NoC),
even outperforming state-of-the-art training based unsupervised methods in most
of the datasets. Code is available at https://github.com/mkarmann/m2n2.
","[{'version': 'v1', 'created': 'Fri, 15 Nov 2024 18:29:59 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 16:15:14 GMT'}]",2025-03-21,"[['Karmann', 'Markus', ''], ['Urfalioglu', 'Onay', '']]","[{'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'Markov transition operator', 'label': 'Attention mechanism'}, {'text': 'Markov chain', 'label': 'Chain of thought'}, {'text': 'Markov chain', 'label': 'Attention mechanism'}, {'text': 'interactive point\nprompt based segmentation', 'label': 'Prompting'}]",Attention mechanism,self-attention,0.7317671179771423
2411.15205,Jignyu Zhuang,"Jingyu Zhuang, Di Kang, Linchao Bao, Liang Lin, Guanbin Li",DAGSM: Disentangled Avatar Generation with GS-enhanced Mesh,Accepted by CVPR 2025,,,,cs.CV cs.GR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text-driven avatar generation has gained significant attention owing to its
convenience. However, existing methods typically model the human body with all
garments as a single 3D model, limiting its usability, such as clothing
replacement, and reducing user control over the generation process. To overcome
the limitations above, we propose DAGSM, a novel pipeline that generates
disentangled human bodies and garments from the given text prompts.
Specifically, we model each part (e.g., body, upper/lower clothes) of the
clothed human as one GS-enhanced mesh (GSM), which is a traditional mesh
attached with 2D Gaussians to better handle complicated textures (e.g., woolen,
translucent clothes) and produce realistic cloth animations. During the
generation, we first create the unclothed body, followed by a sequence of
individual cloth generation based on the body, where we introduce a
semantic-based algorithm to achieve better human-cloth and garment-garment
separation. To improve texture quality, we propose a view-consistent texture
refinement module, including a cross-view attention mechanism for texture style
consistency and an incident-angle-weighted denoising (IAW-DE) strategy to
update the appearance. Extensive experiments have demonstrated that DAGSM
generates high-quality disentangled avatars, supports clothing replacement and
realistic animation, and outperforms the baselines in visual quality.
","[{'version': 'v1', 'created': 'Wed, 20 Nov 2024 07:00:48 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 03:14:15 GMT'}]",2025-03-19,"[['Zhuang', 'Jingyu', ''], ['Kang', 'Di', ''], ['Bao', 'Linchao', ''], ['Lin', 'Liang', ''], ['Li', 'Guanbin', '']]","[{'text': 'cross-view attention mechanism', 'label': 'Attention mechanism'}]",Attention mechanism,cross-view attention mechanism,0.7163535356521606
2411.17214,Xiaoming Zhang,"Chengxing Xie, Xiaoming Zhang, Linze Li, Yuqian Fu, Biao Gong, Tianrui
  Li, Kai Zhang","MAT: Multi-Range Attention Transformer for Efficient Image
  Super-Resolution",IEEE CSVT,,10.1109/TCSVT.2025.3553135,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Image super-resolution (SR) has significantly advanced through the adoption
of Transformer architectures. However, conventional techniques aimed at
enlarging the self-attention window to capture broader contexts come with
inherent drawbacks, especially the significantly increased computational
demands. Moreover, the feature perception within a fixed-size window of
existing models restricts the effective receptive field (ERF) and the
intermediate feature diversity. We demonstrate that a flexible integration of
attention across diverse spatial extents can yield significant performance
enhancements. In line with this insight, we introduce Multi-Range Attention
Transformer (MAT) for SR tasks. MAT leverages the computational advantages
inherent in dilation operation, in conjunction with self-attention mechanism,
to facilitate both multi-range attention (MA) and sparse multi-range attention
(SMA), enabling efficient capture of both regional and sparse global features.
Combined with local feature extraction, MAT adeptly capture dependencies across
various spatial ranges, improving the diversity and efficacy of its feature
representations. We also introduce the MSConvStar module, which augments the
model's ability for multi-range representation learning. Comprehensive
experiments show that our MAT exhibits superior performance to existing
state-of-the-art SR models with remarkable efficiency (~3.3 faster than
SRFormer-light).
","[{'version': 'v1', 'created': 'Tue, 26 Nov 2024 08:30:31 GMT'}, {'version': 'v2', 'created': 'Tue, 17 Dec 2024 03:01:53 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 03:09:55 GMT'}]",2025-03-20,"[['Xie', 'Chengxing', ''], ['Zhang', 'Xiaoming', ''], ['Li', 'Linze', ''], ['Fu', 'Yuqian', ''], ['Gong', 'Biao', ''], ['Li', 'Tianrui', ''], ['Zhang', 'Kai', '']]","[{'text': 'Transformer architectures', 'label': 'Transformers'}, {'text': 'self-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'multi-range attention', 'label': 'Attention mechanism'}, {'text': 'sparse multi-range attention', 'label': 'Attention mechanism'}, {'text': 'multi-range representation learning', 'label': 'Few-shot Learning'}]",Attention mechanism,self-attention mechanism,0.8757837414741516
2412.01537,Xingyu Chen,"Xingyu Chen, Zhuheng Song, Xiaoke Jiang, Yaoqing Hu, Junzhi Yu, Lei
  Zhang",HandOS: 3D Hand Reconstruction in One Stage,,,,,cs.CV cs.GR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Existing approaches of hand reconstruction predominantly adhere to a
multi-stage framework, encompassing detection, left-right classification, and
pose estimation. This paradigm induces redundant computation and cumulative
errors. In this work, we propose HandOS, an end-to-end framework for 3D hand
reconstruction. Our central motivation lies in leveraging a frozen detector as
the foundation while incorporating auxiliary modules for 2D and 3D keypoint
estimation. In this manner, we integrate the pose estimation capacity into the
detection framework, while at the same time obviating the necessity of using
the left-right category as a prerequisite. Specifically, we propose an
interactive 2D-3D decoder, where 2D joint semantics is derived from detection
cues while 3D representation is lifted from those of 2D joints. Furthermore,
hierarchical attention is designed to enable the concurrent modeling of 2D
joints, 3D vertices, and camera translation. Consequently, we achieve an
end-to-end integration of hand detection, 2D pose estimation, and 3D mesh
reconstruction within a one-stage framework, so that the above multi-stage
drawbacks are overcome. Meanwhile, the HandOS reaches state-of-the-art
performances on public benchmarks, e.g., 5.0 PA-MPJPE on FreiHand and 64.6\%
PCK@0.05 on HInt-Ego4D. Project page: idea-research.github.io/HandOSweb.
","[{'version': 'v1', 'created': 'Mon, 2 Dec 2024 14:28:29 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 02:43:35 GMT'}]",2025-03-20,"[['Chen', 'Xingyu', ''], ['Song', 'Zhuheng', ''], ['Jiang', 'Xiaoke', ''], ['Hu', 'Yaoqing', ''], ['Yu', 'Junzhi', ''], ['Zhang', 'Lei', '']]","[{'text': 'hierarchical attention', 'label': 'Attention mechanism'}]",Attention mechanism,hierarchical attention,0.6840362548828125
2412.03150,Siyoon Jin,"Siyoon Jin, Jisu Nam, Jiyoung Kim, Dahyun Chung, Yeong-Seok Kim,
  Joonhyung Park, Heonjeong Chu, Seungryong Kim","Appearance Matching Adapter for Exemplar-based Semantic Image Synthesis
  in-the-Wild",,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Exemplar-based semantic image synthesis generates images aligned with
semantic content while preserving the appearance of an exemplar. Conventional
structure-guidance models like ControlNet, are limited as they rely solely on
text prompts to control appearance and cannot utilize exemplar images as input.
Recent tuning-free approaches address this by transferring local appearance via
implicit cross-image matching in the augmented self-attention mechanism of
pre-trained diffusion models. However, prior works are often restricted to
single-object cases or foreground object appearance transfer, struggling with
complex scenes involving multiple objects. To overcome this, we propose
AM-Adapter (Appearance Matching Adapter) to address exemplar-based semantic
image synthesis in-the-wild, enabling multi-object appearance transfer from a
single scene-level image. AM-Adapter automatically transfers local appearances
from the scene-level input. AM-Adapter alternatively provides controllability
to map user-defined object details to specific locations in the synthesized
images. Our learnable framework enhances cross-image matching within augmented
self-attention by integrating semantic information from segmentation maps. To
disentangle generation and matching, we adopt stage-wise training. We first
train the structure-guidance and generation networks, followed by training the
matching adapter while keeping the others frozen. During inference, we
introduce an automated exemplar retrieval method for selecting exemplar
image-segmentation pairs efficiently. Despite utilizing minimal learnable
parameters, AM-Adapter achieves state-of-the-art performance, excelling in both
semantic alignment and local appearance fidelity. Extensive ablations validate
our design choices. Code and weights will be released.:
https://cvlab-kaist.github.io/AM-Adapter/
","[{'version': 'v1', 'created': 'Wed, 4 Dec 2024 09:17:47 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 07:31:49 GMT'}]",2025-03-19,"[['Jin', 'Siyoon', ''], ['Nam', 'Jisu', ''], ['Kim', 'Jiyoung', ''], ['Chung', 'Dahyun', ''], ['Kim', 'Yeong-Seok', ''], ['Park', 'Joonhyung', ''], ['Chu', 'Heonjeong', ''], ['Kim', 'Seungryong', '']]","[{'text': 'text prompts', 'label': 'Prompting'}, {'text': 'augmented self-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'AM-Adapter', 'label': 'LLM-based'}, {'text': 'AM-Adapter', 'label': 'LLM-based'}, {'text': 'AM-Adapter', 'label': 'LLM-based'}, {'text': 'AM-Adapter', 'label': 'LLM-based'}]",Attention mechanism,augmented self-attention mechanism,0.7923624515533447
2412.08460,Fermin Orozco,"Fermin Orozco, Pedro Porto Buarque de Gusm\~ao, Hongkai Wen, Johan
  Wahlstr\""om, Man Luo","Federated Learning for Traffic Flow Prediction with Synthetic Data
  Augmentation","11 pages, 7 figures, 6 tables, ACM format",,,,cs.LG cs.AI cs.DC,http://creativecommons.org/licenses/by/4.0/,"  Deep-learning based traffic prediction models require vast amounts of data to
learn embedded spatial and temporal dependencies. The inherent privacy and
commercial sensitivity of such data has encouraged a shift towards
decentralised data-driven methods, such as Federated Learning (FL). Under a
traditional Machine Learning paradigm, traffic flow prediction models can
capture spatial and temporal relationships within centralised data. In reality,
traffic data is likely distributed across separate data silos owned by multiple
stakeholders. In this work, a cross-silo FL setting is motivated to facilitate
stakeholder collaboration for optimal traffic flow prediction applications.
This work introduces an FL framework, referred to as FedTPS, to generate
synthetic data to augment each client's local dataset by training a
diffusion-based trajectory generation model through FL. The proposed framework
is evaluated on a large-scale real world ride-sharing dataset using various FL
methods and Traffic Flow Prediction models, including a novel prediction model
we introduce, which leverages Temporal and Graph Attention mechanisms to learn
the Spatio-Temporal dependencies embedded within regional traffic flow data.
Experimental results show that FedTPS outperforms multiple other FL baselines
with respect to global model performance.
","[{'version': 'v1', 'created': 'Wed, 11 Dec 2024 15:25:38 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 13:29:36 GMT'}]",2025-03-21,"[['Orozco', 'Fermin', ''], ['de Gusm√£o', 'Pedro Porto Buarque', ''], ['Wen', 'Hongkai', ''], ['Wahlstr√∂m', 'Johan', ''], ['Luo', 'Man', '']]","[{'text': 'Temporal and Graph Attention mechanisms', 'label': 'Attention mechanism'}]",Attention mechanism,Temporal and Graph Attention mechanisms,0.7344149351119995
2501.01023,Ziyang Chen,"Ziyang Chen, Yongjun Zhang, Wenting Li, Bingshu Wang, Yabo Wu, Yong
  Zhao, C.L. Philip Chen","Hadamard Attention Recurrent Transformer: A Strong Baseline for Stereo
  Matching Transformer",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In light of the advancements in transformer technology, extant research
posits the construction of stereo transformers as a potential solution to the
binocular stereo matching challenge. However, constrained by the low-rank
bottleneck and quadratic complexity of attention mechanisms, stereo
transformers still fail to demonstrate sufficient nonlinear expressiveness
within a reasonable inference time. The lack of focus on key homonymous points
renders the representations of such methods vulnerable to challenging
conditions, including reflections and weak textures. Furthermore, a slow
computing speed is not conducive to the application. To overcome these
difficulties, we present the Hadamard Attention Recurrent Stereo Transformer
(HART) that incorporates the following components: 1) For faster inference, we
present a Hadamard product paradigm for the attention mechanism, achieving
linear computational complexity. 2) We designed a Dense Attention Kernel (DAK)
to amplify the differences between relevant and irrelevant feature responses.
This allows HART to focus on important details. DAK also converts zero elements
to non-zero elements to mitigate the reduced expressiveness caused by the
low-rank bottleneck. 3) To compensate for the spatial and channel interaction
missing in the Hadamard product, we propose MKOI to capture both global and
local information through the interleaving of large and small kernel
convolutions. Experimental results demonstrate the effectiveness of our HART.
In reflective area, HART ranked 1st on the KITTI 2012 benchmark among all
published methods at the time of submission. Code is available at
https://github.com/ZYangChen/HART.
","[{'version': 'v1', 'created': 'Thu, 2 Jan 2025 02:51:16 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 15:30:22 GMT'}]",2025-03-19,"[['Chen', 'Ziyang', ''], ['Zhang', 'Yongjun', ''], ['Li', 'Wenting', ''], ['Wang', 'Bingshu', ''], ['Wu', 'Yabo', ''], ['Zhao', 'Yong', ''], ['Chen', 'C. L. Philip', '']]","[{'text': 'stereo transformers', 'label': 'Transformers'}, {'text': 'attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'stereo\ntransformers', 'label': 'Transformers'}, {'text': 'attention mechanism', 'label': 'Attention mechanism'}]",Attention mechanism,attention mechanism,1.0
2501.06187,Tsai-Shien Chen,"Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot
  Sin Lee, Ivan Skorokhodov, Kfir Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, Sergey
  Tulyakov",Multi-subject Open-set Personalization in Video Generation,"CVPR 2025. Project page:
  https://snap-research.github.io/open-set-video-personalization/",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Video personalization methods allow us to synthesize videos with specific
concepts such as people, pets, and places. However, existing methods often
focus on limited domains, require time-consuming optimization per subject, or
support only a single subject. We present Video Alchemist $-$ a video model
with built-in multi-subject, open-set personalization capabilities for both
foreground objects and background, eliminating the need for time-consuming
test-time optimization. Our model is built on a new Diffusion Transformer
module that fuses each conditional reference image and its corresponding
subject-level text prompt with cross-attention layers. Developing such a large
model presents two main challenges: dataset and evaluation. First, as paired
datasets of reference images and videos are extremely hard to collect, we
sample selected video frames as reference images and synthesize a clip of the
target video. However, while models can easily denoise training videos given
reference frames, they fail to generalize to new contexts. To mitigate this
issue, we design a new automatic data construction pipeline with extensive
image augmentations. Second, evaluating open-set video personalization is a
challenge in itself. To address this, we introduce a personalization benchmark
that focuses on accurate subject fidelity and supports diverse personalization
scenarios. Finally, our extensive experiments show that our method
significantly outperforms existing personalization methods in both quantitative
and qualitative evaluations.
","[{'version': 'v1', 'created': 'Fri, 10 Jan 2025 18:59:54 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 17:59:56 GMT'}]",2025-03-21,"[['Chen', 'Tsai-Shien', ''], ['Siarohin', 'Aliaksandr', ''], ['Menapace', 'Willi', ''], ['Fang', 'Yuwei', ''], ['Lee', 'Kwot Sin', ''], ['Skorokhodov', 'Ivan', ''], ['Aberman', 'Kfir', ''], ['Zhu', 'Jun-Yan', ''], ['Yang', 'Ming-Hsuan', ''], ['Tulyakov', 'Sergey', '']]","[{'text': 'subject-level text prompt', 'label': 'Prompting'}, {'text': 'cross-attention layers', 'label': 'Attention mechanism'}]",Attention mechanism,cross-attention layers,0.5637996196746826
2502.15540,Milad Sefidgaran,Milad Sefidgaran and Abdellatif Zaidi and Piotr Krasnowski,"Generalization Guarantees for Representation Learning via Data-Dependent
  Gaussian Mixture Priors",Accepted as a Spotlight Paper at ICLR 2025,,,,stat.ML cs.IT cs.LG math.IT,http://creativecommons.org/licenses/by/4.0/,"  We establish in-expectation and tail bounds on the generalization error of
representation learning type algorithms. The bounds are in terms of the
relative entropy between the distribution of the representations extracted from
the training and ""test'' datasets and a data-dependent symmetric prior, i.e.,
the Minimum Description Length (MDL) of the latent variables for the training
and test datasets. Our bounds are shown to reflect the ""structure"" and
""simplicity'' of the encoder and significantly improve upon the few existing
ones for the studied model. We then use our in-expectation bound to devise a
suitable data-dependent regularizer; and we investigate thoroughly the
important question of the selection of the prior. We propose a systematic
approach to simultaneously learning a data-dependent Gaussian mixture prior and
using it as a regularizer. Interestingly, we show that a weighted attention
mechanism emerges naturally in this procedure. Our experiments show that our
approach outperforms the now popular Variational Information Bottleneck (VIB)
method as well as the recent Category-Dependent VIB (CDVIB).
","[{'version': 'v1', 'created': 'Fri, 21 Feb 2025 15:43:31 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 22:37:44 GMT'}]",2025-03-21,"[['Sefidgaran', 'Milad', ''], ['Zaidi', 'Abdellatif', ''], ['Krasnowski', 'Piotr', '']]","[{'text': 'weighted attention\nmechanism', 'label': 'Attention mechanism'}]",Attention mechanism,"weighted attention
mechanism",0.8618230819702148
2503.02009,Mohamed Sayed,"Jamie Wynn, Zawar Qureshi, Jakub Powierza, Jamie Watson, Mohamed Sayed",Morpheus: Text-Driven 3D Gaussian Splat Shape and Color Stylization,,,,,cs.CV,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Exploring real-world spaces using novel-view synthesis is fun, and
reimagining those worlds in a different style adds another layer of excitement.
Stylized worlds can also be used for downstream tasks where there is limited
training data and a need to expand a model's training distribution. Most
current novel-view synthesis stylization techniques lack the ability to
convincingly change geometry. This is because any geometry change requires
increased style strength which is often capped for stylization stability and
consistency. In this work, we propose a new autoregressive 3D Gaussian
Splatting stylization method. As part of this method, we contribute a new RGBD
diffusion model that allows for strength control over appearance and shape
stylization. To ensure consistency across stylized frames, we use a combination
of novel depth-guided cross attention, feature injection, and a Warp ControlNet
conditioned on composite frames for guiding the stylization of new frames. We
validate our method via extensive qualitative results, quantitative
experiments, and a user study. Code online.
","[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 19:33:22 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 14:11:26 GMT'}]",2025-03-19,"[['Wynn', 'Jamie', ''], ['Qureshi', 'Zawar', ''], ['Powierza', 'Jakub', ''], ['Watson', 'Jamie', ''], ['Sayed', 'Mohamed', '']]","[{'text': 'novel depth-guided cross attention', 'label': 'Attention mechanism'}, {'text': 'feature injection', 'label': 'Attention mechanism'}]",Attention mechanism,novel depth-guided cross attention,0.532916784286499
2503.05966,Md Talha Mohsin,Md Talha Mohsin and Nabid Bin Nasim,"Explaining the Unexplainable: A Systematic Review of Explainable AI in
  Finance","2 tables, 11 figures",,,,q-fin.GN cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Practitioners and researchers trying to strike a balance between accuracy and
transparency center Explainable Artificial Intelligence (XAI) at the junction
of finance. This paper offers a thorough overview of the changing scene of XAI
applications in finance together with domain-specific implementations,
methodological developments, and trend mapping of research. Using bibliometric
and content analysis, we find topic clusters, significant research, and most
often used explainability strategies used in financial industries. Our results
show a substantial dependence on post-hoc interpretability techniques;
attention mechanisms, feature importance analysis and SHAP are the most often
used techniques among them. This review stresses the need of multidisciplinary
approaches combining financial knowledge with improved explainability paradigms
and exposes important shortcomings in present XAI systems.
","[{'version': 'v1', 'created': 'Fri, 7 Mar 2025 22:36:44 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 15:37:42 GMT'}]",2025-03-18,"[['Mohsin', 'Md Talha', ''], ['Nasim', 'Nabid Bin', '']]","[{'text': 'attention mechanisms', 'label': 'Attention mechanism'}]",Attention mechanism,attention mechanisms,0.9558142423629761
2503.06052,Xuexin Chen,"Xuexin Chen, Ruichu Cai, Zhengting Huang, Zijian Li, Jie Zheng, Min Wu","Interpretable High-order Knowledge Graph Neural Network for Predicting
  Synthetic Lethality in Human Cancers",15 pages. Accepted by Briefings in Bioinformatics,Briefings in Bioinformatics 2025,,,cs.LG q-bio.QM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Synthetic lethality (SL) is a promising gene interaction for cancer therapy.
Recent SL prediction methods integrate knowledge graphs (KGs) into graph neural
networks (GNNs) and employ attention mechanisms to extract local subgraphs as
explanations for target gene pairs. However, attention mechanisms often lack
fidelity, typically generate a single explanation per gene pair, and fail to
ensure trustworthy high-order structures in their explanations. To overcome
these limitations, we propose Diverse Graph Information Bottleneck for
Synthetic Lethality (DGIB4SL), a KG-based GNN that generates multiple faithful
explanations for the same gene pair and effectively encodes high-order
structures. Specifically, we introduce a novel DGIB objective, integrating a
Determinant Point Process (DPP) constraint into the standard IB objective, and
employ 13 motif-based adjacency matrices to capture high-order structures in
gene representations. Experimental results show that DGIB4SL outperforms
state-of-the-art baselines and provides multiple explanations for SL
prediction, revealing diverse biological mechanisms underlying SL inference.
","[{'version': 'v1', 'created': 'Sat, 8 Mar 2025 04:37:28 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 04:23:11 GMT'}]",2025-03-20,"[['Chen', 'Xuexin', ''], ['Cai', 'Ruichu', ''], ['Huang', 'Zhengting', ''], ['Li', 'Zijian', ''], ['Zheng', 'Jie', ''], ['Wu', 'Min', '']]","[{'text': 'attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'attention mechanisms', 'label': 'Attention mechanism'}]",Attention mechanism,attention mechanisms,0.9558142423629761
2503.06473,Hanze Li,"Hanze Li, Xiande Huang","Enhancing Layer Attention Efficiency through Pruning Redundant
  Retrievals","11 pages, 7 figures",,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Growing evidence suggests that layer attention mechanisms, which enhance
interaction among layers in deep neural networks, have significantly advanced
network architectures. However, existing layer attention methods suffer from
redundancy, as attention weights learned by adjacent layers often become highly
similar. This redundancy causes multiple layers to extract nearly identical
features, reducing the model's representational capacity and increasing
training time. To address this issue, we propose a novel approach to quantify
redundancy by leveraging the Kullback-Leibler (KL) divergence between adjacent
layers. Additionally, we introduce an Enhanced Beta Quantile Mapping (EBQM)
method that accurately identifies and skips redundant layers, thereby
maintaining model stability. Our proposed Efficient Layer Attention (ELA)
architecture, improves both training efficiency and overall performance,
achieving a 30\% reduction in training time while enhancing performance in
tasks such as image classification and object detection.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 06:20:11 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 06:21:10 GMT'}]",2025-03-20,"[['Li', 'Hanze', ''], ['Huang', 'Xiande', '']]","[{'text': 'layer attention mechanisms', 'label': 'Attention mechanism'}]",Attention mechanism,layer attention mechanisms,0.8598178029060364
2503.08121,Thanh Nhat Huy Nguyen,"Huy Nguyen, Kien Nguyen, Akila Pemasiri, Feng Liu, Sridha Sridharan,
  Clinton Fookes","AG-VPReID: A Challenging Large-Scale Benchmark for Aerial-Ground
  Video-based Person Re-Identification","Accepted at Computer Vision and Pattern Recognition Conference (CVPR)
  2025",,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  We introduce AG-VPReID, a new large-scale dataset for aerial-ground
video-based person re-identification (ReID) that comprises 6,632 subjects,
32,321 tracklets and over 9.6 million frames captured by drones (altitudes
ranging from 15-120m), CCTV, and wearable cameras. This dataset offers a
real-world benchmark for evaluating the robustness to significant viewpoint
changes, scale variations, and resolution differences in cross-platform
aerial-ground settings. In addition, to address these challenges, we propose
AG-VPReID-Net, an end-to-end framework composed of three complementary streams:
(1) an Adapted Temporal-Spatial Stream addressing motion pattern
inconsistencies and facilitating temporal feature learning, (2) a Normalized
Appearance Stream leveraging physics-informed techniques to tackle resolution
and appearance changes, and (3) a Multi-Scale Attention Stream handling scale
variations across drone altitudes. We integrate visual-semantic cues from all
streams to form a robust, viewpoint-invariant whole-body representation.
Extensive experiments demonstrate that AG-VPReID-Net outperforms
state-of-the-art approaches on both our new dataset and existing video-based
ReID benchmarks, showcasing its effectiveness and generalizability.
Nevertheless, the performance gap observed on AG-VPReID across all methods
underscores the dataset's challenging nature. The dataset, code and trained
models are available at https://github.com/agvpreid25/AG-VPReID-Net.
","[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 07:38:01 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 01:07:25 GMT'}]",2025-03-18,"[['Nguyen', 'Huy', ''], ['Nguyen', 'Kien', ''], ['Pemasiri', 'Akila', ''], ['Liu', 'Feng', ''], ['Sridharan', 'Sridha', ''], ['Fookes', 'Clinton', '']]","[{'text': 'temporal feature learning', 'label': 'Few-shot Learning'}, {'text': 'Multi-Scale Attention Stream', 'label': 'Attention mechanism'}]",Attention mechanism,Multi-Scale Attention Stream,0.5518479347229004
2503.08992,Xuzhong Hu,"Xuzhong Hu, Zaipeng Duan, Pei An, Jun zhang, and Jie Ma","Dual-Domain Homogeneous Fusion with Cross-Modal Mamba and Progressive
  Decoder for 3D Object Detection","13 pages, 9 figures",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fusing LiDAR and image features in a homogeneous BEV domain has become
popular for 3D object detection in autonomous driving. However, this paradigm
is constrained by the excessive feature compression. While some works explore
dense voxel fusion to enable better feature interaction, they face high
computational costs and challenges in query generation. Additionally, feature
misalignment in both domains results in suboptimal detection accuracy. To
address these limitations, we propose a Dual-Domain Homogeneous Fusion network
(DDHFusion), which leverages the complementarily of both BEV and voxel domains
while mitigating their drawbacks. Specifically, we first transform image
features into BEV and sparse voxel representations using lift-splat-shot and
our proposed Semantic-Aware Feature Sampling (SAFS) module. The latter
significantly reduces computational overhead by discarding unimportant voxels.
Next, we introduce Homogeneous Voxel and BEV Fusion (HVF and HBF) networks for
multi-modal fusion within respective domains. They are equipped with novel
cross-modal Mamba blocks to resolve feature misalignment and enable
comprehensive scene perception. The output voxel features are injected into the
BEV space to compensate for the information loss brought by direct height
compression. During query selection, the Progressive Query Generation (PQG)
mechanism is implemented in the BEV domain to reduce false negatives caused by
feature compression. Furthermore, we propose a Progressive Decoder (QD) that
sequentially aggregates not only context-rich BEV features but also
geometry-aware voxel features with deformable attention and the Multi-Modal
Voxel Feature Mixing (MMVFM) block for precise classification and box
regression.
","[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 01:55:02 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 15:33:08 GMT'}]",2025-03-18,"[['Hu', 'Xuzhong', ''], ['Duan', 'Zaipeng', ''], ['An', 'Pei', ''], ['zhang', 'Jun', ''], ['Ma', 'Jie', '']]","[{'text': 'deformable attention', 'label': 'Attention mechanism'}]",Attention mechanism,deformable attention,0.7499341368675232
2503.11737,Jiseong Park,"Jiseong Park, Hanjin Kim, Seojin Kim, Jueun Choi",Multi-View Node Pruning for Accurate Graph Representation,Jiseong Park and Hanjin Kim are co-first author for this work,,,,cs.LG cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Graph pooling, which compresses a whole graph into a smaller coarsened graph,
is an essential component of graph representation learning. To efficiently
compress a given graph, graph pooling methods often drop their nodes with
attention-based scoring with the task loss. However, this often results in
simply removing nodes with lower degrees without consideration of their
feature-level relevance to the given task. To fix this problem, we propose a
Multi-View Pruning(MVP), a graph pruning method based on a multi-view framework
and reconstruction loss. Given a graph, MVP first constructs multiple graphs
for different views either by utilizing the predefined modalities or by
randomly partitioning the input features, to consider the importance of each
node in diverse perspectives. Then, it learns the score for each node by
considering both the reconstruction and the task loss. MVP can be incorporated
with any hierarchical pooling framework to score the nodes. We validate MVP on
multiple benchmark datasets by coupling it with two graph pooling methods, and
show that it significantly improves the performance of the base graph pooling
method, outperforming all baselines. Further analysis shows that both the
encoding of multiple views and the consideration of reconstruction loss are the
key to the success of MVP, and that it indeed identifies nodes that are less
important according to domain knowledge.
","[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 14:44:54 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 14:34:49 GMT'}]",2025-03-19,"[['Park', 'Jiseong', ''], ['Kim', 'Hanjin', ''], ['Kim', 'Seojin', ''], ['Choi', 'Jueun', '']]","[{'text': 'attention-based scoring', 'label': 'Attention mechanism'}]",Attention mechanism,attention-based scoring,0.5608227252960205
2503.11851,Jutika Borah M.Sc.,Jutika Borah and Hidam Kumarjit Singh,"DCAT: Dual Cross-Attention Fusion for Disease Classification in
  Radiological Images with Uncertainty Estimation","18 pages, 8 figures, 5 tables",,,,eess.IV cs.AI cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Accurate and reliable image classification is crucial in radiology, where
diagnostic decisions significantly impact patient outcomes. Conventional deep
learning models tend to produce overconfident predictions despite underlying
uncertainties, potentially leading to misdiagnoses. Attention mechanisms have
emerged as powerful tools in deep learning, enabling models to focus on
relevant parts of the input data. Combined with feature fusion, they can be
effective in addressing uncertainty challenges. Cross-attention has become
increasingly important in medical image analysis for capturing dependencies
across features and modalities. This paper proposes a novel dual
cross-attention fusion model for medical image analysis by addressing key
challenges in feature integration and interpretability. Our approach introduces
a bidirectional cross-attention mechanism with refined channel and spatial
attention that dynamically fuses feature maps from EfficientNetB4 and ResNet34
leveraging multi-network contextual dependencies. The refined features through
channel and spatial attention highlights discriminative patterns crucial for
accurate classification. The proposed model achieved AUC of 99.75%, 100%,
99.93% and 98.69% and AUPR of 99.81%, 100%, 99.97%, and 96.36% on Covid-19,
Tuberculosis, Pneumonia Chest X-ray images and Retinal OCT images respectively.
The entropy values and several high uncertain samples give an interpretable
visualization from the model enhancing transparency. By combining multi-scale
feature extraction, bidirectional attention and uncertainty estimation, our
proposed model strongly impacts medical image analysis.
","[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 20:28:20 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 12:18:48 GMT'}]",2025-03-20,"[['Borah', 'Jutika', ''], ['Singh', 'Hidam Kumarjit', '']]","[{'text': 'Attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'Cross-attention', 'label': 'Attention mechanism'}, {'text': 'bidirectional cross-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'spatial\nattention', 'label': 'Attention mechanism'}, {'text': 'multi-network contextual dependencies', 'label': 'contextual Embedding'}, {'text': 'channel and spatial attention', 'label': 'Attention mechanism'}]",Attention mechanism,Attention mechanisms,0.9558142423629761
2503.11935,Yang Zheng,Jun Yu and Yang Zheng and Lei Wang and Yongqi Wang and Shengfan Xu,"Design of an Expression Recognition Solution Employing the Global
  Channel-Spatial Attention Mechanism",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Facial expression recognition is a challenging classification task with broad
application prospects in the field of human - computer interaction. This paper
aims to introduce the methods of our upcoming 8th Affective Behavior Analysis
in the Wild (ABAW) competition to be held at CVPR2025. To address issues such
as low recognition accuracy caused by subtle expression changes and multi -
scales in facial expression recognition in videos, we propose global channel -
spatial attention and median - enhanced spatial - channel attention to
strengthen feature processing for speech and images respectively. Secondly, to
fully utilize the complementarity between the speech and facial expression
modalities, a speech - and - facial - expression key - frame alignment
technique is adopted to calculate the weights of speech and facial expressions.
These weights are input into the feature fusion layer for multi - scale dilated
fusion, which effectively improves the recognition rate of facial expression
recognition. In the facial expression recognition task of the 6th ABAW
competition, our method achieved excellent results on the official validation
set, which fully demonstrates the effectiveness and competitiveness of the
proposed method.
","[{'version': 'v1', 'created': 'Sat, 15 Mar 2025 00:59:34 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 05:50:24 GMT'}]",2025-03-19,"[['Yu', 'Jun', ''], ['Zheng', 'Yang', ''], ['Wang', 'Lei', ''], ['Wang', 'Yongqi', ''], ['Xu', 'Shengfan', '']]","[{'text': 'global channel -\nspatial attention', 'label': 'Attention mechanism'}, {'text': 'median - enhanced spatial - channel attention', 'label': 'Attention mechanism'}, {'text': 'feature fusion layer', 'label': 'Embedding'}, {'text': 'multi - scale dilated\nfusion', 'label': 'Embedding'}]",Attention mechanism,"global channel -
spatial attention",0.6017642021179199
2503.12461,Fanhu Zeng,"Fanhu Zeng, Hao Tang, Yihua Shao, Siyu Chen, Ling Shao, Yan Wang","MambaIC: State Space Models for High-Performance Learned Image
  Compression",Accepted to CVPR 2025,,,,cs.CV eess.IV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A high-performance image compression algorithm is crucial for real-time
information transmission across numerous fields. Despite rapid progress in
image compression, computational inefficiency and poor redundancy modeling
still pose significant bottlenecks, limiting practical applications. Inspired
by the effectiveness of state space models (SSMs) in capturing long-range
dependencies, we leverage SSMs to address computational inefficiency in
existing methods and improve image compression from multiple perspectives. In
this paper, we integrate the advantages of SSMs for better
efficiency-performance trade-off and propose an enhanced image compression
approach through refined context modeling, which we term MambaIC. Specifically,
we explore context modeling to adaptively refine the representation of hidden
states. Additionally, we introduce window-based local attention into
channel-spatial entropy modeling to reduce potential spatial redundancy during
compression, thereby increasing efficiency. Comprehensive qualitative and
quantitative results validate the effectiveness and efficiency of our approach,
particularly for high-resolution image compression. Code is released at
https://github.com/AuroraZengfh/MambaIC.
","[{'version': 'v1', 'created': 'Sun, 16 Mar 2025 11:32:34 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 02:27:03 GMT'}]",2025-03-21,"[['Zeng', 'Fanhu', ''], ['Tang', 'Hao', ''], ['Shao', 'Yihua', ''], ['Chen', 'Siyu', ''], ['Shao', 'Ling', ''], ['Wang', 'Yan', '']]","[{'text': 'context modeling', 'label': 'contextual Embedding'}, {'text': 'context modeling', 'label': 'contextual Embedding'}, {'text': 'window-based local attention', 'label': 'Attention mechanism'}, {'text': 'channel-spatial entropy modeling', 'label': 'contextual Embedding'}]",Attention mechanism,window-based local attention,0.6087658405303955
2503.12734,Jianliang He,"Jianliang He, Xintian Pan, Siyu Chen, Zhuoran Yang","In-Context Linear Regression Demystified: Training Dynamics and
  Mechanistic Interpretability of Multi-Head Softmax Attention",,,,,cs.LG stat.ML,http://creativecommons.org/licenses/by/4.0/,"  We study how multi-head softmax attention models are trained to perform
in-context learning on linear data. Through extensive empirical experiments and
rigorous theoretical analysis, we demystify the emergence of elegant attention
patterns: a diagonal and homogeneous pattern in the key-query (KQ) weights, and
a last-entry-only and zero-sum pattern in the output-value (OV) weights.
Remarkably, these patterns consistently appear from gradient-based training
starting from random initialization. Our analysis reveals that such emergent
structures enable multi-head attention to approximately implement a debiased
gradient descent predictor -- one that outperforms single-head attention and
nearly achieves Bayesian optimality up to proportional factor. Furthermore,
compared to linear transformers, the softmax attention readily generalizes to
sequences longer than those seen during training. We also extend our study to
scenarios with non-isotropic covariates and multi-task linear regression. In
the former, multi-head attention learns to implement a form of pre-conditioned
gradient descent. In the latter, we uncover an intriguing regime where the
interplay between head number and task number triggers a superposition
phenomenon that efficiently resolves multi-task in-context learning. Our
results reveal that in-context learning ability emerges from the trained
transformer as an aggregated effect of its architecture and the underlying data
distribution, paving the way for deeper understanding and broader applications
of in-context learning.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 02:00:49 GMT'}]",2025-03-18,"[['He', 'Jianliang', ''], ['Pan', 'Xintian', ''], ['Chen', 'Siyu', ''], ['Yang', 'Zhuoran', '']]","[{'text': 'softmax attention', 'label': 'Attention mechanism'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}, {'text': 'multi-head attention', 'label': 'Attention mechanism'}, {'text': 'single-head attention', 'label': 'Attention mechanism'}, {'text': 'linear transformers', 'label': 'Transformers'}, {'text': 'softmax attention', 'label': 'Attention mechanism'}, {'text': 'multi-head attention', 'label': 'Attention mechanism'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}]",Attention mechanism,single-head attention,0.7179856300354004
2503.12838,Guanbin Li,"Junjia Huang, Pengxiang Yan, Jinhang Cai, Jiyang Liu, Zhao Wang,
  Yitong Wang, Xinglong Wu, Guanbin Li",DreamLayer: Simultaneous Multi-Layer Generation via Diffusion Mode,Under submission,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text-driven image generation using diffusion models has recently gained
significant attention. To enable more flexible image manipulation and editing,
recent research has expanded from single image generation to transparent layer
generation and multi-layer compositions. However, existing approaches often
fail to provide a thorough exploration of multi-layer structures, leading to
inconsistent inter-layer interactions, such as occlusion relationships, spatial
layout, and shadowing. In this paper, we introduce DreamLayer, a novel
framework that enables coherent text-driven generation of multiple image
layers, by explicitly modeling the relationship between transparent foreground
and background layers. DreamLayer incorporates three key components, i.e.,
Context-Aware Cross-Attention (CACA) for global-local information exchange,
Layer-Shared Self-Attention (LSSA) for establishing robust inter-layer
connections, and Information Retained Harmonization (IRH) for refining fusion
details at the latent level. By leveraging a coherent full-image context,
DreamLayer builds inter-layer connections through attention mechanisms and
applies a harmonization step to achieve seamless layer fusion. To facilitate
research in multi-layer generation, we construct a high-quality, diverse
multi-layer dataset including 400k samples. Extensive experiments and user
studies demonstrate that DreamLayer generates more coherent and well-aligned
layers, with broad applicability, including latent-space image editing and
image-to-layer decomposition.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 05:34:11 GMT'}]",2025-03-18,"[['Huang', 'Junjia', ''], ['Yan', 'Pengxiang', ''], ['Cai', 'Jinhang', ''], ['Liu', 'Jiyang', ''], ['Wang', 'Zhao', ''], ['Wang', 'Yitong', ''], ['Wu', 'Xinglong', ''], ['Li', 'Guanbin', '']]","[{'text': 'Context-Aware Cross-Attention (CACA)', 'label': 'Attention mechanism'}, {'text': 'Layer-Shared Self-Attention (LSSA)', 'label': 'Attention mechanism'}, {'text': 'attention mechanisms', 'label': 'Attention mechanism'}]",Attention mechanism,attention mechanisms,0.9558142423629761
2503.12847,Chen Liu,"Chen Liu, Peike Li, Liying Yang, Dadong Wang, Lincheng Li, Xin Yu","Robust Audio-Visual Segmentation via Audio-Guided Visual Convergent
  Alignment",Accepted by CVPR2025,,,,cs.SD cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Accurately localizing audible objects based on audio-visual cues is the core
objective of audio-visual segmentation. Most previous methods emphasize spatial
or temporal multi-modal modeling, yet overlook challenges from ambiguous
audio-visual correspondences such as nearby visually similar but acoustically
different objects and frequent shifts in objects' sounding status.
Consequently, they may struggle to reliably correlate audio and visual cues,
leading to over- or under-segmentation. To address these limitations, we
propose a novel framework with two primary components: an audio-guided modality
alignment (AMA) module and an uncertainty estimation (UE) module. Instead of
indiscriminately correlating audio-visual cues through a global attention
mechanism, AMA performs audio-visual interactions within multiple groups and
consolidates group features into compact representations based on their
responsiveness to audio cues, effectively directing the model's attention to
audio-relevant areas. Leveraging contrastive learning, AMA further
distinguishes sounding regions from silent areas by treating features with
strong audio responses as positive samples and weaker responses as negatives.
Additionally, UE integrates spatial and temporal information to identify
high-uncertainty regions caused by frequent changes in sound state, reducing
prediction errors by lowering confidence in these areas. Experimental results
demonstrate that our approach achieves superior accuracy compared to existing
state-of-the-art methods, particularly in challenging scenarios where
traditional approaches struggle to maintain reliable segmentation.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 05:48:22 GMT'}]",2025-03-18,"[['Liu', 'Chen', ''], ['Li', 'Peike', ''], ['Yang', 'Liying', ''], ['Wang', 'Dadong', ''], ['Li', 'Lincheng', ''], ['Yu', 'Xin', '']]","[{'text': 'global attention\nmechanism', 'label': 'Attention mechanism'}, {'text': 'contrastive learning', 'label': 'Few-shot Learning'}]",Attention mechanism,"global attention
mechanism",0.8699287176132202
2503.12853,Jiacheng Hu,"Yanlin Xiang, Qingyuan He, Ting Xu, Ran Hao, Jiacheng Hu, Hanchao
  Zhang","Adaptive Transformer Attention and Multi-Scale Fusion for Spine 3D
  Segmentation",,,,,cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This study proposes a 3D semantic segmentation method for the spine based on
the improved SwinUNETR to improve segmentation accuracy and robustness. Aiming
at the complex anatomical structure of spinal images, this paper introduces a
multi-scale fusion mechanism to enhance the feature extraction capability by
using information of different scales, thereby improving the recognition
accuracy of the model for the target area. In addition, the introduction of the
adaptive attention mechanism enables the model to dynamically adjust the
attention to the key area, thereby optimizing the boundary segmentation effect.
The experimental results show that compared with 3D CNN, 3D U-Net, and 3D U-Net
+ Transformer, the model of this study has achieved significant improvements in
mIoU, mDice, and mAcc indicators, and has better segmentation performance. The
ablation experiment further verifies the effectiveness of the proposed improved
method, proving that multi-scale fusion and adaptive attention mechanism have a
positive effect on the segmentation task. Through the visualization analysis of
the inference results, the model can better restore the real anatomical
structure of the spinal image. Future research can further optimize the
Transformer structure and expand the data scale to improve the generalization
ability of the model. This study provides an efficient solution for the task of
medical image segmentation, which is of great significance to intelligent
medical image analysis.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 06:27:43 GMT'}]",2025-03-18,"[['Xiang', 'Yanlin', ''], ['He', 'Qingyuan', ''], ['Xu', 'Ting', ''], ['Hao', 'Ran', ''], ['Hu', 'Jiacheng', ''], ['Zhang', 'Hanchao', '']]","[{'text': 'multi-scale fusion mechanism', 'label': 'Attention mechanism'}, {'text': 'adaptive attention mechanism', 'label': 'Attention mechanism'}, {'text': 'adaptive attention mechanism', 'label': 'Attention mechanism'}]",Attention mechanism,adaptive attention mechanism,0.8649787902832031
2503.12885,Dewei Zhou,"Dewei Zhou, Mingwei Li, Zongxin Yang, Yi Yang","DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale
  Text-to-Image Models",11 pages,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Image-conditioned generation methods, such as depth- and canny-conditioned
approaches, have demonstrated remarkable abilities for precise image synthesis.
However, existing models still struggle to accurately control the content of
multiple instances (or regions). Even state-of-the-art models like FLUX and
3DIS face challenges, such as attribute leakage between instances, which limits
user control. To address these issues, we introduce DreamRenderer, a
training-free approach built upon the FLUX model. DreamRenderer enables users
to control the content of each instance via bounding boxes or masks, while
ensuring overall visual harmony. We propose two key innovations: 1) Bridge
Image Tokens for Hard Text Attribute Binding, which uses replicated image
tokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely
on text data, bind the correct visual attributes for each instance during Joint
Attention; 2) Hard Image Attribute Binding applied only to vital layers.
Through our analysis of FLUX, we identify the critical layers responsible for
instance attribute rendering and apply Hard Image Attribute Binding only in
these layers, using soft binding in the others. This approach ensures precise
control while preserving image quality. Evaluations on the COCO-POS and
COCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success
Ratio by 17.7% over FLUX and enhances the performance of layout-to-image models
like GLIGEN and 3DIS by up to 26.8%. Project Page:
https://limuloo.github.io/DreamRenderer/.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 07:30:16 GMT'}]",2025-03-18,"[['Zhou', 'Dewei', ''], ['Li', 'Mingwei', ''], ['Yang', 'Zongxin', ''], ['Yang', 'Yi', '']]","[{'text': 'T5 text embeddings', 'label': 'Embedding'}, {'text': 'Joint\nAttention', 'label': 'Attention mechanism'}]",Attention mechanism,"Joint
Attention",0.6880277395248413
2503.12919,Aref Einizade,"Aref Einizade, Dorina Thanou, Fragkiskos D. Malliaros, Jhony H.
  Giraldo",COSMOS: Continuous Simplicial Neural Networks,"17 pages, 6 figures",,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Simplicial complexes provide a powerful framework for modeling high-order
interactions in structured data, making them particularly suitable for
applications such as trajectory prediction and mesh processing. However,
existing simplicial neural networks (SNNs), whether convolutional or
attention-based, rely primarily on discrete filtering techniques, which can be
restrictive. In contrast, partial differential equations (PDEs) on simplicial
complexes offer a principled approach to capture continuous dynamics in such
structures. In this work, we introduce COntinuous SiMplicial neural netwOrkS
(COSMOS), a novel SNN architecture derived from PDEs on simplicial complexes.
We provide theoretical and experimental justifications of COSMOS's stability
under simplicial perturbations. Furthermore, we investigate the over-smoothing
phenomenon, a common issue in geometric deep learning, demonstrating that
COSMOS offers better control over this effect than discrete SNNs. Our
experiments on real-world datasets of ocean trajectory prediction and
regression on partial deformable shapes demonstrate that COSMOS achieves
competitive performance compared to state-of-the-art SNNs in complex and noisy
environments.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 08:31:25 GMT'}]",2025-03-18,"[['Einizade', 'Aref', ''], ['Thanou', 'Dorina', ''], ['Malliaros', 'Fragkiskos D.', ''], ['Giraldo', 'Jhony H.', '']]","[{'text': 'attention-based', 'label': 'Attention mechanism'}, {'text': 'SNNs', 'label': 'Neural Language Model'}]",Attention mechanism,attention-based,0.7038730382919312
2503.12963,Chaolong Yang,"Chaolong Yang, Kai Yao, Yuyao Yan, Chenru Jiang, Weiguang Zhao, Jie
  Sun, Guangliang Cheng, Yifei Zhang, Bin Dong, Kaizhu Huang","Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based
  Spatiotemporal Diffusion for Audio-driven Talking Portrait",,,,,cs.CV,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Audio-driven single-image talking portrait generation plays a crucial role in
virtual reality, digital human creation, and filmmaking. Existing approaches
are generally categorized into keypoint-based and image-based methods.
Keypoint-based methods effectively preserve character identity but struggle to
capture fine facial details due to the fixed points limitation of the 3D
Morphable Model. Moreover, traditional generative networks face challenges in
establishing causality between audio and keypoints on limited datasets,
resulting in low pose diversity. In contrast, image-based approaches produce
high-quality portraits with diverse details using the diffusion network but
incur identity distortion and expensive computational costs. In this work, we
propose KDTalker, the first framework to combine unsupervised implicit 3D
keypoint with a spatiotemporal diffusion model. Leveraging unsupervised
implicit 3D keypoints, KDTalker adapts facial information densities, allowing
the diffusion process to model diverse head poses and capture fine facial
details flexibly. The custom-designed spatiotemporal attention mechanism
ensures accurate lip synchronization, producing temporally consistent,
high-quality animations while enhancing computational efficiency. Experimental
results demonstrate that KDTalker achieves state-of-the-art performance
regarding lip synchronization accuracy, head pose diversity, and execution
efficiency.Our codes are available at https://github.com/chaolongy/KDTalker.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 09:18:31 GMT'}]",2025-03-18,"[['Yang', 'Chaolong', ''], ['Yao', 'Kai', ''], ['Yan', 'Yuyao', ''], ['Jiang', 'Chenru', ''], ['Zhao', 'Weiguang', ''], ['Sun', 'Jie', ''], ['Cheng', 'Guangliang', ''], ['Zhang', 'Yifei', ''], ['Dong', 'Bin', ''], ['Huang', 'Kaizhu', '']]","[{'text': 'spatiotemporal attention mechanism', 'label': 'Attention mechanism'}]",Attention mechanism,spatiotemporal attention mechanism,0.8278745412826538
2503.12992,Michael Pichat,"Michael Pichat, William Pogrund, Paloma Pichat, Armanouche Gasparian,
  Samuel Demarchi, Corbet Alois Georgeon, Michael Veillet-Guillem","Intra-neuronal attention within language models Relationships between
  activation and semantics",,,,,cs.AI cs.CL q-bio.NC,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  This study investigates the ability of perceptron-type neurons in language
models to perform intra-neuronal attention; that is, to identify different
homogeneous categorical segments within the synthetic thought category they
encode, based on a segmentation of specific activation zones for the tokens to
which they are particularly responsive. The objective of this work is therefore
to determine to what extent formal neurons can establish a homomorphic
relationship between activation-based and categorical segmentations. The
results suggest the existence of such a relationship, albeit tenuous, only at
the level of tokens with very high activation levels. This intra-neuronal
attention subsequently enables categorical restructuring processes at the level
of neurons in the following layer, thereby contributing to the progressive
formation of high-level categorical abstractions.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 09:47:11 GMT'}]",2025-03-18,"[['Pichat', 'Michael', ''], ['Pogrund', 'William', ''], ['Pichat', 'Paloma', ''], ['Gasparian', 'Armanouche', ''], ['Demarchi', 'Samuel', ''], ['Georgeon', 'Corbet Alois', ''], ['Veillet-Guillem', 'Michael', '']]","[{'text': 'intra-neuronal attention', 'label': 'Attention mechanism'}, {'text': 'intra-neuronal\nattention', 'label': 'Attention mechanism'}]",Attention mechanism,intra-neuronal attention,0.7111603617668152
2503.13119,Paul Wawerek-L\'opez,"Paul Wawerek-L\'opez, Navid Mahmoudian Bidgoli, Pascal Frossard,
  Andr\'e Kaup, Thomas Maugey","OSLO-IC: On-the-Sphere Learned Omnidirectional Image Compression with
  Attention Modules and Spatial Context","5 pages, 5 figures, accepted for IEEE International Conference on
  Acoustics, Speech and Signal Processing 2025 (IEEE ICASSP 2025)",,,,eess.IV,http://creativecommons.org/licenses/by/4.0/,"  Developing effective 360-degree (spherical) image compression techniques is
crucial for technologies like virtual reality and automated driving. This paper
advances the state-of-the-art in on-the-sphere learning (OSLO) for
omnidirectional image compression framework by proposing spherical attention
modules, residual blocks, and a spatial autoregressive context model. These
improvements achieve a 23.1% bit rate reduction in terms of WS-PSNR BD rate.
Additionally, we introduce a spherical transposed convolution operator for
upsampling, which reduces trainable parameters by a factor of four compared to
the pixel shuffling used in the OSLO framework, while maintaining similar
compression performance. Therefore, in total, our proposed method offers
significant rate savings with a smaller architecture and can be applied to any
spherical convolutional application.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 12:46:14 GMT'}]",2025-03-18,"[['Wawerek-L√≥pez', 'Paul', ''], ['Bidgoli', 'Navid Mahmoudian', ''], ['Frossard', 'Pascal', ''], ['Kaup', 'Andr√©', ''], ['Maugey', 'Thomas', '']]","[{'text': 'on-the-sphere learning', 'label': 'Few-shot Learning'}, {'text': 'spherical attention\nmodules', 'label': 'Attention mechanism'}, {'text': 'residual blocks', 'label': 'contextual Embedding'}]",Attention mechanism,"spherical attention
modules",0.531786322593689
2503.13179,Yi Zhang,"Yi Zhang, Wenye Zhou, Ruonan Lin","A super-resolution reconstruction method for lightweight building images
  based on an expanding feature modulation network",,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  This study proposes a lightweight method for building image super-resolution
using a Dilated Contextual Feature Modulation Network (DCFMN). The process
includes obtaining high-resolution images, down-sampling them to
low-resolution, enhancing the low-resolution images, constructing and training
a lightweight network model, and generating super-resolution outputs. To
address challenges such as regular textures and long-range dependencies in
building images, the DCFMN integrates an expansion separable modulation unit
and a local feature enhancement module. The former employs multiple expansion
convolutions equivalent to a large kernel to efficiently aggregate multi-scale
features while leveraging a simple attention mechanism for adaptivity. The
latter encodes local features, mixes channel information, and ensures no
additional computational burden during inference through reparameterization.
This approach effectively resolves the limitations of existing lightweight
super-resolution networks in modeling long-range dependencies, achieving
accurate and efficient global feature modeling without increasing computational
costs, and significantly improving both reconstruction quality and lightweight
efficiency for building image super-resolution models.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 13:54:26 GMT'}]",2025-03-18,"[['Zhang', 'Yi', ''], ['Zhou', 'Wenye', ''], ['Lin', 'Ruonan', '']]","[{'text': 'simple attention mechanism', 'label': 'Attention mechanism'}]",Attention mechanism,simple attention mechanism,0.9481173157691956
2503.13222,Chi Han,Chi Han,Can Language Models Follow Multiple Turns of Entangled Instructions?,8 pages,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Despite significant achievements in improving the instruction-following
capabilities of large language models (LLMs), the ability to process multiple
potentially entangled or conflicting instructions remains a considerable
challenge. Real-world scenarios often require consistency across multiple
instructions over time, such as secret privacy, personal preferences, and
prioritization, which demand sophisticated abilities to integrate multiple
turns and carefully balance competing objectives when instructions intersect or
conflict. This work presents a systematic investigation of LLMs' capabilities
in handling multiple turns of instructions, covering three levels of
difficulty: (1) retrieving information from instructions, (2) tracking and
reasoning across turns, and (3) resolving conflicts among instructions. We
construct MultiTurnInstruct with around 1.1K high-quality multi-turn
conversations through the human-in-the-loop approach and result in nine
capability categories, including statics and dynamics, reasoning, and
multitasking. Our finding reveals an intriguing trade-off between different
capabilities. While GPT models demonstrate superior memorization, they show
reduced effectiveness in privacy-protection tasks requiring selective
information withholding. Larger models exhibit stronger reasoning capabilities
but still struggle with resolving conflicting instructions. Importantly, these
performance gaps cannot be attributed solely to information loss, as models
demonstrate strong BLEU scores on memorization tasks but their attention
mechanisms fail to integrate multiple related instructions effectively. These
findings highlight critical areas for improvement in complex real-world tasks
involving multi-turn instructions.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 14:31:37 GMT'}]",2025-03-18,"[['Han', 'Chi', '']]","[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'attention\nmechanisms', 'label': 'Attention mechanism'}]",Attention mechanism,"attention
mechanisms",0.9558142423629761
2503.13268,Jian Xiao,"Jian Xiao, Ji Wang, and Yuanwei Liu",Channel Estimation for Pinching-Antenna Systems (PASS),,,,,cs.IT eess.SP math.IT,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pinching Antennas (PAs) represent a revolutionary flexible antenna technology
that leverages dielectric waveguides and electromagnetic coupling to mitigate
large-scale path loss. This letter is the first to explore channel estimation
for Pinching-Antenna SyStems (PASS), addressing their uniquely ill-conditioned
and underdetermined channel characteristics. In particular, two efficient deep
learning-based channel estimators are proposed. 1) PAMoE: This estimator
incorporates dynamic padding, feature embedding, fusion, and mixture of experts
(MoE) modules, which effectively leverage the positional information of PAs and
exploit expert diversity. 2) PAformer: This Transformer-style estimator employs
the self-attention mechanism to predict channel coefficients in a per-antenna
manner, which offers more flexibility to adaptively deal with dynamic numbers
of PAs in practical deployment. Numerical results demonstrate that 1) the
proposed deep learning-based channel estimators outperform conventional methods
and exhibit excellent zero-shot learning capabilities, and 2) PAMoE delivers
higher channel estimation accuracy via MoE specialization, while PAformer
natively handles an arbitrary number of PAs, trading self-attention complexity
for superior scalability.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:20:49 GMT'}]",2025-03-18,"[['Xiao', 'Jian', ''], ['Wang', 'Ji', ''], ['Liu', 'Yuanwei', '']]","[{'text': 'PAMoE', 'label': 'Transformer-based model'}, {'text': 'dynamic padding', 'label': 'Embedding'}, {'text': 'feature embedding', 'label': 'Embedding'}, {'text': 'fusion', 'label': 'Embedding'}, {'text': 'PAformer', 'label': 'Transformer-based model'}, {'text': 'self-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'zero-shot learning', 'label': 'Few-shot Learning'}, {'text': 'PAMoE', 'label': 'Transformer-based model'}, {'text': 'PAformer', 'label': 'Transformer-based model'}]",Attention mechanism,self-attention mechanism,0.8757837414741516
2503.13304,Witold Wydma\'nski,"Witold Wydma\'nski, Marek \'Smieja","GFSNetwork: Differentiable Feature Selection via Gumbel-Sigmoid
  Relaxation",,,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Feature selection in deep learning remains a critical challenge, particularly
for high-dimensional tabular data where interpretability and computational
efficiency are paramount. We present GFSNetwork, a novel neural architecture
that performs differentiable feature selection through temperature-controlled
Gumbel-Sigmoid sampling. Unlike traditional methods, where the user has to
define the requested number of features, GFSNetwork selects it automatically
during an end-to-end process. Moreover, GFSNetwork maintains constant
computational overhead regardless of the number of input features. We evaluate
GFSNetwork on a series of classification and regression benchmarks, where it
consistently outperforms recent methods including DeepLasso, attention maps, as
well as traditional feature selectors, while using significantly fewer
features. Furthermore, we validate our approach on real-world metagenomic
datasets, demonstrating its effectiveness in high-dimensional biological data.
Concluding, our method provides a scalable solution that bridges the gap
between neural network flexibility and traditional feature selection
interpretability. We share our python implementation of GFSNetwork at
https://github.com/wwydmanski/GFSNetwork, as well as a PyPi package
(gfs_network).
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:47:26 GMT'}]",2025-03-18,"[['Wydma≈Ñski', 'Witold', ''], ['≈ömieja', 'Marek', '']]","[{'text': 'GFSNetwork', 'label': 'Neural Language Model'}, {'text': 'attention maps', 'label': 'Attention mechanism'}]",Attention mechanism,attention maps,0.672863245010376
2503.13439,Tianhao Wu,"Tianhao Wu, Chuanxia Zheng, Frank Guan, Andrea Vedaldi, Tat-Jen Cham",Amodal3R: Amodal 3D Reconstruction from Occluded 2D Images,Project Page: https://sm0kywu.github.io/Amodal3R/,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Most image-based 3D object reconstructors assume that objects are fully
visible, ignoring occlusions that commonly occur in real-world scenarios. In
this paper, we introduce Amodal3R, a conditional 3D generative model designed
to reconstruct 3D objects from partial observations. We start from a
""foundation"" 3D generative model and extend it to recover plausible 3D geometry
and appearance from occluded objects. We introduce a mask-weighted multi-head
cross-attention mechanism followed by an occlusion-aware attention layer that
explicitly leverages occlusion priors to guide the reconstruction process. We
demonstrate that, by training solely on synthetic data, Amodal3R learns to
recover full 3D objects even in the presence of occlusions in real scenes. It
substantially outperforms existing methods that independently perform 2D amodal
completion followed by 3D reconstruction, thereby establishing a new benchmark
for occlusion-aware 3D reconstruction.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:59:01 GMT'}]",2025-03-18,"[['Wu', 'Tianhao', ''], ['Zheng', 'Chuanxia', ''], ['Guan', 'Frank', ''], ['Vedaldi', 'Andrea', ''], ['Cham', 'Tat-Jen', '']]","[{'text': 'mask-weighted multi-head\ncross-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'occlusion-aware attention layer', 'label': 'Attention mechanism'}]",Attention mechanism,"mask-weighted multi-head
cross-attention mechanism",0.654481053352356
2503.13617,Hao Li,"Hao Li, Yubin Xiao, Ke Liang, Mengzhu Wang, Long Lan, Kenli Li and
  Xinwang Liu","Let Synthetic Data Shine: Domain Reassembly and Soft-Fusion for Single
  Domain Generalization","26 pages, 10 figures",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Single Domain Generalization (SDG) aims to train models with consistent
performance across diverse scenarios using data from a single source. While
using latent diffusion models (LDMs) show promise in augmenting limited source
data, we demonstrate that directly using synthetic data can be detrimental due
to significant feature distribution discrepancies between synthetic and real
target domains, leading to performance degradation. To address this issue, we
propose Discriminative Domain Reassembly and Soft-Fusion (DRSF), a training
framework leveraging synthetic data to improve model generalization. We employ
LDMs to produce diverse pseudo-target domain samples and introduce two key
modules to handle distribution bias. First, Discriminative Feature Decoupling
and Reassembly (DFDR) module uses entropy-guided attention to recalibrate
channel-level features, suppressing synthetic noise while preserving semantic
consistency. Second, Multi-pseudo-domain Soft Fusion (MDSF) module uses
adversarial training with latent-space feature interpolation, creating
continuous feature transitions between domains. Extensive SDG experiments on
object detection and semantic segmentation tasks demonstrate that DRSF achieves
substantial performance gains with only marginal computational overhead.
Notably, DRSF's plug-and-play architecture enables seamless integration with
unsupervised domain adaptation paradigms, underscoring its broad applicability
in addressing diverse and real-world domain challenges.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 18:08:03 GMT'}]",2025-03-19,"[['Li', 'Hao', ''], ['Xiao', 'Yubin', ''], ['Liang', 'Ke', ''], ['Wang', 'Mengzhu', ''], ['Lan', 'Long', ''], ['Li', 'Kenli', ''], ['Liu', 'Xinwang', '']]","[{'text': 'entropy-guided attention', 'label': 'Attention mechanism'}]",Attention mechanism,entropy-guided attention,0.5984995365142822
2503.13724,Shristi Das Biswas,"Shristi Das Biswas, Efstathia Soufleri, Arani Roy, Kaushik Roy","Towards Scalable Modeling of Compressed Videos for Efficient Action
  Recognition",,,,,cs.CV,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Training robust deep video representations has proven to be computationally
challenging due to substantial decoding overheads, the enormous size of raw
video streams, and their inherent high temporal redundancy. Different from
existing schemes, operating exclusively in the compressed video domain and
exploiting all freely available modalities, i.e., I-frames, and P-frames
(motion vectors and residuals) offers a compute-efficient alternative. Existing
methods approach this task as a naive multi-modality problem, ignoring the
temporal correlation and implicit sparsity across P-frames for modeling
stronger shared representations for videos of the same action, making training
and generalization easier. By revisiting the high-level design of dominant
video understanding backbones, we increase inference speed by a factor of $56$
while retaining similar performance. For this, we propose a hybrid end-to-end
framework that factorizes learning across three key concepts to reduce
inference cost by $330\times$ versus prior art: First, a specially designed
dual-encoder scheme with efficient Spiking Temporal Modulators to minimize
latency while retaining cross-domain feature aggregation. Second, a unified
transformer model to capture inter-modal dependencies using global
self-attention to enhance I-frame -- P-frame contextual interactions. Third, a
Multi-Modal Mixer Block to model rich representations from the joint
spatiotemporal token embeddings. Experiments show that our method results in a
lightweight architecture achieving state-of-the-art video recognition
performance on UCF-101, HMDB-51, K-400, K-600 and SS-v2 datasets with favorable
costs ($0.73$J/V) and fast inference ($16$V/s). Our observations bring new
insights into practical design choices for efficient next-generation
spatiotemporal learners. Code is available.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 21:13:48 GMT'}]",2025-03-19,"[['Biswas', 'Shristi Das', ''], ['Soufleri', 'Efstathia', ''], ['Roy', 'Arani', ''], ['Roy', 'Kaushik', '']]","[{'text': 'global\nself-attention', 'label': 'Attention mechanism'}, {'text': 'joint\nspatiotemporal token embeddings', 'label': 'contextual Embedding'}]",Attention mechanism,"global
self-attention",0.6738587617874146
2503.13740,Yuxuan Jiang,"Yuxuan Jiang, Chengxi Zeng, Siyue Teng, Fan Zhang, Xiaoqing Zhu, Joel
  Sole and David Bull","C2D-ISR: Optimizing Attention-based Image Super-resolution from
  Continuous to Discrete Scales",,,,,cs.CV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  In recent years, attention mechanisms have been exploited in single image
super-resolution (SISR), achieving impressive reconstruction results. However,
these advancements are still limited by the reliance on simple training
strategies and network architectures designed for discrete up-sampling scales,
which hinder the model's ability to effectively capture information across
multiple scales. To address these limitations, we propose a novel framework,
\textbf{C2D-ISR}, for optimizing attention-based image super-resolution models
from both performance and complexity perspectives. Our approach is based on a
two-stage training methodology and a hierarchical encoding mechanism. The new
training methodology involves continuous-scale training for discrete scale
models, enabling the learning of inter-scale correlations and multi-scale
feature representation. In addition, we generalize the hierarchical encoding
mechanism with existing attention-based network structures, which can achieve
improved spatial feature fusion, cross-scale information aggregation, and more
importantly, much faster inference. We have evaluated the C2D-ISR framework
based on three efficient attention-based backbones, SwinIR-L, SRFormer-L and
MambaIRv2-L, and demonstrated significant improvements over the other existing
optimization framework, HiT, in terms of super-resolution performance (up to
0.2dB) and computational complexity reduction (up to 11%). The source code will
be made publicly available at www.github.com.
","[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 21:52:18 GMT'}]",2025-03-19,"[['Jiang', 'Yuxuan', ''], ['Zeng', 'Chengxi', ''], ['Teng', 'Siyue', ''], ['Zhang', 'Fan', ''], ['Zhu', 'Xiaoqing', ''], ['Sole', 'Joel', ''], ['Bull', 'David', '']]","[{'text': 'attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'hierarchical encoding mechanism', 'label': 'Attention mechanism'}, {'text': 'hierarchical encoding\nmechanism', 'label': 'Attention mechanism'}]",Attention mechanism,attention mechanisms,0.9558142423629761
2503.13798,Amirhossein Khakpour,"Amirhossein Khakpour, Lucia Florescu, Richard Tilley, Haibo Jiang, K.
  Swaminathan Iyer, Gustavo Carneiro","AI-Powered Prediction of Nanoparticle Pharmacokinetics: A Multi-View
  Learning Approach",,,,,cs.LG cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The clinical translation of nanoparticle-based treatments remains limited due
to the unpredictability of (nanoparticle) NP
pharmacokinetics$\unicode{x2014}$how they distribute, accumulate, and clear
from the body. Predicting these behaviours is challenging due to complex
biological interactions and the difficulty of obtaining high-quality
experimental datasets. Existing AI-driven approaches rely heavily on
data-driven learning but fail to integrate crucial knowledge about NP
properties and biodistribution mechanisms. We introduce a multi-view deep
learning framework that enhances pharmacokinetic predictions by incorporating
prior knowledge of key NP properties such as size and charge into a
cross-attention mechanism, enabling context-aware feature selection and
improving generalization despite small datasets. To further enhance prediction
robustness, we employ an ensemble learning approach, combining deep learning
with XGBoost (XGB) and Random Forest (RF), which significantly outperforms
existing AI models. Our interpretability analysis reveals key physicochemical
properties driving NP biodistribution, providing biologically meaningful
insights into possible mechanisms governing NP behaviour in vivo rather than a
black-box model. Furthermore, by bridging machine learning with physiologically
based pharmacokinetic (PBPK) modelling, this work lays the foundation for
data-efficient AI-driven drug discovery and precision nanomedicine.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 01:09:32 GMT'}]",2025-03-19,"[['Khakpour', 'Amirhossein', ''], ['Florescu', 'Lucia', ''], ['Tilley', 'Richard', ''], ['Jiang', 'Haibo', ''], ['Iyer', 'K. Swaminathan', ''], ['Carneiro', 'Gustavo', '']]","[{'text': 'cross-attention mechanism', 'label': 'Attention mechanism'}]",Attention mechanism,cross-attention mechanism,0.8302809000015259
2503.13799,Liangrui Pan,"Liangrui Pan, Xiaoyu Li, Yutao Dou, Qiya Song, Jiadi Luo, Qingchun
  Liang, Shaoliang Peng","SMILE: a Scale-aware Multiple Instance Learning Method for Multicenter
  STAS Lung Cancer Histopathology Diagnosis",,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Spread through air spaces (STAS) represents a newly identified aggressive
pattern in lung cancer, which is known to be associated with adverse prognostic
factors and complex pathological features. Pathologists currently rely on time
consuming manual assessments, which are highly subjective and prone to
variation. This highlights the urgent need for automated and precise diag
nostic solutions. 2,970 lung cancer tissue slides are comprised from multiple
centers, re-diagnosed them, and constructed and publicly released three lung
cancer STAS datasets: STAS CSU (hospital), STAS TCGA, and STAS CPTAC. All STAS
datasets provide corresponding pathological feature diagnoses and related
clinical data. To address the bias, sparse and heterogeneous nature of STAS, we
propose an scale-aware multiple instance learning(SMILE) method for STAS
diagnosis of lung cancer. By introducing a scale-adaptive attention mechanism,
the SMILE can adaptively adjust high attention instances, reducing
over-reliance on local regions and promoting consistent detection of STAS
lesions. Extensive experiments show that SMILE achieved competitive diagnostic
results on STAS CSU, diagnosing 251 and 319 STAS samples in CPTAC
andTCGA,respectively, surpassing clinical average AUC. The 11 open baseline
results are the first to be established for STAS research, laying the
foundation for the future expansion, interpretability, and clinical integration
of computational pathology technologies. The datasets and code are available at
https://anonymous.4open.science/r/IJCAI25-1DA1.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 01:09:52 GMT'}]",2025-03-19,"[['Pan', 'Liangrui', ''], ['Li', 'Xiaoyu', ''], ['Dou', 'Yutao', ''], ['Song', 'Qiya', ''], ['Luo', 'Jiadi', ''], ['Liang', 'Qingchun', ''], ['Peng', 'Shaoliang', '']]","[{'text': 'scale-adaptive attention mechanism', 'label': 'Attention mechanism'}]",Attention mechanism,scale-adaptive attention mechanism,0.7705892324447632
2503.13806,Jiancheng Ye,"Wenjie Zhang, Ziyang Zhang, Mengnan He, Jiancheng Ye","Organ-aware Multi-scale Medical Image Segmentation Using Text Prompt
  Engineering",,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Accurate segmentation is essential for effective treatment planning and
disease monitoring. Existing medical image segmentation methods predominantly
rely on uni-modal visual inputs, such as images or videos, requiring
labor-intensive manual annotations. Additionally, medical imaging techniques
capture multiple intertwined organs within a single scan, further complicating
segmentation accuracy. To address these challenges, MedSAM, a large-scale
medical segmentation model based on the Segment Anything Model (SAM), was
developed to enhance segmentation accuracy by integrating image features with
user-provided prompts. While MedSAM has demonstrated strong performance across
various medical segmentation tasks, it primarily relies on geometric prompts
(e.g., points and bounding boxes) and lacks support for text-based prompts,
which could help specify subtle or ambiguous anatomical structures. To overcome
these limitations, we propose the Organ-aware Multi-scale Text-guided Medical
Image Segmentation Model (OMT-SAM) for multi-organ segmentation. Our approach
introduces CLIP encoders as a novel image-text prompt encoder, operating with
the geometric prompt encoder to provide informative contextual guidance. We
pair descriptive textual prompts with corresponding images, processing them
through pre-trained CLIP encoders and a cross-attention mechanism to generate
fused image-text embeddings. Additionally, we extract multi-scale visual
features from MedSAM, capturing fine-grained anatomical details at different
levels of granularity. We evaluate OMT-SAM on the FLARE 2021 dataset,
benchmarking its performance against existing segmentation methods. Empirical
results demonstrate that OMT-SAM achieves a mean Dice Similarity Coefficient of
0.937, outperforming MedSAM (0.893) and other segmentation models, highlighting
its superior capability in handling complex medical image segmentation tasks.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 01:35:34 GMT'}]",2025-03-19,"[['Zhang', 'Wenjie', ''], ['Zhang', 'Ziyang', ''], ['He', 'Mengnan', ''], ['Ye', 'Jiancheng', '']]","[{'text': 'MedSAM', 'label': 'Large Language Model'}, {'text': 'user-provided prompts', 'label': 'Prompting'}, {'text': 'MedSAM', 'label': 'Large Language Model'}, {'text': 'geometric prompts', 'label': 'Prompting'}, {'text': 'text-based prompts', 'label': 'Prompting'}, {'text': 'cross-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'fused image-text embeddings', 'label': 'contextual Embedding'}, {'text': 'MedSAM', 'label': 'Large Language Model'}, {'text': 'MedSAM', 'label': 'Large Language Model'}]",Attention mechanism,cross-attention mechanism,0.8302809000015259
2503.13858,Hongyu Ke,"Hongyu Ke, Jack Morris, Kentaro Oguchi, Xiaofei Cao, Yongkang Liu,
  Haoxin Wang, Yi Ding","MamBEV: Enabling State Space Models to Learn Birds-Eye-View
  Representations",,ICLR 2025,,,cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  3D visual perception tasks, such as 3D detection from multi-camera images,
are essential components of autonomous driving and assistance systems. However,
designing computationally efficient methods remains a significant challenge. In
this paper, we propose a Mamba-based framework called MamBEV, which learns
unified Bird's Eye View (BEV) representations using linear spatio-temporal
SSM-based attention. This approach supports multiple 3D perception tasks with
significantly improved computational and memory efficiency. Furthermore, we
introduce SSM based cross-attention, analogous to standard cross attention,
where BEV query representations can interact with relevant image features.
Extensive experiments demonstrate MamBEV's promising performance across diverse
visual perception metrics, highlighting its advantages in input scaling
efficiency compared to existing benchmark models.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 03:18:45 GMT'}]",2025-03-19,"[['Ke', 'Hongyu', ''], ['Morris', 'Jack', ''], ['Oguchi', 'Kentaro', ''], ['Cao', 'Xiaofei', ''], ['Liu', 'Yongkang', ''], ['Wang', 'Haoxin', ''], ['Ding', 'Yi', '']]","[{'text': 'linear spatio-temporal\nSSM-based attention', 'label': 'Attention mechanism'}, {'text': 'standard cross attention', 'label': 'Attention mechanism'}, {'text': 'BEV', 'label': 'BERT'}, {'text': 'input scaling\nefficiency', 'label': 'Scaling law'}]",Attention mechanism,standard cross attention,0.6138131618499756
2503.13883,Ziyu Lin,"Ziyu Lin, Yunfan Wu, Yuhang Ma, Junzhou Chen, Ronghui Zhang, Jiaming
  Wu, Guodong Yin, and Liang Lin","YOLO-LLTS: Real-Time Low-Light Traffic Sign Detection via Prior-Guided
  Enhancement and Multi-Branch Feature Interaction",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Detecting traffic signs effectively under low-light conditions remains a
significant challenge. To address this issue, we propose YOLO-LLTS, an
end-to-end real-time traffic sign detection algorithm specifically designed for
low-light environments. Firstly, we introduce the High-Resolution Feature Map
for Small Object Detection (HRFM-TOD) module to address indistinct small-object
features in low-light scenarios. By leveraging high-resolution feature maps,
HRFM-TOD effectively mitigates the feature dilution problem encountered in
conventional PANet frameworks, thereby enhancing both detection accuracy and
inference speed. Secondly, we develop the Multi-branch Feature Interaction
Attention (MFIA) module, which facilitates deep feature interaction across
multiple receptive fields in both channel and spatial dimensions, significantly
improving the model's information extraction capabilities. Finally, we propose
the Prior-Guided Enhancement Module (PGFE) to tackle common image quality
challenges in low-light environments, such as noise, low contrast, and
blurriness. This module employs prior knowledge to enrich image details and
enhance visibility, substantially boosting detection performance. To support
this research, we construct a novel dataset, the Chinese Nighttime Traffic Sign
Sample Set (CNTSSS), covering diverse nighttime scenarios, including urban,
highway, and rural environments under varying weather conditions. Experimental
evaluations demonstrate that YOLO-LLTS achieves state-of-the-art performance,
outperforming the previous best methods by 2.7% mAP50 and 1.6% mAP50:95 on
TT100K-night, 1.3% mAP50 and 1.9% mAP50:95 on CNTSSS, and achieving superior
results on the CCTSDB2021 dataset. Moreover, deployment experiments on edge
devices confirm the real-time applicability and effectiveness of our proposed
approach.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 04:28:05 GMT'}]",2025-03-19,"[['Lin', 'Ziyu', ''], ['Wu', 'Yunfan', ''], ['Ma', 'Yuhang', ''], ['Chen', 'Junzhou', ''], ['Zhang', 'Ronghui', ''], ['Wu', 'Jiaming', ''], ['Yin', 'Guodong', ''], ['Lin', 'Liang', '']]","[{'text': 'Multi-branch Feature Interaction\nAttention', 'label': 'Attention mechanism'}]",Attention mechanism,"Multi-branch Feature Interaction
Attention",0.5357130169868469
2503.13926,Huan Ren,"Huan Ren, Wenfei Yang, Xiang Liu, Shifeng Zhang, Tianzhu Zhang","Learning Shape-Independent Transformation via Spherical Representations
  for Category-Level Object Pose Estimation","Accepted by ICLR 2025. Project page is available at
  https://renhuan1999.github.io/SpherePose",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Category-level object pose estimation aims to determine the pose and size of
novel objects in specific categories. Existing correspondence-based approaches
typically adopt point-based representations to establish the correspondences
between primitive observed points and normalized object coordinates. However,
due to the inherent shape-dependence of canonical coordinates, these methods
suffer from semantic incoherence across diverse object shapes. To resolve this
issue, we innovatively leverage the sphere as a shared proxy shape of objects
to learn shape-independent transformation via spherical representations. Based
on this insight, we introduce a novel architecture called SpherePose, which
yields precise correspondence prediction through three core designs. Firstly,
We endow the point-wise feature extraction with SO(3)-invariance, which
facilitates robust mapping between camera coordinate space and object
coordinate space regardless of rotation transformation. Secondly, the spherical
attention mechanism is designed to propagate and integrate features among
spherical anchors from a comprehensive perspective, thus mitigating the
interference of noise and incomplete point cloud. Lastly, a hyperbolic
correspondence loss function is designed to distinguish subtle distinctions,
which can promote the precision of correspondence prediction. Experimental
results on CAMERA25, REAL275 and HouseCat6D benchmarks demonstrate the superior
performance of our method, verifying the effectiveness of spherical
representations and architectural innovations.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 05:43:42 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 11:29:13 GMT'}]",2025-03-20,"[['Ren', 'Huan', ''], ['Yang', 'Wenfei', ''], ['Liu', 'Xiang', ''], ['Zhang', 'Shifeng', ''], ['Zhang', 'Tianzhu', '']]","[{'text': 'spherical\nattention mechanism', 'label': 'Attention mechanism'}]",Attention mechanism,"spherical
attention mechanism",0.7816957831382751
2503.13982,Huy Hoang Bui Mr,"Huy-Hoang Bui, Bach-Thuan Bui, Quang-Vinh Tran, Yasuyuki Fujii, Joo-Ho
  Lee","A-SCoRe: Attention-based Scene Coordinate Regression for wide-ranging
  scenarios",,,,,cs.CV,http://creativecommons.org/publicdomain/zero/1.0/,"  Visual localization is considered to be one of the crucial parts in many
robotic and vision systems. While state-of-the art methods that relies on
feature matching have proven to be accurate for visual localization, its
requirements for storage and compute are burdens. Scene coordinate regression
(SCR) is an alternative approach that remove the barrier for storage by
learning to map 2D pixels to 3D scene coordinates. Most popular SCR use
Convolutional Neural Network (CNN) to extract 2D descriptor, which we would
argue that it miss the spatial relationship between pixels. Inspired by the
success of vision transformer architecture, we present a new SCR architecture,
called A-ScoRe, an Attention-based model which leverage attention on descriptor
map level to produce meaningful and high-semantic 2D descriptors. Since the
operation is performed on descriptor map, our model can work with multiple data
modality whether it is a dense or sparse from depth-map, SLAM to
Structure-from-Motion (SfM). This versatility allows A-SCoRe to operate in
different kind of environments, conditions and achieve the level of flexibility
that is important for mobile robots. Results show our methods achieve
comparable performance with State-of-the-art methods on multiple benchmark
while being light-weighted and much more flexible. Code and pre-trained models
are public in our repository: https://github.com/ais-lab/A-SCoRe.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 07:39:50 GMT'}]",2025-03-19,"[['Bui', 'Huy-Hoang', ''], ['Bui', 'Bach-Thuan', ''], ['Tran', 'Quang-Vinh', ''], ['Fujii', 'Yasuyuki', ''], ['Lee', 'Joo-Ho', '']]","[{'text': 'attention', 'label': 'Attention mechanism'}]",Attention mechanism,attention,0.7383304834365845
2503.13985,Jaewoo Song,"Jaewoo Song, Daemin Park, Kanghyun Baek, Sangyub Lee, Jooyoung Choi,
  Eunji Kim, Sungroh Yoon","DefectFill: Realistic Defect Generation with Inpainting Diffusion Model
  for Visual Inspection",Accepted by CVPR 2025,,,,cs.CV cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Developing effective visual inspection models remains challenging due to the
scarcity of defect data. While image generation models have been used to
synthesize defect images, producing highly realistic defects remains difficult.
We propose DefectFill, a novel method for realistic defect generation that
requires only a few reference defect images. It leverages a fine-tuned
inpainting diffusion model, optimized with our custom loss functions
incorporating defect, object, and attention terms. It enables precise capture
of detailed, localized defect features and their seamless integration into
defect-free objects. Additionally, our Low-Fidelity Selection method further
enhances the defect sample quality. Experiments show that DefectFill generates
high-quality defect images, enabling visual inspection models to achieve
state-of-the-art performance on the MVTec AD dataset.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 07:42:11 GMT'}]",2025-03-19,"[['Song', 'Jaewoo', ''], ['Park', 'Daemin', ''], ['Baek', 'Kanghyun', ''], ['Lee', 'Sangyub', ''], ['Choi', 'Jooyoung', ''], ['Kim', 'Eunji', ''], ['Yoon', 'Sungroh', '']]","[{'text': 'attention terms', 'label': 'Attention mechanism'}]",Attention mechanism,attention terms,0.7109348773956299
2503.14035,Joopyo Hong,"Seung Woo Ko, Joopyo Hong, Suyoung Kim, Seungjai Bang, Sungzoon Cho,
  Nojun Kwak, Hyung-Sin Kim, Joonseok Lee",A Revisit to the Decoder for Camouflaged Object Detection,"Published in BMVC 2024, 13 pages, 7 figures (Appendix: 5 pages, 2
  figures)",British Machine Vision Conference (BMVC) 2024,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Camouflaged object detection (COD) aims to generate a fine-grained
segmentation map of camouflaged objects hidden in their background. Due to the
hidden nature of camouflaged objects, it is essential for the decoder to be
tailored to effectively extract proper features of camouflaged objects and
extra-carefully generate their complex boundaries. In this paper, we propose a
novel architecture that augments the prevalent decoding strategy in COD with
Enrich Decoder and Retouch Decoder, which help to generate a fine-grained
segmentation map. Specifically, the Enrich Decoder amplifies the channels of
features that are important for COD using channel-wise attention. Retouch
Decoder further refines the segmentation maps by spatially attending to
important pixels, such as the boundary regions. With extensive experiments, we
demonstrate that ENTO shows superior performance using various encoders, with
the two novel components playing their unique roles that are mutually
complementary.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 08:51:50 GMT'}]",2025-03-19,"[['Ko', 'Seung Woo', ''], ['Hong', 'Joopyo', ''], ['Kim', 'Suyoung', ''], ['Bang', 'Seungjai', ''], ['Cho', 'Sungzoon', ''], ['Kwak', 'Nojun', ''], ['Kim', 'Hyung-Sin', ''], ['Lee', 'Joonseok', '']]","[{'text': 'channel-wise attention', 'label': 'Attention mechanism'}]",Attention mechanism,channel-wise attention,0.7209811806678772
2503.14037,Liyan Wang,"Cong Wang, Jinshan Pan, Liyan Wang, and Wei Wang","Intra and Inter Parser-Prompted Transformers for Effective Image
  Restoration","This version is accepted by the Association for the Advancement of
  Artificial Intelligence (AAAI-25)",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose Intra and Inter Parser-Prompted Transformers (PPTformer) that
explore useful features from visual foundation models for image restoration.
Specifically, PPTformer contains two parts: an Image Restoration Network
(IRNet) for restoring images from degraded observations and a Parser-Prompted
Feature Generation Network (PPFGNet) for providing IRNet with reliable parser
information to boost restoration. To enhance the integration of the parser
within IRNet, we propose Intra Parser-Prompted Attention (IntraPPA) and Inter
Parser-Prompted Attention (InterPPA) to implicitly and explicitly learn useful
parser features to facilitate restoration. The IntraPPA re-considers cross
attention between parser and restoration features, enabling implicit perception
of the parser from a long-range and intra-layer perspective. Conversely, the
InterPPA initially fuses restoration features with those of the parser,
followed by formulating these fused features within an attention mechanism to
explicitly perceive parser information. Further, we propose a parser-prompted
feed-forward network to guide restoration within pixel-wise gating modulation.
Experimental results show that PPTformer achieves state-of-the-art performance
on image deraining, defocus deblurring, desnowing, and low-light enhancement.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 08:56:02 GMT'}]",2025-03-19,"[['Wang', 'Cong', ''], ['Pan', 'Jinshan', ''], ['Wang', 'Liyan', ''], ['Wang', 'Wei', '']]","[{'text': 'visual foundation models', 'label': 'Foundation Model'}, {'text': 'IntraPPA', 'label': 'Transformers'}, {'text': 'InterPPA', 'label': 'Transformers'}, {'text': 'attention mechanism', 'label': 'Attention mechanism'}]",Attention mechanism,attention mechanism,1.0
2503.14070,Junliang Guo,"Yang Ye, Junliang Guo, Haoyu Wu, Tianyu He, Tim Pearce, Tabish Rashid,
  Katja Hofmann, Jiang Bian",Fast Autoregressive Video Generation with Diagonal Decoding,,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Autoregressive Transformer models have demonstrated impressive performance in
video generation, but their sequential token-by-token decoding process poses a
major bottleneck, particularly for long videos represented by tens of thousands
of tokens. In this paper, we propose Diagonal Decoding (DiagD), a training-free
inference acceleration algorithm for autoregressively pre-trained models that
exploits spatial and temporal correlations in videos. Our method generates
tokens along diagonal paths in the spatial-temporal token grid, enabling
parallel decoding within each frame as well as partially overlapping across
consecutive frames. The proposed algorithm is versatile and adaptive to various
generative models and tasks, while providing flexible control over the
trade-off between inference speed and visual quality. Furthermore, we propose a
cost-effective finetuning strategy that aligns the attention patterns of the
model with our decoding order, further mitigating the training-inference gap on
small-scale models. Experiments on multiple autoregressive video generation
models and datasets demonstrate that DiagD achieves up to $10\times$ speedup
compared to naive sequential decoding, while maintaining comparable visual
fidelity.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 09:42:55 GMT'}]",2025-03-19,"[['Ye', 'Yang', ''], ['Guo', 'Junliang', ''], ['Wu', 'Haoyu', ''], ['He', 'Tianyu', ''], ['Pearce', 'Tim', ''], ['Rashid', 'Tabish', ''], ['Hofmann', 'Katja', ''], ['Bian', 'Jiang', '']]","[{'text': 'cost-effective finetuning strategy', 'label': 'Fine-tuning'}, {'text': 'attention patterns', 'label': 'Attention mechanism'}]",Attention mechanism,attention patterns,0.8255490064620972
2503.14130,Paul Darm,"Paul Darm, James Xie, Annalisa Riccardi","Inference-Time Intervention in Large Language Models for Reliable
  Requirement Verification",,,,,cs.AI cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Steering the behavior of Large Language Models (LLMs) remains a challenge,
particularly in engineering applications where precision and reliability are
critical. While fine-tuning and prompting methods can modify model behavior,
they lack the dynamic and exact control necessary for engineering applications.
Inference-time intervention techniques provide a promising alternative,
allowing targeted adjustments to LLM outputs. In this work, we demonstrate how
interventions enable fine-grained control for automating the usually
time-intensive requirement verification process in Model-Based Systems
Engineering (MBSE). Using two early-stage Capella SysML models of space
missions with associated requirements, we apply the intervened LLMs to reason
over a graph representation of the model to determine whether a requirement is
fulfilled. Our method achieves robust and reliable outputs, significantly
improving over both a baseline model and a fine-tuning approach. By identifying
and modifying as few as one to three specialised attention heads, we can
significantly change the model's behavior. When combined with self-consistency,
this allows us to achieve perfect precision on our holdout test set.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 10:49:36 GMT'}]",2025-03-19,"[['Darm', 'Paul', ''], ['Xie', 'James', ''], ['Riccardi', 'Annalisa', '']]","[{'text': 'specialised attention heads', 'label': 'Attention mechanism'}]",Attention mechanism,specialised attention heads,0.6921795606613159
2503.14150,Yihang Zhou,"Yihang Zhou, Ruige Kong, Zhengsen Xu, Linlin Xu, Sibo Cheng","Comparative and Interpretative Analysis of CNN and Transformer Models in
  Predicting Wildfire Spread Using Remote Sensing Data",,,10.1029/2024JH000409,,cs.CV eess.IV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Facing the escalating threat of global wildfires, numerous computer vision
techniques using remote sensing data have been applied in this area. However,
the selection of deep learning methods for wildfire prediction remains
uncertain due to the lack of comparative analysis in a quantitative and
explainable manner, crucial for improving prevention measures and refining
models. This study aims to thoroughly compare the performance, efficiency, and
explainability of four prevalent deep learning architectures: Autoencoder,
ResNet, UNet, and Transformer-based Swin-UNet. Employing a real-world dataset
that includes nearly a decade of remote sensing data from California, U.S.,
these models predict the spread of wildfires for the following day. Through
detailed quantitative comparison analysis, we discovered that Transformer-based
Swin-UNet and UNet generally outperform Autoencoder and ResNet, particularly
due to the advanced attention mechanisms in Transformer-based Swin-UNet and the
efficient use of skip connections in both UNet and Transformer-based Swin-UNet,
which contribute to superior predictive accuracy and model interpretability.
Then we applied XAI techniques on all four models, this not only enhances the
clarity and trustworthiness of models but also promotes focused improvements in
wildfire prediction capabilities. The XAI analysis reveals that UNet and
Transformer-based Swin-UNet are able to focus on critical features such as
'Previous Fire Mask', 'Drought', and 'Vegetation' more effectively than the
other two models, while also maintaining balanced attention to the remaining
features, leading to their superior performance. The insights from our thorough
comparative analysis offer substantial implications for future model design and
also provide guidance for model selection in different scenarios.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:16:48 GMT'}]",2025-03-19,"[['Zhou', 'Yihang', ''], ['Kong', 'Ruige', ''], ['Xu', 'Zhengsen', ''], ['Xu', 'Linlin', ''], ['Cheng', 'Sibo', '']]","[{'text': 'Transformer-based Swin-UNet', 'label': 'Transformer-based model'}, {'text': 'Transformer-based\nSwin-UNet', 'label': 'Transformer-based model'}, {'text': 'advanced attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'Transformer-based Swin-UNet', 'label': 'Transformer-based model'}, {'text': 'Transformer-based Swin-UNet', 'label': 'Transformer-based model'}, {'text': 'Transformer-based Swin-UNet', 'label': 'Transformer-based model'}]",Attention mechanism,advanced attention mechanisms,0.8760451674461365
2503.14151,Yong Zhong,"Yong Zhong, Zhuoyi Yang, Jiayan Teng, Xiaotao Gu, Chongxuan Li",Concat-ID: Towards Universal Identity-Preserving Video Synthesis,,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present Concat-ID, a unified framework for identity-preserving video
generation. Concat-ID employs Variational Autoencoders to extract image
features, which are concatenated with video latents along the sequence
dimension, leveraging solely 3D self-attention mechanisms without the need for
additional modules. A novel cross-video pairing strategy and a multi-stage
training regimen are introduced to balance identity consistency and facial
editability while enhancing video naturalness. Extensive experiments
demonstrate Concat-ID's superiority over existing methods in both single and
multi-identity generation, as well as its seamless scalability to multi-subject
scenarios, including virtual try-on and background-controllable generation.
Concat-ID establishes a new benchmark for identity-preserving video synthesis,
providing a versatile and scalable solution for a wide range of applications.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:17:32 GMT'}]",2025-03-19,"[['Zhong', 'Yong', ''], ['Yang', 'Zhuoyi', ''], ['Teng', 'Jiayan', ''], ['Gu', 'Xiaotao', ''], ['Li', 'Chongxuan', '']]","[{'text': '3D self-attention mechanisms', 'label': 'Attention mechanism'}]",Attention mechanism,3D self-attention mechanisms,0.7052303552627563
2503.14231,Giovanni Delnevo,"Ziyao Ling, Giovanni Delnevo, Paola Salomoni, Silvia Mirri","Multi-task Learning for Identification of Porcelain in Song and Yuan
  Dynasties",,,,,cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Chinese porcelain holds immense historical and cultural value, making its
accurate classification essential for archaeological research and cultural
heritage preservation. Traditional classification methods rely heavily on
expert analysis, which is time-consuming, subjective, and difficult to scale.
This paper explores the application of DL and transfer learning techniques to
automate the classification of porcelain artifacts across four key attributes:
dynasty, glaze, ware, and type. We evaluate four Convolutional Neural Networks
(CNNs) - ResNet50, MobileNetV2, VGG16, and InceptionV3 - comparing their
performance with and without pre-trained weights. Our results demonstrate that
transfer learning significantly enhances classification accuracy, particularly
for complex tasks like type classification, where models trained from scratch
exhibit lower performance. MobileNetV2 and ResNet50 consistently achieve high
accuracy and robustness across all tasks, while VGG16 struggles with more
diverse classifications. We further discuss the impact of dataset limitations
and propose future directions, including domain-specific pre-training,
integration of attention mechanisms, explainable AI methods, and generalization
to other cultural artifacts.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 13:09:00 GMT'}]",2025-03-19,"[['Ling', 'Ziyao', ''], ['Delnevo', 'Giovanni', ''], ['Salomoni', 'Paola', ''], ['Mirri', 'Silvia', '']]","[{'text': 'transfer learning', 'label': 'Few-shot Learning'}, {'text': 'ResNet50', 'label': 'GPT-2'}, {'text': 'MobileNetV2', 'label': 'GPT-2'}, {'text': 'VGG16', 'label': 'GPT-2'}, {'text': 'InceptionV3', 'label': 'GPT-2'}, {'text': 'transfer learning', 'label': 'Few-shot Learning'}, {'text': 'MobileNetV2', 'label': 'GPT-2'}, {'text': 'attention mechanisms', 'label': 'Attention mechanism'}]",Attention mechanism,attention mechanisms,0.9558142423629761
2503.14240,Viet Nguyen,"Viet The Nguyen, Duy Anh Pham, An Thai Le, Jans Peter, Gunther Gust",Persistent Homology-induced Graph Ensembles for Time Series Regressions,,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The effectiveness of Spatio-temporal Graph Neural Networks (STGNNs) in
time-series applications is often limited by their dependence on fixed,
hand-crafted input graph structures. Motivated by insights from the Topological
Data Analysis (TDA) paradigm, of which real-world data exhibits multi-scale
patterns, we construct several graphs using Persistent Homology Filtration -- a
mathematical framework describing the multiscale structural properties of data
points. Then, we use the constructed graphs as an input to create an ensemble
of Graph Neural Networks. The ensemble aggregates the signals from the
individual learners via an attention-based routing mechanism, thus
systematically encoding the inherent multiscale structures of data. Four
different real-world experiments on seismic activity prediction and traffic
forecasting (PEMS-BAY, METR-LA) demonstrate that our approach consistently
outperforms single-graph baselines while providing interpretable insights.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 13:22:52 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 10:33:40 GMT'}]",2025-03-20,"[['Nguyen', 'Viet The', ''], ['Pham', 'Duy Anh', ''], ['Le', 'An Thai', ''], ['Peter', 'Jans', ''], ['Gust', 'Gunther', '']]","[{'text': 'attention-based routing mechanism', 'label': 'Attention mechanism'}]",Attention mechanism,attention-based routing mechanism,0.631773829460144
2503.14428,Yufan Deng,"Hongyu Zhang, Yufan Deng, Shenghai Yuan, Peng Jin, Zesen Cheng, Yian
  Zhao, Chang Liu, Jie Chen","MagicComp: Training-free Dual-Phase Refinement for Compositional Video
  Generation",Project webpage: https://hong-yu-zhang.github.io/MagicComp-Page/,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text-to-video (T2V) generation has made significant strides with diffusion
models. However, existing methods still struggle with accurately binding
attributes, determining spatial relationships, and capturing complex action
interactions between multiple subjects. To address these limitations, we
propose MagicComp, a training-free method that enhances compositional T2V
generation through dual-phase refinement. Specifically, (1) During the
Conditioning Stage: We introduce the Semantic Anchor Disambiguation to
reinforces subject-specific semantics and resolve inter-subject ambiguity by
progressively injecting the directional vectors of semantic anchors into
original text embedding; (2) During the Denoising Stage: We propose Dynamic
Layout Fusion Attention, which integrates grounding priors and model-adaptive
spatial perception to flexibly bind subjects to their spatiotemporal regions
through masked attention modulation. Furthermore, MagicComp is a model-agnostic
and versatile approach, which can be seamlessly integrated into existing T2V
architectures. Extensive experiments on T2V-CompBench and VBench demonstrate
that MagicComp outperforms state-of-the-art methods, highlighting its potential
for applications such as complex prompt-based and trajectory-controllable video
generation. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:02:14 GMT'}]",2025-03-19,"[['Zhang', 'Hongyu', ''], ['Deng', 'Yufan', ''], ['Yuan', 'Shenghai', ''], ['Jin', 'Peng', ''], ['Cheng', 'Zesen', ''], ['Zhao', 'Yian', ''], ['Liu', 'Chang', ''], ['Chen', 'Jie', '']]","[{'text': 'original text embedding', 'label': 'Embedding'}, {'text': 'Dynamic\nLayout Fusion Attention', 'label': 'Attention mechanism'}, {'text': 'masked attention modulation', 'label': 'Attention mechanism'}]",Attention mechanism,masked attention modulation,0.7005888819694519
2503.14439,Kyriakos Stylianopoulos,"Kyriakos Stylianopoulos, Panagiotis Gavriilidis, Gabriele Gradoni,
  George C. Alexandropoulos","Graph-CNNs for RF Imaging: Learning the Electric Field Integral
  Equations",Submitted to EUSIPCO 2025,,,,cs.LG eess.SP,http://creativecommons.org/licenses/by/4.0/,"  Radio-Frequency (RF) imaging concerns the digital recreation of the surfaces
of scene objects based on the scattered field at distributed receivers. To
solve this difficult inverse scattering problems, data-driven methods are often
employed that extract patterns from similar training examples, while offering
minimal latency. In this paper, we first provide an approximate yet fast
electromagnetic model, which is based on the electric field integral equations,
for data generation, and subsequently propose a Deep Neural Network (DNN)
architecture to learn the corresponding inverse model. A graph-attention
backbone allows for the system geometry to be passed to the DNN, where residual
convolutional layers extract features about the objects, while a UNet head
performs the final image reconstruction. Our quantitative and qualitative
evaluations on two synthetic data sets of different characteristics showcase
the performance gains of thee proposed advanced architecture and its relative
resilience to signal noise levels and various reception configurations.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:16:40 GMT'}]",2025-03-19,"[['Stylianopoulos', 'Kyriakos', ''], ['Gavriilidis', 'Panagiotis', ''], ['Gradoni', 'Gabriele', ''], ['Alexandropoulos', 'George C.', '']]","[{'text': 'graph-attention\nbackbone', 'label': 'Attention mechanism'}]",Attention mechanism,"graph-attention
backbone",0.5559301376342773
2503.14493,Chuxin Wang,"Chuxin Wang, Wenfei Yang, Xiang Liu, Tianzhu Zhang","State Space Model Meets Transformer: A New Paradigm for 3D Object
  Detection","Accepted by ICLR 2025. Project url:
  https://chuxwa.github.io/project_DEST/",,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  DETR-based methods, which use multi-layer transformer decoders to refine
object queries iteratively, have shown promising performance in 3D indoor
object detection. However, the scene point features in the transformer decoder
remain fixed, leading to minimal contributions from later decoder layers,
thereby limiting performance improvement. Recently, State Space Models (SSM)
have shown efficient context modeling ability with linear complexity through
iterative interactions between system states and inputs. Inspired by SSMs, we
propose a new 3D object DEtection paradigm with an interactive STate space
model (DEST). In the interactive SSM, we design a novel state-dependent SSM
parameterization method that enables system states to effectively serve as
queries in 3D indoor detection tasks. In addition, we introduce four key
designs tailored to the characteristics of point cloud and SSM: The
serialization and bidirectional scanning strategies enable bidirectional
feature interaction among scene points within the SSM. The inter-state
attention mechanism models the relationships between state points, while the
gated feed-forward network enhances inter-channel correlations. To the best of
our knowledge, this is the first method to model queries as system states and
scene points as system inputs, which can simultaneously update scene point
features and query features with linear complexity. Extensive experiments on
two challenging datasets demonstrate the effectiveness of our DEST-based
method. Our method improves the GroupFree baseline in terms of AP50 on ScanNet
V2 (+5.3) and SUN RGB-D (+3.2) datasets. Based on the VDETR baseline, Our
method sets a new SOTA on the ScanNetV2 and SUN RGB-D datasets.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:58:03 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 14:10:18 GMT'}]",2025-03-20,"[['Wang', 'Chuxin', ''], ['Yang', 'Wenfei', ''], ['Liu', 'Xiang', ''], ['Zhang', 'Tianzhu', '']]","[{'text': 'inter-state\nattention mechanism', 'label': 'Attention mechanism'}]",Attention mechanism,"inter-state
attention mechanism",0.8573140501976013
2503.14640,Yi Liao,"Yi Liao, Yongsheng Gao, and Weichuan Zhang","Dynamic Accumulated Attention Map for Interpreting Evolution of
  Decision-Making in Vision Transformer",,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Various Vision Transformer (ViT) models have been widely used for image
recognition tasks. However, existing visual explanation methods can not display
the attention flow hidden inside the inner structure of ViT models, which
explains how the final attention regions are formed inside a ViT for its
decision-making. In this paper, a novel visual explanation approach, Dynamic
Accumulated Attention Map (DAAM), is proposed to provide a tool that can
visualize, for the first time, the attention flow from the top to the bottom
through ViT networks. To this end, a novel decomposition module is proposed to
construct and store the spatial feature information by unlocking the [class]
token generated by the self-attention module of each ViT block. The module can
also obtain the channel importance coefficients by decomposing the
classification score for supervised ViT models. Because of the lack of
classification score in self-supervised ViT models, we propose dimension-wise
importance weights to compute the channel importance coefficients. Such spatial
features are linearly combined with the corresponding channel importance
coefficients, forming the attention map for each block. The dynamic attention
flow is revealed by block-wisely accumulating each attention map. The
contribution of this work focuses on visualizing the evolution dynamic of the
decision-making attention for any intermediate block inside a ViT model by
proposing a novel decomposition module and dimension-wise importance weights.
The quantitative and qualitative analysis consistently validate the
effectiveness and superior capacity of the proposed DAAM for not only
interpreting ViT models with the fully-connected layers as the classifier but
also self-supervised ViT models. The code is available at
https://github.com/ly9802/DynamicAccumulatedAttentionMap.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 18:41:01 GMT'}]",2025-03-20,"[['Liao', 'Yi', ''], ['Gao', 'Yongsheng', ''], ['Zhang', 'Weichuan', '']]","[{'text': 'attention flow', 'label': 'Attention mechanism'}, {'text': 'attention flow', 'label': 'Attention mechanism'}, {'text': 'attention\nflow', 'label': 'Attention mechanism'}]",Attention mechanism,attention flow,0.67100989818573
2503.14751,Nicola Franco,"Rohan Menon, Nicola Franco, Stephan G\""unnemann",LipShiFT: A Certifiably Robust Shift-based Vision Transformer,ICLR 2025 Workshop: VerifAI: AI Verification in the Wild,ICLR 2025 Workshop: VerifAI: AI Verification in the Wild,,,cs.LG cs.AI cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Deriving tight Lipschitz bounds for transformer-based architectures presents
a significant challenge. The large input sizes and high-dimensional attention
modules typically prove to be crucial bottlenecks during the training process
and leads to sub-optimal results. Our research highlights practical constraints
of these methods in vision tasks. We find that Lipschitz-based margin training
acts as a strong regularizer while restricting weights in successive layers of
the model. Focusing on a Lipschitz continuous variant of the ShiftViT model, we
address significant training challenges for transformer-based architectures
under norm-constrained input setting. We provide an upper bound estimate for
the Lipschitz constants of this model using the $l_2$ norm on common image
classification datasets. Ultimately, we demonstrate that our method scales to
larger models and advances the state-of-the-art in certified robustness for
transformer-based architectures.
","[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 21:38:18 GMT'}]",2025-03-20,"[['Menon', 'Rohan', ''], ['Franco', 'Nicola', ''], ['G√ºnnemann', 'Stephan', '']]","[{'text': 'high-dimensional attention\nmodules', 'label': 'Attention mechanism'}]",Attention mechanism,"high-dimensional attention
modules",0.5948148965835571
2503.14936,Yifan Zhang,"Yifan Zhang, Chen Huang, Zachary Karas, Dung Thuy Nguyen, Kevin Leach,
  Yu Huang",Enhancing Code LLM Training with Programmer Attention,,,,,cs.SE cs.HC cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Human attention provides valuable yet underexploited signals for code LLM
training, offering a perspective beyond purely machine-driven attention.
Despite the complexity and cost of collecting eye-tracking data, there has also
been limited progress in systematically using these signals for code LLM
training. To address both issues, we propose a cohesive pipeline spanning
augmentation and reward-based fine-tuning. Specifically, we introduce (1) an
eye-tracking path augmentation method to expand programmer attention datasets,
(2) a pattern abstraction step that refines raw fixations into learnable
attention motifs, and (3) a reward-guided strategy for integrating these
insights directly into a CodeT5 supervised fine-tuning process. Our experiments
yield +7.16 in CodeBLEU on the CodeXGlue benchmark for code summarization,
underscoring how uniting human and machine attention can boost code
intelligence. We hope this work encourages broader exploration of human-centric
methods in next-generation AI4SE.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 06:44:29 GMT'}]",2025-03-20,"[['Zhang', 'Yifan', ''], ['Huang', 'Chen', ''], ['Karas', 'Zachary', ''], ['Nguyen', 'Dung Thuy', ''], ['Leach', 'Kevin', ''], ['Huang', 'Yu', '']]","[{'text': 'Human attention', 'label': 'Attention mechanism'}, {'text': 'code LLM', 'label': 'LLM'}, {'text': 'reward-based fine-tuning', 'label': 'Fine-tuning'}, {'text': 'programmer attention', 'label': 'Attention mechanism'}, {'text': 'code summarization', 'label': 'Knowledge distillation'}, {'text': 'human and machine attention', 'label': 'Attention mechanism'}]",Attention mechanism,Human attention,0.7963528633117676
2503.14938,Ci Liu,"Zhong Ji, Ci Liu, Jingren Liu, Chen Tang, Yanwei Pang, Xuelong Li","Optimal Transport Adapter Tuning for Bridging Modality Gaps in Few-Shot
  Remote Sensing Scene Classification",,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Few-Shot Remote Sensing Scene Classification (FS-RSSC) presents the challenge
of classifying remote sensing images with limited labeled samples. Existing
methods typically emphasize single-modal feature learning, neglecting the
potential benefits of optimizing multi-modal representations. To address this
limitation, we propose a novel Optimal Transport Adapter Tuning (OTAT)
framework aimed at constructing an ideal Platonic representational space
through optimal transport (OT) theory. This framework seeks to harmonize rich
visual information with less dense textual cues, enabling effective cross-modal
information transfer and complementarity. Central to this approach is the
Optimal Transport Adapter (OTA), which employs a cross-modal attention
mechanism to enrich textual representations and facilitate subsequent better
information interaction. By transforming the network optimization into an OT
optimization problem, OTA establishes efficient pathways for balanced
information exchange between modalities. Moreover, we introduce a sample-level
Entropy-Aware Weighted (EAW) loss, which combines difficulty-weighted
similarity scores with entropy-based regularization. This loss function
provides finer control over the OT optimization process, enhancing its
solvability and stability. Our framework offers a scalable and efficient
solution for advancing multimodal learning in remote sensing applications.
Extensive experiments on benchmark datasets demonstrate that OTAT achieves
state-of-the-art performance in FS-RSSC, significantly improving the model
performance and generalization.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 07:04:24 GMT'}]",2025-03-20,"[['Ji', 'Zhong', ''], ['Liu', 'Ci', ''], ['Liu', 'Jingren', ''], ['Tang', 'Chen', ''], ['Pang', 'Yanwei', ''], ['Li', 'Xuelong', '']]","[{'text': 'single-modal feature learning', 'label': 'Zero-shot Learning'}, {'text': 'cross-modal attention\nmechanism', 'label': 'Attention mechanism'}, {'text': 'FS-RSSC', 'label': 'Few-shot Learning'}]",Attention mechanism,"cross-modal attention
mechanism",0.7663174867630005
2503.14944,Zihan Cao,"Zihan Cao, Yu Zhong, Ziqi Wang, Liang-Jian Deng","MMAIF: Multi-task and Multi-degradation All-in-One for Image Fusion with
  Language Guidance",,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Image fusion, a fundamental low-level vision task, aims to integrate multiple
image sequences into a single output while preserving as much information as
possible from the input. However, existing methods face several significant
limitations: 1) requiring task- or dataset-specific models; 2) neglecting
real-world image degradations (\textit{e.g.}, noise), which causes failure when
processing degraded inputs; 3) operating in pixel space, where attention
mechanisms are computationally expensive; and 4) lacking user interaction
capabilities. To address these challenges, we propose a unified framework for
multi-task, multi-degradation, and language-guided image fusion. Our framework
includes two key components: 1) a practical degradation pipeline that simulates
real-world image degradations and generates interactive prompts to guide the
model; 2) an all-in-one Diffusion Transformer (DiT) operating in latent space,
which fuses a clean image conditioned on both the degraded inputs and the
generated prompts. Furthermore, we introduce principled modifications to the
original DiT architecture to better suit the fusion task. Based on this
framework, we develop two versions of the model: Regression-based and Flow
Matching-based variants. Extensive qualitative and quantitative experiments
demonstrate that our approach effectively addresses the aforementioned
limitations and outperforms previous restoration+fusion and all-in-one
pipelines. Codes are available at https://github.com/294coder/MMAIF.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 07:20:02 GMT'}]",2025-03-20,"[['Cao', 'Zihan', ''], ['Zhong', 'Yu', ''], ['Wang', 'Ziqi', ''], ['Deng', 'Liang-Jian', '']]","[{'text': 'attention\nmechanisms', 'label': 'Attention mechanism'}, {'text': 'interactive prompts', 'label': 'Prompting'}]",Attention mechanism,"attention
mechanisms",0.9558142423629761
2503.14960,Seungyeon Cho,"Seungyeon Cho, Tae-Kyun Kim","Body-Hand Modality Expertized Networks with Cross-attention for
  Fine-grained Skeleton Action Recognition","7 figures, 8 pages",,,,cs.CV,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Skeleton-based Human Action Recognition (HAR) is a vital technology in
robotics and human-robot interaction. However, most existing methods
concentrate primarily on full-body movements and often overlook subtle hand
motions that are critical for distinguishing fine-grained actions. Recent work
leverages a unified graph representation that combines body, hand, and foot
keypoints to capture detailed body dynamics. Yet, these models often blur fine
hand details due to the disparity between body and hand action characteristics
and the loss of subtle features during the spatial-pooling. In this paper, we
propose BHaRNet (Body-Hand action Recognition Network), a novel framework that
augments a typical body-expert model with a hand-expert model. Our model
jointly trains both streams with an ensemble loss that fosters cooperative
specialization, functioning in a manner reminiscent of a Mixture-of-Experts
(MoE). Moreover, cross-attention is employed via an expertized branch method
and a pooling-attention module to enable feature-level interactions and
selectively fuse complementary information. Inspired by MMNet, we also
demonstrate the applicability of our approach to multi-modal tasks by
leveraging RGB information, where body features guide RGB learning to capture
richer contextual cues. Experiments on large-scale benchmarks (NTU RGB+D 60,
NTU RGB+D 120, PKU-MMD, and Northwestern-UCLA) demonstrate that BHaRNet
achieves SOTA accuracies -- improving from 86.4\% to 93.0\% in hand-intensive
actions -- while maintaining fewer GFLOPs and parameters than the relevant
unified methods.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 07:54:52 GMT'}]",2025-03-20,"[['Cho', 'Seungyeon', ''], ['Kim', 'Tae-Kyun', '']]","[{'text': 'cross-attention', 'label': 'Attention mechanism'}, {'text': 'RGB learning', 'label': 'Few-shot Learning'}]",Attention mechanism,cross-attention,0.6773566007614136
2503.15008,Saddam Hussain Khan,"Aamir Mehmood, Yue Hu, Saddam Hussain Khan (Artificial Intelligence
  Lab, Department of Computer Systems Engineering, University of Engineering
  and Applied Sciences (UEAS), Swat, Pakistan)","A Novel Channel Boosted Residual CNN-Transformer with Regional-Boundary
  Learning for Breast Cancer Detection","12 pages, 10 Figures, 2 Tables. arXiv admin note: substantial text
  overlap with arXiv:2405.12986",,,,eess.IV cs.AI cs.CV cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Recent advancements in detecting tumors using deep learning on breast
ultrasound images (BUSI) have demonstrated significant success. Deep CNNs and
vision-transformers (ViTs) have demonstrated individually promising initial
performance. However, challenges related to model complexity and contrast,
texture, and tumor morphology variations introduce uncertainties that hinder
the effectiveness of current methods. This study introduces a novel hybrid
framework, CB-Res-RBCMT, combining customized residual CNNs and new ViT
components for detailed BUSI cancer analysis. The proposed RBCMT uses stem
convolution blocks with CNN Meet Transformer (CMT) blocks, followed by new
Regional and boundary (RB) feature extraction operations for capturing contrast
and morphological variations. Moreover, the CMT block incorporates global
contextual interactions through multi-head attention, enhancing computational
efficiency with a lightweight design. Additionally, the customized inverse
residual and stem CNNs within the CMT effectively extract local texture
information and handle vanishing gradients. Finally, the new channel-boosted
(CB) strategy enriches the feature diversity of the limited dataset by
combining the original RBCMT channels with transfer learning-based residual
CNN-generated maps. These diverse channels are processed through a spatial
attention block for optimal pixel selection, reducing redundancy and improving
the discrimination of minor contrast and texture variations. The proposed
CB-Res-RBCMT achieves an F1-score of 95.57%, accuracy of 95.63%, sensitivity of
96.42%, and precision of 94.79% on the standard harmonized stringent BUSI
dataset, outperforming existing ViT and CNN methods. These results demonstrate
the versatility of our integrated CNN-Transformer framework in capturing
diverse features and delivering superior performance in BUSI cancer diagnosis.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 08:59:02 GMT'}]",2025-03-20,"[['Mehmood', 'Aamir', '', 'Artificial Intelligence\n  Lab, Department of Computer Systems Engineering, University of Engineering\n  and Applied Sciences'], ['Hu', 'Yue', '', 'Artificial Intelligence\n  Lab, Department of Computer Systems Engineering, University of Engineering\n  and Applied Sciences'], ['Khan', 'Saddam Hussain', '', 'Artificial Intelligence\n  Lab, Department of Computer Systems Engineering, University of Engineering\n  and Applied Sciences']]","[{'text': 'multi-head attention', 'label': 'Attention mechanism'}]",Attention mechanism,multi-head attention,0.6964311599731445
2503.15141,Nikola {\DJ}uki\'c,"Nikola {\DJ}uki\'c, Tim Lebailly, Tinne Tuytelaars",Object-Centric Pretraining via Target Encoder Bootstrapping,ICLR 2025,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Object-centric representation learning has recently been successfully applied
to real-world datasets. This success can be attributed to pretrained
non-object-centric foundation models, whose features serve as reconstruction
targets for slot attention. However, targets must remain frozen throughout the
training, which sets an upper bound on the performance object-centric models
can attain. Attempts to update the target encoder by bootstrapping result in
large performance drops, which can be attributed to its lack of object-centric
inductive biases, causing the object-centric model's encoder to drift away from
representations useful as reconstruction targets. To address these limitations,
we propose Object-CEntric Pretraining by Target Encoder BOotstrapping, a
self-distillation setup for training object-centric models from scratch, on
real-world data, for the first time ever. In OCEBO, the target encoder is
updated as an exponential moving average of the object-centric model, thus
explicitly being enriched with object-centric inductive biases introduced by
slot attention while removing the upper bound on performance present in other
models. We mitigate the slot collapse caused by random initialization of the
target encoder by introducing a novel cross-view patch filtering approach that
limits the supervision to sufficiently informative patches. When pretrained on
241k images from COCO, OCEBO achieves unsupervised object discovery performance
comparable to that of object-centric models with frozen non-object-centric
target encoders pretrained on hundreds of millions of images. The code and
pretrained models are publicly available at https://github.com/djukicn/ocebo.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 12:06:50 GMT'}]",2025-03-20,"[['ƒêukiƒá', 'Nikola', ''], ['Lebailly', 'Tim', ''], ['Tuytelaars', 'Tinne', '']]","[{'text': 'Object-centric representation learning', 'label': 'Few-shot Learning'}, {'text': 'slot attention', 'label': 'Attention mechanism'}, {'text': 'object-centric models', 'label': 'Foundation Model'}, {'text': 'object-centric models', 'label': 'Foundation Model'}, {'text': 'slot attention', 'label': 'Attention mechanism'}, {'text': 'object-centric models', 'label': 'Foundation Model'}]",Attention mechanism,slot attention,0.66867595911026
2503.15404,Yuchen Ren,"Yuchen Ren, Zhengyu Zhao, Chenhao Lin, Bo Yang, Lu Zhou, Zhe Liu, Chao
  Shen","Improving Adversarial Transferability on Vision Transformers via Forward
  Propagation Refinement",CVPR2025,,,,cs.CV cs.CR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Vision Transformers (ViTs) have been widely applied in various computer
vision and vision-language tasks. To gain insights into their robustness in
practical scenarios, transferable adversarial examples on ViTs have been
extensively studied. A typical approach to improving adversarial
transferability is by refining the surrogate model. However, existing work on
ViTs has restricted their surrogate refinement to backward propagation. In this
work, we instead focus on Forward Propagation Refinement (FPR) and specifically
refine two key modules of ViTs: attention maps and token embeddings. For
attention maps, we propose Attention Map Diversification (AMD), which
diversifies certain attention maps and also implicitly imposes beneficial
gradient vanishing during backward propagation. For token embeddings, we
propose Momentum Token Embedding (MTE), which accumulates historical token
embeddings to stabilize the forward updates in both the Attention and MLP
blocks. We conduct extensive experiments with adversarial examples transferred
from ViTs to various CNNs and ViTs, demonstrating that our FPR outperforms the
current best (backward) surrogate refinement by up to 7.0\% on average. We also
validate its superiority against popular defenses and its compatibility with
other transfer methods. Codes and appendix are available at
https://github.com/RYC-98/FPR.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:44:23 GMT'}]",2025-03-20,"[['Ren', 'Yuchen', ''], ['Zhao', 'Zhengyu', ''], ['Lin', 'Chenhao', ''], ['Yang', 'Bo', ''], ['Zhou', 'Lu', ''], ['Liu', 'Zhe', ''], ['Shen', 'Chao', '']]","[{'text': 'Vision Transformers', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'attention maps', 'label': 'Attention mechanism'}, {'text': 'token embeddings', 'label': 'Embedding'}, {'text': 'attention maps', 'label': 'Attention mechanism'}, {'text': 'Attention', 'label': 'Attention mechanism'}, {'text': 'attention maps', 'label': 'Attention mechanism'}, {'text': 'Momentum Token Embedding', 'label': 'Embedding'}, {'text': 'token\nembeddings', 'label': 'Embedding'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}]",Attention mechanism,Attention,0.7383304834365845
2503.15469,ZhengLin Lai,"ZhengLin Lai, MengYao Liao, Dong Xu","Dynamic Bi-Elman Attention Networks (DBEAN): Dual-Directional
  Context-Aware Representation Learning for Enhanced Text Classification","11 pages,1 figure",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Text classification, a fundamental task in natural language processing (NLP),
aims to categorize textual data into predefined labels. Traditional methods
struggled with complex linguistic structures and semantic dependencies. The
advent of deep learning, particularly recurrent neural networks (RNNs) and
Transformer-based models, has significantly advanced the field by enabling
nuanced feature extraction and context-aware predictions. Despite improvements,
existing models exhibit limitations in balancing interpretability,
computational efficiency, and long-range contextual understanding. This paper
proposes the Dynamic Bidirectional Elman with Attention Network (DBEAN), which
integrates bidirectional temporal modelling with self-attention mechanisms.
DBEAN dynamically assigns weights to critical segments of input, improving
contextual representation while maintaining computational efficiency.
","[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:45:13 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 10:09:43 GMT'}]",2025-03-21,"[['Lai', 'ZhengLin', ''], ['Liao', 'MengYao', ''], ['Xu', 'Dong', '']]","[{'text': 'self-attention mechanisms', 'label': 'Attention mechanism'}]",Attention mechanism,self-attention mechanisms,0.8465653657913208
2503.15758,Venmugil Elango,Venmugil Elango,"ATTENTION2D: Communication Efficient Distributed Self-Attention
  Mechanism",,,,,cs.LG cs.AI cs.DC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transformer-based models have emerged as a leading architecture for natural
language processing, natural language generation, and image generation tasks. A
fundamental element of the transformer architecture is self-attention, which
allows the model to capture intricate dependencies within the data. However,
the self-attention mechanism also incurs significant computational and memory
costs, particularly for long sequences.
  In this paper, we introduce ATTENTION2D, a novel approach that exploits
parallelism along two dimensions - query and key/value - of the self-attention
operation. This method enables efficient distribution and parallelization of
computations across multiple devices. Our approach facilitates asymptotically
faster training and inference phases compared to previous methods, without
relying on approximations or incurring additional computational or memory
overheads. Furthermore, unlike existing techniques that struggle to scale with
an increasing number of processing units, our approach effectively scales with
additional processing units.
  Our experimental results confirm the effectiveness of our method in improving
communication efficiency and scalability. Compared to Ring Attention, our
approach demonstrated up to a 5x performance boost on a GPT-3-like model using
64 NVIDIA A100 GPUs across 16 nodes, and up to a 9.4x performance boost on 64
NVIDIA H100 GPUs across 64 nodes.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 00:25:44 GMT'}]",2025-03-21,"[['Elango', 'Venmugil', '']]","[{'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}]",Attention mechanism,self-attention,0.7317671179771423
2503.15815,Vishnu Dasu,"Vishnu Asutosh Dasu, Md Rafi ur Rashid, Vipul Gupta, Saeid
  Tizpaz-Niari, Gang Tan","Attention Pruning: Automated Fairness Repair of Language Models via
  Surrogate Simulated Annealing",,,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  This paper explores pruning attention heads as a post-processing bias
mitigation method for large language models (LLMs). Modern AI systems such as
LLMs are expanding into sensitive social contexts where fairness concerns
become especially crucial. Since LLMs develop decision-making patterns by
training on massive datasets of human-generated content, they naturally encode
and perpetuate societal biases. While modifying training datasets and
algorithms is expensive and requires significant resources; post-processing
techniques-such as selectively deactivating neurons and attention heads in
pre-trained LLMs-can provide feasible and effective approaches to improve
fairness. However, identifying the optimal subset of parameters to prune
presents a combinatorial challenge within LLMs' immense parameter space,
requiring solutions that efficiently balance competing objectives across the
frontiers of model fairness and utility.
  To address the computational challenges, we explore a search-based program
repair approach via randomized simulated annealing. Given the prohibitive
evaluation costs in billion-parameter LLMs, we develop surrogate deep neural
networks that efficiently model the relationship between attention head states
(active/inactive) and their corresponding fairness/utility metrics. This allows
us to perform optimization over the surrogate models and efficiently identify
optimal subsets of attention heads for selective pruning rather than directly
searching through the LLM parameter space. This paper introduces Attention
Pruning, a fairness-aware surrogate simulated annealing approach to prune
attention heads in LLMs that disproportionately contribute to bias while
minimally impacting overall model utility. Our experiments show that Attention
Pruning achieves up to $40\%$ reduction in gender bias and outperforms the
state-of-the-art bias mitigation strategies.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 03:02:32 GMT'}]",2025-03-21,"[['Dasu', 'Vishnu Asutosh', ''], ['Rashid', 'Md Rafi ur', ''], ['Gupta', 'Vipul', ''], ['Tizpaz-Niari', 'Saeid', ''], ['Tan', 'Gang', '']]","[{'text': 'attention heads', 'label': 'Attention mechanism'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'attention heads', 'label': 'Attention mechanism'}, {'text': 'LLMs-can', 'label': 'Large Language Model'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'attention heads', 'label': 'Attention mechanism'}, {'text': 'attention heads', 'label': 'Attention mechanism'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'gender bias', 'label': 'Attention mechanism'}]",Attention mechanism,attention heads,0.716526985168457
2503.15828,Houqi Su,"Xuhui Peng, Houqi Su","Ergodicity of the viscous scalar conservation laws with a degenerate
  noise",,,,,math.PR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper establishes the ergodicity in $H^\mathfrak n,\mathfrak
n=\lfloor\frac{d}{2}+1\rfloor$ of the viscous scalar conservation laws on torus
$\mathbb T^d$ with general polynomial flux and a degenerate noise. The noise
could appear in as few as several directions. We introduce a localized
framework that restricts attention to trajectories with controlled energy
growth, circumventing the limitations of traditional contraction-based
approaches. This localized method allows for a demonstration of e-property and
consequently proves the uniqueness of invariant measure under a
H{\""o}rmander-type condition. Furthermore, we characterize the absolute
continuity of the invariant measure's projections onto any finite-dimensional
subspaces under requirement on an algebraic nondegenerate condition for the
flux.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 03:50:58 GMT'}]",2025-03-21,"[['Peng', 'Xuhui', ''], ['Su', 'Houqi', '']]","[{'text': 'viscous scalar conservation laws', 'label': 'Scaling law'}, {'text': 'torus', 'label': 'Mistral'}, {'text': 'general polynomial flux', 'label': 'Scaling law'}, {'text': 'attention', 'label': 'Attention mechanism'}]",Attention mechanism,attention,0.7383304834365845
2503.15831,Zihao Zhang,"Zihao Zhang, Haoran Chen, Haoyu Zhao, Guansong Lu, Yanwei Fu, Hang Xu,
  Zuxuan Wu","EDEN: Enhanced Diffusion for High-quality Large-motion Video Frame
  Interpolation",CVPR2025,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Handling complex or nonlinear motion patterns has long posed challenges for
video frame interpolation. Although recent advances in diffusion-based methods
offer improvements over traditional optical flow-based approaches, they still
struggle to generate sharp, temporally consistent frames in scenarios with
large motion. To address this limitation, we introduce EDEN, an Enhanced
Diffusion for high-quality large-motion vidEo frame iNterpolation. Our approach
first utilizes a transformer-based tokenizer to produce refined latent
representations of the intermediate frames for diffusion models. We then
enhance the diffusion transformer with temporal attention across the process
and incorporate a start-end frame difference embedding to guide the generation
of dynamic motion. Extensive experiments demonstrate that EDEN achieves
state-of-the-art results across popular benchmarks, including nearly a 10%
LPIPS reduction on DAVIS and SNU-FILM, and an 8% improvement on DAIN-HD.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 03:54:52 GMT'}]",2025-03-21,"[['Zhang', 'Zihao', ''], ['Chen', 'Haoran', ''], ['Zhao', 'Haoyu', ''], ['Lu', 'Guansong', ''], ['Fu', 'Yanwei', ''], ['Xu', 'Hang', ''], ['Wu', 'Zuxuan', '']]","[{'text': 'temporal attention', 'label': 'Attention mechanism'}, {'text': 'start-end frame difference embedding', 'label': 'Embedding'}]",Attention mechanism,temporal attention,0.7036455869674683
2503.15926,Paolo Burelli,"Meisam J. Seikavandi, Maria J. Barrett and Paolo Burelli","Modeling Face Emotion Perception from Naturalistic Face Viewing:
  Insights from Fixational Events and Gaze Strategies",,,,,cs.HC,http://creativecommons.org/licenses/by/4.0/,"  Face Emotion Recognition (FER) is essential for social interactions and
understanding others' mental states. Utilizing eye tracking to investigate FER
has yielded insights into cognitive processes. In this study, we utilized an
instructionless paradigm to collect eye movement data from 21 participants,
examining two FER processes: free viewing and grounded FER. We analyzed
fixational, pupillary, and microsaccadic events from eye movements,
establishing their correlation with emotion perception and performance in the
grounded task. By identifying regions of interest on the face, we explored the
impact of eye-gaze strategies on face processing, their connection to emotions,
and performance in emotion perception. During free viewing, participants
displayed specific attention patterns for various emotions. In grounded tasks,
where emotions were interpreted based on words, we assessed performance and
contextual understanding. Notably, gaze patterns during free viewing predicted
success in grounded FER tasks, underscoring the significance of initial gaze
behavior. We also employed features from pre-trained deep-learning models for
face recognition to enhance the scalability and comparability of attention
analysis during free viewing across different datasets and populations. This
method facilitated the prediction and modeling of individual emotion perception
performance from minimal observations. Our findings advance the understanding
of the link between eye movements and emotion perception, with implications for
psychology, human-computer interaction, and affective computing, and pave the
way for developing precise emotion recognition systems.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:01:59 GMT'}]",2025-03-21,"[['Seikavandi', 'Meisam J.', ''], ['Barrett', 'Maria J.', ''], ['Burelli', 'Paolo', '']]","[{'text': 'free viewing', 'label': 'Attention mechanism'}, {'text': 'attention patterns', 'label': 'Attention mechanism'}, {'text': 'free viewing', 'label': 'Attention mechanism'}, {'text': 'free viewing', 'label': 'Attention mechanism'}]",Attention mechanism,attention patterns,0.8255490064620972
2503.15973,Zichen Liu,"Zichen Liu, Kunlun Xu, Bing Su, Xu Zou, Yuxin Peng, Jiahuan Zhou","STOP: Integrated Spatial-Temporal Dynamic Prompting for Video
  Understanding",,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Pre-trained on tremendous image-text pairs, vision-language models like CLIP
have demonstrated promising zero-shot generalization across numerous
image-based tasks. However, extending these capabilities to video tasks remains
challenging due to limited labeled video data and high training costs. Recent
video prompting methods attempt to adapt CLIP for video tasks by introducing
learnable prompts, but they typically rely on a single static prompt for all
video sequences, overlooking the diverse temporal dynamics and spatial
variations that exist across frames. This limitation significantly hinders the
model's ability to capture essential temporal information for effective video
understanding. To address this, we propose an integrated Spatial-TempOral
dynamic Prompting (STOP) model which consists of two complementary modules, the
intra-frame spatial prompting and inter-frame temporal prompting. Our
intra-frame spatial prompts are designed to adaptively highlight discriminative
regions within each frame by leveraging intra-frame attention and temporal
variation, allowing the model to focus on areas with substantial temporal
dynamics and capture fine-grained spatial details. Additionally, to highlight
the varying importance of frames for video understanding, we further introduce
inter-frame temporal prompts, dynamically inserting prompts between frames with
high temporal variance as measured by frame similarity. This enables the model
to prioritize key frames and enhances its capacity to understand temporal
dependencies across sequences. Extensive experiments on various video
benchmarks demonstrate that STOP consistently achieves superior performance
against state-of-the-art methods. The code is available at
https://github.com/zhoujiahuan1991/CVPR2025-STOP.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 09:16:20 GMT'}]",2025-03-21,"[['Liu', 'Zichen', ''], ['Xu', 'Kunlun', ''], ['Su', 'Bing', ''], ['Zou', 'Xu', ''], ['Peng', 'Yuxin', ''], ['Zhou', 'Jiahuan', '']]","[{'text': 'intra-frame spatial prompting', 'label': 'Prompting'}, {'text': 'inter-frame temporal prompting', 'label': 'Prompting'}, {'text': 'intra-frame spatial prompts', 'label': 'Prompting'}, {'text': 'intra-frame attention', 'label': 'Attention mechanism'}, {'text': 'inter-frame temporal prompts', 'label': 'Prompting'}]",Attention mechanism,intra-frame attention,0.6550148129463196
2503.16036,Zhihang Liu,"Zhihang Liu and Chen-Wei Xie and Pandeng Li and Liming Zhao and
  Longxiang Tang and Yun Zheng and Chuanbin Liu and Hongtao Xie","Hybrid-Level Instruction Injection for Video Token Compression in
  Multi-modal Large Language Models",Accepted to CVPR2025,,,,cs.CV cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent Multi-modal Large Language Models (MLLMs) have been challenged by the
computational overhead resulting from massive video frames, often alleviated
through compression strategies. However, the visual content is not equally
contributed to user instructions, existing strategies (\eg, average pool)
inevitably lead to the loss of potentially useful information. To tackle this,
we propose the Hybrid-level Instruction Injection Strategy for Conditional
Token Compression in MLLMs (HICom), utilizing the instruction as a condition to
guide the compression from both local and global levels. This encourages the
compression to retain the maximum amount of user-focused information while
reducing visual tokens to minimize computational burden. Specifically, the
instruction condition is injected into the grouped visual tokens at the local
level and the learnable tokens at the global level, and we conduct the
attention mechanism to complete the conditional compression. From the
hybrid-level compression, the instruction-relevant visual parts are highlighted
while the temporal-spatial structure is also preserved for easier understanding
of LLMs. To further unleash the potential of HICom, we introduce a new
conditional pre-training stage with our proposed dataset HICom-248K.
Experiments show that our HICom can obtain distinguished video understanding
ability with fewer tokens, increasing the performance by 2.43\% average on
three multiple-choice QA benchmarks and saving 78.8\% tokens compared with the
SOTA method. The code is available at https://github.com/lntzm/HICom.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 11:09:18 GMT'}]",2025-03-21,"[['Liu', 'Zhihang', ''], ['Xie', 'Chen-Wei', ''], ['Li', 'Pandeng', ''], ['Zhao', 'Liming', ''], ['Tang', 'Longxiang', ''], ['Zheng', 'Yun', ''], ['Liu', 'Chuanbin', ''], ['Xie', 'Hongtao', '']]","[{'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'attention mechanism', 'label': 'Attention mechanism'}]",Attention mechanism,attention mechanism,1.0
2503.16047,Akinyemi Sadeeq Akintola,"Bisola Faith Kayode, Akinyemi Sadeeq Akintola, Oluwole Fagbohun,
  Egonna Anaesiuba-Bristol, Onyekachukwu Ojumah, Oluwagbade Odimayo, Toyese
  Oloyede, Aniema Inyang, Teslim Kazeem, Habeeb Alli, Udodirim Ibem Offia,
  Prisca Chinazor Amajuoyi","Temporal-Spatial Attention Network (TSAN) for DoS Attack Detection in
  Network Traffic","19 Pages, 5 figures",,,,cs.CR cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Denial-of-Service (DoS) attacks remain a critical threat to network security,
disrupting services and causing significant economic losses. Traditional
detection methods, including statistical and rule-based models, struggle to
adapt to evolving attack patterns. To address this challenge, we propose a
novel Temporal-Spatial Attention Network (TSAN) architecture for detecting
Denial of Service (DoS) attacks in network traffic. By leveraging both temporal
and spatial features of network traffic, our approach captures complex traffic
patterns and anomalies that traditional methods might miss. The TSAN model
incorporates transformer-based temporal encoding, convolutional spatial
encoding, and a cross-attention mechanism to fuse these complementary feature
spaces. Additionally, we employ multi-task learning with auxiliary tasks to
enhance the model's robustness. Experimental results on the NSL-KDD dataset
demonstrate that TSAN outperforms state-of-the-art models, achieving superior
accuracy, precision, recall, and F1-score while maintaining computational
efficiency for real-time deployment. The proposed architecture offers an
optimal balance between detection accuracy and computational overhead, making
it highly suitable for real-world network security applications.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 11:31:45 GMT'}]",2025-03-21,"[['Kayode', 'Bisola Faith', ''], ['Akintola', 'Akinyemi Sadeeq', ''], ['Fagbohun', 'Oluwole', ''], ['Anaesiuba-Bristol', 'Egonna', ''], ['Ojumah', 'Onyekachukwu', ''], ['Odimayo', 'Oluwagbade', ''], ['Oloyede', 'Toyese', ''], ['Inyang', 'Aniema', ''], ['Kazeem', 'Teslim', ''], ['Alli', 'Habeeb', ''], ['Offia', 'Udodirim Ibem', ''], ['Amajuoyi', 'Prisca Chinazor', '']]","[{'text': 'convolutional spatial\nencoding', 'label': 'Attention mechanism'}, {'text': 'cross-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'multi-task learning', 'label': 'Few-shot Learning'}]",Attention mechanism,cross-attention mechanism,0.8302809000015259
2503.16048,Michael Goodale,"Michael Goodale, Salvador Mascarenhas and Yair Lakretz",Meta-Learning Neural Mechanisms rather than Bayesian Priors,,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Children acquire language despite being exposed to several orders of
magnitude less data than large language models require. Meta-learning has been
proposed as a way to integrate human-like learning biases into neural-network
architectures, combining both the structured generalizations of symbolic models
with the scalability of neural-network models. But what does meta-learning
exactly imbue the model with? We investigate the meta-learning of formal
languages and find that, contrary to previous claims, meta-trained models are
not learning simplicity-based priors when meta-trained on datasets organised
around simplicity. Rather, we find evidence that meta-training imprints neural
mechanisms (such as counters) into the model, which function like cognitive
primitives for the network on downstream tasks. Most surprisingly, we find that
meta-training on a single formal language can provide as much improvement to a
model as meta-training on 5000 different formal languages, provided that the
formal language incentivizes the learning of useful neural mechanisms. Taken
together, our findings provide practical implications for efficient
meta-learning paradigms and new theoretical insights into linking symbolic
theories and neural mechanisms.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 11:33:59 GMT'}]",2025-03-21,"[['Goodale', 'Michael', ''], ['Mascarenhas', 'Salvador', ''], ['Lakretz', 'Yair', '']]","[{'text': 'Meta-learning', 'label': 'Few-shot Learning'}, {'text': 'meta-learning', 'label': 'Few-shot Learning'}, {'text': 'neural\nmechanisms', 'label': 'Attention mechanism'}, {'text': 'meta-training', 'label': 'Few-shot Learning'}, {'text': 'meta-training', 'label': 'Few-shot Learning'}, {'text': 'neural mechanisms', 'label': 'Attention mechanism'}, {'text': 'neural mechanisms', 'label': 'Attention mechanism'}]",Attention mechanism,"neural
mechanisms",0.5939817428588867
2503.16056,Yunzhe Zhang,"Wanshu Fan, Yue Wang, Cong Wang, Yunzhe Zhang, Wei Wang and Dongsheng
  Zhou","Semantic-Guided Global-Local Collaborative Networks for Lightweight
  Image Super-Resolution","14 pages,13 figures, 9 tables",Ieee Transactions on Instrument and Measurement 2025,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Single-Image Super-Resolution (SISR) plays a pivotal role in enhancing the
accuracy and reliability of measurement systems, which are integral to various
vision-based instrumentation and measurement applications. These systems often
require clear and detailed images for precise object detection and recognition.
However, images captured by visual measurement tools frequently suffer from
degradation, including blurring and loss of detail, which can impede
measurement accuracy.As a potential remedy, we in this paper propose a
Semantic-Guided Global-Local Collaborative Network (SGGLC-Net) for lightweight
SISR. Our SGGLC-Net leverages semantic priors extracted from a pre-trained
model to guide the super-resolution process, enhancing image detail quality
effectively. Specifically,we propose a Semantic Guidance Module that seamlessly
integrates the semantic priors into the super-resolution network, enabling the
network to more adeptly capture and utilize semantic priors, thereby enhancing
image details. To further explore both local and non-local interactions for
improved detail rendition,we propose a Global-Local Collaborative Module, which
features three Global and Local Detail Enhancement Modules, as well as a Hybrid
Attention Mechanism to work together to efficiently learn more useful features.
Our extensive experiments show that SGGLC-Net achieves competitive PSNR and
SSIM values across multiple benchmark datasets, demonstrating higher
performance with the multi-adds reduction of 12.81G compared to
state-of-the-art lightweight super-resolution approaches. These improvements
underscore the potential of our approach to enhance the precision and
effectiveness of visual measurement systems. Codes are at
https://github.com/fanamber831/SGGLC-Net.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 11:43:55 GMT'}]",2025-03-21,"[['Fan', 'Wanshu', ''], ['Wang', 'Yue', ''], ['Wang', 'Cong', ''], ['Zhang', 'Yunzhe', ''], ['Wang', 'Wei', ''], ['Zhou', 'Dongsheng', '']]","[{'text': 'Hybrid\nAttention Mechanism', 'label': 'Attention mechanism'}]",Attention mechanism,"Hybrid
Attention Mechanism",0.8765320777893066
2503.16065,Yingmao Miao,"Yingmao Miao, Zhanpeng Huang, Rui Han, Zibin Wang, Chenhao Lin, Chao
  Shen","Shining Yourself: High-Fidelity Ornaments Virtual Try-on with Diffusion
  Model",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  While virtual try-on for clothes and shoes with diffusion models has gained
attraction, virtual try-on for ornaments, such as bracelets, rings, earrings,
and necklaces, remains largely unexplored. Due to the intricate tiny patterns
and repeated geometric sub-structures in most ornaments, it is much more
difficult to guarantee identity and appearance consistency under large pose and
scale variances between ornaments and models. This paper proposes the task of
virtual try-on for ornaments and presents a method to improve the geometric and
appearance preservation of ornament virtual try-ons. Specifically, we estimate
an accurate wearing mask to improve the alignments between ornaments and models
in an iterative scheme alongside the denoising process. To preserve structure
details, we further regularize attention layers to map the reference ornament
mask to the wearing mask in an implicit way. Experimental results demonstrate
that our method successfully wears ornaments from reference images onto target
models, handling substantial differences in scale and pose while preserving
identity and achieving realistic visual effects.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 11:57:32 GMT'}]",2025-03-21,"[['Miao', 'Yingmao', ''], ['Huang', 'Zhanpeng', ''], ['Han', 'Rui', ''], ['Wang', 'Zibin', ''], ['Lin', 'Chenhao', ''], ['Shen', 'Chao', '']]","[{'text': 'attention layers', 'label': 'Attention mechanism'}]",Attention mechanism,attention layers,0.706462025642395
2503.16067,Tim Seizinger,"Tim Seizinger, Florin-Alexandru Vasluianu, Marcos V. Conde, Radu
  Timofte",Bokehlicious: Photorealistic Bokeh Rendering with Controllable Apertures,Technical Report,,,,cs.CV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Bokeh rendering methods play a key role in creating the visually appealing,
softly blurred backgrounds seen in professional photography. While recent
learning-based approaches show promising results, generating realistic Bokeh
with variable strength remains challenging. Existing methods require additional
inputs and suffer from unrealistic Bokeh reproduction due to reliance on
synthetic data. In this work, we propose Bokehlicious, a highly efficient
network that provides intuitive control over Bokeh strength through an
Aperture-Aware Attention mechanism, mimicking the physical lens aperture. To
further address the lack of high-quality real-world data, we present RealBokeh,
a novel dataset featuring 23,000 high-resolution (24-MP) images captured by
professional photographers, covering diverse scenes with varied aperture and
focal length settings. Evaluations on both our new RealBokeh and established
Bokeh rendering benchmarks show that Bokehlicious consistently outperforms SOTA
methods while significantly reducing computational cost and exhibiting strong
zero-shot generalization. Our method and dataset further extend to defocus
deblurring, achieving competitive results on the RealDOF benchmark. Our code
and data can be found at https://github.com/TimSeizinger/Bokehlicious
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 12:00:45 GMT'}]",2025-03-21,"[['Seizinger', 'Tim', ''], ['Vasluianu', 'Florin-Alexandru', ''], ['Conde', 'Marcos V.', ''], ['Timofte', 'Radu', '']]","[{'text': 'Aperture-Aware Attention mechanism', 'label': 'Attention mechanism'}]",Attention mechanism,Aperture-Aware Attention mechanism,0.7342512607574463
2503.16069,Aniek Eijpe,"Aniek Eijpe, Soufyan Lakbir, Melis Erdal Cesur, Sara P. Oliveira,
  Sanne Abeln and Wilson Silva","Disentangled and Interpretable Multimodal Attention Fusion for Cancer
  Survival Prediction","11 pages, 1 figure, 3 tables",,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  To improve the prediction of cancer survival using whole-slide images and
transcriptomics data, it is crucial to capture both modality-shared and
modality-specific information. However, multimodal frameworks often entangle
these representations, limiting interpretability and potentially suppressing
discriminative features. To address this, we propose Disentangled and
Interpretable Multimodal Attention Fusion (DIMAF), a multimodal framework that
separates the intra- and inter-modal interactions within an attention-based
fusion mechanism to learn distinct modality-specific and modality-shared
representations. We introduce a loss based on Distance Correlation to promote
disentanglement between these representations and integrate Shapley additive
explanations to assess their relative contributions to survival prediction. We
evaluate DIMAF on four public cancer survival datasets, achieving a relative
average improvement of 1.85% in performance and 23.7% in disentanglement
compared to current state-of-the-art multimodal models. Beyond improved
performance, our interpretable framework enables a deeper exploration of the
underlying interactions between and within modalities in cancer biology.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 12:02:10 GMT'}]",2025-03-21,"[['Eijpe', 'Aniek', ''], ['Lakbir', 'Soufyan', ''], ['Cesur', 'Melis Erdal', ''], ['Oliveira', 'Sara P.', ''], ['Abeln', 'Sanne', ''], ['Silva', 'Wilson', '']]","[{'text': 'attention-based\nfusion mechanism', 'label': 'Attention mechanism'}]",Attention mechanism,"attention-based
fusion mechanism",0.7242620587348938
2503.16149,Dong Chen,"Dong Chen, Boyue Zhao, Yi Zhang, Meng Zhao","Selective Complementary Feature Fusion and Modal Feature Compression
  Interaction for Brain Tumor Segmentation",,,,,eess.IV cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Efficient modal feature fusion strategy is the key to achieve accurate
segmentation of brain glioma. However, due to the specificity of different MRI
modes, it is difficult to carry out cross-modal fusion with large differences
in modal features, resulting in the model ignoring rich feature information. On
the other hand, the problem of multi-modal feature redundancy interaction
occurs in parallel networks due to the proliferation of feature dimensions,
further increase the difficulty of multi-modal feature fusion at the bottom
end. In order to solve the above problems, we propose a noval complementary
feature compression interaction network (CFCI-Net), which realizes the
complementary fusion and compression interaction of multi-modal feature
information with an efficient mode fusion strategy. Firstly, we propose a
selective complementary feature fusion (SCFF) module, which adaptively fuses
rich cross-modal feature information by complementary soft selection weights.
Secondly, a modal feature compression interaction (MFCI) transformer is
proposed to deal with the multi-mode fusion redundancy problem when the feature
dimension surges. The MFCI transformer is composed of modal feature compression
(MFC) and modal feature interaction (MFI) to realize redundancy feature
compression and multi-mode feature interactive learning. %In MFI, we propose a
hierarchical interactive attention mechanism based on multi-head attention.
Evaluations on the BraTS2019 and BraTS2020 datasets demonstrate that CFCI-Net
achieves superior results compared to state-of-the-art models. Code:
https://github.com/CDmm0/CFCI-Net
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:52:51 GMT'}]",2025-03-21,"[['Chen', 'Dong', ''], ['Zhao', 'Boyue', ''], ['Zhang', 'Yi', ''], ['Zhao', 'Meng', '']]","[{'text': 'multi-mode feature interactive learning', 'label': 'Few-shot Learning'}, {'text': 'MFI', 'label': 'Attention mechanism'}, {'text': 'hierarchical interactive attention mechanism', 'label': 'Attention mechanism'}, {'text': 'multi-head attention', 'label': 'Attention mechanism'}]",Attention mechanism,hierarchical interactive attention mechanism,0.8149070739746094
2503.16284,Sharon Peled,"Sharon Peled, Yosef E. Maruvka, Moti Freiman","PSA-MIL: A Probabilistic Spatial Attention-Based Multiple Instance
  Learning for Whole Slide Image Classification","8 pages, 7 figures",,,,cs.CV,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Whole Slide Images (WSIs) are high-resolution digital scans widely used in
medical diagnostics. WSI classification is typically approached using Multiple
Instance Learning (MIL), where the slide is partitioned into tiles treated as
interconnected instances. While attention-based MIL methods aim to identify the
most informative tiles, they often fail to fully exploit the spatial
relationships among them, potentially overlooking intricate tissue structures
crucial for accurate diagnosis. To address this limitation, we propose
Probabilistic Spatial Attention MIL (PSA-MIL), a novel attention-based MIL
framework that integrates spatial context into the attention mechanism through
learnable distance-decayed priors, formulated within a probabilistic
interpretation of self-attention as a posterior distribution. This formulation
enables a dynamic inference of spatial relationships during training,
eliminating the need for predefined assumptions often imposed by previous
approaches. Additionally, we suggest a spatial pruning strategy for the
posterior, effectively reducing self-attention's quadratic complexity. To
further enhance spatial modeling, we introduce a diversity loss that encourages
variation among attention heads, ensuring each captures distinct spatial
representations. Together, PSA-MIL enables a more data-driven and adaptive
integration of spatial context, moving beyond predefined constraints. We
achieve state-of-the-art performance across both contextual and non-contextual
baselines, while significantly reducing computational costs.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 16:12:42 GMT'}]",2025-03-21,"[['Peled', 'Sharon', ''], ['Maruvka', 'Yosef E.', ''], ['Freiman', 'Moti', '']]","[{'text': 'Multiple\nInstance Learning (MIL)', 'label': 'Few-shot Learning'}, {'text': 'attention mechanism', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'spatial context', 'label': 'Attention mechanism'}]",Attention mechanism,attention mechanism,1.0
2503.16389,Kristin Qi,"Kristin Qi, Xinhan Di","Attentional Triple-Encoder Network in Spatiospectral Domains for Medical
  Image Segmentation",IEEE Conference on Artificial Intelligence (IEEE CAI),,,,eess.IV cs.AI cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Retinal Optical Coherence Tomography (OCT) segmentation is essential for
diagnosing pathology. Traditional methods focus on either spatial or spectral
domains, overlooking their combined dependencies. We propose a triple-encoder
network that integrates CNNs for spatial features, Fast Fourier Convolution
(FFC) for spectral features, and attention mechanisms to capture global
relationships across both domains. Attention fusion modules integrate
convolution and cross-attention to further enhance features. Our method
achieves an average Dice score improvement from 0.855 to 0.864, outperforming
prior work.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:49:01 GMT'}]",2025-03-21,"[['Qi', 'Kristin', ''], ['Di', 'Xinhan', '']]","[{'text': 'attention mechanisms', 'label': 'Attention mechanism'}]",Attention mechanism,attention mechanisms,0.9558142423629761
2503.16396,Chun-Han Yao,"Chun-Han Yao, Yiming Xie, Vikram Voleti, Huaizu Jiang, Varun Jampani","SV4D 2.0: Enhancing Spatio-Temporal Consistency in Multi-View Video
  Diffusion for High-Quality 4D Generation",,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  We present Stable Video 4D 2.0 (SV4D 2.0), a multi-view video diffusion model
for dynamic 3D asset generation. Compared to its predecessor SV4D, SV4D 2.0 is
more robust to occlusions and large motion, generalizes better to real-world
videos, and produces higher-quality outputs in terms of detail sharpness and
spatio-temporal consistency. We achieve this by introducing key improvements in
multiple aspects: 1) network architecture: eliminating the dependency of
reference multi-views and designing blending mechanism for 3D and frame
attention, 2) data: enhancing quality and quantity of training data, 3)
training strategy: adopting progressive 3D-4D training for better
generalization, and 4) 4D optimization: handling 3D inconsistency and large
motion via 2-stage refinement and progressive frame sampling. Extensive
experiments demonstrate significant performance gain by SV4D 2.0 both visually
and quantitatively, achieving better detail (-14\% LPIPS) and 4D consistency
(-44\% FV4D) in novel-view video synthesis and 4D optimization (-12\% LPIPS and
-24\% FV4D) compared to SV4D. Project page: https://sv4d2.0.github.io.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:53:38 GMT'}]",2025-03-21,"[['Yao', 'Chun-Han', ''], ['Xie', 'Yiming', ''], ['Voleti', 'Vikram', ''], ['Jiang', 'Huaizu', ''], ['Jampani', 'Varun', '']]","[{'text': 'blending mechanism', 'label': 'Attention mechanism'}, {'text': '3D and frame\nattention', 'label': 'Attention mechanism'}]",Attention mechanism,"3D and frame
attention",0.5626410245895386
2503.16413,Xueyan Zou,"Xueyan Zou, Yuchen Song, Ri-Zhao Qiu, Xuanbin Peng, Jianglong Ye,
  Sifei Liu, Xiaolong Wang",M3: 3D-Spatial MultiModal Memory,"ICLR2025 homepage: https://m3-spatial-memory.github.io code:
  https://github.com/MaureenZOU/m3-spatial",,,,cs.CV cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present 3D Spatial MultiModal Memory (M3), a multimodal memory system
designed to retain information about medium-sized static scenes through video
sources for visual perception. By integrating 3D Gaussian Splatting techniques
with foundation models, M3 builds a multimodal memory capable of rendering
feature representations across granularities, encompassing a wide range of
knowledge. In our exploration, we identify two key challenges in previous works
on feature splatting: (1) computational constraints in storing high-dimensional
features for each Gaussian primitive, and (2) misalignment or information loss
between distilled features and foundation model features. To address these
challenges, we propose M3 with key components of principal scene components and
Gaussian memory attention, enabling efficient training and inference. To
validate M3, we conduct comprehensive quantitative evaluations of feature
similarity and downstream tasks, as well as qualitative visualizations to
highlight the pixel trace of Gaussian memory attention. Our approach
encompasses a diverse range of foundation models, including vision-language
models (VLMs), perception models, and large multimodal and language models
(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy
M3's feature field in indoor scenes on a quadruped robot. Notably, we claim
that M3 is the first work to address the core compression challenges in 3D
feature distillation.
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:59:12 GMT'}]",2025-03-21,"[['Zou', 'Xueyan', ''], ['Song', 'Yuchen', ''], ['Qiu', 'Ri-Zhao', ''], ['Peng', 'Xuanbin', ''], ['Ye', 'Jianglong', ''], ['Liu', 'Sifei', ''], ['Wang', 'Xiaolong', '']]","[{'text': 'M3', 'label': 'Foundation Model'}, {'text': 'Gaussian memory attention', 'label': 'Attention mechanism'}, {'text': 'Gaussian memory attention', 'label': 'Attention mechanism'}, {'text': 'perception models', 'label': 'Foundation Model'}, {'text': 'M3', 'label': 'Large Language Model'}, {'text': '3D\nfeature distillation', 'label': 'Knowledge distillation'}]",Attention mechanism,Gaussian memory attention,0.6678389310836792
2503.16426,Keyan Chen,"Keyan Chen, Chenyang Liu, Bowen Chen, Wenyuan Li, Zhengxia Zou,
  Zhenwei Shi","DynamicVis: An Efficient and General Visual Foundation Model for Remote
  Sensing Image Understanding",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The advancement of remote sensing technology has improved the spatial
resolution of satellite imagery, facilitating more detailed visual
representations for diverse interpretations. However, existing methods exhibit
limited generalization capabilities across varied applications. While some
contemporary foundation models demonstrate potential, they are hindered by
insufficient cross-task adaptability and primarily process low-resolution
imagery of restricted sizes, thus failing to fully exploit high-resolution data
or leverage comprehensive large-scene semantics. Crucially, remote sensing
imagery differs fundamentally from natural images, as key foreground targets
(eg., maritime objects, artificial structures) often occupy minimal spatial
proportions (~1%) and exhibit sparse distributions. Efficiently modeling
cross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a
significant challenge yet remains critical for remote sensing image
understanding. Motivated by the selective attention mechanisms inherent to the
human visual system, we propose DynamicVis, a dynamic visual perception
foundation model for remote sensing imagery. The framework integrates a novel
dynamic region perception backbone based on the selective state space model,
which strategically balances localized detail extraction with global contextual
integration, enabling computationally efficient encoding of large-scale data
while maintaining architectural scalability. To enhance cross-task knowledge
transferring, we introduce a multi-instance learning paradigm utilizing
meta-embedding representations, trained on million-scale region-level
annotations. Evaluations across nine downstream tasks demonstrate the model's
versatility. DynamicVis achieves multi-level feature modeling with exceptional
efficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and
833 MB GPU memory (3% of ViT's).
","[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:59:54 GMT'}]",2025-03-21,"[['Chen', 'Keyan', ''], ['Liu', 'Chenyang', ''], ['Chen', 'Bowen', ''], ['Li', 'Wenyuan', ''], ['Zou', 'Zhengxia', ''], ['Shi', 'Zhenwei', '']]","[{'text': 'selective attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'DynamicVis', 'label': 'Foundation Model'}, {'text': 'global contextual\nintegration', 'label': 'contextual Embedding'}, {'text': 'architectural scalability', 'label': 'Scaling law'}, {'text': 'meta-embedding representations', 'label': 'contextual Embedding'}, {'text': 'DynamicVis', 'label': 'Foundation Model'}]",Attention mechanism,selective attention mechanisms,0.9153326749801636
