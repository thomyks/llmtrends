{"id":2501.1036,"submitter":"Kartik Narayan","authors":"Kartik Narayan, Vibashan VS, Vishal M. Patel","title":"FaceXBench: Evaluating Multimodal LLMs on Face Understanding","comments":"Project Page: https:\/\/kartik-3004.github.io\/facexbench\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multimodal Large Language Models (MLLMs) demonstrate impressive\nproblem-solving abilities across a wide range of tasks and domains. However,\ntheir capacity for face understanding has not been systematically studied. To\naddress this gap, we introduce FaceXBench, a comprehensive benchmark designed\nto evaluate MLLMs on complex face understanding tasks. FaceXBench includes\n5,000 multimodal multiple-choice questions derived from 25 public datasets and\na newly created dataset, FaceXAPI. These questions cover 14 tasks across 6\nbroad categories, assessing MLLMs' face understanding abilities in bias and\nfairness, face authentication, recognition, analysis, localization and tool\nretrieval. Using FaceXBench, we conduct an extensive evaluation of 26\nopen-source MLLMs alongside 2 proprietary models, revealing the unique\nchallenges in complex face understanding tasks. We analyze the models across\nthree evaluation settings: zero-shot, in-context task description, and\nchain-of-thought prompting. Our detailed analysis reveals that current MLLMs,\nincluding advanced models like GPT-4o, and GeminiPro 1.5, show significant room\nfor improvement. We believe FaceXBench will be a crucial resource for\ndeveloping MLLMs equipped to perform sophisticated face understanding. Code:\nhttps:\/\/github.com\/Kartik-3004\/facexbench\n","versions":"[{'version': 'v1', 'created': 'Fri, 17 Jan 2025 18:59:55 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 18:19:52 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Narayan', 'Kartik', ''], ['VS', 'Vibashan', ''], ['Patel', 'Vishal M.', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'bias and\\nfairness', 'label': 'Model Bias and Fairness'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'zero-shot', 'label': 'Zero-shot Learning'}, {'text': 'chain-of-thought prompting', 'label': 'Prompting'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'GPT-4o', 'label': 'GPT-4'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"GPT-4","matched_keyword":"GPT-4o","similarity_score":0.9017629027}
{"id":2305.08982,"submitter":"Shang-Ling Hsu","authors":"Shang-Ling Hsu, Raj Sanjay Shah, Prathik Senthil, Zahra Ashktorab,\n  Casey Dugan, Werner Geyer, Diyi Yang","title":"Helping the Helper: Supporting Peer Counselors via AI-Empowered Practice\n  and Feedback","comments":"45 pages, 14 figures, CSCW 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Millions of users come to online peer counseling platforms to seek support.\nHowever, studies show that online peer support groups are not always as\neffective as expected, largely due to users' negative experiences with\nunhelpful counselors. Peer counselors are key to the success of online peer\ncounseling platforms, but most often do not receive appropriate training.Hence,\nwe introduce CARE: an AI-based tool to empower and train peer counselors\nthrough practice and feedback. Concretely, CARE helps diagnose which counseling\nstrategies are needed in a given situation and suggests example responses to\ncounselors during their practice sessions. Building upon the Motivational\nInterviewing framework, CARE utilizes large-scale counseling conversation data\nwith text generation techniques to enable these functionalities. We demonstrate\nthe efficacy of CARE by performing quantitative evaluations and qualitative\nuser studies through simulated chats and semi-structured interviews, finding\nthat CARE especially helps novice counselors in challenging situations. The\ncode is available at https:\/\/github.com\/SALT-NLP\/CARE\n","versions":"[{'version': 'v1', 'created': 'Mon, 15 May 2023 19:48:59 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 20:18:39 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Hsu', 'Shang-Ling', ''], ['Shah', 'Raj Sanjay', ''], ['Senthil', 'Prathik', ''], ['Ashktorab', 'Zahra', ''], ['Dugan', 'Casey', ''], ['Geyer', 'Werner', ''], ['Yang', 'Diyi', '']]","extracted_entities":"[{'text': 'simulated chats', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"simulated chats","similarity_score":0.5215293169}
{"id":2401.17477,"submitter":"Fabrizio Marozzo","authors":"Loris Belcastro, Riccardo Cantini, Fabrizio Marozzo, Domenico Talia,\n  Paolo Trunfio","title":"Detecting mental disorder on social media: a ChatGPT-augmented\n  explainable approach","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG cs.SI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In the digital era, the prevalence of depressive symptoms expressed on social\nmedia has raised serious concerns, necessitating advanced methodologies for\ntimely detection. This paper addresses the challenge of interpretable\ndepression detection by proposing a novel methodology that effectively combines\nLarge Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and\nconversational agents like ChatGPT. In our methodology, explanations are\nachieved by integrating BERTweet, a Twitter-specific variant of BERT, into a\nnovel self-explanatory model, namely BERT-XDD, capable of providing both\nclassification and explanations via masked attention. The interpretability is\nfurther enhanced using ChatGPT to transform technical explanations into\nhuman-readable commentaries. By introducing an effective and modular approach\nfor interpretable depression detection, our methodology can contribute to the\ndevelopment of socially responsible digital platforms, fostering early\nintervention and support for mental health challenges under the guidance of\nqualified healthcare professionals.\n","versions":"[{'version': 'v1', 'created': 'Tue, 30 Jan 2024 22:22:55 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 09:32:00 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Belcastro', 'Loris', ''], ['Cantini', 'Riccardo', ''], ['Marozzo', 'Fabrizio', ''], ['Talia', 'Domenico', ''], ['Trunfio', 'Paolo', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'BERT', 'label': 'BERT'}, {'text': 'masked attention', 'label': 'Attention mechanism'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'socially responsible digital platforms', 'label': 'AI Ethics'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2407.12261,"submitter":"Yang Cheng","authors":"Yang Cheng, Qingyuan Shu, Albert Lee, Haoran He, Ivy Zhu, Minzhang\n  Chen, Renhe Chen, Zirui Wang, Hantao Zhang, Chih-Yao Wang, Shan-Yi Yang,\n  Yu-Chen Hsin, Cheng-Yi Shih, Hsin-Han Lee, Ran Cheng, and Kang L. Wang","title":"Voltage-Controlled Magnetoelectric Devices for Neuromorphic Diffusion\n  Process","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.NE cs.ET cs.LG physics.app-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Stochastic diffusion processes are pervasive in nature, from the seemingly\nerratic Brownian motion to the complex interactions of synaptically-coupled\nspiking neurons. Recently, drawing inspiration from Langevin dynamics,\nneuromorphic diffusion models were proposed and have become one of the major\nbreakthroughs in the field of generative artificial intelligence. Unlike\ndiscriminative models that have been well developed to tackle classification or\nregression tasks, diffusion models as well as other generative models such as\nChatGPT aim at creating content based upon contexts learned. However, the more\ncomplex algorithms of these models result in high computational costs using\ntoday's technologies, creating a bottleneck in their efficiency, and impeding\nfurther development. Here, we develop a spintronic voltage-controlled\nmagnetoelectric memory hardware for the neuromorphic diffusion process. The\nin-memory computing capability of our spintronic devices goes beyond current\nVon Neumann architecture, where memory and computing units are separated.\nTogether with the non-volatility of magnetic memory, we can achieve high-speed\nand low-cost computing, which is desirable for the increasing scale of\ngenerative models in the current era. We experimentally demonstrate that the\nhardware-based true random diffusion process can be implemented for image\ngeneration and achieve comparable image quality to software-based training as\nmeasured by the Frechet inception distance (FID) score, achieving ~10^3 better\nenergy-per-bit-per-area over traditional hardware.\n","versions":"[{'version': 'v1', 'created': 'Wed, 17 Jul 2024 02:14:22 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 19:56:34 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Cheng', 'Yang', ''], ['Shu', 'Qingyuan', ''], ['Lee', 'Albert', ''], ['He', 'Haoran', ''], ['Zhu', 'Ivy', ''], ['Chen', 'Minzhang', ''], ['Chen', 'Renhe', ''], ['Wang', 'Zirui', ''], ['Zhang', 'Hantao', ''], ['Wang', 'Chih-Yao', ''], ['Yang', 'Shan-Yi', ''], ['Hsin', 'Yu-Chen', ''], ['Shih', 'Cheng-Yi', ''], ['Lee', 'Hsin-Han', ''], ['Cheng', 'Ran', ''], ['Wang', 'Kang L.', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2410.05628,"submitter":"Jeongeun Park","authors":"Jeongeun Park, Sungjoon Choi, Sangdoo Yun","title":"A Unified Framework for Motion Reasoning and Generation in Human\n  Interaction","comments":"https:\/\/vim-motion-language.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural and contextually relevant text,\nenabling more human-like AI interactions. However, generating and understanding\ninteractive human-like motion, where multiple individuals engage in coordinated\nmovements, remains challenging due to the complexity of modeling these\ninteractions. Additionally, a unified and versatile model is needed to handle\ndiverse interactive scenarios, such as chat systems that dynamically adapt to\nuser instructions and assigned roles. To address these challenges, we introduce\nVIM, the Versatile Interactive Motion-language model, which integrates both\nlanguage and motion modalities to effectively understand, generate, and control\ninteractive motions in multi-turn conversational contexts. Unlike previous\nstudies that primarily focus on uni-directional tasks such as text-to-motion or\nmotion-to-text, VIM employs a unified architecture capable of simultaneously\nunderstanding and generating both motion and text modalities. Given the absence\nof an appropriate dataset to support this task, we introduce Inter-MT2, a\nlarge-scale instruction-tuning dataset containing 82.7K multi-turn interactive\nmotion instructions, covering 153K interactive motion samples. Inter-MT2 spans\ndiverse instructional scenarios, including motion editing, question answering,\nand story generation, leveraging off-the-shelf large language models and motion\ndiffusion models to construct a broad set of interactive motion instructions.\nWe extensively evaluate the versatility of VIM across multiple interactive\nmotion-related tasks, including motion-to-text, text-to-motion, reaction\ngeneration, motion editing, and reasoning about motion sequences.\n","versions":"[{'version': 'v1', 'created': 'Tue, 8 Oct 2024 02:23:53 GMT'}, {'version': 'v2', 'created': 'Mon, 14 Oct 2024 11:22:39 GMT'}, {'version': 'v3', 'created': 'Thu, 24 Oct 2024 12:47:56 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Mar 2025 15:18:47 GMT'}, {'version': 'v5', 'created': 'Wed, 12 Mar 2025 05:54:44 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Park', 'Jeongeun', ''], ['Choi', 'Sungjoon', ''], ['Yun', 'Sangdoo', '']]","extracted_entities":"[{'text': 'chat systems', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"chat systems","similarity_score":0.6067697406}
{"id":2410.18955,"submitter":"Yujuan Fu","authors":"Yujuan Velvin Fu, Giridhar Kaushik Ramachandran, Namu Park, Kevin\n  Lybarger, Fei Xia, Ozlem Uzuner, Meliha Yetisgen","title":"BioMistral-NLU: Towards More Generalizable Medical Language\n  Understanding through Instruction Tuning","comments":"3 figures an 5 tables; Accepted by AMIA 2025 Informatics Summit","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs) such as ChatGPT are fine-tuned on large and\ndiverse instruction-following corpora, and can generalize to new tasks.\nHowever, those instruction-tuned LLMs often perform poorly in specialized\nmedical natural language understanding (NLU) tasks that require domain\nknowledge, granular text comprehension, and structured data extraction. To\nbridge the gap, we: (1) propose a unified prompting format for 7 important NLU\ntasks, (2) curate an instruction-tuning dataset, MNLU-Instruct, utilizing\ndiverse existing open-source medical NLU corpora, and (3) develop\nBioMistral-NLU, a generalizable medical NLU model, through fine-tuning\nBioMistral on MNLU-Instruct. We evaluate BioMistral-NLU in a zero-shot setting,\nacross 6 important NLU tasks, from two widely adopted medical NLU benchmarks:\nBLUE and BLURB. Our experiments show that our BioMistral-NLU outperforms the\noriginal BioMistral, as well as the proprietary LLMs - ChatGPT and GPT-4. Our\ndataset-agnostic prompting strategy and instruction tuning step over diverse\nNLU tasks enhance LLMs' generalizability across diverse medical NLU tasks. Our\nablation experiments show that instruction-tuning on a wider variety of tasks,\neven when the total number of training instances remains constant, enhances\ndownstream zero-shot generalization.\n","versions":"[{'version': 'v1', 'created': 'Thu, 24 Oct 2024 17:53:53 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 07:21:04 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Fu', 'Yujuan Velvin', ''], ['Ramachandran', 'Giridhar Kaushik', ''], ['Park', 'Namu', ''], ['Lybarger', 'Kevin', ''], ['Xia', 'Fei', ''], ['Uzuner', 'Ozlem', ''], ['Yetisgen', 'Meliha', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'ChatGPT', 'label': 'GPT'}, {'text': 'GPT-4', 'label': 'GPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2502.19518,"submitter":"Luiz Franciscatto Guerra","authors":"L. P. Franciscatto Guerra, N. Ernst","title":"Assessing LLMs for Front-end Software Architecture Knowledge","comments":"4 pages, 1 figure, to appear in the International Workshop on\n  Designing Software at ICSE 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) have demonstrated significant promise in\nautomating software development tasks, yet their capabilities with respect to\nsoftware design tasks remains largely unclear. This study investigates the\ncapabilities of an LLM in understanding, reproducing, and generating structures\nwithin the complex VIPER architecture, a design pattern for iOS applications.\nWe leverage Bloom's taxonomy to develop a comprehensive evaluation framework to\nassess the LLM's performance across different cognitive domains such as\nremembering, understanding, applying, analyzing, evaluating, and creating.\nExperimental results, using ChatGPT 4 Turbo 2024-04-09, reveal that the LLM\nexcelled in higher-order tasks like evaluating and creating, but faced\nchallenges with lower-order tasks requiring precise retrieval of architectural\ndetails. These findings highlight both the potential of LLMs to reduce\ndevelopment costs and the barriers to their effective application in real-world\nsoftware design scenarios. This study proposes a benchmark format for assessing\nLLM capabilities in software architecture, aiming to contribute toward more\nrobust and accessible AI-driven development tools.\n","versions":"[{'version': 'v1', 'created': 'Wed, 26 Feb 2025 19:33:35 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 01:43:42 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Guerra', 'L. P. Franciscatto', ''], ['Ernst', 'N.', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2502.19771,"submitter":"Kehan Sheng","authors":"Kehan Sheng, Frank A.M. Tuyttens, Marina A.G. von Keyserlingk","title":"The erasure of intensive livestock farming in text-to-image generative\n  AI","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Generative AI (e.g., ChatGPT) is increasingly integrated into people's daily\nlives. While it is known that AI perpetuates biases against marginalized human\ngroups, their impact on non-human animals remains understudied. We found that\nChatGPT's text-to-image model (DALL-E 3) introduces a strong bias toward\nromanticizing livestock farming as dairy cows on pasture and pigs rooting in\nmud. This bias remained when we requested realistic depictions and was only\nmitigated when the automatic prompt revision was inhibited. Most farmed animal\nin industrialized countries are reared indoors with limited space per animal,\nwhich fail to resonate with societal values. Inhibiting prompt revision\nresulted in images that more closely reflected modern farming practices; for\nexample, cows housed indoors accessing feed through metal headlocks, and pigs\nbehind metal railings on concrete floors in indoor facilities. While OpenAI\nintroduced prompt revision to mitigate bias, in the case of farmed animal\nproduction systems, it paradoxically introduces a strong bias towards\nunrealistic farming practices.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 05:14:04 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 22:35:38 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Sheng', 'Kehan', ''], ['Tuyttens', 'Frank A. M.', ''], ['von Keyserlingk', 'Marina A. G.', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'automatic prompt revision', 'label': 'Prompting'}, {'text': 'societal values', 'label': 'AI Ethics'}, {'text': 'prompt revision', 'label': 'Prompting'}, {'text': 'prompt revision', 'label': 'Prompting'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2503.04758,"submitter":"Sasa Maric","authors":"Sasa Maric, Sonja Maric, Lana Maric","title":"Chat-GPT: An AI Based Educational Revolution","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The AI revolution is gathering momentum at an unprecedented rate. Over the\npast decade, we have witnessed a seemingly inevitable integration of AI in\nevery facet of our lives. Much has been written about the potential\nrevolutionary impact of AI in education. AI has the potential to completely\nrevolutionise the educational landscape as we could see entire courses and\ndegrees developed by programs such as ChatGPT. AI has the potential to develop\ncourses, set assignments, grade and provide feedback to students much faster\nthan a team of teachers. In addition, because of its dynamic nature, it has the\npotential to continuously improve its content. In certain fields such as\ncomputer science, where technology is continuously evolving, AI based\napplications can provide dynamically changing, relevant material to students.\nAI has the potential to replace entire degrees and may challenge the concept of\nhigher education institutions. We could also see entire new disciplines emerge\nas a consequence of AI. This paper examines the practical impact of ChatGPT and\nwhy it is believed that its implementation is a critical step towards a new era\nof education. We investigate the impact that ChatGPT will have on learning,\nproblem solving skills and cognitive ability of students. We examine the\npositives, negatives and many other aspects of AI and its applications\nthroughout this paper.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Feb 2025 13:03:35 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 06:33:07 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Maric', 'Sasa', ''], ['Maric', 'Sonja', ''], ['Maric', 'Lana', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2503.0576,"submitter":"Gokul Puthumanaillam","authors":"Gokul Puthumanaillam, Melkior Ornik","title":"The Lazy Student's Dream: ChatGPT Passing an Engineering Course on Its\n  Own","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper presents a comprehensive investigation into the capability of\nLarge Language Models (LLMs) to successfully complete a semester-long\nundergraduate control systems course. Through evaluation of 115 course\ndeliverables, we assess LLM performance using ChatGPT under a \"minimal effort\"\nprotocol that simulates realistic student usage patterns. The investigation\nemploys a rigorous testing methodology across multiple assessment formats, from\nauto-graded multiple choice questions to complex Python programming tasks and\nlong-form analytical writing. Our analysis provides quantitative insights into\nAI's strengths and limitations in handling mathematical formulations, coding\nchallenges, and theoretical concepts in control systems engineering. The LLM\nachieved a B-grade performance (82.24\\%), approaching but not exceeding the\nclass average (84.99\\%), with strongest results in structured assignments and\ngreatest limitations in open-ended projects. The findings inform discussions\nabout course design adaptation in response to AI advancement, moving beyond\nsimple prohibition towards thoughtful integration of these tools in engineering\neducation. Additional materials including syllabus, examination papers, design\nprojects, and example responses can be found at the project website:\nhttps:\/\/gradegpt.github.io.\n","versions":"[{'version': 'v1', 'created': 'Sun, 23 Feb 2025 18:47:14 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 14:04:58 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Puthumanaillam', 'Gokul', ''], ['Ornik', 'Melkior', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2503.06489,"submitter":"Sirinda Palahan","authors":"Sirinda Palahan","title":"Improving Access to Trade and Investment Information in Thailand through\n  Intelligent Document Retrieval","comments":null,"journal-ref":"International Journal for Computers & Their Applications, 2023,\n  Vol 30, Issue 4","doi":null,"report-no":null,"categories":"cs.IR cs.SI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Overseas investment and trade can be daunting for beginners due to the vast\namount of complex information. This paper presents a chatbot system that\nintegrates natural language processing and information retrieval techniques to\nsimplify the document retrieval process. The proposed system identifies the\nmost relevant content, enabling users to navigate the intricate landscape of\nforeign trade and investment more efficiently. Our methodology combines the\nBM25 model and a deep learning model to rank and retrieve documents, aiming to\nreduce noise in the document content and enhance the accuracy of the results.\nExperiments with Thai natural language queries have demonstrated the\neffectiveness of our system in retrieving pertinent documents. A user\nsatisfaction survey further validated the system's effectiveness. Most\nrespondents found the system helpful and agreed with the suggested documents,\nindicating its potential as a valuable tool for Thai entrepreneurs navigating\nforeign trade and investment.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 07:21:57 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Palahan', 'Sirinda', '']]","extracted_entities":"[{'text': 'chatbot system', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"chatbot system","similarity_score":0.5246143937}
{"id":2503.06551,"submitter":"Marco Giunti","authors":"Marco Giunti","title":"ChatGPT-4 in the Turing Test: A Critical Analysis","comments":"14 pages, 1 Appendix, added 1 missing item in References, corrected\n  typos","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CY cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  This paper critically examines the recent publication \"ChatGPT-4 in the\nTuring Test\" by Restrepo Echavarr\\'ia (2025), challenging its central claims\nregarding the absence of minimally serious test implementations and the\nconclusion that ChatGPT-4 fails the Turing Test. The analysis reveals that the\ncriticisms based on rigid criteria and limited experimental data are not fully\njustified. More importantly, the paper makes several constructive contributions\nthat enrich our understanding of Turing Test implementations. It demonstrates\nthat two distinct formats--the three-player and two-player tests--are both\nvalid, each with unique methodological implications. The work distinguishes\nbetween absolute criteria (reflecting an optimal 50% identification rate in a\nthree-player format) and relative criteria (which measure how closely a\nmachine's performance approximates that of a human), offering a more nuanced\nevaluation framework. Furthermore, the paper clarifies the probabilistic\nunderpinnings of both test types by modeling them as Bernoulli\nexperiments--correlated in the three-player version and uncorrelated in the\ntwo-player version. This formalization allows for a rigorous separation between\nthe theoretical criteria for passing the test, defined in probabilistic terms,\nand the experimental data that require robust statistical methods for proper\ninterpretation. In doing so, the paper not only refutes key aspects of the\ncriticized study but also lays a solid foundation for future research on\nobjective measures of how closely an AI's behavior aligns with, or deviates\nfrom, that of a human being.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 10:43:17 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 12:33:04 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Giunti', 'Marco', '']]","extracted_entities":"[{'text': 'ChatGPT-4', 'label': 'ChatGPT'}, {'text': 'ChatGPT-4', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT-4","similarity_score":0.8939331174}
{"id":2012.09766,"submitter":"Sofian Chaybouti","authors":"Sofian Chaybouti, Achraf Saghe, Aymen Shabou","title":"MIX : a Multi-task Learning Approach to Solve Open-Domain Question\n  Answering","comments":"8 pages, 7 figures, 3 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper introduces MIX, a multi-task deep learning approach to solve\nopen-ended question-answering. First, we design our system as a multi-stage\npipeline of 3 building blocks: a BM25-based Retriever to reduce the search\nspace, a RoBERTa-based Scorer, and an Extractor to rank retrieved paragraphs\nand extract relevant text spans, respectively. Eventually, we further improve\nthe computational efficiency of our system to deal with the scalability\nchallenge: thanks to multi-task learning, we parallelize the close tasks solved\nby the Scorer and the Extractor. Our system is on par with state-of-the-art\nperformances on the squad-open benchmark while being simpler conceptually.\n","versions":"[{'version': 'v1', 'created': 'Thu, 17 Dec 2020 17:22:30 GMT'}, {'version': 'v2', 'created': 'Fri, 29 Jan 2021 20:06:03 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 13:56:45 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chaybouti', 'Sofian', ''], ['Saghe', 'Achraf', ''], ['Shabou', 'Aymen', '']]","extracted_entities":"[{'text': 'MIX', 'label': 'Few-shot Learning'}, {'text': 'RoBERTa-based', 'label': 'RoBERTa'}, {'text': 'multi-task learning', 'label': 'Few-shot Learning'}]","assigned_concept":"RoBERTa","matched_keyword":"RoBERTa-based","similarity_score":0.8695192933}
{"id":2503.03612,"submitter":"Kemal Kirtac","authors":"Kemal Kirtac and Guido Germano","title":"Large language models in finance : what is financial sentiment?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"q-fin.ST q-fin.GN","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Financial sentiment has become a crucial yet complex concept in finance,\nincreasingly used in market forecasting and investment strategies. Despite its\ngrowing importance, there remains a need to define and understand what\nfinancial sentiment truly represents and how it can be effectively measured. We\nexplore the nature of financial sentiment and investigate how large language\nmodels (LLMs) contribute to its estimation. We trace the evolution of sentiment\nmeasurement in finance, from market-based and lexicon-based methods to advanced\nnatural language processing techniques. The emergence of LLMs has significantly\nenhanced sentiment analysis, providing deeper contextual understanding and\ngreater accuracy in extracting sentiment from financial text. We examine how\nBERT-based models, such as RoBERTa and FinBERT, are optimized for structured\nsentiment classification, while GPT-based models, including GPT-4, OPT, and\nLLaMA, excel in financial text generation and real-time sentiment\ninterpretation. A comparative analysis of bidirectional and autoregressive\ntransformer architectures highlights their respective roles in investor\nsentiment analysis, algorithmic trading, and financial decision-making. By\nexploring what financial sentiment is and how it is estimated within LLMs, we\nprovide insights into the growing role of AI-driven sentiment analysis in\nfinance.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 15:51:25 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 13:58:00 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Kirtac', 'Kemal', ''], ['Germano', 'Guido', '']]","extracted_entities":"[{'text': 'RoBERTa', 'label': 'RoBERTa'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'OPT', 'label': 'GPT'}]","assigned_concept":"RoBERTa","matched_keyword":"RoBERTa","similarity_score":1.0}
{"id":2211.13824,"submitter":"\\\"Od\\\"ul Tetik","authors":"\\\"Od\\\"ul Tetik","title":"The stratified Grassmannian and its depth-one subcategories","comments":"53 pages; new results, title changed","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AT math.CT","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We introduce a tangential theory for linked manifolds of depth $1$, i.e., for\nspans $\\mathfrak{S}=(M\\overset{\\pi}{\\twoheadleftarrow}\nL\\overset{\\iota}{\\hookrightarrow}N)$ of smooth manifolds where $\\pi$ is a fibre\nbundle and $\\iota$ is a closed embedding. The tangent classifier of\n$\\mathfrak{S}$ is given as a topological span map $\\mathfrak{S}\\to\nB\\mathrm{O}(n,m)$ where $B\\mathrm{O}(n,m)=(B\\mathrm{O}(n)\\twoheadleftarrow\nB\\mathrm{O}(n)\\times B\\mathrm{O}(m)\\hookrightarrow B\\mathrm{O}(n+m))$. We show\nthat this recovers and generalises the tangential theory introduced by Ayala,\nFrancis and Rozenblyum for conically smooth stratified spaces by constructing\nfully faithful functors\n$\\mathbf{EX}(B\\mathrm{O}(n,m))\\hookrightarrow\\mathbf{V}^{\\hookrightarrow}$ of\nquasi-categories, where $\\mathbf{EX}$ takes the exit path quasi-category of the\nspan, and $\\mathbf{V}^{\\hookrightarrow}$ is a quasi-category model of the\ninfinite stratified Grassmannian of AFR. This result has analogues for other\nclassical structure groups and for Stiefel manifolds. As an application, we\nreduce the classification of conically smooth bundles in depth $1$ to the\nclassification of ordinary bundles on linked manifolds.\n","versions":"[{'version': 'v1', 'created': 'Thu, 24 Nov 2022 23:32:50 GMT'}, {'version': 'v2', 'created': 'Sun, 4 Dec 2022 22:36:08 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 19:19:18 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Tetik', '\u00d6d\u00fcl', '']]","extracted_entities":"[{'text': 'closed embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"closed embedding","similarity_score":0.7480408549}
{"id":2306.0821,"submitter":"Shuyi Chen","authors":"Shuyi Chen, Kaize Ding, Shixiang Zhu","title":"Uncertainty-Aware Robust Learning on Noisy Graphs","comments":"ICASSP 2025 camera ready","journal-ref":"ICASSP 2025 - IEEE International Conference on Acoustics, Speech,\n  and Signal Processing","doi":"10.1109\/ICASSP49660.2025.10888672","report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Graph neural networks (GNNs) have excelled in various graph learning tasks,\nparticularly node classification. However, their performance is often hampered\nby noisy measurements in real-world graphs, which can corrupt critical patterns\nin the data. To address this, we propose a novel uncertainty-aware graph\nlearning framework inspired by distributionally robust optimization.\nSpecifically, we use a graph neural network-based encoder to embed the node\nfeatures and find the optimal node embeddings by minimizing the worst-case risk\nthrough a minimax formulation. Such an uncertainty-aware learning process leads\nto improved node representations and a more robust graph predictive model that\neffectively mitigates the impact of uncertainty arising from data noise. Our\nexperimental results demonstrate superior predictive performance over baselines\nacross noisy scenarios.\n","versions":"[{'version': 'v1', 'created': 'Wed, 14 Jun 2023 02:45:14 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 14:30:06 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chen', 'Shuyi', ''], ['Ding', 'Kaize', ''], ['Zhu', 'Shixiang', '']]","extracted_entities":"[{'text': 'distributionally robust optimization', 'label': 'Fine-tuning'}, {'text': 'node embeddings', 'label': 'Embedding'}, {'text': 'minimax formulation', 'label': 'Fine-tuning'}]","assigned_concept":"Embedding","matched_keyword":"node embeddings","similarity_score":0.7718001008}
{"id":2308.00137,"submitter":"Hemn Abdalla","authors":"Hemn Barzan Abdalla, Awder Ahmed, Bahtiyar Mehmed, Mehdi Gheisari,\n  Maryam Cheraghy, Yang Liu","title":"An Efficient Recommendation System in E-commerce using Passer learning\n  optimization based on Bi-LSTM","comments":"22 pages, 5 figuers, 4 Tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.MM cs.NE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Online reviews play a crucial role in shaping consumer decisions, especially\nin the context of e-commerce. However, the quality and reliability of these\nreviews can vary significantly. Some reviews contain misleading or unhelpful\ninformation, such as advertisements, fake content, or irrelevant details. These\nissues pose significant challenges for recommendation systems, which rely on\nuser-generated reviews to provide personalized suggestions. This article\nintroduces a recommendation system based on Passer Learning\nOptimization-enhanced Bi-LSTM classifier applicable to e-commerce\nrecommendation systems with improved accuracy and efficiency compared to\nstate-of-the-art models. It achieves as low as 1.24% MSE on the baby dataset.\nThis lifts it as high as 88.58%. Besides, there is also robust performance of\nthe system on digital music and patio lawn garden datasets at F1 of 88.46% and\n92.51%, correspondingly. These results, made possible by advanced graph\nembedding for effective knowledge extraction and fine-tuning of classifier\nparameters, establish the suitability of the proposed model in various\ne-commerce environments.\n","versions":"[{'version': 'v1', 'created': 'Mon, 31 Jul 2023 20:09:25 GMT'}, {'version': 'v2', 'created': 'Wed, 2 Aug 2023 07:34:05 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 14:43:36 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Abdalla', 'Hemn Barzan', ''], ['Ahmed', 'Awder', ''], ['Mehmed', 'Bahtiyar', ''], ['Gheisari', 'Mehdi', ''], ['Cheraghy', 'Maryam', ''], ['Liu', 'Yang', '']]","extracted_entities":"[{'text': 'advanced graph\\nembedding', 'label': 'Embedding'}, {'text': 'fine-tuning of classifier\\nparameters', 'label': 'Fine-tuning'}]","assigned_concept":"Embedding","matched_keyword":"advanced graph\nembedding","similarity_score":0.6591576338}
{"id":2309.16633,"submitter":"Zijian Dong","authors":"Yilei Wu, Zijian Dong, Chongyao Chen, Wangchunshu Zhou, Juan Helen\n  Zhou","title":"SupReMix: Supervised Contrastive Learning for Medical Imaging Regression\n  with Mixup","comments":"The first two authors equally contributed to this work","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In medical image analysis, regression plays a critical role in computer-aided\ndiagnosis. It enables quantitative measurements such as age prediction from\nstructural imaging, cardiac function quantification, and molecular measurement\nfrom PET scans. While deep learning has shown promise for these tasks, most\napproaches focus solely on optimizing regression loss or model architecture,\nneglecting the quality of learned feature representations which are crucial for\nrobust clinical predictions. Directly applying representation learning\ntechniques designed for classification to regression often results in\nfragmented representations in the latent space, yielding sub-optimal\nperformance. In this paper, we argue that the potential of contrastive learning\nfor medical image regression has been overshadowed due to the neglect of two\ncrucial aspects: ordinality-awareness and hardness. To address these\nchallenges, we propose Supervised Contrastive Learning for Medical Imaging\nRegression with Mixup (SupReMix). It takes anchor-inclusive mixtures (mixup of\nthe anchor and a distinct negative sample) as hard negative pairs and\nanchor-exclusive mixtures (mixup of two distinct negative samples) as hard\npositive pairs at the embedding level. This strategy formulates harder\ncontrastive pairs by integrating richer ordinal information. Through\ntheoretical analysis and extensive experiments on six datasets spanning MRI,\nX-ray, ultrasound, and PET modalities, we demonstrate that SupReMix fosters\ncontinuous ordered representations, significantly improving regression\nperformance.\n","versions":"[{'version': 'v1', 'created': 'Thu, 28 Sep 2023 17:38:59 GMT'}, {'version': 'v2', 'created': 'Fri, 29 Sep 2023 04:22:54 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 19:37:46 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Wu', 'Yilei', ''], ['Dong', 'Zijian', ''], ['Chen', 'Chongyao', ''], ['Zhou', 'Wangchunshu', ''], ['Zhou', 'Juan Helen', '']]","extracted_entities":"[{'text': 'Supervised Contrastive Learning', 'label': 'Few-shot Learning'}, {'text': 'embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embedding","similarity_score":1.0}
{"id":2311.0935,"submitter":"Wei-Di Chang","authors":"Wei-Di Chang, Francois Hogan, Scott Fujimoto, David Meger, and Gregory\n  Dudek","title":"Generalizable Imitation Learning Through Pre-Trained Representations","comments":"ICRA 2025 Version","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this paper, we leverage self-supervised vision transformer models and\ntheir emergent semantic abilities to improve the generalization abilities of\nimitation learning policies. We introduce DVK, an imitation learning algorithm\nthat leverages rich pre-trained Visual Transformer patch-level embeddings to\nobtain better generalization when learning through demonstrations. Our learner\nsees the world by clustering appearance features into groups associated with\nsemantic concepts, forming stable keypoints that generalize across a wide range\nof appearance variations and object types. We demonstrate how this\nrepresentation enables generalized behaviour by evaluating imitation learning\nacross a diverse dataset of object manipulation tasks. To facilitate further\nstudy of generalization in Imitation Learning, all of our code for the method\nand evaluation, as well as the dataset, is made available.\n","versions":"[{'version': 'v1', 'created': 'Wed, 15 Nov 2023 20:15:51 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 18:57:28 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Chang', 'Wei-Di', ''], ['Hogan', 'Francois', ''], ['Fujimoto', 'Scott', ''], ['Meger', 'David', ''], ['Dudek', 'Gregory', '']]","extracted_entities":"[{'text': 'patch-level embeddings', 'label': 'Embedding'}, {'text': 'imitation learning', 'label': 'Few-shot Learning'}, {'text': 'Imitation Learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Embedding","matched_keyword":"patch-level embeddings","similarity_score":0.687147975}
{"id":2312.04539,"submitter":"Osman \\\"Ulger","authors":"Osman \\\"Ulger, Maksymilian Kulicki, Yuki Asano, Martin R. Oswald","title":"Auto-Vocabulary Semantic Segmentation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Open-Vocabulary Segmentation (OVS) methods are capable of performing semantic\nsegmentation without relying on a fixed vocabulary, and in some cases, without\ntraining or fine-tuning. However, OVS methods typically require a human in the\nloop to specify the vocabulary based on the task or dataset at hand. In this\npaper, we introduce Auto-Vocabulary Semantic Segmentation (AVS), advancing\nopen-ended image understanding by eliminating the necessity to predefine object\ncategories for segmentation. Our approach, AutoSeg, presents a framework that\nautonomously identifies relevant class names using semantically enhanced BLIP\nembeddings and segments them afterwards. Given that open-ended object category\npredictions cannot be directly compared with a fixed ground truth, we develop a\nLarge Language Model-based Auto-Vocabulary Evaluator (LAVE) to efficiently\nevaluate the automatically generated classes and their corresponding segments.\nWith AVS, our method sets new benchmarks on datasets PASCAL VOC, Context,\nADE20K, and Cityscapes, while showing competitive performance to OVS methods\nthat require specified class names.\n","versions":"[{'version': 'v1', 'created': 'Thu, 7 Dec 2023 18:55:52 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Mar 2024 16:11:22 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 12:39:35 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['\u00dclger', 'Osman', ''], ['Kulicki', 'Maksymilian', ''], ['Asano', 'Yuki', ''], ['Oswald', 'Martin R.', '']]","extracted_entities":"[{'text': 'BLIP\\nembeddings', 'label': 'Embedding'}, {'text': 'PASCAL VOC', 'label': 'Large Language Model'}, {'text': 'Context', 'label': 'Large Language Model'}, {'text': 'ADE20K', 'label': 'Large Language Model'}, {'text': 'Cityscapes', 'label': 'Large Language Model'}]","assigned_concept":"Embedding","matched_keyword":"BLIP\nembeddings","similarity_score":0.6483724713}
{"id":2402.01329,"submitter":"Grigor Sargsyan","authors":"Douglas Blue and Grigor Sargsyan","title":"AD$^+$ implies that $\\omega_1$ is a $\\Theta$-Berkeley cardinal","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.LO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Following \\cite{bagaria2019large}, given cardinals $\\kappa<\\lambda$, we say\n$\\kappa$ is a club $\\lambda$-Berkeley cardinal if for every transitive set $N$\nof size $<\\lambda$ such that $\\kappa\\subseteq N$, there is a club $C\\subseteq\n\\kappa$ with the property that for every $\\eta\\in C$ there is an elementary\nembedding $j: N\\rightarrow N$ with crit$(j)=\\eta$. We say $\\kappa$ is\n$\\nu$-club $\\lambda$-Berkeley if $C\\subseteq \\kappa$ as above is a $\\nu$-club.\nWe say $\\kappa$ is $\\lambda$-Berkeley if $C$ is unbounded in $\\kappa$. We show\nthat under AD$^+$, (1) every regular Suslin cardinal is $\\omega$-club\n$\\Theta$-Berkeley (see \\rthm{main theorem}), (2) $\\omega_1$ is club\n$\\Theta$-Berkeley (see \\rthm{main theorem lr} and \\rthm{main theorem}), and (3)\nthe ${\\tilde\\delta}^1_{2n}$'s are $\\Theta$-Berkeley -- in particular,\n$\\omega_2$ is $\\Theta$-Berkeley (see \\rrem{omega2}).\n  Along the way, we represent regular Suslin cardinals in direct limits as\ncutpoint cardinals (see \\rthm{char extenders}). This topic has been studied in\n\\cite{MPSC} and \\cite{jackson2022suslin}, albeit from a different point of\nview. We also show that, assuming $V=L(\\mathbb{R})+{\\mathrm{AD}}$, $\\omega_1$\nis not $\\Theta^+$-Berkeley, so the result stated in the title is optimal (see\n\\rthm{lr optimal} and \\rthm{thetareg optimal}).\n","versions":"[{'version': 'v1', 'created': 'Fri, 2 Feb 2024 11:30:21 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 14:38:39 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Blue', 'Douglas', ''], ['Sargsyan', 'Grigor', '']]","extracted_entities":"[{'text': 'elementary\\nembedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"elementary\nembedding","similarity_score":0.8491398096}
{"id":2402.01974,"submitter":"LIanhao Yin","authors":"Lianhao Yin, Yutong Ban, Jennifer Eckhoff, Ozanan Meireles, Daniela\n  Rus, Guy Rosman","title":"Hypergraph-Transformer (HGT) for Interactive Event Prediction in\n  Laparoscopic and Robotic Surgery","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Understanding and anticipating intraoperative events and actions is critical\nfor intraoperative assistance and decision-making during minimally invasive\nsurgery. Automated prediction of events, actions, and the following\nconsequences is addressed through various computational approaches with the\nobjective of augmenting surgeons' perception and decision-making capabilities.\nWe propose a predictive neural network that is capable of understanding and\npredicting critical interactive aspects of surgical workflow from\nintra-abdominal video, while flexibly leveraging surgical knowledge graphs. The\napproach incorporates a hypergraph-transformer (HGT) structure that encodes\nexpert knowledge into the network design and predicts the hidden embedding of\nthe graph. We verify our approach on established surgical datasets and\napplications, including the detection and prediction of action triplets, and\nthe achievement of the Critical View of Safety (CVS). Moreover, we address\nspecific, safety-related tasks, such as predicting the clipping of cystic duct\nor artery without prior achievement of the CVS. Our results demonstrate the\nsuperiority of our approach compared to unstructured alternatives.\n","versions":"[{'version': 'v1', 'created': 'Sat, 3 Feb 2024 00:58:05 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 21:58:42 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Yin', 'Lianhao', ''], ['Ban', 'Yutong', ''], ['Eckhoff', 'Jennifer', ''], ['Meireles', 'Ozanan', ''], ['Rus', 'Daniela', ''], ['Rosman', 'Guy', '']]","extracted_entities":"[{'text': 'hidden embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"hidden embedding","similarity_score":0.8065743446}
{"id":2402.14327,"submitter":"Delong Chen","authors":"Delong Chen, Samuel Cahyawijaya, Jianfeng Liu, Baoyuan Wang, Pascale\n  Fung","title":"Subobject-level Image Tokenization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Patch-based image tokenization ignores the morphology of the visual world,\nlimiting effective and efficient learning of image understanding. Inspired by\nsubword tokenization, we introduce subobject-level adaptive token segmentation\nand explore several approaches, including superpixel, SAM, and a proposed\nEfficient and PanOptiC (EPOC) image tokenizer. Our EPOC combines boundary\ndetection -- a simple task that can be handled well by a compact model -- with\nwatershed segmentation, which inherently guarantees no pixels are left\nunsegmented. Intrinsic evaluations across 5 datasets demonstrate that EPOC's\nsegmentation aligns well with human annotations of both object- and part-level\nvisual morphology, producing more monosemantic tokens and offering substantial\nefficiency advantages. For extrinsic evaluation, we designed a token embedding\nthat handles arbitrary-shaped tokens, and trained VLMs with different\ntokenizers on 4 datasets of object recognition and detailed captioning. The\nresults reveal that subobject tokenization enables faster convergence and\nbetter generalization while using fewer visual tokens.\n","versions":"[{'version': 'v1', 'created': 'Thu, 22 Feb 2024 06:47:44 GMT'}, {'version': 'v2', 'created': 'Tue, 23 Apr 2024 13:41:47 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 18:22:25 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chen', 'Delong', ''], ['Cahyawijaya', 'Samuel', ''], ['Liu', 'Jianfeng', ''], ['Wang', 'Baoyuan', ''], ['Fung', 'Pascale', '']]","extracted_entities":"[{'text': 'token embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"token embedding","similarity_score":0.6665632129}
{"id":2404.10419,"submitter":"Matthieu Futeral","authors":"Matthieu Futeral, Andrea Agostinelli, Marco Tagliasacchi, Neil\n  Zeghidour, Eugene Kharitonov","title":"MAD Speech: Measures of Acoustic Diversity of Speech","comments":"NAACL 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.AS cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Generative spoken language models produce speech in a wide range of voices,\nprosody, and recording conditions, seemingly approaching the diversity of\nnatural speech. However, the extent to which generated speech is acoustically\ndiverse remains unclear due to a lack of appropriate metrics. We address this\ngap by developing lightweight metrics of acoustic diversity, which we\ncollectively refer to as MAD Speech. We focus on measuring five facets of\nacoustic diversity: voice, gender, emotion, accent, and background noise. We\nconstruct the metrics as a composition of specialized, per-facet embedding\nmodels and an aggregation function that measures diversity within the embedding\nspace. Next, we build a series of datasets with a priori known diversity\npreferences for each facet. Using these datasets, we demonstrate that our\nproposed metrics achieve a stronger agreement with the ground-truth diversity\nthan baselines. Finally, we showcase the applicability of our proposed metrics\nacross several real-life evaluation scenarios. MAD Speech is made publicly\naccessible.\n","versions":"[{'version': 'v1', 'created': 'Tue, 16 Apr 2024 09:35:27 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 12:02:06 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Futeral', 'Matthieu', ''], ['Agostinelli', 'Andrea', ''], ['Tagliasacchi', 'Marco', ''], ['Zeghidour', 'Neil', ''], ['Kharitonov', 'Eugene', '']]","extracted_entities":"[{'text': 'embedding\\nspace', 'label': 'Embedding'}, {'text': 'publicly\\naccessible', 'label': 'Open-source LLMs'}]","assigned_concept":"Embedding","matched_keyword":"embedding\nspace","similarity_score":0.8514168859}
{"id":2405.10075,"submitter":"Kun Yuan","authors":"Kun Yuan, Vinkle Srivastav, Nassir Navab, Nicolas Padoy","title":"HecVL: Hierarchical Video-Language Pretraining for Zero-shot Surgical\n  Phase Recognition","comments":"Accepted by MICCAI2024","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Natural language could play an important role in developing generalist\nsurgical models by providing a broad source of supervision from raw texts. This\nflexible form of supervision can enable the model's transferability across\ndatasets and tasks as natural language can be used to reference learned visual\nconcepts or describe new ones. In this work, we present HecVL, a novel\nhierarchical video-language pretraining approach for building a generalist\nsurgical model. Specifically, we construct a hierarchical video-text paired\ndataset by pairing the surgical lecture video with three hierarchical levels of\ntexts: at clip-level, atomic actions using transcribed audio texts; at\nphase-level, conceptual text summaries; and at video-level, overall abstract\ntext of the surgical procedure. Then, we propose a novel fine-to-coarse\ncontrastive learning framework that learns separate embedding spaces for the\nthree video-text hierarchies using a single model. By disentangling embedding\nspaces of different hierarchical levels, the learned multi-modal\nrepresentations encode short-term and long-term surgical concepts in the same\nmodel. Thanks to the injected textual semantics, we demonstrate that the HecVL\napproach can enable zero-shot surgical phase recognition without any human\nannotation. Furthermore, we show that the same HecVL model for surgical phase\nrecognition can be transferred across different surgical procedures and medical\ncenters. The code is available at https:\/\/github.com\/CAMMA-public\/SurgVLP\n","versions":"[{'version': 'v1', 'created': 'Thu, 16 May 2024 13:14:43 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 15:27:41 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Yuan', 'Kun', ''], ['Srivastav', 'Vinkle', ''], ['Navab', 'Nassir', ''], ['Padoy', 'Nicolas', '']]","extracted_entities":"[{'text': 'embedding spaces', 'label': 'Embedding'}, {'text': 'embedding\\nspaces', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embedding spaces","similarity_score":0.8289273977}
{"id":2405.15779,"submitter":"Van-Truong Pham","authors":"Ngoc-Du Tran, Thi-Thao Tran, Quang-Huy Nguyen, Manh-Hung Vu,\n  Van-Truong Pham","title":"LiteNeXt: A Novel Lightweight ConvMixer-based Model with Self-embedding\n  Representation Parallel for Medical Image Segmentation","comments":"This manuscript has been accepted by Biomedical Signal Processing and\n  Control","journal-ref":"Biomedical Signal Processing and Control, 2025","doi":null,"report-no":null,"categories":"eess.IV cs.AI cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The emergence of deep learning techniques has advanced the image segmentation\ntask, especially for medical images. Many neural network models have been\nintroduced in the last decade bringing the automated segmentation accuracy\nclose to manual segmentation. However, cutting-edge models like\nTransformer-based architectures rely on large scale annotated training data,\nand are generally designed with densely consecutive layers in the encoder,\ndecoder, and skip connections resulting in large number of parameters.\nAdditionally, for better performance, they often be pretrained on a larger\ndata, thus requiring large memory size and increasing resource expenses. In\nthis study, we propose a new lightweight but efficient model, namely LiteNeXt,\nbased on convolutions and mixing modules with simplified decoder, for medical\nimage segmentation. The model is trained from scratch with small amount of\nparameters (0.71M) and Giga Floating Point Operations Per Second (0.42). To\nhandle boundary fuzzy as well as occlusion or clutter in objects especially in\nmedical image regions, we propose the Marginal Weight Loss that can help\neffectively determine the marginal boundary between object and background.\nAdditionally, the Self-embedding Representation Parallel technique is proposed\nas an innovative data augmentation strategy that utilizes the network\narchitecture itself for self-learning augmentation, enhancing feature\nextraction robustness without external data. Experiments on public datasets\nincluding Data Science Bowls, GlaS, ISIC2018, PH2, Sunnybrook, and Lung X-ray\ndata show promising results compared to other state-of-the-art CNN-based and\nTransformer-based architectures. Our code is released at:\nhttps:\/\/github.com\/tranngocduvnvp\/LiteNeXt.\n","versions":"[{'version': 'v1', 'created': 'Thu, 4 Apr 2024 01:59:19 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 08:54:13 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Tran', 'Ngoc-Du', ''], ['Tran', 'Thi-Thao', ''], ['Nguyen', 'Quang-Huy', ''], ['Vu', 'Manh-Hung', ''], ['Pham', 'Van-Truong', '']]","extracted_entities":"[{'text': 'Self-embedding Representation Parallel technique', 'label': 'Embedding'}, {'text': 'GlaS', 'label': 'Large Language Model'}, {'text': 'ISIC2018', 'label': 'Large Language Model'}, {'text': 'PH2', 'label': 'Large Language Model'}, {'text': 'Sunnybrook', 'label': 'Large Language Model'}]","assigned_concept":"Embedding","matched_keyword":"Self-embedding Representation Parallel technique","similarity_score":0.5879593492}
{"id":2406.16038,"submitter":"Delin Qu","authors":"Delin Qu, Qizhi Chen, Pingrui Zhang, Xianqiang Gao, Junzhe Li, Bin\n  Zhao, Dong Wang and Xuelong Li","title":"LiveScene: Language Embedding Interactive Radiance Fields for Physical\n  Scene Rendering and Control","comments":"Accepted at Neurips 2024","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper scales object-level reconstruction to complex scenes, advancing\ninteractive scene reconstruction. We introduce two datasets, OmniSim and\nInterReal, featuring 28 scenes with multiple interactive objects. To tackle the\nchallenge of inaccurate interactive motion recovery in complex scenes, we\npropose LiveScene, a scene-level language-embedded interactive radiance field\nthat efficiently reconstructs and controls multiple objects. By decomposing the\ninteractive scene into local deformable fields, LiveScene enables separate\nreconstruction of individual object motions, reducing memory consumption.\nAdditionally, our interaction-aware language embedding localizes individual\ninteractive objects, allowing for arbitrary control using natural language. Our\napproach demonstrates significant superiority in novel view synthesis,\ninteractive scene control, and language grounding performance through extensive\nexperiments. Project page: https:\/\/livescenes.github.io.\n","versions":"[{'version': 'v1', 'created': 'Sun, 23 Jun 2024 07:26:13 GMT'}, {'version': 'v2', 'created': 'Sun, 3 Nov 2024 07:37:05 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 03:19:42 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Qu', 'Delin', ''], ['Chen', 'Qizhi', ''], ['Zhang', 'Pingrui', ''], ['Gao', 'Xianqiang', ''], ['Li', 'Junzhe', ''], ['Zhao', 'Bin', ''], ['Wang', 'Dong', ''], ['Li', 'Xuelong', '']]","extracted_entities":"[{'text': 'LiveScene', 'label': 'Embedding'}, {'text': 'LiveScene', 'label': 'Embedding'}, {'text': 'interaction-aware language embedding', 'label': 'Embedding'}, {'text': 'livescenes', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"interaction-aware language embedding","similarity_score":0.5357202291}
{"id":2406.17281,"submitter":"Dong Liu","authors":"Dong Liu, Yanxuan Yu","title":"Adaptive Topology Reconstruction for Robust Graph Representation\n  Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Graph Neural Networks (GNNs) have become fundamental in semi-supervised\nlearning for graph representation, leveraging their ability to capture complex\nnode relationships. A recent trend in GNN research focuses on adaptive\nmulti-hop structure learning, moving beyond fixed-hop aggregation to more\nflexible and dynamic neighborhood selection. While GAMLP \\citep{Zhang_2022}\nemploys separate MLP layers for each multi-hop domain and ImprovingTE\n\\citep{Yao2023ImprovingTE} enhances this by injecting contextualized\nsubstructure information, these methods still rely heavily on predefined\nsampling strategies, which may limit their ability to generalize and maintain\nstable accuracy. To address these limitations, we propose an \\textbf{adaptive\nreconstruction framework} that dynamically refines multi-hop structure\nlearning. Inspired by \"coreset selection\" \\citep{guo2022deepcore}, our approach\nadaptively \\textbf{reconstructs} node neighborhoods to optimize message\npassing, ensuring more \\textbf{effective and context-aware information flow}\nacross the graph. To further enhance structural robustness, we introduce two\nkey modules: the \\textbf{Distance Recomputator} and the \\textbf{Topology\nReconstructor} (\\textcolor{blue}{DRTR}). The Distance Recomputator\n\\textbf{reassesses and recalibrates} node distances based on adaptive graph\nproperties, leading to \\textbf{improved node embeddings} that better reflect\nlatent relationships. Meanwhile, the Topology Reconstructor \\textbf{dynamically\nrefines local graph structures}, enabling the model to \\textbf{adapt to\nevolving graph topologies} and mitigate the impact of noise and mislabeled\ndata. Empirical evaluations demonstrate that our \\textbf{adaptive\nreconstruction framework} achieves \\textbf{significant improvements} over\nexisting multi-hop-based models, providing more \\textbf{stable and accurate}\nperformance in various graph learning benchmarks.\n","versions":"[{'version': 'v1', 'created': 'Tue, 25 Jun 2024 05:12:51 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Feb 2025 17:17:47 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 15:34:12 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Liu', 'Dong', ''], ['Yu', 'Yanxuan', '']]","extracted_entities":"[{'text': 'adaptive\\nmulti-hop structure learning', 'label': 'Few-shot Learning'}, {'text': 'multi-hop structure\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'node embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"node embeddings","similarity_score":0.7718001008}
{"id":2406.17674,"submitter":"Mauricio Che","authors":"Mauricio Che","title":"Optimal partial transport for metric pairs","comments":"25 pages. We have added new references, fixed typos, and polished the\n  exposition","journal-ref":null,"doi":null,"report-no":null,"categories":"math.MG math.AT","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this article we study Figalli and Gigli's formulation of optimal transport\nbetween non-negative Radon measures in the setting of metric pairs. We carry\nover classical characterisations of optimal plans to this setting and prove\nthat the resulting spaces of measures, $\\mathcal{M}_p(X,A)$, are complete,\nseparable and geodesic whenever the underlying space, $X$, is so. We also prove\nthat, for $p>1$, $\\mathcal{M}_p(X,A)$ preserves the property of being\nnon-branching, and for $p=2$ it preserves non-negative curvature in the\nAlexandrov sense. Finally, we prove isometric embeddings of generalised spaces\nof persistence diagrams $\\mathcal{D}_p(X,A)$ into the corresponding spaces\n$\\mathcal{M}_p(X,A)$, generalising a result by Divol and Lacombe. As an\napplication of this framework, we show that several known geometric properties\nof spaces of persistence diagrams follow from those of $\\mathcal{M}_p(X,A)$,\nincluding the fact that $\\mathcal{D}_2(X,A)$ is an Alexandrov space of\nnon-negative curvature whenever $X$ is a proper non-negatively curved\nAlexandrov space.\n","versions":"[{'version': 'v1', 'created': 'Tue, 25 Jun 2024 16:05:22 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 18:23:37 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Che', 'Mauricio', '']]","extracted_entities":"[{'text': 'isometric embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"isometric embeddings","similarity_score":0.6878781319}
{"id":2407.03715,"submitter":"Flavio Tonioni","authors":"C\\'edric Debusschere, Flavio Tonioni, Thomas Van Riet","title":"A distance conjecture beyond moduli?","comments":"8+1 pages and references, comments welcome!; v2: 9+2 pages and\n  references, with typos fixed, refs. added, and an extra appendix comparing\n  with hep-th\/2407.02705; v3, JHEP version: 11+2 pages and references, with\n  improved tests of the proposal in sec. 4, including 3 figs. and refs. added","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-th","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The distance conjecture states that for theories with moduli coupled to\ngravity a tower of states becomes light exponentially in the geodesic distance\nin moduli space. This specifies how effective field theories break down for\nlarge field values. However, phenomenological field theories have no moduli,\nbut a scalar potential that deforms dynamical trajectories away from geodesic\ncurves. In this note we speculate on how one should generalise the distance\nconjecture, in asymptotic field regimes, to include a scalar potential. We test\nthe generalised distance conjecture in a few cases, demonstrate a link with\npseudo-\/fake supersymmetry and apply it to the ekpyrotic scenario in cosmology.\nFor the latter we observe that the pre-uplift KKLT potential could provide a\nstringy embedding of ekpyrosis away from the asymptotic regimes in field space.\n","versions":"[{'version': 'v1', 'created': 'Thu, 4 Jul 2024 08:02:44 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Aug 2024 13:49:06 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 12:10:12 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Debusschere', 'C\u00e9dric', ''], ['Tonioni', 'Flavio', ''], ['Van Riet', 'Thomas', '']]","extracted_entities":"[{'text': 'stringy embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"stringy embedding","similarity_score":0.737077713}
{"id":2407.08031,"submitter":"Benedikt Petko","authors":"Marc Arnaudon, Xue-Mei Li, Benedikt Petko","title":"Coarse extrinsic curvature of Riemannian submanifolds","comments":"Accepted version; 50 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.DG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We introduce a novel concept of coarse extrinsic curvature for Riemannian\nsubmanifolds, inspired by Ollivier's notion of coarse Ricci curvature. This\ncurvature is derived from the Wasserstein 1-distance between probability\nmeasures supported in the tubular neighborhood of a submanifold, providing new\ninsights into the extrinsic curvature of isometrically embedded manifolds in\nEuclidean spaces. The framework also offers a method to approximate the mean\ncurvature from statistical data, such as point clouds generated by a Poisson\npoint process. This approach has potential applications in manifold learning\nand the study of metric embeddings, enabling the inference of geometric\ninformation from empirical data.\n","versions":"[{'version': 'v1', 'created': 'Wed, 10 Jul 2024 20:14:48 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 18:53:24 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Arnaudon', 'Marc', ''], ['Li', 'Xue-Mei', ''], ['Petko', 'Benedikt', '']]","extracted_entities":"[{'text': 'manifold learning', 'label': 'Few-shot Learning'}, {'text': 'metric embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"metric embeddings","similarity_score":0.6859562397}
{"id":2408.05117,"submitter":"Shouyue Liu","authors":"Shouyue Liu, Ziyi Zhang, Yuanyuan Gu, Jinkui Hao, Yonghuai Liu, Huazhu\n  Fu, Xinyu Guo, Hong Song, Shuting Zhang and Yitian Zhao","title":"Beyond the Eye: A Relational Model for Early Dementia Detection Using\n  Retinal OCTA Images","comments":null,"journal-ref":null,"doi":"10.1016\/j.media.2025.103513","report-no":null,"categories":"eess.IV cs.AI cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Early detection of dementia, such as Alzheimer's disease (AD) or mild\ncognitive impairment (MCI), is essential to enable timely intervention and\npotential treatment. Accurate detection of AD\/MCI is challenging due to the\nhigh complexity, cost, and often invasive nature of current diagnostic\ntechniques, which limit their suitability for large-scale population screening.\nGiven the shared embryological origins and physiological characteristics of the\nretina and brain, retinal imaging is emerging as a potentially rapid and\ncost-effective alternative for the identification of individuals with or at\nhigh risk of AD. In this paper, we present a novel PolarNet+ that uses retinal\noptical coherence tomography angiography (OCTA) to discriminate early-onset AD\n(EOAD) and MCI subjects from controls. Our method first maps OCTA images from\nCartesian coordinates to polar coordinates, allowing approximate sub-region\ncalculation to implement the clinician-friendly early treatment of diabetic\nretinopathy study (ETDRS) grid analysis. We then introduce a multi-view module\nto serialize and analyze the images along three dimensions for comprehensive,\nclinically useful information extraction. Finally, we abstract the sequence\nembedding into a graph, transforming the detection task into a general graph\nclassification problem. A regional relationship module is applied after the\nmulti-view module to excavate the relationship between the sub-regions. Such\nregional relationship analyses validate known eye-brain links and reveal new\ndiscriminative patterns.\n","versions":"[{'version': 'v1', 'created': 'Fri, 9 Aug 2024 15:10:34 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 08:58:41 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Liu', 'Shouyue', ''], ['Zhang', 'Ziyi', ''], ['Gu', 'Yuanyuan', ''], ['Hao', 'Jinkui', ''], ['Liu', 'Yonghuai', ''], ['Fu', 'Huazhu', ''], ['Guo', 'Xinyu', ''], ['Song', 'Hong', ''], ['Zhang', 'Shuting', ''], ['Zhao', 'Yitian', '']]","extracted_entities":"[{'text': 'sequence\\nembedding', 'label': 'Embedding'}, {'text': 'regional relationship module', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"sequence\nembedding","similarity_score":0.7107915878}
{"id":2408.13885,"submitter":"Anastasis Kratsios","authors":"Haitz S\\'aez de Oc\\'ariz Borde, Anastasis Kratsios, Marc T. Law,\n  Xiaowen Dong, Michael Bronstein","title":"Neural Spacetimes for DAG Representation Learning","comments":"12 pages: main body and 19 pages: appendix","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.DM cs.NE math.MG stat.ML","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We propose a class of trainable deep learning-based geometries called Neural\nSpacetimes (NSTs), which can universally represent nodes in weighted directed\nacyclic graphs (DAGs) as events in a spacetime manifold. While most works in\nthe literature focus on undirected graph representation learning or causality\nembedding separately, our differentiable geometry can encode both graph edge\nweights in its spatial dimensions and causality in the form of edge\ndirectionality in its temporal dimensions. We use a product manifold that\ncombines a quasi-metric (for space) and a partial order (for time). NSTs are\nimplemented as three neural networks trained in an end-to-end manner: an\nembedding network, which learns to optimize the location of nodes as events in\nthe spacetime manifold, and two other networks that optimize the space and time\ngeometries in parallel, which we call a neural (quasi-)metric and a neural\npartial order, respectively. The latter two networks leverage recent ideas at\nthe intersection of fractal geometry and deep learning to shape the geometry of\nthe representation space in a data-driven fashion, unlike other works in the\nliterature that use fixed spacetime manifolds such as Minkowski space or De\nSitter space to embed DAGs. Our main theoretical guarantee is a universal\nembedding theorem, showing that any $k$-point DAG can be embedded into an NST\nwith $1+\\mathcal{O}(\\log(k))$ distortion while exactly preserving its causal\nstructure. The total number of parameters defining the NST is sub-cubic in $k$\nand linear in the width of the DAG. If the DAG has a planar Hasse diagram, this\nis improved to $\\mathcal{O}(\\log(k)) + 2)$ spatial and 2 temporal dimensions.\nWe validate our framework computationally with synthetic weighted DAGs and\nreal-world network embeddings; in both cases, the NSTs achieve lower embedding\ndistortions than their counterparts using fixed spacetime geometries.\n","versions":"[{'version': 'v1', 'created': 'Sun, 25 Aug 2024 16:26:55 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 17:33:35 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Borde', 'Haitz S\u00e1ez de Oc\u00e1riz', ''], ['Kratsios', 'Anastasis', ''], ['Law', 'Marc T.', ''], ['Dong', 'Xiaowen', ''], ['Bronstein', 'Michael', '']]","extracted_entities":"[{'text': 'undirected graph representation learning', 'label': 'Few-shot Learning'}, {'text': 'causality\\nembedding', 'label': 'Embedding'}, {'text': 'real-world network embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"real-world network embeddings","similarity_score":0.6517338157}
{"id":2408.16543,"submitter":"Cl\\'ementine Chazal","authors":"Cl\\'ementine Chazal, Anna Korba, Francis Bach","title":"Statistical and Geometrical properties of regularized Kernel\n  Kullback-Leibler divergence","comments":"Paper accepted to NeurIPS 2024","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG math.FA math.OC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this paper, we study the statistical and geometrical properties of the\nKullback-Leibler divergence with kernel covariance operators (KKL) introduced\nby Bach [2022]. Unlike the classical Kullback-Leibler (KL) divergence that\ninvolves density ratios, the KKL compares probability distributions through\ncovariance operators (embeddings) in a reproducible kernel Hilbert space\n(RKHS), and compute the Kullback-Leibler quantum divergence. This novel\ndivergence hence shares parallel but different aspects with both the standard\nKullback-Leibler between probability distributions and kernel embeddings\nmetrics such as the maximum mean discrepancy. A limitation faced with the\noriginal KKL divergence is its inability to be defined for distributions with\ndisjoint supports. To solve this problem, we propose in this paper a\nregularised variant that guarantees that the divergence is well defined for all\ndistributions. We derive bounds that quantify the deviation of the regularised\nKKL to the original one, as well as finite-sample bounds. In addition, we\nprovide a closed-form expression for the regularised KKL, specifically\napplicable when the distributions consist of finite sets of points, which makes\nit implementable. Furthermore, we derive a Wasserstein gradient descent scheme\nof the KKL divergence in the case of discrete distributions, and study\nempirically its properties to transport a set of points to a target\ndistribution.\n","versions":"[{'version': 'v1', 'created': 'Thu, 29 Aug 2024 14:01:30 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 12:23:23 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Chazal', 'Cl\u00e9mentine', ''], ['Korba', 'Anna', ''], ['Bach', 'Francis', '']]","extracted_entities":"[{'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2408.16783,"submitter":"Mikhail Borovoi","authors":"Mikhail Borovoi","title":"Is there a group structure on the Galois cohomology of a reductive group\n  over a global field?","comments":"6 pages, the final version to be published in Archiv der Mathematik.\n  This is a part of arXiv:2403.07659 to be published separately","journal-ref":null,"doi":null,"report-no":"MPIM-Bonn-2024","categories":"math.NT math.AG math.GR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Let K be a global field, that is, a number field or a global function field.\nIt is known that the answer to the question in the title over K is \"Yes\" when K\nhas no real embeddings. We show that otherwise the answer is \"No\". Namely, we\nshow that when K is a number field admitting a real embedding, it is impossible\nto define a group structure on the first Galois cohomology sets H^1(K,G) for\nall reductive K-groups G in a functorial way.\n","versions":"[{'version': 'v1', 'created': 'Mon, 19 Aug 2024 06:24:13 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 17:19:17 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Borovoi', 'Mikhail', '']]","extracted_entities":"[{'text': 'real embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"real embedding","similarity_score":0.8649382591}
{"id":2409.10419,"submitter":"Vineet Bhat","authors":"Vineet Bhat and Prashanth Krishnamurthy and Ramesh Karri and Farshad\n  Khorrami","title":"HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping\n  Using Vision-Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Robots interacting with humans through natural language can unlock numerous\napplications such as Referring Grasp Synthesis (RGS). Given a text query, RGS\ndetermines a stable grasp pose to manipulate the referred object in the robot's\nworkspace. RGS comprises two steps: visual grounding and grasp pose estimation.\nRecent studies leverage powerful Vision-Language Models (VLMs) for visually\ngrounding free-flowing natural language in real-world robotic execution.\nHowever, comparisons in complex, cluttered environments with multiple instances\nof the same object are lacking. This paper introduces HiFi-CS, featuring\nhierarchical application of Featurewise Linear Modulation (FiLM) to fuse image\nand text embeddings, enhancing visual grounding for complex attribute rich text\nqueries encountered in robotic grasping. Visual grounding associates an object\nin 2D\/3D space with natural language input and is studied in two scenarios:\nClosed and Open Vocabulary. HiFi-CS features a lightweight decoder combined\nwith a frozen VLM and outperforms competitive baselines in closed vocabulary\nsettings while being 100x smaller in size. Our model can effectively guide\nopen-set object detectors like GroundedSAM to enhance open-vocabulary\nperformance. We validate our approach through real-world RGS experiments using\na 7-DOF robotic arm, achieving 90.33\\% visual grounding accuracy in 15 tabletop\nscenes. Our codebase is provided here: https:\/\/github.com\/vineet2104\/hifics\n","versions":"[{'version': 'v1', 'created': 'Mon, 16 Sep 2024 15:50:39 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 21:30:37 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Bhat', 'Vineet', ''], ['Krishnamurthy', 'Prashanth', ''], ['Karri', 'Ramesh', ''], ['Khorrami', 'Farshad', '']]","extracted_entities":"[{'text': 'image\\nand text embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"image\nand text embeddings","similarity_score":0.7801387906}
{"id":2409.15949,"submitter":"Adithi Satish","authors":"Danqing Chen, Adithi Satish, Rasul Khanbayov, Carolin M. Schuster and\n  Georg Groh","title":"Tuning Into Bias: A Computational Study of Gender Bias in Song Lyrics","comments":"Accepted to be presented at the 9th Joint SIGHUM Workshop on\n  Computational Linguistics for Cultural Heritage, Social Sciences, Humanities\n  and Literature, co-located with NAACL 2025; also accepted and presented as\n  working paper at the SBP-BRiMS 2024 (see\n  https:\/\/sbp-brims.org\/2024\/papers\/working-papers\/Chen_SBP-BRiMS2024_Final_31.pdf\n  )","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The application of text mining methods is becoming increasingly prevalent,\nparticularly within Humanities and Computational Social Sciences, as well as in\na broader range of disciplines. This paper presents an analysis of gender bias\nin English song lyrics using topic modeling and bias measurement techniques.\nLeveraging BERTopic, we cluster a dataset of 537,553 English songs into\ndistinct topics and analyze their temporal evolution. Our results reveal a\nsignificant thematic shift in song lyrics over time, transitioning from\nromantic themes to a heightened focus on the sexualization of women.\nAdditionally, we observe a substantial prevalence of profanity and misogynistic\ncontent across various topics, with a particularly high concentration in the\nlargest thematic cluster. To further analyse gender bias across topics and\ngenres in a quantitative way, we employ the Single Category Word Embedding\nAssociation Test (SC-WEAT) to calculate bias scores for word embeddings trained\non the most prominent topics as well as individual genres. The results indicate\na consistent male bias in words associated with intelligence and strength,\nwhile appearance and weakness words show a female bias. Further analysis\nhighlights variations in these biases across topics, illustrating the interplay\nbetween thematic content and gender stereotypes in song lyrics.\n","versions":"[{'version': 'v1', 'created': 'Tue, 24 Sep 2024 10:24:53 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 20:54:07 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Chen', 'Danqing', ''], ['Satish', 'Adithi', ''], ['Khanbayov', 'Rasul', ''], ['Schuster', 'Carolin M.', ''], ['Groh', 'Georg', '']]","extracted_entities":"[{'text': 'BERTopic', 'label': 'BERT'}, {'text': 'word embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"word embeddings","similarity_score":0.742625773}
{"id":2410.18857,"submitter":"Sanghyuk Chun","authors":"Sanghyuk Chun and Wonjae Kim and Song Park and Sangdoo Yun","title":"Probabilistic Language-Image Pre-Training","comments":"Code: https:\/\/github.com\/naver-ai\/prolip HuggingFace Hub:\n  https:\/\/huggingface.co\/collections\/SanghyukChun\/prolip-6712595dfc87fd8597350291\n  33 pages, 4.8 MB; LongProLIP paper: arXiv:2503.08048","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Vision-language models (VLMs) embed aligned image-text pairs into a joint\nspace but often rely on deterministic embeddings, assuming a one-to-one\ncorrespondence between images and texts. This oversimplifies real-world\nrelationships, which are inherently many-to-many, with multiple captions\ndescribing a single image and vice versa. We introduce Probabilistic\nLanguage-Image Pre-training (ProLIP), the first probabilistic VLM pre-trained\non a billion-scale image-text dataset using only probabilistic objectives,\nachieving a strong zero-shot capability (e.g., 74.6% ImageNet zero-shot\naccuracy with ViT-B\/16). ProLIP efficiently estimates uncertainty by an\n\"uncertainty token\" without extra parameters. We also introduce a novel\ninclusion loss that enforces distributional inclusion relationships between\nimage-text pairs and between original and masked inputs. Experiments\ndemonstrate that, by leveraging uncertainty estimates, ProLIP benefits\ndownstream tasks and aligns with intuitive notions of uncertainty, e.g.,\nshorter texts being more uncertain and more general inputs including specific\nones. Utilizing text uncertainties, we further improve ImageNet accuracy from\n74.6% to 75.8% (under a few-shot setting), supporting the practical advantages\nof our probabilistic approach. The code is available at\nhttps:\/\/github.com\/naver-ai\/prolip\n","versions":"[{'version': 'v1', 'created': 'Thu, 24 Oct 2024 15:42:25 GMT'}, {'version': 'v2', 'created': 'Fri, 6 Dec 2024 15:20:28 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 14:03:31 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Chun', 'Sanghyuk', ''], ['Kim', 'Wonjae', ''], ['Park', 'Song', ''], ['Yun', 'Sangdoo', '']]","extracted_entities":"[{'text': 'deterministic embeddings', 'label': 'Embedding'}, {'text': 'ProLIP', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'ProLIP', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'ProLIP', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'prolip', 'label': 'Generative Pre-trained Transformer (GPT)'}]","assigned_concept":"Embedding","matched_keyword":"deterministic embeddings","similarity_score":0.6727190018}
{"id":2410.1959,"submitter":"Fanqi Pu","authors":"Fanqi Pu, Yifan Wang, Jiru Deng, Wenming Yang","title":"MonoDGP: Monocular 3D Object Detection with Decoupled-Query and\n  Geometry-Error Priors","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Perspective projection has been extensively utilized in monocular 3D object\ndetection methods. It introduces geometric priors from 2D bounding boxes and 3D\nobject dimensions to reduce the uncertainty of depth estimation. However, due\nto depth errors originating from the object's visual surface, the height of the\nbounding box often fails to represent the actual projected central height,\nwhich undermines the effectiveness of geometric depth. Direct prediction for\nthe projected height unavoidably results in a loss of 2D priors, while\nmulti-depth prediction with complex branches does not fully leverage geometric\ndepth. This paper presents a Transformer-based monocular 3D object detection\nmethod called MonoDGP, which adopts perspective-invariant geometry errors to\nmodify the projection formula. We also try to systematically discuss and\nexplain the mechanisms and efficacy behind geometry errors, which serve as a\nsimple but effective alternative to multi-depth prediction. Additionally,\nMonoDGP decouples the depth-guided decoder and constructs a 2D decoder only\ndependent on visual features, providing 2D priors and initializing object\nqueries without the disturbance of 3D detection. To further optimize and\nfine-tune input tokens of the transformer decoder, we also introduce a Region\nSegment Head (RSH) that generates enhanced features and segment embeddings. Our\nmonocular method demonstrates state-of-the-art performance on the KITTI\nbenchmark without extra data. Code is available at\nhttps:\/\/github.com\/PuFanqi23\/MonoDGP.\n","versions":"[{'version': 'v1', 'created': 'Fri, 25 Oct 2024 14:31:43 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 14:48:22 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Pu', 'Fanqi', ''], ['Wang', 'Yifan', ''], ['Deng', 'Jiru', ''], ['Yang', 'Wenming', '']]","extracted_entities":"[{'text': 'MonoDGP', 'label': 'Transformer-based model'}, {'text': 'MonoDGP', 'label': 'Transformer-based model'}, {'text': 'segment embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"segment embeddings","similarity_score":0.741707921}
{"id":2410.21931,"submitter":"Assaf Naor","authors":"Alan Chang, Assaf Naor, Kevin Ren","title":"Random zero sets with local growth guarantees","comments":"added Section 1.1 (informal overview), the rest of the material is\n  the same","journal-ref":null,"doi":null,"report-no":null,"categories":"math.MG cs.DS math.FA","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We prove that if $(\\mathcal{M},d)$ is an $n$-point metric space that embeds\nquasisymmetrically into a Hilbert space, then for every $\\tau>0$ there is a\nrandom subset $\\mathcal{Z}$ of $\\mathcal{M}$ such that for any pair of points\n$x,y\\in \\mathcal{M}$ with $d(x,y)\\ge \\tau$, the probability that both $x\\in\n\\mathcal{Z}$ and $d(y,\\mathcal{Z})\\ge \\beta\\tau\/\\sqrt{1+\\log (|B(y,\\kappa \\beta\n\\tau)|\/|B(y,\\beta \\tau)|)}$ is $\\Omega(1)$, where $\\kappa>1$ is a universal\nconstant and $\\beta>0$ depends only on the modulus of the quasisymmetric\nembedding. The proof relies on a refinement of the Arora--Rao--Vazirani\nrounding technique. Among the applications of this result is that the largest\npossible Euclidean distortion of an $n$-point subset of $\\ell_1$ is\n$\\Theta(\\sqrt{\\log n})$, and the integrality gap of the Goemans--Linial\nsemidefinite program for the Sparsest Cut problem on inputs of size $n$ is\n$\\Theta(\\sqrt{\\log n})$. Multiple further applications are given.\n","versions":"[{'version': 'v1', 'created': 'Tue, 29 Oct 2024 10:47:12 GMT'}, {'version': 'v2', 'created': 'Wed, 27 Nov 2024 13:58:37 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 20:51:51 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Chang', 'Alan', ''], ['Naor', 'Assaf', ''], ['Ren', 'Kevin', '']]","extracted_entities":"[{'text': 'quasisymmetric\\nembedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"quasisymmetric\nembedding","similarity_score":0.6111426353}
{"id":2411.03508,"submitter":"Yuri  Zarhin G.","authors":"Boris M. Bekker and Yuri G. Zarhin","title":"Torsion points of small order on cyclic covers of $\\mathbb P^1$","comments":"23 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Let $d\\geq 2$ be a positive integer, $K$ an algebraically closed field of\ncharacteristic not dividing $d$, $n\\geq d+1$ a positive integer that is prime\nto $d$, $f(x)\\in K[x]$ a degree $n$ monic polynomial without multiple roots,\n$C_{f,d}: y^d=f(x)$ the corresponding smooth plane affine curve over $K$,\n$\\mathcal{C}_{f,d}$ a smooth projective model of $C_{f,d}$ and\n$J(\\mathcal{C}_{f,d})$ the Jacobian of $\\mathcal{C}_{f,d} $. We identify\n$\\mathcal{C}_{f,d}$ with the image of its canonical embedding into\n$J(\\mathcal{C}_{f,d})$ (such that the infinite point of $\\mathcal{C}_{f,d}$\ngoes to the zero of the group law on $J(\\mathcal{C}_{f,d})$).\n  Earlier the second named author proved that if $d=2$ and $n=2g+1 \\ge 5$ then\nthe genus $g$ hyperelliptic curve $\\mathcal{C}_{f,2}$ contains no points of\norders lying between $3$ and $n-1=2g$.\n  In the present paper we generalize this result to the case of arbitrary $d$.\nNamely, we prove that if $P$ is a point of order $m>1$ on $\\mathcal{C}_{f,d}$,\nthen either $m=d$ or $m\\geq n$. We also describe all curves $\\mathcal{C}_{f,d}$\nhaving a point of order $n$.\n","versions":"[{'version': 'v1', 'created': 'Tue, 5 Nov 2024 20:54:41 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 23:43:43 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Bekker', 'Boris M.', ''], ['Zarhin', 'Yuri G.', '']]","extracted_entities":"[{'text': 'canonical embedding', 'label': 'Embedding'}, {'text': 'group law', 'label': 'Scaling law'}]","assigned_concept":"Embedding","matched_keyword":"canonical embedding","similarity_score":0.746309638}
{"id":2412.02692,"submitter":"Fengyuan Shi","authors":"Fengyuan Shi, Zhuoyan Luo, Yixiao Ge, Yujiu Yang, Ying Shan, Limin\n  Wang","title":"Scalable Image Tokenization with Index Backpropagation Quantization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Existing vector quantization (VQ) methods struggle with scalability, largely\nattributed to the instability of the codebook that undergoes partial updates\nduring training. The codebook is prone to collapse as utilization decreases,\ndue to the progressively widening distribution gap between non-activated codes\nand visual features. To solve the problem, we propose Index Backpropagation\nQuantization (IBQ), a new VQ method for the joint optimization of all codebook\nembeddings and the visual encoder. Applying a straight-through estimator on the\none-hot categorical distribution between the encoded feature and codebook, all\ncodes are differentiable and maintain a consistent latent space with the visual\nencoder. IBQ enables scalable training of visual tokenizers and, for the first\ntime, achieves a large-scale codebook ($2^{18}$) with high dimension ($256$)\nand high utilization. Experiments on the standard ImageNet benchmark\ndemonstrate the scalability and superiority of IBQ, achieving competitive\nresults on reconstruction and the application of autoregressive visual\ngeneration. The code and models are available at\nhttps:\/\/github.com\/TencentARC\/SEED-Voken.\n","versions":"[{'version': 'v1', 'created': 'Tue, 3 Dec 2024 18:59:10 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 09:01:48 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Shi', 'Fengyuan', ''], ['Luo', 'Zhuoyan', ''], ['Ge', 'Yixiao', ''], ['Yang', 'Yujiu', ''], ['Shan', 'Ying', ''], ['Wang', 'Limin', '']]","extracted_entities":"[{'text': 'Index Backpropagation\\nQuantization', 'label': 'quantisation'}, {'text': 'codebook\\nembeddings', 'label': 'Embedding'}, {'text': 'IBQ', 'label': 'quantisation'}, {'text': 'IBQ', 'label': 'quantisation'}]","assigned_concept":"Embedding","matched_keyword":"codebook\nembeddings","similarity_score":0.6539586186}
{"id":2412.03059,"submitter":"Runjian Chen","authors":"Runjian Chen, Hang Zhang, Avinash Ravichandran, Hyoungseob Park, Wenqi\n  Shao, Alex Wong, Ping Luo","title":"CLAP: Unsupervised 3D Representation Learning for Fusion 3D Perception\n  via Curvature Sampling and Prototype Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Unsupervised 3D representation learning reduces the burden of labeling\nmultimodal 3D data for fusion perception tasks. Among different pre-training\nparadigms, differentiable-rendering-based methods have shown most promise.\nHowever, existing works separately conduct pre-training for each modalities due\nto computational costs of processing large point clouds with images. As such,\nmutual benefit of high-level semantics (from image) and 3D structure (from\npoint cloud) has not been exploited. To address this gap, we propose a joint\nunsupervised differentiable-rendering-based pre-training method for images and\npoint clouds, termed CLAP, short for Curvature sampLing and leArnable\nPrototype. Specifically, our method overcomes the computational hurdle by\nCurvature Sampling to select the more informative points\/pixels for\npre-training. To uncover the performance benefits brought by their\ncomplementarity, we propose to use learnable prototypes to represent parts of\nthe 3D scenes in a common feature space and an Expectation-Maximization\ntraining scheme to associate embeddings of each modality to prototypes. We\nfurther propose a swapping prediction loss that explores their interplay\nthrough prototypes along with a Gram Matrix Regularization term to maintain\ntraining stability. Experiments on NuScenes and Waymo datasets show that CLAP\nachieves up to 100% more performance gain as compared to previous SOTA\npre-training methods. Codes and models will be released.\n","versions":"[{'version': 'v1', 'created': 'Wed, 4 Dec 2024 06:26:12 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 03:54:25 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Chen', 'Runjian', ''], ['Zhang', 'Hang', ''], ['Ravichandran', 'Avinash', ''], ['Park', 'Hyoungseob', ''], ['Shao', 'Wenqi', ''], ['Wong', 'Alex', ''], ['Luo', 'Ping', '']]","extracted_entities":"[{'text': 'Unsupervised 3D representation learning', 'label': 'Few-shot Learning'}, {'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2412.04766,"submitter":"Shadab Ahamed","authors":"Shadab Ahamed, Eldad Haber","title":"DAWN-FM: Data-Aware and Noise-Informed Flow Matching for Solving Inverse\n  Problems","comments":"27 pages, 11 figures, 6 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.AI cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Inverse problems, which involve estimating parameters from incomplete or\nnoisy observations, arise in various fields such as medical imaging,\ngeophysics, and signal processing. These problems are often ill-posed,\nrequiring regularization techniques to stabilize the solution. In this work, we\nemploy Flow Matching (FM), a generative framework that integrates a\ndeterministic processes to map a simple reference distribution, such as a\nGaussian, to the target distribution. Our method DAWN-FM: Data-AWare and\nNoise-informed Flow Matching incorporates data and noise embedding, allowing\nthe model to access representations about the measured data explicitly and also\naccount for noise in the observations, making it particularly robust in\nscenarios where data is noisy or incomplete. By learning a time-dependent\nvelocity field, FM not only provides accurate solutions but also enables\nuncertainty quantification by generating multiple plausible outcomes. Unlike\npre-trained diffusion models, which may struggle in highly ill-posed settings,\nour approach is trained specifically for each inverse problem and adapts to\nvarying noise levels. We validate the effectiveness and robustness of our\nmethod through extensive numerical experiments on tasks such as image\ndeblurring and tomography.\n","versions":"[{'version': 'v1', 'created': 'Fri, 6 Dec 2024 04:18:49 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 17:30:41 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Ahamed', 'Shadab', ''], ['Haber', 'Eldad', '']]","extracted_entities":"[{'text': 'noise embedding', 'label': 'Embedding'}, {'text': 'uncertainty quantification', 'label': 'quantisation'}]","assigned_concept":"Embedding","matched_keyword":"noise embedding","similarity_score":0.6727086902}
{"id":2412.09165,"submitter":"Zhijie Nie","authors":"Zhijie Nie, Zhangchi Feng, Mingxin Li, Cunwang Zhang, Yanzhao Zhang,\n  Dingkun Long, Richong Zhang","title":"When Text Embedding Meets Large Language Model: A Comprehensive Survey","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.IR","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications-such as semantic matching, clustering,\nand information retrieval-continue to rely on text embeddings for their\nefficiency and effectiveness. Therefore, how to combine the LLMs and the text\nembeddings has become one of the hotspots of academic attention in recent\nyears. In this survey, we categorize the interplay between LLMs and text\nembeddings into three overarching themes: (1) LLM-augmented text embedding,\nenhancing traditional embedding methods with LLMs; (2) LLMs as text embedders,\nadapting their innate capabilities for high-quality embedding; and (3) Text\nembedding understanding with LLMs, leveraging LLMs to analyze and interpret\nembeddings. By organizing recent works based on interaction patterns rather\nthan specific downstream applications, we offer a novel and systematic overview\nof contributions from various research and application domains in the era of\nLLMs. Furthermore, we highlight the unresolved challenges that persisted in the\npre-LLM era with pre-trained language models (PLMs) and explore the emerging\nobstacles brought forth by LLMs. Building on this analysis, we outline\nprospective directions for the evolution of text embedding, addressing both\ntheoretical and practical opportunities in the rapidly advancing landscape of\nNLP.\n","versions":"[{'version': 'v1', 'created': 'Thu, 12 Dec 2024 10:50:26 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 16:11:43 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Nie', 'Zhijie', ''], ['Feng', 'Zhangchi', ''], ['Li', 'Mingxin', ''], ['Zhang', 'Cunwang', ''], ['Zhang', 'Yanzhao', ''], ['Long', 'Dingkun', ''], ['Zhang', 'Richong', '']]","extracted_entities":"[{'text': 'Text embedding', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'text embeddings', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'text\\nembeddings', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'text\\nembeddings', 'label': 'Embedding'}, {'text': 'text embedding', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Text\\nembedding', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'text embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"Text embedding","similarity_score":0.8247289658}
{"id":2412.16919,"submitter":"Xuying Zhang","authors":"Xuying Zhang and Yutong Liu and Yangguang Li and Renrui Zhang and\n  Yufei Liu and Kai Wang and Wanli Ouyang and Zhiwei Xiong and Peng Gao and\n  Qibin Hou and Ming-Ming Cheng","title":"TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present TAR3D, a novel framework that consists of a 3D-aware Vector\nQuantized-Variational AutoEncoder (VQ-VAE) and a Generative Pre-trained\nTransformer (GPT) to generate high-quality 3D assets. The core insight of this\nwork is to migrate the multimodal unification and promising learning\ncapabilities of the next-token prediction paradigm to conditional 3D object\ngeneration. To achieve this, the 3D VQ-VAE first encodes a wide range of 3D\nshapes into a compact triplane latent space and utilizes a set of discrete\nrepresentations from a trainable codebook to reconstruct fine-grained\ngeometries under the supervision of query point occupancy. Then, the 3D GPT,\nequipped with a custom triplane position embedding called TriPE, predicts the\ncodebook index sequence with prefilling prompt tokens in an autoregressive\nmanner so that the composition of 3D geometries can be modeled part by part.\nExtensive experiments on ShapeNet and Objaverse demonstrate that TAR3D can\nachieve superior generation quality over existing methods in text-to-3D and\nimage-to-3D tasks\n","versions":"[{'version': 'v1', 'created': 'Sun, 22 Dec 2024 08:28:20 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 03:21:56 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Zhang', 'Xuying', ''], ['Liu', 'Yutong', ''], ['Li', 'Yangguang', ''], ['Zhang', 'Renrui', ''], ['Liu', 'Yufei', ''], ['Wang', 'Kai', ''], ['Ouyang', 'Wanli', ''], ['Xiong', 'Zhiwei', ''], ['Gao', 'Peng', ''], ['Hou', 'Qibin', ''], ['Cheng', 'Ming-Ming', '']]","extracted_entities":"[{'text': 'GPT', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'GPT', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'triplane position embedding', 'label': 'Embedding'}, {'text': 'TriPE', 'label': 'contextual Embedding'}, {'text': 'prefilling prompt tokens', 'label': 'Prompting'}]","assigned_concept":"Embedding","matched_keyword":"triplane position embedding","similarity_score":0.5317654014}
{"id":2501.10157,"submitter":"Jie Wen","authors":"Jinrong Cui, Xiaohuang Wu, Haitao Zhang, Chongjie Dong, Jie Wen","title":"Structure-guided Deep Multi-View Clustering","comments":"We have found that our paper has many imperfections and incorrect\n  formulas and derivations, and we insist on retracting the manuscript in order\n  to avoid misleading readers","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Deep multi-view clustering seeks to utilize the abundant information from\nmultiple views to improve clustering performance. However, most of the existing\nclustering methods often neglect to fully mine multi-view structural\ninformation and fail to explore the distribution of multi-view data, limiting\nclustering performance. To address these limitations, we propose a\nstructure-guided deep multi-view clustering model. Specifically, we introduce a\npositive sample selection strategy based on neighborhood relationships, coupled\nwith a corresponding loss function. This strategy constructs multi-view nearest\nneighbor graphs to dynamically redefine positive sample pairs, enabling the\nmining of local structural information within multi-view data and enhancing the\nreliability of positive sample selection. Additionally, we introduce a Gaussian\ndistribution model to uncover latent structural information and introduce a\nloss function to reduce discrepancies between view embeddings. These two\nstrategies explore multi-view structural information and data distribution from\ndifferent perspectives, enhancing consistency across views and increasing\nintra-cluster compactness. Experimental evaluations demonstrate the efficacy of\nour method, showing significant improvements in clustering performance on\nmultiple benchmark datasets compared to state-of-the-art multi-view clustering\napproaches.\n","versions":"[{'version': 'v1', 'created': 'Fri, 17 Jan 2025 12:42:30 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 13:49:58 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Cui', 'Jinrong', ''], ['Wu', 'Xiaohuang', ''], ['Zhang', 'Haitao', ''], ['Dong', 'Chongjie', ''], ['Wen', 'Jie', '']]","extracted_entities":"[{'text': 'view embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"view embeddings","similarity_score":0.7708809376}
{"id":2501.12673,"submitter":"Daniel Ruberman","authors":"Dave Auckly, Daniel Ruberman","title":"Exotic families of embeddings","comments":"25 page, 9 figures. Added acknowledgment to 2nd version","journal-ref":"Frontiers in geometry and topology, Proc. Sympos. Pure Math., 109,\n  71--98, (2024) Amer. Math. Soc., Providence, RI","doi":null,"report-no":null,"categories":"math.GT","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We construct a number of topologically trivial but smoothly non-trivial\nfamilies of embeddings of 3-manifolds in 4-manifolds. These include embeddings\nof homology spheres in $S^4$ that are not isotopic but have diffeomorphic\ncomplements, and families (parameterized by high-dimensional spheres) of\nembeddings of any 3-manifold that embeds in a blown-up K3 surface. In each\ncase, the families are constructed so as to be topologically trivial in an\nappropriate sense. We also illustrate a general technique for converting a\nnon-trivial family of embeddings into a non-trivial family of submanifolds.\n","versions":"[{'version': 'v1', 'created': 'Wed, 22 Jan 2025 06:16:27 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 12:40:53 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Auckly', 'Dave', ''], ['Ruberman', 'Daniel', '']]","extracted_entities":"[{'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2501.13352,"submitter":"Tianyuan Yao","authors":"Tianyuan Yao, Zhiyuan Li, Praitayini Kanakaraj, Derek B. Archer, Kurt\n  Schilling, Lori Beason-Held, Susan Resnick, Bennett A. Landman, Yuankai Huo","title":"Polyhedra Encoding Transformers: Enhancing Diffusion MRI Analysis Beyond\n  Voxel and Volumetric Embedding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Diffusion-weighted Magnetic Resonance Imaging (dMRI) is an essential tool in\nneuroimaging. It is arguably the sole noninvasive technique for examining the\nmicrostructural properties and structural connectivity of the brain. Recent\nyears have seen the emergence of machine learning and data-driven approaches\nthat enhance the speed, accuracy, and consistency of dMRI data analysis.\nHowever, traditional deep learning models often fell short, as they typically\nutilize pixel-level or volumetric patch-level embeddings similar to those used\nin structural MRI, and do not account for the unique distribution of various\ngradient encodings. In this paper, we propose a novel method called Polyhedra\nEncoding Transformer (PE-Transformer) for dMRI, designed specifically to handle\nspherical signals. Our approach involves projecting an icosahedral polygon onto\na unit sphere to resample signals from predetermined directions. These\nresampled signals are then transformed into embeddings, which are processed by\na transformer encoder that incorporates orientational information reflective of\nthe icosahedral structure. Through experimental validation with various\ngradient encoding protocols, our method demonstrates superior accuracy in\nestimating multi-compartment models and Fiber Orientation Distributions (FOD),\noutperforming both conventional CNN architectures and standard transformers.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 Jan 2025 03:32:52 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 19:37:53 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Yao', 'Tianyuan', ''], ['Li', 'Zhiyuan', ''], ['Kanakaraj', 'Praitayini', ''], ['Archer', 'Derek B.', ''], ['Schilling', 'Kurt', ''], ['Beason-Held', 'Lori', ''], ['Resnick', 'Susan', ''], ['Landman', 'Bennett A.', ''], ['Huo', 'Yuankai', '']]","extracted_entities":"[{'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'standard transformers', 'label': 'Transformers'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2501.15988,"submitter":"K. Dunnett","authors":"K. Dunnett and M. H. Magnusson","title":"Qualitative observations in university physics laboratories: an example\n  from classical mechanics","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.ed-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  One of the key skills of a researcher is noticing what's going on. Both in\nthe experiment one's performing and in one's data: is there something\ninteresting, reason to doubt one's data or suspect that one's theoretical\ndescription is insufficient? Many experiments developed for undergraduate\nteaching still focus on quantitative evaluation. Here we take an alternative\napproach where careful observation identifies the interesting qualitative\nbehaviour of a ball dropped with a water bottle balanced on top of it, but\nwhere numerical agreement with a simple theoretical model is impossible. Thus\n'success' occurs when students are satisfied with their efforts and the\ndevelopment of their experimental process. Laboratory note keeping can also be\nintroduced in a meaningful, non-formulaic way since students are making\nindependent observations and method changes. We describe pedagogical and\ndidactic considerations for the implementation of the experiment in a\nclassroom, including variations and extensions, and give examples of\nexperimental outcomes. We suggest that considering qualitative behaviour may be\na fruitful strategy for identifying experiments that are both amenable to\nstudent autonomy and embedding skills such as laboratory note keeping in a\nflexible and genuine way.\n","versions":"[{'version': 'v1', 'created': 'Mon, 27 Jan 2025 12:15:56 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Jan 2025 07:07:09 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 08:36:24 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Dunnett', 'K.', ''], ['Magnusson', 'M. H.', '']]","extracted_entities":"[{'text': 'embedding skills', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embedding skills","similarity_score":0.697471261}
{"id":2502.13763,"submitter":"Eva Zangerle","authors":"Andreas Peintner and Marta Moscati and Emilia Parada-Cabaleiro and\n  Markus Schedl and Eva Zangerle","title":"Unsupervised Graph Embeddings for Session-based Recommendation with Item\n  Features","comments":"Paper accepted at CARS: Workshop on Context-Aware Recommender Systems\n  at the 16th ACM Conference on Recommender Systems (RecSys) 2022","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In session-based recommender systems, predictions are based on the user's\npreceding behavior in the session. State-of-the-art sequential recommendation\nalgorithms either use graph neural networks to model sessions in a graph or\nleverage the similarity of sessions by exploiting item features. In this paper,\nwe combine these two approaches and propose a novel method, Graph Convolutional\nNetwork Extension (GCNext), which incorporates item features directly into the\ngraph representation via graph convolutional networks. GCNext creates a\nfeature-rich item co-occurrence graph and learns the corresponding item\nembeddings in an unsupervised manner. We show on three datasets that\nintegrating GCNext into sequential recommendation algorithms significantly\nboosts the performance of nearest-neighbor methods as well as neural network\nmodels. Our flexible extension is easy to incorporate in state-of-the-art\nmethods and increases the MRR@20 by up to 12.79%.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Feb 2025 14:23:18 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 18:52:16 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Peintner', 'Andreas', ''], ['Moscati', 'Marta', ''], ['Parada-Cabaleiro', 'Emilia', ''], ['Schedl', 'Markus', ''], ['Zangerle', 'Eva', '']]","extracted_entities":"[{'text': 'item\\nembeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"item\nembeddings","similarity_score":0.6796104312}
{"id":2502.13833,"submitter":"Milton Nicol\\'as Plasencia Palacios","authors":"Milton Nicol\\'as Plasencia Palacios, Sebastiano Saccani, Gabriele\n  Sgroi, Alexander Boudewijn and Luca Bortolussi","title":"Contrastive Learning-Based privacy metrics in Tabular Synthetic Datasets","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Synthetic data has garnered attention as a Privacy Enhancing Technology (PET)\nin sectors such as healthcare and finance. When using synthetic data in\npractical applications, it is important to provide protection guarantees. In\nthe literature, two family of approaches are proposed for tabular data: on the\none hand, Similarity-based methods aim at finding the level of similarity\nbetween training and synthetic data. Indeed, a privacy breach can occur if the\ngenerated data is consistently too similar or even identical to the train data.\nOn the other hand, Attack-based methods conduce deliberate attacks on synthetic\ndatasets. The success rates of these attacks reveal how secure the synthetic\ndatasets are.\n  In this paper, we introduce a contrastive method that improves privacy\nassessment of synthetic datasets by embedding the data in a more representative\nspace. This overcomes obstacles surrounding the multitude of data types and\nattributes. It also makes the use of intuitive distance metrics possible for\nsimilarity measurements and as an attack vector. In a series of experiments\nwith publicly available datasets, we compare the performances of\nsimilarity-based and attack-based methods, both with and without use of the\ncontrastive learning-based embeddings. Our results show that relatively\nefficient, easy to implement privacy metrics can perform equally well as more\nadvanced metrics explicitly modeling conditions for privacy referred to by the\nGDPR.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Feb 2025 15:52:23 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 09:01:19 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Palacios', 'Milton Nicol\u00e1s Plasencia', ''], ['Saccani', 'Sebastiano', ''], ['Sgroi', 'Gabriele', ''], ['Boudewijn', 'Alexander', ''], ['Bortolussi', 'Luca', '']]","extracted_entities":"[{'text': 'Similarity-based methods', 'label': 'Embedding'}, {'text': 'Attack-based methods', 'label': 'Embedding'}, {'text': 'publicly available datasets', 'label': 'Open-source LLMs'}, {'text': 'contrastive learning-based embeddings', 'label': 'Embedding'}, {'text': 'GDPR', 'label': 'AI Ethics'}]","assigned_concept":"Embedding","matched_keyword":"contrastive learning-based embeddings","similarity_score":0.6816989779}
{"id":2502.15602,"submitter":"Yoonjin Chung","authors":"Yoonjin Chung, Pilsun Eu, Junwon Lee, Keunwoo Choi, Juhan Nam, Ben\n  Sangbae Chon","title":"KAD: No More FAD! An Effective and Efficient Evaluation Metric for Audio\n  Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.AI cs.LG eess.AS","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Although being widely adopted for evaluating generated audio signals, the\nFr\\'echet Audio Distance (FAD) suffers from significant limitations, including\nreliance on Gaussian assumptions, sensitivity to sample size, and high\ncomputational complexity. As an alternative, we introduce the Kernel Audio\nDistance (KAD), a novel, distribution-free, unbiased, and computationally\nefficient metric based on Maximum Mean Discrepancy (MMD). Through analysis and\nempirical validation, we demonstrate KAD's advantages: (1) faster convergence\nwith smaller sample sizes, enabling reliable evaluation with limited data; (2)\nlower computational cost, with scalable GPU acceleration; and (3) stronger\nalignment with human perceptual judgments. By leveraging advanced embeddings\nand characteristic kernels, KAD captures nuanced differences between real and\ngenerated audio. Open-sourced in the kadtk toolkit, KAD provides an efficient,\nreliable, and perceptually aligned benchmark for evaluating generative audio\nmodels.\n","versions":"[{'version': 'v1', 'created': 'Fri, 21 Feb 2025 17:19:15 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 06:46:13 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Chung', 'Yoonjin', ''], ['Eu', 'Pilsun', ''], ['Lee', 'Junwon', ''], ['Choi', 'Keunwoo', ''], ['Nam', 'Juhan', ''], ['Chon', 'Ben Sangbae', '']]","extracted_entities":"[{'text': 'advanced embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"advanced embeddings","similarity_score":0.8284353614}
{"id":2503.06094,"submitter":"Yong He","authors":"Yong He, Hongshan Yu, Mingtao Feng, Tongjia Chen, Zechuan Li, Anwaar\n  Ulhaq, Saeed Anwar, Ajmal Saeed Mian","title":"PointDiffuse: A Dual-Conditional Diffusion Model for Enhanced Point\n  Cloud Semantic Segmentation","comments":"8 pages, 3 figures, 7 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Diffusion probabilistic models are traditionally used to generate colors at\nfixed pixel positions in 2D images. Building on this, we extend diffusion\nmodels to point cloud semantic segmentation, where point positions also remain\nfixed, and the diffusion model generates point labels instead of colors. To\naccelerate the denoising process in reverse diffusion, we introduce a noisy\nlabel embedding mechanism. This approach integrates semantic information into\nthe noisy label, providing an initial semantic reference that improves the\nreverse diffusion efficiency. Additionally, we propose a point frequency\ntransformer that enhances the adjustment of high-level context in point clouds.\nTo reduce computational complexity, we introduce the position condition into\nMLP and propose denoising PointNet to process the high-resolution point cloud\nwithout sacrificing geometric details. Finally, we integrate the proposed noisy\nlabel embedding, point frequency transformer and denoising PointNet in our\nproposed dual conditional diffusion model-based network (PointDiffuse) to\nperform large-scale point cloud semantic segmentation. Extensive experiments on\nfive benchmarks demonstrate the superiority of PointDiffuse, achieving the\nstate-of-the-art mIoU of 74.2\\% on S3DIS Area 5, 81.2\\% on S3DIS 6-fold and\n64.8\\% on SWAN dataset.\n","versions":"[{'version': 'v1', 'created': 'Sat, 8 Mar 2025 06:53:22 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 14:59:28 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['He', 'Yong', ''], ['Yu', 'Hongshan', ''], ['Feng', 'Mingtao', ''], ['Chen', 'Tongjia', ''], ['Li', 'Zechuan', ''], ['Ulhaq', 'Anwaar', ''], ['Anwar', 'Saeed', ''], ['Mian', 'Ajmal Saeed', '']]","extracted_entities":"[{'text': 'noisy\\nlabel embedding mechanism', 'label': 'Embedding'}, {'text': 'noisy\\nlabel embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"noisy\nlabel embedding","similarity_score":0.5850905776}
{"id":2503.06277,"submitter":"Siyi Du","authors":"Siyi Du, Xinzhe Luo, Declan P. O'Regan, Chen Qin","title":"STiL: Semi-supervised Tabular-Image Learning for Comprehensive\n  Task-Relevant Information Exploration in Multimodal Classification","comments":"16 pages (including 5 pages of supplementary materials), accepted by\n  CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multimodal image-tabular learning is gaining attention, yet it faces\nchallenges due to limited labeled data. While earlier work has applied\nself-supervised learning (SSL) to unlabeled data, its task-agnostic nature\noften results in learning suboptimal features for downstream tasks.\nSemi-supervised learning (SemiSL), which combines labeled and unlabeled data,\noffers a promising solution. However, existing multimodal SemiSL methods\ntypically focus on unimodal or modality-shared features, ignoring valuable\ntask-relevant modality-specific information, leading to a Modality Information\nGap. In this paper, we propose STiL, a novel SemiSL tabular-image framework\nthat addresses this gap by comprehensively exploring task-relevant information.\nSTiL features a new disentangled contrastive consistency module to learn\ncross-modal invariant representations of shared information while retaining\nmodality-specific information via disentanglement. We also propose a novel\nconsensus-guided pseudo-labeling strategy to generate reliable pseudo-labels\nbased on classifier consensus, along with a new prototype-guided label\nsmoothing technique to refine pseudo-label quality with prototype embeddings,\nthereby enhancing task-relevant information learning in unlabeled data.\nExperiments on natural and medical image datasets show that STiL outperforms\nthe state-of-the-art supervised\/SSL\/SemiSL image\/multimodal approaches. Our\ncode is available at https:\/\/github.com\/siyi-wind\/STiL.\n","versions":"[{'version': 'v1', 'created': 'Sat, 8 Mar 2025 16:51:45 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 18:40:36 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Du', 'Siyi', ''], ['Luo', 'Xinzhe', ''], [\"O'Regan\", 'Declan P.', ''], ['Qin', 'Chen', '']]","extracted_entities":"[{'text': 'Multimodal image-tabular learning', 'label': 'Few-shot Learning'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'Semi-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'prototype embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"prototype embeddings","similarity_score":0.7434612513}
{"id":2503.06378,"submitter":"Lexin Zhou","authors":"Lexin Zhou, Lorenzo Pacchiardi, Fernando Mart\\'inez-Plumed, Katherine\n  M. Collins, Yael Moros-Daval, Seraphina Zhang, Qinlin Zhao, Yitian Huang,\n  Luning Sun, Jonathan E. Prunty, Zongqian Li, Pablo S\\'anchez-Garc\\'ia, Kexin\n  Jiang Chen, Pablo A. M. Casares, Jiyun Zu, John Burden, Behzad Mehrbakhsh,\n  David Stillwell, Manuel Cebrian, Jindong Wang, Peter Henderson, Sherry\n  Tongshuang Wu, Patrick C. Kyllonen, Lucy Cheke, Xing Xie, Jos\\'e\n  Hern\\'andez-Orallo","title":"General Scales Unlock AI Evaluation with Explanatory and Predictive\n  Power","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.CY","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Ensuring safe and effective use of AI requires understanding and anticipating\nits performance on novel tasks, from advanced scientific challenges to\ntransformed workplace activities. So far, benchmarking has guided progress in\nAI, but it has offered limited explanatory and predictive power for\ngeneral-purpose AI systems, given the low transferability across diverse tasks.\nIn this paper, we introduce general scales for AI evaluation that can explain\nwhat common AI benchmarks really measure, extract ability profiles of AI\nsystems, and predict their performance for new task instances, in- and\nout-of-distribution. Our fully-automated methodology builds on 18 newly-crafted\nrubrics that place instance demands on general scales that do not saturate.\nIllustrated for 15 large language models and 63 tasks, high explanatory power\nis unleashed from inspecting the demand and ability profiles, bringing insights\non the sensitivity and specificity exhibited by different benchmarks, and how\nknowledge, metacognition and reasoning are affected by model size,\nchain-of-thought and distillation. Surprisingly, high predictive power at the\ninstance level becomes possible using these demand levels, providing superior\nestimates over black-box baseline predictors based on embeddings or finetuning,\nespecially in out-of-distribution settings (new tasks and new benchmarks). The\nscales, rubrics, battery, techniques and results presented here represent a\nmajor step for AI evaluation, underpinning the reliable deployment of AI in the\nyears ahead.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 01:13:56 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhou', 'Lexin', ''], ['Pacchiardi', 'Lorenzo', ''], ['Mart\u00ednez-Plumed', 'Fernando', ''], ['Collins', 'Katherine M.', ''], ['Moros-Daval', 'Yael', ''], ['Zhang', 'Seraphina', ''], ['Zhao', 'Qinlin', ''], ['Huang', 'Yitian', ''], ['Sun', 'Luning', ''], ['Prunty', 'Jonathan E.', ''], ['Li', 'Zongqian', ''], ['S\u00e1nchez-Garc\u00eda', 'Pablo', ''], ['Chen', 'Kexin Jiang', ''], ['Casares', 'Pablo A. M.', ''], ['Zu', 'Jiyun', ''], ['Burden', 'John', ''], ['Mehrbakhsh', 'Behzad', ''], ['Stillwell', 'David', ''], ['Cebrian', 'Manuel', ''], ['Wang', 'Jindong', ''], ['Henderson', 'Peter', ''], ['Wu', 'Sherry Tongshuang', ''], ['Kyllonen', 'Patrick C.', ''], ['Cheke', 'Lucy', ''], ['Xie', 'Xing', ''], ['Hern\u00e1ndez-Orallo', 'Jos\u00e9', '']]","extracted_entities":"[{'text': 'chain-of-thought', 'label': 'Chain of thought'}, {'text': 'distillation', 'label': 'Knowledge distillation'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'finetuning', 'label': 'Fine-tuning'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2503.06437,"submitter":"Juhyeon Park","authors":"Juhyeon Park, Peter Yongho Kim, Jiook Cha, Shinjae Yoo, Taesup Moon","title":"SEED: Towards More Accurate Semantic Evaluation for Visual Brain\n  Decoding","comments":"Under Review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  We present SEED (\\textbf{Se}mantic \\textbf{E}valuation for Visual Brain\n\\textbf{D}ecoding), a novel metric for evaluating the semantic decoding\nperformance of visual brain decoding models. It integrates three complementary\nmetrics, each capturing a different aspect of semantic similarity between\nimages. Using carefully crowd-sourced human judgment data, we demonstrate that\nSEED achieves the highest alignment with human evaluations, outperforming other\nwidely used metrics. Through the evaluation of existing visual brain decoding\nmodels, we further reveal that crucial information is often lost in\ntranslation, even in state-of-the-art models that achieve near-perfect scores\non existing metrics. To facilitate further research, we open-source the human\njudgment data, encouraging the development of more advanced evaluation methods\nfor brain decoding models. Additionally, we propose a novel loss function\ndesigned to enhance semantic decoding performance by leveraging the order of\npairwise cosine similarity in CLIP image embeddings. This loss function is\ncompatible with various existing methods and has been shown to consistently\nimprove their semantic decoding performances when used for training, with\nrespect to both existing metrics and SEED.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 04:25:39 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Park', 'Juhyeon', ''], ['Kim', 'Peter Yongho', ''], ['Cha', 'Jiook', ''], ['Yoo', 'Shinjae', ''], ['Moon', 'Taesup', '']]","extracted_entities":"[{'text': 'SEED', 'label': 'BERT'}, {'text': 'SEED', 'label': 'BERT'}, {'text': 'CLIP image embeddings', 'label': 'Embedding'}, {'text': 'SEED', 'label': 'BERT'}]","assigned_concept":"Embedding","matched_keyword":"CLIP image embeddings","similarity_score":0.6026087999}
{"id":2503.06457,"submitter":"Yanbiao Ma","authors":"Yanbiao Ma, Wei Dai, Wenke Huang, Jiayi Chen","title":"Geometric Knowledge-Guided Localized Global Distribution Alignment for\n  Federated Learning","comments":"Accepted by CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Data heterogeneity in federated learning, characterized by a significant\nmisalignment between local and global distributions, leads to divergent local\noptimization directions and hinders global model training. Existing studies\nmainly focus on optimizing local updates or global aggregation, but these\nindirect approaches demonstrate instability when handling highly heterogeneous\ndata distributions, especially in scenarios where label skew and domain skew\ncoexist. To address this, we propose a geometry-guided data generation method\nthat centers on simulating the global embedding distribution locally. We first\nintroduce the concept of the geometric shape of an embedding distribution and\nthen address the challenge of obtaining global geometric shapes under privacy\nconstraints. Subsequently, we propose GGEUR, which leverages global geometric\nshapes to guide the generation of new samples, enabling a closer approximation\nto the ideal global distribution. In single-domain scenarios, we augment\nsamples based on global geometric shapes to enhance model generalization; in\nmulti-domain scenarios, we further employ class prototypes to simulate the\nglobal distribution across domains. Extensive experimental results demonstrate\nthat our method significantly enhances the performance of existing approaches\nin handling highly heterogeneous data, including scenarios with label skew,\ndomain skew, and their coexistence. Code published at:\nhttps:\/\/github.com\/WeiDai-David\/2025CVPR_GGEUR\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 05:30:28 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Ma', 'Yanbiao', ''], ['Dai', 'Wei', ''], ['Huang', 'Wenke', ''], ['Chen', 'Jiayi', '']]","extracted_entities":"[{'text': 'embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embedding","similarity_score":1.0}
{"id":2503.06475,"submitter":"Ali Sarabadani","authors":"Ali Sarabadani, Kheirolah Rahsepar Fard, Hamid Dalvand","title":"SKG-LLM: Developing a Mathematical Model for Stroke Knowledge Graph\n  Construction Using Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The purpose of this study is to introduce SKG-LLM. A knowledge graph (KG) is\nconstructed from stroke-related articles using mathematical and large language\nmodels (LLMs). SKG-LLM extracts and organizes complex relationships from the\nbiomedical literature, using it to increase the accuracy and depth of KG in\nstroke research. In the proposed method, GPT-4 was used for data\npre-processing, and the extraction of embeddings was also done by GPT-4 in the\nwhole KG construction process. The performance of the proposed model was tested\nwith two evaluation criteria: Precision and Recall. For further validation of\nthe proposed model, GPT-4 was used. Compared with Wikidata and WN18RR, the\nproposed KG-LLM approach performs better, especially in precision and recall.\nBy including GPT-4 in the preprocessing process, the SKG-LLM model achieved a\nprecision score of 0.906 and a recall score of 0.923. Expert reviews further\nimproved the results and increased precision to 0.923 and recall to 0.918. The\nknowledge graph constructed by SKG-LLM contains 2692 nodes and 5012 edges,\nwhich are 13 distinct types of nodes and 24 types of edges.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 06:25:37 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Sarabadani', 'Ali', ''], ['Fard', 'Kheirolah Rahsepar', ''], ['Dalvand', 'Hamid', '']]","extracted_entities":"[{'text': 'mathematical and large language\\nmodels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'GPT-4', 'label': 'GPT'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2503.06483,"submitter":"Hong Jiang","authors":"Zhe-Bin Guan and Hong Jiang","title":"Density-Matrix Embedding Based Multi-Configurational Perturbation Theory\n  Approach to Single-Ion Magnets","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.chem-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multi-configurational wave-function theory (MC-WFT) that combines complete\nactive space self-consistent field (CASSCF) approach with subsequent state\ninteraction (SI) treatment of spin-orbit coupling (SOC), abbreviated as\nCASSCF-SO, plays important roles in microscopic understanding of single-ion\nmagnets (SIMs) with different central transition metal or lanthanide ions and\nvarious coordination environments, but its application to SIMs with complex\nstructure is severely limited due to its highly demanding computational cost.\nDensity-matrix embedding theory (DMET) provides a systematic and mathematically\nrigorous framework to combine low-level mean field approaches like Hartree-Fock\nand high-level MC-WFT methods like CASSCF-SO, which is particularly promising\nto SIMs. As a continuation of our previous work on DMET+CASSCF for $3d$ SIMs\n(Ai, Sun, and Jiang, J. Phys. Chem. Lett. 2022, 13, 10627), we extend the\nmethodology by considering dynamic correlation on top of CASSCF using the\nsecond-order $n$-electron valence perturbation theory (NEVPT2) in the DMET\nframework, abbreviated as DMET+NEVPT2, and benchmark the accuracy of this\napproach to molecular magnetic anisotropy in a set of typical transition metal\ncomplexes. We found that DMET+NEVPT2 can give the results very close to\nall-electron treatment, and can be systematically improved for higher accuracy\nby expanding the region treated as the central cluster, while the computation\ncost is dramatically reduced due to the reduction of the number of orbitals by\nDMET construction. Our findings suggest that DMET is capable of accounting for\nmost of the dynamic correlation that is important for magnetic anisotropy in\ntypical SIMs, and can be useful for further high-accuracy spin-phonon study and\nhigh-throughput computations.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 06:52:43 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Guan', 'Zhe-Bin', ''], ['Jiang', 'Hong', '']]","extracted_entities":"[{'text': 'CASSCF-SO', 'label': 'Embedding'}, {'text': 'Density-matrix embedding theory', 'label': 'Embedding'}, {'text': 'SIMs', 'label': 'LLMs'}, {'text': 'DMET', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"Density-matrix embedding theory","similarity_score":0.5964437723}
{"id":2503.06542,"submitter":"Yukang Feng","authors":"Jianwen Sun, Yukang Feng, Chuanhao Li, Fanrui Zhang, Zizhen Li, Jiaxin\n  Ai, Sizhuo Zhou, Yu Dai, Shenglin Zhang, Kaipeng Zhang","title":"ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model\n  with Interleaved Multimodal Generation via Asymmetric Synergy","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Unified models (UniMs) for multimodal understanding and generation have\nrecently received much attention in the area of vision and language. Existing\nUniMs are designed to simultaneously learn both multimodal understanding and\ngeneration capabilities, demanding substantial computational resources, and\noften struggle to generate interleaved text-image. We present ARMOR, a\nresource-efficient and pure autoregressive framework that achieves both\nunderstanding and generation by fine-tuning existing multimodal large language\nmodels (MLLMs). Specifically, ARMOR extends existing MLLMs from three\nperspectives: (1) For model architecture, an asymmetric encoder-decoder\narchitecture with a forward-switching mechanism is introduced to unify\nembedding space integrating textual and visual modalities for enabling natural\ntext-image interleaved generation with minimal computational overhead. (2) For\ntraining data, a meticulously curated, high-quality interleaved dataset is\ncollected for fine-tuning MLLMs. (3) For the training algorithm, we propose a\n``what or how to generate\" algorithm to empower existing MLLMs with multimodal\ngeneration capabilities while preserving their multimodal understanding\ncapabilities, through three progressive training stages based on the collected\ndataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to\nUniMs with promising image generation capabilities, using limited training\nresources. Our code will be released soon at https:\/\/armor.github.io.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 10:15:39 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Sun', 'Jianwen', ''], ['Feng', 'Yukang', ''], ['Li', 'Chuanhao', ''], ['Zhang', 'Fanrui', ''], ['Li', 'Zizhen', ''], ['Ai', 'Jiaxin', ''], ['Zhou', 'Sizhuo', ''], ['Dai', 'Yu', ''], ['Zhang', 'Shenglin', ''], ['Zhang', 'Kaipeng', '']]","extracted_entities":"[{'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'embedding space', 'label': 'Embedding'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Embedding","matched_keyword":"embedding space","similarity_score":0.8514168859}
{"id":2308.04371,"submitter":"Yifan Zhang","authors":"Yifan Zhang, Jingqin Yang, Yang Yuan, Andrew Chi-Chih Yao","title":"Cumulative Reasoning with Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advancements in large language models (LLMs) have shown remarkable\nprogress, yet their ability to solve complex problems remains limited. In this\nwork, we introduce Cumulative Reasoning (CR), an approach that utilizes LLMs\ncumulatively and iteratively, mirroring human thought processes for\nproblem-solving. CR decomposes tasks into smaller, manageable components and\nleverages previous propositions for effective composition, significantly\nenhancing problem-solving capabilities. We demonstrate CR's advantage through\nseveral complex reasoning tasks: it outperforms existing methods in logical\ninference tasks with up to a 9.3% improvement, achieving 98.04% accuracy on the\ncurated FOLIO wiki dataset. In the Game of 24, it achieves 98% accuracy,\nmarking a 24% improvement over the prior state-of-the-art. In solving MATH\nproblems, CR achieves a 4.2% increase from previous methods and a 43% relative\nimprovement in the most challenging level 5 problems. When incorporating a code\nenvironment with CR, we further harness LLMs' reasoning capabilities and\noutperform the Program of Thought (PoT) method by 38.8%. The code is available\nat https:\/\/github.com\/iiis-ai\/cumulative-reasoning.\n","versions":"[{'version': 'v1', 'created': 'Tue, 8 Aug 2023 16:18:20 GMT'}, {'version': 'v2', 'created': 'Wed, 9 Aug 2023 14:37:37 GMT'}, {'version': 'v3', 'created': 'Thu, 10 Aug 2023 08:24:09 GMT'}, {'version': 'v4', 'created': 'Fri, 25 Aug 2023 02:40:37 GMT'}, {'version': 'v5', 'created': 'Sat, 2 Dec 2023 02:59:12 GMT'}, {'version': 'v6', 'created': 'Tue, 2 Apr 2024 03:37:39 GMT'}, {'version': 'v7', 'created': 'Wed, 12 Mar 2025 02:55:36 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Zhang', 'Yifan', ''], ['Yang', 'Jingqin', ''], ['Yuan', 'Yang', ''], ['Yao', 'Andrew Chi-Chih', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'human thought processes', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Chain of thought","matched_keyword":"human thought processes","similarity_score":0.6232779026}
{"id":2404.14812,"submitter":"Yufeng Zhang","authors":"Yufeng Zhang, Xuepeng Wang, Lingxiang Wu, Jinqiao Wang","title":"Enhancing Chain of Thought Prompting in Large Language Models via\n  Reasoning Patterns","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Chain of Thought (CoT) prompting can encourage language models to engage in\nmulti-step logical reasoning. The quality of the provided demonstrations\nsignificantly influences the success of downstream inference tasks. Current\nunsupervised CoT methods primarily select examples based on the semantics of\nthe questions, which can introduce noise and lack interpretability. In this\npaper, we propose leveraging reasoning patterns to enhance CoT prompting\neffectiveness. Reasoning patterns represent the process by which language\nmodels arrive at their final results. By utilizing prior knowledge and\nprompt-based methods from large models, we first construct task-specific\npattern sets. We then select diverse demonstrations based on different\nreasoning patterns. This approach not only mitigates the impact of noise but\nalso provides explicit interpretability to help us understand the mechanisms of\nCoT. Extensive experiments demonstrate that our method is more robust and\nconsistently leads to improvements across various reasoning tasks.\n","versions":"[{'version': 'v1', 'created': 'Tue, 23 Apr 2024 07:50:00 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 03:03:57 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhang', 'Yufeng', ''], ['Wang', 'Xuepeng', ''], ['Wu', 'Lingxiang', ''], ['Wang', 'Jinqiao', '']]","extracted_entities":"[{'text': 'Chain of Thought', 'label': 'Chain of thought'}, {'text': 'CoT', 'label': 'Chain of thought'}, {'text': 'CoT prompting', 'label': 'Prompting'}, {'text': 'Reasoning patterns', 'label': 'Chain of thought'}, {'text': 'reasoning patterns', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain of Thought","similarity_score":0.9999998808}
{"id":2405.04776,"submitter":"Karthik Valmeekam","authors":"Kaya Stechly, Karthik Valmeekam, Subbarao Kambhampati","title":"Chain of Thoughtlessness? An Analysis of CoT in Planning","comments":"NeurIPS 2024","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language model (LLM) performance on reasoning problems typically does\nnot generalize out of distribution. Previous work has claimed that this can be\nmitigated with chain of thought prompting-a method of demonstrating solution\nprocedures-with the intuition that it is possible to in-context teach an LLM an\nalgorithm for solving the problem. This paper presents a case study of chain of\nthought on problems from Blocksworld, a classical planning domain, and examines\nthe performance of two state-of-the-art LLMs across two axes: generality of\nexamples given in prompt, and complexity of problems queried with each prompt.\nWhile our problems are very simple, we only find meaningful performance\nimprovements from chain of thought prompts when those prompts are exceedingly\nspecific to their problem class, and that those improvements quickly\ndeteriorate as the size n of the query-specified stack grows past the size of\nstacks shown in the examples. We also create scalable variants of three domains\ncommonly studied in previous CoT papers and demonstrate the existence of\nsimilar failure modes. Our results hint that, contrary to previous claims in\nthe literature, CoT's performance improvements do not stem from the model\nlearning general algorithmic procedures via demonstrations but depend on\ncarefully engineering highly problem specific prompts. This spotlights\ndrawbacks of chain of thought, especially the sharp tradeoff between possible\nperformance gains and the amount of human labor necessary to generate examples\nwith correct reasoning traces.\n","versions":"[{'version': 'v1', 'created': 'Wed, 8 May 2024 02:48:28 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Jun 2024 02:44:52 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 04:56:46 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Stechly', 'Kaya', ''], ['Valmeekam', 'Karthik', ''], ['Kambhampati', 'Subbarao', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'chain of thought', 'label': 'Chain of thought'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'chain of\\nthought', 'label': 'Chain of thought'}, {'text': 'chain of thought', 'label': 'Chain of thought'}, {'text': 'chain of thought', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"chain of thought","similarity_score":0.9999998808}
{"id":2408.06631,"submitter":"Mingning Guo","authors":"Mingning Guo, Mengwei Wu, Yuxiang Shen, Haifeng Li and Chao Tao","title":"IFShip: Interpretable Fine-grained Ship Classification with Domain\n  Knowledge-Enhanced Vision-Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  End-to-end interpretation currently dominates the remote sensing fine-grained\nship classification (RS-FGSC) task. However, the inference process remains\nuninterpretable, leading to criticisms of these models as \"black box\" systems.\nTo address this issue, we propose a domain knowledge-enhanced Chain-of-Thought\n(CoT) prompt generation mechanism, which is used to semi-automatically\nconstruct a task-specific instruction-following dataset, TITANIC-FGS. By\ntraining on TITANIC-FGS, we adapt general-domain vision-language models (VLMs)\nto the FGSC task, resulting in a model named IFShip. Building upon IFShip, we\ndevelop an FGSC visual chatbot that redefines the FGSC problem as a\nstep-by-step reasoning task and conveys the reasoning process in natural\nlanguage. Experimental results show that IFShip outperforms state-of-the-art\nFGSC algorithms in both interpretability and classification accuracy.\nFurthermore, compared to VLMs such as LLaVA and MiniGPT-4, IFShip demonstrates\nsuperior performance on the FGSC task. It provides an accurate chain of\nreasoning when fine-grained ship types are recognizable to the human eye and\noffers interpretable explanations when they are not.\n","versions":"[{'version': 'v1', 'created': 'Tue, 13 Aug 2024 04:36:18 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 12:02:01 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Guo', 'Mingning', ''], ['Wu', 'Mengwei', ''], ['Shen', 'Yuxiang', ''], ['Li', 'Haifeng', ''], ['Tao', 'Chao', '']]","extracted_entities":"[{'text': 'chain of\\nreasoning', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"chain of\nreasoning","similarity_score":0.6542659998}
{"id":2410.19419,"submitter":"Hamna Abid","authors":"Hamna and Deepthi Sudharsan and Agrima Seth and Ritvik Budhiraja and\n  Deepika Khullar and Vyshak Jain and Kalika Bali and Aditya Vashistha and\n  Sameer Segal","title":"KAHANI: Culturally-Nuanced Visual Storytelling Tool for Non-Western\n  Cultures","comments":"Under review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Large Language Models (LLMs) and Text-To-Image (T2I) models have demonstrated\nthe ability to generate compelling text and visual stories. However, their\noutputs are predominantly aligned with the sensibilities of the Global North,\noften resulting in an outsider's gaze on other cultures. As a result,\nnon-Western communities have to put extra effort into generating culturally\nspecific stories. To address this challenge, we developed a visual storytelling\ntool called Kahani that generates culturally grounded visual stories for\nnon-Western cultures. Our tool leverages off-the-shelf models GPT-4 Turbo and\nStable Diffusion XL (SDXL). By using Chain of Thought (CoT) and T2I prompting\ntechniques, we capture the cultural context from user's prompt and generate\nvivid descriptions of the characters and scene compositions. To evaluate the\neffectiveness of Kahani, we conducted a comparative user study with ChatGPT-4\n(with DALL-E3) in which participants from different regions of India compared\nthe cultural relevance of stories generated by the two tools. The results of\nthe qualitative and quantitative analysis performed in the user study show that\nKahani's visual stories are more culturally nuanced than those generated by\nChatGPT-4. In 27 out of 36 comparisons, Kahani outperformed or was on par with\nChatGPT-4, effectively capturing cultural nuances and incorporating more\nCulturally Specific Items (CSI), validating its ability to generate culturally\ngrounded visual stories.\n","versions":"[{'version': 'v1', 'created': 'Fri, 25 Oct 2024 09:23:24 GMT'}, {'version': 'v2', 'created': 'Mon, 28 Oct 2024 08:39:18 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 04:10:57 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Hamna', '', ''], ['Sudharsan', 'Deepthi', ''], ['Seth', 'Agrima', ''], ['Budhiraja', 'Ritvik', ''], ['Khullar', 'Deepika', ''], ['Jain', 'Vyshak', ''], ['Bali', 'Kalika', ''], ['Vashistha', 'Aditya', ''], ['Segal', 'Sameer', '']]","extracted_entities":"[{'text': 'Chain of Thought', 'label': 'Chain of thought'}, {'text': \"user's prompt\", 'label': 'Prompting'}, {'text': 'ChatGPT-4', 'label': 'ChatGPT'}, {'text': 'ChatGPT-4', 'label': 'ChatGPT'}, {'text': 'ChatGPT-4', 'label': 'ChatGPT'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain of Thought","similarity_score":0.9999998808}
{"id":2502.06772,"submitter":"Ling Yang","authors":"Ling Yang, Zhaochen Yu, Bin Cui, Mengdi Wang","title":"ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates","comments":"Code: https:\/\/github.com\/Gen-Verse\/ReasonFlux","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present that hierarchical LLM reasoning via scaling thought templates can\neffectively optimize the reasoning search space and outperform the mathematical\nreasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.\nWe train our ReasonFlux-32B model with only 8 GPUs and introduces three\ninnovations: (i) a structured and generic thought template library, containing\naround 500 high-level thought templates capable of generalizing to similar or\nrelevant reasoning problems; (ii) performing hierarchical reinforcement\nlearning on a sequence of thought templates instead of long CoTs, optimizing a\nbase LLM to plan out an optimal template trajectory for gradually handling\ncomplex problems; (iii) a brand new inference scaling system that enables\nhierarchical LLM reasoning by adaptively scaling thought templates at inference\ntime. With a template trajectory containing more explainable reasoning\nstructures than DeepSeek-R1 and o3-mini, our ReasonFlux-32B significantly\nadvances math reasoning capabilities to state-of-the-art levels. Notably, on\nthe MATH benchmark, it achieves an accuracy of 91.2% and surpasses o1-preview\nby 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an\naverage of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and\n45%, respectively. Code: https:\/\/github.com\/Gen-Verse\/ReasonFlux\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Feb 2025 18:51:47 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 02:46:19 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Yang', 'Ling', ''], ['Yu', 'Zhaochen', ''], ['Cui', 'Bin', ''], ['Wang', 'Mengdi', '']]","extracted_entities":"[{'text': 'hierarchical reinforcement\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'sequence of thought templates', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"sequence of thought templates","similarity_score":0.5206565857}
{"id":2502.20129,"submitter":"Yifan Zhang","authors":"Yifan Zhang, Wenyu Du, Dongming Jin, Jie Fu, Zhi Jin","title":"Finite State Automata Inside Transformers with Chain-of-Thought: A\n  Mechanistic Study on State Tracking","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Chain-of-Thought (CoT) significantly enhances the performance of large\nlanguage models (LLMs) across a wide range of tasks, and prior research shows\nthat CoT can theoretically increase expressiveness. However, there is limited\nmechanistic understanding of the algorithms that Transformer+CoT can learn. In\nthis work, we (1) evaluate the state tracking capabilities of Transformer+CoT\nand its variants, confirming the effectiveness of CoT. (2) Next, we identify\nthe circuit, a subset of model components, responsible for tracking the world\nstate, finding that late-layer MLP neurons play a key role. We propose two\nmetrics, compression and distinction, and show that the neuron sets for each\nstate achieve nearly 100% accuracy, providing evidence of an implicit finite\nstate automaton (FSA) embedded within the model. (3) Additionally, we explore\nthree realistic settings: skipping intermediate steps, introducing data noise,\nand testing length generalization. Our results demonstrate that Transformer+CoT\nlearns robust algorithms (FSA), highlighting its resilience in challenging\nscenarios.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 14:24:51 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 15:47:08 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Zhang', 'Yifan', ''], ['Du', 'Wenyu', ''], ['Jin', 'Dongming', ''], ['Fu', 'Jie', ''], ['Jin', 'Zhi', '']]","extracted_entities":"[{'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'CoT', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain-of-Thought","similarity_score":0.9539169669}
{"id":2503.03205,"submitter":"Ruida Wang","authors":"Ruida Wang, Rui Pan, Yuxin Li, Jipeng Zhang, Yizhen Jia, Shizhe Diao,\n  Renjie Pi, Junjie Hu, Tong Zhang","title":"MA-LoT: Multi-Agent Lean-based Long Chain-of-Thought Reasoning enhances\n  Formal Theorem Proving","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Solving mathematical problems using computer-verifiable languages like Lean\nhas significantly impacted mathematical and computer science communities.\nState-of-the-art methods utilize single Large Language Models (LLMs) as agents\nor provers to either generate complete proof or perform tree searches. However,\nsingle-agent methods inherently lack a structured way to combine high-level\nreasoning in Natural Language (NL) with Formal Language (FL) verification\nfeedback. To solve these issues, we propose MA-LoT: Multi-Agent Lean-based Long\nChain-of-Thought framework, (to the best of our knowledge), the first\nmulti-agent framework for Lean4 theorem proving that balance high-level NL\nreasoning and FL verification in Long CoT. Using this structured interaction,\nour approach enables deeper insights and long-term coherence in proof\ngeneration, with which past methods struggle. We do this by leveraging emergent\nformal reasoning ability in Long CoT using our novel LoT-Transfer Learning\ntraining-inference pipeline. Extensive experiments show that our framework\nachieves a 61.07% accuracy rate on the Lean4 version of the MiniF2F-Test\ndataset, largely outperforming GPT-4 (22.95%), single-agent tree search\n(InternLM-Step-Prover, 50.70%), and whole-proof generation (Godel-Prover,\n55.33%) baselines. Furthermore, our findings highlight the potential of\ncombining Long CoT with formal verification for a more insightful generation in\na broader perspective.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 05:50:31 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 17:39:42 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Wang', 'Ruida', ''], ['Pan', 'Rui', ''], ['Li', 'Yuxin', ''], ['Zhang', 'Jipeng', ''], ['Jia', 'Yizhen', ''], ['Diao', 'Shizhe', ''], ['Pi', 'Renjie', ''], ['Hu', 'Junjie', ''], ['Zhang', 'Tong', '']]","extracted_entities":"[{'text': 'Long\\nChain-of-Thought', 'label': 'Chain of thought'}, {'text': 'LoT-Transfer Learning', 'label': 'Few-shot Learning'}, {'text': 'GPT-4', 'label': 'GPT'}]","assigned_concept":"Chain of thought","matched_keyword":"Long\nChain-of-Thought","similarity_score":0.8942457438}
{"id":2503.06416,"submitter":"Michelle Vaccaro","authors":"Michelle Vaccaro, Michael Caoson, Harang Ju, Sinan Aral, and Jared R.\n  Curhan","title":"Advancing AI Negotiations: New Theory and Evidence from a Large-Scale\n  Autonomous Negotiations Competition","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Despite the rapid proliferation of artificial intelligence (AI) negotiation\nagents, there has been limited integration of computer science research and\nestablished negotiation theory to develop new theories of AI negotiation. To\nbridge this gap, we conducted an International AI Negotiations Competition in\nwhich participants iteratively designed and refined prompts for large language\nmodel (LLM) negotiation agents. We then facilitated over 120,000 negotiations\nbetween these agents across multiple scenarios with diverse characteristics and\nobjectives. Our findings revealed that fundamental principles from established\nhuman-human negotiation theory remain crucial in AI-AI negotiations.\nSpecifically, agents exhibiting high warmth fostered higher counterpart\nsubjective value and reached deals more frequently, which enabled them to\ncreate and claim more value in integrative settings. However, conditional on\nreaching a deal, warm agents claimed less value while dominant agents claimed\nmore value. These results align with classic negotiation theory emphasizing\nrelationship-building, assertiveness, and preparation. Our analysis also\nrevealed unique dynamics in AI-AI negotiations not fully explained by\nnegotiation theory, particularly regarding the effectiveness of AI-specific\nstrategies like chain-of-thought reasoning and prompt injection. The agent that\nwon our competition implemented an approach that blended traditional\nnegotiation preparation frameworks with AI-specific methods. Together, these\nresults suggest the importance of establishing a new theory of AI negotiations\nthat integrates established negotiation theory with AI-specific strategies to\noptimize agent performance. Our research suggests this new theory must account\nfor the unique characteristics of autonomous agents and establish the\nconditions under which traditional negotiation theory applies in automated\nsettings.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 03:25:48 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Vaccaro', 'Michelle', ''], ['Caoson', 'Michael', ''], ['Ju', 'Harang', ''], ['Aral', 'Sinan', ''], ['Curhan', 'Jared R.', '']]","extracted_entities":"[{'text': 'prompts', 'label': 'Prompting'}, {'text': 'chain-of-thought reasoning', 'label': 'Chain of thought'}, {'text': 'prompt injection', 'label': 'Prompting'}]","assigned_concept":"Chain of thought","matched_keyword":"chain-of-thought reasoning","similarity_score":0.8011320829}
{"id":2503.06514,"submitter":"Haoqiang Kang","authors":"Haoqiang Kang, Enna Sachdeva, Piyush Gupta, Sangjae Bae, Kwonjoon Lee","title":"GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with\n  Generative Flow Networks","comments":null,"journal-ref":"CVPR 2025","doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Vision-Language Models (VLMs) have recently shown promising advancements in\nsequential decision-making tasks through task-specific fine-tuning. However,\ncommon fine-tuning methods, such as Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) techniques like Proximal Policy Optimization (PPO),\npresent notable limitations: SFT assumes Independent and Identically\nDistributed (IID) data, while PPO focuses on maximizing cumulative rewards.\nThese limitations often restrict solution diversity and hinder generalization\nin multi-step reasoning tasks. To address these challenges, we introduce a\nnovel framework, GFlowVLM, a framework that fine-tune VLMs using Generative\nFlow Networks (GFlowNets) to promote generation of diverse solutions for\ncomplex reasoning tasks. GFlowVLM models the environment as a non-Markovian\ndecision process, allowing it to capture long-term dependencies essential for\nreal-world applications. It takes observations and task descriptions as inputs\nto prompt chain-of-thought (CoT) reasoning which subsequently guides action\nselection. We use task based rewards to fine-tune VLM with GFlowNets. This\napproach enables VLMs to outperform prior fine-tuning methods, including SFT\nand RL. Empirical results demonstrate the effectiveness of GFlowVLM on complex\ntasks such as card games (NumberLine, BlackJack) and embodied planning tasks\n(ALFWorld), showing enhanced training efficiency, solution diversity, and\nstronger generalization capabilities across both in-distribution and\nout-of-distribution scenarios.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 08:38:10 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Kang', 'Haoqiang', ''], ['Sachdeva', 'Enna', ''], ['Gupta', 'Piyush', ''], ['Bae', 'Sangjae', ''], ['Lee', 'Kwonjoon', '']]","extracted_entities":"[{'text': 'task-specific fine-tuning', 'label': 'Fine-tuning'}, {'text': 'SFT', 'label': 'BERT'}, {'text': 'GFlowVLM', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'chain-of-thought', 'label': 'Chain of thought'}, {'text': 'GFlowNets', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'SFT', 'label': 'BERT'}]","assigned_concept":"Chain of thought","matched_keyword":"chain-of-thought","similarity_score":0.9539169669}
{"id":2502.07036,"submitter":"Aditya Patwardhan","authors":"Aditya Patwardhan, Vivek Vaidya, Ashish Kundu","title":"Automated Consistency Analysis of LLMs","comments":"10 pages, 12 figures, 3 tables, 3 algorithms, 2024 IEEE 6th\n  International Conference on Trust, Privacy and Security in Intelligent\n  Systems, and Applications (TPS-ISA), Washington, DC, USA","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Generative AI (Gen AI) with large language models (LLMs) are being widely\nadopted across the industry, academia and government. Cybersecurity is one of\nthe key sectors where LLMs can be and\/or are already being used. There are a\nnumber of problems that inhibit the adoption of trustworthy Gen AI and LLMs in\ncybersecurity and such other critical areas. One of the key challenge to the\ntrustworthiness and reliability of LLMs is: how consistent an LLM is in its\nresponses? In this paper, we have analyzed and developed a formal definition of\nconsistency of responses of LLMs. We have formally defined what is consistency\nof responses and then develop a framework for consistency evaluation. The paper\nproposes two approaches to validate consistency: self-validation, and\nvalidation across multiple LLMs. We have carried out extensive experiments for\nseveral LLMs such as GPT4oMini, GPT3.5, Gemini, Cohere, and Llama3, on a\nsecurity benchmark consisting of several cybersecurity questions: informational\nand situational. Our experiments corroborate the fact that even though these\nLLMs are being considered and\/or already being used for several cybersecurity\ntasks today, they are often inconsistent in their responses, and thus are\nuntrustworthy and unreliable for cybersecurity.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Feb 2025 21:03:24 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 18:14:34 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Patwardhan', 'Aditya', ''], ['Vaidya', 'Vivek', ''], ['Kundu', 'Ashish', '']]","extracted_entities":"[{'text': 'GPT4oMini', 'label': 'GPT'}, {'text': 'GPT3.5', 'label': 'GPT'}, {'text': 'Gemini', 'label': 'GPT-3'}, {'text': 'Cohere', 'label': 'GPT-4'}, {'text': 'Llama3', 'label': 'Llama'}]","assigned_concept":"Llama","matched_keyword":"Llama3","similarity_score":0.8217295408}
{"id":2502.1675,"submitter":"Saikat Barua","authors":"Saikat Barua, Mostafizur Rahman, Md Jafor Sadek, Rafiul Islam,\n  Shehenaz Khaled, Ahmedul Kabir","title":"Guardians of the Agentic System: Preventing Many Shots Jailbreak with\n  Agentic System","comments":"18 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The autonomous AI agents using large language models can create undeniable\nvalues in all span of the society but they face security threats from\nadversaries that warrants immediate protective solutions because trust and\nsafety issues arise. Considering the many-shot jailbreaking and deceptive\nalignment as some of the main advanced attacks, that cannot be mitigated by the\nstatic guardrails used during the supervised training, points out a crucial\nresearch priority for real world robustness. The combination of static\nguardrails in dynamic multi-agent system fails to defend against those attacks.\nWe intend to enhance security for LLM-based agents through the development of\nnew evaluation frameworks which identify and counter threats for safe\noperational deployment. Our work uses three examination methods to detect rogue\nagents through a Reverse Turing Test and analyze deceptive alignment through\nmulti-agent simulations and develops an anti-jailbreaking system by testing it\nwith GEMINI 1.5 pro and llama-3.3-70B, deepseek r1 models using tool-mediated\nadversarial scenarios. The detection capabilities are strong such as 94\\%\naccuracy for GEMINI 1.5 pro yet the system suffers persistent vulnerabilities\nwhen under long attacks as prompt length increases attack success rates (ASR)\nand diversity metrics become ineffective in prediction while revealing multiple\ncomplex system faults. The findings demonstrate the necessity of adopting\nflexible security systems based on active monitoring that can be performed by\nthe agents themselves together with adaptable interventions by system admin as\nthe current models can create vulnerabilities that can lead to the unreliable\nand vulnerable system. So, in our work, we try to address such situations and\npropose a comprehensive framework to counteract the security issues.\n","versions":"[{'version': 'v1', 'created': 'Sun, 23 Feb 2025 23:35:15 GMT'}, {'version': 'v2', 'created': 'Wed, 5 Mar 2025 22:17:18 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 09:16:06 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Barua', 'Saikat', ''], ['Rahman', 'Mostafizur', ''], ['Sadek', 'Md Jafor', ''], ['Islam', 'Rafiul', ''], ['Khaled', 'Shehenaz', ''], ['Kabir', 'Ahmedul', '']]","extracted_entities":"[{'text': 'llama-3.3-70B', 'label': 'Llama'}, {'text': 'prompt length', 'label': 'Prompting'}]","assigned_concept":"Llama","matched_keyword":"llama-3.3-70B","similarity_score":0.6499458551}
{"id":2503.03592,"submitter":"Karl Audun Borgersen","authors":"Karl Audun Borgersen","title":"English K_Quantization of LLMs Does Not Disproportionately Diminish\n  Multilingual Performance","comments":"8 pages, 6 figures, v2","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  For consumer usage of locally deployed LLMs, the GGUF format and\nk\\_quantization are invaluable tools for maintaining the performance of the\noriginal model while reducing it to sizes deployable with consumer-grade\nhardware. The number of bits dedicated to each weight from the original model\nis reduced based on how important they are thought to be during model\ninference. This importance is arrived at through the application of an\n'importance matrix'-a relatively small text document meant to be representative\nof the LLM's standard use-cases. In the vast majority of quants available\nonline, this document is primarily written in English. It was therefore an open\nquestion whether performance on English language tasks was preserved through\nthe sacrifice of multilingual performance and whether it can be preserved with\nalternate importance matrices. This article investigates these hypotheses by\nquantizing Llama3.3 70B on importance matrices written in three languages\n(English, Norwegian, and Malayalam) and evaluating them on the MixEval dataset\nin both English and Norwegian. All experiments related to yielded\nnon-significant results indicating that current quantization practices do not\ndisproportionately harm multilingual performance.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 15:26:59 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 07:36:46 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Borgersen', 'Karl Audun', '']]","extracted_entities":"[{'text': 'k\\\\_quantization', 'label': 'quantisation'}, {'text': 'Llama3', 'label': 'Llama'}]","assigned_concept":"Llama","matched_keyword":"Llama3","similarity_score":0.8217295408}
{"id":2410.02189,"submitter":"Yuexiang Xie","authors":"Ao Li, Yuexiang Xie, Songze Li, Fugee Tsung, Bolin Ding, Yaliang Li","title":"Agent-Oriented Planning in Multi-Agent Systems","comments":"Accepted by ICLR'2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.LG cs.MA","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Through the collaboration of multiple LLM-empowered agents possessing diverse\nexpertise and tools, multi-agent systems achieve impressive progress in solving\nreal-world problems. Given the user queries, the meta-agents, serving as the\nbrain within multi-agent systems, are required to decompose the queries into\nmultiple sub-tasks that can be allocated to suitable agents capable of solving\nthem, so-called agent-oriented planning. In this study, we identify three\ncritical design principles of agent-oriented planning, including solvability,\ncompleteness, and non-redundancy, to ensure that each sub-task can be\neffectively resolved, resulting in satisfactory responses to user queries.\nThese principles further inspire us to propose AOP, a novel framework for\nagent-oriented planning in multi-agent systems, leveraging a fast task\ndecomposition and allocation process followed by an effective and efficient\nevaluation via a reward model. According to the evaluation results, the\nmeta-agent is also responsible for promptly making necessary adjustments to\nsub-tasks and scheduling. Besides, we integrate a feedback loop into AOP to\nfurther enhance the effectiveness and robustness of such a problem-solving\nprocess. Extensive experiments demonstrate the advancement of AOP in solving\nreal-world problems compared to both single-agent systems and existing planning\nstrategies for multi-agent systems. The source code is available at\nhttps:\/\/github.com\/lalaliat\/Agent-Oriented-Planning\n","versions":"[{'version': 'v1', 'created': 'Thu, 3 Oct 2024 04:07:51 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 11:22:17 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Li', 'Ao', ''], ['Xie', 'Yuexiang', ''], ['Li', 'Songze', ''], ['Tsung', 'Fugee', ''], ['Ding', 'Bolin', ''], ['Li', 'Yaliang', '']]","extracted_entities":"[{'text': 'LLM-empowered agents', 'label': 'LLM-powered'}, {'text': 'promptly', 'label': 'Prompting'}, {'text': 'AOP', 'label': 'LLM-based'}, {'text': 'AOP', 'label': 'LLM-based'}]","assigned_concept":"LLM-powered","matched_keyword":"LLM-empowered agents","similarity_score":0.5614640117}
{"id":2307.00184,"submitter":"Mustafa Safdari","authors":"Greg Serapio-Garc\\'ia, Mustafa Safdari, Cl\\'ement Crepy, Luning Sun,\n  Stephen Fitz, Peter Romero, Marwa Abdulhai, Aleksandra Faust, Maja Matari\\'c","title":"Personality Traits in Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CY cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The advent of large language models (LLMs) has revolutionized natural\nlanguage processing, enabling the generation of coherent and contextually\nrelevant human-like text. As LLMs increasingly powerconversational agents used\nby the general public world-wide, the synthetic personality traits embedded in\nthese models, by virtue of training on large amounts of human data, is becoming\nincreasingly important. Since personality is a key factor determining the\neffectiveness of communication, we present a novel and comprehensive\npsychometrically valid and reliable methodology for administering and\nvalidating personality tests on widely-used LLMs, as well as for shaping\npersonality in the generated text of such LLMs. Applying this method to 18\nLLMs, we found: 1) personality measurements in the outputs of some LLMs under\nspecific prompting configurations are reliable and valid; 2) evidence of\nreliability and validity of synthetic LLM personality is stronger for larger\nand instruction fine-tuned models; and 3) personality in LLM outputs can be\nshaped along desired dimensions to mimic specific human personality profiles.\nWe discuss the application and ethical implications of the measurement and\nshaping method, in particular regarding responsible AI.\n","versions":"[{'version': 'v1', 'created': 'Sat, 1 Jul 2023 00:58:51 GMT'}, {'version': 'v2', 'created': 'Wed, 13 Sep 2023 22:37:29 GMT'}, {'version': 'v3', 'created': 'Thu, 21 Sep 2023 21:10:56 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Mar 2025 21:11:39 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Serapio-Garc\u00eda', 'Greg', ''], ['Safdari', 'Mustafa', ''], ['Crepy', 'Cl\u00e9ment', ''], ['Sun', 'Luning', ''], ['Fitz', 'Stephen', ''], ['Romero', 'Peter', ''], ['Abdulhai', 'Marwa', ''], ['Faust', 'Aleksandra', ''], ['Matari\u0107', 'Maja', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'specific prompting configurations', 'label': 'Prompting'}, {'text': 'ethical implications', 'label': 'AI Ethics'}]","assigned_concept":"Prompting","matched_keyword":"specific prompting configurations","similarity_score":0.6582940817}
{"id":2311.14282,"submitter":"Zheng Chen","authors":"Zheng Chen, Yulun Zhang, Jinjin Gu, Xin Yuan, Linghe Kong, Guihai\n  Chen, Xiaokang Yang","title":"Image Super-Resolution with Text Prompt Diffusion","comments":"Code is available at https:\/\/github.com\/zhengchen1999\/PromptSR","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Image super-resolution (SR) methods typically model degradation to improve\nreconstruction accuracy in complex and unknown degradation scenarios. However,\nextracting degradation information from low-resolution images is challenging,\nwhich limits the model performance. To boost image SR performance, one feasible\napproach is to introduce additional priors. Inspired by advancements in\nmulti-modal methods and text prompt image processing, we introduce text prompts\nto image SR to provide degradation priors. Specifically, we first design a\ntext-image generation pipeline to integrate text into the SR dataset through\nthe text degradation representation and degradation model. By adopting a\ndiscrete design, the text representation is flexible and user-friendly.\nMeanwhile, we propose the PromptSR to realize the text prompt SR. The PromptSR\nleverages the latest multi-modal large language model (MLLM) to generate\nprompts from low-resolution images. It also utilizes the pre-trained language\nmodel (e.g., T5 or CLIP) to enhance text comprehension. We train the PromptSR\non the text-image dataset. Extensive experiments indicate that introducing text\nprompts into SR, yields impressive results on both synthetic and real-world\nimages. Code: https:\/\/github.com\/zhengchen1999\/PromptSR.\n","versions":"[{'version': 'v1', 'created': 'Fri, 24 Nov 2023 05:11:35 GMT'}, {'version': 'v2', 'created': 'Tue, 12 Mar 2024 12:14:51 GMT'}, {'version': 'v3', 'created': 'Tue, 8 Oct 2024 10:30:00 GMT'}, {'version': 'v4', 'created': 'Thu, 10 Oct 2024 05:47:46 GMT'}, {'version': 'v5', 'created': 'Tue, 11 Mar 2025 02:20:58 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Chen', 'Zheng', ''], ['Zhang', 'Yulun', ''], ['Gu', 'Jinjin', ''], ['Yuan', 'Xin', ''], ['Kong', 'Linghe', ''], ['Chen', 'Guihai', ''], ['Yang', 'Xiaokang', '']]","extracted_entities":"[{'text': 'text prompt', 'label': 'Prompting'}, {'text': 'text prompts', 'label': 'Prompting'}, {'text': 'text prompt', 'label': 'Prompting'}, {'text': 'text\\nprompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"text prompt","similarity_score":0.6277507544}
{"id":2401.16796,"submitter":"Weibin Liao","authors":"Weibin Liao, Yinghao Zhu, Zhongji Zhang, Yuhang Wang, Zixiang Wang, Xu\n  Chu, Yasha Wang, Liantao Ma","title":"Learnable Prompt as Pseudo-Imputation: Rethinking the Necessity of\n  Traditional EHR Data Imputation in Downstream Clinical Prediction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Analyzing the health status of patients based on Electronic Health Records\n(EHR) is a fundamental research problem in medical informatics. The presence of\nextensive missing values in EHR makes it challenging for deep neural networks\n(DNNs) to directly model the patient's health status. Existing DNNs training\nprotocols, including Impute-then-Regress Procedure and Jointly Optimizing of\nImpute-n-Regress Procedure, require the additional imputation models to\nreconstruction missing values. However, Impute-then-Regress Procedure\nintroduces the risk of injecting imputed, non-real data into downstream\nclinical prediction tasks, resulting in power loss, biased estimation, and\npoorly performing models, while Jointly Optimizing of Impute-n-Regress\nProcedure is also difficult to generalize due to the complex optimization space\nand demanding data requirements. Inspired by the recent advanced literature of\nlearnable prompt in the fields of NLP and CV, in this work, we rethought the\nnecessity of the imputation model in downstream clinical tasks, and proposed\nLearnable Prompt as Pseudo-Imputation (PAI) as a new training protocol to\nassist EHR analysis. PAI no longer introduces any imputed data but constructs a\nlearnable prompt to model the implicit preferences of the downstream model for\nmissing values, resulting in a significant performance improvement for all\nstate-of-the-arts EHR analysis models on four real-world datasets across two\nclinical prediction tasks. Further experimental analysis indicates that PAI\nexhibits higher robustness in situations of data insufficiency and high missing\nrates. More importantly, as a plug-and-play protocol, PAI can be easily\nintegrated into any existing or even imperceptible future EHR analysis models.\n","versions":"[{'version': 'v1', 'created': 'Tue, 30 Jan 2024 07:19:36 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 06:17:29 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Liao', 'Weibin', ''], ['Zhu', 'Yinghao', ''], ['Zhang', 'Zhongji', ''], ['Wang', 'Yuhang', ''], ['Wang', 'Zixiang', ''], ['Chu', 'Xu', ''], ['Wang', 'Yasha', ''], ['Ma', 'Liantao', '']]","extracted_entities":"[{'text': 'Jointly Optimizing of\\nImpute-n-Regress Procedure', 'label': 'Prompting'}, {'text': 'learnable prompt', 'label': 'Prompting'}, {'text': 'Learnable Prompt', 'label': 'Prompting'}, {'text': 'PAI', 'label': 'Prompting'}, {'text': 'PAI', 'label': 'Prompting'}, {'text': 'learnable prompt', 'label': 'Prompting'}, {'text': 'PAI', 'label': 'Prompting'}, {'text': 'PAI', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"learnable prompt","similarity_score":0.6185894608}
{"id":2405.10311,"submitter":"Sahel Sharifymoghaddam","authors":"Sahel Sharifymoghaddam, Shivani Upadhyay, Wenhu Chen, Jimmy Lin","title":"UniRAG: Universal Retrieval Augmentation for Large Vision Language\n  Models","comments":"14 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recently, Large Vision Language Models (LVLMs) have unlocked many complex use\ncases that require Multi-Modal (MM) understanding (e.g., image captioning or\nvisual question answering) and MM generation (e.g., text-guided image\ngeneration or editing) capabilities. To further improve the output fidelityof\nLVLMs we introduce UniRAG, a plug-and-play technique that adds relevant\nretrieved information to prompts as few-shot examples during inference. Unlike\nthe common belief that Retrieval Augmentation (RA) mainly improves generation\nor understanding of uncommon entities, our evaluation results on the MSCOCO\ndataset with common entities show that both proprietary models like GPT-4o and\nGemini-Pro and smaller open-source models like LLaVA, LaVIT, and Emu2\nsignificantly enhance their generation quality when their input prompts are\naugmented with relevant information retrieved by Vision-Language (VL)\nretrievers like UniIR models. All the necessary code to reproduce our results\nis available at https:\/\/github.com\/castorini\/UniRAG\n","versions":"[{'version': 'v1', 'created': 'Thu, 16 May 2024 17:58:45 GMT'}, {'version': 'v2', 'created': 'Sun, 20 Oct 2024 05:49:18 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 19:13:53 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Sharifymoghaddam', 'Sahel', ''], ['Upadhyay', 'Shivani', ''], ['Chen', 'Wenhu', ''], ['Lin', 'Jimmy', '']]","extracted_entities":"[{'text': 'UniRAG', 'label': 'RAG'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'UniRAG', 'label': 'RAG'}]","assigned_concept":"Prompting","matched_keyword":"prompts","similarity_score":0.7638334036}
{"id":2405.17631,"submitter":"Yusuf Roohani","authors":"Yusuf Roohani, Andrew Lee, Qian Huang, Jian Vora, Zachary Steinhart,\n  Kexin Huang, Alexander Marson, Percy Liang, Jure Leskovec","title":"BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation\n  Experiments","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CE cs.MA","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Agents based on large language models have shown great potential in\naccelerating scientific discovery by leveraging their rich background knowledge\nand reasoning capabilities. In this paper, we introduce BioDiscoveryAgent, an\nagent that designs new experiments, reasons about their outcomes, and\nefficiently navigates the hypothesis space to reach desired solutions. We\ndemonstrate our agent on the problem of designing genetic perturbation\nexperiments, where the aim is to find a small subset out of many possible genes\nthat, when perturbed, result in a specific phenotype (e.g., cell growth).\nUtilizing its biological knowledge, BioDiscoveryAgent can uniquely design new\nexperiments without the need to train a machine learning model or explicitly\ndesign an acquisition function as in Bayesian optimization. Moreover,\nBioDiscoveryAgent, using Claude 3.5 Sonnet, achieves an average of 21%\nimprovement in predicting relevant genetic perturbations across six datasets,\nand a 46% improvement in the harder task of non-essential gene perturbation,\ncompared to existing Bayesian optimization baselines specifically trained for\nthis task. Our evaluation includes one dataset that is unpublished, ensuring it\nis not part of the language model's training data. Additionally,\nBioDiscoveryAgent predicts gene combinations to perturb more than twice as\naccurately as a random baseline, a task so far not explored in the context of\nclosed-loop experiment design. The agent also has access to tools for searching\nthe biomedical literature, executing code to analyze biological datasets, and\nprompting another agent to critically evaluate its predictions. Overall,\nBioDiscoveryAgent is interpretable at every stage, representing an accessible\nnew paradigm in the computational design of biological experiments with the\npotential to augment scientists' efficacy.\n","versions":"[{'version': 'v1', 'created': 'Mon, 27 May 2024 19:57:17 GMT'}, {'version': 'v2', 'created': 'Sun, 6 Oct 2024 04:55:16 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 21:57:20 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Roohani', 'Yusuf', ''], ['Lee', 'Andrew', ''], ['Huang', 'Qian', ''], ['Vora', 'Jian', ''], ['Steinhart', 'Zachary', ''], ['Huang', 'Kexin', ''], ['Marson', 'Alexander', ''], ['Liang', 'Percy', ''], ['Leskovec', 'Jure', '']]","extracted_entities":"[{'text': 'prompting', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2405.19653,"submitter":"Patrick Emami","authors":"Patrick Emami, Zhaonan Li, Saumya Sinha, Truc Nguyen","title":"SysCaps: Language Interfaces for Simulation Surrogates of Complex\n  Systems","comments":"Accepted at ICLR 2025. 23 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL cs.SY eess.SY","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Surrogate models are used to predict the behavior of complex energy systems\nthat are too expensive to simulate with traditional numerical methods. Our work\nintroduces the use of language descriptions, which we call ``system captions''\nor SysCaps, to interface with such surrogates. We argue that interacting with\nsurrogates through text, particularly natural language, makes these models more\naccessible for both experts and non-experts. We introduce a lightweight\nmultimodal text and timeseries regression model and a training pipeline that\nuses large language models (LLMs) to synthesize high-quality captions from\nsimulation metadata. Our experiments on two real-world simulators of buildings\nand wind farms show that our SysCaps-augmented surrogates have better accuracy\non held-out systems than traditional methods while enjoying new generalization\nabilities, such as handling semantically related descriptions of the same test\nsystem. Additional experiments also highlight the potential of SysCaps to\nunlock language-driven design space exploration and to regularize training\nthrough prompt augmentation.\n","versions":"[{'version': 'v1', 'created': 'Thu, 30 May 2024 03:12:04 GMT'}, {'version': 'v2', 'created': 'Wed, 2 Oct 2024 16:23:12 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 00:01:37 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Emami', 'Patrick', ''], ['Li', 'Zhaonan', ''], ['Sinha', 'Saumya', ''], ['Nguyen', 'Truc', '']]","extracted_entities":"[{'text': 'SysCaps', 'label': 'LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompt augmentation', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt augmentation","similarity_score":0.6048725843}
{"id":2407.04619,"submitter":"Niki Amini-Naieni","authors":"Niki Amini-Naieni, Tengda Han, Andrew Zisserman","title":"CountGD: Multi-Modal Open-World Counting","comments":"NeurIPS 2024","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The goal of this paper is to improve the generality and accuracy of\nopen-vocabulary object counting in images. To improve the generality, we\nrepurpose an open-vocabulary detection foundation model (GroundingDINO) for the\ncounting task, and also extend its capabilities by introducing modules to\nenable specifying the target object to count by visual exemplars. In turn,\nthese new capabilities - being able to specify the target object by\nmulti-modalites (text and exemplars) - lead to an improvement in counting\naccuracy.\n  We make three contributions: First, we introduce the first open-world\ncounting model, CountGD, where the prompt can be specified by a text\ndescription or visual exemplars or both; Second, we show that the performance\nof the model significantly improves the state of the art on multiple counting\nbenchmarks - when using text only, CountGD is comparable to or outperforms all\nprevious text-only works, and when using both text and visual exemplars, we\noutperform all previous models; Third, we carry out a preliminary study into\ndifferent interactions between the text and visual exemplar prompts, including\nthe cases where they reinforce each other and where one restricts the other.\nThe code and an app to test the model are available at\nhttps:\/\/www.robots.ox.ac.uk\/~vgg\/research\/countgd\/.\n","versions":"[{'version': 'v1', 'created': 'Fri, 5 Jul 2024 16:20:48 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 20:54:29 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Amini-Naieni', 'Niki', ''], ['Han', 'Tengda', ''], ['Zisserman', 'Andrew', '']]","extracted_entities":"[{'text': 'GroundingDINO', 'label': 'Foundation Model'}, {'text': 'prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt","similarity_score":0.7767513394}
{"id":2407.10645,"submitter":"Charles Arnal","authors":"Louis Abraham, Charles Arnal, Antoine Marie","title":"Prompt Selection Matters: Enhancing Text Annotations for Social Sciences\n  with Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models have recently been applied to text annotation tasks\nfrom social sciences, equalling or surpassing the performance of human workers\nat a fraction of the cost. However, no inquiry has yet been made on the impact\nof prompt selection on labelling accuracy. In this study, we show that\nperformance greatly varies between prompts, and we apply the method of\nautomatic prompt optimization to systematically craft high quality prompts. We\nalso provide the community with a simple, browser-based implementation of the\nmethod at https:\/\/prompt-ultra.github.io\/ .\n","versions":"[{'version': 'v1', 'created': 'Mon, 15 Jul 2024 12:04:32 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 10:35:53 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Abraham', 'Louis', ''], ['Arnal', 'Charles', ''], ['Marie', 'Antoine', '']]","extracted_entities":"[{'text': 'prompts', 'label': 'Prompting'}, {'text': 'automatic prompt optimization', 'label': 'Fine-tuning'}, {'text': 'prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompts","similarity_score":0.7638334036}
{"id":2408.09108,"submitter":"Yongqi Ding","authors":"Lin Zuo, Yongqi Ding, Wenwei Luo, Mengmeng Jing, Kunshan Yang","title":"Temporal Reversal Regularization for Spiking Neural Networks: Hybrid\n  Spatio-Temporal Invariance for Generalization","comments":"17 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Spiking neural networks (SNNs) have received widespread attention as an\nultra-low power computing paradigm. Recent studies have shown that SNNs suffer\nfrom severe overfitting, which limits their generalization performance. In this\npaper, we propose a simple yet effective Temporal Reversal Regularization (TRR)\nto mitigate overfitting during training and facilitate generalization of SNNs.\nWe exploit the inherent temporal properties of SNNs to perform input\/feature\ntemporal reversal perturbations, prompting the SNN to produce original-reversed\nconsistent outputs and learn perturbation-invariant representations. To further\nenhance generalization, we utilize the lightweight ``star operation\" (Hadamard\nproduct) for feature hybridization of original and temporally reversed spike\nfiring rates, which expands the implicit dimensionality and acts as a\nspatio-temporal regularizer. We show theoretically that our method is able to\ntighten the upper bound of the generalization error, and extensive experiments\non static\/neuromorphic recognition as well as 3D point cloud classification\ntasks demonstrate its effectiveness, versatility, and adversarial robustness.\nIn particular, our regularization significantly improves the recognition\naccuracy of low-latency SNN for neuromorphic objects, contributing to the\nreal-world deployment of neuromorphic computational software-hardware\nintegration.\n","versions":"[{'version': 'v1', 'created': 'Sat, 17 Aug 2024 06:23:38 GMT'}, {'version': 'v2', 'created': 'Wed, 27 Nov 2024 04:25:26 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 08:30:52 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zuo', 'Lin', ''], ['Ding', 'Yongqi', ''], ['Luo', 'Wenwei', ''], ['Jing', 'Mengmeng', ''], ['Yang', 'Kunshan', '']]","extracted_entities":"[{'text': 'SNNs', 'label': 'Neural Language Model'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'feature hybridization', 'label': 'Embedding'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2409.06214,"submitter":"Kim Jaewoo","authors":"Jaewoo Kim, Uehwan Kim","title":"Towards Generalizable Scene Change Detection","comments":"Camera-ready version. Accepted to CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  While current state-of-the-art Scene Change Detection (SCD) approaches\nachieve impressive results in well-trained research data, they become\nunreliable under unseen environments and different temporal conditions;\nin-domain performance drops from 77.6% to 8.0% in a previously unseen\nenvironment and to 4.6% under a different temporal condition -- calling for\ngeneralizable SCD and benchmark. In this work, we propose the Generalizable\nScene Change Detection Framework (GeSCF), which addresses unseen domain\nperformance and temporal consistency -- to meet the growing demand for anything\nSCD. Our method leverages the pre-trained Segment Anything Model (SAM) in a\nzero-shot manner. For this, we design Initial Pseudo-mask Generation and\nGeometric-Semantic Mask Matching -- seamlessly turning user-guided prompt and\nsingle-image based segmentation into scene change detection for a pair of\ninputs without guidance. Furthermore, we define the Generalizable Scene Change\nDetection (GeSCD) benchmark along with novel metrics and an evaluation protocol\nto facilitate SCD research in generalizability. In the process, we introduce\nthe ChangeVPR dataset, a collection of challenging image pairs with diverse\nenvironmental scenarios -- including urban, suburban, and rural settings.\nExtensive experiments across various datasets demonstrate that GeSCF achieves\nan average performance gain of 19.2% on existing SCD datasets and 30.0% on the\nChangeVPR dataset, nearly doubling the prior art performance. We believe our\nwork can lay a solid foundation for robust and generalizable SCD research.\n","versions":"[{'version': 'v1', 'created': 'Tue, 10 Sep 2024 04:45:25 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Feb 2025 05:28:05 GMT'}, {'version': 'v3', 'created': 'Mon, 3 Mar 2025 01:46:42 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 13:55:30 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Kim', 'Jaewoo', ''], ['Kim', 'Uehwan', '']]","extracted_entities":"[{'text': 'user-guided prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"user-guided prompt","similarity_score":0.7089807987}
{"id":2409.2056,"submitter":"Jiachen Li","authors":"Xiaopan Zhang and Hao Qin and Fuquan Wang and Yue Dong and Jiachen Li","title":"LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and\n  Planning with LM-Driven PDDL Planner","comments":"IEEE Conference on Robotics and Automation (ICRA 2025); Project\n  website: https:\/\/lamma-p.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI cs.CV cs.LG cs.MA","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Language models (LMs) possess a strong capability to comprehend natural\nlanguage, making them effective in translating human instructions into detailed\nplans for simple robot tasks. Nevertheless, it remains a significant challenge\nto handle long-horizon tasks, especially in subtask identification and\nallocation for cooperative heterogeneous robot teams. To address this issue, we\npropose a Language Model-Driven Multi-Agent PDDL Planner (LaMMA-P), a novel\nmulti-agent task planning framework that achieves state-of-the-art performance\non long-horizon tasks. LaMMA-P integrates the strengths of the LMs' reasoning\ncapability and the traditional heuristic search planner to achieve a high\nsuccess rate and efficiency while demonstrating strong generalization across\ntasks. Additionally, we create MAT-THOR, a comprehensive benchmark that\nfeatures household tasks with two different levels of complexity based on the\nAI2-THOR environment. The experimental results demonstrate that LaMMA-P\nachieves a 105% higher success rate and 36% higher efficiency than existing\nLM-based multiagent planners. The experimental videos, code, datasets, and\ndetailed prompts used in each module can be found on the project website:\nhttps:\/\/lamma-p.github.io.\n","versions":"[{'version': 'v1', 'created': 'Mon, 30 Sep 2024 17:58:18 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 06:17:58 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhang', 'Xiaopan', ''], ['Qin', 'Hao', ''], ['Wang', 'Fuquan', ''], ['Dong', 'Yue', ''], ['Li', 'Jiachen', '']]","extracted_entities":"[{'text': 'detailed prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"detailed prompts","similarity_score":0.640391469}
{"id":2410.01405,"submitter":"Kevin Xu","authors":"Kevin Xu and Issei Sato","title":"On Expressive Power of Looped Transformers: Theoretical Analysis and\n  Enhancement via Timestep Encoding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Looped Transformers provide advantages in parameter efficiency, computational\ncapabilities, and generalization for reasoning tasks. However, their expressive\npower regarding function approximation remains underexplored. In this paper, we\nestablish the approximation rate of Looped Transformers by defining the modulus\nof continuity for sequence-to-sequence functions. This reveals a limitation\nspecific to the looped architecture. That is, the analysis prompts the\nincorporation of scaling parameters for each loop, conditioned on timestep\nencoding. Experiments validate the theoretical results, showing that increasing\nthe number of loops enhances performance, with further gains achieved through\nthe timestep encoding.\n","versions":"[{'version': 'v1', 'created': 'Wed, 2 Oct 2024 10:31:17 GMT'}, {'version': 'v2', 'created': 'Tue, 8 Oct 2024 16:41:40 GMT'}, {'version': 'v3', 'created': 'Mon, 25 Nov 2024 08:17:14 GMT'}, {'version': 'v4', 'created': 'Mon, 3 Feb 2025 07:30:21 GMT'}, {'version': 'v5', 'created': 'Tue, 11 Mar 2025 15:51:21 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Xu', 'Kevin', ''], ['Sato', 'Issei', '']]","extracted_entities":"[{'text': 'Looped Transformers', 'label': 'Transformers'}, {'text': 'Looped Transformers', 'label': 'Transformers'}, {'text': 'modulus\\nof continuity', 'label': 'Scaling law'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'timestep\\nencoding', 'label': 'Embedding'}, {'text': 'timestep encoding', 'label': 'Embedding'}]","assigned_concept":"Prompting","matched_keyword":"prompts","similarity_score":0.7638334036}
{"id":2410.04579,"submitter":"Tianjian Li","authors":"Tianjian Li, Haoran Xu, Weiting Tan, Kenton Murray, Daniel Khashabi","title":"Upsample or Upweight? Balanced Training on Heavily Imbalanced Datasets","comments":"19 pages, 9 figures, accepted to NAACL 2025 main conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG stat.ML","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Data abundance across different domains exhibits a long-tailed distribution:\nfew domains have abundant data, while most face data scarcity. Our work focuses\non a multilingual setting, where available data is heavily skewed towards\nhigh-resource languages. Two common strategies to address this disparity are\nupsampling low-resource data (Temperature Sampling) and upweighting\nlow-resource loss (Scalarization). These methods are often assumed to be\nequivalent, but this equivalence has not been rigorously established, prompting\nour investigation.\n  Through theoretical and empirical analysis, we identify when these two\nmethods are equivalent and when they diverge. We prove that they are equivalent\nunder full gradient descent but differ under stochastic gradient descent due to\ndifferences in gradient variance. Specifically, Temperature Sampling exhibits\nlower variance in gradient estimation compared to Scalarization, leading to\nfaster convergence but a higher risk of overfitting. Based on these insights,\nwe propose Cooldown, a strategy that starts by heavily upsampling low-resource\nlanguages to accelerate convergence and gradually reduces the upsampling to\nprevent overfitting -- achieving the best of both worlds. Our method competes\neffectively with existing data re-weighting techniques while offering\ncomputational efficiency.\n","versions":"[{'version': 'v1', 'created': 'Sun, 6 Oct 2024 18:29:46 GMT'}, {'version': 'v2', 'created': 'Thu, 10 Oct 2024 14:16:43 GMT'}, {'version': 'v3', 'created': 'Sat, 9 Nov 2024 03:06:21 GMT'}, {'version': 'v4', 'created': 'Fri, 15 Nov 2024 21:33:18 GMT'}, {'version': 'v5', 'created': 'Sun, 9 Mar 2025 23:07:33 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Li', 'Tianjian', ''], ['Xu', 'Haoran', ''], ['Tan', 'Weiting', ''], ['Murray', 'Kenton', ''], ['Khashabi', 'Daniel', '']]","extracted_entities":"[{'text': 'Temperature Sampling', 'label': 'Zero-shot Learning'}, {'text': 'Scalarization', 'label': 'Few-shot Learning'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'Temperature Sampling', 'label': 'Few-shot Learning'}, {'text': 'Scalarization', 'label': 'Few-shot Learning'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2410.11843,"submitter":"Zeru Shi","authors":"Zeru Shi, Kai Mei, Mingyu Jin, Yongye Su, Chaoji Zuo, Wenyue Hua,\n  Wujiang Xu, Yujie Ren, Zirui Liu, Mengnan Du, Dong Deng, Yongfeng Zhang","title":"From Commands to Prompts: LLM-based Semantic File System for AIOS","comments":null,"journal-ref":"ICLR2025","doi":null,"report-no":null,"categories":"cs.HC cs.AI cs.DB cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs) have demonstrated significant potential in the\ndevelopment of intelligent applications and systems such as LLM-based agents\nand agent operating systems (AIOS). However, when these applications and\nsystems interact with the underlying file system, the file system still remains\nthe traditional paradigm: reliant on manual navigation through precise\ncommands. This paradigm poses a bottleneck to the usability of these systems as\nusers are required to navigate complex folder hierarchies and remember cryptic\nfile names. To address this limitation, we propose an LLM-based semantic file\nsystem ( LSFS ) for prompt-driven file management. Unlike conventional\napproaches, LSFS incorporates LLMs to enable users or agents to interact with\nfiles through natural language prompts, facilitating semantic file management.\nAt the macro-level, we develop a comprehensive API set to achieve semantic file\nmanagement functionalities, such as semantic file retrieval, file update\nmonitoring and summarization, and semantic file rollback). At the micro-level,\nwe store files by constructing semantic indexes for them, design and implement\nsyscalls of different semantic operations (e.g., CRUD, group by, join) powered\nby vector database. Our experiments show that LSFS offers significant\nimprovements over traditional file systems in terms of user convenience, the\ndiversity of supported functions, and the accuracy and efficiency of file\noperations. Additionally, with the integration of LLM, our system enables more\nintelligent file management tasks, such as content summarization and version\ncomparison, further enhancing its capabilities.\n","versions":"[{'version': 'v1', 'created': 'Mon, 23 Sep 2024 08:39:16 GMT'}, {'version': 'v2', 'created': 'Fri, 27 Dec 2024 08:32:38 GMT'}, {'version': 'v3', 'created': 'Fri, 28 Feb 2025 15:41:00 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Mar 2025 10:50:44 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Shi', 'Zeru', ''], ['Mei', 'Kai', ''], ['Jin', 'Mingyu', ''], ['Su', 'Yongye', ''], ['Zuo', 'Chaoji', ''], ['Hua', 'Wenyue', ''], ['Xu', 'Wujiang', ''], ['Ren', 'Yujie', ''], ['Liu', 'Zirui', ''], ['Du', 'Mengnan', ''], ['Deng', 'Dong', ''], ['Zhang', 'Yongfeng', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'natural language prompts', 'label': 'Prompting'}, {'text': 'LSFS', 'label': 'Large Language Model'}]","assigned_concept":"Prompting","matched_keyword":"natural language prompts","similarity_score":0.6456617713}
{"id":2410.14405,"submitter":"Denitsa Saynova","authors":"Denitsa Saynova, Lovisa Hagstr\\\"om, Moa Johansson, Richard Johansson,\n  Marco Kuhlmann","title":"Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of\n  Language Models for Fact Completion","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Language models (LMs) can make a correct prediction based on many possible\nsignals in a prompt, not all corresponding to recall of factual associations.\nHowever, current interpretations of LMs fail to take this into account. For\nexample, given the query \"Astrid Lindgren was born in\" with the corresponding\ncompletion \"Sweden\", no difference is made between whether the prediction was\nbased on knowing where the author was born or assuming that a person with a\nSwedish-sounding name was born in Sweden. In this paper, we present a\nmodel-specific recipe - PrISM - for constructing datasets with examples of four\ndifferent prediction scenarios: generic language modeling, guesswork,\nheuristics recall and exact fact recall. We apply two popular interpretability\nmethods to the scenarios: causal tracing (CT) and information flow analysis. We\nfind that both yield distinct results for each scenario. Results for exact fact\nrecall and generic language modeling scenarios confirm previous conclusions\nabout the importance of mid-range MLP sublayers for fact recall, while results\nfor guesswork and heuristics indicate a critical role of late last token\nposition MLP sublayers. In summary, we contribute resources for a more\nextensive and granular study of fact completion in LMs, together with analyses\nthat provide a more nuanced understanding of how LMs process fact-related\nqueries.\n","versions":"[{'version': 'v1', 'created': 'Fri, 18 Oct 2024 12:08:07 GMT'}, {'version': 'v2', 'created': 'Thu, 31 Oct 2024 08:44:13 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 12:47:31 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Saynova', 'Denitsa', ''], ['Hagstr\u00f6m', 'Lovisa', ''], ['Johansson', 'Moa', ''], ['Johansson', 'Richard', ''], ['Kuhlmann', 'Marco', '']]","extracted_entities":"[{'text': 'prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt","similarity_score":0.7767513394}
{"id":2410.17448,"submitter":"Tyler Josephson","authors":"Samiha Sharlin, Tyler R. Josephson","title":"In Context Learning and Reasoning for Symbolic Regression with Large\n  Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) are transformer-based machine learning models\nthat have shown remarkable performance in tasks for which they were not\nexplicitly trained. Here, we explore the potential of LLMs to perform symbolic\nregression -- a machine-learning method for finding simple and accurate\nequations from datasets. We prompt GPT-4 to suggest expressions from data,\nwhich are then optimized and evaluated using external Python tools. These\nresults are fed back to GPT-4, which proposes improved expressions while\noptimizing for complexity and loss. Using chain-of-thought prompting, we\ninstruct GPT-4 to analyze the data, prior expressions, and the scientific\ncontext (expressed in natural language) for each problem before generating new\nexpressions. We evaluated the workflow in rediscovery of five well-known\nscientific equations from experimental data, and on an additional dataset\nwithout a known equation. GPT-4 successfully rediscovered all five equations,\nand in general, performed better when prompted to use a scratchpad and consider\nscientific context. We demonstrate how strategic prompting improves the model's\nperformance and how the natural language interface simplifies integrating\ntheory with data. We also observe how theory can sometimes offset noisy data\nand, in other cases, data can make up for poor context. Although this approach\ndoes not outperform established SR programs where target equations are more\ncomplex, LLMs can nonetheless iterate toward improved solutions while following\ninstructions and incorporating scientific context in natural language.\n","versions":"[{'version': 'v1', 'created': 'Tue, 22 Oct 2024 21:50:52 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 13:14:22 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Sharlin', 'Samiha', ''], ['Josephson', 'Tyler R.', '']]","extracted_entities":"[{'text': 'chain-of-thought prompting', 'label': 'Prompting'}, {'text': 'scientific\\ncontext', 'label': 'contextual Embedding'}, {'text': 'scientific context', 'label': 'contextual Embedding'}, {'text': 'scientific context', 'label': 'contextual Embedding'}]","assigned_concept":"Prompting","matched_keyword":"chain-of-thought prompting","similarity_score":0.6491806507}
{"id":2410.23746,"submitter":"Runzhe Zhan","authors":"Junchao Wu, Runzhe Zhan, Derek F. Wong, Shu Yang, Xinyi Yang, Yulin\n  Yuan, Lidia S. Chao","title":"DetectRL: Benchmarking LLM-Generated Text Detection in Real-World\n  Scenarios","comments":"Accepted to NeurIPS 2024 Datasets and Benchmarks Track (Camera-Ready)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Detecting text generated by large language models (LLMs) is of great recent\ninterest. With zero-shot methods like DetectGPT, detection capabilities have\nreached impressive levels. However, the reliability of existing detectors in\nreal-world applications remains underexplored. In this study, we present a new\nbenchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection\ntechniques still underperformed in this task. We collected human-written\ndatasets from domains where LLMs are particularly prone to misuse. Using\npopular LLMs, we generated data that better aligns with real-world\napplications. Unlike previous studies, we employed heuristic rules to create\nadversarial LLM-generated text, simulating various prompts usages, human\nrevisions like word substitutions, and writing noises like spelling mistakes.\nOur development of DetectRL reveals the strengths and limitations of current\nSOTA detectors. More importantly, we analyzed the potential impact of writing\nstyles, model types, attack methods, the text lengths, and real-world human\nwriting factors on different types of detectors. We believe DetectRL could\nserve as an effective benchmark for assessing detectors in real-world\nscenarios, evolving with advanced attack methods, thus providing more stressful\nevaluation to drive the development of more efficient detectors. Data and code\nare publicly available at: https:\/\/github.com\/NLP2CT\/DetectRL.\n","versions":"[{'version': 'v1', 'created': 'Thu, 31 Oct 2024 09:01:25 GMT'}, {'version': 'v2', 'created': 'Fri, 7 Mar 2025 09:06:03 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 10:08:22 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Wu', 'Junchao', ''], ['Zhan', 'Runzhe', ''], ['Wong', 'Derek F.', ''], ['Yang', 'Shu', ''], ['Yang', 'Xinyi', ''], ['Yuan', 'Yulin', ''], ['Chao', 'Lidia S.', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompts","similarity_score":0.7638334036}
{"id":2411.05039,"submitter":"Subhankar Maity","authors":"Aniket Deroy, Subhankar Maity","title":"YouTube Comments Decoded: Leveraging LLMs for Low Resource Language\n  Classification","comments":"Updated and Final Version","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Sarcasm detection is a significant challenge in sentiment analysis,\nparticularly due to its nature of conveying opinions where the intended meaning\ndeviates from the literal expression. This challenge is heightened in social\nmedia contexts where code-mixing, especially in Dravidian languages, is\nprevalent. Code-mixing involves the blending of multiple languages within a\nsingle utterance, often with non-native scripts, complicating the task for\nsystems trained on monolingual data. This shared task introduces a novel gold\nstandard corpus designed for sarcasm and sentiment detection within code-mixed\ntexts, specifically in Tamil-English and Malayalam-English languages. The\nprimary objective of this task is to identify sarcasm and sentiment polarity\nwithin a code-mixed dataset of Tamil-English and Malayalam-English comments and\nposts collected from social media platforms. Each comment or post is annotated\nat the message level for sentiment polarity, with particular attention to the\nchallenges posed by class imbalance, reflecting real-world scenarios.In this\nwork, we experiment with state-of-the-art large language models like GPT-3.5\nTurbo via prompting to classify comments into sarcastic or non-sarcastic\ncategories. We obtained a macro-F1 score of 0.61 for Tamil language. We\nobtained a macro-F1 score of 0.50 for Malayalam language.\n","versions":"[{'version': 'v1', 'created': 'Wed, 6 Nov 2024 17:58:01 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 16:17:21 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Deroy', 'Aniket', ''], ['Maity', 'Subhankar', '']]","extracted_entities":"[{'text': 'prompting', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2411.10639,"submitter":"Yunsheng Ma","authors":"Yunsheng Ma, Burhaneddin Yaman, Xin Ye, Jingru Luo, Feng Tao, Abhirup\n  Mallik, Ziran Wang, Liu Ren","title":"MTA: Multimodal Task Alignment for BEV Perception and Captioning","comments":"10 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Bird's eye view (BEV)-based 3D perception plays a crucial role in autonomous\ndriving applications. The rise of large language models has spurred interest in\nBEV-based captioning to understand object behavior in the surrounding\nenvironment. However, existing approaches treat perception and captioning as\nseparate tasks, focusing on the performance of only one task and overlooking\nthe potential benefits of multimodal alignment. To bridge this gap between\nmodalities, we introduce MTA, a novel multimodal task alignment framework that\nboosts both BEV perception and captioning. MTA consists of two key components:\n(1) BEV-Language Alignment (BLA), a contextual learning mechanism that aligns\nthe BEV scene representations with ground-truth language representations, and\n(2) Detection-Captioning Alignment (DCA), a cross-modal prompting mechanism\nthat aligns detection and captioning outputs. MTA seamlessly integrates into\nstate-of-the-art baselines during training, adding no extra computational\ncomplexity at runtime. Extensive experiments on the nuScenes and TOD3Cap\ndatasets show that MTA significantly outperforms state-of-the-art baselines in\nboth tasks, achieving a 10.7% improvement in challenging rare perception\nscenarios and a 9.2% improvement in captioning. These results underscore the\neffectiveness of unified alignment in reconciling BEV-based perception and\ncaptioning.\n","versions":"[{'version': 'v1', 'created': 'Sat, 16 Nov 2024 00:14:13 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 20:59:22 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Ma', 'Yunsheng', ''], ['Yaman', 'Burhaneddin', ''], ['Ye', 'Xin', ''], ['Luo', 'Jingru', ''], ['Tao', 'Feng', ''], ['Mallik', 'Abhirup', ''], ['Wang', 'Ziran', ''], ['Ren', 'Liu', '']]","extracted_entities":"[{'text': 'MTA', 'label': 'contextual Embedding'}, {'text': 'MTA', 'label': 'contextual Embedding'}, {'text': 'BEV-Language Alignment', 'label': 'contextual Embedding'}, {'text': 'Detection-Captioning Alignment', 'label': 'contextual Embedding'}, {'text': 'cross-modal prompting mechanism', 'label': 'Prompting'}, {'text': 'MTA', 'label': 'contextual Embedding'}, {'text': 'MTA', 'label': 'contextual Embedding'}]","assigned_concept":"Prompting","matched_keyword":"cross-modal prompting mechanism","similarity_score":0.6364893913}
{"id":2411.15922,"submitter":"Chia-Ming Lee","authors":"Chia-Ming Lee and Ching-Heng Cheng and Yu-Fan Lin and Yi-Ching Cheng\n  and Wo-Ting Liao and Fu-En Yang and Yu-Chiang Frank Wang and Chih-Chung Hsu","title":"PromptHSI: Universal Hyperspectral Image Restoration with\n  Vision-Language Modulated Frequency Adaptation","comments":"Project page: https:\/\/chingheng0808.github.io\/prompthsiP\/static.html","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent advances in All-in-One (AiO) RGB image restoration have demonstrated\nthe effectiveness of prompt learning in handling multiple degradations within a\nsingle model. However, extending these approaches to hyperspectral image (HSI)\nrestoration is challenging due to the domain gap between RGB and HSI features,\ninformation loss in visual prompts under severe composite degradations, and\ndifficulties in capturing HSI-specific degradation patterns via text prompts.\nIn this paper, we propose PromptHSI, the first universal AiO HSI restoration\nframework that addresses these challenges. By incorporating frequency-aware\nfeature modulation, which utilizes frequency analysis to narrow down the\nrestoration search space and employing vision-language model (VLM)-guided\nprompt learning, our approach decomposes text prompts into intensity and bias\ncontrollers that effectively guide the restoration process while mitigating\ndomain discrepancies. Extensive experiments demonstrate that our unified\narchitecture excels at both fine-grained recovery and global information\nrestoration across diverse degradation scenarios, highlighting its significant\npotential for practical remote sensing applications. The source code is\navailable at https:\/\/github.com\/chingheng0808\/PromptHSI.\n","versions":"[{'version': 'v1', 'created': 'Sun, 24 Nov 2024 17:08:58 GMT'}, {'version': 'v2', 'created': 'Fri, 29 Nov 2024 02:26:50 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 06:47:38 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Lee', 'Chia-Ming', ''], ['Cheng', 'Ching-Heng', ''], ['Lin', 'Yu-Fan', ''], ['Cheng', 'Yi-Ching', ''], ['Liao', 'Wo-Ting', ''], ['Yang', 'Fu-En', ''], ['Wang', 'Yu-Chiang Frank', ''], ['Hsu', 'Chih-Chung', '']]","extracted_entities":"[{'text': 'prompt learning', 'label': 'Prompting'}, {'text': 'prompt learning', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt learning","similarity_score":0.5904975533}
{"id":2412.02542,"submitter":"Quang Nguyen","authors":"Quang H. Nguyen, Hoang Phan, Khoa D. Doan","title":"Unveiling Concept Attribution in Diffusion Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Diffusion models have shown remarkable abilities in generating realistic and\nhigh-quality images from text prompts. However, a trained model remains largely\nblack-box; little do we know about the roles of its components in exhibiting a\nconcept such as objects or styles. Recent works employ causal tracing to\nlocalize knowledge-storing layers in generative models without showing how\nother layers contribute to the target concept. In this work, we approach\ndiffusion models' interpretability problem from a more general perspective and\npose a question: \\textit{``How do model components work jointly to demonstrate\nknowledge?''}. To answer this question, we decompose diffusion models using\ncomponent attribution, systematically unveiling the importance of each\ncomponent (specifically the model parameter) in generating a concept. The\nproposed framework, called \\textbf{C}omponent \\textbf{A}ttribution for\n\\textbf{D}iffusion Model (CAD), discovers the localization of concept-inducing\n(positive) components, while interestingly uncovers another type of components\nthat contribute negatively to generating a concept, which is missing in the\nprevious knowledge localization work. Based on this holistic understanding of\ndiffusion models, we introduce two fast, inference-time model editing\nalgorithms, CAD-Erase and CAD-Amplify; in particular, CAD-Erase enables erasure\nand CAD-Amplify allows amplification of a generated concept by ablating the\npositive and negative components, respectively, while retaining knowledge of\nother concepts. Extensive experimental results validate the significance of\nboth positive and negative components pinpointed by our framework,\ndemonstrating the potential of providing a complete view of interpreting\ngenerative models. Our code is available\n\\href{https:\/\/github.com\/mail-research\/CAD-attribution4diffusion}{here}.\n","versions":"[{'version': 'v1', 'created': 'Tue, 3 Dec 2024 16:34:49 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 09:02:44 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Nguyen', 'Quang H.', ''], ['Phan', 'Hoang', ''], ['Doan', 'Khoa D.', '']]","extracted_entities":"[{'text': 'text prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"text prompts","similarity_score":0.6106933355}
{"id":2412.04106,"submitter":"Haoning Wu","authors":"Haoning Wu, Ziheng Zhao, Ya Zhang, Yanfeng Wang, Weidi Xie","title":"MRGen: Segmentation Data Engine For Underrepresented MRI Modalities","comments":"Technical Report; Project Page:\n  https:\/\/haoningwu3639.github.io\/MRGen\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Training medical image segmentation models for rare yet clinically\nsignificant imaging modalities is challenging due to the scarcity of annotated\ndata, and manual mask annotations can be costly and labor-intensive to acquire.\nThis paper investigates leveraging generative models to synthesize training\ndata, to train segmentation models for underrepresented modalities,\nparticularly on annotation-scarce MRI. Concretely, our contributions are\nthreefold: (i) we introduce MRGen-DB, a large-scale radiology image-text\ndataset comprising extensive samples with rich metadata, including modality\nlabels, attributes, regions, and organs information, with a subset having\npixelwise mask annotations; (ii) we present MRGen, a diffusion-based data\nengine for controllable medical image synthesis, conditioned on text prompts\nand segmentation masks. MRGen can generate realistic images for diverse MRI\nmodalities lacking mask annotations, facilitating segmentation training in\nlow-source domains; (iii) extensive experiments across multiple modalities\ndemonstrate that MRGen significantly improves segmentation performance on\nunannotated modalities by providing high-quality synthetic data. We believe\nthat our method bridges a critical gap in medical image analysis, extending\nsegmentation capabilities to scenarios that are challenging to acquire manual\nannotations.\n","versions":"[{'version': 'v1', 'created': 'Wed, 4 Dec 2024 16:34:22 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 11:59:46 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Wu', 'Haoning', ''], ['Zhao', 'Ziheng', ''], ['Zhang', 'Ya', ''], ['Wang', 'Yanfeng', ''], ['Xie', 'Weidi', '']]","extracted_entities":"[{'text': 'text prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"text prompts","similarity_score":0.6106933355}
{"id":2412.06089,"submitter":"Ashish Goswami","authors":"Ashish Goswami, Satyam Kumar Modi, Santhosh Rishi Deshineni, Harman\n  Singh, Prathosh A. P, Parag Singla","title":"GraPE: A Generate-Plan-Edit Framework for Compositional T2I Synthesis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Text-to-image (T2I) generation has seen significant progress with diffusion\nmodels, enabling generation of photo-realistic images from text prompts.\nDespite this progress, existing methods still face challenges in following\ncomplex text prompts, especially those requiring compositional and multi-step\nreasoning. Given such complex instructions, SOTA models often make mistakes in\nfaithfully modeling object attributes, and relationships among them. In this\nwork, we present an alternate paradigm for T2I synthesis, decomposing the task\nof complex multi-step generation into three steps, (a) Generate: we first\ngenerate an image using existing diffusion models (b) Plan: we make use of\nMulti-Modal LLMs (MLLMs) to identify the mistakes in the generated image\nexpressed in terms of individual objects and their properties, and produce a\nsequence of corrective steps required in the form of an edit-plan. (c) Edit: we\nmake use of an existing text-guided image editing models to sequentially\nexecute our edit-plan over the generated image to get the desired image which\nis faithful to the original instruction. Our approach derives its strength from\nthe fact that it is modular in nature, is training free, and can be applied\nover any combination of image generation and editing models. As an added\ncontribution, we also develop a model capable of compositional editing, which\nfurther helps improve the overall accuracy of our proposed approach. Our method\nflexibly trades inference time compute with performance on compositional text\nprompts. We perform extensive experimental evaluation across 3 benchmarks and\n10 T2I models including DALLE-3 and the latest -- SD-3.5-Large. Our approach\nnot only improves the performance of the SOTA models, by upto 3 points, it also\nreduces the performance gap between weaker and stronger models.\n$\\href{https:\/\/dair-iitd.github.io\/GraPE\/}{https:\/\/dair-iitd.github.io\/GraPE\/}$\n","versions":"[{'version': 'v1', 'created': 'Sun, 8 Dec 2024 22:29:56 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 15:34:16 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Goswami', 'Ashish', ''], ['Modi', 'Satyam Kumar', ''], ['Deshineni', 'Santhosh Rishi', ''], ['Singh', 'Harman', ''], ['P', 'Prathosh A.', ''], ['Singla', 'Parag', '']]","extracted_entities":"[{'text': 'text prompts', 'label': 'Prompting'}, {'text': 'text prompts', 'label': 'Prompting'}, {'text': 'Multi-Modal LLMs', 'label': 'LLM'}, {'text': 'text\\nprompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"text prompts","similarity_score":0.6106933355}
{"id":2412.07205,"submitter":"Yingchu Wang","authors":"Yingchu Wang, Ji He, Shijie Yu","title":"CrackESS: A Self-Prompting Crack Segmentation System for Edge Devices","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Structural Health Monitoring (SHM) is a sustainable and essential approach\nfor infrastructure maintenance, enabling the early detection of structural\ndefects. Leveraging computer vision (CV) methods for automated infrastructure\nmonitoring can significantly enhance monitoring efficiency and precision.\nHowever, these methods often face challenges in efficiency and accuracy,\nparticularly in complex environments. Recent CNN-based and SAM-based approaches\nhave demonstrated excellent performance in crack segmentation, but their high\ncomputational demands limit their applicability on edge devices. This paper\nintroduces CrackESS, a novel system for detecting and segmenting concrete\ncracks. The approach first utilizes a YOLOv8 model for self-prompting and a\nLoRA-based fine-tuned SAM model for crack segmentation, followed by refining\nthe segmentation masks through the proposed Crack Mask Refinement Module\n(CMRM). We conduct experiments on three datasets(Khanhha's dataset, Crack500,\nCrackCR) and validate CrackESS on a climbing robot system to demonstrate the\nadvantage and effectiveness of our approach.\n","versions":"[{'version': 'v1', 'created': 'Tue, 10 Dec 2024 05:50:50 GMT'}, {'version': 'v2', 'created': 'Fri, 13 Dec 2024 12:38:04 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 12:55:57 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Wang', 'Yingchu', ''], ['He', 'Ji', ''], ['Yu', 'Shijie', '']]","extracted_entities":"[{'text': 'self-prompting', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"self-prompting","similarity_score":0.8885782957}
{"id":2412.07923,"submitter":"Sagi Shaier","authors":"Sagi Shaier, Mario Sanz-Guerrero, Katharina von der Wense","title":"Asking Again and Again: Exploring LLM Robustness to Repeated Questions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This study investigates whether repeating questions within prompts influences\nthe performance of large language models (LLMs). We hypothesize that\nreiterating a question within a single prompt might enhance the model's focus\non key elements of the query. We evaluate five recent LLMs -- including\nGPT-4o-mini, DeepSeek-V3, and smaller open-source models -- on three reading\ncomprehension datasets under different prompt settings, varying question\nrepetition levels (1, 3, or 5 times per prompt). Our results demonstrate that\nquestion repetition can increase models' accuracy by up to $6\\%$. However,\nacross all models, settings, and datasets, we do not find the result\nstatistically significant. These findings provide insights into prompt design\nand LLM behavior, suggesting that repetition alone does not significantly\nimpact output quality.\n","versions":"[{'version': 'v1', 'created': 'Tue, 10 Dec 2024 21:09:12 GMT'}, {'version': 'v2', 'created': 'Sat, 8 Mar 2025 16:42:51 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 13:48:12 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Shaier', 'Sagi', ''], ['Sanz-Guerrero', 'Mario', ''], ['von der Wense', 'Katharina', '']]","extracted_entities":"[{'text': 'prompt', 'label': 'Prompting'}, {'text': 'smaller open-source models', 'label': 'Open-source LLMs'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt","similarity_score":0.7767513394}
{"id":2501.09484,"submitter":"Zhaocheng Liu","authors":"Zhaocheng Liu, Quan Tu, Wen Ye, Yu Xiao, Zhishou Zhang, Hengfu Cui,\n  Yalun Zhu, Qiang Ju, Shizheng Li, Jian Xie","title":"Exploring the Inquiry-Diagnosis Relationship with Advanced Patient\n  Simulators","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Recently, large language models have shown great potential to transform\nonline medical consultation. Despite this, most research targets improving\ndiagnostic accuracy with ample information, often overlooking the inquiry\nphase. Some studies try to evaluate or refine doctor models by using\nprompt-engineered patient agents. However, prompt engineering alone falls short\nin accurately simulating real patients. We need to explore new paradigms for\npatient simulation. Furthermore, the relationship between inquiry and diagnosis\nremains unexplored. This paper extracts dialogue strategies from real\ndoctor-patient conversations to guide the training of a patient simulator. Our\nsimulator shows higher anthropomorphism and lower hallucination rates, using\ndynamic dialogue strategies. This innovation offers a more accurate evaluation\nof diagnostic models and generates realistic synthetic data. We conduct\nextensive experiments on the relationship between inquiry and diagnosis,\nshowing they adhere to Liebig's law: poor inquiry limits diagnosis\neffectiveness, regardless of diagnostic skill, and vice versa. The experiments\nalso reveal substantial differences in inquiry performance among models. To\ndelve into this phenomenon, the inquiry process is categorized into four\ndistinct types. Analyzing the distribution of inquiries across these types\nhelps explain the performance differences. The weights of our patient simulator\nare available https:\/\/github.com\/PatientSimulator\/PatientSimulator.\n","versions":"[{'version': 'v1', 'created': 'Thu, 16 Jan 2025 11:41:14 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 06:54:09 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Liu', 'Zhaocheng', ''], ['Tu', 'Quan', ''], ['Ye', 'Wen', ''], ['Xiao', 'Yu', ''], ['Zhang', 'Zhishou', ''], ['Cui', 'Hengfu', ''], ['Zhu', 'Yalun', ''], ['Ju', 'Qiang', ''], ['Li', 'Shizheng', ''], ['Xie', 'Jian', '']]","extracted_entities":"[{'text': 'prompt engineering', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt engineering","similarity_score":0.5494377613}
{"id":2501.13667,"submitter":"Fu Rong","authors":"Fu Rong, Meng Lan, Qian Zhang, Lefei Zhang","title":"MPG-SAM 2: Adapting SAM 2 with Mask Priors and Global Context for\n  Referring Video Object Segmentation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Referring video object segmentation (RVOS) aims to segment objects in a video\naccording to textual descriptions, which requires the integration of multimodal\ninformation and temporal dynamics perception. The Segment Anything Model 2 (SAM\n2) has shown great effectiveness across various video segmentation tasks.\nHowever, its application to offline RVOS is challenged by the translation of\nthe text into effective prompts and a lack of global context awareness. In this\npaper, we propose a novel RVOS framework, termed MPG-SAM 2, to address these\nchallenges. Specifically, MPG-SAM 2 employs a unified multimodal encoder to\njointly encode video and textual features, generating semantically aligned\nvideo and text embeddings, along with multimodal class tokens. A mask prior\ngenerator utilizes the video embeddings and class tokens to create pseudo masks\nof target objects and global context. These masks are fed into the prompt\nencoder as dense prompts along with multimodal class tokens as sparse prompts\nto generate accurate prompts for SAM 2. To provide the online SAM 2 with a\nglobal view, we introduce a hierarchical global-historical aggregator, which\nallows SAM 2 to aggregate global and historical information of target objects\nat both pixel and object levels, enhancing the target representation and\ntemporal consistency. Extensive experiments on several RVOS benchmarks\ndemonstrate the superiority of MPG-SAM 2 and the effectiveness of our proposed\nmodules.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 Jan 2025 13:53:33 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 11:56:33 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Rong', 'Fu', ''], ['Lan', 'Meng', ''], ['Zhang', 'Qian', ''], ['Zhang', 'Lefei', '']]","extracted_entities":"[{'text': 'prompts', 'label': 'Prompting'}, {'text': 'video and text embeddings', 'label': 'Embedding'}, {'text': 'multimodal class tokens', 'label': 'Embedding'}, {'text': 'video embeddings', 'label': 'Embedding'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompts","similarity_score":0.7638334036}
{"id":2501.18328,"submitter":"Yicheng Wu","authors":"Yicheng Wu, Tao Song, Zhonghua Wu, Jin Ye, Zongyuan Ge, Zhaolin Chen,\n  Jianfei Cai","title":"CodeBrain: Imputing Any Brain MRI via Modality- and Instance-Specific\n  Codes","comments":"CodeBrain v2","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Unified MRI imputation, which can adapt to diverse imputation scenarios, is\nhighly desirable as it reduces scanning costs and provides comprehensive MRI\ninformation for improved clinical diagnosis. Existing unified MRI imputation\nmethods either rely on specific prompts to guide their transformation network\nor require multiple modality-specific modules. However, these approaches\nstruggle to capture large modality and instance variations or become too\ncomplex to generalize effectively. To address these limitations, we propose\nCodeBrain, a fundamentally different pipeline for unified brain MRI imputation.\nOur key idea is to reframe various inter-modality transformations as a\nfull-modality code prediction task via a two-stage framework. In the first\nstage, CodeBrain reconstructs a target modality from any other modalities by\nlearning a compact scalar-quantized code for each instance and modality. Any\ntarget modality can then be reconstructed with high fidelity by combining the\ncorresponding code with shared features extracted from any available modality.\nIn the second stage, a projection encoder is trained to predict full-modality\ncompact codes from any incomplete MRI samples, effectively simulating various\nimputation scenarios. We evaluate our CodeBrain on two public brain MRI\ndatasets (i.e., IXI and BraTS 2023). Extensive experiments demonstrate that\nCodeBrain outperforms state-of-the-art methods, setting a new benchmark for\nunified brain MRI imputation. Our code will be released.\n","versions":"[{'version': 'v1', 'created': 'Thu, 30 Jan 2025 13:14:40 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 02:55:58 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Wu', 'Yicheng', ''], ['Song', 'Tao', ''], ['Wu', 'Zhonghua', ''], ['Ye', 'Jin', ''], ['Ge', 'Zongyuan', ''], ['Chen', 'Zhaolin', ''], ['Cai', 'Jianfei', '']]","extracted_entities":"[{'text': 'specific prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"specific prompts","similarity_score":0.6510914564}
{"id":2501.18883,"submitter":"Jae Yong Lee","authors":"Jae Yong Lee, Sungmin Kang, Shin Yoo","title":"Predictive Prompt Analysis","comments":"Accepted by FSE 2025, 5 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) are machine learning models that have seen\nwidespread adoption due to their capability of handling previously difficult\ntasks. LLMs, due to their training, are sensitive to how exactly a question is\npresented, also known as prompting. However, prompting well is challenging, as\nit has been difficult to uncover principles behind prompting -- generally,\ntrial-and-error is the most common way of improving prompts, despite its\nsignificant computational cost. In this context, we argue it would be useful to\nperform `predictive prompt analysis', in which an automated technique would\nperform a quick analysis of a prompt and predict how the LLM would react to it,\nrelative to a goal provided by the user. As a demonstration of the concept, we\npresent Syntactic Prevalence Analyzer (SPA), a predictive prompt analysis\napproach based on sparse autoencoders (SAEs). SPA accurately predicted how\noften an LLM would generate target syntactic structures during code synthesis,\nwith up to 0.994 Pearson correlation between the predicted and actual\nprevalence of the target structure. At the same time, SPA requires only 0.4\\%\nof the time it takes to run the LLM on a benchmark. As LLMs are increasingly\nused during and integrated into modern software development, our proposed\npredictive prompt analysis concept has the potential to significantly ease the\nuse of LLMs for both practitioners and researchers.\n","versions":"[{'version': 'v1', 'created': 'Fri, 31 Jan 2025 04:34:43 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 07:23:59 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Lee', 'Jae Yong', ''], ['Kang', 'Sungmin', ''], ['Yoo', 'Shin', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2502.06432,"submitter":"Huaqiu Li","authors":"Huaqiu Li, Wang Zhang, Xiaowan Hu, Tao Jiang, Zikang Chen, Haoqian\n  Wang","title":"Prompt-SID: Learning Structural Representation Prompt via Latent\n  Diffusion for Single-Image Denoising","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Many studies have concentrated on constructing supervised models utilizing\npaired datasets for image denoising, which proves to be expensive and\ntime-consuming. Current self-supervised and unsupervised approaches typically\nrely on blind-spot networks or sub-image pairs sampling, resulting in pixel\ninformation loss and destruction of detailed structural information, thereby\nsignificantly constraining the efficacy of such methods. In this paper, we\nintroduce Prompt-SID, a prompt-learning-based single image denoising framework\nthat emphasizes preserving of structural details. This approach is trained in a\nself-supervised manner using downsampled image pairs. It captures\noriginal-scale image information through structural encoding and integrates\nthis prompt into the denoiser. To achieve this, we propose a structural\nrepresentation generation model based on the latent diffusion process and\ndesign a structural attention module within the transformer-based denoiser\narchitecture to decode the prompt. Additionally, we introduce a scale replay\ntraining mechanism, which effectively mitigates the scale gap from images of\ndifferent resolutions. We conduct comprehensive experiments on synthetic,\nreal-world, and fluorescence imaging datasets, showcasing the remarkable\neffectiveness of Prompt-SID. Our code will be released at\nhttps:\/\/github.com\/huaqlili\/Prompt-SID.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Feb 2025 13:09:47 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 12:49:20 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Huaqiu', ''], ['Zhang', 'Wang', ''], ['Hu', 'Xiaowan', ''], ['Jiang', 'Tao', ''], ['Chen', 'Zikang', ''], ['Wang', 'Haoqian', '']]","extracted_entities":"[{'text': 'prompt', 'label': 'Prompting'}, {'text': 'structural attention module', 'label': 'Attention mechanism'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'scale replay\\ntraining mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Prompting","matched_keyword":"prompt","similarity_score":0.7767513394}
{"id":2502.12691,"submitter":"Stanislav Frolov","authors":"Timon Winter, Stanislav Frolov, Brian Bernhard Moser, Andreas Dengel","title":"Spherical Dense Text-to-Image Synthesis","comments":"Link to project page https:\/\/sdt2i.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent advancements in text-to-image (T2I) have improved synthesis results,\nbut challenges remain in layout control and generating omnidirectional\npanoramic images. Dense T2I (DT2I) and spherical T2I (ST2I) models address\nthese issues, but so far no unified approach exists. Trivial approaches, like\nprompting a DT2I model to generate panoramas can not generate proper spherical\ndistortions and seamless transitions at the borders. Our work shows that\nspherical dense text-to-image (SDT2I) can be achieved by integrating\ntraining-free DT2I approaches into finetuned panorama models. Specifically, we\npropose MultiStitchDiffusion (MSTD) and MultiPanFusion (MPF) by integrating\nMultiDiffusion into StitchDiffusion and PanFusion, respectively. Since no\nbenchmark for SDT2I exists, we further construct Dense-Synthetic-View\n(DSynView), a new synthetic dataset containing spherical layouts to evaluate\nour models. Our results show that MSTD outperforms MPF across image quality as\nwell as prompt- and layout adherence. MultiPanFusion generates more diverse\nimages but struggles to synthesize flawless foreground objects. We propose\nbootstrap-coupling and turning off equirectangular perspective-projection\nattention in the foreground as an improvement of MPF. Link to code\nhttps:\/\/github.com\/sdt2i\/spherical-dense-text-to-image\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Feb 2025 09:51:11 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Feb 2025 13:00:18 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 18:50:41 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Winter', 'Timon', ''], ['Frolov', 'Stanislav', ''], ['Moser', 'Brian Bernhard', ''], ['Dengel', 'Andreas', '']]","extracted_entities":"[{'text': 'prompting', 'label': 'Prompting'}, {'text': 'MultiStitchDiffusion', 'label': 'Embedding'}, {'text': 'MultiPanFusion', 'label': 'Embedding'}, {'text': 'MultiDiffusion', 'label': 'Embedding'}, {'text': 'StitchDiffusion', 'label': 'Embedding'}, {'text': 'PanFusion', 'label': 'Embedding'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'MultiPanFusion', 'label': 'Embedding'}, {'text': 'equirectangular perspective-projection\\nattention', 'label': 'Attention mechanism'}, {'text': 'MPF', 'label': 'Embedding'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2502.18798,"submitter":"Gyeongje Cho","authors":"Gyeongje Cho, Yeonkyoung So and Jaejin Lee","title":"ANPMI: Assessing the True Comprehension Capabilities of LLMs for\n  Multiple Choice Questions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Multiple-choice benchmarks, consisting of various prompts and choices, are\namong the most widely used methods to assess a language model's natural\nlanguage understanding capability. Given a specific prompt, we typically\ncompute $P(Choice|Prompt)$ to evaluate how likely a language model is to\ngenerate the correct choice compared to incorrect ones. However, we observe\nthat performance measured using this approach reflects not only the model's\ncomprehension of the prompt but also its inherent biases for certain choices\nregardless of the prompt. This issue makes it challenging to accurately measure\na model's natural language understanding, as models may select the answer\nwithout fully understanding the prompt. To address this limitation, we propose\na novel metric called ANPMI, which normalizes Pointwise Mutual Information\n(PMI) by $-\\log P(Choice)$. ANPMI provides a more accurate assessment of the\nmodel's natural language understanding by ensuring that it is challenging to\nanswer a question without properly understanding the prompt.\n","versions":"[{'version': 'v1', 'created': 'Wed, 26 Feb 2025 04:10:18 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 08:11:40 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 16:27:59 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Cho', 'Gyeongje', ''], ['So', 'Yeonkyoung', ''], ['Lee', 'Jaejin', '']]","extracted_entities":"[{'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt","similarity_score":0.7767513394}
{"id":2502.19363,"submitter":"Ru Peng","authors":"Ru Peng, Kexin Yang, Yawen Zeng, Junyang Lin, Dayiheng Liu, Junbo Zhao","title":"DataMan: Data Manager for Pre-training Large Language Models","comments":"ICLR2025 paper","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The performance emergence of large language models (LLMs) driven by data\nscaling laws makes the selection of pre-training data increasingly important.\nHowever, existing methods rely on limited heuristics and human intuition,\nlacking comprehensive and clear guidelines. To address this, we are inspired by\n``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit\nits performance. As its pre-training capabilities are related to perplexity\n(PPL), we derive 14 quality criteria from the causes of text perplexity\nanomalies and introduce 15 common application domains to support domain mixing.\nIn this paper, we train a Data Manager (DataMan) to learn quality ratings and\ndomain recognition from pointwise rating, and use it to annotate a 447B token\npre-training corpus with 14 quality ratings and domain type. Our experiments\nvalidate our approach, using DataMan to select 30B tokens to train a\n1.3B-parameter language model, demonstrating significant improvements in\nin-context learning (ICL), perplexity, and instruction-following ability over\nthe state-of-the-art baseline. The best-performing model, based on the Overall\nScore l=5 surpasses a model trained with 50% more data using uniform sampling.\nWe continue pre-training with high-rated, domain-specific data annotated by\nDataMan to enhance domain-specific ICL performance and thus verify DataMan's\ndomain mixing ability. Our findings emphasize the importance of quality\nranking, the complementary nature of quality criteria, and their low\ncorrelation with perplexity, analyzing misalignment between PPL and ICL\nperformance. We also thoroughly analyzed our pre-training dataset, examining\nits composition, the distribution of quality ratings, and the original document\nsources.\n","versions":"[{'version': 'v1', 'created': 'Wed, 26 Feb 2025 18:01:19 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 15:42:07 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Peng', 'Ru', ''], ['Yang', 'Kexin', ''], ['Zeng', 'Yawen', ''], ['Lin', 'Junyang', ''], ['Liu', 'Dayiheng', ''], ['Zhao', 'Junbo', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'data\\nscaling laws', 'label': 'Scaling law'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2503.00897,"submitter":"Shashank Gupta","authors":"Shashank Gupta, Chaitanya Ahuja, Tsung-Yu Lin, Sreya Dutta Roy, Harrie\n  Oosterhuis, Maarten de Rijke, Satya Narayan Shukla","title":"A Simple and Effective Reinforcement Learning Method for Text-to-Image\n  Diffusion Fine-tuning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Reinforcement learning (RL)-based fine-tuning has emerged as a powerful\napproach for aligning diffusion models with black-box objectives. Proximal\npolicy optimization (PPO) is the most popular choice of method for policy\noptimization. While effective in terms of performance, PPO is highly sensitive\nto hyper-parameters and involves substantial computational overhead. REINFORCE,\non the other hand, mitigates some computational complexities such as high\nmemory overhead and sensitive hyper-parameter tuning, but has suboptimal\nperformance due to high-variance and sample inefficiency. While the variance of\nthe REINFORCE can be reduced by sampling multiple actions per input prompt and\nusing a baseline correction term, it still suffers from sample inefficiency. To\naddress these challenges, we systematically analyze the\nefficiency-effectiveness trade-off between REINFORCE and PPO, and propose\nleave-one-out PPO (LOOP), a novel RL for diffusion fine-tuning method. LOOP\ncombines variance reduction techniques from REINFORCE, such as sampling\nmultiple actions per input prompt and a baseline correction term, with the\nrobustness and sample efficiency of PPO via clipping and importance sampling.\nOur results demonstrate that LOOP effectively improves diffusion models on\nvarious black-box objectives, and achieves a better balance between\ncomputational efficiency and performance.\n","versions":"[{'version': 'v1', 'created': 'Sun, 2 Mar 2025 13:43:53 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Mar 2025 08:46:27 GMT'}, {'version': 'v3', 'created': 'Thu, 6 Mar 2025 17:19:22 GMT'}, {'version': 'v4', 'created': 'Wed, 12 Mar 2025 12:43:07 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Gupta', 'Shashank', ''], ['Ahuja', 'Chaitanya', ''], ['Lin', 'Tsung-Yu', ''], ['Roy', 'Sreya Dutta', ''], ['Oosterhuis', 'Harrie', ''], ['de Rijke', 'Maarten', ''], ['Shukla', 'Satya Narayan', '']]","extracted_entities":"[{'text': 'REINFORCE', 'label': 'Fine-tuning'}, {'text': 'input prompt', 'label': 'Prompting'}, {'text': 'REINFORCE', 'label': 'Fine-tuning'}, {'text': 'PPO', 'label': 'Few-shot Learning'}, {'text': 'input prompt', 'label': 'Prompting'}, {'text': 'LOOP', 'label': 'LLM'}]","assigned_concept":"Prompting","matched_keyword":"input prompt","similarity_score":0.6253764629}
{"id":2503.02078,"submitter":"Gal Niv","authors":"Jonathan Jacobi and Gal Niv","title":"Superscopes: Amplifying Internal Feature Representations for Language\n  Model Interpretation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Understanding and interpreting the internal representations of large language\nmodels (LLMs) remains an open challenge. Patchscopes introduced a method for\nprobing internal activations by patching them into new prompts, prompting\nmodels to self-explain their hidden representations. We introduce Superscopes,\na technique that systematically amplifies superposed features in MLP outputs\n(multilayer perceptron) and hidden states before patching them into new\ncontexts. Inspired by the \"features as directions\" perspective and the\nClassifier-Free Guidance (CFG) approach from diffusion models, Superscopes\namplifies weak but meaningful features, enabling the interpretation of internal\nrepresentations that previous methods failed to explain-all without requiring\nadditional training. This approach provides new insights into how LLMs build\ncontext and represent complex concepts, further advancing mechanistic\ninterpretability.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 21:58:12 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 10:27:43 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Jacobi', 'Jonathan', ''], ['Niv', 'Gal', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'new prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Prompting","matched_keyword":"new prompts","similarity_score":0.6180645823}
{"id":2503.03594,"submitter":"Haoran Fan","authors":"Haoran Fan, Bin Li, Yixuan Weng and Shoujun Zhou","title":"Small but Mighty: Enhancing Time Series Forecasting with Lightweight\n  LLMs","comments":"20 pages, 10 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  While LLMs have demonstrated remarkable potential in time series forecasting,\ntheir practical deployment remains constrained by excessive computational\ndemands and memory footprints. Existing LLM-based approaches typically suffer\nfrom three critical limitations: Inefficient parameter utilization in handling\nnumerical time series patterns; Modality misalignment between continuous\ntemporal signals and discrete text embeddings; and Inflexibility for real-time\nexpert knowledge integration. We present SMETimes, the first systematic\ninvestigation of sub-3B parameter SLMs for efficient and accurate time series\nforecasting. Our approach centers on three key innovations: A\nstatistically-enhanced prompting mechanism that bridges numerical time series\nwith textual semantics through descriptive statistical features; A adaptive\nfusion embedding architecture that aligns temporal patterns with language model\ntoken spaces through learnable parameters; And a dynamic mixture-of-experts\nframework enabled by SLMs' computational efficiency, adaptively combining base\npredictions with domain-specific models. Extensive evaluations across seven\nbenchmark datasets demonstrate that our 3B-parameter SLM achieves\nstate-of-the-art performance on five primary datasets while maintaining 3.8x\nfaster training and 5.2x lower memory consumption compared to 7B-parameter LLM\nbaselines. Notably, the proposed model exhibits better learning capabilities,\nachieving 12.3% lower MSE than conventional LLM. Ablation studies validate that\nour statistical prompting and cross-modal fusion modules respectively\ncontribute 15.7% and 18.2% error reduction in long-horizon forecasting tasks.\nBy redefining the efficiency-accuracy trade-off landscape, this work\nestablishes SLMs as viable alternatives to resource-intensive LLMs for\npractical time series forecasting. Code and models are available at\nhttps:\/\/github.com\/xiyan1234567\/SMETimes.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 15:27:36 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 10:56:53 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Fan', 'Haoran', ''], ['Li', 'Bin', ''], ['Weng', 'Yixuan', ''], ['Zhou', 'Shoujun', '']]","extracted_entities":"[{'text': 'discrete text embeddings', 'label': 'Embedding'}, {'text': 'statistically-enhanced prompting mechanism', 'label': 'Prompting'}, {'text': 'adaptive\\nfusion embedding architecture', 'label': 'Embedding'}, {'text': 'SLMs', 'label': 'LLMs'}, {'text': 'statistical prompting', 'label': 'Prompting'}, {'text': 'SLMs', 'label': 'LLM'}]","assigned_concept":"Prompting","matched_keyword":"statistically-enhanced prompting mechanism","similarity_score":0.7294894457}
{"id":2503.04479,"submitter":"Ivan Milev","authors":"Ivan Milev, Mislav Balunovi\\'c, Maximilian Baader and Martin Vechev","title":"ToolFuzz -- Automated Agent Tool Testing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Large Language Model (LLM) Agents leverage the advanced reasoning\ncapabilities of LLMs in real-world applications. To interface with an\nenvironment, these agents often rely on tools, such as web search or database\nAPIs. As the agent provides the LLM with tool documentation along the user\nquery, the completeness and correctness of this documentation is critical.\nHowever, tool documentation is often over-, under-, or ill-specified, impeding\nthe agent's accuracy. Standard software testing approaches struggle to identify\nthese errors as they are expressed in natural language. Thus, despite its\nimportance, there currently exists no automated method to test the tool\ndocumentation for agents. To address this issue, we present ToolFuzz, the first\nmethod for automated testing of tool documentations. ToolFuzz is designed to\ndiscover two types of errors: (1) user queries leading to tool runtime errors\nand (2) user queries that lead to incorrect agent responses. ToolFuzz can\ngenerate a large and diverse set of natural inputs, effectively finding tool\ndescription errors at a low false positive rate. Further, we present two\nstraightforward prompt-engineering approaches. We evaluate all three tool\ntesting approaches on 32 common LangChain tools and 35 newly created custom\ntools and 2 novel benchmarks to further strengthen the assessment. We find that\nmany publicly available tools suffer from underspecification. Specifically, we\nshow that ToolFuzz identifies 20x more erroneous inputs compared to the\nprompt-engineering approaches, making it a key component for building reliable\nAI agents.\n","versions":"[{'version': 'v1', 'created': 'Thu, 6 Mar 2025 14:29:52 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 13:01:58 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 14:28:13 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Milev', 'Ivan', ''], ['Balunovi\u0107', 'Mislav', ''], ['Baader', 'Maximilian', ''], ['Vechev', 'Martin', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'prompt-engineering approaches', 'label': 'Prompting'}, {'text': 'publicly available tools', 'label': 'Open-source LLMs'}]","assigned_concept":"Prompting","matched_keyword":"prompt-engineering approaches","similarity_score":0.5158341527}
{"id":2503.0487,"submitter":"Devi Dutta Biswajeet","authors":"Devi Dutta Biswajeet and Sara Kadkhodaei","title":"Leveraging Large Language Models to Address Data Scarcity in Machine\n  Learning: Applications in Graphene Synthesis","comments":"20 pages, 10 figures, 4 tables; Supplementary Material with 13\n  figures and 4 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.comp-ph cond-mat.mtrl-sci cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Machine learning in materials science faces challenges due to limited\nexperimental data, as generating synthesis data is costly and time-consuming,\nespecially with in-house experiments. Mining data from existing literature\nintroduces issues like mixed data quality, inconsistent formats, and variations\nin reporting experimental parameters, complicating the creation of consistent\nfeatures for the learning algorithm. Additionally, combining continuous and\ndiscrete features can hinder the learning process with limited data. Here, we\npropose strategies that utilize large language models (LLMs) to enhance machine\nlearning performance on a limited, heterogeneous dataset of graphene chemical\nvapor deposition synthesis compiled from existing literature. These strategies\ninclude prompting modalities for imputing missing data points and leveraging\nlarge language model embeddings to encode the complex nomenclature of\nsubstrates reported in chemical vapor deposition experiments. The proposed\nstrategies enhance graphene layer classification using a support vector machine\n(SVM) model, increasing binary classification accuracy from 39% to 65% and\nternary accuracy from 52% to 72%. We compare the performance of the SVM and a\nGPT-4 model, both trained and fine-tuned on the same data. Our results\ndemonstrate that the numerical classifier, when combined with LLM-driven data\nenhancements, outperforms the standalone LLM predictor, highlighting that in\ndata-scarce scenarios, improving predictive learning with LLM strategies\nrequires more than simple fine-tuning on datasets. Instead, it necessitates\nsophisticated approaches for data imputation and feature space homogenization\nto achieve optimal performance. The proposed strategies emphasize data\nenhancement techniques, offering a broadly applicable framework for improving\nmachine learning performance on scarce, inhomogeneous datasets.\n","versions":"[{'version': 'v1', 'created': 'Thu, 6 Mar 2025 16:04:01 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 14:04:38 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Biswajeet', 'Devi Dutta', ''], ['Kadkhodaei', 'Sara', '']]","extracted_entities":"[{'text': 'prompting modalities', 'label': 'Prompting'}, {'text': 'large language model embeddings', 'label': 'Embedding'}]","assigned_concept":"Prompting","matched_keyword":"prompting modalities","similarity_score":0.707855165}
{"id":2503.06259,"submitter":"PanPan Shi","authors":"Xiao-Yu Zhang, Pan-Pan Shi, Feng-Kun Guo","title":"Production of $1^{-+}$ exotic charmonium-like states in\n  electron-positron collisions","comments":"18 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph hep-ex","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The absence of observed charmonium-like states with the exotic quantum\nnumbers $J^{PC}=1^{-+}$ has prompted us to investigate the production rates of\nthe $1^{-+}$ $D\\bar D_1(2420)$ and $D^*\\bar D_1(2420)$ hadronic molecules,\nwhich we refer to as $\\eta_{c1}$ and $\\eta_{c1}^{\\prime}$, respectively, in\nelectron-positron collisions. Assuming a hadronic molecular nature for the\nvector charmonium-like states $\\psi(4360)$ and $\\psi(4415)$, we evaluate the\nradiative decay widths of $\\psi(4360)\\to\\gamma\\eta_{c1}$ and\n$\\psi(4415)\\to\\gamma\\eta_{c1}^{\\prime}$. Using these decay widths, we estimate\nthe cross sections for producing $\\eta_{c1}$ and $\\eta_{c1}^{\\prime}$ in\nelectron-positron annihilations, as well as the event numbers at the planned\nSuper $\\tau$-Charm Facility. Our results suggest that the ideal energy region\nfor observing these states is around $4.44$ and $4.50$ GeV, just above the $D^*\n\\bar D_1(2420)$ and $D^*\\bar D_2^*(2460)$ thresholds, respectively.\n","versions":"[{'version': 'v1', 'created': 'Sat, 8 Mar 2025 16:11:56 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 10:12:15 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Zhang', 'Xiao-Yu', ''], ['Shi', 'Pan-Pan', ''], ['Guo', 'Feng-Kun', '']]","extracted_entities":"[{'text': 'prompted', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompted","similarity_score":0.7553437948}
{"id":2503.06453,"submitter":"Shengfang Zhai","authors":"Shengfang Zhai, Jiajun Li, Yue Liu, Huanran Chen, Zhihua Tian, Wenjie\n  Qu, Qingni Shen, Ruoxi Jia, Yinpeng Dong, Jiaheng Zhang","title":"NaviDet: Efficient Input-level Backdoor Detection on Text-to-Image\n  Synthesis via Neuron Activation Variation","comments":"18 pages. The tiny version is accepted by ICLR 2025 Workshop FM-Wild","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  In recent years, text-to-image (T2I) diffusion models have garnered\nsignificant attention for their ability to generate high-quality images\nreflecting text prompts. However, their growing popularity has also led to the\nemergence of backdoor threats, posing substantial risks. Currently, effective\ndefense strategies against such threats are lacking due to the diversity of\nbackdoor targets in T2I synthesis. In this paper, we propose NaviDet, the first\ngeneral input-level backdoor detection framework for identifying backdoor\ninputs across various backdoor targets. Our approach is based on the new\nobservation that trigger tokens tend to induce significant neuron activation\nvariation in the early stage of the diffusion generation process, a phenomenon\nwe term Early-step Activation Variation. Leveraging this insight, NaviDet\ndetects malicious samples by analyzing neuron activation variations caused by\ninput tokens. Through extensive experiments, we demonstrate the effectiveness\nand efficiency of our method against various T2I backdoor attacks, surpassing\nexisting baselines with significantly lower computational overhead.\nFurthermore, we rigorously demonstrate that our method remains effective\nagainst potential adaptive attacks.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 05:27:44 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhai', 'Shengfang', ''], ['Li', 'Jiajun', ''], ['Liu', 'Yue', ''], ['Chen', 'Huanran', ''], ['Tian', 'Zhihua', ''], ['Qu', 'Wenjie', ''], ['Shen', 'Qingni', ''], ['Jia', 'Ruoxi', ''], ['Dong', 'Yinpeng', ''], ['Zhang', 'Jiaheng', '']]","extracted_entities":"[{'text': 'text prompts', 'label': 'Prompting'}, {'text': 'trigger tokens', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"text prompts","similarity_score":0.6106933355}
{"id":2503.06506,"submitter":"Mahdieh Soleymani Baghshah","authors":"Amir Mohammad Izadi, Seyed Mohammad Hadi Hosseini, Soroush Vafaie\n  Tabar, Ali Abdollahi, Armin Saghafian, and Mahdieh Soleymani Baghshah","title":"Fine-Grained Alignment and Noise Refinement for Compositional\n  Text-to-Image Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Text-to-image generative models have made significant advancements in recent\nyears; however, accurately capturing intricate details in textual prompts, such\nas entity missing, attribute binding errors, and incorrect relationships\nremains a formidable challenge. In response, we present an innovative,\ntraining-free method that directly addresses these challenges by incorporating\ntailored objectives to account for textual constraints. Unlike layout-based\napproaches that enforce rigid structures and limit diversity, our proposed\napproach offers a more flexible arrangement of the scene by imposing just the\nextracted constraints from the text, without any unnecessary additions. These\nconstraints are formulated as losses-entity missing, entity mixing, attribute\nbinding, and spatial relationships, integrated into a unified loss that is\napplied in the first generation stage. Furthermore, we introduce a\nfeedback-driven system for fine-grained initial noise refinement. This system\nintegrates a verifier that evaluates the generated image, identifies\ninconsistencies, and provides corrective feedback. Leveraging this feedback,\nour refinement method first targets the unmet constraints by refining the\nfaulty attention maps caused by initial noise, through the optimization of\nselective losses associated with these constraints. Subsequently, our unified\nloss function is reapplied to proceed the second generation phase. Experimental\nresults demonstrate that our method, relying solely on our proposed objective\nfunctions, significantly enhances compositionality, achieving a 24% improvement\nin human evaluation and a 25% gain in spatial relationships. Furthermore, our\nfine-grained noise refinement proves effective, boosting performance by up to\n5%. Code is available at https:\/\/github.com\/hadi-hosseini\/noise-refinement.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 08:18:43 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Izadi', 'Amir Mohammad', ''], ['Hosseini', 'Seyed Mohammad Hadi', ''], ['Tabar', 'Soroush Vafaie', ''], ['Abdollahi', 'Ali', ''], ['Saghafian', 'Armin', ''], ['Baghshah', 'Mahdieh Soleymani', '']]","extracted_entities":"[{'text': 'textual prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"textual prompts","similarity_score":0.6302489042}
{"id":2503.06515,"submitter":"Jing Zhang","authors":"Jing Zhang, Zhikai Li, Qingyi Gu","title":"SAQ-SAM: Semantically-Aligned Quantization for Segment Anything Model","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Segment Anything Model (SAM) exhibits remarkable zero-shot segmentation\ncapability; however, its prohibitive computational costs make edge deployment\nchallenging. Although post-training quantization (PTQ) offers a promising\ncompression solution, existing methods yield unsatisfactory results when\napplied to SAM, owing to its specialized model components and promptable\nworkflow: (i) The mask decoder's attention exhibits extreme outliers, and we\nfind that aggressive clipping (ranging down to even 100$\\times$), instead of\nsmoothing or isolation, is effective in suppressing outliers while maintaining\nsemantic capabilities. Unfortunately, traditional metrics (e.g., MSE) fail to\nprovide such large-scale clipping. (ii) Existing reconstruction methods\npotentially neglect prompts' intention, resulting in distorted visual encodings\nduring prompt interactions. To address the above issues, we propose SAQ-SAM in\nthis paper, which boosts PTQ of SAM with semantic alignment. Specifically, we\npropose Perceptual-Consistency Clipping, which exploits attention focus overlap\nas clipping metric, to significantly suppress outliers. Furthermore, we propose\nPrompt-Aware Reconstruction, which incorporates visual-prompt interactions by\nleveraging cross-attention responses in mask decoder, thus facilitating\nalignment in both distribution and semantics. To ensure the interaction\nefficiency, we also introduce a layer-skipping strategy for visual tokens.\nExtensive experiments are conducted on different segmentation tasks and SAMs of\nvarious sizes, and the results show that the proposed SAQ-SAM consistently\noutperforms baselines. For example, when quantizing SAM-B to 4-bit, our method\nachieves 11.7% higher mAP than the baseline in instance segmentation task.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 08:38:32 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhang', 'Jing', ''], ['Li', 'Zhikai', ''], ['Gu', 'Qingyi', '']]","extracted_entities":"[{'text': 'post-training quantization', 'label': 'quantisation'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompt interactions', 'label': 'Prompting'}, {'text': 'attention', 'label': 'Attention mechanism'}]","assigned_concept":"Prompting","matched_keyword":"prompts","similarity_score":0.7638334036}
{"id":2503.06552,"submitter":"Md. Tanzib Hosain","authors":"Rajan Das Gupta, Md. Tanzib Hosain, M. F. Mridha and Salah Uddin Ahmed","title":"Multimodal Programming in Computer Science with Interactive Assistance\n  Powered by Large Language Model","comments":"Accepted in Proceedings of the 27th International Conference on.\n  Human-Computer Interaction, 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  LLM chatbot interfaces allow students to get instant, interactive assistance\nwith homework, but doing so carelessly may not advance educational objectives.\nIn this study, an interactive homework help system based on DeepSeek R1 is\ndeveloped and first implemented for students enrolled in a large computer\nscience beginning programming course. In addition to an assist button in a\nwell-known code editor, our assistant also has a feedback option in our\ncommand-line automatic evaluator. It wraps student work in a personalized\nprompt that advances our educational objectives without offering answers\nstraight away. We have discovered that our assistant can recognize students'\nconceptual difficulties and provide ideas, plans, and template code in\npedagogically appropriate ways. However, among other mistakes, it occasionally\nincorrectly labels the correct student code as incorrect or encourages students\nto use correct-but-lesson-inappropriate approaches, which can lead to long and\nfrustrating journeys for the students. After discussing many development and\ndeployment issues, we provide our conclusions and future actions.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 10:48:47 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 13:42:46 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Gupta', 'Rajan Das', ''], ['Hosain', 'Md. Tanzib', ''], ['Mridha', 'M. F.', ''], ['Ahmed', 'Salah Uddin', '']]","extracted_entities":"[{'text': 'chatbot', 'label': 'ChatGPT'}, {'text': 'personalized\\nprompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"personalized\nprompt","similarity_score":0.6369344592}
{"id":2503.06553,"submitter":"Jiaxin Ai","authors":"Jiaxin Ai, Pengfei Zhou, Zhaopan Xu, Ming Li, Fanrui Zhang, Zizhen Li,\n  Jianwen Sun, Yukang Feng, Baojin Huang, Zhongyuan Wang, Kaipeng Zhang","title":"ProJudge: A Multi-Modal Multi-Discipline Benchmark and\n  Instruction-Tuning Dataset for MLLM-based Process Judges","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CV cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  As multi-modal large language models (MLLMs) frequently exhibit errors when\nsolving scientific problems, evaluating the validity of their reasoning\nprocesses is critical for ensuring reliability and uncovering fine-grained\nmodel weaknesses. Since human evaluation is laborious and costly, prompting\nMLLMs as automated process judges has become a common practice. However, the\nreliability of these model-based judges remains uncertain. To address this, we\nintroduce ProJudgeBench, the first comprehensive benchmark specifically\ndesigned for evaluating abilities of MLLM-based process judges. ProJudgeBench\ncomprises 2,400 test cases and 50,118 step-level labels, spanning four\nscientific disciplines with diverse difficulty levels and multi-modal content.\nIn ProJudgeBench, each step is meticulously annotated by human experts for\ncorrectness, error type, and explanation, enabling a systematic evaluation of\njudges' capabilities to detect, classify and diagnose errors. Evaluation on\nProJudgeBench reveals a significant performance gap between open-source and\nproprietary models. To bridge this gap, we further propose ProJudge-173k, a\nlarge-scale instruction-tuning dataset, and a Dynamic Dual-Phase fine-tuning\nstrategy that encourages models to explicitly reason through problem-solving\nbefore assessing solutions. Both contributions significantly enhance the\nprocess evaluation capabilities of open-source models. All the resources will\nbe released to foster future research of reliable multi-modal process\nevaluation.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 10:55:51 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Ai', 'Jiaxin', ''], ['Zhou', 'Pengfei', ''], ['Xu', 'Zhaopan', ''], ['Li', 'Ming', ''], ['Zhang', 'Fanrui', ''], ['Li', 'Zizhen', ''], ['Sun', 'Jianwen', ''], ['Feng', 'Yukang', ''], ['Huang', 'Baojin', ''], ['Wang', 'Zhongyuan', ''], ['Zhang', 'Kaipeng', '']]","extracted_entities":"[{'text': 'multi-modal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'Dynamic Dual-Phase fine-tuning\\nstrategy', 'label': 'Fine-tuning'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2503.06573,"submitter":"Gili Lior","authors":"Gili Lior, Asaf Yehudai, Ariel Gera, Liat Ein-Dor","title":"WildIFEval: Instruction Following in the Wild","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Recent LLMs have shown remarkable success in following user instructions, yet\nhandling instructions with multiple constraints remains a significant\nchallenge. In this work, we introduce WildIFEval - a large-scale dataset of 12K\nreal user instructions with diverse, multi-constraint conditions. Unlike prior\ndatasets, our collection spans a broad lexical and topical spectrum of\nconstraints, in natural user prompts. We categorize these constraints into\neight high-level classes to capture their distribution and dynamics in\nreal-world scenarios. Leveraging WildIFEval, we conduct extensive experiments\nto benchmark the instruction-following capabilities of leading LLMs. Our\nfindings reveal that all evaluated models experience performance degradation\nwith an increasing number of constraints. Thus, we show that all models have a\nlarge room for improvement on such tasks. Moreover, we observe that the\nspecific type of constraint plays a critical role in model performance. We\nrelease our dataset to promote further research on instruction-following under\ncomplex, realistic conditions.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 12:06:29 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Lior', 'Gili', ''], ['Yehudai', 'Asaf', ''], ['Gera', 'Ariel', ''], ['Ein-Dor', 'Liat', '']]","extracted_entities":"[{'text': 'WildIFEval', 'label': 'Open-source LLMs'}, {'text': 'natural user prompts', 'label': 'Prompting'}, {'text': 'WildIFEval', 'label': 'Open-source LLMs'}]","assigned_concept":"Prompting","matched_keyword":"natural user prompts","similarity_score":0.6830462217}
{"id":2503.0658,"submitter":"Yuxiang Zhang","authors":"Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Xinyan Wen, Jitao Sang","title":"Agent models: Internalizing Chain-of-Action Generation into Reasoning\n  models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Traditional agentic workflows rely on external prompts to manage interactions\nwith tools and the environment, which limits the autonomy of reasoning models.\nWe position \\emph{Large Agent Models (LAMs)} that internalize the generation of\n\\emph{Chain-of-Action (CoA)}, enabling the model to autonomously decide when\nand how to use external tools. Our proposed AutoCoA framework combines\nsupervised fine-tuning (SFT) and reinforcement learning (RL), allowing the\nmodel to seamlessly switch between reasoning and action while efficiently\nmanaging environment interactions. Main components include step-level action\ntriggering, trajectory-level CoA optimization, and an internal world model to\nreduce real-environment interaction costs. Evaluations on open-domain QA tasks\ndemonstrate that AutoCoA-trained agent models significantly outperform\nReAct-based workflows in task completion, especially in tasks that require\nlong-term reasoning and multi-step actions. Code and dataset are available at\nhttps:\/\/github.com\/ADaM-BJTU\/AutoCoA\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 12:19:47 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhang', 'Yuxiang', ''], ['Yang', 'Yuqi', ''], ['Shu', 'Jiangming', ''], ['Wen', 'Xinyan', ''], ['Sang', 'Jitao', '']]","extracted_entities":"[{'text': 'external prompts', 'label': 'Prompting'}, {'text': 'Large Agent Models', 'label': 'Large Language Model'}, {'text': 'Chain-of-Action (CoA)', 'label': 'Chain of thought'}, {'text': 'step-level action\\ntriggering', 'label': 'Fine-tuning'}, {'text': 'trajectory-level CoA optimization', 'label': 'Fine-tuning'}]","assigned_concept":"Prompting","matched_keyword":"external prompts","similarity_score":0.5768461227}
{"id":2503.06632,"submitter":"Mingxiao Li","authors":"Mingxiao Li, Tingyu Qu, Tinne Tuytelaars, Marie-Francine Moens","title":"Towards More Accurate Personalized Image Generation: Addressing\n  Overfitting and Evaluation Bias","comments":"18","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Personalized image generation via text prompts has great potential to improve\ndaily life and professional work by facilitating the creation of customized\nvisual content. The aim of image personalization is to create images based on a\nuser-provided subject while maintaining both consistency of the subject and\nflexibility to accommodate various textual descriptions of that subject.\nHowever, current methods face challenges in ensuring fidelity to the text\nprompt while not overfitting to the training data. In this work, we introduce a\nnovel training pipeline that incorporates an attractor to filter out\ndistractions in training images, allowing the model to focus on learning an\neffective representation of the personalized subject. Moreover, current\nevaluation methods struggle due to the lack of a dedicated test set. The\nevaluation set-up typically relies on the training data of the personalization\ntask to compute text-image and image-image similarity scores, which, while\nuseful, tend to overestimate performance. Although human evaluations are\ncommonly used as an alternative, they often suffer from bias and inconsistency.\nTo address these issues, we curate a diverse and high-quality test set with\nwell-designed prompts. With this new benchmark, automatic evaluation metrics\ncan reliably assess model performance\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 14:14:02 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Li', 'Mingxiao', ''], ['Qu', 'Tingyu', ''], ['Tuytelaars', 'Tinne', ''], ['Moens', 'Marie-Francine', '']]","extracted_entities":"[{'text': 'text prompts', 'label': 'Prompting'}, {'text': 'well-designed prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"well-designed prompts","similarity_score":0.6277580261}
{"id":2405.04732,"submitter":"Vishnu Sashank Dorbala","authors":"Vishnu Sashank Dorbala, Prasoon Goyal, Robinson Piramuthu, Michael\n  Johnston, Reza Ghanadhan, Dinesh Manocha","title":"Is the House Ready For Sleeptime? Generating and Evaluating Situational\n  Queries for Embodied Question Answering","comments":"10 Pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We present and tackle the problem of Embodied Question Answering (EQA) with\nSituational Queries (S-EQA) in a household environment. Unlike prior EQA work\ntackling simple queries that directly reference target objects and properties\n(\"What is the color of the car?\"), situational queries (such as \"Is the house\nready for sleeptime?\") are challenging as they require the agent to correctly\nidentify multiple object-states (Doors: Closed, Lights: Off, etc.) and reach a\nconsensus on their states for an answer. Towards this objective, we first\nintroduce a novel Prompt-Generate-Evaluate (PGE) scheme that wraps around an\nLLM's output to generate unique situational queries and corresponding consensus\nobject information. PGE is used to generate 2K datapoints in the VirtualHome\nsimulator, which is then annotated for ground truth answers via a large scale\nuser-study conducted on M-Turk. With a high rate of answerability (97.26%) on\nthis study, we establish that LLMs are good at generating situational data.\nHowever, in evaluating the data using an LLM, we observe a low correlation of\n46.2% with the ground truth human annotations; indicating that while LLMs are\ngood at generating situational data, they struggle to answer them according to\nconsensus. When asked for reasoning, we observe the LLM often goes against\ncommonsense in justifying its answer. Finally, we utilize PGE to generate\nsituational data in a real-world environment, exposing LLM hallucination in\ngenerating reliable object-states when a structured scene graph is unavailable.\nTo the best of our knowledge, this is the first work to introduce EQA in the\ncontext of situational queries and also the first to present a generative\napproach for query creation. We aim to foster research on improving the\nreal-world usability of embodied agents through this work.\n","versions":"[{'version': 'v1', 'created': 'Wed, 8 May 2024 00:45:20 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Oct 2024 20:43:33 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 21:12:19 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Dorbala', 'Vishnu Sashank', ''], ['Goyal', 'Prasoon', ''], ['Piramuthu', 'Robinson', ''], ['Johnston', 'Michael', ''], ['Ghanadhan', 'Reza', ''], ['Manocha', 'Dinesh', '']]","extracted_entities":"[{'text': 'Prompt-Generate-Evaluate', 'label': 'Prompting'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2405.13518,"submitter":"Muhammad Ibraheem Siddiqui","authors":"Muhammad Ibraheem Siddiqui, Muhammad Umer Sheikh, Hassan Abid and\n  Muhammad Haris Khan","title":"PerSense: Personalized Instance Segmentation in Dense Images","comments":"Technical report of PerSense","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  The emergence of foundational models has significantly advanced segmentation\napproaches. However, existing models still face challenges in automatically\nsegmenting personalized instances in dense scenarios, where severe occlusions,\nscale variations, and background clutter hinder precise instance delineation.\nTo address this, we propose PerSense, an end-to-end, training-free, and\nmodel-agnostic one-shot framework for personalized instance segmentation in\ndense images. We start with developing a new baseline capable of automatically\ngenerating instance-level point prompts via proposing a novel Instance\nDetection Module (IDM) that leverages density maps, encapsulating spatial\ndistribution of objects in an image. To reduce false positives, we design the\nPoint Prompt Selection Module (PPSM), which refines the output of IDM based on\nan adaptive threshold. Both IDM and PPSM seamlessly integrate into our\nmodel-agnostic framework. Furthermore, we introduce a feedback mechanism which\nenables PerSense to improve the accuracy of density maps by automating the\nexemplar selection process for density map generation. Finally, to promote\nalgorithmic advances and effective tools for this relatively underexplored\ntask, we introduce PerSense-D, an evaluation benchmark exclusive to\npersonalized instance segmentation in dense images. Our extensive experiments\nestablish PerSense superiority in dense scenarios compared to SOTA approaches.\nAdditionally, our qualitative findings demonstrate the adaptability of our\nframework to images captured in-the-wild.\n","versions":"[{'version': 'v1', 'created': 'Wed, 22 May 2024 10:26:44 GMT'}, {'version': 'v2', 'created': 'Wed, 2 Oct 2024 11:45:38 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 08:25:54 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Siddiqui', 'Muhammad Ibraheem', ''], ['Sheikh', 'Muhammad Umer', ''], ['Abid', 'Hassan', ''], ['Khan', 'Muhammad Haris', '']]","extracted_entities":"[{'text': 'instance-level point prompts', 'label': 'Prompting'}, {'text': 'IDM', 'label': 'LLM'}, {'text': 'PPSM', 'label': 'LLM'}, {'text': 'IDM', 'label': 'LLM'}, {'text': 'IDM', 'label': 'LLM'}, {'text': 'PPSM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"IDM","similarity_score":0.5949400663}
{"id":2405.1645,"submitter":"Chan-Hung Yu","authors":"Max Liu, Chan-Hung Yu, Wei-Hsu Lee, Cheng-Wei Hung, Yen-Chun Chen,\n  Shao-Hua Sun","title":"Synthesizing Programmatic Reinforcement Learning Policies with Large\n  Language Model Guided Search","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.PL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Programmatic reinforcement learning (PRL) has been explored for representing\npolicies through programs as a means to achieve interpretability and\ngeneralization. Despite promising outcomes, current state-of-the-art PRL\nmethods are hindered by sample inefficiency, necessitating tens of millions of\nprogram-environment interactions. To tackle this challenge, we introduce a\nnovel LLM-guided search framework (LLM-GS). Our key insight is to leverage the\nprogramming expertise and common sense reasoning of LLMs to enhance the\nefficiency of assumption-free, random-guessing search methods. We address the\nchallenge of LLMs' inability to generate precise and grammatically correct\nprograms in domain-specific languages (DSLs) by proposing a Pythonic-DSL\nstrategy - an LLM is instructed to initially generate Python codes and then\nconvert them into DSL programs. To further optimize the LLM-generated programs,\nwe develop a search algorithm named Scheduled Hill Climbing, designed to\nefficiently explore the programmatic search space to improve the programs\nconsistently. Experimental results in the Karel domain demonstrate our LLM-GS\nframework's superior effectiveness and efficiency. Extensive ablation studies\nfurther verify the critical role of our Pythonic-DSL strategy and Scheduled\nHill Climbing algorithm. Moreover, we conduct experiments with two novel tasks,\nshowing that LLM-GS enables users without programming skills and knowledge of\nthe domain or DSL to describe the tasks in natural language to obtain\nperformant programs.\n","versions":"[{'version': 'v1', 'created': 'Sun, 26 May 2024 06:33:48 GMT'}, {'version': 'v2', 'created': 'Sun, 13 Oct 2024 16:12:02 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 12:52:28 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Liu', 'Max', ''], ['Yu', 'Chan-Hung', ''], ['Lee', 'Wei-Hsu', ''], ['Hung', 'Cheng-Wei', ''], ['Chen', 'Yen-Chun', ''], ['Sun', 'Shao-Hua', '']]","extracted_entities":"[{'text': 'Programmatic reinforcement learning', 'label': 'Few-shot Learning'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM-GS', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2406.0587,"submitter":"Avital Shafran","authors":"Avital Shafran, Roei Schuster, Vitaly Shmatikov","title":"Machine Against the RAG: Jamming Retrieval-Augmented Generation with\n  Blocker Documents","comments":"To appear in USENIX Security Symposium 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Retrieval-augmented generation (RAG) systems respond to queries by retrieving\nrelevant documents from a knowledge database and applying an LLM to the\nretrieved documents. We demonstrate that RAG systems that operate on databases\nwith untrusted content are vulnerable to denial-of-service attacks we call\njamming. An adversary can add a single ``blocker'' document to the database\nthat will be retrieved in response to a specific query and result in the RAG\nsystem not answering this query, ostensibly because it lacks relevant\ninformation or because the answer is unsafe.\n  We describe and measure the efficacy of several methods for generating\nblocker documents, including a new method based on black-box optimization. Our\nmethod (1) does not rely on instruction injection, (2) does not require the\nadversary to know the embedding or LLM used by the target RAG system, and (3)\ndoes not employ an auxiliary LLM.\n  We evaluate jamming attacks on several embeddings and LLMs and demonstrate\nthat the existing safety metrics for LLMs do not capture their vulnerability to\njamming. We then discuss defenses against blocker documents.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Jun 2024 17:55:55 GMT'}, {'version': 'v2', 'created': 'Mon, 16 Sep 2024 14:52:46 GMT'}, {'version': 'v3', 'created': 'Mon, 20 Jan 2025 18:01:06 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Mar 2025 12:56:54 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Shafran', 'Avital', ''], ['Schuster', 'Roei', ''], ['Shmatikov', 'Vitaly', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}, {'text': 'black-box optimization', 'label': 'Fine-tuning'}, {'text': 'embedding', 'label': 'Embedding'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2406.20076,"submitter":"Tianheng Cheng","authors":"Yuxuan Zhang, Tianheng Cheng, Lianghui Zhu, Rui Hu, Lei Liu, Heng Liu,\n  Longjin Ran, Xiaoxin Chen, Wenyu Liu, Xinggang Wang","title":"EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything\n  Model","comments":"Preprint. Update: (1) better performance and (2) versatile\n  segmentation. Code and models are available at:\n  https:\/\/github.com\/hustvl\/EVF-SAM","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Segment Anything Model (SAM) has attracted widespread attention for its\nsuperior interactive segmentation capabilities with visual prompts while\nlacking further exploration of text prompts. In this paper, we empirically\ninvestigate what text prompt encoders (e.g., CLIP or LLM) are good for adapting\nSAM for referring expression segmentation and introduce the Early\nVision-language Fusion-based SAM (EVF-SAM). EVF-SAM is a simple yet effective\nreferring segmentation method which exploits multimodal prompts (i.e., image\nand text) and comprises a pre-trained vision-language model to generate\nreferring prompts and a SAM model for segmentation. Surprisingly, we observe\nthat: (1) multimodal prompts and (2) vision-language models with early fusion\n(e.g., BEIT-3) are beneficial for prompting SAM for accurate referring\nsegmentation. Our experiments show that the proposed EVF-SAM based on BEIT-3\ncan obtain state-of-the-art performance on RefCOCO\/+\/g for referring expression\nsegmentation and demonstrate the superiority of prompting SAM with early\nvision-language fusion. In addition, the proposed EVF-SAM with 1.32B parameters\nachieves remarkably higher performance while reducing nearly 82% of parameters\ncompared to previous SAM methods based on large multimodal models.\n","versions":"[{'version': 'v1', 'created': 'Fri, 28 Jun 2024 17:38:18 GMT'}, {'version': 'v2', 'created': 'Wed, 3 Jul 2024 07:59:52 GMT'}, {'version': 'v3', 'created': 'Fri, 9 Aug 2024 03:52:31 GMT'}, {'version': 'v4', 'created': 'Tue, 15 Oct 2024 06:17:10 GMT'}, {'version': 'v5', 'created': 'Mon, 10 Mar 2025 12:34:24 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhang', 'Yuxuan', ''], ['Cheng', 'Tianheng', ''], ['Zhu', 'Lianghui', ''], ['Hu', 'Rui', ''], ['Liu', 'Lei', ''], ['Liu', 'Heng', ''], ['Ran', 'Longjin', ''], ['Chen', 'Xiaoxin', ''], ['Liu', 'Wenyu', ''], ['Wang', 'Xinggang', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'prompting', 'label': 'Prompting'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2409.05816,"submitter":"Tristan Thrush","authors":"Tristan Thrush, Christopher Potts, Tatsunori Hashimoto","title":"Improving Pretraining Data Using Perplexity Correlations","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG stat.ML","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Quality pretraining data is often seen as the key to high-performance\nlanguage models. However, progress in understanding pretraining data has been\nslow due to the costly pretraining runs required for data selection\nexperiments. We present a framework that avoids these costs and selects\nhigh-quality pretraining data without any LLM training of our own. Our work is\nbased on a simple observation: LLM losses on many pretraining texts are\ncorrelated with downstream benchmark performance, and selecting\nhigh-correlation documents is an effective pretraining data selection method.\nWe build a new statistical framework for data selection centered around\nestimates of perplexity-benchmark correlations and perform data selection using\na sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of\nthousands of web domains. In controlled pretraining experiments at the 160M\nparameter scale on 8 benchmarks, our approach outperforms DSIR on every\nbenchmark, while matching the best data selector found in DataComp-LM, a\nhand-engineered bigram classifier. We have now also updated this paper to\ninclude results from preregistered experiments with new pretraining data on an\naggregation of 22 benchmarks up to the 1.4B scale, showing increasing\nimprovements of our method over others with more scale. A pip package with full\ndocumentation can be found here:\nhttps:\/\/github.com\/TristanThrush\/perplexity-correlations.\n","versions":"[{'version': 'v1', 'created': 'Mon, 9 Sep 2024 17:23:29 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 17:56:18 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Thrush', 'Tristan', ''], ['Potts', 'Christopher', ''], ['Hashimoto', 'Tatsunori', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'Open LLM Leaderboard', 'label': 'Open-source LLMs'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2410.06215,"submitter":"Zaid Khan","authors":"Zaid Khan, Elias Stengel-Eskin, Jaemin Cho, Mohit Bansal","title":"DataEnvGym: Data Generation Agents in Teacher Environments with Student\n  Feedback","comments":"ICLR 2025 Spotlight; Project Page: https:\/\/DataEnvGym.github.io","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The process of creating training data to teach models is currently driven by\nhumans, who manually analyze model weaknesses and plan how to create data that\nimproves a student model. Approaches using LLMs as annotators reduce human\neffort, but still require humans to interpret feedback from evaluations and\ncontrol the LLM to produce data the student needs. Automating this\nlabor-intensive process by creating autonomous data generation agents - or\nteachers - is desirable, but requires environments that can simulate the\nfeedback-driven, iterative, closed loop of data creation. To enable rapid,\nscalable testing for such agents and their modules, we introduce DataEnvGym, a\ntestbed of teacher environments for data generation agents. DataEnvGym frames\ndata generation as a sequential decision-making task, involving an agent\nconsisting of a data generation policy (which generates a plan for creating\ntraining data) and a data generation engine (which transforms the plan into\ndata), inside an environment that provides student feedback. The agent's goal\nis to improve student performance. Students are iteratively trained and\nevaluated on generated data, and their feedback (in the form of errors or weak\nskills) is reported to the agent after each iteration. DataEnvGym includes\nmultiple teacher environment instantiations across 3 levels of structure in the\nstate representation and action space. More structured environments are based\non inferred skills and offer more interpretability and curriculum control. We\nsupport 4 domains (math, code, VQA, and tool-use) and test multiple students\nand teachers. Example agents in our teaching environments can iteratively\nimprove students across tasks and settings. Moreover, we show that environments\nteach different skill levels and test variants of key modules, pointing to\nfuture work in improving data generation agents, engines, and feedback\nmechanisms.\n","versions":"[{'version': 'v1', 'created': 'Tue, 8 Oct 2024 17:20:37 GMT'}, {'version': 'v2', 'created': 'Tue, 17 Dec 2024 18:54:45 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 17:30:48 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Khan', 'Zaid', ''], ['Stengel-Eskin', 'Elias', ''], ['Cho', 'Jaemin', ''], ['Bansal', 'Mohit', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2410.12464,"submitter":"Qian Wang","authors":"Qian Wang, Yuchen Gao, Zhenheng Tang, Bingqiao Luo, Nuo Chen,\n  Bingsheng He","title":"Exploring LLM Cryptocurrency Trading Through Fact-Subjectivity Aware\n  Reasoning","comments":"Accepted at ICLR 2025 Financial AI Workshop","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.MA","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While many studies show that more advanced LLMs excel in tasks such as\nmathematics and coding, we observe that in cryptocurrency trading, stronger\nLLMs sometimes underperform compared to weaker ones. To investigate this\ncounterintuitive phenomenon, we examine how LLMs reason when making trading\ndecisions. Our findings reveal that (1) stronger LLMs show a preference for\nfactual information over subjectivity; (2) separating the reasoning process\ninto factual and subjective components leads to higher profits. Building on\nthese insights, we propose a multi-agent framework, FS-ReasoningAgent, which\nenables LLMs to recognize and learn from both factual and subjective reasoning.\nExtensive experiments demonstrate that this fine-grained reasoning approach\nenhances LLM trading performance in cryptocurrency markets, yielding profit\nimprovements of 7\\% in BTC, 2\\% in ETH, and 10\\% in SOL. Additionally, an\nablation study reveals that relying on subjective news generates higher returns\nin bull markets, while focusing on factual information yields better results in\nbear markets. Code is available at\nhttps:\/\/github.com\/Persdre\/FS-ReasoningAgent.\n","versions":"[{'version': 'v1', 'created': 'Wed, 16 Oct 2024 11:25:13 GMT'}, {'version': 'v2', 'created': 'Thu, 17 Oct 2024 09:01:11 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 12:50:00 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Wang', 'Qian', ''], ['Gao', 'Yuchen', ''], ['Tang', 'Zhenheng', ''], ['Luo', 'Bingqiao', ''], ['Chen', 'Nuo', ''], ['He', 'Bingsheng', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2410.1364,"submitter":"Yiming Wang","authors":"Yiming Wang, Pei Zhang, Baosong Yang, Derek F. Wong, Rui Wang","title":"Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation","comments":"Accepted by ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  LLM self-evaluation relies on the LLM's own ability to estimate response\ncorrectness, which can greatly improve its deployment reliability. In this\nresearch track, we propose the Chain-of-Embedding (CoE) in the latent space to\nenable LLMs to perform output-free self-evaluation. CoE consists of all\nprogressive hidden states produced during the inference time, which can be\ntreated as the latent thinking path of LLMs. We find that when LLMs respond\ncorrectly and incorrectly, their CoE features differ, these discrepancies\nassist us in estimating LLM response correctness. Experiments in four diverse\ndomains and seven LLMs fully demonstrate the effectiveness of our method.\nMeanwhile, its label-free design intent without any training and\nmillisecond-level computational cost ensures real-time feedback in large-scale\nscenarios. More importantly, we provide interesting insights into LLM response\ncorrectness from the perspective of hidden state changes inside LLMs.\n","versions":"[{'version': 'v1', 'created': 'Thu, 17 Oct 2024 15:09:24 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 16:16:12 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Yiming', ''], ['Zhang', 'Pei', ''], ['Yang', 'Baosong', ''], ['Wong', 'Derek F.', ''], ['Wang', 'Rui', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'Chain-of-Embedding', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'CoE', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'CoE', 'label': 'Embedding'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2410.20643,"submitter":"Wilson Wongso","authors":"Wilson Wongso, Hao Xue, Flora D. Salim","title":"GenUP: Generative User Profilers as In-Context Learners for Next POI\n  Recommender Systems","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Traditional Point-of-Interest (POI) recommendation systems often lack\ntransparency, interpretability, and scrutability due to their reliance on dense\nvector-based user embeddings. Furthermore, the cold-start problem -- where\nsystems have insufficient data for new users -- limits their ability to\ngenerate accurate recommendations. Existing methods often address this by\nleveraging similar trajectories from other users, but this approach can be\ncomputationally expensive and increases the context length for LLM-based\nmethods, making them difficult to scale. To address these limitations, we\npropose a method that generates natural language (NL) user profiles from\nlarge-scale, location-based social network (LBSN) check-ins, utilizing robust\npersonality assessments and behavioral theories. These NL profiles capture user\npreferences, routines, and behaviors, improving POI prediction accuracy while\noffering enhanced transparency. By incorporating NL profiles as system prompts\nto LLMs, our approach reduces reliance on extensive historical data, while\nremaining flexible, easily updated, and computationally efficient. Our method\nis not only competitive with other LLM-based and complex agentic frameworks but\nis also more scalable for real-world POI recommender systems. Results\ndemonstrate that our approach consistently outperforms baseline methods,\noffering a more interpretable and resource-efficient solution for POI\nrecommendation systems. Our source code is available at:\nhttps:\/\/github.com\/w11wo\/GenUP\/.\n","versions":"[{'version': 'v1', 'created': 'Mon, 28 Oct 2024 00:39:22 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 00:54:57 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wongso', 'Wilson', ''], ['Xue', 'Hao', ''], ['Salim', 'Flora D.', '']]","extracted_entities":"[{'text': 'dense\\nvector-based user embeddings', 'label': 'Embedding'}, {'text': 'system prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2410.22269,"submitter":"Nate Gillman","authors":"Nate Gillman, Daksh Aggarwal, Michael Freeman, Saurabh Singh, Chen Sun","title":"Fourier Head: Helping Large Language Models Learn Complex Probability\n  Distributions","comments":"Camera ready version (ICLR 2025). Code at\n  https:\/\/nategillman.com\/fourier-head","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL stat.ML","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  As the quality of large language models has improved, there has been\nincreased interest in using them to model non-linguistic tokens. For example,\nthe Decision Transformer recasts agentic decision making as a sequence modeling\nproblem, using a decoder-only LLM to model the distribution over the discrete\naction space for an Atari agent. However, when adapting LLMs to non-linguistic\ndomains, it remains unclear if softmax over discrete bins captures the\ncontinuous structure of the tokens and the potentially complex distributions\nneeded for high quality token generation. We introduce a neural network layer,\nconstructed using Fourier series, which we can easily substitute for any linear\nlayer if we want the outputs to have a more continuous structure. We perform\nextensive analysis on synthetic datasets, as well as on large-scale decision\nmaking and time series forecasting tasks. We also provide theoretical evidence\nthat this layer can better learn signal from data while ignoring high-frequency\nnoise. All of our results support the effectiveness of our proposed Fourier\nhead in scenarios where the underlying data distribution has a natural\ncontinuous structure. For example, the Fourier head improves a Decision\nTransformer agent's returns across four benchmark Atari games by as much as\n377%, and increases a state-of-the-art times series foundation model's\nforecasting performance by 3.5% across 20 benchmarks unseen during training.\n","versions":"[{'version': 'v1', 'created': 'Tue, 29 Oct 2024 17:27:58 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 23:59:12 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Gillman', 'Nate', ''], ['Aggarwal', 'Daksh', ''], ['Freeman', 'Michael', ''], ['Singh', 'Saurabh', ''], ['Sun', 'Chen', '']]","extracted_entities":"[{'text': 'decoder-only LLM', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'state-of-the-art times series foundation model', 'label': 'Foundation Model'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2411.18363,"submitter":"Qing Jiang","authors":"Qing Jiang, Gen Luo, Yuqin Yang, Yuda Xiong, Yihao Chen, Zhaoyang\n  Zeng, Tianhe Ren, Lei Zhang","title":"ChatRex: Taming Multimodal LLM for Joint Perception and Understanding","comments":"35 pages, 19 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Perception and understanding are two pillars of computer vision. While\nmultimodal large language models (MLLM) have demonstrated remarkable visual\nunderstanding capabilities, they arguably lack accurate perception abilities,\ne.g. the stage-of-the-art model Qwen2-VL only achieves a 43.9 recall rate on\nthe COCO dataset, limiting many tasks requiring the combination of perception\nand understanding. In this work, we aim to bridge this perception gap from both\nmodel designing and data development perspectives. We first introduce ChatRex,\nan MLLM with a decoupled perception design. Instead of having the LLM directly\npredict box coordinates, we feed the output boxes from a universal proposal\nnetwork into the LLM, allowing it to output the corresponding box indices to\nrepresent its detection results, turning the regression task into a\nretrieval-based task that LLM handles more proficiently. From the data\nperspective, we build a fully automated data engine and construct the\nRexverse-2M dataset which possesses multiple granularities to support the joint\ntraining of perception and understanding. After a three-stage training\napproach, ChatRex demonstrates strong perception and understanding performance,\nand the combination of these two capabilities also unlocks many attractive\napplications, demonstrating their complementary roles in MLLM. Code is\navailable at https:\/\/github.com\/IDEA-Research\/ChatRex.\n","versions":"[{'version': 'v1', 'created': 'Wed, 27 Nov 2024 14:11:10 GMT'}, {'version': 'v2', 'created': 'Mon, 2 Dec 2024 07:04:40 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 14:19:42 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Jiang', 'Qing', ''], ['Luo', 'Gen', ''], ['Yang', 'Yuqin', ''], ['Xiong', 'Yuda', ''], ['Chen', 'Yihao', ''], ['Zeng', 'Zhaoyang', ''], ['Ren', 'Tianhe', ''], ['Zhang', 'Lei', '']]","extracted_entities":"[{'text': 'MLLM', 'label': 'Large Language Model'}, {'text': 'ChatRex', 'label': 'ChatGPT'}, {'text': 'MLLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'ChatRex', 'label': 'ChatGPT'}, {'text': 'MLLM', 'label': 'Large Language Model'}, {'text': 'ChatRex', 'label': 'ChatGPT'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2412.05003,"submitter":"Cameon Braunstein","authors":"Cameron Braunstein, Hevra Petekkaya, Jan Eric Lenssen, Mariya Toneva,\n  Eddy Ilg","title":"SLayR: Scene Layout Generation with Rectified Flow","comments":"43 pages, 29 figures, 6 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We introduce SLayR, Scene Layout Generation with Rectified flow, a novel\ntransformer-based model for text-to-layout generation which can then be paired\nwith existing layout-to-image models to produce images. SLayR addresses a\ndomain in which current text-to-image pipelines struggle: generating scene\nlayouts that are of significant variety and plausibility, when the given prompt\nis ambiguous and does not provide constraints on the scene. SLayR surpasses\nexisting baselines including LLMs in unconstrained generation, and can generate\nlayouts from an open caption set. To accurately evaluate the layout generation,\nwe introduce a new benchmark suite, including numerical metrics and a carefully\ndesigned repeatable human-evaluation procedure that assesses the plausibility\nand variety of generated images. We show that our method sets a new state of\nthe art for achieving both at the same time, while being at least 3x times\nsmaller in the number of parameters.\n","versions":"[{'version': 'v1', 'created': 'Fri, 6 Dec 2024 12:58:58 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 10:40:48 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Braunstein', 'Cameron', ''], ['Petekkaya', 'Hevra', ''], ['Lenssen', 'Jan Eric', ''], ['Toneva', 'Mariya', ''], ['Ilg', 'Eddy', '']]","extracted_entities":"[{'text': 'SLayR', 'label': 'Transformer-based model'}, {'text': 'given prompt', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2412.15584,"submitter":"Jessica Bo","authors":"Jessica Y. Bo, Sophia Wan, and Ashton Anderson","title":"To Rely or Not to Rely? Evaluating Interventions for Appropriate\n  Reliance on Large Language Models","comments":null,"journal-ref":"Proceedings of the 2025 CHI Conference on Human Factors in\n  Computing Systems","doi":null,"report-no":null,"categories":"cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  As Large Language Models become integral to decision-making, optimism about\ntheir power is tempered with concern over their errors. Users may over-rely on\nLLM advice that is confidently stated but wrong, or under-rely due to mistrust.\nReliance interventions have been developed to help users of LLMs, but they lack\nrigorous evaluation for appropriate reliance. We benchmark the performance of\nthree relevant interventions by conducting a randomized online experiment with\n400 participants attempting two challenging tasks: LSAT logical reasoning and\nimage-based numerical estimation. For each question, participants first\nanswered independently, then received LLM advice modified by one of three\nreliance interventions and answered the question again. Our findings indicate\nthat while interventions reduce over-reliance, they generally fail to improve\nappropriate reliance. Furthermore, people became more confident after making\nwrong reliance decisions in certain contexts, demonstrating poor calibration.\nBased on our findings, we discuss implications for designing effective reliance\ninterventions in human-LLM collaboration.\n","versions":"[{'version': 'v1', 'created': 'Fri, 20 Dec 2024 05:40:32 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 03:39:53 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Bo', 'Jessica Y.', ''], ['Wan', 'Sophia', ''], ['Anderson', 'Ashton', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2501.05712,"submitter":"Hyunwoo Ko","authors":"Guijin Son, Hyunwoo Ko, Dasol Choi","title":"Multi-Step Reasoning in Korean and the Emergent Mirage","comments":"C3NLP @ NAACL 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  We introduce HRMCR (HAE-RAE Multi-Step Commonsense Reasoning), a benchmark\ndesigned to evaluate large language models' ability to perform multi-step\nreasoning in culturally specific contexts, focusing on Korean. The questions\nare automatically generated via templates and algorithms, requiring LLMs to\nintegrate Korean cultural knowledge into sequential reasoning steps. Consistent\nwith prior observations on emergent abilities, our experiments reveal that\nmodels trained on fewer than \\(2 \\cdot 10^{25}\\) training FLOPs struggle to\nsolve any questions, showing near-zero performance. Beyond this threshold,\nperformance improves sharply. State-of-the-art models (e.g., O1) still score\nunder 50\\%, underscoring the difficulty of our tasks. Notably, stepwise\nanalysis suggests the observed emergent behavior may stem from compounding\nerrors across multiple steps rather than reflecting a genuinely new capability.\nWe publicly release the benchmark and commit to regularly updating the dataset\nto prevent contamination.\n","versions":"[{'version': 'v1', 'created': 'Fri, 10 Jan 2025 05:07:27 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 08:45:28 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Son', 'Guijin', ''], ['Ko', 'Hyunwoo', ''], ['Choi', 'Dasol', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2501.108,"submitter":"Emanuele La Malfa","authors":"Oliver Goldstein, Emanuele La Malfa, Felix Drinkall, Samuele Marro,\n  Michael Wooldridge","title":"Jailbreaking Large Language Models in Infinitely Many Ways","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We discuss the ``Infinitely Many Paraphrases'' attacks (IMP), a category of\njailbreaks that leverages the increasing capabilities of a model to handle\nparaphrases and encoded communications to bypass their defensive mechanisms.\nIMPs' viability pairs and grows with a model's capabilities to handle and bind\nthe semantics of simple mappings between tokens and work extremely well in\npractice, posing a concrete threat to the users of the most powerful LLMs in\ncommerce. We show how one can bypass the safeguards of the most powerful open-\nand closed-source LLMs and generate content that explicitly violates their\nsafety policies. One can protect against IMPs by improving the guardrails and\nmaking them scale with the LLMs' capabilities. For two categories of attacks\nthat are straightforward to implement, i.e., bijection and encoding, we discuss\ntwo defensive strategies, one in token and the other in embedding space. We\nconclude with some research questions we believe should be prioritised to\nenhance the defensive mechanisms of LLMs and our understanding of their safety.\n","versions":"[{'version': 'v1', 'created': 'Sat, 18 Jan 2025 15:39:53 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 08:43:27 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Goldstein', 'Oliver', ''], ['La Malfa', 'Emanuele', ''], ['Drinkall', 'Felix', ''], ['Marro', 'Samuele', ''], ['Wooldridge', 'Michael', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}, {'text': 'safety policies', 'label': 'AI Ethics'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'embedding space', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2502.11995,"submitter":"Siddhesh Pawar","authors":"Siddhesh Pawar, Arnav Arora, Lucie-Aim\\'ee Kaffee, Isabelle Augenstein","title":"Presumed Cultural Identity: How Names Shape LLM Responses","comments":"23 Pages, 13 Figures, 4 Tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Names are deeply tied to human identity. They can serve as markers of\nindividuality, cultural heritage, and personal history. However, using names as\na core indicator of identity can lead to over-simplification of complex\nidentities. When interacting with LLMs, user names are an important point of\ninformation for personalisation. Names can enter chatbot conversations through\ndirect user input (requested by chatbots), as part of task contexts such as CV\nreviews, or as built-in memory features that store user information for\npersonalisation. We study biases associated with names by measuring cultural\npresumptions in the responses generated by LLMs when presented with common\nsuggestion-seeking queries, which might involve making assumptions about the\nuser. Our analyses demonstrate strong assumptions about cultural identity\nassociated with names present in LLM generations across multiple cultures. Our\nwork has implications for designing more nuanced personalisation systems that\navoid reinforcing stereotypes while maintaining meaningful customisation.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Feb 2025 16:35:15 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 10:48:57 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Pawar', 'Siddhesh', ''], ['Arora', 'Arnav', ''], ['Kaffee', 'Lucie-Aim\u00e9e', ''], ['Augenstein', 'Isabelle', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}, {'text': 'chatbots', 'label': 'ChatGPT'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2502.2014,"submitter":"Max Melchior Lang","authors":"Max M. Lang, Sol Eskenazi","title":"Telephone Surveys Meet Conversational AI: Evaluating a LLM-Based\n  Telephone Survey System at Scale","comments":"Accepted at 80th AAPOR Conference 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Telephone surveys remain a valuable tool for gathering insights but typically\nrequire substantial resources in training and coordinating human interviewers.\nThis work presents an AI-driven telephone survey system integrating\ntext-to-speech (TTS), a large language model (LLM), and speech-to-text (STT)\nthat mimics the versatility of human-led interviews (full-duplex dialogues) at\nscale.\n  We tested the system across two populations, a pilot study in the United\nStates (n = 75) and a large-scale deployment in Peru (n = 2,739), inviting\nparticipants via web-based links and contacting them via direct phone calls.\nThe AI agent successfully administered open-ended and closed-ended questions,\nhandled basic clarifications, and dynamically navigated branching logic,\nallowing fast large-scale survey deployment without interviewer recruitment or\ntraining.\n  Our findings demonstrate that while the AI system's probing for qualitative\ndepth was more limited than human interviewers, overall data quality approached\nhuman-led standards for structured items. This study represents one of the\nfirst successful large-scale deployments of an LLM-based telephone interviewer\nin a real-world survey context. The AI-powered telephone survey system has the\npotential for expanding scalable, consistent data collecting across market\nresearch, social science, and public opinion studies, thus improving\noperational efficiency while maintaining appropriate data quality for research.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 14:31:42 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 00:52:23 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Lang', 'Max M.', ''], ['Eskenazi', 'Sol', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2503.02191,"submitter":"Mia Mohammad Imran","authors":"Mia Mohammad Imran, Robert Zita, Rebekah Copeland, Preetha Chatterjee,\n  Rahat Rizvi Rahman, and Kostadin Damevski","title":"Understanding and Predicting Derailment in Toxic Conversations on GitHub","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Software projects thrive on the involvement and contributions of individuals\nfrom different backgrounds. However, toxic language and negative interactions\ncan hinder the participation and retention of contributors and alienate\nnewcomers. Proactive moderation strategies aim to prevent toxicity from\noccurring by addressing conversations that have derailed from their intended\npurpose. This study aims to understand and predict conversational derailment\nleading to toxicity on GitHub.\n  To facilitate this research, we curate a novel dataset comprising 202 toxic\nconversations from GitHub with annotated derailment points, along with 696\nnon-toxic conversations as a baseline. Based on this dataset, we identify\nunique characteristics of toxic conversations and derailment points, including\nlinguistic markers such as second-person pronouns, negation terms, and tones of\nBitter Frustration and Impatience, as well as patterns in conversational\ndynamics between project contributors and external participants.\n  Leveraging these empirical observations, we propose a proactive moderation\napproach to automatically detect and address potentially harmful conversations\nbefore escalation. By utilizing modern LLMs, we develop a conversation\ntrajectory summary technique that captures the evolution of discussions and\nidentifies early signs of derailment. Our experiments demonstrate that LLM\nprompts tailored to provide summaries of GitHub conversations achieve 70%\nF1-Score in predicting conversational derailment, strongly improving over a set\nof baseline approaches.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 02:01:37 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 03:25:44 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Imran', 'Mia Mohammad', ''], ['Zita', 'Robert', ''], ['Copeland', 'Rebekah', ''], ['Chatterjee', 'Preetha', ''], ['Rahman', 'Rahat Rizvi', ''], ['Damevski', 'Kostadin', '']]","extracted_entities":"[{'text': 'GitHub', 'label': 'Open-source LLMs'}, {'text': 'GitHub', 'label': 'Open-source LLMs'}, {'text': 'modern LLMs', 'label': 'LLM'}, {'text': 'GitHub', 'label': 'Open-source LLMs'}]","assigned_concept":"LLM","matched_keyword":"modern LLMs","similarity_score":0.7401012182}
{"id":2503.0528,"submitter":"Jiarui Liu","authors":"Neemesh Yadav, Jiarui Liu, Francesco Ortu, Roya Ensafi, Zhijing Jin,\n  Rada Mihalcea","title":"Revealing Hidden Mechanisms of Cross-Country Content Moderation with\n  Natural Language Processing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The ability of Natural Language Processing (NLP) methods to categorize text\ninto multiple classes has motivated their use in online content moderation\ntasks, such as hate speech and fake news detection. However, there is limited\nunderstanding of how or why these methods make such decisions, or why certain\ncontent is moderated in the first place. To investigate the hidden mechanisms\nbehind content moderation, we explore multiple directions: 1) training\nclassifiers to reverse-engineer content moderation decisions across countries;\n2) explaining content moderation decisions by analyzing Shapley values and\nLLM-guided explanations. Our primary focus is on content moderation decisions\nmade across countries, using pre-existing corpora sampled from the Twitter\nStream Grab. Our experiments reveal interesting patterns in censored posts,\nboth across countries and over time. Through human evaluations of LLM-generated\nexplanations across three LLMs, we assess the effectiveness of using LLMs in\ncontent moderation. Finally, we discuss potential future directions, as well as\nthe limitations and ethical considerations of this work. Our code and data are\navailable at https:\/\/github.com\/causalNLP\/censorship\n","versions":"[{'version': 'v1', 'created': 'Fri, 7 Mar 2025 09:49:31 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 04:41:06 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Yadav', 'Neemesh', ''], ['Liu', 'Jiarui', ''], ['Ortu', 'Francesco', ''], ['Ensafi', 'Roya', ''], ['Jin', 'Zhijing', ''], ['Mihalcea', 'Rada', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'ethical considerations', 'label': 'AI Ethics'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2503.05965,"submitter":"Luke Guerdan","authors":"Luke Guerdan, Solon Barocas, Kenneth Holstein, Hanna Wallach, Zhiwei\n  Steven Wu, Alexandra Chouldechova","title":"Validating LLM-as-a-Judge Systems in the Absence of Gold Labels","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CY cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The LLM-as-a-judge paradigm, in which a judge LLM system replaces human\nraters in rating the outputs of other generative AI (GenAI) systems, has come\nto play a critical role in scaling and standardizing GenAI evaluations. To\nvalidate judge systems, evaluators collect multiple human ratings for each item\nin a validation corpus, and then aggregate the ratings into a single, per-item\ngold label rating. High agreement rates between these gold labels and judge\nsystem ratings are then taken as a sign of good judge system performance. In\nmany cases, however, items or rating criteria may be ambiguous, or there may be\nprincipled disagreement among human raters. In such settings, gold labels may\nnot exist for many of the items. In this paper, we introduce a framework for\nLLM-as-a-judge validation in the absence of gold labels. We present a\ntheoretical analysis drawing connections between different measures of judge\nsystem performance under different rating elicitation and aggregation schemes.\nWe also demonstrate empirically that existing validation approaches can select\njudge systems that are highly suboptimal, performing as much as 34% worse than\nthe systems selected by alternative approaches that we describe. Based on our\nfindings, we provide concrete recommendations for developing more reliable\napproaches to LLM-as-a-judge validation.\n","versions":"[{'version': 'v1', 'created': 'Fri, 7 Mar 2025 22:09:47 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 21:21:35 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Guerdan', 'Luke', ''], ['Barocas', 'Solon', ''], ['Holstein', 'Kenneth', ''], ['Wallach', 'Hanna', ''], ['Wu', 'Zhiwei Steven', ''], ['Chouldechova', 'Alexandra', '']]","extracted_entities":"[{'text': 'LLM-as-a-judge', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM-as-a-judge', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2503.06509,"submitter":"Shubham Kumar","authors":"Shubham Kumar, Nihar Kumar Mahatoa, Debdas Ghosh","title":"Robust Optimization Approach for Solving Uncertain Multiobjective\n  Optimization Problems Using the Projected Gradient Method","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Numerous real-world applications of uncertain multiobjective optimization\nproblems (UMOPs) can be found in science, engineering, business, and\nmanagement. To handle the solution of uncertain optimization problems, robust\noptimization is a relatively new field. An extended version of the projected\ngradient method (PGM) for a deterministic smooth multiobjective optimization\nproblem (MOP) is presented in the current study as a PGM for UMOP. An\nobjective-wise worst-case cost (OWWC) type robust counterpart is considered,\nand the PGM is used to solve a UMOP by using OWWC. A projected gradient descent\nalgorithm is created using theoretical findings. It is demonstrated that the\nprojected gradient descent algorithm's generated sequence converges to the\nrobust counterpart's weak Pareto optimal solution, which will be the robust\nweak Pareto optimal solution for UMOP. Under a few reasonable presumptions, the\nprojected gradient descent algorithm's full convergent behavior is also\njustified. Finally, numerical tests are presented to validate the proposed\nmethod.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 08:28:54 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Kumar', 'Shubham', ''], ['Mahatoa', 'Nihar Kumar', ''], ['Ghosh', 'Debdas', '']]","extracted_entities":"[{'text': 'PGM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"PGM","similarity_score":0.5396178961}
{"id":2409.19075,"submitter":"Jie He","authors":"Yu Fu, Jie He, Yifan Yang, Qun Liu, Deyi Xiong","title":"Meta-RTL: Reinforcement-Based Meta-Transfer Learning for Low-Resource\n  Commonsense Reasoning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Meta learning has been widely used to exploit rich-resource source tasks to\nimprove the performance of low-resource target tasks. Unfortunately, most\nexisting meta learning approaches treat different source tasks equally,\nignoring the relatedness of source tasks to the target task in knowledge\ntransfer. To mitigate this issue, we propose a reinforcement-based multi-source\nmeta-transfer learning framework (Meta-RTL) for low-resource commonsense\nreasoning. In this framework, we present a reinforcement-based approach to\ndynamically estimating source task weights that measure the contribution of the\ncorresponding tasks to the target task in the meta-transfer learning. The\ndifferences between the general loss of the meta model and task-specific losses\nof source-specific temporal meta models on sampled target data are fed into the\npolicy network of the reinforcement learning module as rewards. The policy\nnetwork is built upon LSTMs that capture long-term dependencies on source task\nweight estimation across meta learning iterations. We evaluate the proposed\nMeta-RTL using both BERT and ALBERT as the backbone of the meta model on three\ncommonsense reasoning benchmark datasets. Experimental results demonstrate that\nMeta-RTL substantially outperforms strong baselines and previous task selection\nstrategies and achieves larger improvements on extremely low-resource settings.\n","versions":"[{'version': 'v1', 'created': 'Fri, 27 Sep 2024 18:22:22 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 09:31:15 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Fu', 'Yu', ''], ['He', 'Jie', ''], ['Yang', 'Yifan', ''], ['Liu', 'Qun', ''], ['Xiong', 'Deyi', '']]","extracted_entities":"[{'text': 'Meta learning', 'label': 'Few-shot Learning'}, {'text': 'meta learning', 'label': 'Few-shot Learning'}, {'text': 'meta-transfer learning', 'label': 'Few-shot Learning'}, {'text': 'meta learning', 'label': 'Few-shot Learning'}, {'text': 'BERT', 'label': 'BERT'}, {'text': 'ALBERT', 'label': 'ALBERT'}]","assigned_concept":"BERT","matched_keyword":"BERT","similarity_score":1.0}
{"id":2410.01306,"submitter":"Abdur Rasool","authors":"Abdur Rasool, Muhammad Irfan Shahzad, Hafsa Aslam, Vincent Chan,\n  Muhammad Ali Arshad","title":"Emotion-Aware Embedding Fusion in LLMs (Flan-T5, LLAMA 2, DeepSeek-R1,\n  and ChatGPT 4) for Intelligent Response Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CY","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Empathetic and coherent responses are critical in auto-mated\nchatbot-facilitated psychotherapy. This study addresses the challenge of\nenhancing the emotional and contextual understanding of large language models\n(LLMs) in psychiatric applications. We introduce Emotion-Aware Embedding\nFusion, a novel framework integrating hierarchical fusion and attention\nmechanisms to prioritize semantic and emotional features in therapy\ntranscripts. Our approach combines multiple emotion lexicons, including NRC\nEmotion Lexicon, VADER, WordNet, and SentiWordNet, with state-of-the-art LLMs\nsuch as Flan-T5, LLAMA 2, DeepSeek-R1, and ChatGPT 4. Therapy session\ntranscripts, comprising over 2,000 samples are segmented into hierarchical\nlevels (word, sentence, and session) using neural networks, while hierarchical\nfusion combines these features with pooling techniques to refine emotional\nrepresentations. Atten-tion mechanisms, including multi-head self-attention and\ncross-attention, further prioritize emotional and contextual features, enabling\ntemporal modeling of emotion-al shifts across sessions. The processed\nembeddings, computed using BERT, GPT-3, and RoBERTa are stored in the Facebook\nAI similarity search vector database, which enables efficient similarity search\nand clustering across dense vector spaces. Upon user queries, relevant segments\nare retrieved and provided as context to LLMs, enhancing their ability to\ngenerate empathetic and con-textually relevant responses. The proposed\nframework is evaluated across multiple practical use cases to demonstrate\nreal-world applicability, including AI-driven therapy chatbots. The system can\nbe integrated into existing mental health platforms to generate personalized\nresponses based on retrieved therapy session data.\n","versions":"[{'version': 'v1', 'created': 'Wed, 2 Oct 2024 08:01:05 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 10:08:37 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Rasool', 'Abdur', ''], ['Shahzad', 'Muhammad Irfan', ''], ['Aslam', 'Hafsa', ''], ['Chan', 'Vincent', ''], ['Arshad', 'Muhammad Ali', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'attention\\nmechanisms', 'label': 'Attention mechanism'}, {'text': 'Flan-T5', 'label': 'Large Language Model'}, {'text': 'ChatGPT 4', 'label': 'ChatGPT'}, {'text': 'Atten-tion mechanisms', 'label': 'Attention mechanism'}, {'text': 'multi-head self-attention', 'label': 'Attention mechanism'}, {'text': 'cross-attention', 'label': 'Attention mechanism'}, {'text': 'BERT', 'label': 'BERT'}, {'text': 'GPT-3', 'label': 'ChatGPT'}, {'text': 'RoBERTa', 'label': 'RoBERTa'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"BERT","matched_keyword":"BERT","similarity_score":1.0}
{"id":2410.18403,"submitter":"Jiarui Lu","authors":"Jiarui Lu, Xiaoyin Chen, Stephen Zhewen Lu, Chence Shi, Hongyu Guo,\n  Yoshua Bengio and Jian Tang","title":"Structure Language Models for Protein Conformation Generation","comments":"Published as a conference paper at ICLR 2025, see\n  https:\/\/openreview.net\/forum?id=OzUNDnpQyd","journal-ref":null,"doi":null,"report-no":null,"categories":"q-bio.BM cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Proteins adopt multiple structural conformations to perform their diverse\nbiological functions, and understanding these conformations is crucial for\nadvancing drug discovery. Traditional physics-based simulation methods often\nstruggle with sampling equilibrium conformations and are computationally\nexpensive. Recently, deep generative models have shown promise in generating\nprotein conformations as a more efficient alternative. However, these methods\npredominantly rely on the diffusion process within a 3D geometric space, which\ntypically centers around the vicinity of metastable states and is often\ninefficient in terms of runtime. In this paper, we introduce Structure Language\nModeling (SLM) as a novel framework for efficient protein conformation\ngeneration. Specifically, the protein structures are first encoded into a\ncompact latent space using a discrete variational auto-encoder, followed by\nconditional language modeling that effectively captures sequence-specific\nconformation distributions. This enables a more efficient and interpretable\nexploration of diverse ensemble modes compared to existing methods. Based on\nthis general framework, we instantiate SLM with various popular LM\narchitectures as well as proposing the ESMDiff, a novel BERT-like structure\nlanguage model fine-tuned from ESM3 with masked diffusion. We verify our\napproach in various scenarios, including the equilibrium dynamics of BPTI,\nconformational change pairs, and intrinsically disordered proteins. SLM\nprovides a highly efficient solution, offering a 20-100x speedup than existing\nmethods in generating diverse conformations, shedding light on promising\navenues for future research.\n","versions":"[{'version': 'v1', 'created': 'Thu, 24 Oct 2024 03:38:51 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 19:06:38 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Lu', 'Jiarui', ''], ['Chen', 'Xiaoyin', ''], ['Lu', 'Stephen Zhewen', ''], ['Shi', 'Chence', ''], ['Guo', 'Hongyu', ''], ['Bengio', 'Yoshua', ''], ['Tang', 'Jian', '']]","extracted_entities":"[{'text': 'BERT-like', 'label': 'BERT'}]","assigned_concept":"BERT","matched_keyword":"BERT-like","similarity_score":0.8565326333}
{"id":2502.19339,"submitter":"Tohida Rehman Ms.","authors":"Tohida Rehman, Soumabha Ghosh, Kuntal Das, Souvik Bhattacharjee,\n  Debarshi Kumar Sanyal, Samiran Chattopadhyay","title":"Evaluating LLMs and Pre-trained Models for Text Summarization Across\n  Diverse Datasets","comments":"5 pages, 2 figures, 6 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Text summarization plays a crucial role in natural language processing by\ncondensing large volumes of text into concise and coherent summaries. As\ndigital content continues to grow rapidly and the demand for effective\ninformation retrieval increases, text summarization has become a focal point of\nresearch in recent years. This study offers a thorough evaluation of four\nleading pre-trained and open-source large language models: BART, FLAN-T5,\nLLaMA-3-8B, and Gemma-7B, across five diverse datasets CNN\/DM, Gigaword, News\nSummary, XSum, and BBC News. The evaluation employs widely recognized automatic\nmetrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and METEOR, to assess\nthe models' capabilities in generating coherent and informative summaries. The\nresults reveal the comparative strengths and limitations of these models in\nprocessing various text types.\n","versions":"[{'version': 'v1', 'created': 'Wed, 26 Feb 2025 17:32:07 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 09:40:42 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Rehman', 'Tohida', ''], ['Ghosh', 'Soumabha', ''], ['Das', 'Kuntal', ''], ['Bhattacharjee', 'Souvik', ''], ['Sanyal', 'Debarshi Kumar', ''], ['Chattopadhyay', 'Samiran', '']]","extracted_entities":"[{'text': 'Text summarization', 'label': 'Knowledge distillation'}, {'text': 'text summarization', 'label': 'Knowledge distillation'}, {'text': 'FLAN-T5', 'label': 'Large Language Model'}, {'text': 'Gigaword', 'label': 'Large Language Model'}, {'text': 'ROUGE-1', 'label': 'BERT'}, {'text': 'BERTScore', 'label': 'BERT'}]","assigned_concept":"BERT","matched_keyword":"BERTScore","similarity_score":0.7477546334}
{"id":2310.1938,"submitter":"Meng Lou","authors":"Meng Lou, Shu Zhang, Hong-Yu Zhou, Chuan Wu, Sibei Yang, Yizhou Yu","title":"TransXNet: Learning Both Global and Local Dynamics with a Dual Dynamic\n  Token Mixer for Visual Recognition","comments":"Accepted by IEEE TNNLS. Code is available at\n  https:\/\/github.com\/LMMMEng\/TransXNet","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent studies have integrated convolutions into transformers to introduce\ninductive bias and improve generalization performance. However, the static\nnature of conventional convolution prevents it from dynamically adapting to\ninput variations, resulting in a representation discrepancy between convolution\nand self-attention as self-attention calculates attention matrices dynamically.\nFurthermore, when stacking token mixers that consist of convolution and\nself-attention to form a deep network, the static nature of convolution hinders\nthe fusion of features previously generated by self-attention into convolution\nkernels. These two limitations result in a sub-optimal representation capacity\nof the constructed networks. To find a solution, we propose a lightweight Dual\nDynamic Token Mixer (D-Mixer) to simultaneously learn global and local\ndynamics, that is, mechanisms that compute weights for aggregating global\ncontexts and local details in an input-dependent manner. D-Mixer works by\napplying an efficient global attention module and an input-dependent depthwise\nconvolution separately on evenly split feature segments, endowing the network\nwith strong inductive bias and an enlarged effective receptive field. We use\nD-Mixer as the basic building block to design TransXNet, a novel hybrid\nCNN-Transformer vision backbone network that delivers compelling performance.\nIn the ImageNet-1K classification task, TransXNet-T surpasses Swin-T by 0.3% in\ntop-1 accuracy while requiring less than half of the computational cost.\nFurthermore, TransXNet-S and TransXNet-B exhibit excellent model scalability,\nachieving top-1 accuracy of 83.8% and 84.6% respectively, with reasonable\ncomputational costs. Additionally, our proposed network architecture\ndemonstrates strong generalization capabilities in various dense prediction\ntasks, outperforming other state-of-the-art networks while having lower\ncomputational costs.\n","versions":"[{'version': 'v1', 'created': 'Mon, 30 Oct 2023 09:35:56 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Nov 2023 01:48:03 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 09:09:26 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Lou', 'Meng', ''], ['Zhang', 'Shu', ''], ['Zhou', 'Hong-Yu', ''], ['Wu', 'Chuan', ''], ['Yang', 'Sibei', ''], ['Yu', 'Yizhou', '']]","extracted_entities":"[{'text': 'transformers', 'label': 'Transformers'}, {'text': 'inductive bias', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'global attention', 'label': 'Attention mechanism'}, {'text': 'inductive bias', 'label': 'Attention mechanism'}]","assigned_concept":"Transformers","matched_keyword":"transformers","similarity_score":1.0}
{"id":2311.00226,"submitter":"Vishnu Teja Kunde","authors":"Vishnu Teja Kunde, Vicram Rajagopalan, Chandra Shekhara Kaushik\n  Valmeekam, Krishna Narayanan, Srinivas Shakkottai, Dileep Kalathil,\n  Jean-Francois Chamberland","title":"Transformers are Provably Optimal In-context Estimators for Wireless\n  Communications","comments":"Accepted at AISTATS 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Pre-trained transformers exhibit the capability of adapting to new tasks\nthrough in-context learning (ICL), where they efficiently utilize a limited set\nof prompts without explicit model optimization. The canonical communication\nproblem of estimating transmitted symbols from received observations can be\nmodeled as an in-context learning problem: received observations are a noisy\nfunction of transmitted symbols, and this function can be represented by an\nunknown parameter whose statistics depend on an unknown latent context. This\nproblem, which we term in-context estimation (ICE), has significantly greater\ncomplexity than the extensively studied linear regression problem. The optimal\nsolution to the ICE problem is a non-linear function of the underlying context.\nIn this paper, we prove that, for a subclass of such problems, a single-layer\nsoftmax attention transformer (SAT) computes the optimal solution of the above\nestimation problem in the limit of large prompt length. We also prove that the\noptimal configuration of such a transformer is indeed the minimizer of the\ncorresponding training loss. Further, we empirically demonstrate the\nproficiency of multi-layer transformers in efficiently solving broader\nin-context estimation problems. Through extensive simulations, we show that\nsolving ICE problems using transformers significantly outperforms standard\napproaches. Moreover, just with a few context examples, it achieves the same\nperformance as an estimator with perfect knowledge of the latent context. The\ncode is available\n\\href{https:\/\/github.com\/vishnutez\/in-context-estimation}{here}.\n","versions":"[{'version': 'v1', 'created': 'Wed, 1 Nov 2023 02:16:24 GMT'}, {'version': 'v2', 'created': 'Sun, 3 Dec 2023 04:31:28 GMT'}, {'version': 'v3', 'created': 'Fri, 14 Jun 2024 18:05:14 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Mar 2025 16:24:05 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Kunde', 'Vishnu Teja', ''], ['Rajagopalan', 'Vicram', ''], ['Valmeekam', 'Chandra Shekhara Kaushik', ''], ['Narayanan', 'Krishna', ''], ['Shakkottai', 'Srinivas', ''], ['Kalathil', 'Dileep', ''], ['Chamberland', 'Jean-Francois', '']]","extracted_entities":"[{'text': 'Pre-trained transformers', 'label': 'Transformers'}, {'text': 'in-context learning', 'label': 'Few-shot Learning'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'in-context learning', 'label': 'Few-shot Learning'}, {'text': 'in-context estimation', 'label': 'Few-shot Learning'}, {'text': 'multi-layer transformers', 'label': 'Transformers'}, {'text': 'transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"transformers","similarity_score":1.0}
{"id":2401.0074,"submitter":"Zeke Zexi Hu","authors":"Zeke Zexi Hu, Xiaoming Chen, Vera Yuk Ying Chung, Yiran Shen","title":"Beyond Subspace Isolation: Many-to-Many Transformer for Light Field\n  Image Super-resolution","comments":"Accepted by IEEE Transactions on Multimedia","journal-ref":null,"doi":"10.1109\/TMM.2024.3521795","report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The effective extraction of spatial-angular features plays a crucial role in\nlight field image super-resolution (LFSR) tasks, and the introduction of\nconvolution and Transformers leads to significant improvement in this area.\nNevertheless, due to the large 4D data volume of light field images, many\nexisting methods opted to decompose the data into a number of lower-dimensional\nsubspaces and perform Transformers in each sub-space individually. As a side\neffect, these methods inadvertently restrict the self-attention mechanisms to a\nOne-to-One scheme accessing only a limited subset of LF data, explicitly\npreventing comprehensive optimization on all spatial and angular cues. In this\npaper, we identify this limitation as subspace isolation and introduce a novel\nMany-to-Many Transformer (M2MT) to address it. M2MT aggregates angular\ninformation in the spatial subspace before performing the self-attention\nmechanism. It enables complete access to all information across all\nsub-aperture images (SAIs) in a light field image. Consequently, M2MT is\nenabled to comprehensively capture long-range correlation dependencies. With\nM2MT as the pivotal component, we develop a simple yet effective M2MT network\nfor LFSR. Our experimental results demonstrate that M2MT achieves\nstate-of-the-art performance across various public datasets. We further conduct\nin-depth analysis using local attribution maps (LAM) to obtain visual\ninterpretability, and the results validate that M2MT is empowered with a truly\nnon-local context in both spatial and angular subspaces to mitigate subspace\nisolation and acquire effective spatial-angular representation.\n","versions":"[{'version': 'v1', 'created': 'Mon, 1 Jan 2024 12:48:23 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 12:54:24 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Hu', 'Zeke Zexi', ''], ['Chen', 'Xiaoming', ''], ['Chung', 'Vera Yuk Ying', ''], ['Shen', 'Yiran', '']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'self-attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'self-attention\\nmechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2402.09469,"submitter":"Zhenmei Shi","authors":"Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Tianyi Zhou","title":"Fourier Circuits in Neural Networks and Transformers: A Case Study of\n  Modular Arithmetic with Multiple Inputs","comments":"AIStats 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ML","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In the evolving landscape of machine learning, a pivotal challenge lies in\ndeciphering the internal representations harnessed by neural networks and\nTransformers. Building on recent progress toward comprehending how networks\nexecute distinct target functions, our study embarks on an exploration of the\nunderlying reasons behind networks adopting specific computational strategies.\nWe direct our focus to the complex algebraic learning task of modular addition\ninvolving $k$ inputs. Our research presents a thorough analytical\ncharacterization of the features learned by stylized one-hidden layer neural\nnetworks and one-layer Transformers in addressing this task. A cornerstone of\nour theoretical framework is the elucidation of how the principle of margin\nmaximization shapes the features adopted by one-hidden layer neural networks.\nLet $p$ denote the modulus, $D_p$ denote the dataset of modular arithmetic with\n$k$ inputs and $m$ denote the network width. We demonstrate that a neuron count\nof $ m \\geq 2^{2k-2} \\cdot (p-1) $, these networks attain a maximum $ L_{2,k+1}\n$-margin on the dataset $ D_p $. Furthermore, we establish that each\nhidden-layer neuron aligns with a specific Fourier spectrum, integral to\nsolving modular addition problems. By correlating our findings with the\nempirical observations of similar studies, we contribute to a deeper\ncomprehension of the intrinsic computational mechanisms of neural networks.\nFurthermore, we observe similar computational mechanisms in attention matrices\nof one-layer Transformers. Our work stands as a significant stride in\nunraveling their operation complexities, particularly in the realm of complex\nalgebraic tasks.\n","versions":"[{'version': 'v1', 'created': 'Mon, 12 Feb 2024 05:52:06 GMT'}, {'version': 'v2', 'created': 'Fri, 24 May 2024 07:28:24 GMT'}, {'version': 'v3', 'created': 'Wed, 16 Oct 2024 06:48:42 GMT'}, {'version': 'v4', 'created': 'Sun, 9 Mar 2025 07:14:46 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Li', 'Chenyang', ''], ['Liang', 'Yingyu', ''], ['Shi', 'Zhenmei', ''], ['Song', 'Zhao', ''], ['Zhou', 'Tianyi', '']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'one-layer Transformers', 'label': 'Transformers'}, {'text': 'attention matrices', 'label': 'Attention mechanism'}, {'text': 'one-layer Transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2404.02082,"submitter":"Dong Chen","authors":"Chen Yang, Yangfan He, Aaron Xuxiang Tian, Dong Chen, Jianhui Wang,\n  Tianyu Shi, Arsalan Heydarian, Pei Liu","title":"WcDT: World-centric Diffusion Transformer for Traffic Scene Generation","comments":"7 pages, 5 figures","journal-ref":"ICRA 2025","doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this paper, we introduce a novel approach for autonomous driving\ntrajectory generation by harnessing the complementary strengths of diffusion\nprobabilistic models (a.k.a., diffusion models) and transformers. Our proposed\nframework, termed the \"World-Centric Diffusion Transformer\"(WcDT), optimizes\nthe entire trajectory generation process, from feature extraction to model\ninference. To enhance the scene diversity and stochasticity, the historical\ntrajectory data is first preprocessed into \"Agent Move Statement\" and encoded\ninto latent space using Denoising Diffusion Probabilistic Models (DDPM)\nenhanced with Diffusion with Transformer (DiT) blocks. Then, the latent\nfeatures, historical trajectories, HD map features, and historical traffic\nsignal information are fused with various transformer-based encoders that are\nused to enhance the interaction of agents with other elements in the traffic\nscene. The encoded traffic scenes are then decoded by a trajectory decoder to\ngenerate multimodal future trajectories. Comprehensive experimental results\nshow that the proposed approach exhibits superior performance in generating\nboth realistic and diverse trajectories, showing its potential for integration\ninto automatic driving simulation systems. Our code is available at\n\\url{https:\/\/github.com\/yangchen1997\/WcDT}.\n","versions":"[{'version': 'v1', 'created': 'Tue, 2 Apr 2024 16:28:41 GMT'}, {'version': 'v2', 'created': 'Sat, 28 Sep 2024 20:12:51 GMT'}, {'version': 'v3', 'created': 'Fri, 4 Oct 2024 01:10:29 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Mar 2025 02:32:41 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Yang', 'Chen', ''], ['He', 'Yangfan', ''], ['Tian', 'Aaron Xuxiang', ''], ['Chen', 'Dong', ''], ['Wang', 'Jianhui', ''], ['Shi', 'Tianyu', ''], ['Heydarian', 'Arsalan', ''], ['Liu', 'Pei', '']]","extracted_entities":"[{'text': 'transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"transformers","similarity_score":1.0}
{"id":2404.06564,"submitter":"Haoyang He","authors":"Haoyang He, Yuhu Bai, Jiangning Zhang, Qingdong He, Hongxu Chen,\n  Zhenye Gan, Chengjie Wang, Xiangtai Li, Guanzhong Tian, Lei Xie","title":"MambaAD: Exploring State Space Models for Multi-class Unsupervised\n  Anomaly Detection","comments":"NeurIPS'24","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advancements in anomaly detection have seen the efficacy of CNN- and\ntransformer-based approaches. However, CNNs struggle with long-range\ndependencies, while transformers are burdened by quadratic computational\ncomplexity. Mamba-based models, with their superior long-range modeling and\nlinear efficiency, have garnered substantial attention. This study pioneers the\napplication of Mamba to multi-class unsupervised anomaly detection, presenting\nMambaAD, which consists of a pre-trained encoder and a Mamba decoder featuring\n(Locality-Enhanced State Space) LSS modules at multi-scales. The proposed LSS\nmodule, integrating parallel cascaded (Hybrid State Space) HSS blocks and\nmulti-kernel convolutions operations, effectively captures both long-range and\nlocal information. The HSS block, utilizing (Hybrid Scanning) HS encoders,\nencodes feature maps into five scanning methods and eight directions, thereby\nstrengthening global connections through the (State Space Model) SSM. The use\nof Hilbert scanning and eight directions significantly improves feature\nsequence modeling. Comprehensive experiments on six diverse anomaly detection\ndatasets and seven metrics demonstrate state-of-the-art performance,\nsubstantiating the method's effectiveness. The code and models are available at\nhttps:\/\/lewandofskee.github.io\/projects\/MambaAD.\n","versions":"[{'version': 'v1', 'created': 'Tue, 9 Apr 2024 18:28:55 GMT'}, {'version': 'v2', 'created': 'Thu, 11 Apr 2024 16:06:39 GMT'}, {'version': 'v3', 'created': 'Sun, 14 Apr 2024 09:14:23 GMT'}, {'version': 'v4', 'created': 'Sun, 9 Mar 2025 15:56:38 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['He', 'Haoyang', ''], ['Bai', 'Yuhu', ''], ['Zhang', 'Jiangning', ''], ['He', 'Qingdong', ''], ['Chen', 'Hongxu', ''], ['Gan', 'Zhenye', ''], ['Wang', 'Chengjie', ''], ['Li', 'Xiangtai', ''], ['Tian', 'Guanzhong', ''], ['Xie', 'Lei', '']]","extracted_entities":"[{'text': 'transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"transformers","similarity_score":1.0}
{"id":2407.11496,"submitter":"Xinyi Wang","authors":"Xinyi Wang, Angeliki Katsenou, and David Bull","title":"ReLaX-VQA: Residual Fragment and Layer Stack Extraction for Enhancing\n  Video Quality Assessment","comments":"10 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV cs.MM","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  With the rapid growth of User-Generated Content (UGC) exchanged between users\nand sharing platforms, the need for video quality assessment in the wild is\nincreasingly evident. UGC is typically acquired using consumer devices and\nundergoes multiple rounds of compression (transcoding) before reaching the end\nuser. Therefore, traditional quality metrics that employ the original content\nas a reference are not suitable. In this paper, we propose ReLaX-VQA, a novel\nNo-Reference Video Quality Assessment (NR-VQA) model that aims to address the\nchallenges of evaluating the quality of diverse video content without reference\nto the original uncompressed videos. ReLaX-VQA uses frame differences to select\nspatio-temporal fragments intelligently together with different expressions of\nspatial features associated with the sampled frames. These are then used to\nbetter capture spatial and temporal variabilities in the quality of\nneighbouring frames. Furthermore, the model enhances abstraction by employing\nlayer-stacking techniques in deep neural network features from Residual\nNetworks and Vision Transformers. Extensive testing across four UGC datasets\ndemonstrates that ReLaX-VQA consistently outperforms existing NR-VQA methods,\nachieving an average SRCC of 0.8658 and PLCC of 0.8873. Open-source code and\ntrained models that will facilitate further research and applications of NR-VQA\ncan be found at https:\/\/github.com\/xinyiW915\/ReLaX-VQA.\n","versions":"[{'version': 'v1', 'created': 'Tue, 16 Jul 2024 08:33:55 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 17:37:47 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 18:07:16 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Xinyi', ''], ['Katsenou', 'Angeliki', ''], ['Bull', 'David', '']]","extracted_entities":"[{'text': 'Vision Transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Vision Transformers","similarity_score":0.7330732346}
{"id":2408.07514,"submitter":"Andr\\'as Kalapos","authors":"Andr\\'as Kalapos, B\\'alint Gyires-T\\'oth","title":"CNN-JEPA: Self-Supervised Pretraining Convolutional Neural Networks\n  Using Joint Embedding Predictive Architecture","comments":"Preprint","journal-ref":"2024 International Conference on Machine Learning and Applications\n  (ICMLA), Miami, FL, USA, 2024, pp. 1111-1114","doi":"10.1109\/ICMLA61862.2024.00169","report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Self-supervised learning (SSL) has become an important approach in\npretraining large neural networks, enabling unprecedented scaling of model and\ndataset sizes. While recent advances like I-JEPA have shown promising results\nfor Vision Transformers, adapting such methods to Convolutional Neural Networks\n(CNNs) presents unique challenges. In this paper, we introduce CNN-JEPA, a\nnovel SSL method that successfully applies the joint embedding predictive\narchitecture approach to CNNs. Our method incorporates a sparse CNN encoder to\nhandle masked inputs, a fully convolutional predictor using depthwise separable\nconvolutions, and an improved masking strategy. We demonstrate that CNN-JEPA\noutperforms I-JEPA with ViT architectures on ImageNet-100, achieving a 73.3%\nlinear top-1 accuracy using a standard ResNet-50 encoder. Compared to other\nCNN-based SSL methods, CNN-JEPA requires 17-35% less training time for the same\nnumber of epochs and approaches the linear and k-NN top-1 accuracies of BYOL,\nSimCLR, and VICReg. Our approach offers a simpler, more efficient alternative\nto existing SSL methods for CNNs, requiring minimal augmentations and no\nseparate projector network.\n","versions":"[{'version': 'v1', 'created': 'Wed, 14 Aug 2024 12:48:37 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 09:42:28 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Kalapos', 'Andr\u00e1s', ''], ['Gyires-T\u00f3th', 'B\u00e1lint', '']]","extracted_entities":"[{'text': 'Self-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'Vision Transformers', 'label': 'Transformers'}, {'text': 'SSL', 'label': 'Few-shot Learning'}, {'text': 'depthwise separable\\nconvolutions', 'label': 'Embedding'}]","assigned_concept":"Transformers","matched_keyword":"Vision Transformers","similarity_score":0.7330732346}
{"id":2408.15993,"submitter":"Sungduk Yu","authors":"Sungduk Yu, Brian L. White, Anahita Bhiwandiwalla, Musashi Hinck,\n  Matthew Lyle Olson, Yaniv Gurwicz, Raanan Y. Rohekar, Tung Nguyen, Vasudev\n  Lal","title":"ClimDetect: A Benchmark Dataset for Climate Change Detection and\n  Attribution","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG physics.ao-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Detecting and attributing temperature increases driven by climate change is\ncrucial for understanding global warming and informing adaptation strategies.\nHowever, distinguishing human-induced climate signals from natural variability\nremains challenging for traditional detection and attribution (D&A) methods,\nwhich rely on identifying specific \"fingerprints\" -- spatial patterns expected\nto emerge from external forcings such as greenhouse gas emissions. Deep\nlearning offers promise in discerning these complex patterns within expansive\nspatial datasets, yet the lack of standardized protocols has hindered\nconsistent comparisons across studies.\n  To address this gap, we introduce ClimDetect, a standardized dataset\ncomprising 1.17M daily climate snapshots paired with target climate change\nindicator variables. The dataset is curated from both CMIP6 climate model\nsimulations and real-world observation-assimilated reanalysis datasets (ERA5,\nJRA-3Q, and MERRA-2), and is designed to enhance model accuracy in detecting\nclimate change signals. ClimDetect integrates various input and target\nvariables used in previous research, ensuring comparability and consistency\nacross studies. We also explore the application of vision transformers (ViT) to\nclimate data -- a novel approach that, to our knowledge, has not been attempted\nbefore for climate change detection tasks. Our open-access data serve as a\nbenchmark for advancing climate science by enabling end-to-end model\ndevelopment and evaluation. ClimDetect is publicly accessible via Hugging Face\ndataset repository at: https:\/\/huggingface.co\/datasets\/ClimDetect\/ClimDetect.\n","versions":"[{'version': 'v1', 'created': 'Wed, 28 Aug 2024 17:58:53 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 20:45:11 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Yu', 'Sungduk', ''], ['White', 'Brian L.', ''], ['Bhiwandiwalla', 'Anahita', ''], ['Hinck', 'Musashi', ''], ['Olson', 'Matthew Lyle', ''], ['Gurwicz', 'Yaniv', ''], ['Rohekar', 'Raanan Y.', ''], ['Nguyen', 'Tung', ''], ['Lal', 'Vasudev', '']]","extracted_entities":"[{'text': 'Deep\\nlearning', 'label': 'Open-source LLMs'}, {'text': 'vision transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"vision transformers","similarity_score":0.7330732346}
{"id":2410.03522,"submitter":"Songsong Xiong","authors":"Songsong Xiong, Hamidreza Kasaei","title":"HMT-Grasp: A Hybrid Mamba-Transformer Approach for Robot Grasping in\n  Cluttered Environments","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Robot grasping, whether handling isolated objects, cluttered items, or\nstacked objects, plays a critical role in industrial and service applications.\nHowever, current visual grasp detection methods based on Convolutional Neural\nNetworks (CNNs) and Vision Transformers (ViTs) often struggle to adapt to\ndiverse scenarios, as they tend to emphasize either local or global features\nexclusively, neglecting complementary cues. In this paper, we propose a novel\nhybrid Mamba-Transformer approach to address these challenges. Our method\nimproves robotic visual grasping by effectively capturing both global and local\ninformation through the integration of Vision Mamba and parallel\nconvolutional-transformer blocks. This hybrid architecture significantly\nimproves adaptability, precision, and flexibility across various robotic tasks.\nTo ensure a fair evaluation, we conducted extensive experiments on the Cornell,\nJacquard, and OCID-Grasp datasets, ranging from simple to complex scenarios.\nAdditionally, we performed both simulated and real-world robotic experiments.\nThe results demonstrate that our method not only surpasses state-of-the-art\ntechniques on standard grasping datasets but also delivers strong performance\nin both simulation and real-world robot applications.\n","versions":"[{'version': 'v1', 'created': 'Fri, 4 Oct 2024 15:43:01 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 17:26:26 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Xiong', 'Songsong', ''], ['Kasaei', 'Hamidreza', '']]","extracted_entities":"[{'text': 'Vision Transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Vision Transformers","similarity_score":0.7330732346}
{"id":2410.13981,"submitter":"Renpu Liu","authors":"Renpu Liu, Ruida Zhou, Cong Shen, Jing Yang","title":"On the Learn-to-Optimize Capabilities of Transformers in In-Context\n  Sparse Recovery","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  An intriguing property of the Transformer is its ability to perform\nin-context learning (ICL), where the Transformer can solve different inference\ntasks without parameter updating based on the contextual information provided\nby the corresponding input-output demonstration pairs. It has been\ntheoretically proved that ICL is enabled by the capability of Transformers to\nperform gradient-descent algorithms (Von Oswald et al., 2023a; Bai et al.,\n2024). This work takes a step further and shows that Transformers can perform\nlearning-to-optimize (L2O) algorithms. Specifically, for the ICL sparse\nrecovery (formulated as LASSO) tasks, we show that a K-layer Transformer can\nperform an L2O algorithm with a provable convergence rate linear in K. This\nprovides a new perspective explaining the superior ICL capability of\nTransformers, even with only a few layers, which cannot be achieved by the\nstandard gradient-descent algorithms. Moreover, unlike the conventional L2O\nalgorithms that require the measurement matrix involved in training to match\nthat in testing, the trained Transformer is able to solve sparse recovery\nproblems generated with different measurement matrices. Besides, Transformers\nas an L2O algorithm can leverage structural information embedded in the\ntraining tasks to accelerate its convergence during ICL, and generalize across\ndifferent lengths of demonstration pairs, where conventional L2O algorithms\ntypically struggle or fail. Such theoretical findings are supported by our\nexperimental results.\n","versions":"[{'version': 'v1', 'created': 'Thu, 17 Oct 2024 19:18:28 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 05:09:21 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Liu', 'Renpu', ''], ['Zhou', 'Ruida', ''], ['Shen', 'Cong', ''], ['Yang', 'Jing', '']]","extracted_entities":"[{'text': 'in-context learning', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'ICL', 'label': 'contextual Embedding'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2411.0639,"submitter":"Yutong Chen","authors":"Yutong Chen, Marko Mihajlovic, Xiyi Chen, Yiming Wang, Sergey Prokudin\n  and Siyu Tang","title":"SplatFormer: Point Transformer for Robust 3D Gaussian Splatting","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  3D Gaussian Splatting (3DGS) has recently transformed photorealistic\nreconstruction, achieving high visual fidelity and real-time performance.\nHowever, rendering quality significantly deteriorates when test views deviate\nfrom the camera angles used during training, posing a major challenge for\napplications in immersive free-viewpoint rendering and navigation. In this\nwork, we conduct a comprehensive evaluation of 3DGS and related novel view\nsynthesis methods under out-of-distribution (OOD) test camera scenarios. By\ncreating diverse test cases with synthetic and real-world datasets, we\ndemonstrate that most existing methods, including those incorporating various\nregularization techniques and data-driven priors, struggle to generalize\neffectively to OOD views. To address this limitation, we introduce SplatFormer,\nthe first point transformer model specifically designed to operate on Gaussian\nsplats. SplatFormer takes as input an initial 3DGS set optimized under limited\ntraining views and refines it in a single forward pass, effectively removing\npotential artifacts in OOD test views. To our knowledge, this is the first\nsuccessful application of point transformers directly on 3DGS sets, surpassing\nthe limitations of previous multi-scene training methods, which could handle\nonly a restricted number of input views during inference. Our model\nsignificantly improves rendering quality under extreme novel views, achieving\nstate-of-the-art performance in these challenging scenarios and outperforming\nvarious 3DGS regularization techniques, multi-scene models tailored for sparse\nview synthesis, and diffusion-based frameworks.\n","versions":"[{'version': 'v1', 'created': 'Sun, 10 Nov 2024 08:23:27 GMT'}, {'version': 'v2', 'created': 'Tue, 12 Nov 2024 06:41:21 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 08:37:42 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Chen', 'Yutong', ''], ['Mihajlovic', 'Marko', ''], ['Chen', 'Xiyi', ''], ['Wang', 'Yiming', ''], ['Prokudin', 'Sergey', ''], ['Tang', 'Siyu', '']]","extracted_entities":"[{'text': 'point transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"point transformers","similarity_score":0.7047149539}
{"id":2411.15958,"submitter":"Enea Monzio Compagnoni Mr.","authors":"Enea Monzio Compagnoni, Tianlin Liu, Rustem Islamov, Frank Norbert\n  Proske, Antonio Orvieto, Aurelien Lucchi","title":"Adaptive Methods through the Lens of SDEs: Theoretical Insights on the\n  Role of Noise","comments":"Accepted at ICLR 2025 (Poster); An earlier version, titled 'SDEs for\n  Adaptive Methods: The Role of Noise' and dated May 2024, is available on\n  OpenReview","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Despite the vast empirical evidence supporting the efficacy of adaptive\noptimization methods in deep learning, their theoretical understanding is far\nfrom complete. This work introduces novel SDEs for commonly used adaptive\noptimizers: SignSGD, RMSprop(W), and Adam(W). These SDEs offer a quantitatively\naccurate description of these optimizers and help illuminate an intricate\nrelationship between adaptivity, gradient noise, and curvature. Our novel\nanalysis of SignSGD highlights a noteworthy and precise contrast to SGD in\nterms of convergence speed, stationary distribution, and robustness to\nheavy-tail noise. We extend this analysis to AdamW and RMSpropW, for which we\nobserve that the role of noise is much more complex. Crucially, we support our\ntheoretical analysis with experimental evidence by verifying our insights: this\nincludes numerically integrating our SDEs using Euler-Maruyama discretization\non various neural network architectures such as MLPs, CNNs, ResNets, and\nTransformers. Our SDEs accurately track the behavior of the respective\noptimizers, especially when compared to previous SDEs derived for Adam and\nRMSprop. We believe our approach can provide valuable insights into best\ntraining practices and novel scaling rules.\n","versions":"[{'version': 'v1', 'created': 'Sun, 24 Nov 2024 19:07:31 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 23:28:01 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Compagnoni', 'Enea Monzio', ''], ['Liu', 'Tianlin', ''], ['Islamov', 'Rustem', ''], ['Proske', 'Frank Norbert', ''], ['Orvieto', 'Antonio', ''], ['Lucchi', 'Aurelien', '']]","extracted_entities":"[{'text': 'SignSGD', 'label': 'BERT'}, {'text': 'RMSprop', 'label': 'BERT'}, {'text': 'Adam(W)', 'label': 'ALBERT'}, {'text': 'SignSGD', 'label': 'ALBERT'}, {'text': 'AdamW', 'label': 'BERT'}, {'text': 'RMSpropW', 'label': 'BERT'}, {'text': 'ResNets', 'label': 'Transformers'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Adam', 'label': 'BERT'}, {'text': 'RMSprop', 'label': 'BERT'}, {'text': 'novel scaling rules', 'label': 'Scaling law'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2412.00776,"submitter":"Chongyang Zhao","authors":"Chongyang Zhao and Dong Gong","title":"Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation.\n","versions":"[{'version': 'v1', 'created': 'Sun, 1 Dec 2024 11:43:46 GMT'}, {'version': 'v2', 'created': 'Mon, 3 Feb 2025 09:31:32 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 02:19:22 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhao', 'Chongyang', ''], ['Gong', 'Dong', '']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Linear Transformers', 'label': 'Transformers'}, {'text': 'MCL', 'label': 'LLM'}, {'text': 'selectivity regularization', 'label': 'Fine-tuning'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'MCL', 'label': 'LLM'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2412.04532,"submitter":"Md Khairul Islam","authors":"Md. Khairul Islam, Judy Fox","title":"WinTSR: A Windowed Temporal Saliency Rescaling Method for Interpreting\n  Time Series Deep Learning Models","comments":"11 pages, 14 figures, GitHub\n  https:\/\/github.com\/khairulislam\/Timeseries-Explained","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Interpreting complex time series forecasting models is challenging due to the\ntemporal dependencies between time steps and the dynamic relevance of input\nfeatures over time. Existing interpretation methods are limited by focusing\nmostly on classification tasks, evaluating using custom baseline models instead\nof the latest time series models, using simple synthetic datasets, and\nrequiring training another model. We introduce a novel interpretation method,\n\\textit{Windowed Temporal Saliency Rescaling (WinTSR)} addressing these\nlimitations. WinTSR explicitly captures temporal dependencies among the past\ntime steps and efficiently scales the feature importance with this time\nimportance. We benchmark WinTSR against 10 recent interpretation techniques\nwith 5 state-of-the-art deep-learning models of different architectures,\nincluding a time series foundation model. We use 3 real-world datasets for both\ntime-series classification and regression. Our comprehensive analysis shows\nthat WinTSR significantly outperforms other local interpretation methods in\noverall performance. Finally, we provide a novel, open-source framework to\ninterpret the latest time series transformers and foundation models.\n","versions":"[{'version': 'v1', 'created': 'Thu, 5 Dec 2024 17:15:07 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 16:41:01 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 03:16:36 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Islam', 'Md. Khairul', ''], ['Fox', 'Judy', '']]","extracted_entities":"[{'text': 'time series foundation model', 'label': 'Foundation Model'}, {'text': 'open-source framework', 'label': 'Open-source LLMs'}, {'text': 'time series transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"time series transformers","similarity_score":0.7990843058}
{"id":2412.07752,"submitter":"Korbinian P\\\"oppel","authors":"Korbinian P\\\"oppel, Maximilian Beck, Sepp Hochreiter","title":"FlashRNN: I\/O-Aware Optimization of Traditional RNNs on modern hardware","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https:\/\/github.com\/NX-AI\/flashrnn\n","versions":"[{'version': 'v1', 'created': 'Tue, 10 Dec 2024 18:50:37 GMT'}, {'version': 'v2', 'created': 'Mon, 13 Jan 2025 17:34:22 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 11:14:49 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['P\u00f6ppel', 'Korbinian', ''], ['Beck', 'Maximilian', ''], ['Hochreiter', 'Sepp', '']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'sLSTM', 'label': 'Transformers'}, {'text': 'RNNs', 'label': 'AI model'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'RNNs', 'label': 'AI model'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2501.01423,"submitter":"Jingfeng Yao","authors":"Jingfeng Yao, Bin Yang and Xinggang Wang","title":"Reconstruction vs. Generation: Taming Optimization Dilemma in Latent\n  Diffusion Models","comments":"Models and codes are available at:\n  https:\/\/github.com\/hustvl\/LightningDiT","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Latent diffusion models with Transformer architectures excel at generating\nhigh-fidelity images. However, recent studies reveal an optimization dilemma in\nthis two-stage design: while increasing the per-token feature dimension in\nvisual tokenizers improves reconstruction quality, it requires substantially\nlarger diffusion models and more training iterations to achieve comparable\ngeneration performance. Consequently, existing systems often settle for\nsub-optimal solutions, either producing visual artifacts due to information\nloss within tokenizers or failing to converge fully due to expensive\ncomputation costs. We argue that this dilemma stems from the inherent\ndifficulty in learning unconstrained high-dimensional latent spaces. To address\nthis, we propose aligning the latent space with pre-trained vision foundation\nmodels when training the visual tokenizers. Our proposed VA-VAE (Vision\nfoundation model Aligned Variational AutoEncoder) significantly expands the\nreconstruction-generation frontier of latent diffusion models, enabling faster\nconvergence of Diffusion Transformers (DiT) in high-dimensional latent spaces.\nTo exploit the full potential of VA-VAE, we build an enhanced DiT baseline with\nimproved training strategies and architecture designs, termed LightningDiT. The\nintegrated system achieves state-of-the-art (SOTA) performance on ImageNet\n256x256 generation with an FID score of 1.35 while demonstrating remarkable\ntraining efficiency by reaching an FID score of 2.11 in just 64\nepochs--representing an over 21 times convergence speedup compared to the\noriginal DiT. Models and codes are available at:\nhttps:\/\/github.com\/hustvl\/LightningDiT.\n","versions":"[{'version': 'v1', 'created': 'Thu, 2 Jan 2025 18:59:40 GMT'}, {'version': 'v2', 'created': 'Mon, 6 Jan 2025 15:28:11 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 11:43:42 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Yao', 'Jingfeng', ''], ['Yang', 'Bin', ''], ['Wang', 'Xinggang', '']]","extracted_entities":"[{'text': 'Diffusion Transformers', 'label': 'Transformers'}, {'text': 'LightningDiT', 'label': 'LLM-based'}]","assigned_concept":"Transformers","matched_keyword":"Diffusion Transformers","similarity_score":0.5920959711}
{"id":2501.09096,"submitter":"Badhan Kumar Das","authors":"Badhan Kumar Das, Gengyan Zhao, Han Liu, Thomas J. Re, Dorin\n  Comaniciu, Eli Gibson, and Andreas Maier","title":"Self Pre-training with Adaptive Mask Autoencoders for Variable-Contrast\n  3D Medical Imaging","comments":"5 pages, ISBI 2025 accepted","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The Masked Autoencoder (MAE) has recently demonstrated effectiveness in\npre-training Vision Transformers (ViT) for analyzing natural images. By\nreconstructing complete images from partially masked inputs, the ViT encoder\ngathers contextual information to predict the missing regions. This capability\nto aggregate context is especially important in medical imaging, where\nanatomical structures are functionally and mechanically linked to surrounding\nregions. However, current methods do not consider variations in the number of\ninput images, which is typically the case in real-world Magnetic Resonance (MR)\nstudies. To address this limitation, we propose a 3D Adaptive Masked\nAutoencoders (AMAE) architecture that accommodates a variable number of 3D\ninput contrasts per subject. A magnetic resonance imaging (MRI) dataset of\n45,364 subjects was used for pretraining and a subset of 1648 training, 193\nvalidation and 215 test subjects were used for finetuning. The performance\ndemonstrates that self pre-training of this adaptive masked autoencoders can\nenhance the infarct segmentation performance by 2.8%-3.7% for ViT-based\nsegmentation models.\n","versions":"[{'version': 'v1', 'created': 'Wed, 15 Jan 2025 19:29:31 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 18:48:15 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Das', 'Badhan Kumar', ''], ['Zhao', 'Gengyan', ''], ['Liu', 'Han', ''], ['Re', 'Thomas J.', ''], ['Comaniciu', 'Dorin', ''], ['Gibson', 'Eli', ''], ['Maier', 'Andreas', '']]","extracted_entities":"[{'text': 'Vision Transformers', 'label': 'Transformers'}, {'text': 'finetuning', 'label': 'Fine-tuning'}]","assigned_concept":"Transformers","matched_keyword":"Vision Transformers","similarity_score":0.7330732346}
{"id":2501.13353,"submitter":"Aman Urumbekov","authors":"Aman Urumbekov, Zheng Chen","title":"Contrast: A Hybrid Architecture of Transformers and State Space Models\n  for Low-Level Vision","comments":"10 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Transformers have become increasingly popular for image super-resolution (SR)\ntasks due to their strong global context modeling capabilities. However, their\nquadratic computational complexity necessitates the use of window-based\nattention mechanisms, which restricts the receptive field and limits effective\ncontext expansion. Recently, the Mamba architecture has emerged as a promising\nalternative with linear computational complexity, allowing it to avoid window\nmechanisms and maintain a large receptive field. Nevertheless, Mamba faces\nchallenges in handling long-context dependencies when high pixel-level\nprecision is required, as in SR tasks. This is due to its hidden state\nmechanism, which can compress and store a substantial amount of context but\nonly in an approximate manner, leading to inaccuracies that transformers do not\nsuffer from. In this paper, we propose \\textbf{Contrast}, a hybrid SR model\nthat combines \\textbf{Con}volutional, \\textbf{Tra}nsformer, and \\textbf{St}ate\nSpace components, effectively blending the strengths of transformers and Mamba\nto address their individual limitations. By integrating transformer and state\nspace mechanisms, \\textbf{Contrast} compensates for the shortcomings of each\napproach, enhancing both global context modeling and pixel-level accuracy. We\ndemonstrate that combining these two architectures allows us to mitigate the\nproblems inherent in each, resulting in improved performance on image\nsuper-resolution tasks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 Jan 2025 03:34:14 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 22:07:50 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Urumbekov', 'Aman', ''], ['Chen', 'Zheng', '']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'window-based\\nattention mechanisms', 'label': 'Attention mechanism'}, {'text': 'transformers', 'label': 'Transformers'}, {'text': 'transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2501.13484,"submitter":"Yuxuan Yue","authors":"Zukang Xu, Yuxuan Yue, Xing Hu, Zhihang Yuan, Zixu Jiang, Zhixuan\n  Chen, Jiangyong Yu, Chen Xu, Sifan Zhou, Dawei Yang","title":"MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation\n  Methods","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Mamba is an efficient sequence model that rivals Transformers and\ndemonstrates significant potential as a foundational architecture for various\ntasks. Quantization is commonly used in neural networks to reduce model size\nand computational latency. However, applying quantization to Mamba remains\nunderexplored, and existing quantization methods, which have been effective for\nCNN and Transformer models, appear inadequate for Mamba models (e.g., Quarot\nsuffers a 21% accuracy drop on Vim-T$^\\dagger$ even under W8A8). We have\npioneered the exploration of this issue and identified several key challenges.\nFirst, significant outliers are present in gate projections, output\nprojections, and matrix multiplications. Second, Mamba's unique parallel scan\nfurther amplifies these outliers, leading to uneven and heavy-tailed data\ndistributions. Third, even with the application of the Hadamard transform, the\nvariance across channels in weights and activations still remains inconsistent.\nTo these ends, we propose MambaQuant, a post-training quantization (PTQ)\nframework consisting of: 1) Karhunen-Loeve Transformation (KLT) enhanced\nrotation, rendering the rotation matrix adaptable to diverse channel\ndistributions. 2) Smooth-Fused rotation, which equalizes channel variances and\ncan merge additional parameters into model weights. Experiments show that\nMambaQuant can quantize both weights and activations into 8-bit with less than\n1% accuracy loss for Mamba-based vision and language tasks. To the best of our\nknowledge, MambaQuant is the first comprehensive PTQ design for the Mamba\nfamily, paving the way for further advancements in its application.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 Jan 2025 08:57:33 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Feb 2025 08:05:38 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 06:49:47 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Xu', 'Zukang', ''], ['Yue', 'Yuxuan', ''], ['Hu', 'Xing', ''], ['Yuan', 'Zhihang', ''], ['Jiang', 'Zixu', ''], ['Chen', 'Zhixuan', ''], ['Yu', 'Jiangyong', ''], ['Xu', 'Chen', ''], ['Zhou', 'Sifan', ''], ['Yang', 'Dawei', '']]","extracted_entities":"[{'text': 'Mamba', 'label': 'Foundation Model'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Quantization', 'label': 'quantisation'}, {'text': 'quantization', 'label': 'quantisation'}, {'text': 'Mamba', 'label': 'Foundation Model'}, {'text': 'quantization', 'label': 'quantisation'}, {'text': 'Mamba', 'label': 'Foundation Model'}, {'text': 'Mamba', 'label': 'Foundation Model'}, {'text': 'MambaQuant', 'label': 'Foundation Model'}, {'text': 'quantization', 'label': 'quantisation'}, {'text': 'MambaQuant', 'label': 'Foundation Model'}, {'text': 'quantize', 'label': 'quantisation'}, {'text': 'MambaQuant', 'label': 'Foundation Model'}, {'text': 'Mamba', 'label': 'Foundation Model'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2501.19255,"submitter":"Mian Muhammad Naeem Abid","authors":"Mian Muhammad Naeem Abid, Nancy Mehta, Zongwei Wu, Radu Timofte","title":"ContextFormer: Redefining Efficiency in Semantic Segmentation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Semantic segmentation assigns labels to pixels in images, a critical yet\nchallenging task in computer vision. Convolutional methods, although capturing\nlocal dependencies well, struggle with long-range relationships. Vision\nTransformers (ViTs) excel in global context capture but are hindered by high\ncomputational demands, especially for high-resolution inputs. Most research\noptimizes the encoder architecture, leaving the bottleneck underexplored - a\nkey area for enhancing performance and efficiency. We propose ContextFormer, a\nhybrid framework leveraging the strengths of CNNs and ViTs in the bottleneck to\nbalance efficiency, accuracy, and robustness for real-time semantic\nsegmentation. The framework's efficiency is driven by three synergistic\nmodules: the Token Pyramid Extraction Module (TPEM) for hierarchical\nmulti-scale representation, the Transformer and Branched DepthwiseConv\n(Trans-BDC) block for dynamic scale-aware feature modeling, and the Feature\nMerging Module (FMM) for robust integration with enhanced spatial and\ncontextual consistency. Extensive experiments on ADE20K, Pascal Context,\nCityScapes, and COCO-Stuff datasets show ContextFormer significantly\noutperforms existing models, achieving state-of-the-art mIoU scores, setting a\nnew benchmark for efficiency and performance. The codes will be made publicly\navailable upon acceptance.\n","versions":"[{'version': 'v1', 'created': 'Fri, 31 Jan 2025 16:11:04 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 14:00:08 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Abid', 'Mian Muhammad Naeem', ''], ['Mehta', 'Nancy', ''], ['Wu', 'Zongwei', ''], ['Timofte', 'Radu', '']]","extracted_entities":"[{'text': 'Vision\\nTransformers', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ADE20K', 'label': 'Large Language Model'}, {'text': 'Pascal Context', 'label': 'Large Language Model'}]","assigned_concept":"Transformers","matched_keyword":"Vision\nTransformers","similarity_score":0.7330732346}
{"id":2502.18913,"submitter":"Jiaming Zhou","authors":"Jiaming Zhou, Yujie Guo, Shiwan Zhao, Haoqin Sun, Hui Wang, Jiabei He,\n  Aobo Kong, Shiyao Wang, Xi Yang, Yequan Wang, Yonghua Lin, Yong Qin","title":"CS-Dialogue: A 104-Hour Dataset of Spontaneous Mandarin-English\n  Code-Switching Dialogues for Speech Recognition","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.SD eess.AS","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Code-switching (CS), the alternation between two or more languages within a\nsingle conversation, presents significant challenges for automatic speech\nrecognition (ASR) systems. Existing Mandarin-English code-switching datasets\noften suffer from limitations in size, spontaneity, and the lack of full-length\ndialogue recordings with transcriptions, hindering the development of robust\nASR models for real-world conversational scenarios. This paper introduces\nCS-Dialogue, a novel large-scale Mandarin-English code-switching speech dataset\ncomprising 104 hours of spontaneous conversations from 200 speakers. Unlike\nprevious datasets, CS-Dialogue provides full-length dialogue recordings with\ncomplete transcriptions, capturing naturalistic code-switching patterns in\ncontinuous speech. We describe the data collection and annotation processes,\npresent detailed statistics of the dataset, and establish benchmark ASR\nperformance using state-of-the-art models. Our experiments, using Transformer,\nConformer, and Branchformer, demonstrate the challenges of code-switching ASR,\nand show that existing pre-trained models such as Whisper still have the space\nto improve. The CS-Dialogue dataset will be made freely available for all\nacademic purposes.\n","versions":"[{'version': 'v1', 'created': 'Wed, 26 Feb 2025 07:59:55 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 03:06:01 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Zhou', 'Jiaming', ''], ['Guo', 'Yujie', ''], ['Zhao', 'Shiwan', ''], ['Sun', 'Haoqin', ''], ['Wang', 'Hui', ''], ['He', 'Jiabei', ''], ['Kong', 'Aobo', ''], ['Wang', 'Shiyao', ''], ['Yang', 'Xi', ''], ['Wang', 'Yequan', ''], ['Lin', 'Yonghua', ''], ['Qin', 'Yong', '']]","extracted_entities":"[{'text': 'CS-Dialogue', 'label': 'Large Language Model'}, {'text': 'CS-Dialogue', 'label': 'Large Language Model'}, {'text': 'Transformer', 'label': 'Transformers'}, {'text': 'Conformer', 'label': 'Transformers'}, {'text': 'CS-Dialogue', 'label': 'Large Language Model'}]","assigned_concept":"Transformers","matched_keyword":"Transformer","similarity_score":0.7812911272}
{"id":2503.06368,"submitter":"Leonardo Scabini","authors":"Leonardo Scabini, Kallil M. Zielinski, Emir Konuk, Ricardo T. Fares,\n  Lucas C. Ribas, Kevin Smith, and Odemir M. Bruno","title":"VORTEX: Challenging CNNs at Texture Recognition by using Vision\n  Transformers with Orderless and Randomized Token Encodings","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Texture recognition has recently been dominated by ImageNet-pre-trained deep\nConvolutional Neural Networks (CNNs), with specialized modifications and\nfeature engineering required to achieve state-of-the-art (SOTA) performance.\nHowever, although Vision Transformers (ViTs) were introduced a few years ago,\nlittle is known about their texture recognition ability. Therefore, in this\nwork, we introduce VORTEX (ViTs with Orderless and Randomized Token Encodings\nfor Texture Recognition), a novel method that enables the effective use of ViTs\nfor texture analysis. VORTEX extracts multi-depth token embeddings from\npre-trained ViT backbones and employs a lightweight module to aggregate\nhierarchical features and perform orderless encoding, obtaining a better image\nrepresentation for texture recognition tasks. This approach allows seamless\nintegration with any ViT with the common transformer architecture. Moreover, no\nfine-tuning of the backbone is performed, since they are used only as frozen\nfeature extractors, and the features are fed to a linear SVM. We evaluate\nVORTEX on nine diverse texture datasets, demonstrating its ability to achieve\nor surpass SOTA performance in a variety of texture analysis scenarios. By\nbridging the gap between texture recognition with CNNs and transformer-based\narchitectures, VORTEX paves the way for adopting emerging transformer\nfoundation models. Furthermore, VORTEX demonstrates robust computational\nefficiency when coupled with ViT backbones compared to CNNs with similar costs.\nThe method implementation and experimental scripts are publicly available in\nour online repository.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 00:36:02 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Scabini', 'Leonardo', ''], ['Zielinski', 'Kallil M.', ''], ['Konuk', 'Emir', ''], ['Fares', 'Ricardo T.', ''], ['Ribas', 'Lucas C.', ''], ['Smith', 'Kevin', ''], ['Bruno', 'Odemir M.', '']]","extracted_entities":"[{'text': 'Vision Transformers', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'multi-depth token embeddings', 'label': 'Embedding'}, {'text': 'ViT', 'label': 'Transformers'}, {'text': 'ViT', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Vision Transformers","similarity_score":0.7330732346}
{"id":2503.06369,"submitter":"Sahar Dastani","authors":"Sahar Dastani, Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, David\n  Osowiechi, Gustavo Adolfo Vargas Hakim, Farzad Beizaee, Milad\n  Cheraghalikhani, Arnab Kumar Mondal, Herve Lombaert, Christian Desrosiers","title":"Spectral State Space Model for Rotation-Invariant Visual Representation\n  Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  State Space Models (SSMs) have recently emerged as an alternative to Vision\nTransformers (ViTs) due to their unique ability of modeling global\nrelationships with linear complexity. SSMs are specifically designed to capture\nspatially proximate relationships of image patches. However, they fail to\nidentify relationships between conceptually related yet not adjacent patches.\nThis limitation arises from the non-causal nature of image data, which lacks\ninherent directional relationships. Additionally, current vision-based SSMs are\nhighly sensitive to transformations such as rotation. Their predefined scanning\ndirections depend on the original image orientation, which can cause the model\nto produce inconsistent patch-processing sequences after rotation. To address\nthese limitations, we introduce Spectral VMamba, a novel approach that\neffectively captures the global structure within an image by leveraging\nspectral information derived from the graph Laplacian of image patches. Through\nspectral decomposition, our approach encodes patch relationships independently\nof image orientation, achieving rotation invariance with the aid of our\nRotational Feature Normalizer (RFN) module. Our experiments on classification\ntasks show that Spectral VMamba outperforms the leading SSM models in vision,\nsuch as VMamba, while maintaining invariance to rotations and a providing a\nsimilar runtime efficiency.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 00:37:43 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 02:10:35 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Dastani', 'Sahar', ''], ['Bahri', 'Ali', ''], ['Yazdanpanah', 'Moslem', ''], ['Noori', 'Mehrdad', ''], ['Osowiechi', 'David', ''], ['Hakim', 'Gustavo Adolfo Vargas', ''], ['Beizaee', 'Farzad', ''], ['Cheraghalikhani', 'Milad', ''], ['Mondal', 'Arnab Kumar', ''], ['Lombaert', 'Herve', ''], ['Desrosiers', 'Christian', '']]","extracted_entities":"[{'text': 'Vision\\nTransformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Vision\nTransformers","similarity_score":0.7330732346}
{"id":2503.06537,"submitter":"Xiaoyang Liu","authors":"Xiaoyang Liu, Yuquan Wang, Zheng Chen, Jiezhang Cao, He Zhang, Yulun\n  Zhang and Xiaokang Yang","title":"One-Step Diffusion Model for Image Motion-Deblurring","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Currently, methods for single-image deblurring based on CNNs and transformers\nhave demonstrated promising performance. However, these methods often suffer\nfrom perceptual limitations, poor generalization ability, and struggle with\nheavy or complex blur. While diffusion-based methods can partially address\nthese shortcomings, their multi-step denoising process limits their practical\nusage. In this paper, we conduct an in-depth exploration of diffusion models in\ndeblurring and propose a one-step diffusion model for deblurring (OSDD), a\nnovel framework that reduces the denoising process to a single step,\nsignificantly improving inference efficiency while maintaining high fidelity.\nTo tackle fidelity loss in diffusion models, we introduce an enhanced\nvariational autoencoder (eVAE), which improves structural restoration.\nAdditionally, we construct a high-quality synthetic deblurring dataset to\nmitigate perceptual collapse and design a dynamic dual-adapter (DDA) to enhance\nperceptual quality while preserving fidelity. Extensive experiments demonstrate\nthat our method achieves strong performance on both full and no-reference\nmetrics. Our code and pre-trained model will be publicly available at\nhttps:\/\/github.com\/xyLiu339\/OSDD.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 09:39:57 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Liu', 'Xiaoyang', ''], ['Wang', 'Yuquan', ''], ['Chen', 'Zheng', ''], ['Cao', 'Jiezhang', ''], ['Zhang', 'He', ''], ['Zhang', 'Yulun', ''], ['Yang', 'Xiaokang', '']]","extracted_entities":"[{'text': 'transformers', 'label': 'Transformers'}, {'text': 'OSDD', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"transformers","similarity_score":1.0}
{"id":2503.06625,"submitter":"Chaocan Xue","authors":"Chaocan Xue, Bineng Zhong, Qihua Liang, Yaozong Zheng, Ning Li,\n  Yuanliang Xue, Shuxiang Song","title":"Similarity-Guided Layer-Adaptive Vision Transformer for UAV Tracking","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Vision transformers (ViTs) have emerged as a popular backbone for visual\ntracking. However, complete ViT architectures are too cumbersome to deploy for\nunmanned aerial vehicle (UAV) tracking which extremely emphasizes efficiency.\nIn this study, we discover that many layers within lightweight ViT-based\ntrackers tend to learn relatively redundant and repetitive target\nrepresentations. Based on this observation, we propose a similarity-guided\nlayer adaptation approach to optimize the structure of ViTs. Our approach\ndynamically disables a large number of representation-similar layers and\nselectively retains only a single optimal layer among them, aiming to achieve a\nbetter accuracy-speed trade-off. By incorporating this approach into existing\nViTs, we tailor previously complete ViT architectures into an efficient\nsimilarity-guided layer-adaptive framework, namely SGLATrack, for real-time UAV\ntracking. Extensive experiments on six tracking benchmarks verify the\neffectiveness of the proposed approach, and show that our SGLATrack achieves a\nstate-of-the-art real-time speed while maintaining competitive tracking\nprecision. Codes and models are available at\nhttps:\/\/github.com\/GXNU-ZhongLab\/SGLATrack.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 14:02:30 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Xue', 'Chaocan', ''], ['Zhong', 'Bineng', ''], ['Liang', 'Qihua', ''], ['Zheng', 'Yaozong', ''], ['Li', 'Ning', ''], ['Xue', 'Yuanliang', ''], ['Song', 'Shuxiang', '']]","extracted_entities":"[{'text': 'Vision transformers', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ViT', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ViT', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Vision transformers","similarity_score":0.7330732346}
{"id":2404.0715,"submitter":"Cristiano Capone","authors":"Cristiano Capone, Luca Falorsi","title":"Adaptive behavior with stable synapses","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"q-bio.NC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Behavioral changes in animals and humans, as a consequence of an error or a\nverbal instruction, can be extremely rapid. Improvement in behavioral\nperformances are usually associated in machine learning and reinforcement\nlearning to synaptic plasticity, and, in general, to changes and optimization\nof network parameters. However, such rapid changes are not coherent with the\ntimescales of synaptic plasticity, suggesting that the mechanism responsible\nfor that could be a dynamical network reconfiguration. In the last few years,\nsimilar capabilities have been observed in transformers, foundational\narchitecture in the field of machine learning that are widely used in\napplications such as natural language and image processing. Transformers are\ncapable of in-context learning, the ability to adapt and acquire new\ninformation dynamically within the context of the task or environment they are\ncurrently engaged in, without the need for significant changes to their\nunderlying parameters. Building upon the notion of something unique within\ntransformers enabling the emergence of this property, we claim that it could\nalso be supported by input segregation and dendritic amplification, features\nextensively observed in biological networks. We propose an architecture\ncomposed of gain-modulated recurrent networks that excels at in-context\nlearning, showing abilities inaccessible to standard networks.\n","versions":"[{'version': 'v1', 'created': 'Wed, 10 Apr 2024 16:33:55 GMT'}, {'version': 'v2', 'created': 'Tue, 28 May 2024 09:48:40 GMT'}, {'version': 'v3', 'created': 'Wed, 8 Jan 2025 14:42:43 GMT'}, {'version': 'v4', 'created': 'Thu, 9 Jan 2025 13:54:35 GMT'}, {'version': 'v5', 'created': 'Tue, 11 Mar 2025 11:03:41 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Capone', 'Cristiano', ''], ['Falorsi', 'Luca', '']]","extracted_entities":"[{'text': 'transformers', 'label': 'Foundation Model'}, {'text': 'Transformers', 'label': 'Foundation Model'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}, {'text': 'transformers', 'label': 'Foundation Model'}, {'text': 'in-context\\nlearning', 'label': 'contextual Embedding'}]","assigned_concept":"contextual Embedding","matched_keyword":"in-context learning","similarity_score":0.6167421341}
{"id":2406.14678,"submitter":"Pamela Riviere","authors":"Pamela D. Rivi\\`ere (1), Anne L. Beatty-Mart\\'inez (1) and Sean Trott\n  (1 and 2) ((1) Department of Cognitive Science UC San Diego, (2)\n  Computational Social Science UC San Diego)","title":"Evaluating Contextualized Representations of (Spanish) Ambiguous Words:\n  A New Lexical Resource and Empirical Analysis","comments":"17 pages, 12 figures, accepted at NAACL 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Lexical ambiguity -- where a single wordform takes on distinct,\ncontext-dependent meanings -- serves as a useful tool to compare across\ndifferent language models' (LMs') ability to form distinct, contextualized\nrepresentations of the same stimulus. Few studies have systematically compared\nLMs' contextualized word embeddings for languages beyond English. Here, we\nevaluate semantic representations of Spanish ambiguous nouns in context in a\nsuite of Spanish-language monolingual and multilingual BERT-based models. We\ndevelop a novel dataset of minimal-pair sentences evoking the same or different\nsense for a target ambiguous noun. In a pre-registered study, we collect\ncontextualized human relatedness judgments for each sentence pair. We find that\nvarious BERT-based LMs' contextualized semantic representations capture some\nvariance in human judgments but fall short of the human benchmark. In\nexploratory work, we find that performance scales with model size. We also\nidentify stereotyped trajectories of target noun disambiguation as a proportion\nof traversal through a given LM family's architecture, which we partially\nreplicate in English. We contribute (1) a dataset of controlled, Spanish\nsentence stimuli with human relatedness norms, and (2) to our evolving\nunderstanding of the impact that LM specification (architectures, training\nprotocols) exerts on contextualized embeddings.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Jun 2024 18:58:11 GMT'}, {'version': 'v2', 'created': 'Thu, 17 Oct 2024 19:06:26 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 19:31:41 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Rivi\u00e8re', 'Pamela D.', '', '1 and 2'], ['Beatty-Mart\u00ednez', 'Anne L.', '', '1 and 2'], ['Trott', 'Sean', '', '1 and 2']]","extracted_entities":"[{'text': 'contextualized word embeddings', 'label': 'contextual Embedding'}, {'text': 'BERT-based', 'label': 'BERT'}, {'text': 'contextualized semantic representations', 'label': 'contextual Embedding'}, {'text': 'contextualized embeddings', 'label': 'contextual Embedding'}]","assigned_concept":"contextual Embedding","matched_keyword":"contextualized embeddings","similarity_score":0.9548683167}
{"id":2412.08014,"submitter":"Yun Xing","authors":"Yun Xing, Nhat Chung, Jie Zhang, Yue Cao, Ivor Tsang, Yang Liu, Lei\n  Ma, Qing Guo","title":"MAGIC: Mastering Physical Adversarial Generation in Context through\n  Collaborative LLM Agents","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Physical adversarial attacks in driving scenarios can expose critical\nvulnerabilities in visual perception models. However, developing such attacks\nremains challenging due to diverse real-world environments and the requirement\nfor maintaining visual naturality. Building upon this challenge, we reformulate\nphysical adversarial attacks as a one-shot patch generation problem. Our\napproach generates adversarial patches through a deep generative model that\nconsiders the specific scene context, enabling direct physical deployment in\nmatching environments. The primary challenge lies in simultaneously achieving\ntwo objectives: generating adversarial patches that effectively mislead object\ndetection systems while determining contextually appropriate deployment within\nthe scene. We propose MAGIC (Mastering Physical Adversarial Generation In\nContext), a novel framework powered by multi-modal LLM agents to address these\nchallenges. MAGIC automatically understands scene context and generates\nadversarial patch through the synergistic interaction of language and vision\ncapabilities. In particular, MAGIC orchestrates three specialized LLM agents:\nThe adv-patch generation agent (GAgent) masters the creation of deceptive\npatches through strategic prompt engineering for text-to-image models. The\nadv-patch deployment agent (DAgent) ensures contextual coherence by determining\noptimal deployment strategies based on scene understanding. The\nself-examination agent (EAgent) completes this trilogy by providing critical\noversight and iterative refinement of both processes. We validate our method on\nboth digital and physical levels, i.e., nuImage and manually captured\nreal-world scenes, where both statistical and visual results prove that our\nMAGIC is powerful and effective for attacking widely applied object detection\nsystems, i.e., YOLO and DETR series.\n","versions":"[{'version': 'v1', 'created': 'Wed, 11 Dec 2024 01:41:19 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 07:15:54 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Xing', 'Yun', ''], ['Chung', 'Nhat', ''], ['Zhang', 'Jie', ''], ['Cao', 'Yue', ''], ['Tsang', 'Ivor', ''], ['Liu', 'Yang', ''], ['Ma', 'Lei', ''], ['Guo', 'Qing', '']]","extracted_entities":"[{'text': 'MAGIC', 'label': 'LLM'}, {'text': 'scene context', 'label': 'contextual Embedding'}, {'text': 'strategic prompt engineering', 'label': 'Prompting'}]","assigned_concept":"contextual Embedding","matched_keyword":"scene context","similarity_score":0.5142388344}
{"id":2502.15996,"submitter":"Aditya Kumar","authors":"Aditya Kumar, Simon Rauch, Mario Cypko and Oliver Amft","title":"Med-gte-hybrid: A contextual embedding transformer model for extracting\n  actionable information from clinical texts","comments":"22 pages, 4 figures, 2 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We introduce a novel contextual embedding model med-gte-hybrid that was\nderived from the gte-large sentence transformer to extract information from\nunstructured clinical narratives. Our model tuning strategy for med-gte-hybrid\ncombines contrastive learning and a denoising autoencoder. To evaluate the\nperformance of med-gte-hybrid, we investigate several clinical prediction tasks\nin large patient cohorts extracted from the MIMIC-IV dataset, including Chronic\nKidney Disease (CKD) patient prognosis, estimated glomerular filtration rate\n(eGFR) prediction, and patient mortality prediction. Furthermore, we\ndemonstrate that the med-gte-hybrid model improves patient stratification,\nclustering, and text retrieval, thus outperforms current state-of-the-art\nmodels on the Massive Text Embedding Benchmark (MTEB). While some of our\nevaluations focus on CKD, our hybrid tuning of sentence transformers could be\ntransferred to other medical domains and has the potential to improve clinical\ndecision-making and personalised treatment pathways in various healthcare\napplications.\n","versions":"[{'version': 'v1', 'created': 'Fri, 21 Feb 2025 23:17:31 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 16:17:01 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Kumar', 'Aditya', ''], ['Rauch', 'Simon', ''], ['Cypko', 'Mario', ''], ['Amft', 'Oliver', '']]","extracted_entities":"[{'text': 'contrastive learning', 'label': 'Few-shot Learning'}, {'text': 'Massive Text Embedding Benchmark (MTEB)', 'label': 'contextual Embedding'}]","assigned_concept":"contextual Embedding","matched_keyword":"Massive Text Embedding Benchmark (MTEB)","similarity_score":0.512490809}
{"id":2503.05631,"submitter":"Aaditya K Singh","authors":"Aaditya K. Singh, Ted Moskovitz, Sara Dragutinovic, Felix Hill,\n  Stephanie C.Y. Chan, Andrew M. Saxe","title":"Strategy Coopetition Explains the Emergence and Transience of In-Context\n  Learning","comments":"20 pages, 18 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In-context learning (ICL) is a powerful ability that emerges in transformer\nmodels, enabling them to learn from context without weight updates. Recent work\nhas established emergent ICL as a transient phenomenon that can sometimes\ndisappear after long training times. In this work, we sought a mechanistic\nunderstanding of these transient dynamics. Firstly, we find that, after the\ndisappearance of ICL, the asymptotic strategy is a remarkable hybrid between\nin-weights and in-context learning, which we term \"context-constrained\nin-weights learning\" (CIWL). CIWL is in competition with ICL, and eventually\nreplaces it as the dominant strategy of the model (thus leading to ICL\ntransience). However, we also find that the two competing strategies actually\nshare sub-circuits, which gives rise to cooperative dynamics as well. For\nexample, in our setup, ICL is unable to emerge quickly on its own, and can only\nbe enabled through the simultaneous slow development of asymptotic CIWL. CIWL\nthus both cooperates and competes with ICL, a phenomenon we term \"strategy\ncoopetition.\" We propose a minimal mathematical model that reproduces these key\ndynamics and interactions. Informed by this model, we were able to identify a\nsetup where ICL is truly emergent and persistent.\n","versions":"[{'version': 'v1', 'created': 'Fri, 7 Mar 2025 17:54:05 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 07:13:09 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Singh', 'Aaditya K.', ''], ['Moskovitz', 'Ted', ''], ['Dragutinovic', 'Sara', ''], ['Hill', 'Felix', ''], ['Chan', 'Stephanie C. Y.', ''], ['Saxe', 'Andrew M.', '']]","extracted_entities":"[{'text': 'In-context learning', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}]","assigned_concept":"contextual Embedding","matched_keyword":"In-context learning","similarity_score":0.6167421341}
{"id":2311.03254,"submitter":"Somnath Pradhan Dr.","authors":"Somnath Pradhan and Serdar Y\\\"uksel","title":"Controlled Diffusions under Full, Partial and Decentralized Information:\n  Existence of Optimal Policies and Discrete-Time Approximations","comments":"27","journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present existence and discrete-time approximation results on optimal\ncontrol policies for continuous-time stochastic control problems under a\nvariety of information structures. These include fully observed models,\npartially observed models and multi-agent models with decentralized information\nstructures. While there exist comprehensive existence and approximations\nresults for the fully observed setup in the literature, few prior research\nexists on discrete-time approximation results for partially observed models.\nFor decentralized models, even existence results have not received much\nattention except for specialized models and approximation has been an open\nproblem. Our existence and approximations results lead to the applicability of\nwell-established partially observed Markov decision processes and the\nrelatively more mature theory of discrete-time decentralized stochastic control\nto be applicable for computing near optimal solutions for continuous-time\nstochastic control.\n","versions":"[{'version': 'v1', 'created': 'Mon, 6 Nov 2023 16:40:31 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 12:11:12 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Pradhan', 'Somnath', ''], ['Y\u00fcksel', 'Serdar', '']]","extracted_entities":"[{'text': 'fully observed models', 'label': 'AI model'}, {'text': 'partially observed models', 'label': 'AI model'}, {'text': 'multi-agent models', 'label': 'AI model'}, {'text': 'partially observed models', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"multi-agent models","similarity_score":0.6083590984}
{"id":2403.0142,"submitter":"Yang Xu","authors":"Yang Xu, Yihong Gu, Cong Fang","title":"The Implicit Bias of Heterogeneity towards Invariance: A Study of\n  Multi-Environment Matrix Sensing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG math.OC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Models are expected to engage in invariance learning, which involves\ndistinguishing the core relations that remain consistent across varying\nenvironments to ensure the predictions are safe, robust and fair. While\nexisting works consider specific algorithms to realize invariance learning, we\nshow that model has the potential to learn invariance through standard training\nprocedures. In other words, this paper studies the implicit bias of Stochastic\nGradient Descent (SGD) over heterogeneous data and shows that the implicit bias\ndrives the model learning towards an invariant solution. We call the phenomenon\nthe implicit invariance learning. Specifically, we theoretically investigate\nthe multi-environment low-rank matrix sensing problem where in each\nenvironment, the signal comprises (i) a lower-rank invariant part shared across\nall environments; and (ii) a significantly varying environment-dependent\nspurious component. The key insight is, through simply employing the large step\nsize large-batch SGD sequentially in each environment without any explicit\nregularization, the oscillation caused by heterogeneity can provably prevent\nmodel learning spurious signals. The model reaches the invariant solution after\ncertain iterations. In contrast, model learned using pooled SGD over all data\nwould simultaneously learn both the invariant and spurious signals. Overall, we\nunveil another implicit bias that is a result of the symbiosis between the\nheterogeneity of data and modern algorithms, which is, to the best of our\nknowledge, first in the literature.\n","versions":"[{'version': 'v1', 'created': 'Sun, 3 Mar 2024 07:38:24 GMT'}, {'version': 'v2', 'created': 'Sat, 16 Nov 2024 04:49:06 GMT'}, {'version': 'v3', 'created': 'Tue, 19 Nov 2024 06:10:32 GMT'}, {'version': 'v4', 'created': 'Sun, 9 Mar 2025 06:47:55 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Xu', 'Yang', ''], ['Gu', 'Yihong', ''], ['Fang', 'Cong', '']]","extracted_entities":"[{'text': 'invariance learning', 'label': 'Few-shot Learning'}, {'text': 'invariance learning', 'label': 'Zero-shot Learning'}, {'text': 'model', 'label': 'AI model'}, {'text': 'implicit bias', 'label': 'Model Bias and Fairness'}, {'text': 'implicit bias', 'label': 'Model Bias and Fairness'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'invariance learning', 'label': 'Zero-shot Learning'}, {'text': 'model', 'label': 'AI model'}, {'text': 'model', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"model","similarity_score":0.6292717457}
{"id":2408.12577,"submitter":"Joseph Chow","authors":"Xiyuan Ren, Joseph Y. J. Chow, Venktesh Pandey, Linfei Yuan","title":"A nested nonparametric logit model for microtransit revenue management\n  supplemented with citywide synthetic data","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"econ.EM","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  As an IT-enabled multi-passenger mobility service, microtransit can improve\naccessibility, reduce congestion, and enhance flexibility. However, its\nheterogeneous impacts across travelers necessitate better tools for\nmicrotransit forecasting and revenue management, especially when actual usage\ndata are limited. We propose a nested nonparametric model for joint travel mode\nand ride pass subscription choice, estimated using marginal subscription data\nand synthetic populations. The model improves microtransit choice modeling by\n(1) leveraging citywide synthetic data for greater spatiotemporal granularity,\n(2) employing an agent-based estimation approach to capture heterogeneous user\npreferences, and (3) integrating mode choice parameters into subscription\nchoice modeling. We apply our methodology to a case study in Arlington, TX,\nusing synthetic data from Replica Inc. and microtransit data from Via. Our\nmodel accurately predicts the number of subscribers in the upper branch and\nachieves a high McFadden R2 in the lower branch (0.603 for weekday trips and\n0.576 for weekend trips), while also retrieving interpretable elasticities and\nconsumer surplus. We further integrate the model into a simulation-based\nframework for microtransit revenue management. For the ride pass pricing\npolicy, our simulation results show that reducing the price of the weekly pass\n($25 -> $18.9) and monthly pass ($80 -> $71.5) would surprisingly increase\ntotal revenue by $127 per day. For the subsidy policy, our simulation results\nshow that a 100% fare discount would reduce 61 car trips to AT&T Stadium for a\ngame event, and increase 82 microtransit trips to Medical City Arlington, but\nrequire subsidies of $533 per event and $483 per day, respectively.\n","versions":"[{'version': 'v1', 'created': 'Thu, 22 Aug 2024 17:43:04 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 18:25:08 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Ren', 'Xiyuan', ''], ['Chow', 'Joseph Y. J.', ''], ['Pandey', 'Venktesh', ''], ['Yuan', 'Linfei', '']]","extracted_entities":"[{'text': 'Replica Inc.', 'label': 'Open-source LLMs'}, {'text': 'Via', 'label': 'Open-source LLMs'}, {'text': 'model', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"model","similarity_score":0.6292717457}
{"id":2410.13985,"submitter":"Aditya Narendra","authors":"Aditya Narendra, Maria Dainotti, Milind Sarkar, Aleksander Lenart,\n  Malgorzata Bogdan, Agnieszka Pollo, Bing Zhang, Aleksandra Rabeda, Vahe\n  Petrosian, and Iwasaki Kazunari","title":"GRB Redshift Estimation using Machine Learning and the Associated\n  Web-App","comments":"20 Figures, 3 tables. Submitted for publication in Astronomy and\n  Astrophysics journal","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.HE astro-ph.IM","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Context. Gamma-ray bursts (GRBs), observed at redshifts as high as 9.4, could\nserve as valuable probes for investigating the distant Universe. However, this\nnecessitates an increase in the number of GRBs with determined redshifts, as\ncurrently, only 12% of GRBs have known redshifts due to observational biases.\nAims. We aim to address the shortage of GRBs with measured redshifts, enabling\nus to fully realize their potential as valuable cosmological probes Methods.\nFollowing Dainotti et al. (2024c), we have taken a second step to overcome this\nissue by adding 30 more GRBs to our ensemble supervised machine learning\ntraining sample, an increase of 20%, which will help us obtain better redshift\nestimates. In addition, we have built a freely accessible and user-friendly web\napp that infers the redshift of long GRBs (LGRBs) with plateau emission using\nour machine learning model. The web app is the first of its kind for such a\nstudy and will allow the community to obtain redshift estimates by entering the\nGRB parameters in the app. Results. Through our machine learning model, we have\nsuccessfully estimated redshifts for 276 LGRBs using X-ray afterglow parameters\ndetected by the Neil Gehrels Swift Observatory and increased the sample of\nLGRBs with known redshifts by 110%. We also perform Monte Carlo simulations to\ndemonstrate the future applicability of this research. Conclusions. The results\npresented in this research will enable the community to increase the sample of\nGRBs with known redshift estimates. This can help address many outstanding\nissues, such as GRB formation rate, luminosity function, and the true nature of\nlow-luminosity GRBs, and enable the application of GRBs as standard candles\n","versions":"[{'version': 'v1', 'created': 'Thu, 17 Oct 2024 19:30:58 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 04:21:03 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Narendra', 'Aditya', ''], ['Dainotti', 'Maria', ''], ['Sarkar', 'Milind', ''], ['Lenart', 'Aleksander', ''], ['Bogdan', 'Malgorzata', ''], ['Pollo', 'Agnieszka', ''], ['Zhang', 'Bing', ''], ['Rabeda', 'Aleksandra', ''], ['Petrosian', 'Vahe', ''], ['Kazunari', 'Iwasaki', '']]","extracted_entities":"[{'text': 'machine learning model', 'label': 'AI model'}, {'text': 'machine learning model', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"machine learning model","similarity_score":0.7232564688}
{"id":2410.22369,"submitter":"Jhordan Silveira De Borba","authors":"Jhordan Silveira Borba and Sebastian Gon\\c{c}alves and Celia Anteneodo","title":"Inequality in a model of capitalist economy","comments":"16 pages, 10 figures","journal-ref":"Physica A: Statistical Mechanics and its Applications 664 (2025)\n  130457","doi":"10.1016\/j.physa.2025.130457","report-no":null,"categories":"physics.soc-ph econ.GN q-fin.EC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We analyze inequality aspects of the agent-based model of capitalist economy\nnamed it Social Architecture of Capitalism that has been introduced by Ian\nWright. The model contemplates two main types of agents, workers and\ncapitalists, which can also be unemployed. Starting from a state where all\nagents are unemployed and possess the same initial wealth, the system, governed\nby a few simple rules, quickly self-organizes into two classes. After a\ntransient, the model reproduces the statistics of many relevant macroeconomic\nquantities of real economies worldwide, notably the two regimes of the\ndistributions of wealth and income. We perform extensive simulations testing\nthe role of the model parameters (number of agents, total wealth, and salary\nrange) on the resulting distribution of wealth and income, the social\ndistribution of agents, and other stylized facts of the dynamics. Our main\nfinding is that, according to the model, in an economy where total wealth is\nconserved and with a fixed average wage, the increase in wealth per capita\ncomes with more inequality.\n","versions":"[{'version': 'v1', 'created': 'Mon, 28 Oct 2024 21:28:35 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 14:52:24 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Borba', 'Jhordan Silveira', ''], ['Gon\u00e7alves', 'Sebastian', ''], ['Anteneodo', 'Celia', '']]","extracted_entities":"[{'text': 'model', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"model","similarity_score":0.6292717457}
{"id":2502.171,"submitter":"Shao Xinyu","authors":"Yinchuan Li, Xinyu Shao, Jianping Zhang, Haozhi Wang, Leo Maxime\n  Brunswic, Kaiwen Zhou, Jiqian Dong, Kaiyang Guo, Xiu Li, Zhitang Chen, Jun\n  Wang, Jianye Hao","title":"Generative Models in Decision Making: A Survey","comments":"Project\n  page:https:\/\/github.com\/xyshao23\/Awesome-Generative-Models-for-Decision-Making-Taxonomy","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In recent years, the exceptional performance of generative models in\ngenerative tasks has sparked significant interest in their integration into\ndecision-making processes. Due to their ability to handle complex data\ndistributions and their strong model capacity, generative models can be\neffectively incorporated into decision-making systems by generating\ntrajectories that guide agents toward high-reward state-action regions or\nintermediate sub-goals. This paper presents a comprehensive review of the\napplication of generative models in decision-making tasks. We classify seven\nfundamental types of generative models: energy-based models, generative\nadversarial networks, variational autoencoders, normalizing flows, diffusion\nmodels, generative flow networks, and autoregressive models. Regarding their\napplications, we categorize their functions into three main roles: controllers,\nmodelers and optimizers, and discuss how each role contributes to\ndecision-making. Furthermore, we examine the deployment of these models across\nfive critical real-world decision-making scenarios. Finally, we summarize the\nstrengths and limitations of current approaches and propose three key\ndirections for advancing next-generation generative directive models:\nhigh-performance algorithms, large-scale generalized decision-making models,\nand self-evolving and adaptive models.\n","versions":"[{'version': 'v1', 'created': 'Mon, 24 Feb 2025 12:31:28 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 08:01:55 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 02:32:00 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Li', 'Yinchuan', ''], ['Shao', 'Xinyu', ''], ['Zhang', 'Jianping', ''], ['Wang', 'Haozhi', ''], ['Brunswic', 'Leo Maxime', ''], ['Zhou', 'Kaiwen', ''], ['Dong', 'Jiqian', ''], ['Guo', 'Kaiyang', ''], ['Li', 'Xiu', ''], ['Chen', 'Zhitang', ''], ['Wang', 'Jun', ''], ['Hao', 'Jianye', '']]","extracted_entities":"[{'text': 'generative models', 'label': 'AI model'}, {'text': 'generative models', 'label': 'AI model'}, {'text': 'generative models', 'label': 'AI model'}, {'text': 'energy-based models', 'label': 'AI model'}, {'text': 'generative\\nadversarial networks', 'label': 'AI model'}, {'text': 'variational autoencoders', 'label': 'AI model'}, {'text': 'normalizing flows', 'label': 'AI model'}, {'text': 'diffusion\\nmodels', 'label': 'AI model'}, {'text': 'generative flow networks', 'label': 'AI model'}, {'text': 'autoregressive models', 'label': 'AI model'}, {'text': 'high-performance algorithms', 'label': 'AI model'}, {'text': 'large-scale generalized decision-making models', 'label': 'AI model'}, {'text': 'self-evolving and adaptive models', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"self-evolving and adaptive models","similarity_score":0.5533747673}
{"id":2503.0485,"submitter":"Qin Wang","authors":"Minh Trung Tran, Nasrin Sohrabi, Zahir Tari, Qin Wang, Xiaoyu Xia","title":"Slow is Fast! Dissecting Ethereum's Slow Liquidity Drain Scams","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We identify the slow liquidity drain (SLID) scam, an insidious and highly\nprofitable threat to decentralized finance (DeFi), posing a large-scale,\npersistent, and growing risk to the ecosystem. Unlike traditional scams such as\nrug pulls or honeypots (USENIX Sec'19, USENIX Sec'23), SLID gradually siphons\nfunds from liquidity pools over extended periods, making detection\nsignificantly more challenging. In this paper, we conducted the first\nlarge-scale empirical analysis of 319,166 liquidity pools across six major\ndecentralized exchanges (DEXs) since 2018. We identified 3,117 SLID affected\nliquidity pools, resulting in cumulative losses of more than US$103 million. We\npropose a rule-based heuristic and an enhanced machine learning model for early\ndetection. Our machine learning model achieves a detection speed 4.77 times\nfaster than the heuristic while maintaining 95% accuracy. Our study establishes\na foundation for protecting DeFi investors at an early stage and promoting\ntransparency in the DeFi ecosystem.\n","versions":"[{'version': 'v1', 'created': 'Thu, 6 Mar 2025 02:24:35 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 05:01:22 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Tran', 'Minh Trung', ''], ['Sohrabi', 'Nasrin', ''], ['Tari', 'Zahir', ''], ['Wang', 'Qin', ''], ['Xia', 'Xiaoyu', '']]","extracted_entities":"[{'text': 'machine learning model', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"machine learning model","similarity_score":0.7232564688}
{"id":2503.05577,"submitter":"Henrik Schopmans","authors":"Daniel Hollarek, Henrik Schopmans, Jona \\\"Ostreicher, Jonas Teufel,\n  Bin Cao, Adie Alwen, Simon Schweidler, Mriganka Singh, Tim Kodalle, Hanlin\n  Hu, Gregoire Heymans, Maged Abdelsamie, Arthur Hardiagon, Alexander\n  Wieczorek, Siarhei Zhuk, Ruth Schwaiger, Sebastian Siol, Fran\\c{c}ois-Xavier\n  Coudert, Moritz Wolf, Carolin M. Sutter-Fella, Ben Breitung, Andrea M. Hodge,\n  Tong-yi Zhang, Pascal Friederich","title":"opXRD: Open Experimental Powder X-ray Diffraction Database","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Powder X-ray diffraction (pXRD) experiments are a cornerstone for materials\nstructure characterization. Despite their widespread application, analyzing\npXRD diffractograms still presents a significant challenge to automation and a\nbottleneck in high-throughput discovery in self-driving labs. Machine learning\npromises to resolve this bottleneck by enabling automated powder diffraction\nanalysis. A notable difficulty in applying machine learning to this domain is\nthe lack of sufficiently sized experimental datasets, which has constrained\nresearchers to train primarily on simulated data. However, models trained on\nsimulated pXRD patterns showed limited generalization to experimental patterns,\nparticularly for low-quality experimental patterns with high noise levels and\nelevated backgrounds. With the Open Experimental Powder X-Ray Diffraction\nDatabase (opXRD), we provide an openly available and easily accessible dataset\nof labeled and unlabeled experimental powder diffractograms. Labeled opXRD data\ncan be used to evaluate the performance of models on experimental data and\nunlabeled opXRD data can help improve the performance of models on experimental\ndata, e.g. through transfer learning methods. We collected 92552\ndiffractograms, 2179 of them labeled, from a wide spectrum of materials\nclasses. We hope this ongoing effort can guide machine learning research toward\nfully automated analysis of pXRD data and thus enable future self-driving\nmaterials labs.\n","versions":"[{'version': 'v1', 'created': 'Fri, 7 Mar 2025 16:59:18 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 07:35:46 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Hollarek', 'Daniel', ''], ['Schopmans', 'Henrik', ''], ['\u00d6streicher', 'Jona', ''], ['Teufel', 'Jonas', ''], ['Cao', 'Bin', ''], ['Alwen', 'Adie', ''], ['Schweidler', 'Simon', ''], ['Singh', 'Mriganka', ''], ['Kodalle', 'Tim', ''], ['Hu', 'Hanlin', ''], ['Heymans', 'Gregoire', ''], ['Abdelsamie', 'Maged', ''], ['Hardiagon', 'Arthur', ''], ['Wieczorek', 'Alexander', ''], ['Zhuk', 'Siarhei', ''], ['Schwaiger', 'Ruth', ''], ['Siol', 'Sebastian', ''], ['Coudert', 'Fran\u00e7ois-Xavier', ''], ['Wolf', 'Moritz', ''], ['Sutter-Fella', 'Carolin M.', ''], ['Breitung', 'Ben', ''], ['Hodge', 'Andrea M.', ''], ['Zhang', 'Tong-yi', ''], ['Friederich', 'Pascal', '']]","extracted_entities":"[{'text': 'machine learning', 'label': 'Open-source LLMs'}, {'text': 'models', 'label': 'AI model'}, {'text': 'models', 'label': 'AI model'}, {'text': 'models', 'label': 'AI model'}, {'text': 'machine learning', 'label': 'Open-source LLMs'}]","assigned_concept":"AI model","matched_keyword":"models","similarity_score":0.6660560966}
{"id":2407.12899,"submitter":"Huiguo He","authors":"Huiguo He, Huan Yang, Zixi Tuo, Yuan Zhou, Qiuyue Wang, Yuhang Zhang,\n  Zeyu Liu, Wenhao Huang, Hongyang Chao, Jian Yin","title":"DreamStory: Open-Domain Story Visualization by LLM-Guided Multi-Subject\n  Consistent Diffusion","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.MM","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Story visualization aims to create visually compelling images or videos\ncorresponding to textual narratives. Despite recent advances in diffusion\nmodels yielding promising results, existing methods still struggle to create a\ncoherent sequence of subject-consistent frames based solely on a story. To this\nend, we propose DreamStory, an automatic open-domain story visualization\nframework by leveraging the LLMs and a novel multi-subject consistent diffusion\nmodel. DreamStory consists of (1) an LLM acting as a story director and (2) an\ninnovative Multi-Subject consistent Diffusion model (MSD) for generating\nconsistent multi-subject across the images. First, DreamStory employs the LLM\nto generate descriptive prompts for subjects and scenes aligned with the story,\nannotating each scene's subjects for subsequent subject-consistent generation.\nSecond, DreamStory utilizes these detailed subject descriptions to create\nportraits of the subjects, with these portraits and their corresponding textual\ninformation serving as multimodal anchors (guidance). Finally, the MSD uses\nthese multimodal anchors to generate story scenes with consistent\nmulti-subject. Specifically, the MSD includes Masked Mutual Self-Attention\n(MMSA) and Masked Mutual Cross-Attention (MMCA) modules. MMSA and MMCA modules\nensure appearance and semantic consistency with reference images and text,\nrespectively. Both modules employ masking mechanisms to prevent subject\nblending. To validate our approach and promote progress in story visualization,\nwe established a benchmark, DS-500, which can assess the overall performance of\nthe story visualization framework, subject-identification accuracy, and the\nconsistency of the generation model. Extensive experiments validate the\neffectiveness of DreamStory in both subjective and objective evaluations.\nPlease visit our project homepage at https:\/\/dream-xyz.github.io\/dreamstory.\n","versions":"[{'version': 'v1', 'created': 'Wed, 17 Jul 2024 17:54:12 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 13:33:40 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['He', 'Huiguo', ''], ['Yang', 'Huan', ''], ['Tuo', 'Zixi', ''], ['Zhou', 'Yuan', ''], ['Wang', 'Qiuyue', ''], ['Zhang', 'Yuhang', ''], ['Liu', 'Zeyu', ''], ['Huang', 'Wenhao', ''], ['Chao', 'Hongyang', ''], ['Yin', 'Jian', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLMs'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'MSD', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'descriptive prompts', 'label': 'Prompting'}, {'text': 'Masked Mutual Self-Attention\\n(MMSA)', 'label': 'Attention mechanism'}, {'text': 'Masked Mutual Cross-Attention (MMCA)', 'label': 'Attention mechanism'}, {'text': 'MMSA', 'label': 'Attention mechanism'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2408.07237,"submitter":"Byunghwee Lee","authors":"Byunghwee Lee, Rachith Aiyappa, Yong-Yeol Ahn, Haewoon Kwak, Jisun An","title":"Neural embedding of beliefs reveals the role of relative dissonance in\n  human decision-making","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CY physics.soc-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Beliefs form the foundation of human cognition and decision-making, guiding\nour actions and social connections. A model encapsulating beliefs and their\ninterrelationships is crucial for understanding their influence on our actions.\nHowever, research on belief interplay has often been limited to beliefs related\nto specific issues and relied heavily on surveys. We propose a method to study\nthe nuanced interplay between thousands of beliefs by leveraging an online user\ndebate data and mapping beliefs onto a neural embedding space constructed using\na fine-tuned large language model (LLM). This belief space captures the\ninterconnectedness and polarization of diverse beliefs across social issues.\nOur findings show that positions within this belief space predict new beliefs\nof individuals and estimate cognitive dissonance based on the distance between\nexisting and new beliefs. This study demonstrates how LLMs, combined with\ncollective online records of human beliefs, can offer insights into the\nfundamental principles that govern human decision-making.\n","versions":"[{'version': 'v1', 'created': 'Tue, 13 Aug 2024 23:58:45 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 19:50:34 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Lee', 'Byunghwee', ''], ['Aiyappa', 'Rachith', ''], ['Ahn', 'Yong-Yeol', ''], ['Kwak', 'Haewoon', ''], ['An', 'Jisun', '']]","extracted_entities":"[{'text': 'neural embedding space', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2410.07516,"submitter":"Pengyu Xue","authors":"Pengyu Xue, Linhao Wu, Zhen Yang, Zhongxing Yu, Zhi Jin, Ge Li, Yan\n  Xiao, Shuo Liu, Xinyi Li, Hongyi Lin and Jingwen Wu","title":"Exploring and Lifting the Robustness of LLM-powered Automated Program\n  Repair with Metamorphic Testing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In recent years, Large language model-powered Automated Program Repair (LAPR)\ntechniques have achieved state-of-the-art bug-fixing performance and have been\npervasively applied and studied in both industry and academia. Nonetheless,\nLLMs were proved to be highly sensitive to input prompts, with slight\ndifferences in the expressions of semantically equivalent programs potentially\ncausing repair failures. Therefore, it is crucial to conduct robustness testing\non LAPR techniques before their practical deployment. However, related research\nis scarce. To this end, we propose MT-LAPR, a Metamorphic Testing framework\nexclusively for LAPR techniques, which summarizes nine widely-recognized\nMetamorphic Relations (MRs) by developers across three perturbation levels:\ntoken, statement, and block. Afterward, our proposed MRs are applied to buggy\ncodes to generate test cases, which are semantically equivalent yet to affect\nthe inference of LAPR. Experiments are carried out on two extensively examined\nbug-fixing datasets, i.e., Defect4J and QuixBugs, and four bug-fixing abled\nLLMs released recently, demonstrating that 34.4% - 48.5% of the test cases\nexpose the instability of LAPR techniques on average, showing the effectiveness\nof MT-LAPR and uncovering a positive correlation between code readability and\nthe robustness of LAPR techniques. Inspired by the above findings, this paper\nuses the test cases generated by MT-LAPR as samples to train a CodeT5-based\ncode editing model aiming at improving code readability and then embeds it into\nthe LAPR workflow as a data preprocessing step. Extensive experiments\ndemonstrate that this approach significantly enhances the robustness of LAPR by\n49.32% at most.\n","versions":"[{'version': 'v1', 'created': 'Thu, 10 Oct 2024 01:14:58 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 09:37:03 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Xue', 'Pengyu', ''], ['Wu', 'Linhao', ''], ['Yang', 'Zhen', ''], ['Yu', 'Zhongxing', ''], ['Jin', 'Zhi', ''], ['Li', 'Ge', ''], ['Xiao', 'Yan', ''], ['Liu', 'Shuo', ''], ['Li', 'Xinyi', ''], ['Lin', 'Hongyi', ''], ['Wu', 'Jingwen', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLMs'}, {'text': 'input prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2410.10324,"submitter":"Krzysztof Gogol","authors":"Krzysztof Gogol, Manvir Schneider, Claudio Tessone, Benjamin Livshits","title":"Liquidity Fragmentation or Optimization? Analyzing Automated Market\n  Makers Across Ethereum and Rollups","comments":"The 4th International Workshop on Cryptoasset Analytics (CAAW) at\n  FC25","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Layer-2 (L2) blockchains inherit Ethereums security guarantees while reducing\ngas fees. As a result, they are gaining traction among traders at Automated\nMarket Makers (AMMs), sparking debate over whether they contribute to liquidity\nfragmentation of Ethereum. Our research suggests that such fragmentation is not\ncurrently occurring. However, it could emerge in the future, particularly if\nLiquidity Providers (LPs) recognize the higher returns available on L2s. Using\nLagrangian optimization, we develop a model for optimal liquidity allocation\nacross AMMs on Ethereum and its L2s, using staking as a benchmark. We show\nthat, in equilibrium, AMM liquidity provision returns converge to this\nreference rate. Additionally, we measure the elasticity of trading volume with\nrespect to Total Value Locked (TVL) in AMMs and find that, on well-established\nblockchains, an increase in TVL does not necessarily lead to higher trading\nvolume. Finally, our empirical findings reveal that Ethereums liquidity pools\nare oversubscribed compared to those on L2s and often yield lower returns than\nstaking Ether. LPs could maximize their rewards by reallocating more than\ntwo-thirds of their liquidity to L2s and staking.\n","versions":"[{'version': 'v1', 'created': 'Mon, 14 Oct 2024 09:36:23 GMT'}, {'version': 'v2', 'created': 'Sun, 19 Jan 2025 09:42:07 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 21:02:23 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Gogol', 'Krzysztof', ''], ['Schneider', 'Manvir', ''], ['Tessone', 'Claudio', ''], ['Livshits', 'Benjamin', '']]","extracted_entities":"[{'text': 'Automated\\nMarket Makers (AMMs)', 'label': 'LLMs'}, {'text': 'AMMs', 'label': 'LLMs'}, {'text': 'AMMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"AMMs","similarity_score":0.546841085}
{"id":2410.2259,"submitter":"Juan Diego Rodriguez","authors":"Juan Diego Rodriguez, Aaron Mueller, Kanishka Misra","title":"Characterizing the Role of Similarity in the Property Inferences of\n  Language Models","comments":"Published at NAACL 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Property inheritance -- a phenomenon where novel properties are projected\nfrom higher level categories (e.g., birds) to lower level ones (e.g., sparrows)\n-- provides a unique window into how humans organize and deploy conceptual\nknowledge. It is debated whether this ability arises due to explicitly stored\ntaxonomic knowledge vs. simple computations of similarity between mental\nrepresentations. How are these mechanistic hypotheses manifested in\ncontemporary language models? In this work, we investigate how LMs perform\nproperty inheritance with behavioral and causal representational analysis\nexperiments. We find that taxonomy and categorical similarities are not\nmutually exclusive in LMs' property inheritance behavior. That is, LMs are more\nlikely to project novel properties from one category to the other when they are\ntaxonomically related and at the same time, highly similar. Our findings\nprovide insight into the conceptual structure of language models and may\nsuggest new psycholinguistic experiments for human subjects.\n","versions":"[{'version': 'v1', 'created': 'Tue, 29 Oct 2024 23:05:41 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 17:54:32 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Rodriguez', 'Juan Diego', ''], ['Mueller', 'Aaron', ''], ['Misra', 'Kanishka', '']]","extracted_entities":"[{'text': 'LMs', 'label': 'LLMs'}, {'text': 'LMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"LMs","similarity_score":0.6942881346}
{"id":2412.10443,"submitter":"Zhentao Tan","authors":"Zhentao Tan, Ben Xue, Jian Jia, Junhao Wang, Wencai Ye, Shaoyun Shi,\n  Mingjie Sun, Wenjin Wu, Quan Chen, Peng Jiang","title":"SweetTok: Semantic-Aware Spatial-Temporal Tokenizer for Compact Video\n  Discretization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper presents the \\textbf{S}emantic-a\\textbf{W}ar\\textbf{E}\nspatial-t\\textbf{E}mporal \\textbf{T}okenizer (SweetTok), a novel video\ntokenizer to overcome the limitations in current video tokenization methods for\ncompacted yet effective discretization. Unlike previous approaches that process\nflattened local visual patches via direct discretization or adaptive query\ntokenization, SweetTok proposes a decoupling framework, compressing visual\ninputs through distinct spatial and temporal queries via \\textbf{D}ecoupled\n\\textbf{Q}uery \\textbf{A}uto\\textbf{E}ncoder (DQAE). This design allows\nSweetTok to efficiently compress video token count while achieving superior\nfidelity by capturing essential information across spatial and temporal\ndimensions. Furthermore, we design a \\textbf{M}otion-enhanced \\textbf{L}anguage\n\\textbf{C}odebook (MLC) tailored for spatial and temporal compression to\naddress the differences in semantic representation between appearance and\nmotion information. SweetTok significantly improves video reconstruction\nresults by \\textbf{42.8\\%} w.r.t rFVD on UCF-101 dataset. With a better token\ncompression strategy, it also boosts downstream video generation results by\n\\textbf{15.1\\%} w.r.t gFVD. Additionally, the compressed decoupled tokens are\nimbued with semantic information, enabling few-shot recognition capabilities\npowered by LLMs in downstream applications.\n","versions":"[{'version': 'v1', 'created': 'Wed, 11 Dec 2024 13:48:06 GMT'}, {'version': 'v2', 'created': 'Tue, 17 Dec 2024 03:55:34 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 03:19:42 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Tan', 'Zhentao', ''], ['Xue', 'Ben', ''], ['Jia', 'Jian', ''], ['Wang', 'Junhao', ''], ['Ye', 'Wencai', ''], ['Shi', 'Shaoyun', ''], ['Sun', 'Mingjie', ''], ['Wu', 'Wenjin', ''], ['Chen', 'Quan', ''], ['Jiang', 'Peng', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2412.10471,"submitter":"Zeyuan Yang","authors":"Zeyuan Yang, Delin Chen, Xueyang Yu, Maohao Shen, Chuang Gan","title":"VCA: Video Curious Agent for Long Video Understanding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Long video understanding poses unique challenges due to their temporal\ncomplexity and low information density. Recent works address this task by\nsampling numerous frames or incorporating auxiliary tools using LLMs, both of\nwhich result in high computational costs. In this work, we introduce a\ncuriosity-driven video agent with self-exploration capability, dubbed as VCA.\nBuilt upon VLMs, VCA autonomously navigates video segments and efficiently\nbuilds a comprehensive understanding of complex video sequences. Instead of\ndirectly sampling frames, VCA employs a tree-search structure to explore video\nsegments and collect frames. Rather than relying on external feedback or\nreward, VCA leverages VLM's self-generated intrinsic reward to guide its\nexploration, enabling it to capture the most crucial information for reasoning.\nExperimental results on multiple long video benchmarks demonstrate our\napproach's superior effectiveness and efficiency.\n","versions":"[{'version': 'v1', 'created': 'Thu, 12 Dec 2024 23:39:54 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 03:35:16 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Yang', 'Zeyuan', ''], ['Chen', 'Delin', ''], ['Yu', 'Xueyang', ''], ['Shen', 'Maohao', ''], ['Gan', 'Chuang', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLMs'}, {'text': 'VLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2502.11926,"submitter":"Nedjma Ousidhoum","authors":"Shamsuddeen Hassan Muhammad, Nedjma Ousidhoum, Idris Abdulmumin, Jan\n  Philip Wahle, Terry Ruas, Meriem Beloucif, Christine de Kock, Nirmal Surange,\n  Daniela Teodorescu, Ibrahim Said Ahmad, David Ifeoluwa Adelani, Alham Fikri\n  Aji, Felermino D. M. A. Ali, Ilseyar Alimova, Vladimir Araujo, Nikolay\n  Babakov, Naomi Baes, Ana-Maria Bucur, Andiswa Bukula, Guanqun Cao, Rodrigo\n  Tufino Cardenas, Rendi Chevi, Chiamaka Ijeoma Chukwuneke, Alexandra\n  Ciobotaru, Daryna Dementieva, Murja Sani Gadanya, Robert Geislinger, Bela\n  Gipp, Oumaima Hourrane, Oana Ignat, Falalu Ibrahim Lawan, Rooweither Mabuya,\n  Rahmad Mahendra, Vukosi Marivate, Andrew Piper, Alexander Panchenko, Charles\n  Henrique Porto Ferreira, Vitaly Protasov, Samuel Rutunda, Manish Shrivastava,\n  Aura Cristina Udrea, Lilian Diana Awuor Wanzare, Sophie Wu, Florian Valentin\n  Wunderlich, Hanif Muhammad Zhafran, Tianhui Zhang, Yi Zhou, Saif M. Mohammad","title":"BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion\n  Recognition Datasets for 28 Languages","comments":"20 pages, under review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  People worldwide use language in subtle and complex ways to express emotions.\nWhile emotion recognition -- an umbrella term for several NLP tasks --\nsignificantly impacts different applications in NLP and other fields, most work\nin the area is focused on high-resource languages. Therefore, this has led to\nmajor disparities in research and proposed solutions, especially for\nlow-resource languages that suffer from the lack of high-quality datasets. In\nthis paper, we present BRIGHTER -- a collection of multilabeled\nemotion-annotated datasets in 28 different languages. BRIGHTER covers\npredominantly low-resource languages from Africa, Asia, Eastern Europe, and\nLatin America, with instances from various domains annotated by fluent\nspeakers. We describe the data collection and annotation processes and the\nchallenges of building these datasets. Then, we report different experimental\nresults for monolingual and crosslingual multi-label emotion identification, as\nwell as intensity-level emotion recognition. We investigate results with and\nwithout using LLMs and analyse the large variability in performance across\nlanguages and text domains. We show that BRIGHTER datasets are a step towards\nbridging the gap in text-based emotion recognition and discuss their impact and\nutility.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Feb 2025 15:39:50 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 12:20:14 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Muhammad', 'Shamsuddeen Hassan', ''], ['Ousidhoum', 'Nedjma', ''], ['Abdulmumin', 'Idris', ''], ['Wahle', 'Jan Philip', ''], ['Ruas', 'Terry', ''], ['Beloucif', 'Meriem', ''], ['de Kock', 'Christine', ''], ['Surange', 'Nirmal', ''], ['Teodorescu', 'Daniela', ''], ['Ahmad', 'Ibrahim Said', ''], ['Adelani', 'David Ifeoluwa', ''], ['Aji', 'Alham Fikri', ''], ['Ali', 'Felermino D. M. A.', ''], ['Alimova', 'Ilseyar', ''], ['Araujo', 'Vladimir', ''], ['Babakov', 'Nikolay', ''], ['Baes', 'Naomi', ''], ['Bucur', 'Ana-Maria', ''], ['Bukula', 'Andiswa', ''], ['Cao', 'Guanqun', ''], ['Cardenas', 'Rodrigo Tufino', ''], ['Chevi', 'Rendi', ''], ['Chukwuneke', 'Chiamaka Ijeoma', ''], ['Ciobotaru', 'Alexandra', ''], ['Dementieva', 'Daryna', ''], ['Gadanya', 'Murja Sani', ''], ['Geislinger', 'Robert', ''], ['Gipp', 'Bela', ''], ['Hourrane', 'Oumaima', ''], ['Ignat', 'Oana', ''], ['Lawan', 'Falalu Ibrahim', ''], ['Mabuya', 'Rooweither', ''], ['Mahendra', 'Rahmad', ''], ['Marivate', 'Vukosi', ''], ['Piper', 'Andrew', ''], ['Panchenko', 'Alexander', ''], ['Ferreira', 'Charles Henrique Porto', ''], ['Protasov', 'Vitaly', ''], ['Rutunda', 'Samuel', ''], ['Shrivastava', 'Manish', ''], ['Udrea', 'Aura Cristina', ''], ['Wanzare', 'Lilian Diana Awuor', ''], ['Wu', 'Sophie', ''], ['Wunderlich', 'Florian Valentin', ''], ['Zhafran', 'Hanif Muhammad', ''], ['Zhang', 'Tianhui', ''], ['Zhou', 'Yi', ''], ['Mohammad', 'Saif M.', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2502.16457,"submitter":"Heegyu Kim","authors":"Heegyu Kim, Taeyang Jeon, Seungtaek Choi, Ji Hoon Hong, Dong Won Jeon,\n  Ga-Yeon Baek, Kyeong-Won Kwak, Dong-Hee Lee, Jisu Bae, Chihoon Lee, Yunseo\n  Kim, Seon-Jin Choi, Jin-Seong Park, Sung Beom Cho, Hyunsouk Cho","title":"Towards Fully-Automated Materials Discovery via Large-Scale Synthesis\n  Dataset and Expert-Level LLM-as-a-Judge","comments":"under review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Materials synthesis is vital for innovations such as energy storage,\ncatalysis, electronics, and biomedical devices. Yet, the process relies heavily\non empirical, trial-and-error methods guided by expert intuition. Our work aims\nto support the materials science community by providing a practical,\ndata-driven resource. We have curated a comprehensive dataset of 17K\nexpert-verified synthesis recipes from open-access literature, which forms the\nbasis of our newly developed benchmark, AlchemyBench. AlchemyBench offers an\nend-to-end framework that supports research in large language models applied to\nsynthesis prediction. It encompasses key tasks, including raw materials and\nequipment prediction, synthesis procedure generation, and characterization\noutcome forecasting. We propose an LLM-as-a-Judge framework that leverages\nlarge language models for automated evaluation, demonstrating strong\nstatistical agreement with expert assessments. Overall, our contributions offer\na supportive foundation for exploring the capabilities of LLMs in predicting\nand guiding materials synthesis, ultimately paving the way for more efficient\nexperimental design and accelerated innovation in materials science.\n","versions":"[{'version': 'v1', 'created': 'Sun, 23 Feb 2025 06:16:23 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Mar 2025 00:40:18 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 14:00:39 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Kim', 'Heegyu', ''], ['Jeon', 'Taeyang', ''], ['Choi', 'Seungtaek', ''], ['Hong', 'Ji Hoon', ''], ['Jeon', 'Dong Won', ''], ['Baek', 'Ga-Yeon', ''], ['Kwak', 'Kyeong-Won', ''], ['Lee', 'Dong-Hee', ''], ['Bae', 'Jisu', ''], ['Lee', 'Chihoon', ''], ['Kim', 'Yunseo', ''], ['Choi', 'Seon-Jin', ''], ['Park', 'Jin-Seong', ''], ['Cho', 'Sung Beom', ''], ['Cho', 'Hyunsouk', '']]","extracted_entities":"[{'text': 'open-access literature', 'label': 'Open-source LLMs'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2502.16965,"submitter":"Miaomiao Cai","authors":"Miaomiao Cai, Guanjie Wang, Wei Li, Zhijun Tu, Hanting Chen, Shaohui\n  Lin, Jie Hu","title":"Autoregressive Image Generation with Vision Full-view Prompt","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In autoregressive (AR) image generation, models based on the 'next-token\nprediction' paradigm of LLMs have shown comparable performance to diffusion\nmodels by reducing inductive biases. However, directly applying LLMs to complex\nimage generation can struggle with reconstructing the image's structure and\ndetails, impacting the generation's accuracy and stability. Additionally, the\n'next-token prediction' paradigm in the AR model does not align with the\ncontextual scanning and logical reasoning processes involved in human visual\nperception, limiting effective image generation. Prompt engineering, as a key\ntechnique for guiding LLMs, leverages specifically designed prompts to improve\nmodel performance on complex natural language processing (NLP) tasks, enhancing\naccuracy and stability of generation while maintaining contextual coherence and\nlogical consistency, similar to human reasoning. Inspired by prompt engineering\nfrom the field of NLP, we propose Vision Full-view prompt (VF prompt) to\nenhance autoregressive image generation. Specifically, we design specialized\nimage-related VF prompts for AR image generation to simulate the process of\nhuman image creation. This enhances contextual logic ability by allowing the\nmodel to first perceive overall distribution information before generating the\nimage, and improve generation stability by increasing the inference steps.\nCompared to the AR method without VF prompts, our method shows outstanding\nperformance and achieves an approximate improvement of 20%.\n","versions":"[{'version': 'v1', 'created': 'Mon, 24 Feb 2025 08:44:01 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 11:15:13 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 10:09:21 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Cai', 'Miaomiao', ''], ['Wang', 'Guanjie', ''], ['Li', 'Wei', ''], ['Tu', 'Zhijun', ''], ['Chen', 'Hanting', ''], ['Lin', 'Shaohui', ''], ['Hu', 'Jie', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLMs'}, {'text': 'LLMs', 'label': 'LLMs'}, {'text': 'Prompt engineering', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'LLMs'}, {'text': 'prompt engineering', 'label': 'Prompting'}, {'text': 'Vision Full-view prompt', 'label': 'Prompting'}, {'text': 'VF prompt', 'label': 'Prompting'}, {'text': 'VF prompts', 'label': 'Prompting'}, {'text': 'VF prompts', 'label': 'Prompting'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2502.19649,"submitter":"Jan Wehner","authors":"Jan Wehner, Sahar Abdelnabi, Daniel Tan, David Krueger, Mario Fritz","title":"Taxonomy, Opportunities, and Challenges of Representation Engineering\n  for Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Representation Engineering (RepE) is a novel paradigm for controlling the\nbehavior of LLMs. Unlike traditional approaches that modify inputs or fine-tune\nthe model, RepE directly manipulates the model's internal representations. As a\nresult, it may offer more effective, interpretable, data-efficient, and\nflexible control over models' behavior. We present the first comprehensive\nsurvey of RepE for LLMs, reviewing the rapidly growing literature to address\nkey questions: What RepE methods exist and how do they differ? For what\nconcepts and problems has RepE been applied? What are the strengths and\nweaknesses of RepE compared to other methods? To answer these, we propose a\nunified framework describing RepE as a pipeline comprising representation\nidentification, operationalization, and control. We posit that while RepE\nmethods offer significant potential, challenges remain, including managing\nmultiple concepts, ensuring reliability, and preserving models' performance.\nTowards improving RepE, we identify opportunities for experimental and\nmethodological improvements and construct a guide for best practices.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 00:40:01 GMT'}, {'version': 'v2', 'created': 'Sun, 2 Mar 2025 11:23:58 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 13:31:36 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Wehner', 'Jan', ''], ['Abdelnabi', 'Sahar', ''], ['Tan', 'Daniel', ''], ['Krueger', 'David', ''], ['Fritz', 'Mario', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLMs'}, {'text': 'LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2502.20167,"submitter":"Allen Schmaltz","authors":"Allen Schmaltz","title":"Similarity-Distance-Magnitude Universal Verification","comments":"35 pages (8 Tables, 4 Algorithms, 5 Listings)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We address the neural network robustness problem by adding Similarity (i.e.,\ncorrectly predicted depth-matches into training)-awareness and\nDistance-to-training-distribution-awareness to the existing output Magnitude\n(i.e., decision-boundary)-awareness of the softmax function. The resulting sdm\nactivation function provides strong signals of the relative epistemic\n(reducible) predictive uncertainty. We use this novel behavior to further\naddress the complementary HCI problem of mapping the output to\nhuman-interpretable summary statistics over relevant partitions of a held-out\ncalibration set. Estimates of prediction-conditional uncertainty are obtained\nvia a parsimonious learned transform over the class-conditional empirical CDFs\nof the output of a final-layer sdm activation function. For decision-making and\nas an intrinsic model check, estimates of class-conditional accuracy are\nobtained by further partitioning the high-probability regions of this\ncalibrated output into class-conditional, region-specific CDFs. The uncertainty\nestimates from sdm calibration are remarkably robust to test-time distribution\nshifts and out-of-distribution inputs; incorporate awareness of the effective\nsample size; provide estimates of uncertainty from the learning and data\nsplitting processes; and are well-suited for selective classification and\nconditional branching for additional test-time compute based on the predictive\nuncertainty, as for selective LLM generation, routing, and composition over\nmultiple models and retrieval. Finally, we construct sdm networks, LLMs with\nuncertainty-aware verification and interpretability-by-exemplar as intrinsic\nproperties. We provide open-source software implementing these results.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 15:05:00 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 20:21:05 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Schmaltz', 'Allen', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2503.02783,"submitter":"Haoling Li","authors":"Jie Wu, Haoling Li, Xin Zhang, Jianwen Luo, Yangyu Huang, Ruihang Chu,\n  Yujiu Yang, Scarlett Li","title":"IterPref: Focal Preference Learning for Code Generation via Iterative\n  Debugging","comments":"The code and data will be released soon","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Preference learning enhances Code LLMs beyond supervised fine-tuning by\nleveraging relative quality comparisons. Existing methods construct preference\npairs from\n  candidates based on test case success, treating the higher pass rate sample\nas positive and the lower as negative. However, this approach does not pinpoint\nspecific errors in the code, which prevents the model from learning more\ninformative error correction patterns, as aligning failing code as a whole\nlacks the granularity needed to capture meaningful error-resolution\nrelationships. To address these issues, we propose IterPref, a new preference\nalignment framework that mimics human iterative debugging to refine Code LLMs.\nIterPref explicitly locates error regions and aligns the corresponding tokens\nvia a tailored DPO algorithm. To generate informative pairs, we introduce the\nCodeFlow dataset, where samples are iteratively refined until passing tests,\nwith modifications capturing error corrections. Extensive experiments show that\na diverse suite of Code LLMs equipped with IterPref achieves significant\nperformance gains in code generation and improves on challenging tasks like\nBigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our\ncode and data will be made publicaly available.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 16:56:34 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 18:08:16 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Wu', 'Jie', ''], ['Li', 'Haoling', ''], ['Zhang', 'Xin', ''], ['Luo', 'Jianwen', ''], ['Huang', 'Yangyu', ''], ['Chu', 'Ruihang', ''], ['Yang', 'Yujiu', ''], ['Li', 'Scarlett', '']]","extracted_entities":"[{'text': 'Preference learning', 'label': 'Few-shot Learning'}, {'text': 'Code LLMs', 'label': 'LLMs'}, {'text': 'Code LLMs', 'label': 'LLMs'}, {'text': 'Code LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"Code LLMs","similarity_score":0.7989426255}
{"id":2503.0424,"submitter":"Ruizhe Chen","authors":"Ruizhe Chen, Wenhao Chai, Zhifei Yang, Xiaotian Zhang, Joey Tianyi\n  Zhou, Tony Quek, Soujanya Poria, Zuozhu Liu","title":"DiffPO: Diffusion-styled Preference Optimization for Efficient\n  Inference-Time Alignment of Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Inference-time alignment provides an efficient alternative for aligning LLMs\nwith humans. However, these approaches still face challenges, such as limited\nscalability due to policy-specific value functions and latency during the\ninference phase. In this paper, we propose a novel approach, Diffusion-styled\nPreference Optimization (\\model), which provides an efficient and\npolicy-agnostic solution for aligning LLMs with humans. By directly performing\nalignment at sentence level, \\model~avoids the time latency associated with\ntoken-level generation. Designed as a plug-and-play module, \\model~can be\nseamlessly integrated with various base models to enhance their alignment.\nExtensive experiments on AlpacaEval 2, MT-bench, and HH-RLHF demonstrate that\n\\model~achieves superior alignment performance across various settings,\nachieving a favorable trade-off between alignment quality and inference-time\nlatency. Furthermore, \\model~demonstrates model-agnostic scalability,\nsignificantly improving the performance of large models such as Llama-3-70B.\n","versions":"[{'version': 'v1', 'created': 'Thu, 6 Mar 2025 09:21:54 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 14:36:12 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Chen', 'Ruizhe', ''], ['Chai', 'Wenhao', ''], ['Yang', 'Zhifei', ''], ['Zhang', 'Xiaotian', ''], ['Zhou', 'Joey Tianyi', ''], ['Quek', 'Tony', ''], ['Poria', 'Soujanya', ''], ['Liu', 'Zuozhu', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLMs'}, {'text': 'LLMs', 'label': 'LLMs'}, {'text': 'Llama-3-70B', 'label': 'Llama'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2503.06366,"submitter":"Henry Kvinge","authors":"Herman Chau, Helen Jenne, Davis Brown, Jesse He, Mark Raugas, Sara\n  Billey, Henry Kvinge","title":"Machine Learning meets Algebraic Combinatorics: A Suite of Datasets\n  Capturing Research-level Conjecturing Ability in Pure Mathematics","comments":"26 pages, comments welcome","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI math.CO math.RT","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  With recent dramatic increases in AI system capabilities, there has been\ngrowing interest in utilizing machine learning for reasoning-heavy,\nquantitative tasks, particularly mathematics. While there are many resources\ncapturing mathematics at the high-school, undergraduate, and graduate level,\nthere are far fewer resources available that align with the level of difficulty\nand open endedness encountered by professional mathematicians working on open\nproblems. To address this, we introduce a new collection of datasets, the\nAlgebraic Combinatorics Dataset Repository (ACD Repo), representing either\nfoundational results or open problems in algebraic combinatorics, a subfield of\nmathematics that studies discrete structures arising from abstract algebra.\nFurther differentiating our dataset collection is the fact that it aims at the\nconjecturing process. Each dataset includes an open-ended research-level\nquestion and a large collection of examples (up to 10M in some cases) from\nwhich conjectures should be generated. We describe all nine datasets, the\ndifferent ways machine learning models can be applied to them (e.g., training\nwith narrow models followed by interpretability analysis or program synthesis\nwith LLMs), and discuss some of the challenges involved in designing datasets\nlike these.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 00:11:40 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Chau', 'Herman', ''], ['Jenne', 'Helen', ''], ['Brown', 'Davis', ''], ['He', 'Jesse', ''], ['Raugas', 'Mark', ''], ['Billey', 'Sara', ''], ['Kvinge', 'Henry', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2503.06479,"submitter":"Ali Sarabadani","authors":"Ali Sarabadani (1), Kheirolah Rahsepar Fard (2), and Hamid Dalvand (3)\n  ((1) Department of Computer Engineering and Information Technology,\n  University of Qom, Iran, (2) Department of Computer Engineering and\n  Information Technology, University of Qom, Iran, (3) Department of\n  Occupational Therapy, School of Rehabilitation, Tehran University of Medical\n  Sciences, Iran)","title":"ExKG-LLM: Leveraging Large Language Models for Automated Expansion of\n  Cognitive Neuroscience Knowledge Graphs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The paper introduces ExKG-LLM, a framework designed to automate the expansion\nof cognitive neuroscience knowledge graphs (CNKG) using large language models\n(LLMs). It addresses limitations in existing tools by enhancing accuracy,\ncompleteness, and usefulness in CNKG. The framework leverages a large dataset\nof scientific papers and clinical reports, applying state-of-the-art LLMs to\nextract, optimize, and integrate new entities and relationships. Evaluation\nmetrics include precision, recall, and graph density. Results show significant\nimprovements: precision (0.80, +6.67%), recall (0.81, +15.71%), F1 score\n(0.805, +11.81%), and increased edge nodes (21.13% and 31.92%). Graph density\nslightly decreased, reflecting a broader but more fragmented structure.\nEngagement rates rose by 20%, while CNKG diameter increased to 15, indicating a\nmore distributed structure. Time complexity improved to O(n log n), but space\ncomplexity rose to O(n2), indicating higher memory usage. ExKG-LLM demonstrates\npotential for enhancing knowledge generation, semantic search, and clinical\ndecision-making in cognitive neuroscience, adaptable to broader scientific\nfields.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 06:32:56 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Sarabadani', 'Ali', ''], ['Fard', 'Kheirolah Rahsepar', ''], ['Dalvand', 'Hamid', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLMs'}, {'text': 'Graph density', 'label': 'quantisation'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2503.06533,"submitter":"Guo Long","authors":"Long Guo, Ying Zhang, Qi Qin, Guanjun Liu, Hanyu Chen, Yan-an Yao","title":"Hierarchical Multi-Objective Optimization for Precise Performance Design\n  of Closed-Chain Legged Mechanisms","comments":"to be published in Swarm and Evolutionary Computation","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CE","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Over the past decades, the performance design of closed-chain legged\nmechanisms (CLMs) has not been adequately addressed. Most existing design\nmethodologies have predominantly relied on trajectory synthesis, which\ninadvertently prioritizes less critical performance aspects. This study\nproposes a hierarchical multi-objective optimization strategy to address this\nlimitation. First, the numerical performance-trajectory mapping is derived\nbased on a foot-ground contact model, aiming to decouple the performance\ncharacteristics. Subsequently, a hierarchical optimization strategy is employed\nfor two CLM design scenarios: In trajectory shape-constrained scenarios, a\ncoarse-to-fine optimization process, integrating Fourier descriptors, refines\nthe design from overall shape to local features. In scenarios without\ntrajectory shape constraints, a stepwise optimization process is proposed for\nreconfigurable CLMs to transition from primary motion to auxiliary motion. The\nrobustness of the proposed design strategy is validated across three\nconfigurations and seven algorithms. The effectiveness of the proposed design\nstrategy is verified by comparison with other existing CLM design methods. The\napplicability of the proposed strategy is confirmed through simulation and\nprototype experiments. The results demonstrate that the hierarchical strategy\neffectively addresses the challenges of precise performance design in CLMs. Our\nwork provides a general framework for the CLM design and offers insights for\nthe optimization design of other closed-chain linkages.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 09:30:15 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Guo', 'Long', ''], ['Zhang', 'Ying', ''], ['Qin', 'Qi', ''], ['Liu', 'Guanjun', ''], ['Chen', 'Hanyu', ''], ['Yao', 'Yan-an', '']]","extracted_entities":"[{'text': 'CLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"CLMs","similarity_score":0.5790475607}
{"id":2109.1138,"submitter":"Pawe{\\l} Duch","authors":"Pawe{\\l} Duch","title":"Flow equation approach to singular stochastic PDEs","comments":"146 pages, minor changes to match the published version, added list\n  of symbols","journal-ref":"Probability and Mathematical Physics 6-2 (2025), 327--437","doi":"10.2140\/pmp.2025.6.327","report-no":null,"categories":"math.PR math-ph math.MP","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We prove universality of a macroscopic behavior of solutions of a large class\nof semi-linear parabolic SPDEs on $\\mathbb{R}_+\\times\\mathbb{T}$ with\nfractional Laplacian $(-\\Delta)^{\\sigma\/2}$, additive noise and polynomial\nnon-linearity, where $\\mathbb{T}$ is the $d$-dimensional torus. We consider the\nweakly non-linear regime and not necessarily Gaussian noises which are\nstationary, centered, sufficiently regular and satisfy some integrability and\nmixing conditions. We prove that the macroscopic scaling limit exists and has a\nuniversal law characterized by parameters of the relevant perturbations of the\nlinear equation. We develop a new solution theory for singular SPDEs of the\nabove-mentioned form using the Wilsonian renormalization group theory and the\nPolchinski flow equation. In particular, in the case of $d=4$ and the cubic\nnon-linearity our analysis covers the whole sub-critical regime $\\sigma>2$. Our\ntechnique avoids completely all the algebraic and combinatorial problems\narising in different approaches.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 Sep 2021 13:51:22 GMT'}, {'version': 'v2', 'created': 'Thu, 28 Apr 2022 11:25:06 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 12:59:29 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Duch', 'Pawe\u0142', '']]","extracted_entities":"[{'text': 'macroscopic scaling limit', 'label': 'Scaling law'}, {'text': 'universal law', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"macroscopic scaling limit","similarity_score":0.6548899412}
{"id":2111.02019,"submitter":"Juho Timonen","authors":"Juho Timonen and Harri L\\\"ahdesm\\\"aki","title":"Scalable mixed-domain Gaussian process modeling and model reduction for\n  longitudinal data","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.CO cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Gaussian process (GP) models that combine both categorical and continuous\ninput variables have found use in analysis of longitudinal data and computer\nexperiments. However, standard inference for these models has the typical cubic\nscaling, and common scalable approximation schemes for GPs cannot be applied\nsince the covariance function is non-continuous. In this work, we derive a\nbasis function approximation scheme for mixed-domain covariance functions,\nwhich scales linearly with respect to the number of observations and total\nnumber of basis functions. The proposed approach is naturally applicable to\nalso Bayesian GP regression with discrete observation models. We demonstrate\nthe scalability of the approach and compare model reduction techniques for\nadditive GP models in a longitudinal data context. We confirm that we can\napproximate the exact GP model accurately in a fraction of the runtime compared\nto fitting the corresponding exact model. In addition, we demonstrate a\nscalable model reduction workflow for obtaining smaller and more interpretable\nmodels when dealing with a large number of candidate predictors.\n","versions":"[{'version': 'v1', 'created': 'Wed, 3 Nov 2021 04:47:37 GMT'}, {'version': 'v2', 'created': 'Fri, 6 Sep 2024 09:06:25 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 00:52:01 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Timonen', 'Juho', ''], ['L\u00e4hdesm\u00e4ki', 'Harri', '']]","extracted_entities":"[{'text': 'cubic\\nscaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"cubic\nscaling","similarity_score":0.5120496154}
{"id":2203.14846,"submitter":"C. Miguel Barriuso G.","authors":"Miguel Ruiz-Garcia, C. Miguel Barriuso G., Lachlan C. Alexander, Dirk\n  G. A. L. Aarts, Luca Ghiringhelli and Chantal Valeriani","title":"Discovering dynamic laws from observations: the case of self-propelled,\n  interacting colloids","comments":"16 pages, 9 figures","journal-ref":"Physical Review E 109, 064611 (2024)","doi":"10.1103\/PhysRevE.109.064611","report-no":null,"categories":"cond-mat.soft cond-mat.dis-nn","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Active matter spans a wide range of time and length scales, from groups of\ncells and synthetic self-propelled particles to schools of fish, flocks of\nbirds, or even human crowds. The theoretical framework describing these systems\nhas shown tremendous success at finding universal phenomenology. However,\nfurther progress is often burdened by the difficulty of determining the forces\nthat control the dynamics of the individual elements within each system.\nAccessing this local information is key to understanding the physics dominating\nthe system and to create the models that can explain the observed collective\nphenomena. In this work, we present a machine-learning model, a graph neural\nnetwork, that uses the collective movement of the system to learn the active\nand two-body forces controlling the individual dynamics of the particles. We\nverify our approach using numerical simulations of active brownian particles,\nconsidering different interaction potentials and levels of activity. Finally,\nwe apply our model to experiments of electrophoretic Janus particles,\nextracting the active and two-body forces that control the dynamics of the\ncolloids. Due to this, we can uncover the physics dominating the behavior of\nthe system. We extract an active force that depends on the electric field and\nalso area fraction. We also discover a dependence of the two-body interaction\nwith the electric field that leads us to propose that the dominant force\nbetween these colloids is a screened electrostatic interaction with a constant\nlength scale. We expect that this methodology can open a new avenue for the\nstudy and modeling of experimental systems of active particles.\n","versions":"[{'version': 'v1', 'created': 'Mon, 28 Mar 2022 15:38:35 GMT'}, {'version': 'v2', 'created': 'Tue, 19 Apr 2022 16:28:57 GMT'}, {'version': 'v3', 'created': 'Fri, 21 Apr 2023 08:14:03 GMT'}, {'version': 'v4', 'created': 'Wed, 19 Jul 2023 15:06:47 GMT'}, {'version': 'v5', 'created': 'Wed, 12 Mar 2025 13:20:22 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Ruiz-Garcia', 'Miguel', ''], ['G.', 'C. Miguel Barriuso', ''], ['Alexander', 'Lachlan C.', ''], ['Aarts', 'Dirk G. A. L.', ''], ['Ghiringhelli', 'Luca', ''], ['Valeriani', 'Chantal', '']]","extracted_entities":"[{'text': 'graph neural\\nnetwork', 'label': 'Neural Language Model'}, {'text': 'constant\\nlength scale', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"constant\nlength scale","similarity_score":0.5186585784}
{"id":2210.14183,"submitter":"Lei Chen","authors":"Haoyu Hu, Lei Chen, Qimiao Si","title":"Quantum critical metals and loss of quasiparticles","comments":"Nature Physics, Review article; 39 pages, 6 figures, 2 info-boxes","journal-ref":"Nat. Phys. 20, 1863-1873 (2024)","doi":"10.1038\/s41567-024-02679-7","report-no":null,"categories":"cond-mat.str-el cond-mat.supr-con","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Strange metals develop near quantum critical points in a variety of strongly\ncorrelated systems. Some of the issues that are central to the field include\nhow the quantum-critical state loses quasiparticles, how it drives\nsuperconductivity, and to what extent the strange-metal physics in different\nclasses of correlated systems are interconnected. In this Review, we survey\nsome of these issues from the vantage point of heavy fermion metals. We will\ndescribe the notion of Kondo destruction and how it leads to several crucial\neffects. These include a transformation of the Fermi surface from large to\nsmall when the system is tuned across the quantum-critical point, a loss of\nquasiparticles everywhere on the Fermi surface when it is perched at the\nquantum-critical point, and a dynamical Planckian scaling in various physical\nproperties including charge responses. We close with a discussion about the\nconnections between the strange-metal physics in heavy fermion metals and its\ncounterparts in the cuprates and other correlated materials.\n","versions":"[{'version': 'v1', 'created': 'Tue, 25 Oct 2022 17:21:32 GMT'}, {'version': 'v2', 'created': 'Sun, 21 Jul 2024 10:04:55 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 12:03:53 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Hu', 'Haoyu', ''], ['Chen', 'Lei', ''], ['Si', 'Qimiao', '']]","extracted_entities":"[{'text': 'Planckian scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"Planckian scaling","similarity_score":0.5853167772}
{"id":2307.16721,"submitter":"Apostolos Papaioannou Dr.","authors":"Zbigniew Palmowski, Meral \\c{S}im\\c{s}ek and Apostolos D. Papaioannou","title":"Fluctuations of Omega-killed level-dependent spectrally negative L\\'evy\n  processes","comments":"arXiv:2307.16721v1 had mistakes that we have fixed and replaced in\n  this updated version","journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper, we solve exit problems for a level-dependent L\\'evy process\nwhich is exponentially killed with a killing intensity that depends on the\npresent state of the process. Moreover, we analyse the respective resolvents.\nAll identities are given in terms of new generalisations of scale functions\n(counterparts of the scale function from the theory of L\\'evy processes), which\nare solutions of Volterra integral equations. Furthermore, we obtain similar\nresults for the reflected level-dependent L\\'evy processes. The existence of\nthe solution of the stochastic differential equation for reflected\nlevel-dependent L\\'evy processes is also discussed. Finally, to illustrate our\nresult, the probability of bankruptcy is obtained for an insurance risk\nprocess.\n","versions":"[{'version': 'v1', 'created': 'Mon, 31 Jul 2023 14:41:59 GMT'}, {'version': 'v2', 'created': 'Sat, 23 Dec 2023 19:31:53 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 13:54:06 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Palmowski', 'Zbigniew', ''], ['\u015eim\u015fek', 'Meral', ''], ['Papaioannou', 'Apostolos D.', '']]","extracted_entities":"[{'text': 'scale functions', 'label': 'Scaling law'}, {'text': 'Volterra integral equations', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scale functions","similarity_score":0.5663737059}
{"id":2308.11745,"submitter":"William Green","authors":"M. Burak Erdogan, Michael Goldberg, William R. Green","title":"Dispersive estimates for higher order Schr\\\"odinger operators with\n  scaling-critical potentials","comments":"Updated to reflect referee comments. To appear in Mathematische\n  Annalen, 25 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We prove a family of dispersive estimates for the higher order Schr\\\"odinger\nequation $iu_t=(-\\Delta)^mu +Vu$ for $m\\in \\mathbb N$ with $m>1$ and $2m<n<4m$.\nHere $V$ is a real-valued potential belonging to the closure of $C_0$ functions\nwith respect to the generalized Kato norm, which has critical scaling. Under\nstandard assumptions on the spectrum, we show that $e^{-itH}P_{ac}(H)$\nsatisfies a $|t|^{-\\frac{n}{2m}}$ bound mapping $L^1$ to $L^\\infty$ by adapting\na Wiener inversion theorem. We further show the lack of positive resonances for\nthe operator $(-\\Delta)^m +V$ and a family of dispersive estimates for\noperators of the form $|H|^{\\beta-\\frac{n}{2m}}e^{-itH}P_{ac}(H)$ for\n$0<\\beta\\leq \\frac{n}{2}$. The results apply in both even and odd dimensions in\nthe allowed range.\n","versions":"[{'version': 'v1', 'created': 'Tue, 22 Aug 2023 19:10:29 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 13:39:01 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Erdogan', 'M. Burak', ''], ['Goldberg', 'Michael', ''], ['Green', 'William R.', '']]","extracted_entities":"[{'text': 'generalized Kato norm', 'label': 'Scaling law'}, {'text': 'critical scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"critical scaling","similarity_score":0.6000217795}
{"id":2308.11832,"submitter":"Morris Ang","authors":"Morris Ang, Ewain Gwynne","title":"Supercritical Liouville quantum gravity and CLE$_4$","comments":"36 pages, 2 figures. Version 4 includes updates on progress on the\n  listed open problems and applications of the present work","journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR math-ph math.MP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We establish the first relationship between Schramm-Loewner evolution (SLE)\nand Liouville quantum gravity (LQG) in the supercritical (a.k.a. strongly\ncoupled) phase, which corresponds to central charge values $\\mathbf c_{\\mathrm\nL} \\in (1,25)$ or equivalently to complex values of $\\gamma$ with $|\\gamma|=2$.\nMore precisely, we introduce a canonical supercritical LQG surface with the\ntopology of the disk. We then show that for each $\\mathbf c_{\\mathrm L} \\in\n(1,25)$ there is a coupling of this LQG surface with a conformal loop ensemble\nwith parameter $\\kappa=4$ (CLE$_4$) wherein the LQG surfaces parametrized by\nthe regions enclosed by the CLE$_4$ loops are conditionally independent\nsupercritical LQG disks given their boundary lengths. In this coupling, the\nCLE$_4$ is neither determined by nor independent from the LQG. Guided by our\ncoupling result, we exhibit a combinatorially natural family of loop-decorated\nrandom planar maps whose scaling limit we conjecture to be the supercritical\nLQG disk coupled to CLE$_4$. We include a substantial list of open problems.\n","versions":"[{'version': 'v1', 'created': 'Tue, 22 Aug 2023 23:43:54 GMT'}, {'version': 'v2', 'created': 'Sun, 22 Oct 2023 23:33:23 GMT'}, {'version': 'v3', 'created': 'Tue, 30 Apr 2024 12:01:48 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Mar 2025 04:37:44 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Ang', 'Morris', ''], ['Gwynne', 'Ewain', '']]","extracted_entities":"[{'text': 'scaling limit', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling limit","similarity_score":0.6638188362}
{"id":2404.02175,"submitter":"Javier Mar\\'in","authors":"Javier Marin","title":"Symmetries, Scaling Laws and Phase Transitions in Consumer Advertising\n  Response","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.soc-ph cs.LG q-fin.GN","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Understanding how consumers respond to business advertising efforts is\nessential for optimizing marketing investment. This research introduces a new\nmodeling approach based on the concepts of symmetries and scaling laws in\nphysics to describe consumer response to advertising dynamics. Drawing from\nmathematical frameworks used in physics and social sciences, we propose a model\nthat accounts for a key aspect: the saturation effect. The model is validated\nagainst commonly used models, including the Michaelis-Menten and Hill\nequations, showing its ability to better capture nonlinearities in advertising\neffects. We introduce new key parameters like Marketing Sensitivity, Response\nSensitivity, and Behavioral Sensitivit, that offer additional insights into the\ndrivers of audience engagement and advertising performance. Our model provides\na rigorous yet practical tool for understanding audience behavior, contributing\nto the improvement of budget allocation strategies.\n","versions":"[{'version': 'v1', 'created': 'Mon, 1 Apr 2024 11:23:31 GMT'}, {'version': 'v2', 'created': 'Fri, 18 Oct 2024 06:33:19 GMT'}, {'version': 'v3', 'created': 'Sun, 10 Nov 2024 10:10:44 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Mar 2025 18:32:02 GMT'}, {'version': 'v5', 'created': 'Thu, 13 Mar 2025 08:48:26 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Marin', 'Javier', '']]","extracted_entities":"[{'text': 'scaling laws', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling laws","similarity_score":0.9373526573}
{"id":2404.1515,"submitter":"Katerina Batziakoudi","authors":"Katerina Batziakoudi, Florent Cabric, St\\'ephanie Rey, Jean-Daniel\n  Fekete","title":"Lost in Magnitudes: Exploring Visualization Designs for Large Value\n  Ranges","comments":"CHI25, Yokohama, Japan","journal-ref":null,"doi":"10.1145\/3706598.3713487","report-no":null,"categories":"cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We explore the design of visualizations for values spanning multiple orders\nof magnitude; we call them Orders of Magnitude Values (OMVs). Visualization\nresearchers have shown that separating OMVs into two components, the mantissa\nand the exponent, and encoding them separately overcomes limitations of linear\nand logarithmic scales. However, only a small number of such visualizations\nhave been tested, and the design guidelines for visualizing the mantissa and\nexponent separately remain under-explored. To initiate this exploration, better\nunderstand the factors influencing the effectiveness of these visualizations,\nand create guidelines, we adopt a multi-stage workflow. We introduce a design\nspace for visualizing mantissa and exponent, systematically generating and\nqualitatively evaluating all possible visualizations within it. From this\nevaluation, we derive guidelines. We select two visualizations that align with\nour guidelines and test them using a crowdsourcing experiment, showing they\nfacilitate quantitative comparisons and increase confidence in interpretation\ncompared to the state-of-the-art.\n","versions":"[{'version': 'v1', 'created': 'Tue, 23 Apr 2024 15:52:53 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 09:26:35 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Batziakoudi', 'Katerina', ''], ['Cabric', 'Florent', ''], ['Rey', 'St\u00e9phanie', ''], ['Fekete', 'Jean-Daniel', '']]","extracted_entities":"[{'text': 'OMVs', 'label': 'LLMs'}, {'text': 'linear\\nand logarithmic scales', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"linear\nand logarithmic scales","similarity_score":0.5184875727}
{"id":2404.17365,"submitter":"Fleur Hendriks","authors":"Fleur Hendriks (1), Vlado Menkovski (1), Martin Do\\v{s}k\\'a\\v{r} (2),\n  Marc G. D. Geers (1), Ond\\v{r}ej Roko\\v{s} (1) ((1) Eindhoven University of\n  Technology, (2) Czech Technical University in Prague)","title":"Similarity Equivariant Graph Neural Networks for Homogenization of\n  Metamaterials","comments":"60 pages, 22 figures. Published in CMAME (Computer Methods in Applied\n  Mechanics and Engineering)","journal-ref":null,"doi":"10.1016\/j.cma.2025.117867","report-no":null,"categories":"cond-mat.soft cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Soft, porous mechanical metamaterials exhibit pattern transformations that\nmay have important applications in soft robotics, sound reduction and\nbiomedicine. To design these innovative materials, it is important to be able\nto simulate them accurately and quickly, in order to tune their mechanical\nproperties. Since conventional simulations using the finite element method\nentail a high computational cost, in this article we aim to develop a machine\nlearning-based approach that scales favorably to serve as a surrogate model. To\nensure that the model is also able to handle various microstructures, including\nthose not encountered during training, we include the microstructure as part of\nthe network input. Therefore, we introduce a graph neural network that predicts\nglobal quantities (energy, stress stiffness) as well as the pattern\ntransformations that occur (the kinematics). To make our model as accurate and\ndata-efficient as possible, various symmetries are incorporated into the model.\nThe starting point is an E(n)-equivariant graph neural network (which respects\ntranslation, rotation and reflection) that has periodic boundary conditions\n(i.e., it is in-\/equivariant with respect to the choice of RVE), is scale\nin-\/equivariant, can simulate large deformations, and can predict scalars,\nvectors as well as second and fourth order tensors (specifically energy, stress\nand stiffness). The incorporation of scale equivariance makes the model\nequivariant with respect to the similarities group, of which the Euclidean\ngroup E(n) is a subgroup. We show that this network is more accurate and\ndata-efficient than graph neural networks with fewer symmetries. To create an\nefficient graph representation of the finite element discretization, we use\nonly the internal geometrical hole boundaries from the finite element mesh to\nachieve a better speed-up and scaling with the mesh size.\n","versions":"[{'version': 'v1', 'created': 'Fri, 26 Apr 2024 12:30:32 GMT'}, {'version': 'v2', 'created': 'Mon, 9 Dec 2024 15:10:19 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 14:48:27 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Hendriks', 'Fleur', ''], ['Menkovski', 'Vlado', ''], ['Do\u0161k\u00e1\u0159', 'Martin', ''], ['Geers', 'Marc G. D.', ''], ['Roko\u0161', 'Ond\u0159ej', '']]","extracted_entities":"[{'text': 'graph neural network', 'label': 'Neural Language Model'}, {'text': 'graph neural network', 'label': 'Neural Language Model'}, {'text': 'scale equivariance', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scale equivariance","similarity_score":0.5122733116}
{"id":2406.0534,"submitter":"Yucheng Liu","authors":"Yucheng Liu and Xiaodong Li","title":"Selecting the Number of Communities for Weighted Degree-Corrected\n  Stochastic Block Models","comments":"4 figures, 2 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ME stat.ML","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We investigate how to select the number of communities for weighted networks\nwithout a full likelihood modeling. First, we propose a novel weighted\ndegree-corrected stochastic block model (DCSBM), where the mean adjacency\nmatrix is modeled in the same way as in the standard DCSBM, while the variance\nprofile matrix is assumed to be related to the mean adjacency matrix through a\ngiven variance function. Our method of selecting the number of communities is\nbased on a sequential testing framework. In each step, the weighted DCSBM is\nfitted via some spectral clustering method. A key component of our method is\nmatrix scaling on the estimated variance profile matrix. The resulting scaling\nfactors can be used to normalize the adjacency matrix, from which the test\nstatistic is then obtained. Under mild conditions on the weighted DCSBM, our\nproposed procedure is shown to be consistent in estimating the true number of\ncommunities. Numerical experiments on both simulated and real-world network\ndata demonstrate the desirable empirical properties of our method.\n","versions":"[{'version': 'v1', 'created': 'Sat, 8 Jun 2024 03:47:38 GMT'}, {'version': 'v2', 'created': 'Tue, 8 Oct 2024 06:01:43 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 06:43:46 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Liu', 'Yucheng', ''], ['Li', 'Xiaodong', '']]","extracted_entities":"[{'text': 'matrix scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"matrix scaling","similarity_score":0.5287396908}
{"id":2406.1341,"submitter":"Fabian Thielemann","authors":"Fabian Thielemann, Joachim Siemund, Daniel von Schoenfeld, Wei Wu,\n  Pascal Weckesser, Krzysztof Jachymski, Thomas Walker and Tobias Schaetz","title":"Exploring atom-ion Feshbach resonances below the s-wave limit","comments":null,"journal-ref":"Phys. Rev. X 15, 011051 (2025)","doi":"10.1103\/PhysRevX.15.011051","report-no":null,"categories":"physics.atom-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Revealing the quantum properties of matter requires a high degree of\nexperimental control accompanied by a profound theoretical understanding. At\nultracold temperatures, quantities that appear continuous in everyday life,\nsuch as the motional angular momentum of two colliding particles, become\nquantized, leaving a measurable imprint on experimental results. Embedding a\nsingle particle within a larger quantum bath at lowest temperatures can result\nin resonant partial-wave dependent interaction, whose strength near zero energy\nis dictated by universal threshold scaling laws. Hybrid atom-ion systems have\nemerged as a novel platform in which a single charged atom in an ultracold bath\nserves as a well-controlled impurity of variable energy. However, entering the\nlow-energy s-wave regime and exploring the role of higher-partial-wave\nscattering within has remained an open challenge. Here, we immerse a Barium ion\nin a cloud of ultracold spin-polarized Lithium atoms, realize tunable collision\nenergies below the s-wave limit and explore resonant higher-partial-wave\nscattering by studying the energy dependence of Feshbach resonances. Utilizing\nprecise electric field control, we tune the collision energy over four orders\nof magnitude, reaching from the many-parital-wave to the s-wave regime. At the\nlowest energies, we probe the energy dependence of an isolated s-wave Feshbach\nresonance and introduce a theoretical model that allows to distinguish it from\nhigher-partial-wave resonances. Additionally, at energies around the p-wave\nbarrier, we find and identify an open-channel f-wave resonance, consistent with\nthreshold laws. Our findings highlight and benchmark the importance of\nhigher-partial-wave scattering well within the s-wave regime and offer control\nover chemical reactions and complex many-body dynamics in atom-ion ensembles -\non the level of individual angular momentum quanta.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Jun 2024 09:58:49 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 10:46:04 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Thielemann', 'Fabian', ''], ['Siemund', 'Joachim', ''], ['von Schoenfeld', 'Daniel', ''], ['Wu', 'Wei', ''], ['Weckesser', 'Pascal', ''], ['Jachymski', 'Krzysztof', ''], ['Walker', 'Thomas', ''], ['Schaetz', 'Tobias', '']]","extracted_entities":"[{'text': 'universal threshold scaling laws', 'label': 'Scaling law'}, {'text': 'threshold laws', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"universal threshold scaling laws","similarity_score":0.660700798}
{"id":2407.05147,"submitter":"Qi-Ming Chen","authors":"Aarne Ker\\\"anen, Qi-Ming Chen, Andr\\'as Gunyh\\'o, Priyank Singh, Jian\n  Ma, Visa Vesterinen, Joonas Govenius, Mikko M\\\"ott\\\"onen","title":"Correlation measurement of propagating microwave photons at millikelvin","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.mes-hall","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Microwave photons are important carriers of quantum information in many\npromising platforms for quantum computing. They can be routinely generated,\ncontrolled, and teleported in experiments, indicating a variety of applications\nin quantum technology. However, observation of quantum statistical properties\nof microwave photons remains demanding: The energy of several microwave photons\nis considerably smaller than the thermal fluctuation of any room-temperature\ndetector, while amplification necessarily induces noise. Here, we present a\nmeasurement technique with a nanobolometer that directly measures the photon\nstatistics at the millikelvin temperature and overcomes this trade-off. We\napply our method to thermal states generated by a blackbody radiator operating\nin the regime of circuit quantum electrodynamics. We demonstrate the photon\nnumber resolvedness of the nanobolometer, and reveal the n(n+1)-scaling law of\nthe photon number variance as indicated by the Bose--Einstein distribution. By\nengineering the coherent and incoherent proportions of the input field, we\nobserve the transition between super-Poissonian and Poissonian statistics of\nthe microwave photons from the bolometric second-order correlation measurement.\nThis technique is poised to serve in fundamental tests of quantum mechanics\nwith microwave photons and function as a scalable readout solution for a\nquantum information processor.\n","versions":"[{'version': 'v1', 'created': 'Sat, 6 Jul 2024 18:15:08 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 12:14:38 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Ker\u00e4nen', 'Aarne', ''], ['Chen', 'Qi-Ming', ''], ['Gunyh\u00f3', 'Andr\u00e1s', ''], ['Singh', 'Priyank', ''], ['Ma', 'Jian', ''], ['Vesterinen', 'Visa', ''], ['Govenius', 'Joonas', ''], ['M\u00f6tt\u00f6nen', 'Mikko', '']]","extracted_entities":"[{'text': 'n(n+1)-scaling law', 'label': 'Scaling law'}, {'text': 'Bose--Einstein distribution', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"n(n+1)-scaling law","similarity_score":0.7386004925}
{"id":2407.0602,"submitter":"Zhuo Liu","authors":"Zhuo Liu, Caio Silva, Lucio M. Milanese, Muni Zhou, Noah R. Mandell,\n  and Nuno F. Loureiro","title":"Electron-only reconnection and inverse magnetic-energy transfer at\n  sub-ion scales","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.plasm-ph astro-ph.SR physics.space-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We derive, and validate numerically, an analytical model for electron-only\nmagnetic reconnection applicable to strongly magnetized plasmas. Our model\npredicts sub-ion-scale reconnection rates significantly higher than those\npertaining to large-scale reconnection, aligning with recent observations and\nsimulations. We apply this reconnection model to the problem of inverse\nmagnetic energy transfer at sub-ion scales. We derive time-dependent scaling\nlaws for the magnetic energy decay and the typical magnetic structure\ndimensions that differ from those previously found in the MHD regime. These\nscaling laws are validated via two- and three-dimensional simulations,\ndemonstrating that sub-ion scale magnetic fields can reach large, system-size\nscales via successive coalescence.\n","versions":"[{'version': 'v1', 'created': 'Mon, 8 Jul 2024 15:09:04 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 04:08:15 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Liu', 'Zhuo', ''], ['Silva', 'Caio', ''], ['Milanese', 'Lucio M.', ''], ['Zhou', 'Muni', ''], ['Mandell', 'Noah R.', ''], ['Loureiro', 'Nuno F.', '']]","extracted_entities":"[{'text': 'time-dependent scaling\\nlaws', 'label': 'Scaling law'}, {'text': 'scaling laws', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling laws","similarity_score":0.9373526573}
{"id":2407.19466,"submitter":"Michio Otsuki","authors":"Hiroki Oba and Michio Otsuki","title":"Scaling laws for velocity profile of granular flow in rotating drums","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.soft cond-mat.stat-mech","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We numerically analyze the steady flow of two-dimensional granular materials\nin a rotating drum using the discrete element method and a continuum model. The\nvelocity fields obtained from both methods are quantitatively consistent. Two\ndistinct regions exist in the granular flow: the surface flow layer and the\nstatic flow regime associated with the rigid rotation near the bottom. The\nthickness of the surface flow layer increases with the drum diameter and shows\na slight dependence on the angular velocity of the rotating drum. We derived\nscaling laws for the velocity profile and surface layer thickness using\ndimensional analysis applied to the continuum equations. The validity of the\nscaling laws is confirmed numerically.\n","versions":"[{'version': 'v1', 'created': 'Sun, 28 Jul 2024 11:31:09 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 02:14:18 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Oba', 'Hiroki', ''], ['Otsuki', 'Michio', '']]","extracted_entities":"[{'text': 'scaling laws', 'label': 'Scaling law'}, {'text': 'scaling laws', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling laws","similarity_score":0.9373526573}
{"id":2407.20342,"submitter":"Manas Kulkarni","authors":"Manas Kulkarni, Satya N. Majumdar, Sanjib Sabhapandit","title":"Dynamically emergent correlations in bosons via quantum resetting","comments":"29 pages, 9 figures","journal-ref":"J. Phys. A: Math. Theor. 58 105003 (2025)","doi":"10.1088\/1751-8121\/adb6db","report-no":null,"categories":"cond-mat.quant-gas cond-mat.stat-mech quant-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We study the nonequilibrium stationary state (NESS) induced by quantum\nresetting of a system of $N$ noninteracting bosons in a harmonic trap. Our\nprotocol consists of preparing initially the system in the ground state of a\nharmonic oscillator centered at $+a$, followed by a rapid quench where the\ncenter is shifted to $-a$ and the system is allowed to evolve unitarily up to a\nrandom Poissonian time $\\tau$ distributed via $r\\, e^{-r \\tau}$. Then the trap\ncenter is reset to $+a$ again and the system is assumed to cool instantaneously\nto the initial ground state. The system is again allowed to evolve unitarily in\nthe trap centered at $-a$ up to a random time, and the procedure is repeated.\nUnder repeated resetting, the system reaches a NESS where the positions of\nbosons get strongly correlated due to simultaneous resetting induced by the\ntrap. We fully characterize the steady state by analytically computing several\nphysical observables such as the average density, extreme value statistics,\norder and gap statistics, and also the distribution of the number of particles\nin a region $[-L,L]$, known as the full counting statistics (FCS). In\nparticular, we show that in the large $N$ limit, the scaling function\ndescribing the FCS exhibits a striking feature: it is supported over a\nnontrivial finite interval, and moreover is discontinuous at an interior point\nof the support. Our results are supported by numerical simulations. This is a\nrare example of a strongly correlated quantum many-body NESS where various\nobservables can be exactly computed.\n","versions":"[{'version': 'v1', 'created': 'Mon, 29 Jul 2024 18:00:35 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 04:27:48 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Kulkarni', 'Manas', ''], ['Majumdar', 'Satya N.', ''], ['Sabhapandit', 'Sanjib', '']]","extracted_entities":"[{'text': 'scaling function', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling function","similarity_score":0.622145474}
{"id":2407.20579,"submitter":"Anna Coerver","authors":"A. Coerver, J. A. Zebrowski, S. Takakura, W. L. Holzapfel, P. A. R.\n  Ade, A. J. Anderson, Z. Ahmed, B. Ansarinejad, M. Archipley, L. Balkenhol, D.\n  Barron, K. Benabed, A. N. Bender, B. A. Benson, F. Bianchini, L. E. Bleem, F.\n  R. Bouchet, L. Bryant, E. Camphuis, J. E. Carlstrom, T. W. Cecil, C. L.\n  Chang, P. Chaubal, P. M. Chichura, A. Chokshi, T.-L. Chou, T. M. Crawford, A.\n  Cukierman, C. Daley, T. de Haan, K. R. Dibert, M. A. Dobbs, A. Doussot, D.\n  Dutcher, W. Everett, C. Feng, K. R. Ferguson, K. Fichman, A. Foster, S.\n  Galli, A. E. Gambrel, R. W. Gardner, F. Ge, N. Goeckner-Wald, R. Gualtieri,\n  F. Guidi, S. Guns, N. W. Halverson, E. Hivon, G. P. Holder, J. C. Hood, A.\n  Hryciuk, N. Huang, F. Keruzore, A. R. Khalife, L. Knox, M. Korman, K.\n  Kornoelje, C.-L. Kuo, A. T. Lee, K. Levy, A. E. Lowitz, C. Lu, A. Maniyar, E.\n  S. Martsen, F. Menanteau, M. Millea, J. Montgomery, Y. Nakato, T. Natoli, G.\n  I. Noble, V. Novosad, Y. Omori, S. Padin, Z. Pan, P. Paschos, K. A. Phadke,\n  A. W. Pollak, K. Prabhu, W. Quan, M. Rahimi, A. Rahlin, C. L. Reichardt, M.\n  Rouble, J. E. Ruhl, E. Schiappucci, G. Smecher, J. A. Sobrin, A. A. Stark, J.\n  Stephen, A. Suzuki, C. Tandoi, K. L. Thompson, B. Thorne, C. Trendafilova, C.\n  Tucker, C. Umilta, J. D. Vieira, A. Vitrier, Y. Wan, G. Wang, N. Whitehorn,\n  W. L. K. Wu, V. Yefremenko, M. R. Young","title":"Measurement and Modeling of Polarized Atmosphere at the South Pole with\n  SPT-3G","comments":"25 pages, 28 figures","journal-ref":null,"doi":"10.3847\/1538-4357\/ada35d","report-no":null,"categories":"astro-ph.IM astro-ph.CO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present the detection and characterization of fluctuations in linearly\npolarized emission from the atmosphere above the South Pole. These measurements\nmake use of data from the SPT-3G receiver on the South Pole Telescope in three\nfrequency bands centered at 95, 150, and 220 GHz. We use the cross-correlation\nbetween detectors to produce an unbiased estimate of the power in Stokes I, Q,\nand U parameters on large angular scales. Our results are consistent with the\npolarized signal being produced by the combination of Rayleigh scattering of\nthermal radiation from the ground and thermal emission from a population of\nhorizontally aligned ice crystals with an anisotropic distribution described by\nKolmogorov turbulence. The measured spatial scaling, frequency scaling, and\nelevation dependence of the polarized emission are explained by this model.\nPolarized atmospheric emission has the potential to significantly impact\nobservations on the large angular scales being targeted by searches for\ninflationary B-mode CMB polarization. We present the distribution of measured\nangular power spectrum amplitudes in Stokes Q and I for 4 yr of Austral winter\nobservations, which can be used to simulate the impact of atmospheric\npolarization and intensity fluctuations at the South Pole on a specified\nexperiment and observation strategy. We present a mitigation strategy that\ninvolves both downweighting significantly contaminated observations and\nsubtracting a polarized atmospheric signal from the 150 GHz band maps. In\nobservations with the SPT-3G instrument, the polarized atmospheric signal is a\nwell-understood and subdominant contribution to the measured noise after\nimplementing the mitigation strategies described here.\n","versions":"[{'version': 'v1', 'created': 'Tue, 30 Jul 2024 06:24:49 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 22:59:34 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Coerver', 'A.', ''], ['Zebrowski', 'J. A.', ''], ['Takakura', 'S.', ''], ['Holzapfel', 'W. L.', ''], ['Ade', 'P. A. R.', ''], ['Anderson', 'A. J.', ''], ['Ahmed', 'Z.', ''], ['Ansarinejad', 'B.', ''], ['Archipley', 'M.', ''], ['Balkenhol', 'L.', ''], ['Barron', 'D.', ''], ['Benabed', 'K.', ''], ['Bender', 'A. N.', ''], ['Benson', 'B. A.', ''], ['Bianchini', 'F.', ''], ['Bleem', 'L. E.', ''], ['Bouchet', 'F. R.', ''], ['Bryant', 'L.', ''], ['Camphuis', 'E.', ''], ['Carlstrom', 'J. E.', ''], ['Cecil', 'T. W.', ''], ['Chang', 'C. L.', ''], ['Chaubal', 'P.', ''], ['Chichura', 'P. M.', ''], ['Chokshi', 'A.', ''], ['Chou', 'T. -L.', ''], ['Crawford', 'T. M.', ''], ['Cukierman', 'A.', ''], ['Daley', 'C.', ''], ['de Haan', 'T.', ''], ['Dibert', 'K. R.', ''], ['Dobbs', 'M. A.', ''], ['Doussot', 'A.', ''], ['Dutcher', 'D.', ''], ['Everett', 'W.', ''], ['Feng', 'C.', ''], ['Ferguson', 'K. R.', ''], ['Fichman', 'K.', ''], ['Foster', 'A.', ''], ['Galli', 'S.', ''], ['Gambrel', 'A. E.', ''], ['Gardner', 'R. W.', ''], ['Ge', 'F.', ''], ['Goeckner-Wald', 'N.', ''], ['Gualtieri', 'R.', ''], ['Guidi', 'F.', ''], ['Guns', 'S.', ''], ['Halverson', 'N. W.', ''], ['Hivon', 'E.', ''], ['Holder', 'G. P.', ''], ['Hood', 'J. C.', ''], ['Hryciuk', 'A.', ''], ['Huang', 'N.', ''], ['Keruzore', 'F.', ''], ['Khalife', 'A. R.', ''], ['Knox', 'L.', ''], ['Korman', 'M.', ''], ['Kornoelje', 'K.', ''], ['Kuo', 'C. -L.', ''], ['Lee', 'A. T.', ''], ['Levy', 'K.', ''], ['Lowitz', 'A. E.', ''], ['Lu', 'C.', ''], ['Maniyar', 'A.', ''], ['Martsen', 'E. S.', ''], ['Menanteau', 'F.', ''], ['Millea', 'M.', ''], ['Montgomery', 'J.', ''], ['Nakato', 'Y.', ''], ['Natoli', 'T.', ''], ['Noble', 'G. I.', ''], ['Novosad', 'V.', ''], ['Omori', 'Y.', ''], ['Padin', 'S.', ''], ['Pan', 'Z.', ''], ['Paschos', 'P.', ''], ['Phadke', 'K. A.', ''], ['Pollak', 'A. W.', ''], ['Prabhu', 'K.', ''], ['Quan', 'W.', ''], ['Rahimi', 'M.', ''], ['Rahlin', 'A.', ''], ['Reichardt', 'C. L.', ''], ['Rouble', 'M.', ''], ['Ruhl', 'J. E.', ''], ['Schiappucci', 'E.', ''], ['Smecher', 'G.', ''], ['Sobrin', 'J. A.', ''], ['Stark', 'A. A.', ''], ['Stephen', 'J.', ''], ['Suzuki', 'A.', ''], ['Tandoi', 'C.', ''], ['Thompson', 'K. L.', ''], ['Thorne', 'B.', ''], ['Trendafilova', 'C.', ''], ['Tucker', 'C.', ''], ['Umilta', 'C.', ''], ['Vieira', 'J. D.', ''], ['Vitrier', 'A.', ''], ['Wan', 'Y.', ''], ['Wang', 'G.', ''], ['Whitehorn', 'N.', ''], ['Wu', 'W. L. K.', ''], ['Yefremenko', 'V.', ''], ['Young', 'M. R.', '']]","extracted_entities":"[{'text': 'frequency scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"frequency scaling","similarity_score":0.5297503471}
{"id":2408.10321,"submitter":"Jonathan Classen-Howes","authors":"Jonathan Classen-Howes and Riccardo Senese and Abhishodh Prakash","title":"Universal Freezing Transitions of Dipole-Conserving Chains","comments":"31 + 29 pages, 15 figures, 3 tables. New section on critical scaling,\n  various minor improvements","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.str-el cond-mat.dis-nn cond-mat.stat-mech","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We argue for the existence of a universal phase diagram of quantum chains\nwith range-$k$ interactions subject to the conservation of a total charge and\nits dipole moment. These systems exhibit \"freezing\" transitions between\nstrongly and weakly Hilbert-space-fragmented phases as the charge filling $\\nu$\nis varied. We show that these continuous phase transitions occur at a critical\ncharge filling of $\\nu_c=(k-2)^{-1}$ independently of the on-site Hilbert space\ndimension $d$. To this end, we analytically prove that for any $d$, any state\nfor $\\nu<\\nu_c$ hosts a finite density of sites belonging to \"blockages\", which\nwe define as subregions of the chain across which transport of charge and\ndipole moment cannot occur. Some blockages arise from sequences of frozen\nsites, i.e. sites with an unchanging on-site charge, while others do not\ninvolve frozen sites at all. We prove that the presence of blockages implies\nstrong fragmentation of typical symmetry sectors into Krylov subspaces that\neach form an exponentially vanishing fraction of the total sector. By studying\nthe distribution of blockages we analytically characterise how typical states\nare subdivided into dynamically disconnected local \"active bubbles\", and prove\nthat typical eigenstates at these charge fillings exhibit area-law entanglement\nentropy, with rare \"inverse quantum many-body scar\" eigenstates featuring\nnon-area-law scaling. We also numerically show that for $\\nu>\\nu_c$ and\narbitrary $d$, typical symmetry sectors are weakly fragmented, with their\ndominant Krylov sectors constituted of states that are free of blockages. We\nanalytically derive some critical exponents characterizing the transition, and\nnumerically determine the density of blockages at $\\nu=\\nu_c$, with clear\nimplications for transport at the critical point. Finally, we investigate the\nproperties of certain special-case models for which no phase transitions occur.\n","versions":"[{'version': 'v1', 'created': 'Mon, 19 Aug 2024 18:00:06 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 00:56:23 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Classen-Howes', 'Jonathan', ''], ['Senese', 'Riccardo', ''], ['Prakash', 'Abhishodh', '']]","extracted_entities":"[{'text': 'area-law entanglement\\nentropy', 'label': 'Scaling law'}, {'text': 'non-area-law scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"non-area-law scaling","similarity_score":0.7592328191}
{"id":2409.00425,"submitter":"Ning Liu","authors":"Lu Chen, Baopi Liu, and Ning Liu","title":"Phase behaviors and dynamics of active particle systems in double-well\n  potential","comments":"7 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.soft cond-mat.stat-mech physics.bio-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this study, we investigate the behaviors and dynamics of self-propelled\nparticles with active reorientation (AR) in a double-well potential. We explore\nthe competition between AR and external potentials, revealing that\nself-propelled particles exhibit flocking and clustering behaviors in an\nasymmetric potential trap. Through molecular dynamics simulations, we obtain a\nphase diagram that illustrates flocking behavior as a function of active\nreorientation and potential asymmetry. We compare the responses of inactive and\nactive particles to the potential, finding that active reorientation\nsignificantly increases aggregation on one side of the asymmetric potential\nwell. Additionally, by calculating the mean squared displacement and scaling\nexponent, we identify distinct diffusion regimes. Our findings demonstrate that\nactive particles with active reorientation are more sensitive to variations in\ndouble-well potentials.\n","versions":"[{'version': 'v1', 'created': 'Sat, 31 Aug 2024 11:53:02 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 09:17:41 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chen', 'Lu', ''], ['Liu', 'Baopi', ''], ['Liu', 'Ning', '']]","extracted_entities":"[{'text': 'scaling\\nexponent', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling\nexponent","similarity_score":0.6063866615}
{"id":2409.11697,"submitter":"Thieu Vo","authors":"Viet-Hoang Tran and Thieu N. Vo and Tho H. Tran and An T. Nguyen and\n  Tan M. Nguyen","title":"Monomial Matrix Group Equivariant Neural Functional Networks","comments":"10 pages in the main text. Published at NeurIPS 2024. The code is\n  available at https:\/\/github.com\/MathematicalAI-NUS\/Monomial-NFN","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Neural functional networks (NFNs) have recently gained significant attention\ndue to their diverse applications, ranging from predicting network\ngeneralization and network editing to classifying implicit neural\nrepresentation. Previous NFN designs often depend on permutation symmetries in\nneural networks' weights, which traditionally arise from the unordered\narrangement of neurons in hidden layers. However, these designs do not take\ninto account the weight scaling symmetries of $\\ReLU$ networks, and the weight\nsign flipping symmetries of $\\sin$ or $\\Tanh$ networks. In this paper, we\nextend the study of the group action on the network weights from the group of\npermutation matrices to the group of monomial matrices by incorporating\nscaling\/sign-flipping symmetries. Particularly, we encode these\nscaling\/sign-flipping symmetries by designing our corresponding equivariant and\ninvariant layers. We name our new family of NFNs the Monomial Matrix Group\nEquivariant Neural Functional Networks (Monomial-NFN). Because of the expansion\nof the symmetries, Monomial-NFN has much fewer independent trainable parameters\ncompared to the baseline NFNs in the literature, thus enhancing the model's\nefficiency. Moreover, for fully connected and convolutional neural networks, we\ntheoretically prove that all groups that leave these networks invariant while\nacting on their weight spaces are some subgroups of the monomial matrix group.\nWe provide empirical evidence to demonstrate the advantages of our model over\nexisting baselines, achieving competitive performance and efficiency.\n","versions":"[{'version': 'v1', 'created': 'Wed, 18 Sep 2024 04:36:05 GMT'}, {'version': 'v2', 'created': 'Thu, 31 Oct 2024 22:55:21 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 15:36:01 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Tran', 'Viet-Hoang', ''], ['Vo', 'Thieu N.', ''], ['Tran', 'Tho H.', ''], ['Nguyen', 'An T.', ''], ['Nguyen', 'Tan M.', '']]","extracted_entities":"[{'text': 'weight scaling symmetries', 'label': 'Scaling law'}, {'text': 'scaling\/sign-flipping symmetries', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"weight scaling symmetries","similarity_score":0.5615555048}
{"id":2410.04229,"submitter":"Daniel de Andres","authors":"Andr\\'es Caro, Daniel de Andres, Weiguang Cui, Gustavo Yepes, Marco De\n  Petris, Antonio Ferragamo, F\\'elicien Schiltz and Am\\'elie Nef","title":"Deep Learning generated observations of galaxy clusters from\n  dark-matter-only simulations","comments":"16 pages, 13 Figures. Accepted in RASTI","journal-ref":null,"doi":"10.1093\/rasti\/rzaf007","report-no":null,"categories":"astro-ph.CO astro-ph.GA astro-ph.IM","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Hydrodynamical simulations play a fundamental role in modern cosmological\nresearch, serving as a crucial bridge between theoretical predictions and\nobservational data. However, due to their computational intensity, these\nsimulations are currently constrained to relatively small volumes. Therefore,\nthis study investigates the feasibility of utilising dark matter-only\nsimulations to generate observable maps of galaxy clusters using a deep\nlearning approach based on the U-Net architecture. We focus on reconstructing\nCompton-y parameter maps (SZ maps) and bolometric X-ray surface brightness maps\n(X-ray maps) from total mass density maps. We leverage data from \\textsc{The\nThree Hundred} simulations, selecting galaxy clusters ranging in mass from\n$10^{13.5} h^{-1}M_{\\odot}\\leq M_{200} \\leq 10^{15.5} h^{-1}M_{\\odot}$. Despite\nthe machine learning models being independent of baryonic matter assumptions, a\nnotable limitation is their dependency on the underlying physics of\nhydrodynamical simulations. To evaluate the reliability of our generated\nobservable maps, we employ various metrics and compare the observable-mass\nscaling relations. For clusters with masses greater than $2 \\times 10^{14}\nh^{-1} M_{\\odot}$, the predictions show excellent agreement with the\nground-truth datasets, with percentage errors averaging (0.5 $\\pm$ 0.1)\\% for\nthe parameters of the scaling laws.\n","versions":"[{'version': 'v1', 'created': 'Sat, 5 Oct 2024 17:07:08 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 14:42:07 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Caro', 'Andr\u00e9s', ''], ['de Andres', 'Daniel', ''], ['Cui', 'Weiguang', ''], ['Yepes', 'Gustavo', ''], ['De Petris', 'Marco', ''], ['Ferragamo', 'Antonio', ''], ['Schiltz', 'F\u00e9licien', ''], ['Nef', 'Am\u00e9lie', '']]","extracted_entities":"[{'text': 'observable-mass\\nscaling relations', 'label': 'Scaling law'}, {'text': 'scaling laws', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling laws","similarity_score":0.9373526573}
{"id":2410.10599,"submitter":"Christian Hughes","authors":"Christian Hughes, Houston Warren, Darrick Lee, Fabio Ramos, Ian\n  Abraham","title":"Ergodic Trajectory Optimization on Generalized Domains Using Maximum\n  Mean Discrepancy","comments":"6 pages (excluding references), 1 table, 8 figures, submitted to ICRA\n  2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  We present a novel formulation of ergodic trajectory optimization that can be\nspecified over general domains using kernel maximum mean discrepancy. Ergodic\ntrajectory optimization is an effective approach that generates coverage paths\nfor problems related to robotic inspection, information gathering problems, and\nsearch and rescue. These optimization schemes compel the robot to spend time in\na region proportional to the expected utility of visiting that region. Current\nmethods for ergodic trajectory optimization rely on domain-specific knowledge,\ne.g., a defined utility map, and well-defined spatial basis functions to\nproduce ergodic trajectories. Here, we present a generalization of ergodic\ntrajectory optimization based on maximum mean discrepancy that requires only\nsamples from the search domain. We demonstrate the ability of our approach to\nproduce coverage trajectories on a variety of problem domains including robotic\ninspection of objects with differential kinematics constraints and on Lie\ngroups without having access to domain specific knowledge. Furthermore, we show\nfavorable computational scaling compared to existing state-of-the-art methods\nfor ergodic trajectory optimization with a trade-off between domain specific\nknowledge and computational scaling, thus extending the versatility of ergodic\ncoverage on a wider application domain.\n","versions":"[{'version': 'v1', 'created': 'Mon, 14 Oct 2024 15:10:35 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 17:35:56 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Hughes', 'Christian', ''], ['Warren', 'Houston', ''], ['Lee', 'Darrick', ''], ['Ramos', 'Fabio', ''], ['Abraham', 'Ian', '']]","extracted_entities":"[{'text': 'computational scaling', 'label': 'Scaling law'}, {'text': 'computational scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"computational scaling","similarity_score":0.6406878233}
{"id":2411.19843,"submitter":"Jong-Phil Lee","authors":"Jong-Phil Lee","title":"New physics effects on $B\\to D^{(*)}\\tau\\nu$ decays","comments":"20 pages, 8 figures; more discussions","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We investigate new physics effects on $B\\to D^{(*)}\\tau\\nu$ decays in a\ngeneral and model-independent way. The $\\chi^2$ fits for fractions of the\nbranching ratios $R(D^{(*)})$ and other polarization parameters are\nimplemented. We parameterize the relevant Wilson coefficients with a new\nphysics scale and its power together with combined fermionic couplings.\nConstraints from $B_c\\to\\tau\\nu$ are imposed such that its branching ratio is\nless than 30%. For a moderate range of our parameters we find that the new\nphysics scale goes up to $\\lesssim 27$ TeV for ordinary new particle\ncontributions. It turns out that the polarization asymmetry of $\\tau$ for $B\\to\nD$ transition can be negative only for a few combinations of the new physics\noperators. We also discuss related processes $B_c\\to J\/\\Psi\\tau\\nu$ and\n$\\Lambda_b\\to\\Lambda_c\\tau\\nu$ decays.\n","versions":"[{'version': 'v1', 'created': 'Fri, 29 Nov 2024 16:58:04 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 11:21:09 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Lee', 'Jong-Phil', '']]","extracted_entities":"[{'text': 'new\\nphysics scale', 'label': 'Scaling law'}, {'text': 'new\\nphysics scale', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"new\nphysics scale","similarity_score":0.5770435333}
{"id":2411.19951,"submitter":"Shukang Yin","authors":"Shukang Yin, Chaoyou Fu, Sirui Zhao, Yunhang Shen, Chunjiang Ge, Yan\n  Yang, Zuwei Long, Yuhan Dai, Yongdong Luo, Haoyu Cao, Tong Xu, Xing Sun,\n  Caifeng Shan, Ran He, Enhong Chen","title":"Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation","comments":"Project page: https:\/\/github.com\/VITA-MLLM\/Sparrow","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent years have witnessed the success of Multimodal Large Language Models\n(MLLMs) in the vision understanding domain. The success of these models can\nlargely be attributed to the dominant scaling law, which states that larger\nparameter sizes and data volumes contribute to better performance. Notably,\ndata scaling has mainly been powered by automatic data pipelines, which center\naround the self-instruction of LLMs. The paradigm has been taken for granted\nfor quite some time, but the study of the effectiveness of scaling with these\ndata has been neglected for a long time. In this context, this work revisits\nscaling with synthetic data and focuses on developing video-LLMs from a\ndata-centric perspective. Our main study approach is fine-tuning pre-trained\nimage-LLMs with video data and investigating learning efficiency through data\nscaling. Results from our preliminary experiments reveal a low learning\nefficiency phenomenon when simply scaling up video data samples, which, through\nour probing, can be ascribed to a lack of instruction diversity. Aiming at this\nissue, we propose a data augmentation method called Sparrow, which synthesizes\nvideo-like samples from pure text instruction data. Mixing these synthetic\nsamples with the video data enables a more efficient training scheme. Through\ncomprehensive experiments, we demonstrate that our proposed method achieves\nperformance comparable to or even superior to baselines trained with many more\nsamples. Meanwhile, we find that incorporating these synthetic samples can\nboost the performance of long video understanding without training with long\nvideo data. The code and data examples are available at\nhttps:\/\/github.com\/VITA-MLLM\/Sparrow.\n","versions":"[{'version': 'v1', 'created': 'Fri, 29 Nov 2024 18:59:54 GMT'}, {'version': 'v2', 'created': 'Mon, 2 Dec 2024 06:54:47 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 15:44:34 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Yin', 'Shukang', ''], ['Fu', 'Chaoyou', ''], ['Zhao', 'Sirui', ''], ['Shen', 'Yunhang', ''], ['Ge', 'Chunjiang', ''], ['Yang', 'Yan', ''], ['Long', 'Zuwei', ''], ['Dai', 'Yuhan', ''], ['Luo', 'Yongdong', ''], ['Cao', 'Haoyu', ''], ['Xu', 'Tong', ''], ['Sun', 'Xing', ''], ['Shan', 'Caifeng', ''], ['He', 'Ran', ''], ['Chen', 'Enhong', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'dominant scaling law', 'label': 'Scaling law'}, {'text': 'VITA-MLLM', 'label': 'Open-source LLMs'}]","assigned_concept":"Scaling law","matched_keyword":"dominant scaling law","similarity_score":0.8317065239}
{"id":2412.18602,"submitter":"Qiang Miao","authors":"Qiang Miao, Tianyi Wang, Kenneth R. Brown, Thomas Barthel, Marko\n  Cetina","title":"Probing Entanglement Scaling Across a Quantum Phase Transition on a\n  Quantum Computer","comments":"16 pages, 13 figures, minor improvements","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The investigation of strongly-correlated quantum matter is difficult due to\nthe curse of dimensionality and intricate entanglement structures. These\nchallenges are particularly pronounced in the vicinity of continuous quantum\nphase transitions, where quantum fluctuations manifest across all length\nscales. While quantum simulators give controlled access to a number of strongly\ncorrelated systems, the study of critical phenomena has been hampered by\nfinite-size effects arising from diverging correlation lengths. Moreover, the\nexperimental investigation of entanglement in many-body systems has been\nhindered by limitations in measurement protocols. To address these challenges,\nwe employ the multiscale entanglement renormalization ansatz (MERA) and\nimplement a holographic scheme for subsystem tomography on a fully-connected\ntrapped-ion quantum computer. Our method accurately represents infinite systems\nand long-range correlations with few qubits, facilitating the efficient\nextraction of observables and entanglement properties, even at criticality. We\nobserve a quantum phase transition with spontaneous symmetry breaking and\nreveal the evolution of entanglement properties across the critical point. For\nthe first time, we demonstrate log-law scaling of subsystem entanglement\nentropies at criticality on a digital quantum computer. This achievement\nhighlights the potential of MERA for the investigation of strongly-correlated\nmany-body systems on quantum computers.\n","versions":"[{'version': 'v1', 'created': 'Tue, 24 Dec 2024 18:56:44 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 17:53:35 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Miao', 'Qiang', ''], ['Wang', 'Tianyi', ''], ['Brown', 'Kenneth R.', ''], ['Barthel', 'Thomas', ''], ['Cetina', 'Marko', '']]","extracted_entities":"[{'text': 'entanglement', 'label': 'quantisation'}, {'text': 'entanglement', 'label': 'quantisation'}, {'text': 'entanglement', 'label': 'quantisation'}, {'text': 'log-law scaling', 'label': 'Scaling law'}, {'text': 'entanglement', 'label': 'quantisation'}]","assigned_concept":"Scaling law","matched_keyword":"log-law scaling","similarity_score":0.7648306489}
{"id":2501.06838,"submitter":"Du Chen","authors":"Du Chen, Liyi Chen, Zhengqiang Zhang, Lei Zhang","title":"Generalized and Efficient 2D Gaussian Splatting for Arbitrary-scale\n  Super-Resolution","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Implicit Neural Representation (INR) has been successfully employed for\nArbitrary-scale Super-Resolution (ASR). However, INR-based models need to query\nthe multi-layer perceptron module numerous times and render a pixel in each\nquery, resulting in insufficient representation capability and computational\nefficiency. Recently, Gaussian Splatting (GS) has shown its advantages over INR\nin both visual quality and rendering speed in 3D tasks, which motivates us to\nexplore whether GS can be employed for the ASR task. However, directly applying\nGS to ASR is exceptionally challenging because the original GS is an\noptimization-based method through overfitting each single scene, while in ASR\nwe aim to learn a single model that can generalize to different images and\nscaling factors. We overcome these challenges by developing two novel\ntechniques. Firstly, to generalize GS for ASR, we elaborately design an\narchitecture to predict the corresponding image-conditioned Gaussians of the\ninput low-resolution image in a feed-forward manner. Each Gaussian can fit the\nshape and direction of an area of complex textures, showing powerful\nrepresentation capability. Secondly, we implement an efficient differentiable\n2D GPU\/CUDA-based scale-aware rasterization to render super-resolved images by\nsampling discrete RGB values from the predicted continuous Gaussians. Via\nend-to-end training, our optimized network, namely GSASR, can perform ASR for\nany image and unseen scaling factors. Extensive experiments validate the\neffectiveness of our proposed method.\n","versions":"[{'version': 'v1', 'created': 'Sun, 12 Jan 2025 15:14:58 GMT'}, {'version': 'v2', 'created': 'Tue, 14 Jan 2025 14:09:23 GMT'}, {'version': 'v3', 'created': 'Thu, 27 Feb 2025 09:07:43 GMT'}, {'version': 'v4', 'created': 'Wed, 12 Mar 2025 12:03:16 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Chen', 'Du', ''], ['Chen', 'Liyi', ''], ['Zhang', 'Zhengqiang', ''], ['Zhang', 'Lei', '']]","extracted_entities":"[{'text': 'unseen scaling factors', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"unseen scaling factors","similarity_score":0.523728013}
{"id":2501.07643,"submitter":"Andrew Larkoski","authors":"Andrew J. Larkoski","title":"A Step Toward Interpretability: Smearing the Likelihood","comments":"16+1 pages, 3 figures; v2: JHEP version, added more motivation and\n  context in introduction, added more future directions and follow-ups in\n  conclusion, fixed some typos","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph cs.LG hep-ex stat.ML","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The problem of interpretability of machine learning architecture in particle\nphysics has no agreed-upon definition, much less any proposed solution. We\npresent a first modest step toward these goals by proposing a definition and\ncorresponding practical method for isolation and identification of relevant\nphysical energy scales exploited by the machine. This is accomplished by\nsmearing or averaging over all input events that lie within a prescribed metric\nenergy distance of one another and correspondingly renders any quantity\nmeasured on a finite, discrete dataset continuous over the dataspace. Within\nthis approach, we are able to explicitly demonstrate that (approximate) scaling\nlaws are a consequence of extreme value theory applied to analysis of the\ndistribution of the irreducible minimal distance over which a machine must\nextrapolate given a finite dataset. As an example, we study quark versus gluon\njet identification, construct the smeared likelihood, and show that\ndiscrimination power steadily increases as resolution decreases, indicating\nthat the true likelihood for the problem is sensitive to emissions at all\nscales.\n","versions":"[{'version': 'v1', 'created': 'Mon, 13 Jan 2025 19:09:42 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 16:35:05 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Larkoski', 'Andrew J.', '']]","extracted_entities":"[{'text': 'scaling\\nlaws', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling\nlaws","similarity_score":0.9373526573}
{"id":2501.12973,"submitter":"Da-Sol Joo","authors":"Da-Sol Joo","title":"A global similarity correction for the RANS modeling of natural\n  convection in unstably stratified flows","comments":"39 pages, 11 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.flu-dyn","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This study proposes a global similarity correction for Reynolds-averaged\nNavier--Stokes (RANS) modeling of buoyancy effects in unstably stratified\nflows. Conventional two-equation RANS models (e.g., the $k$-$\\varepsilon$\nmodel) lack a clear criterion for incorporating unstable buoyancy effects in\ntheir scale-determining equations (e.g., $\\varepsilon$-equation). To address\nthis gap, a global correction function is introduced, derived from a\ngeneralized algebraic formulation that incorporates available potential energy\nas an additional parameter. This function reproduces a global similarity law\ncommonly observed in natural convection flows--for instance, the correlation\namong the Nusselt, Rayleigh, and Prandtl numbers, which can be approximately\nexpressed as a single power law over a wide parameter range. A calibration\nmethod is proposed in which an approximate analytical solution for\nRayleigh--B\\'enard convection is obtained via equilibrium analysis, confirming\nthat the proposed model captures similarity relations not addressed by\nconventional one-point closures. Numerical results show significantly improved\nagreement with experimental data, accurately reproducing Nusselt number\ndependencies over broad ranges of Rayleigh and Prandtl numbers in unstably\nstratified flows, such as Rayleigh--B\\'enard convection and two types of\ninternally heated convection. The method remains fully compatible with standard\nRANS frameworks and reverts to traditional turbulence treatments in\nshear-driven flows where buoyant effects are negligible. By introducing only a\nsingle, simple, algebraic global function in the conventional\n$\\varepsilon$-equation, this approach significantly enhances the accuracy and\nrobustness of buoyancy-driven turbulence simulations.\n","versions":"[{'version': 'v1', 'created': 'Wed, 22 Jan 2025 15:57:40 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 16:27:13 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 03:28:37 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 02:47:37 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Joo', 'Da-Sol', '']]","extracted_entities":"[{'text': 'global similarity law', 'label': 'Scaling law'}, {'text': 'Nusselt', 'label': 'BERT'}, {'text': 'Prandtl', 'label': 'BERT'}, {'text': 'single power law', 'label': 'Scaling law'}, {'text': 'Nusselt', 'label': 'BERT'}, {'text': 'Prandtl', 'label': 'BERT'}]","assigned_concept":"Scaling law","matched_keyword":"global similarity law","similarity_score":0.5060133934}
{"id":2501.17515,"submitter":"Genevi\\`eve Dusson","authors":"Genevi\\`eve Dusson, Claudia Kl\\\"uppelberg, Gero Friesecke","title":"Copula methods for modeling pair densities in density functional theory","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.comp-ph math.ST stat.TH","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We propose a new approach towards approximating the density-to-pair-density\nmap based on copula theory from statistics. We extend the copula theory to\nmulti-dimensional marginals, and deduce that one can describe any (exact or\napproximate) pair density by the single-particle density and a copula. We\npresent analytical formulas for the exact copula in scaling limits, numerically\ncompute the copula for dissociating systems with two to four particles in one\ndimension, and propose accurate approximations of the copula between\nequilibrium and dissociation for two-particle systems.\n","versions":"[{'version': 'v1', 'created': 'Wed, 29 Jan 2025 09:39:13 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 21:37:34 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Dusson', 'Genevi\u00e8ve', ''], ['Kl\u00fcppelberg', 'Claudia', ''], ['Friesecke', 'Gero', '']]","extracted_entities":"[{'text': 'scaling limits', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling limits","similarity_score":0.6726377606}
{"id":2502.14831,"submitter":"Ivan Skorokhodov","authors":"Ivan Skorokhodov, Sharath Girish, Benran Hu, Willi Menapace, Yanyu Li,\n  Rameen Abdal, Sergey Tulyakov, Aliaksandr Siarohin","title":"Improving the Diffusability of Autoencoders","comments":"26 pages, 22 figures, 9 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Latent diffusion models have emerged as the leading approach for generating\nhigh-quality images and videos, utilizing compressed latent representations to\nreduce the computational burden of the diffusion process. While recent\nadvancements have primarily focused on scaling diffusion backbones and\nimproving autoencoder reconstruction quality, the interaction between these\ncomponents has received comparatively less attention. In this work, we perform\na spectral analysis of modern autoencoders and identify inordinate\nhigh-frequency components in their latent spaces, which are especially\npronounced in the autoencoders with a large bottleneck channel size. We\nhypothesize that this high-frequency component interferes with the\ncoarse-to-fine nature of the diffusion synthesis process and hinders the\ngeneration quality. To mitigate the issue, we propose scale equivariance: a\nsimple regularization strategy that aligns latent and RGB spaces across\nfrequencies by enforcing scale equivariance in the decoder. It requires minimal\ncode changes and only up to 20K autoencoder fine-tuning steps, yet\nsignificantly improves generation quality, reducing FID by 19% for image\ngeneration on ImageNet-1K 256x256 and FVD by at least 44% for video generation\non Kinetics-700 17x256x256.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Feb 2025 18:45:44 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 22:08:10 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Skorokhodov', 'Ivan', ''], ['Girish', 'Sharath', ''], ['Hu', 'Benran', ''], ['Menapace', 'Willi', ''], ['Li', 'Yanyu', ''], ['Abdal', 'Rameen', ''], ['Tulyakov', 'Sergey', ''], ['Siarohin', 'Aliaksandr', '']]","extracted_entities":"[{'text': 'scale equivariance', 'label': 'Scaling law'}, {'text': 'scale equivariance', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scale equivariance","similarity_score":0.5122733116}
{"id":2503.01413,"submitter":"Bapi Dutta","authors":"Bapi Dutta, Diego Garc\\'ia-Zamora, Jos\\'e Rui Figueira, Luis\n  Mart\\'inez","title":"Building Interval Type-2 Fuzzy Membership Function: A Deck of Cards\n  based Co-constructive Approach","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI math.OC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Since its inception, Fuzzy Set has been widely used to handle uncertainty and\nimprecision in decision-making. However, conventional fuzzy sets, often\nreferred to as type-1 fuzzy sets (T1FSs) have limitations in capturing higher\nlevels of uncertainty, particularly when decision-makers (DMs) express\nhesitation or ambiguity in membership degree. To address this, Interval Type-2\nFuzzy Sets (IT2FSs) have been introduced by incorporating uncertainty in\nmembership degree allocation, which enhanced flexibility in modelling\nsubjective judgments. Despite their advantages, existing IT2FS construction\nmethods often lack active involvement from DMs and that limits the\ninterpretability and effectiveness of decision models. This study proposes a\nsocio-technical co-constructive approach for developing IT2FS models of\nlinguistic terms by facilitating the active involvement of DMs in preference\nelicitation and its application in multicriteria decision-making (MCDM)\nproblems. Our methodology is structured in two phases. The first phase involves\nan interactive process between the DM and the decision analyst, in which a\nmodified version of Deck-of-Cards (DoC) method is proposed to construct T1FS\nmembership functions on a ratio scale. We then extend this method to\nincorporate ambiguity in subjective judgment and that resulted in an IT2FS\nmodel that better captures uncertainty in DM's linguistic assessments. The\nsecond phase formalizes the constructed IT2FS model for application in MCDM by\ndefining an appropriate mathematical representation of such information,\naggregation rules, and an admissible ordering principle. The proposed framework\nenhances the reliability and effectiveness of fuzzy decision-making not only by\naccurately representing DM's personalized semantics of linguistic information.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 11:08:18 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 15:37:21 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Dutta', 'Bapi', ''], ['Garc\u00eda-Zamora', 'Diego', ''], ['Figueira', 'Jos\u00e9 Rui', ''], ['Mart\u00ednez', 'Luis', '']]","extracted_entities":"[{'text': 'ratio scale', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"ratio scale","similarity_score":0.5325724483}
{"id":2503.01643,"submitter":"Liu Liu","authors":"Jiayu Wan, Liu Liu","title":"Error estimates of asymptotic-preserving neural networks in\n  approximating stochastic linearized Boltzmann equation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.NA","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this paper, we construct an asymptotic-preserving neural networks (APNNs)\n[21] for the linearized Boltzmann equation in the acoustic scaling and with\nuncertain parameters. Utilizing the micro-macro decomposition, we design the\nloss function based on the stochastic-Galerkin system conducted from the\nmicro-macro equations. Rigorous analysis is provided to show the capability of\nneural networks in approximating solutions near the global Maxwellian. By\nemploying hypocoercivity techniques, we demonstrate two key results: the\nexistence of APNNs when the loss function approaches zero, and the convergence\nof the APNN approximated solution as the loss tends to zero, with the error\nexhibiting an exponential decay in time.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 15:22:26 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 13:32:55 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Wan', 'Jiayu', ''], ['Liu', 'Liu', '']]","extracted_entities":"[{'text': 'acoustic scaling', 'label': 'Scaling law'}, {'text': 'APNNs', 'label': 'LLMs'}]","assigned_concept":"Scaling law","matched_keyword":"acoustic scaling","similarity_score":0.5143906474}
{"id":2503.03834,"submitter":"Prerna Sharma","authors":"Prince Vibek Baruah, Nadia Bihari Padhan, Biswajit Maji, Rahul Pandit\n  and Prerna Sharma","title":"Emergent active turbulence and intermittency in dense algal suspensions\n  of Chlamydomonas reinhardtii","comments":"9 pages (main article), 5 figures (main article), 3 pages\n  (supplementary information), 3 figures (supplementary information)","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.soft cond-mat.stat-mech nlin.CD","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Active-fluid turbulence has been found in bacterial suspensions, but not so\nfar in their algal counterparts. We present the first experimental evidence for\nturbulence in dense algal suspensions of Chlamydomonas reinhardtii. We carry\nout a detailed analysis of the statistical properties of the flow present in\nthese cell suspensions and show that they are quantitatively distinct from\ntheir counterparts in two-dimensional fluid and bacterial turbulence. Both\nkinetic-energy and density spectra of the fluid flow in algal turbulence show\npower-law regimes with unique scaling exponents. The fluid velocity probability\ndistribution function (PDF) is strongly non-Gaussian and the length dependence\nof the PDF of fluid-velocity increments indicates small-scale intermittency. We\ncompare and contrast our results with recent theoretical predictions for\nactive-scalar turbulence and active glasses. Overall, our results highlight\nthat active turbulence can arise, even in absence of orientational\ninstabilities, so it is not limited to bacterial suspensions but it can also be\nfound in many biological systems with free-swimming micro-organisms.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 19:01:08 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 13:38:56 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Baruah', 'Prince Vibek', ''], ['Padhan', 'Nadia Bihari', ''], ['Maji', 'Biswajit', ''], ['Pandit', 'Rahul', ''], ['Sharma', 'Prerna', '']]","extracted_entities":"[{'text': 'power-law regimes', 'label': 'Scaling law'}, {'text': 'unique scaling exponents', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"unique scaling exponents","similarity_score":0.5268271565}
{"id":2503.04715,"submitter":"Qiufeng Wang","authors":"Houyi Li, Wenzheng Zheng, Jingcheng Hu, Qiufeng Wang, Hanshan Zhang,\n  Zili Wang, Shijie Xuyang, Yuantao Fan, Shuigeng Zhou, Xiangyu Zhang, Daxin\n  Jiang","title":"Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large\n  Language Model Pretraining","comments":"19 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well-established, yet their effective deployment necessitates\ncareful hyperparameter optimization. Through extensive empirical studies\ninvolving grid searches across diverse configurations, we discover universal\nscaling laws governing these hyperparameters: optimal learning rate follows a\npower-law relationship with both model parameters and data sizes, while optimal\nbatch size scales primarily with data sizes. Our analysis reveals a convex\noptimization landscape for hyperparameters under fixed models and data size\nconditions. This convexity implies an optimal hyperparameter plateau. We\ncontribute a universal, plug-and-play optimal hyperparameter tool for the\ncommunity. Its estimated values on the test set are merely 0.09% away from the\nglobally optimal LLM performance found via an exhaustive search. These laws\ndemonstrate remarkable robustness across variations in model sparsity, training\ndata distribution, and model shape. To our best known, this is the first work\nthat unifies different model shapes and structures, such as Mixture-of-Experts\nmodels and dense transformers, as well as establishes optimal hyperparameter\nscaling laws across diverse data distributions. This exhaustive optimization\nprocess demands substantial computational resources, utilizing nearly one\nmillion NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and\nhyperparameters from scratch and consuming approximately 100 trillion tokens in\ntotal. To facilitate reproducibility and further research, we will\nprogressively release all loss measurements and model checkpoints through our\ndesignated repository https:\/\/step-law.github.io\/\n","versions":"[{'version': 'v1', 'created': 'Thu, 6 Mar 2025 18:58:29 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 17:59:40 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Li', 'Houyi', ''], ['Zheng', 'Wenzheng', ''], ['Hu', 'Jingcheng', ''], ['Wang', 'Qiufeng', ''], ['Zhang', 'Hanshan', ''], ['Wang', 'Zili', ''], ['Xuyang', 'Shijie', ''], ['Fan', 'Yuantao', ''], ['Zhou', 'Shuigeng', ''], ['Zhang', 'Xiangyu', ''], ['Jiang', 'Daxin', '']]","extracted_entities":"[{'text': 'universal\\nscaling laws', 'label': 'Scaling law'}, {'text': 'optimal learning rate', 'label': 'Scaling law'}, {'text': 'dense transformers', 'label': 'Transformers'}, {'text': 'optimal hyperparameter\\nscaling laws', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"universal\nscaling laws","similarity_score":0.7954784632}
{"id":2503.0648,"submitter":"Pankaj Popli","authors":"Pankaj Popli, Ananyo Maitra, and Sriram Ramaswamy","title":"Don't look back: Ordering and defect cloaking in non-reciprocal lattice\n  XY models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.soft cond-mat.stat-mech","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  We present a detailed analytical and numerical examination, on square and\ntriangular lattices, of the non-reciprocal planar spin model introduced in\nDadhichi et al., Phys. Rev. E 101, 052601 (2020). We show that in principle the\neffect of lattice anisotropy should persist at large scales, leading to a\n''mass'' for the angle field of the spins, and behaviour not in the\n''Malthusian Toner-Tu'' universality class. Numerically, however, we find\npower-law scaling of long-wavelength equal-time correlators in the\npolar-ordered phase of our lattice model. The mass, if present, is very small.\nFocussing on topological defects, we show numerically that defect interactions\nare highly anisotropic with respect to the mean ordering direction. In\nparticular, the constituents of a $\\pm 1$ pair are shielded from each other in\na class of configurations, deferring their annihilation and allowing time for\nthe nucleation of further defects. The result, we show numerically, is the\ndestruction of the polarised phase via an aster apocalypse reminiscent of that\nfound by Besse et al., Phys. Rev. Lett. 129, 268003 (2022), for the Malthusian\nToner-Tu equation.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 06:37:34 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Popli', 'Pankaj', ''], ['Maitra', 'Ananyo', ''], ['Ramaswamy', 'Sriram', '']]","extracted_entities":"[{'text': 'power-law scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"power-law scaling","similarity_score":0.7775874138}
{"id":1908.11464,"submitter":"Trevor Ruiz","authors":"Trevor D. Ruiz, Sharmodeep Bhattacharyya, Mahesh Balasubramanian,\n  Kristofer E. Bouchard","title":"Sparse and Low-bias Estimation of High Dimensional Vector Autoregressive\n  Models","comments":null,"journal-ref":"Proceedings of the 2nd Conference on Learning for Dynamics and\n  Control, in Proceedings of Machine Learning Research 120 (2020) pp. 55-64","doi":null,"report-no":null,"categories":"stat.ME","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Vector autoregressive (VAR) models are widely used for causal discovery and\nforecasting in multivariate time series analysis. In the high-dimensional\nsetting, which is increasingly common in fields such as neuroscience and\neconometrics, model parameters are inferred by L1-regularized maximum\nlikelihood (RML). A well-known feature of RML inference is that in general the\ntechnique produces a trade-off between sparsity and bias that depends on the\nchoice of the regularization hyperparameter. In the context of multivariate\ntime series analysis, sparse estimates are favorable for causal discovery and\nlow-bias estimates are favorable for forecasting. However, owing to a paucity\nof research on hyperparameter selection methods, practitioners must rely on\nad-hoc methods such as cross-validation (or manual tuning). The particular\nbalance that such approaches achieve between the two goals -- causal discovery\nand forecasting -- is poorly understood. Our paper investigates this behavior\nand proposes a method (UoI-VAR) that achieves a better balance between sparsity\nand bias when the underlying causal influences are in fact sparse. We\ndemonstrate through simulation that RML with a hyperparameter selected by\ncross-validation tends to overfit, producing relatively dense estimates. We\nfurther demonstrate that UoI-VAR much more effectively approximates the correct\nsparsity pattern with only a minor compromise in model fit, particularly so for\nlarger data dimensions, and that the estimates produced by UoI-VAR exhibit less\nbias. We conclude that our method achieves improved performance especially\nwell-suited to applications involving simultaneous causal discovery and\nforecasting in high-dimensional settings.\n","versions":"[{'version': 'v1', 'created': 'Thu, 29 Aug 2019 22:07:00 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 16:58:07 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 18:07:18 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Ruiz', 'Trevor D.', ''], ['Bhattacharyya', 'Sharmodeep', ''], ['Balasubramanian', 'Mahesh', ''], ['Bouchard', 'Kristofer E.', '']]","extracted_entities":"[{'text': 'manual tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"manual tuning","similarity_score":0.7453469038}
{"id":2103.01901,"submitter":"Shuxiao Chen","authors":"Shuxiao Chen, Qinqing Zheng, Qi Long, Weijie J. Su","title":"Minimax Estimation for Personalized Federated Learning: An Alternative\n  between FedAvg and Local Training?","comments":"JMLR published version","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  A widely recognized difficulty in federated learning arises from the\nstatistical heterogeneity among clients: local datasets often originate from\ndistinct yet not entirely unrelated probability distributions, and\npersonalization is, therefore, necessary to achieve optimal results from each\nindividual's perspective. In this paper, we show how the excess risks of\npersonalized federated learning using a smooth, strongly convex loss depend on\ndata heterogeneity from a minimax point of view, with a focus on the FedAvg\nalgorithm (McMahan et al., 2017) and pure local training (i.e., clients solve\nempirical risk minimization problems on their local datasets without any\ncommunication). Our main result reveals an approximate alternative between\nthese two baseline algorithms for federated learning: the former algorithm is\nminimax rate optimal over a collection of instances when data heterogeneity is\nsmall, whereas the latter is minimax rate optimal when data heterogeneity is\nlarge, and the threshold is sharp up to a constant.\n  As an implication, our results show that from a worst-case point of view, a\ndichotomous strategy that makes a choice between the two baseline algorithms is\nrate-optimal. Another implication is that the popular FedAvg following by local\nfine tuning strategy is also minimax optimal under additional regularity\nconditions. Our analysis relies on a new notion of algorithmic stability that\ntakes into account the nature of federated learning.\n","versions":"[{'version': 'v1', 'created': 'Tue, 2 Mar 2021 17:58:20 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 02:36:12 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Chen', 'Shuxiao', ''], ['Zheng', 'Qinqing', ''], ['Long', 'Qi', ''], ['Su', 'Weijie J.', '']]","extracted_entities":"[{'text': 'federated learning', 'label': 'Zero-shot Learning'}, {'text': 'federated learning', 'label': 'Zero-shot Learning'}, {'text': 'pure local training', 'label': 'Few-shot Learning'}, {'text': 'federated learning', 'label': 'Zero-shot Learning'}, {'text': 'local\\nfine tuning strategy', 'label': 'Fine-tuning'}, {'text': 'federated learning', 'label': 'Zero-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"local\nfine tuning strategy","similarity_score":0.7791124582}
{"id":2111.10722,"submitter":"Lulu Kang","authors":"Yindong Chen, Yiwei Wang, Lulu Kang, Chun Liu","title":"A Deterministic Sampling Method via Maximum Mean Discrepancy Flow with\n  Adaptive Kernel","comments":"30 pages, 10 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG stat.CO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We propose a novel deterministic sampling method to approximate a target\ndistribution $\\rho^*$ by minimizing the kernel discrepancy, also known as the\nMaximum Mean Discrepancy (MMD). By employing the general \\emph{energetic\nvariational inference} framework (Wang et al., 2021), we convert the problem of\nminimizing MMD to solving a dynamic ODE system of the particles. We adopt the\nimplicit Euler numerical scheme to solve the ODE systems. This leads to a\nproximal minimization problem in each iteration of updating the particles,\nwhich can be solved by optimization algorithms such as L-BFGS. The proposed\nmethod is named EVI-MMD. To overcome the long-existing issue of bandwidth\nselection of the Gaussian kernel, we propose a novel way to specify the\nbandwidth dynamically. Through comprehensive numerical studies, we have shown\nthe proposed adaptive bandwidth significantly improves the EVI-MMD. We use the\nEVI-MMD algorithm to solve two types of sampling problems. In the first type,\nthe target distribution is given by a fully specified density function. The\nsecond type is a \"two-sample problem\", where only training data are available.\nThe EVI-MMD method is used as a generative learning model to generate new\nsamples that follow the same distribution as the training data. With the\nrecommended settings of the tuning parameters, we show that the proposed\nEVI-MMD method outperforms some existing methods for both types of problems.\n","versions":"[{'version': 'v1', 'created': 'Sun, 21 Nov 2021 03:09:07 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Nov 2022 05:13:59 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 16:09:45 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Chen', 'Yindong', ''], ['Wang', 'Yiwei', ''], ['Kang', 'Lulu', ''], ['Liu', 'Chun', '']]","extracted_entities":"[{'text': 'tuning parameters', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"tuning parameters","similarity_score":0.6904253364}
{"id":2312.05657,"submitter":"Nikos Kanakaris","authors":"Shukai Duan, Nikos Kanakaris, Xiongye Xiao, Heng Ping, Chenyu Zhou,\n  Nesreen K. Ahmed, Guixiang Ma, Mihai Capota, Theodore L. Willke, Shahin\n  Nazarian, Paul Bogdan","title":"PerfRL: A Small Language Model Framework for Efficient Code Optimization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.PL cs.SE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Code optimization is a challenging task requiring a substantial level of\nexpertise from developers. Nonetheless, this level of human capacity is not\nsufficient considering the rapid evolution of new hardware architectures and\nsoftware environments. In light of this, recent research proposes adopting\nmachine learning and artificial intelligence techniques to automate the code\noptimization process. In this paper, we introduce PerfRL, an innovative\nframework designed to tackle the problem of code optimization. Our framework\nleverages the capabilities of small language models (SLMs) and reinforcement\nlearning (RL), facilitating a system where SLMs can assimilate feedback from\ntheir environment during the fine-tuning phase, notably through unit tests.\nWhen benchmarked against existing models, PerfRL demonstrates superior\nefficiency in terms of speed and computational resource usage, attributed to\nits reduced need for training steps and its compatibility with SLMs.\nFurthermore, it substantially diminishes the risk of logical and syntactical\nerrors. To evaluate our framework, we conduct experiments on the PIE dataset\nusing a lightweight large language model (i.e., CodeT5) and a new reinforcement\nlearning algorithm, namely RRHF. For evaluation purposes, we use a list of\nevaluation metrics related to optimization quality and speedup. The evaluation\nresults show that our approach achieves similar or better results compared to\nstate-of-the-art models using shorter training times and smaller pre-trained\nmodels.\n","versions":"[{'version': 'v1', 'created': 'Sat, 9 Dec 2023 19:50:23 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 05:01:42 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Duan', 'Shukai', ''], ['Kanakaris', 'Nikos', ''], ['Xiao', 'Xiongye', ''], ['Ping', 'Heng', ''], ['Zhou', 'Chenyu', ''], ['Ahmed', 'Nesreen K.', ''], ['Ma', 'Guixiang', ''], ['Capota', 'Mihai', ''], ['Willke', 'Theodore L.', ''], ['Nazarian', 'Shahin', ''], ['Bogdan', 'Paul', '']]","extracted_entities":"[{'text': 'fine-tuning phase', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning phase","similarity_score":0.7284630537}
{"id":2402.07818,"submitter":"Zhihao Liu","authors":"Z Liu, J Lou, W Bao, Y Hu, B Li, Z Qin, K Ren","title":"Differentially Private Zeroth-Order Methods for Scalable Large Language\n  Model Finetuning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Fine-tuning on task-specific datasets is a widely-embraced paradigm of\nharnessing the powerful capability of pretrained LLMs for various downstream\ntasks. Due to the popularity of LLMs fine-tuning and its accompanying privacy\nconcerns, differentially private (DP) fine-tuning of pretrained LLMs has been\nwidely used to safeguarding the privacy of task-specific datasets. Lying at the\ndesign core of DP LLM fine-tuning methods is the satisfactory tradeoff among\nprivacy, utility, and scalability. Most existing methods build upon the seminal\nwork of DP-SGD. Despite pushing the scalability of DP-SGD to its limit,\nDP-SGD-based fine-tuning methods are unfortunately limited by the inherent\ninefficiency of SGD.\n  In this paper, we investigate the potential of DP zeroth-order methods for\nLLM pretraining, which avoids the scalability bottleneck of SGD by\napproximating the gradient with the more efficient zeroth-order gradient.\nRather than treating the zeroth-order method as a drop-in replacement for SGD,\nthis paper presents a comprehensive study both theoretically and empirically.\nFirst, we propose the stagewise DP zeroth-order method (DP-ZOSO) that\ndynamically schedules key hyperparameters. This design is grounded on the\nsynergy between DP random perturbation and the gradient approximation error of\nthe zeroth-order method, and its effect on fine-tuning trajectory.\n  We provide theoretical analysis for both proposed methods. We conduct\nextensive empirical analysis on both encoder-only masked language model and\ndecoder-only autoregressive language model, achieving impressive results in\nterms of scalability and utility regardless of the class of tasks (compared\nwith DPZero, DP-ZOPO improves $4.5\\%$ on SST-5, $5.5\\%$ on MNLI with\nRoBERTa-Large and 9.2\\% on CB, 3.9\\% on BoolQ with OPT-2.7b when $\\epsilon=4$,\ndemonstrates more significant enhancement in performance on more complicated\ntasks).\n","versions":"[{'version': 'v1', 'created': 'Mon, 12 Feb 2024 17:24:15 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Feb 2024 06:11:02 GMT'}, {'version': 'v3', 'created': 'Wed, 8 May 2024 07:14:42 GMT'}, {'version': 'v4', 'created': 'Thu, 9 May 2024 09:41:23 GMT'}, {'version': 'v5', 'created': 'Mon, 2 Dec 2024 12:29:47 GMT'}, {'version': 'v6', 'created': 'Mon, 10 Mar 2025 06:52:03 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Liu', 'Z', ''], ['Lou', 'J', ''], ['Bao', 'W', ''], ['Hu', 'Y', ''], ['Li', 'B', ''], ['Qin', 'Z', ''], ['Ren', 'K', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'pretrained LLMs', 'label': 'LLM-based'}, {'text': 'RoBERTa-Large', 'label': 'RoBERTa'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2405.13637,"submitter":"Radu Tudor Ionescu","authors":"Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Nicu Sebe,\n  Mubarak Shah","title":"Curriculum Direct Preference Optimization for Diffusion and Consistency\n  Models","comments":"Accepted at CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Direct Preference Optimization (DPO) has been proposed as an effective and\nefficient alternative to reinforcement learning from human feedback (RLHF). In\nthis paper, we propose a novel and enhanced version of DPO based on curriculum\nlearning for text-to-image generation. Our method is divided into two training\nstages. First, a ranking of the examples generated for each prompt is obtained\nby employing a reward model. Then, increasingly difficult pairs of examples are\nsampled and provided to a text-to-image generative (diffusion or consistency)\nmodel. Generated samples that are far apart in the ranking are considered to\nform easy pairs, while those that are close in the ranking form hard pairs. In\nother words, we use the rank difference between samples as a measure of\ndifficulty. The sampled pairs are split into batches according to their\ndifficulty levels, which are gradually used to train the generative model. Our\napproach, Curriculum DPO, is compared against state-of-the-art fine-tuning\napproaches on nine benchmarks, outperforming the competing methods in terms of\ntext alignment, aesthetics and human preference. Our code is available at\nhttps:\/\/github.com\/CroitoruAlin\/Curriculum-DPO.\n","versions":"[{'version': 'v1', 'created': 'Wed, 22 May 2024 13:36:48 GMT'}, {'version': 'v2', 'created': 'Fri, 24 May 2024 13:14:40 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 16:44:48 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Croitoru', 'Florinel-Alin', ''], ['Hondru', 'Vlad', ''], ['Ionescu', 'Radu Tudor', ''], ['Sebe', 'Nicu', ''], ['Shah', 'Mubarak', '']]","extracted_entities":"[{'text': 'prompt', 'label': 'Prompting'}, {'text': 'state-of-the-art fine-tuning\\napproaches', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"state-of-the-art fine-tuning\napproaches","similarity_score":0.7898973227}
{"id":2405.14529,"submitter":"Simon Damm","authors":"Simon Damm, Mike Laszkiewicz, Johannes Lederer, Asja Fischer","title":"AnomalyDINO: Boosting Patch-based Few-shot Anomaly Detection with DINOv2","comments":"Accepted at WACV 2025 (Oral)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent advances in multimodal foundation models have set new standards in\nfew-shot anomaly detection. This paper explores whether high-quality visual\nfeatures alone are sufficient to rival existing state-of-the-art\nvision-language models. We affirm this by adapting DINOv2 for one-shot and\nfew-shot anomaly detection, with a focus on industrial applications. We show\nthat this approach does not only rival existing techniques but can even\noutmatch them in many settings. Our proposed vision-only approach, AnomalyDINO,\nfollows the well-established patch-level deep nearest neighbor paradigm, and\nenables both image-level anomaly prediction and pixel-level anomaly\nsegmentation. The approach is methodologically simple and training-free and,\nthus, does not require any additional data for fine-tuning or meta-learning.\nThe approach is methodologically simple and training-free and, thus, does not\nrequire any additional data for fine-tuning or meta-learning. Despite its\nsimplicity, AnomalyDINO achieves state-of-the-art results in one- and few-shot\nanomaly detection (e.g., pushing the one-shot performance on MVTec-AD from an\nAUROC of 93.1% to 96.6%). The reduced overhead, coupled with its outstanding\nfew-shot performance, makes AnomalyDINO a strong candidate for fast deployment,\ne.g., in industrial contexts.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 May 2024 13:15:13 GMT'}, {'version': 'v2', 'created': 'Thu, 12 Sep 2024 09:23:32 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 09:32:39 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Damm', 'Simon', ''], ['Laszkiewicz', 'Mike', ''], ['Lederer', 'Johannes', ''], ['Fischer', 'Asja', '']]","extracted_entities":"[{'text': 'multimodal foundation models', 'label': 'Foundation Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2405.16397,"submitter":"Mahdi S. Hosseini Dr.","authors":"Damien Martins Gomes and Yanlei Zhang and Eugene Belilovsky and Guy\n  Wolf and Mahdi S. Hosseini","title":"AdaFisher: Adaptive Second Order Optimization via Fisher Information","comments":"Accepted in ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG math.OC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  First-order optimization methods are currently the mainstream in training\ndeep neural networks (DNNs). Optimizers like Adam incorporate limited curvature\ninformation by employing the diagonal matrix preconditioning of the stochastic\ngradient during the training. Despite their widespread, second-order\noptimization algorithms exhibit superior convergence properties compared to\ntheir first-order counterparts e.g. Adam and SGD. However, their practicality\nin training DNNs is still limited due to increased per-iteration computations\ncompared to the first-order methods. We present \\emph{AdaFisher}--an adaptive\nsecond-order optimizer that leverages a \\emph{diagonal block-Kronecker}\napproximation of the Fisher information matrix for adaptive gradient\npreconditioning. AdaFisher aims to bridge the gap between enhanced\n\\emph{convergence\/generalization} capabilities and computational efficiency in\nsecond-order optimization framework for training DNNs. Despite the slow pace of\nsecond-order optimizers, we showcase that AdaFisher can be reliably adopted for\nimage classification, language modeling and stands out for its stability and\nrobustness in hyper-parameter tuning. We demonstrate that AdaFisher\n\\textbf{outperforms the SOTA optimizers} in terms of both accuracy and\nconvergence speed. Code is available from\nhttps:\/\/github.com\/AtlasAnalyticsLab\/AdaFisher.\n","versions":"[{'version': 'v1', 'created': 'Sun, 26 May 2024 01:25:02 GMT'}, {'version': 'v2', 'created': 'Thu, 17 Oct 2024 23:51:23 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 18:42:22 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Gomes', 'Damien Martins', ''], ['Zhang', 'Yanlei', ''], ['Belilovsky', 'Eugene', ''], ['Wolf', 'Guy', ''], ['Hosseini', 'Mahdi S.', '']]","extracted_entities":"[{'text': 'Adam', 'label': 'ALBERT'}, {'text': 'Adam', 'label': 'ALBERT'}, {'text': 'hyper-parameter tuning', 'label': 'Fine-tuning'}, {'text': 'AdaFisher', 'label': 'ALBERT'}]","assigned_concept":"Fine-tuning","matched_keyword":"hyper-parameter tuning","similarity_score":0.628062129}
{"id":2405.17532,"submitter":"Jiannan Huang","authors":"Jiannan Huang, Jun Hao Liew, Hanshu Yan, Yuyang Yin, Yao Zhao, Yunchao\n  Wei","title":"ClassDiffusion: More Aligned Personalization Tuning with Explicit Class\n  Guidance","comments":"Accepted to ICLR2025, Code is available at\n  https:\/\/github.com\/Rbrq03\/ClassDiffusion","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Recent text-to-image customization works have proven successful in generating\nimages of given concepts by fine-tuning diffusion models on a few examples.\nHowever, tuning-based methods inherently tend to overfit the concepts,\nresulting in failure to create the concept under multiple conditions (*e.g.*,\nheadphone is missing when generating \"a `dog wearing a headphone\").\nInterestingly, we notice that the base model before fine-tuning exhibits the\ncapability to compose the base concept with other elements (*e.g.*, \"a dog\nwearing a headphone\"), implying that the compositional ability only disappears\nafter personalization tuning. We observe a semantic shift in the customized\nconcept after fine-tuning, indicating that the personalized concept is not\naligned with the original concept, and further show through theoretical\nanalyses that this semantic shift leads to increased difficulty in sampling the\njoint conditional probability distribution, resulting in the loss of the\ncompositional ability. Inspired by this finding, we present **ClassDiffusion**,\na technique that leverages a **semantic preservation loss** to explicitly\nregulate the concept space when learning a new concept. Although simple, this\napproach effectively prevents semantic drift during the fine-tuning process of\nthe target concepts. Extensive qualitative and quantitative experiments\ndemonstrate that the use of semantic preservation loss effectively improves the\ncompositional abilities of fine-tuning models. Lastly, we also extend our\nClassDiffusion to personalized video generation, demonstrating its flexibility.\n","versions":"[{'version': 'v1', 'created': 'Mon, 27 May 2024 17:50:10 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 17:45:13 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Huang', 'Jiannan', ''], ['Liew', 'Jun Hao', ''], ['Yan', 'Hanshu', ''], ['Yin', 'Yuyang', ''], ['Zhao', 'Yao', ''], ['Wei', 'Yunchao', '']]","extracted_entities":"[{'text': 'base model', 'label': 'Foundation Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2406.04094,"submitter":"Zixi Chen","authors":"Zixi Chen, Xuyang Ren, Yuya Hamamatsu, Gastone Ciuti, Cesare Stefanini","title":"A Generalized Adaptive Jacobian Controller for Soft Robots","comments":"10 pages, 8 figures, 4 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The nonlinearity and hysteresis of soft robot motions have posed challenges\nin control. The Jacobian controller is transferred from rigid robot controllers\nand exhibits conciseness, but the improper assumption of soft robots induces\nthe feasibility only in a small local area. Accurate controllers like neural\nnetworks can deal with delayed and nonlinear motion, achieving high accuracy,\nbut they suffer from the high data amount requirement and black-box property.\nInspired by these approaches, we propose an adaptive generalized Jacobian\ncontroller for soft robots. This controller is constructed by the concise\nformat of the Jacobian controller but includes more states and independent\nmatrices, which is suitable for soft robotics. In addition, the initialization\nleverages the motor babbling strategy and batch optimization from neural\nnetwork controllers. In experiments, we first analyze the online controllers,\nincluding the Jacobian controller, the Gaussian process regression, and our\ncontroller. Real experiments have validated that our controller outperforms the\nRNN controller even with fewer data samples, and it is adaptive to various\nsituations without fine-tuning, like different control frequencies, softness,\nand even manufacturing errors. Future work may include online adjustment of the\ncontroller format and adaptability validation in more scenarios.\n","versions":"[{'version': 'v1', 'created': 'Thu, 6 Jun 2024 14:11:09 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 13:04:28 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chen', 'Zixi', ''], ['Ren', 'Xuyang', ''], ['Hamamatsu', 'Yuya', ''], ['Ciuti', 'Gastone', ''], ['Stefanini', 'Cesare', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2406.1248,"submitter":"Stefan Sylvius Wagner","authors":"Stefan Sylvius Wagner and Maike Behrendt and Marc Ziegele and Stefan\n  Harmeling","title":"The Power of LLM-Generated Synthetic Data for Stance Detection in Online\n  Political Discussions","comments":"ICLR 2025 Spotlight","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Stance detection holds great potential to improve online political\ndiscussions through its deployment in discussion platforms for purposes such as\ncontent moderation, topic summarization or to facilitate more balanced\ndiscussions. Typically, transformer-based models are employed directly for\nstance detection, requiring vast amounts of data. However, the wide variety of\ndebate topics in online political discussions makes data collection\nparticularly challenging. LLMs have revived stance detection, but their online\ndeployment in online political discussions faces challenges like inconsistent\noutputs, biases, and vulnerability to adversarial attacks. We show how\nLLM-generated synthetic data can improve stance detection for online political\ndiscussions by using reliable traditional stance detection models for online\ndeployment, while leveraging the text generation capabilities of LLMs for\nsynthetic data generation in a secure offline environment. To achieve this, (i)\nwe generate synthetic data for specific debate questions by prompting a\nMistral-7B model and show that fine-tuning with the generated synthetic data\ncan substantially improve the performance of stance detection, while remaining\ninterpretable and aligned with real world data. (ii) Using the synthetic data\nas a reference, we can improve performance even further by identifying the most\ninformative samples in an unlabelled dataset, i.e., those samples which the\nstance detection model is most uncertain about and can benefit from the most.\nBy fine-tuning with both synthetic data and the most informative samples, we\nsurpass the performance of the baseline model that is fine-tuned on all true\nlabels, while labelling considerably less data.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Jun 2024 10:36:21 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 22:04:34 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wagner', 'Stefan Sylvius', ''], ['Behrendt', 'Maike', ''], ['Ziegele', 'Marc', ''], ['Harmeling', 'Stefan', '']]","extracted_entities":"[{'text': 'prompting', 'label': 'Prompting'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2406.13035,"submitter":"Zhongwei Wan","authors":"Zhongwei Wan, Xinjian Wu, Yu Zhang, Yi Xin, Chaofan Tao, Zhihong Zhu,\n  Xin Wang, Siqi Luo, Jing Xiong, Longyue Wang, Mi Zhang","title":"D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Jun 2024 20:01:51 GMT'}, {'version': 'v2', 'created': 'Sun, 23 Jun 2024 08:27:48 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 03:16:43 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wan', 'Zhongwei', ''], ['Wu', 'Xinjian', ''], ['Zhang', 'Yu', ''], ['Xin', 'Yi', ''], ['Tao', 'Chaofan', ''], ['Zhu', 'Zhihong', ''], ['Wang', 'Xin', ''], ['Luo', 'Siqi', ''], ['Xiong', 'Jing', ''], ['Wang', 'Longyue', ''], ['Zhang', 'Mi', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2407.01131,"submitter":"Xuyang Liu","authors":"Xuyang Liu, Ting Liu, Siteng Huang, Yi Xin, Yue Hu, Quanjun Yin,\n  Donglin Wang, Yuanyuan Wu, Honggang Chen","title":"M2IST: Multi-Modal Interactive Side-Tuning for Efficient Referring\n  Expression Comprehension","comments":"Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Referring expression comprehension (REC) is a vision-language task to locate\na target object in an image based on a language expression. Fully fine-tuning\ngeneral-purpose pre-trained vision-language foundation models for REC yields\nimpressive performance but becomes increasingly costly. Parameter-efficient\ntransfer learning (PETL) methods have shown strong performance with fewer\ntunable parameters. However, directly applying PETL to REC faces two\nchallenges: (1) insufficient multi-modal interaction between pre-trained\nvision-language foundation models, and (2) high GPU memory usage due to\ngradients passing through the heavy vision-language foundation models. To this\nend, we present M2IST: Multi-Modal Interactive Side-Tuning with M3ISAs: Mixture\nof Multi-Modal Interactive Side-Adapters. During fine-tuning, we fix the\npre-trained uni-modal encoders and update M3ISAs to enable efficient\nvision-language alignment for REC. Empirical results reveal that M2IST achieves\nbetter performance-efficiency trade-off than full fine-tuning and other PETL\nmethods, requiring only 2.11\\% tunable parameters, 39.61\\% GPU memory, and\n63.46\\% training time while maintaining competitive performance. Our code is\nreleased at https:\/\/github.com\/xuyang-liu16\/M2IST.\n","versions":"[{'version': 'v1', 'created': 'Mon, 1 Jul 2024 09:53:53 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Oct 2024 12:57:42 GMT'}, {'version': 'v3', 'created': 'Sun, 16 Feb 2025 18:44:39 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 08:48:16 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Liu', 'Xuyang', ''], ['Liu', 'Ting', ''], ['Huang', 'Siteng', ''], ['Xin', 'Yi', ''], ['Hu', 'Yue', ''], ['Yin', 'Quanjun', ''], ['Wang', 'Donglin', ''], ['Wu', 'Yuanyuan', ''], ['Chen', 'Honggang', '']]","extracted_entities":"[{'text': 'PETL', 'label': 'Few-shot Learning'}, {'text': 'M3ISAs', 'label': 'Foundation Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'M3ISAs', 'label': 'Foundation Model'}, {'text': 'full fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2407.20158,"submitter":"Christof Sch\\\"otz","authors":"Christof Sch\\\"otz, Alistair White, Maximilian Gelbrecht, Niklas Boers","title":"Machine Learning for Predicting Chaotic Systems","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG nlin.CD","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Predicting chaotic dynamical systems is critical in many scientific fields,\nsuch as weather forecasting, but challenging due to the characteristic\nsensitive dependence on initial conditions. Traditional modeling approaches\nrequire extensive domain knowledge, often leading to a shift towards\ndata-driven methods using machine learning. However, existing research provides\ninconclusive results on which machine learning methods are best suited for\npredicting chaotic systems. In this paper, we compare different lightweight and\nheavyweight machine learning architectures using extensive existing benchmark\ndatabases, as well as a newly introduced database that allows for uncertainty\nquantification in the benchmark results. In addition to state-of-the-art\nmethods from the literature, we also present new advantageous variants of\nestablished methods. Hyperparameter tuning is adjusted based on computational\ncost, with more tuning allocated to less costly methods. Furthermore, we\nintroduce the cumulative maximum error, a novel metric that combines desirable\nproperties of traditional metrics and is tailored for chaotic systems. Our\nresults show that well-tuned simple methods, as well as untuned baseline\nmethods, often outperform state-of-the-art deep learning models, but their\nperformance can vary significantly with different experimental setups. These\nfindings highlight the importance of aligning prediction methods with data\ncharacteristics and caution against the indiscriminate use of overly complex\nmodels.\n","versions":"[{'version': 'v1', 'created': 'Mon, 29 Jul 2024 16:34:47 GMT'}, {'version': 'v2', 'created': 'Mon, 16 Dec 2024 13:28:45 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 15:30:13 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Sch\u00f6tz', 'Christof', ''], ['White', 'Alistair', ''], ['Gelbrecht', 'Maximilian', ''], ['Boers', 'Niklas', '']]","extracted_entities":"[{'text': 'uncertainty\\nquantification', 'label': 'quantisation'}, {'text': 'Hyperparameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"Hyperparameter tuning","similarity_score":0.6193697453}
{"id":2408.07931,"submitter":"Haofeng Liu","authors":"Haofeng Liu, Erli Zhang, Junde Wu, Mingxuan Hong, Yueming Jin","title":"Surgical SAM 2: Real-time Segment Anything in Surgical Video by\n  Efficient Frame Pruning","comments":"Accepted by NeurIPS 2024 Workshop AIM-FM","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.RO eess.IV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Surgical video segmentation is a critical task in computer-assisted surgery\nand is vital for enhancing surgical quality and patient outcomes. Recently, the\nSegment Anything Model 2 (SAM2) framework has shown superior advancements in\nimage and video segmentation. However, SAM2 struggles with efficiency due to\nthe high computational demands of processing high-resolution images and complex\nand long-range temporal dynamics in surgical videos. To address these\nchallenges, we introduce Surgical SAM 2 (SurgSAM2), an advanced model to\nutilize SAM2 with an Efficient Frame Pruning (EFP) mechanism, to facilitate\nreal-time surgical video segmentation. The EFP mechanism dynamically manages\nthe memory bank by selectively retaining only the most informative frames,\nreducing memory usage and computational cost while maintaining high\nsegmentation accuracy. Our extensive experiments demonstrate that SurgSAM2\nsignificantly improves both efficiency and segmentation accuracy compared to\nthe vanilla SAM2. Remarkably, SurgSAM2 achieves a 3$\\times$ FPS compared with\nSAM2, while also delivering state-of-the-art performance after fine-tuning with\nlower-resolution data. These advancements establish SurgSAM2 as a leading model\nfor surgical video analysis, making real-time surgical video segmentation in\nresource-constrained environments a reality. Our source code is available at\nhttps:\/\/github.com\/jinlab-imvr\/Surgical-SAM-2.\n","versions":"[{'version': 'v1', 'created': 'Thu, 15 Aug 2024 04:59:12 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 12:57:43 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Liu', 'Haofeng', ''], ['Zhang', 'Erli', ''], ['Wu', 'Junde', ''], ['Hong', 'Mingxuan', ''], ['Jin', 'Yueming', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2408.14325,"submitter":"Lucia Pezzetti","authors":"Lucia Pezzetti, Stefano Favaro and Stefano Peluchetti","title":"Function-Space MCMC for Bayesian Wide Neural Networks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ML","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Bayesian Neural Networks represent a fascinating confluence of deep learning\nand probabilistic reasoning, offering a compelling framework for understanding\nuncertainty in complex predictive models. In this paper, we investigate the use\nof the preconditioned Crank-Nicolson algorithm and its Langevin version to\nsample from a reparametrised posterior distribution of the neural network's\nweights, as the widths grow larger. In addition to being robust in the\ninfinite-dimensional setting, we prove that the acceptance probabilities of the\nproposed algorithms approach 1 as the width of the network increases,\nindependently of any stepsize tuning. Moreover, we examine and compare how the\nmixing speeds of the underdamped Langevin Monte Carlo, the preconditioned\nCrank-Nicolson and the preconditioned Crank-Nicolson Langevin samplers are\ninfluenced by changes in the network width in some real-world cases. Our\nfindings suggest that, in wide Bayesian Neural Networks configurations, the\npreconditioned Crank-Nicolson algorithm allows for a scalable and more\nefficient sampling of the reparametrised posterior distribution, as also\nevidenced by a higher effective sample size and improved diagnostic results\ncompared with the other analysed algorithms.\n","versions":"[{'version': 'v1', 'created': 'Mon, 26 Aug 2024 14:54:13 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Aug 2024 20:17:18 GMT'}, {'version': 'v3', 'created': 'Mon, 7 Oct 2024 14:23:50 GMT'}, {'version': 'v4', 'created': 'Sun, 9 Mar 2025 18:32:27 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Pezzetti', 'Lucia', ''], ['Favaro', 'Stefano', ''], ['Peluchetti', 'Stefano', '']]","extracted_entities":"[{'text': 'stepsize tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"stepsize tuning","similarity_score":0.5802767277}
{"id":2409.01156,"submitter":"Leqi Shen","authors":"Leqi Shen, Tianxiang Hao, Tao He, Sicheng Zhao, Yifeng Zhang,\n  Pengzhang Liu, Yongjun Bao, Guiguang Ding","title":"TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Most text-video retrieval methods utilize the text-image pre-trained models\nlike CLIP as a backbone. These methods process each sampled frame independently\nby the image encoder, resulting in high computational overhead and limiting\npractical deployment. Addressing this, we focus on efficient text-video\nretrieval by tackling two key challenges: 1. From the perspective of trainable\nparameters, current parameter-efficient fine-tuning methods incur high\ninference costs; 2. From the perspective of model complexity, current token\ncompression methods are mainly designed for images to reduce spatial redundancy\nbut overlook temporal redundancy in consecutive frames of a video. To tackle\nthese challenges, we propose Temporal Token Merging (TempMe), a\nparameter-efficient and training-inference efficient text-video retrieval\narchitecture that minimizes trainable parameters and model complexity.\nSpecifically, we introduce a progressive multi-granularity framework. By\ngradually combining neighboring clips, we reduce spatio-temporal redundancy and\nenhance temporal modeling across different frames, leading to improved\nefficiency and performance. Extensive experiments validate the superiority of\nour TempMe. Compared to previous parameter-efficient text-video retrieval\nmethods, TempMe achieves superior performance with just 0.50M trainable\nparameters. It significantly reduces output tokens by 95% and GFLOPs by 51%,\nwhile achieving a 1.8X speedup and a 4.4% R-Sum improvement. With full\nfine-tuning, TempMe achieves a significant 7.9% R-Sum improvement, trains 1.57X\nfaster, and utilizes 75.2% GPU memory usage. The code is available at\nhttps:\/\/github.com\/LunarShen\/TempMe.\n","versions":"[{'version': 'v1', 'created': 'Mon, 2 Sep 2024 10:42:30 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 09:11:37 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Shen', 'Leqi', ''], ['Hao', 'Tianxiang', ''], ['He', 'Tao', ''], ['Zhao', 'Sicheng', ''], ['Zhang', 'Yifeng', ''], ['Liu', 'Pengzhang', ''], ['Bao', 'Yongjun', ''], ['Ding', 'Guiguang', '']]","extracted_entities":"[{'text': 'full\\nfine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"full\nfine-tuning","similarity_score":0.9569243193}
{"id":2409.15658,"submitter":"Siyuan Liu","authors":"Siyuan Liu, Jiawei Du, Sicheng Xiang, Zibo Wang and Dingsheng Luo","title":"Long-horizon Embodied Planning with Implicit Logical Inference and\n  Hallucination Mitigation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Long-horizon embodied planning underpins embodied AI. To accomplish\nlong-horizon tasks, one of the most feasible ways is to decompose abstract\ninstructions into a sequence of actionable steps. Foundation models still face\nlogical errors and hallucinations in long-horizon planning, unless provided\nwith highly relevant examples to the tasks. However, providing highly relevant\nexamples for any random task is unpractical. Therefore, we present ReLEP, a\nnovel framework for Real-time Long-horizon Embodied Planning. ReLEP can\ncomplete a wide range of long-horizon tasks without in-context examples by\nlearning implicit logical inference through fine-tuning. The fine-tuned large\nvision-language model formulates plans as sequences of skill functions. These\nfunctions are selected from a carefully designed skill library. ReLEP is also\nequipped with a Memory module for plan and status recall, and a Robot\nConfiguration module for versatility across robot types. In addition, we\npropose a data generation pipeline to tackle dataset scarcity. When\nconstructing the dataset, we considered the implicit logical relationships,\nenabling the model to learn implicit logical relationships and dispel\nhallucinations. Through comprehensive evaluations across various long-horizon\ntasks, ReLEP demonstrates high success rates and compliance to execution even\non unseen tasks and outperforms state-of-the-art baseline methods.\n","versions":"[{'version': 'v1', 'created': 'Tue, 24 Sep 2024 01:47:23 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 10:15:59 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Liu', 'Siyuan', ''], ['Du', 'Jiawei', ''], ['Xiang', 'Sicheng', ''], ['Wang', 'Zibo', ''], ['Luo', 'Dingsheng', '']]","extracted_entities":"[{'text': 'Foundation models', 'label': 'Foundation Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2410.01054,"submitter":"Johannes Lachner","authors":"Johannes Lachner, Federico Tessari, A. Michael West Jr., Moses C. Nah,\n  Neville Hogan","title":"Divide et Impera: Decoding Impedance Strategies for Robotic Peg-in-Hole\n  Assembly","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  This paper investigates robotic peg-in-hole assembly using the Elementary\nDynamic Actions (EDA) framework, which models contact-rich tasks through a\ncombination of submovements, oscillations, and mechanical impedance. Rather\nthan focusing on a single optimal parameter set, we analyze the distribution\nand structure of multiple successful impedance solutions, revealing patterns\nthat guide impedance selection in contactrich robotic manipulation. Experiments\nwith a real robot and four different peg types demonstrate the presence of\ntask-specific and generalized assembly strategies, identified through K-means\nClustering. Principal Component Analysis (PCA) is used to represent these\nfindings, highlighting patterns in successful impedance selections.\nAdditionally, a neural-network-based success predictor accurately estimates\nfeasible impedance parameters, reducing the need for extensive trial-and-error\ntuning. By providing publicly available code, CAD files, and a trained model,\nthis work enhances the accessibility of impedance control and offers a\nstructured approach to programming robotic assembly tasks, particularly for\nless-experienced users.\n","versions":"[{'version': 'v1', 'created': 'Tue, 1 Oct 2024 20:29:03 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 23:07:59 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Lachner', 'Johannes', ''], ['Tessari', 'Federico', ''], ['West', 'A. Michael', 'Jr.'], ['Nah', 'Moses C.', ''], ['Hogan', 'Neville', '']]","extracted_entities":"[{'text': 'trial-and-error\\ntuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"trial-and-error\ntuning","similarity_score":0.695029974}
{"id":2410.05116,"submitter":"Shang-Fu Chen","authors":"Ayano Hiranaka, Shang-Fu Chen, Chieh-Hsin Lai, Dongjun Kim, Naoki\n  Murata, Takashi Shibuya, Wei-Hsiang Liao, Shao-Hua Sun, Yuki Mitsufuji","title":"HERO: Human-Feedback Efficient Reinforcement Learning for Online\n  Diffusion Model Finetuning","comments":"Published in International Conference on Learning Representations\n  (ICLR) 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Controllable generation through Stable Diffusion (SD) fine-tuning aims to\nimprove fidelity, safety, and alignment with human guidance. Existing\nreinforcement learning from human feedback methods usually rely on predefined\nheuristic reward functions or pretrained reward models built on large-scale\ndatasets, limiting their applicability to scenarios where collecting such data\nis costly or difficult. To effectively and efficiently utilize human feedback,\nwe develop a framework, HERO, which leverages online human feedback collected\non the fly during model learning. Specifically, HERO features two key\nmechanisms: (1) Feedback-Aligned Representation Learning, an online training\nmethod that captures human feedback and provides informative learning signals\nfor fine-tuning, and (2) Feedback-Guided Image Generation, which involves\ngenerating images from SD's refined initialization samples, enabling faster\nconvergence towards the evaluator's intent. We demonstrate that HERO is 4x more\nefficient in online feedback for body part anomaly correction compared to the\nbest existing method. Additionally, experiments show that HERO can effectively\nhandle tasks like reasoning, counting, personalization, and reducing NSFW\ncontent with only 0.5K online feedback. The code and project page are available\nat https:\/\/hero-dm.github.io\/.\n","versions":"[{'version': 'v1', 'created': 'Mon, 7 Oct 2024 15:12:01 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Mar 2025 17:11:55 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 08:12:07 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Hiranaka', 'Ayano', ''], ['Chen', 'Shang-Fu', ''], ['Lai', 'Chieh-Hsin', ''], ['Kim', 'Dongjun', ''], ['Murata', 'Naoki', ''], ['Shibuya', 'Takashi', ''], ['Liao', 'Wei-Hsiang', ''], ['Sun', 'Shao-Hua', ''], ['Mitsufuji', 'Yuki', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Feedback-Aligned Representation Learning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Feedback-Guided Image Generation', 'label': 'Few-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2410.06846,"submitter":"Mutian He","authors":"Mutian He, Philip N. Garner","title":"Joint Fine-tuning and Conversion of Pretrained Speech and Language\n  Models towards Linear Complexity","comments":"18 pages, 5 figures; ICLR 2025 camera ready. Code:\n  https:\/\/github.com\/idiap\/linearize-distill-pretrained-transformers","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG cs.SD eess.AS","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Architectures such as Linformer and Mamba have recently emerged as\ncompetitive linear time replacements for transformers. However, corresponding\nlarge pretrained models are often unavailable, especially in non-text domains.\nTo remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)\napproach that jointly converts a transformer model to a linear time substitute\nand fine-tunes it to a target task. We also compare several means to guide the\nfine-tuning to optimally retain the desired inference capability from the\noriginal model. The methods differ in their use of the target model and the\ntrajectory of the parameters. In a series of empirical studies on language\nprocessing, language modeling, and speech processing, we show that CALD can\neffectively recover the result of the original model, and that the guiding\nstrategy contributes to the result. Some reasons for the variation are\nsuggested.\n","versions":"[{'version': 'v1', 'created': 'Wed, 9 Oct 2024 13:06:43 GMT'}, {'version': 'v2', 'created': 'Mon, 23 Dec 2024 13:53:32 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Feb 2025 13:08:42 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 16:17:19 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['He', 'Mutian', ''], ['Garner', 'Philip N.', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'guiding\\nstrategy', 'label': 'Prompting'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2410.07659,"submitter":"Sparsh Mittal","authors":"Onkar Susladkar, Jishu Sen Gupta, Chirag Sehgal, Sparsh Mittal, Rekha\n  Singhal","title":"MotionAura: Generating High-Quality and Motion Consistent Videos using\n  Discrete Diffusion","comments":"Accepted in ICLR 2025 (spotlight paper)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  The spatio-temporal complexity of video data presents significant challenges\nin tasks such as compression, generation, and inpainting. We present four key\ncontributions to address the challenges of spatiotemporal video processing.\nFirst, we introduce the 3D Mobile Inverted Vector-Quantization Variational\nAutoencoder (3D-MBQ-VAE), which combines Variational Autoencoders (VAEs) with\nmasked token modeling to enhance spatiotemporal video compression. The model\nachieves superior temporal consistency and state-of-the-art (SOTA)\nreconstruction quality by employing a novel training strategy with full frame\nmasking. Second, we present MotionAura, a text-to-video generation framework\nthat utilizes vector-quantized diffusion models to discretize the latent space\nand capture complex motion dynamics, producing temporally coherent videos\naligned with text prompts. Third, we propose a spectral transformer-based\ndenoising network that processes video data in the frequency domain using the\nFourier Transform. This method effectively captures global context and\nlong-range dependencies for high-quality video generation and denoising.\nLastly, we introduce a downstream task of Sketch Guided Video Inpainting. This\ntask leverages Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning.\nOur models achieve SOTA performance on a range of benchmarks. Our work offers\nrobust frameworks for spatiotemporal modeling and user-driven video content\nmanipulation. We will release the code, datasets, and models in open-source.\n","versions":"[{'version': 'v1', 'created': 'Thu, 10 Oct 2024 07:07:56 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 05:19:31 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Susladkar', 'Onkar', ''], ['Gupta', 'Jishu Sen', ''], ['Sehgal', 'Chirag', ''], ['Mittal', 'Sparsh', ''], ['Singhal', 'Rekha', '']]","extracted_entities":"[{'text': 'full frame\\nmasking', 'label': 'quantisation'}, {'text': 'text prompts', 'label': 'Prompting'}, {'text': 'Fourier Transform', 'label': 'quantisation'}, {'text': 'Sketch Guided Video Inpainting', 'label': 'contextual Embedding'}, {'text': 'parameter-efficient fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"parameter-efficient fine-tuning","similarity_score":0.7545099258}
{"id":2410.08633,"submitter":"Juno Kim","authors":"Juno Kim and Taiji Suzuki","title":"Transformers Provably Solve Parity Efficiently with Chain of Thought","comments":"ICLR 2025 Oral","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ML","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This work provides the first theoretical analysis of training transformers to\nsolve complex problems by recursively generating intermediate states, analogous\nto fine-tuning for chain-of-thought (CoT) reasoning. We consider training a\none-layer transformer to solve the fundamental $k$-parity problem, extending\nthe work on RNNs by Wies et al. (2023). We establish three key results: (1) any\nfinite-precision gradient-based algorithm, without intermediate supervision,\nrequires substantial iterations to solve parity with finite samples. (2) In\ncontrast, when intermediate parities are incorporated into the loss function,\nour model can learn parity in one gradient update when aided by \\emph{teacher\nforcing}, where ground-truth labels of the reasoning chain are provided at each\ngeneration step. (3) Even without teacher forcing, where the model must\ngenerate CoT chains end-to-end, parity can be learned efficiently if augmented\ndata is employed to internally verify the soundness of intermediate steps. Our\nfindings, supported by numerical experiments, show that task decomposition and\nstepwise reasoning naturally arise from optimizing transformers with CoT;\nmoreover, self-consistency checking can improve multi-step reasoning ability,\naligning with empirical studies of CoT.\n","versions":"[{'version': 'v1', 'created': 'Fri, 11 Oct 2024 08:55:17 GMT'}, {'version': 'v2', 'created': 'Mon, 25 Nov 2024 03:39:51 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 14:26:41 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Kim', 'Juno', ''], ['Suzuki', 'Taiji', '']]","extracted_entities":"[{'text': 'transformers', 'label': 'Transformers'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'chain-of-thought', 'label': 'Chain of thought'}, {'text': 'CoT', 'label': 'Chain of thought'}, {'text': 'teacher\\nforcing', 'label': 'Prompting'}, {'text': 'teacher forcing', 'label': 'Prompting'}, {'text': 'CoT', 'label': 'Chain of thought'}, {'text': 'transformers', 'label': 'Transformers'}, {'text': 'CoT', 'label': 'Chain of thought'}, {'text': 'CoT', 'label': 'Chain of thought'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2410.12854,"submitter":"Weibin Liao","authors":"Weibin Liao, Xu Chu, Yasha Wang","title":"TPO: Aligning Large Language Models with Multi-branch & Multi-step\n  Preference Trees","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In the domain of complex reasoning tasks, such as mathematical reasoning,\nrecent advancements have proposed the use of Direct Preference Optimization\n(DPO) to suppress output of dispreferred responses, thereby enhancing the\nlong-chain reasoning capabilities of large language models (LLMs). To this end,\nthese studies employed LLMs to generate preference trees via Tree-of-thoughts\n(ToT) and sample the paired preference responses required by the DPO algorithm.\nHowever, the DPO algorithm based on binary preference optimization is unable to\nlearn multiple responses with varying degrees of preference\/dispreference that\nprovided by the preference trees, resulting in incomplete preference learning.\nIn this work, we introduce Tree Preference Optimization (TPO), that does not\nsample paired preference responses from the preference tree; instead, it\ndirectly learns from the entire preference tree during the fine-tuning.\nSpecifically, TPO formulates the language model alignment as a Preference List\nRanking problem, where the policy can potentially learn more effectively from a\nranked preference list of responses given the prompt. In addition, to further\nassist LLMs in identifying discriminative steps within long-chain reasoning and\nincrease the relative reward margin in the preference list, TPO utilizes\nAdaptive Step Reward to adjust the reward values of each step in trajectory for\nperforming fine-grained preference optimization. We carry out extensive\nexperiments on mathematical reasoning tasks to evaluate TPO. The experimental\nresults indicate that TPO consistently outperforms DPO across five public large\nlanguage models on four datasets.\n","versions":"[{'version': 'v1', 'created': 'Thu, 10 Oct 2024 22:22:05 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 06:40:44 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Liao', 'Weibin', ''], ['Chu', 'Xu', ''], ['Wang', 'Yasha', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'Tree-of-thoughts', 'label': 'Chain of thought'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'large\\nlanguage models', 'label': 'Large Language Model'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2410.20293,"submitter":"Yuchen Cao","authors":"Yunchong Liu, Xiaorui Shen, Yeyubei Zhang, Zhongyan Wang, Yexin Tian,\n  Jianglai Dai, and Yuchen Cao","title":"A Systematic Review of Machine Learning Approaches for Detecting\n  Deceptive Activities on Social Media: Methods, Challenges, and Biases","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ML","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Social media platforms like Twitter, Facebook, and Instagram have facilitated\nthe spread of misinformation, necessitating automated detection systems. This\nsystematic review evaluates 36 studies that apply machine learning (ML) and\ndeep learning (DL) models to detect fake news, spam, and fake accounts on\nsocial media. Using the Prediction model Risk Of Bias ASsessment Tool\n(PROBAST), the review identified key biases across the ML lifecycle: selection\nbias due to non-representative sampling, inadequate handling of class\nimbalance, insufficient linguistic preprocessing (e.g., negations), and\ninconsistent hyperparameter tuning. Although models such as Support Vector\nMachines (SVM), Random Forests, and Long Short-Term Memory (LSTM) networks\nshowed strong potential, over-reliance on accuracy as an evaluation metric in\nimbalanced data settings was a common flaw. The review highlights the need for\nimproved data preprocessing (e.g., resampling techniques), consistent\nhyperparameter tuning, and the use of appropriate metrics like precision,\nrecall, F1 score, and AUROC. Addressing these limitations can lead to more\nreliable and generalizable ML\/DL models for detecting deceptive content,\nultimately contributing to the reduction of misinformation on social media.\n","versions":"[{'version': 'v1', 'created': 'Sat, 26 Oct 2024 23:55:50 GMT'}, {'version': 'v2', 'created': 'Mon, 9 Dec 2024 20:22:10 GMT'}, {'version': 'v3', 'created': 'Sat, 15 Feb 2025 07:58:19 GMT'}, {'version': 'v4', 'created': 'Sun, 9 Mar 2025 07:42:04 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Liu', 'Yunchong', ''], ['Shen', 'Xiaorui', ''], ['Zhang', 'Yeyubei', ''], ['Wang', 'Zhongyan', ''], ['Tian', 'Yexin', ''], ['Dai', 'Jianglai', ''], ['Cao', 'Yuchen', '']]","extracted_entities":"[{'text': 'selection\\nbias', 'label': 'Model Bias and Fairness'}, {'text': 'inadequate handling of class\\nimbalance', 'label': 'Model Bias and Fairness'}, {'text': 'inconsistent hyperparameter tuning', 'label': 'Fine-tuning'}, {'text': 'consistent\\nhyperparameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"consistent\nhyperparameter tuning","similarity_score":0.6018594503}
{"id":2410.22743,"submitter":"Mingtang Deng Prof.","authors":"Mingtang Deng, Chunlin Yu, Guangyao Huang, P. Caroff, and H. Q. Xu","title":"Quantum transport in an ambipolar InSb nanowire quantum dot device","comments":null,"journal-ref":"Phys. Rev. B 111, 115409 (2025)","doi":"10.1103\/PhysRevB.111.115409","report-no":null,"categories":"cond-mat.mes-hall","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Semiconductor InSb nanowires present a highly intriguing platform with\nimmense potential for applications in spintronics and topological quantum\ndevices. The narrow band gap exhibited by InSb allows for precise tuning of\nthese nanowires, facilitating smooth transitions between the electron transport\nregion and the hole transport region. In this study, we demonstrate quantum\ntransport measurements obtained from a high-quality InSb nanowire quantum dot\ndevice. By utilizing a back gate, this device can be adjusted from an\nelectron-populated quantum dot regime to a hole-populated one. Within both\nregimes, we have observed dozens of consecutive quantum levels without any\ncharge rearrangement or impurity-induced interruptions. Our investigations in\nthe electron transport regime have explored phenomena such as Coulomb blockade\neffect, Zeeman effect,and Kondo effect. Meanwhile, in the hole-transport\nregime, we have identified conductance peaks induced by lead states.\nParticularly, we have created a tomographic analysis method of these lead\nstates by tracking the behavior of these conductance peaks across consecutive\nCoulomb diamond structures.\n","versions":"[{'version': 'v1', 'created': 'Wed, 30 Oct 2024 06:56:15 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 01:01:15 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Deng', 'Mingtang', ''], ['Yu', 'Chunlin', ''], ['Huang', 'Guangyao', ''], ['Caroff', 'P.', ''], ['Xu', 'H. Q.', '']]","extracted_entities":"[{'text': 'precise tuning', 'label': 'Fine-tuning'}, {'text': 'Coulomb blockade\\neffect', 'label': 'quantisation'}, {'text': 'Zeeman effect', 'label': 'quantisation'}, {'text': 'Kondo effect', 'label': 'quantisation'}]","assigned_concept":"Fine-tuning","matched_keyword":"precise tuning","similarity_score":0.7851148248}
{"id":2410.22796,"submitter":"Viggo Moro","authors":"Viggo Moro, Luiz F. O. Chamon","title":"Solving Differential Equations with Constrained Learning","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  (Partial) differential equations (PDEs) are fundamental tools for describing\nnatural phenomena, making their solution crucial in science and engineering.\nWhile traditional methods, such as the finite element method, provide reliable\nsolutions, their accuracy is often tied to the use of computationally intensive\nfine meshes. Moreover, they do not naturally account for measurements or prior\nsolutions, and any change in the problem parameters requires results to be\nfully recomputed. Neural network-based approaches, such as physics-informed\nneural networks and neural operators, offer a mesh-free alternative by directly\nfitting those models to the PDE solution. They can also integrate prior\nknowledge and tackle entire families of PDEs by simply aggregating additional\ntraining losses. Nevertheless, they are highly sensitive to hyperparameters\nsuch as collocation points and the weights associated with each loss. This\npaper addresses these challenges by developing a science-constrained learning\n(SCL) framework. It demonstrates that finding a (weak) solution of a PDE is\nequivalent to solving a constrained learning problem with worst-case losses.\nThis explains the limitations of previous methods that minimize the expected\nvalue of aggregated losses. SCL also organically integrates structural\nconstraints (e.g., invariances) and (partial) measurements or known solutions.\nThe resulting constrained learning problems can be tackled using a practical\nalgorithm that yields accurate solutions across a variety of PDEs, neural\nnetwork architectures, and prior knowledge levels without extensive\nhyperparameter tuning and sometimes even at a lower computational cost.\n","versions":"[{'version': 'v1', 'created': 'Wed, 30 Oct 2024 08:20:39 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 15:22:06 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Moro', 'Viggo', ''], ['Chamon', 'Luiz F. O.', '']]","extracted_entities":"[{'text': 'PDEs', 'label': 'BERT'}, {'text': 'PDEs', 'label': 'BERT'}, {'text': 'PDEs', 'label': 'BERT'}, {'text': 'extensive\\nhyperparameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"extensive\nhyperparameter tuning","similarity_score":0.639097929}
{"id":2411.13022,"submitter":"Ya\\c{s}ar Utku Al\\c{c}alar","authors":"Ya\\c{s}ar Utku Al\\c{c}alar, Merve G\\\"ulle, Mehmet Ak\\c{c}akaya","title":"Fast MRI for All: Bridging Equity Gaps via Training without Raw Data\n  Access","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.AI cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Physics-driven deep learning (PD-DL) approaches have become popular for\nimproved reconstruction of fast magnetic resonance imaging (MRI) scans. Though\nPD-DL offers higher acceleration rates than existing clinical fast MRI\ntechniques, their use has been limited outside specialized MRI centers. A key\nchallenge is generalization to underrepresented pathologies or populations,\nnoted in multiple studies, with fine-tuning on target populations suggested for\nimprovement. However, current approaches for PD-DL training require access to\nraw k-space measurements, which is typically only available at specialized MRI\ncenters that have research agreements for such data access. This is especially\nan issue for rural and underserved areas, where commercial MRI scanners only\nprovide access to a final reconstructed image. To tackle these challenges, we\npropose Compressibility-inspired Unsupervised Learning via Parallel Imaging\nFidelity (CUPID) for high-quality PD-DL training using only routine clinical\nreconstructed images exported from an MRI scanner. CUPID evaluates output\nquality with a compressibility-based approach while ensuring that the output\nstays consistent with the clinical parallel imaging reconstruction through\nwell-designed perturbations. Our results show CUPID achieves similar quality to\nestablished PD-DL training that requires k-space data while outperforming\ncompressed sensing (CS) and diffusion-based generative methods. We further\ndemonstrate its effectiveness in a zero-shot training setup for retrospectively\nand prospectively sub-sampled acquisitions, attesting to its minimal training\nburden. As an approach that radically deviates from existing strategies, CUPID\npresents an opportunity to provide equitable access to fast MRI for underserved\npopulations in an attempt to reduce the inequalities associated with this\nexpensive imaging modality.\n","versions":"[{'version': 'v1', 'created': 'Wed, 20 Nov 2024 03:53:41 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 15:54:28 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Al\u00e7alar', 'Ya\u015far Utku', ''], ['G\u00fclle', 'Merve', ''], ['Ak\u00e7akaya', 'Mehmet', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'research agreements', 'label': 'AI Ethics'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2411.1558,"submitter":"Ryugo Morita","authors":"Ryugo Morita, Stanislav Frolov, Brian Bernhard Moser, Takahiro\n  Shirakawa, Ko Watanabe, Andreas Dengel, Jinjia Zhou","title":"TKG-DM: Training-free Chroma Key Content Generation Diffusion Model","comments":"Accepted to CVPR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Diffusion models have enabled the generation of high-quality images with a\nstrong focus on realism and textual fidelity. Yet, large-scale text-to-image\nmodels, such as Stable Diffusion, struggle to generate images where foreground\nobjects are placed over a chroma key background, limiting their ability to\nseparate foreground and background elements without fine-tuning. To address\nthis limitation, we present a novel Training-Free Chroma Key Content Generation\nDiffusion Model (TKG-DM), which optimizes the initial random noise to produce\nimages with foreground objects on a specifiable color background. Our proposed\nmethod is the first to explore the manipulation of the color aspects in initial\nnoise for controlled background generation, enabling precise separation of\nforeground and background without fine-tuning. Extensive experiments\ndemonstrate that our training-free method outperforms existing methods in both\nqualitative and quantitative evaluations, matching or surpassing fine-tuned\nmodels. Finally, we successfully extend it to other tasks (e.g., consistency\nmodels and text-to-video), highlighting its transformative potential across\nvarious generative applications where independent control of foreground and\nbackground is crucial.\n","versions":"[{'version': 'v1', 'created': 'Sat, 23 Nov 2024 15:07:15 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 02:37:06 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Morita', 'Ryugo', ''], ['Frolov', 'Stanislav', ''], ['Moser', 'Brian Bernhard', ''], ['Shirakawa', 'Takahiro', ''], ['Watanabe', 'Ko', ''], ['Dengel', 'Andreas', ''], ['Zhou', 'Jinjia', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2411.17274,"submitter":"Yikun Li","authors":"Yikun Li, Ting Zhang, Ratnadira Widyasari, Yan Naing Tun, Huu Hung\n  Nguyen, Tan Bui, Ivana Clairine Irsan, Yiran Cheng, Xiang Lan, Han Wei Ang,\n  Frank Liauw, Martin Weyssow, Hong Jin Kang, Eng Lieh Ouh, Lwin Khin Shar,\n  David Lo","title":"CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.CR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 8,203\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul.\n","versions":"[{'version': 'v1', 'created': 'Tue, 26 Nov 2024 09:51:55 GMT'}, {'version': 'v2', 'created': 'Thu, 28 Nov 2024 03:52:23 GMT'}, {'version': 'v3', 'created': 'Thu, 16 Jan 2025 04:08:15 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 10:41:04 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Yikun', ''], ['Zhang', 'Ting', ''], ['Widyasari', 'Ratnadira', ''], ['Tun', 'Yan Naing', ''], ['Nguyen', 'Huu Hung', ''], ['Bui', 'Tan', ''], ['Irsan', 'Ivana Clairine', ''], ['Cheng', 'Yiran', ''], ['Lan', 'Xiang', ''], ['Ang', 'Han Wei', ''], ['Liauw', 'Frank', ''], ['Weyssow', 'Martin', ''], ['Kang', 'Hong Jin', ''], ['Ouh', 'Eng Lieh', ''], ['Shar', 'Lwin Khin', ''], ['Lo', 'David', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2411.17376,"submitter":"Ryo Fujii","authors":"Ryo Fujii, Hideo Saito and Ryo Hachiuma","title":"RealTraj: Towards Real-World Pedestrian Trajectory Forecasting","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  This paper jointly addresses three key limitations in conventional pedestrian\ntrajectory forecasting: pedestrian perception errors, real-world data\ncollection costs, and person ID annotation costs. We propose a novel framework,\nRealTraj, that enhances the real-world applicability of trajectory forecasting.\nOur approach includes two training phases -- self-supervised pretraining on\nsynthetic data and weakly-supervised fine-tuning with limited real-world data\n-- to minimize data collection efforts. To improve robustness to real-world\nerrors, we focus on both model design and training objectives. Specifically, we\npresent Det2TrajFormer, a trajectory forecasting model that remains invariant\nto tracking noise by using past detections as inputs. Additionally, we pretrain\nthe model using multiple pretext tasks, which enhance robustness and improve\nforecasting performance based solely on detection data. Unlike previous\ntrajectory forecasting methods, our approach fine-tunes the model using only\nground-truth detections, reducing the need for costly person ID annotations. In\nthe experiments, we comprehensively verify the effectiveness of the proposed\nmethod against the limitations, and the method outperforms state-of-the-art\ntrajectory forecasting methods on multiple datasets. The code will be released\nat https:\/\/fujiry0.github.io\/RealTraj-project-page.\n","versions":"[{'version': 'v1', 'created': 'Tue, 26 Nov 2024 12:35:26 GMT'}, {'version': 'v2', 'created': 'Wed, 27 Nov 2024 06:08:02 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 13:26:35 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Fujii', 'Ryo', ''], ['Saito', 'Hideo', ''], ['Hachiuma', 'Ryo', '']]","extracted_entities":"[{'text': 'weakly-supervised fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"weakly-supervised fine-tuning","similarity_score":0.6190373898}
{"id":2412.00071,"submitter":"Jinqi Xiao","authors":"Jinqi Xiao, Shen Sang, Tiancheng Zhi, Jing Liu, Qing Yan, Yuqian\n  Zhang, Linjie Luo, Bo Yuan","title":"COAP: Memory-Efficient Training with Correlation-Aware Gradient\n  Projection","comments":"CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Training large-scale neural networks in vision, and multimodal domains\ndemands substantial memory resources, primarily due to the storage of optimizer\nstates. While LoRA, a popular parameter-efficient method, reduces memory usage,\nit often suffers from suboptimal performance due to the constraints of low-rank\nupdates. Low-rank gradient projection methods (e.g., GaLore, Flora) reduce\noptimizer memory by projecting gradients and moment estimates into low-rank\nspaces via singular value decomposition or random projection. However, they\nfail to account for inter-projection correlation, causing performance\ndegradation, and their projection strategies often incur high computational\ncosts. In this paper, we present COAP (Correlation-Aware Gradient Projection),\na memory-efficient method that minimizes computational overhead while\nmaintaining training performance. Evaluated across various vision, language,\nand multimodal tasks, COAP outperforms existing methods in both training speed\nand model performance. For LLaMA-1B, it reduces optimizer memory by 61% with\nonly 2% additional time cost, achieving the same PPL as AdamW. With 8-bit\nquantization, COAP cuts optimizer memory by 81% and achieves 4x speedup over\nGaLore for LLaVA-v1.5-7B fine-tuning, while delivering higher accuracy.\n","versions":"[{'version': 'v1', 'created': 'Tue, 26 Nov 2024 03:50:52 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 00:36:08 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Xiao', 'Jinqi', ''], ['Sang', 'Shen', ''], ['Zhi', 'Tiancheng', ''], ['Liu', 'Jing', ''], ['Yan', 'Qing', ''], ['Zhang', 'Yuqian', ''], ['Luo', 'Linjie', ''], ['Yuan', 'Bo', '']]","extracted_entities":"[{'text': '8-bit\\nquantization', 'label': 'quantisation'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2412.00136,"submitter":"Wenda Shi","authors":"Wenda Shi and Yiren Song and Dengming Zhang and Jiaming Liu and\n  Xingxing Zou","title":"FonTS: Text Rendering with Typography and Style Controls","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Visual text rendering are widespread in various real-world applications,\nrequiring careful font selection and typographic choices. Recent progress in\ndiffusion transformer (DiT)-based text-to-image (T2I) models show promise in\nautomating these processes. However, these methods still encounter challenges\nlike inconsistent fonts, style variation, and limited fine-grained control,\nparticularly at the word-level. This paper proposes a two-stage DiT-based\npipeline to address these problems by enhancing controllability over typography\nand style in text rendering. We introduce typography control fine-tuning\n(TC-FT), an parameter-efficient fine-tuning method (on $5\\%$ key parameters)\nwith enclosing typography control tokens (ETC-tokens), which enables precise\nword-level application of typographic features. To further address style\ninconsistency in text rendering, we propose a text-agnostic style control\nadapter (SCA) that prevents content leakage while enhancing style consistency.\nTo implement TC-FT and SCA effectively, we incorporated HTML-render into the\ndata synthesis pipeline and proposed the first word-level controllable dataset.\nThrough comprehensive experiments, we demonstrate the effectiveness of our\napproach in achieving superior word-level typographic control, font\nconsistency, and style consistency in text rendering tasks. The datasets and\nmodels will be available for academic use.\n","versions":"[{'version': 'v1', 'created': 'Thu, 28 Nov 2024 16:19:37 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 08:43:03 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Shi', 'Wenda', ''], ['Song', 'Yiren', ''], ['Zhang', 'Dengming', ''], ['Liu', 'Jiaming', ''], ['Zou', 'Xingxing', '']]","extracted_entities":"[{'text': 'typography control fine-tuning', 'label': 'Fine-tuning'}, {'text': 'TC-FT', 'label': 'Fine-tuning'}, {'text': 'TC-FT', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"typography control fine-tuning","similarity_score":0.527905643}
{"id":2412.00139,"submitter":"Muhammad Huzaifa","authors":"Muhammad Huzaifa, Yova Kementchedjhieva","title":"EFSA: Episodic Few-Shot Adaptation for Text-to-Image Retrieval","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Text-to-image retrieval is a critical task for managing diverse visual\ncontent, but common benchmarks for the task rely on small, single-domain\ndatasets that fail to capture real-world complexity. Pre-trained\nvision-language models tend to perform well with easy negatives but struggle\nwith hard negatives--visually similar yet incorrect images--especially in\nopen-domain scenarios. To address this, we introduce Episodic Few-Shot\nAdaptation (EFSA), a novel test-time framework that adapts pre-trained models\ndynamically to a query's domain by fine-tuning on top-k retrieved candidates\nand synthetic captions generated for them. EFSA improves performance across\ndiverse domains while preserving generalization, as shown in evaluations on\nqueries from eight highly distinct visual domains and an open-domain retrieval\npool of over one million images. Our work highlights the potential of episodic\nfew-shot adaptation to enhance robustness in the critical and understudied task\nof open-domain text-to-image retrieval.\n","versions":"[{'version': 'v1', 'created': 'Thu, 28 Nov 2024 17:09:20 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 09:54:42 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Huzaifa', 'Muhammad', ''], ['Kementchedjhieva', 'Yova', '']]","extracted_entities":"[{'text': 'Episodic Few-Shot\\nAdaptation', 'label': 'Few-shot Learning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'episodic\\nfew-shot adaptation', 'label': 'Few-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2412.00196,"submitter":"Xiyuan Gao","authors":"Xiyuan Gao","title":"Spontaneous CP Violation and Flavor Changing Neutral Currents in Minimal\n  SO(10)","comments":"24 pages, 4 figures; version published in PRD","journal-ref":"Phys. Rev. D 111, 055013 (2025)","doi":"10.1103\/PhysRevD.111.055013","report-no":null,"categories":"hep-ph hep-ex","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We explore spontaneous CP violation (SCPV) in the minimal non-supersymmetric\nSO(10) grand unified theory (GUT), with a scalar sector comprising a CP-even\n$45_H$, a $126_H$, and a complex $10_H$. All renormalizable couplings are real\ndue to CP symmetry, and the Kobayashi-Maskawa phase arises solely from complex\nelectroweak vacuum expectation values. The model requires an additional Higgs\ndoublet fine-tuned below 500 GeV and constrains new Yukawa couplings, linking\ncertain flavor-violating (FV) processes. Future proton decay observations may\nreveal correlated FV decay ratios, offering insights into minimal SO(10).\n","versions":"[{'version': 'v1', 'created': 'Fri, 29 Nov 2024 19:00:26 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 09:01:27 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Gao', 'Xiyuan', '']]","extracted_entities":"[{'text': 'fine-tuned below 500 GeV', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuned below 500 GeV","similarity_score":0.5089265108}
{"id":2412.01254,"submitter":"Liangwei Jiang","authors":"Liangwei Jiang, Ruida Li, Zhifeng Zhang, Shuo Fang, Chenguang Ma","title":"EmojiDiff: Advanced Facial Expression Control with High Identity\n  Preservation in Portrait Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper aims to bring fine-grained expression control while maintaining\nhigh-fidelity identity in portrait generation. This is challenging due to the\nmutual interference between expression and identity: (i) fine expression\ncontrol signals inevitably introduce appearance-related semantics (e.g., facial\ncontours, and ratio), which impact the identity of the generated portrait; (ii)\neven coarse-grained expression control can cause facial changes that compromise\nidentity, since they all act on the face. These limitations remain unaddressed\nby previous generation methods, which primarily rely on coarse control signals\nor two-stage inference that integrates portrait animation. Here, we introduce\nEmojiDiff, the first end-to-end solution that enables simultaneous control of\nextremely detailed expression (RGB-level) and high-fidelity identity in\nportrait generation. To address the above challenges, EmojiDiff adopts a\ntwo-stage scheme involving decoupled training and fine-tuning. For decoupled\ntraining, we innovate ID-irrelevant Data Iteration (IDI) to synthesize\ncross-identity expression pairs by dividing and optimizing the processes of\nmaintaining expression and altering identity, thereby ensuring stable and\nhigh-quality data generation. Training the model with this data, we effectively\ndisentangle fine expression features in the expression template from other\nextraneous information (e.g., identity, skin). Subsequently, we present\nID-enhanced Contrast Alignment (ICA) for further fine-tuning. ICA achieves\nrapid reconstruction and joint supervision of identity and expression\ninformation, thus aligning identity representations of images with and without\nexpression control. Experimental results demonstrate that our method remarkably\noutperforms counterparts, achieves precise expression control with highly\nmaintained identity, and generalizes well to various diffusion models.\n","versions":"[{'version': 'v1', 'created': 'Mon, 2 Dec 2024 08:24:11 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 08:32:46 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Jiang', 'Liangwei', ''], ['Li', 'Ruida', ''], ['Zhang', 'Zhifeng', ''], ['Fang', 'Shuo', ''], ['Ma', 'Chenguang', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2412.04243,"submitter":"Nicholas Konz","authors":"Yixin Zhang, Nicholas Konz, Kevin Kramer, Maciej A. Mazurowski","title":"Quantifying the Limits of Segmentation Foundation Models: Modeling\n  Challenges in Segmenting Tree-Like and Low-Contrast Objects","comments":"Code: https:\/\/github.com\/mazurowski-lab\/SAM-TexturalConfusion-Metrics","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG eess.IV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Image segmentation foundation models (SFMs) like Segment Anything Model (SAM)\nhave achieved impressive zero-shot and interactive segmentation across diverse\ndomains. However, they struggle to segment objects with certain structures,\nparticularly those with dense, tree-like morphology and low textural contrast\nfrom their surroundings. These failure modes are crucial for understanding the\nlimitations of SFMs in real-world applications. To systematically study this\nissue, we introduce interpretable metrics quantifying object tree-likeness and\ntextural separability. On carefully controlled synthetic experiments and\nreal-world datasets, we show that SFM performance (e.g., SAM, SAM 2, HQ-SAM)\nnoticeably correlates with these factors. We link these failures to \"textural\nconfusion\", where models misinterpret local structure as global texture,\ncausing over-segmentation or difficulty distinguishing objects from similar\nbackgrounds. Notably, targeted fine-tuning fails to resolve this issue,\nindicating a fundamental limitation. Our study provides the first quantitative\nframework for modeling the behavior of SFMs on challenging structures, offering\ninterpretable insights into their segmentation capabilities.\n","versions":"[{'version': 'v1', 'created': 'Thu, 5 Dec 2024 15:25:51 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 14:42:44 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhang', 'Yixin', ''], ['Konz', 'Nicholas', ''], ['Kramer', 'Kevin', ''], ['Mazurowski', 'Maciej A.', '']]","extracted_entities":"[{'text': 'SFMs', 'label': 'Foundation Model'}, {'text': 'Segment Anything Model', 'label': 'Foundation Model'}, {'text': 'SAM', 'label': 'Foundation Model'}, {'text': 'SFMs', 'label': 'Foundation Model'}, {'text': 'SAM', 'label': 'Foundation Model'}, {'text': 'SAM 2', 'label': 'Foundation Model'}, {'text': 'HQ-SAM', 'label': 'Foundation Model'}, {'text': 'targeted fine-tuning', 'label': 'Fine-tuning'}, {'text': 'SFMs', 'label': 'Foundation Model'}]","assigned_concept":"Fine-tuning","matched_keyword":"targeted fine-tuning","similarity_score":0.8541433811}
{"id":2412.04829,"submitter":"Mohammad Bajelani","authors":"Nima Maghooli, Omid Mahdizadeh, Mohammad Bajelani, S. Ali A. Moosavian","title":"Learning-based Control for Tendon-Driven Continuum Robotic Arms","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.SY eess.SY","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  This paper presents a learning-based approach for centralized position\ncontrol of Tendon Driven Continuum Robots (TDCRs) using Deep Reinforcement\nLearning (DRL), with a particular focus on the Sim-to-Real transfer of control\npolicies. The proposed control method employs the Modified Transpose Jacobian\n(MTJ) control strategy, with its parameters optimally tuned using the Deep\nDeterministic Policy Gradient (DDPG) algorithm. Classical model-based\ncontrollers encounter significant challenges due to the inherent uncertainties\nand nonlinear dynamics of continuum robots. In contrast, model-free control\nstrategies require efficient gain-tuning to handle diverse operational\nscenarios. This research aims to develop a model-free controller with\nperformance comparable to model-based strategies by integrating an optimal\nadaptive gain-tuning system. Both simulations and real-world implementations\ndemonstrate that the proposed method significantly enhances the\ntrajectory-tracking performance of continuum robots independent of initial\nconditions and paths within the operational task-space, effectively\nestablishing a task-free controller.\n","versions":"[{'version': 'v1', 'created': 'Fri, 6 Dec 2024 07:46:23 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 09:36:16 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Maghooli', 'Nima', ''], ['Mahdizadeh', 'Omid', ''], ['Bajelani', 'Mohammad', ''], ['Moosavian', 'S. Ali A.', '']]","extracted_entities":"[{'text': 'Deep Reinforcement\\nLearning', 'label': 'Few-shot Learning'}, {'text': 'optimally tuned', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"optimally tuned","similarity_score":0.6957836151}
{"id":2412.05673,"submitter":"Saptarshi Chakraborty","authors":"Saptarshi Chakraborty, Kshitij Khare, George Michailidis","title":"A generalized Bayesian approach for high-dimensional robust regression\n  with serially correlated errors and predictors","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ME math.ST stat.CO stat.TH","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper introduces a loss-based generalized Bayesian methodology for\nhigh-dimensional robust regression with serially correlated errors and\npredictors. The proposed framework employs a novel scaled pseudo-Huber (SPH)\nloss function, which smooths the well-known Huber loss, effectively balancing\nquadratic ($\\ell_2$) and absolute linear ($\\ell_1$) loss behaviors. This\nflexibility enables the framework to accommodate both thin-tailed and\nheavy-tailed data efficiently. The generalized Bayesian approach constructs a\nworking likelihood based on the SPH loss, facilitating efficient and stable\nestimation while providing rigorous uncertainty quantification for all model\nparameters. Notably, this approach allows formal statistical inference without\nrequiring ad hoc tuning parameter selection while adaptively addressing a wide\nrange of tail behavior in the errors. By specifying appropriate prior\ndistributions for the regression coefficients--such as ridge priors for small\nor moderate-dimensional settings and spike-and-slab priors for high-dimensional\nsettings--the framework ensures principled inference. We establish rigorous\ntheoretical guarantees for accurate parameter estimation and correct predictor\nselection under sparsity assumptions for a wide range of data generating\nsetups. Extensive simulation studies demonstrate the superior performance of\nour approach compared to traditional Bayesian regression methods based on\n$\\ell_2$ and $\\ell_1$-loss functions. The results highlight its flexibility and\nrobustness, particularly in challenging high-dimensional settings characterized\nby data contamination.\n","versions":"[{'version': 'v1', 'created': 'Sat, 7 Dec 2024 14:38:56 GMT'}, {'version': 'v2', 'created': 'Fri, 21 Feb 2025 22:10:18 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 15:27:49 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Chakraborty', 'Saptarshi', ''], ['Khare', 'Kshitij', ''], ['Michailidis', 'George', '']]","extracted_entities":"[{'text': 'ad hoc tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"ad hoc tuning","similarity_score":0.6913866401}
{"id":2412.08619,"submitter":"Mohammadmehdi Ataei","authors":"Vahid Balazadeh, Mohammadmehdi Ataei, Hyunmin Cheong, Amir Hosein\n  Khasahmadi, Rahul G. Krishnan","title":"Physics Context Builders: A Modular Framework for Physical Reasoning in\n  Vision-Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Physical reasoning, which involves interpreting object behaviors within\ndynamic environments, remains a significant challenge for Vision-Language\nModels (VLMs). The limitations in physical reasoning arise from an inability to\ntranslate learned knowledge into predictions about physical behavior. We\nperform a careful study to show how continual fine-tuning can mitigate this\nissue. However, fine-tuning is expensive for large models and impractical to\nrepeatedly perform for every task. This necessitates the creation of modular\nand scalable ways to teach VLMs about physical reasoning. To that end, we\nintroduce Physics Context Builders (PCBs), a novel modular framework where\nspecialized VLMs are fine-tuned to generate detailed physical scene\ndescriptions. These can be used as physical contexts for larger VLMs to enhance\ntheir reasoning capabilities. PCBs enable the separation of visual perception\nfrom reasoning, allowing us to analyze their relative contributions to physical\nunderstanding. We perform careful experiments on CLEVRER and on Falling Tower,\na stability detection dataset with both simulated and real-world scenes, to\ndemonstrate that PCBs provide substantial performance improvements, increasing\naverage accuracy by up to 13.8% on complex physical reasoning tasks. Notably,\nPCBs show strong Sim2Real transfer, successfully generalizing from simulated\ntraining data to real-world scenes. Our work demonstrates that enhancing visual\nperception through modular, simulation-trained components offers a practical\napproach to improving physical reasoning in VLMs, while providing insights into\nthe factors affecting physical understanding in these models.\n","versions":"[{'version': 'v1', 'created': 'Wed, 11 Dec 2024 18:40:16 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 17:01:51 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Balazadeh', 'Vahid', ''], ['Ataei', 'Mohammadmehdi', ''], ['Cheong', 'Hyunmin', ''], ['Khasahmadi', 'Amir Hosein', ''], ['Krishnan', 'Rahul G.', '']]","extracted_entities":"[{'text': 'Vision-Language\\nModels', 'label': 'Large Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'Physics Context Builders', 'label': 'contextual Embedding'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'physical contexts', 'label': 'contextual Embedding'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'PCBs', 'label': 'contextual Embedding'}, {'text': 'PCBs', 'label': 'contextual Embedding'}, {'text': 'VLMs', 'label': 'Large Language Model'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2412.11464,"submitter":"Quan-Sheng Zeng","authors":"Quan-Sheng Zeng, Yunheng Li, Daquan Zhou, Guanbin Li, Qibin Hou,\n  Ming-Ming Cheng","title":"High-Quality Mask Tuning Matters for Open-Vocabulary Segmentation","comments":"Revised version according to comments from reviewers of ICLR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Open-vocabulary image segmentation has been advanced through the synergy\nbetween mask generators and vision-language models like Contrastive\nLanguage-Image Pre-training (CLIP). Previous approaches focus on generating\nmasks while aligning mask features with text embeddings during training. In\nthis paper, we observe that relying on generated low-quality masks can weaken\nthe alignment of vision and language in regional representations. This\nmotivates us to present a new fine-tuning framework, named MaskCLIP++, which\nuses ground-truth masks instead of generated masks to enhance the mask\nclassification capability of CLIP. Due to the limited diversity of image\nsegmentation datasets with mask annotations, we propose incorporating a\nconsistency alignment principle during fine-tuning, which alleviates\ncategorical bias toward the fine-tuning dataset. After low-cost fine-tuning,\nMaskCLIP++ significantly improves the mask classification performance on\nmulti-domain datasets. Combining with the mask generator in previous\nstate-of-the-art mask-based open vocabulary segmentation methods, we achieve\nperformance improvements of +1.7, +2.3, +2.1, +3.1, and +0.3 mIoU on the A-847,\nPC-459, A-150, PC-59, and PAS-20 datasets, respectively. Code is avaliable at\nhttps:\/\/github.com\/HVision-NKU\/MaskCLIPpp .\n","versions":"[{'version': 'v1', 'created': 'Mon, 16 Dec 2024 05:44:45 GMT'}, {'version': 'v2', 'created': 'Tue, 24 Dec 2024 04:13:08 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 08:04:32 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Zeng', 'Quan-Sheng', ''], ['Li', 'Yunheng', ''], ['Zhou', 'Daquan', ''], ['Li', 'Guanbin', ''], ['Hou', 'Qibin', ''], ['Cheng', 'Ming-Ming', '']]","extracted_entities":"[{'text': 'text embeddings', 'label': 'Embedding'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'low-cost fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2412.11706,"submitter":"Wenhao Sun","authors":"Wenhao Sun and Rong-Cheng Tu and Jingyi Liao and Zhao Jin and Dacheng\n  Tao","title":"AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration","comments":"16 pages, 12 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup.\n","versions":"[{'version': 'v1', 'created': 'Mon, 16 Dec 2024 12:28:22 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 16:14:51 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Sun', 'Wenhao', ''], ['Tu', 'Rong-Cheng', ''], ['Liao', 'Jingyi', ''], ['Jin', 'Zhao', ''], ['Tao', 'Dacheng', '']]","extracted_entities":"[{'text': 'Diffusion Transformers', 'label': 'Transformers'}, {'text': 'DiTs', 'label': 'Transformers'}, {'text': 'costly fine-tuning', 'label': 'Fine-tuning'}, {'text': 'DiTs', 'label': 'Transformers'}, {'text': 'DiTs', 'label': 'Transformers'}, {'text': 'attention operation', 'label': 'Attention mechanism'}]","assigned_concept":"Fine-tuning","matched_keyword":"costly fine-tuning","similarity_score":0.8156909943}
{"id":2412.12049,"submitter":"Mohammad Sadegh Salehi","authors":"Mohammad Sadegh Salehi, Subhadip Mukherjee, Lindon Roberts, Matthias\n  J. Ehrhardt","title":"Bilevel Learning with Inexact Stochastic Gradients","comments":"Accepted to the 10th International Conference on Scale Space and\n  Variational Methods in Computer Vision (SSVM 2025)","journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Bilevel learning has gained prominence in machine learning, inverse problems,\nand imaging applications, including hyperparameter optimization, learning\ndata-adaptive regularizers, and optimizing forward operators. The large-scale\nnature of these problems has led to the development of inexact and\ncomputationally efficient methods. Existing adaptive methods predominantly rely\non deterministic formulations, while stochastic approaches often adopt a\ndoubly-stochastic framework with impractical variance assumptions, enforces a\nfixed number of lower-level iterations, and requires extensive tuning. In this\nwork, we focus on bilevel learning with strongly convex lower-level problems\nand a nonconvex sum-of-functions in the upper-level. Stochasticity arises from\ndata sampling in the upper-level which leads to inexact stochastic\nhypergradients. We establish their connection to state-of-the-art stochastic\noptimization theory for nonconvex objectives. Furthermore, we prove the\nconvergence of inexact stochastic bilevel optimization under mild assumptions.\nOur empirical results highlight significant speed-ups and improved\ngeneralization in imaging tasks such as image denoising and deblurring in\ncomparison with adaptive deterministic bilevel methods.\n","versions":"[{'version': 'v1', 'created': 'Mon, 16 Dec 2024 18:18:47 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 12:56:44 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Salehi', 'Mohammad Sadegh', ''], ['Mukherjee', 'Subhadip', ''], ['Roberts', 'Lindon', ''], ['Ehrhardt', 'Matthias J.', '']]","extracted_entities":"[{'text': 'Bilevel learning', 'label': 'Zero-shot Learning'}, {'text': 'hyperparameter optimization', 'label': 'Few-shot Learning'}, {'text': 'extensive tuning', 'label': 'Fine-tuning'}, {'text': 'bilevel learning', 'label': 'Zero-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"extensive tuning","similarity_score":0.8043005466}
{"id":2412.12778,"submitter":"Huihui Fang Miss","authors":"Chengzhou Yu (South China University of Technology), Huihui Fang\n  (Pazhou Laboratory), Hongqiu Wang (The Hong Kong University of Science and\n  Technology (Guangzhou)), Ting Deng (South China University of Technology),\n  Qing Du (South China University of Technology), Yanwu Xu (South China\n  University of Technology), and Weihua Yang (Shenzhen Eye Hospital)","title":"Rethinking Diffusion-Based Image Generators for Fundus Fluorescein\n  Angiography Synthesis on Limited Data","comments":"The first author has a conflict with the data access authority","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Fundus imaging is a critical tool in ophthalmology, with different imaging\nmodalities offering unique advantages. For instance, fundus fluorescein\nangiography (FFA) can accurately identify eye diseases. However, traditional\ninvasive FFA involves the injection of sodium fluorescein, which can cause\ndiscomfort and risks. Generating corresponding FFA images from non-invasive\nfundus images holds significant practical value but also presents challenges.\nFirst, limited datasets constrain the performance and effectiveness of models.\nSecond, previous studies have primarily focused on generating FFA for single\ndiseases or single modalities, often resulting in poor performance for patients\nwith various ophthalmic conditions. To address these issues, we propose a novel\nlatent diffusion model-based framework, Diffusion, which introduces a\nfine-tuning protocol to overcome the challenge of limited medical data and\nunleash the generative capabilities of diffusion models. Furthermore, we\ndesigned a new approach to tackle the challenges of generating across different\nmodalities and disease types. On limited datasets, our framework achieves\nstate-of-the-art results compared to existing methods, offering significant\npotential to enhance ophthalmic diagnostics and patient care. Our code will be\nreleased soon to support further research in this field.\n","versions":"[{'version': 'v1', 'created': 'Tue, 17 Dec 2024 10:37:46 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 02:53:38 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Yu', 'Chengzhou', '', 'South China University of Technology'], ['Fang', 'Huihui', '', 'Pazhou Laboratory'], ['Wang', 'Hongqiu', '', 'The Hong Kong University of Science and\\n  Technology'], ['Deng', 'Ting', '', 'South China University of Technology'], ['Du', 'Qing', '', 'South China University of Technology'], ['Xu', 'Yanwu', '', 'South China\\n  University of Technology'], ['Yang', 'Weihua', '', 'Shenzhen Eye Hospital']]","extracted_entities":"[{'text': 'fine-tuning protocol', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning protocol","similarity_score":0.6145582199}
{"id":2412.13196,"submitter":"Mazeyu Ji","authors":"Mazeyu Ji, Xuanbin Peng, Fangchen Liu, Jialong Li, Ge Yang, Xuxin\n  Cheng, Xiaolong Wang","title":"ExBody2: Advanced Expressive Humanoid Whole-Body Control","comments":"website: https:\/\/exbody2.github.io","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper tackles the challenge of enabling real-world humanoid robots to\nperform expressive and dynamic whole-body motions while maintaining overall\nstability and robustness. We propose Advanced Expressive Whole-Body Control\n(Exbody2), a method for producing whole-body tracking controllers that are\ntrained on both human motion capture and simulated data and then transferred to\nthe real world. We introduce a technique for decoupling the velocity tracking\nof the entire body from tracking body landmarks. We use a teacher policy to\nproduce intermediate data that better conforms to the robot's kinematics and to\nautomatically filter away infeasible whole-body motions. This two-step approach\nenabled us to produce a student policy that can be deployed on the robot that\ncan walk, crouch, and dance. We also provide insight into the trade-off between\nversatility and the tracking performance on specific motions. We observed\nsignificant improvement of tracking performance after fine-tuning on a small\namount of data, at the expense of the others.\n","versions":"[{'version': 'v1', 'created': 'Tue, 17 Dec 2024 18:59:51 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 00:40:43 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Ji', 'Mazeyu', ''], ['Peng', 'Xuanbin', ''], ['Liu', 'Fangchen', ''], ['Li', 'Jialong', ''], ['Yang', 'Ge', ''], ['Cheng', 'Xuxin', ''], ['Wang', 'Xiaolong', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2412.14327,"submitter":"Xijun Wang","authors":"Xijun Wang, Prateek Chennuri, Yu Yuan, Bole Ma, Xingguang Zhang,\n  Stanley Chan","title":"Personalized Generative Low-light Image Denoising and Enhancement","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  While smartphone cameras today can produce astonishingly good photos, their\nperformance in low light is still not completely satisfactory because of the\nfundamental limits in photon shot noise and sensor read noise. Generative image\nrestoration methods have demonstrated promising results compared to traditional\nmethods, but they suffer from hallucinatory content generation when the\nsignal-to-noise ratio (SNR) is low. Recognizing the availability of\npersonalized photo galleries on users' smartphones, we propose Personalized\nGenerative Denoising (PGD) by building a diffusion model customized for\ndifferent users. Our core innovation is an identity-consistent physical buffer\nthat extracts the physical attributes of the person from the gallery. This\nID-consistent physical buffer provides a strong prior that can be integrated\nwith the diffusion model to restore the degraded images, without the need of\nfine-tuning. Over a wide range of low-light testing scenarios, we show that PGD\nachieves superior image denoising and enhancement performance compared to\nexisting diffusion-based denoising approaches.\n","versions":"[{'version': 'v1', 'created': 'Wed, 18 Dec 2024 20:43:38 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 15:25:25 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Wang', 'Xijun', ''], ['Chennuri', 'Prateek', ''], ['Yuan', 'Yu', ''], ['Ma', 'Bole', ''], ['Zhang', 'Xingguang', ''], ['Chan', 'Stanley', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2412.1678,"submitter":"Changchang Sun","authors":"Changchang Sun and Ren Wang and Yihua Zhang and Jinghan Jia and\n  Jiancheng Liu and Gaowen Liu and Sijia Liu and Yan Yan","title":"Forget Vectors at Play: Universal Input Perturbations Driving Machine\n  Unlearning in Image Classification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Machine unlearning (MU), which seeks to erase the influence of specific\nunwanted data from already-trained models, is becoming increasingly vital in\nmodel editing, particularly to comply with evolving data regulations like the\n``right to be forgotten''. Conventional approaches are predominantly\nmodel-based, typically requiring retraining or fine-tuning the model's weights\nto meet unlearning requirements. In this work, we approach the MU problem from\na novel input perturbation-based perspective, where the model weights remain\nintact throughout the unlearning process. We demonstrate the existence of a\nproactive input-based unlearning strategy, referred to forget vector, which can\nbe generated as an input-agnostic data perturbation and remains as effective as\nmodel-based approximate unlearning approaches. We also explore forget vector\narithmetic, whereby multiple class-specific forget vectors are combined through\nsimple operations (e.g., linear combinations) to generate new forget vectors\nfor unseen unlearning tasks, such as forgetting arbitrary subsets across\nclasses. Extensive experiments validate the effectiveness and adaptability of\nthe forget vector, showcasing its competitive performance relative to\nstate-of-the-art model-based methods. Codes are available at\nhttps:\/\/github.com\/Changchangsun\/Forget-Vector.\n","versions":"[{'version': 'v1', 'created': 'Sat, 21 Dec 2024 21:27:22 GMT'}, {'version': 'v2', 'created': 'Wed, 8 Jan 2025 17:00:18 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 01:25:27 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Sun', 'Changchang', ''], ['Wang', 'Ren', ''], ['Zhang', 'Yihua', ''], ['Jia', 'Jinghan', ''], ['Liu', 'Jiancheng', ''], ['Liu', 'Gaowen', ''], ['Liu', 'Sijia', ''], ['Yan', 'Yan', '']]","extracted_entities":"[{'text': 'Machine unlearning', 'label': 'Zero-shot Learning'}, {'text': 'evolving data regulations', 'label': 'AI Ethics'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2412.17741,"submitter":"Rui Qian","authors":"Rui Qian, Xin Yin, Dejing Dou","title":"Reasoning to Attend: Try to Understand How <SEG> Token Works","comments":"This work has been accepted to CVPR 2025, please refer to\n  https:\/\/github.com\/rui-qian\/READ","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Current Large Multimodal Models (LMMs) empowered visual grounding typically\nrely on $\\texttt{<SEG>}$ tokens as a text prompt to jointly optimize the\nvision-language model (e.g., LLaVA) and the downstream task-specific model\n(e.g., SAM). However, we observe that little research has looked into how it\nworks.In this work, we first visualize the similarity maps, which are obtained\nby computing the semantic similarity between the $\\texttt{<SEG>}$ token and the\nimage token embeddings derived from the last hidden layer in both the LLaVA\nencoder and SAM decoder. Intriguingly, we have found that a striking\nconsistency holds in terms of activation responses in the similarity map, which\nreveals that what the $\\texttt{<SEG>}$ token contributes to is semantic\nsimilarity within image-text pairs. Specifically, the $\\texttt{<SEG>}$ token, a\nplaceholder expanded in text vocabulary, extensively queries among individual\ntokenized image patches to match the semantics of an object from text to the\npaired image, while the Large Language Models (LLMs) are being fine-tuned. Upon\nthe above findings, we present READ, which facilitates LMMs' resilient\n$\\textbf{REA}$soning capability of where to atten$\\textbf{D}$ under the\nguidance of highly activated points borrowed from similarity maps. Remarkably,\nREAD features an intuitive design, Similarity as Points module (SasP), which\ncan be seamlessly applied to $\\texttt{<SEG>}$-like paradigms in a plug-and-play\nfashion. Also, extensive experiments have been conducted on ReasonSeg and\nRefCOCO(+\/g) datasets. To validate whether READ suffers from catastrophic\nforgetting of previous skills after fine-tuning, we further assess its\ngeneration ability on an augmented FP-RefCOCO(+\/g) dataset. All codes and\nmodels are publicly available at https:\/\/github.com\/rui-qian\/READ.\n","versions":"[{'version': 'v1', 'created': 'Mon, 23 Dec 2024 17:44:05 GMT'}, {'version': 'v2', 'created': 'Wed, 25 Dec 2024 10:19:44 GMT'}, {'version': 'v3', 'created': 'Mon, 20 Jan 2025 07:57:50 GMT'}, {'version': 'v4', 'created': 'Wed, 5 Mar 2025 15:55:51 GMT'}, {'version': 'v5', 'created': 'Thu, 6 Mar 2025 04:11:30 GMT'}, {'version': 'v6', 'created': 'Thu, 13 Mar 2025 14:04:12 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Qian', 'Rui', ''], ['Yin', 'Xin', ''], ['Dou', 'Dejing', '']]","extracted_entities":"[{'text': 'text prompt', 'label': 'Prompting'}, {'text': 'SAM', 'label': 'Large Language Model'}, {'text': 'image token embeddings', 'label': 'Embedding'}, {'text': 'SAM', 'label': 'Large Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2412.19098,"submitter":"Aecheon Jung","authors":"Aecheon Jung, Seunghwan Lee, Dongyoon Han, Sungeun Hong","title":"Why Train Everything? Tint a Single Layer for Multi-task Model Merging","comments":"Additional experimental results","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Model merging integrates independently fine-tuned models into a single\nmulti-task model, offering a flexible alternative to joint training. However,\nmany existing model merging methods introduce additional task-specific\ncomponents, increasing complexity and requiring extra modifications. We propose\nModel Tinting, a lightweight yet highly effective approach that improves model\nmerging by updating just a single layer, accounting for as low as 0.5% of total\nparameters. Our key observation is that explicit task-specific modules are not\nnecessary; instead, subtle adjustments to a single layer can effectively\ncapture task-specific variations within the merged model while maintaining\ngeneralization. We introduce a confidence-based filtering mechanism to\nalleviate the impact of unreliable predictions from individual models on the\nmerged model. Extensive experiments across vision and NLP tasks demonstrate\nthat Model Tinting achieves state-of-the-art performance, even in challenging\ndense prediction tasks. Our code is available at\nhttps:\/\/github.com\/AIM-SKKU\/ModelTinting\n","versions":"[{'version': 'v1', 'created': 'Thu, 26 Dec 2024 07:42:06 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 04:21:56 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Jung', 'Aecheon', ''], ['Lee', 'Seunghwan', ''], ['Han', 'Dongyoon', ''], ['Hong', 'Sungeun', '']]","extracted_entities":"[{'text': 'Model merging', 'label': 'Embedding'}, {'text': 'Model Tinting', 'label': 'Embedding'}, {'text': 'model\\nmerging', 'label': 'Embedding'}, {'text': 'subtle adjustments', 'label': 'Fine-tuning'}, {'text': 'confidence-based filtering mechanism', 'label': 'Embedding'}, {'text': 'Model Tinting', 'label': 'Embedding'}]","assigned_concept":"Fine-tuning","matched_keyword":"subtle adjustments","similarity_score":0.5197140574}
{"id":2501.12106,"submitter":"Stefan Lenz","authors":"Stefan Lenz, Arsenij Ustjanzew, Marco Jeray, Torsten Panholzer","title":"Can open source large language models be used for tumor documentation in\n  Germany? -- An evaluation on urological doctors' notes","comments":"48 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https:\/\/github.com\/stefan-m-lenz\/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP.\n","versions":"[{'version': 'v1', 'created': 'Tue, 21 Jan 2025 12:56:47 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 08:48:46 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Lenz', 'Stefan', ''], ['Ustjanzew', 'Arsenij', ''], ['Jeray', 'Marco', ''], ['Panholzer', 'Torsten', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'few-shot prompting', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Llama 3.1 8B', 'label': 'Mistral'}, {'text': 'Mistral 7B', 'label': 'Mistral'}, {'text': 'Mistral NeMo 12 B', 'label': 'Mistral'}, {'text': 'few-shot prompting', 'label': 'Few-shot Learning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'tailored fine-tuning', 'label': 'Fine-tuning'}, {'text': 'well-designed prompting', 'label': 'Prompting'}]","assigned_concept":"Fine-tuning","matched_keyword":"tailored fine-tuning","similarity_score":0.8861455917}
{"id":2501.12969,"submitter":"Johanna Menn","authors":"Johanna Menn, Pietro Pelizzari, Michael Fleps-Dezasse, Sebastian\n  Trimpe","title":"Lipschitz Safe Bayesian Optimization for Automotive Control","comments":"Accepted for publication at 63rd Conference on Decision and Control,\n  December 16-19, 2024 in Milano, Italy","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Controller tuning is a labor-intensive process that requires human\nintervention and expert knowledge. Bayesian optimization has been applied\nsuccessfully in different fields to automate this process. However, when tuning\non hardware, such as in automotive applications, strict safety requirements\noften arise. To obtain safety guarantees, many existing safe Bayesian\noptimization methods rely on assumptions that are hard to verify in practice.\nThis leads to the use of unjustified heuristics in many applications, which\ninvalidates the theoretical safety guarantees. Furthermore, applications often\nrequire multiple safety constraints to be satisfied simultaneously. Building on\nrecently proposed Lipschitz-only safe Bayesian optimization, we develop an\nalgorithm that relies on readily interpretable assumptions and satisfies\nmultiple safety constraints at the same time. We apply this algorithm to the\nproblem of automatically tuning a trajectory-tracking controller of a\nself-driving car. Results both from simulations and an actual test vehicle\nunderline the algorithm's ability to learn tracking controllers without leaving\nthe track or violating any other safety constraints.\n","versions":"[{'version': 'v1', 'created': 'Wed, 22 Jan 2025 15:51:40 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 08:31:10 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Menn', 'Johanna', ''], ['Pelizzari', 'Pietro', ''], ['Fleps-Dezasse', 'Michael', ''], ['Trimpe', 'Sebastian', '']]","extracted_entities":"[{'text': 'Controller tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"Controller tuning","similarity_score":0.6682211161}
{"id":2501.15187,"submitter":"Zecheng Li","authors":"Zecheng Li, Wengang Zhou, Weichao Zhao, Kepeng Wu, Hezhen Hu, Houqiang\n  Li","title":"Uni-Sign: Toward Unified Sign Language Understanding at Scale","comments":"Accepted by ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Sign language pre-training has gained increasing attention for its ability to\nenhance performance across various sign language understanding (SLU) tasks.\nHowever, existing methods often suffer from a gap between pre-training and\nfine-tuning, leading to suboptimal results. To address this, we propose\nUni-Sign, a unified pre-training framework that eliminates the gap between\npre-training and downstream SLU tasks through a large-scale generative\npre-training strategy and a novel fine-tuning paradigm. First, we introduce\nCSL-News, a large-scale Chinese Sign Language (CSL) dataset containing 1,985\nhours of video paired with textual annotations, which enables effective\nlarge-scale pre-training. Second, Uni-Sign unifies SLU tasks by treating\ndownstream tasks as a single sign language translation (SLT) task during\nfine-tuning, ensuring seamless knowledge transfer between pre-training and\nfine-tuning. Furthermore, we incorporate a prior-guided fusion (PGF) module and\na score-aware sampling strategy to efficiently fuse pose and RGB information,\naddressing keypoint inaccuracies and improving computational efficiency.\nExtensive experiments across multiple SLU benchmarks demonstrate that Uni-Sign\nachieves state-of-the-art performance across multiple downstream SLU tasks.\nDataset and code are available at github.com\/ZechengLi19\/Uni-Sign.\n","versions":"[{'version': 'v1', 'created': 'Sat, 25 Jan 2025 11:51:23 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Jan 2025 09:44:28 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 12:51:29 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Zecheng', ''], ['Zhou', 'Wengang', ''], ['Zhao', 'Weichao', ''], ['Wu', 'Kepeng', ''], ['Hu', 'Hezhen', ''], ['Li', 'Houqiang', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2502.00575,"submitter":"Hashim A. Hashim","authors":"Khashayar Ghanizadegan and Hashim A. Hashim","title":"DeepUKF-VIN: Adaptively-tuned Deep Unscented Kalman Filter for 3D\n  Visual-Inertial Navigation based on IMU-Vision-Net","comments":null,"journal-ref":null,"doi":"10.1016\/j.eswa.2025.126656","report-no":null,"categories":"cs.RO cs.SY eess.SY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper addresses the challenge of estimating the orientation, position,\nand velocity of a vehicle operating in three-dimensional (3D) space with six\ndegrees of freedom (6-DoF). A Deep Learning-based Adaptation Mechanism (DLAM)\nis proposed to adaptively tune the noise covariance matrices of Kalman-type\nfilters for the Visual-Inertial Navigation (VIN) problem, leveraging\nIMU-Vision-Net. Subsequently, an adaptively tuned Deep Learning Unscented\nKalman Filter for 3D VIN (DeepUKF-VIN) is introduced to utilize the proposed\nDLAM, thereby robustly estimating key navigation components, including\norientation, position, and linear velocity. The proposed DeepUKF-VIN integrates\ndata from onboard sensors, specifically an inertial measurement unit (IMU) and\nvisual feature points extracted from a camera, and is applicable for GPS-denied\nnavigation. Its quaternion-based design effectively captures navigation\nnonlinearities and avoids the singularities commonly encountered with\nEuler-angle-based filters. Implemented in discrete space, the DeepUKF-VIN\nfacilitates practical filter deployment. The filter's performance is evaluated\nusing real-world data collected from an IMU and a stereo camera at low sampling\nrates. The results demonstrate filter stability and rapid attenuation of\nestimation errors, highlighting its high estimation accuracy. Furthermore,\ncomparative testing against the standard Unscented Kalman Filter (UKF) in two\nscenarios consistently shows superior performance across all navigation\ncomponents, thereby validating the efficacy and robustness of the proposed\nDeepUKF-VIN. Keywords: Deep Learning, Unscented Kalman Filter, Adaptive tuning,\nEstimation, Navigation, Unmanned Aerial Vehicle, Sensor-fusion.\n","versions":"[{'version': 'v1', 'created': 'Sat, 1 Feb 2025 21:59:40 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 17:21:45 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Ghanizadegan', 'Khashayar', ''], ['Hashim', 'Hashim A.', '']]","extracted_entities":"[{'text': 'Adaptive tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"Adaptive tuning","similarity_score":0.724874258}
{"id":2502.00987,"submitter":"Paul Albert Dr.","authors":"Paul Albert, Frederic Z. Zhang, Hemanth Saratchandran, Cristian\n  Rodriguez-Opazo, Anton van den Hengel, Ehsan Abbasnejad","title":"RandLoRA: Full-rank parameter-efficient fine-tuning of large models","comments":"To appear at the International Conference on Learning Representations\n  (ICLR) 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Low-Rank Adaptation (LoRA) and its variants have shown impressive results in\nreducing the number of trainable parameters and memory requirements of large\ntransformer networks while maintaining fine-tuning performance. The low-rank\nnature of the weight update inherently limits the representation power of\nfine-tuned models, however, thus potentially compromising performance on\ncomplex tasks. This raises a critical question: when a performance gap between\nLoRA and standard fine-tuning is observed, is it due to the reduced number of\ntrainable parameters or the rank deficiency? This paper aims to answer this\nquestion by introducing RandLoRA, a parameter-efficient method that performs\nfull-rank updates using a learned linear combinations of low-rank,\nnon-trainable random matrices. Our method limits the number of trainable\nparameters by restricting optimization to diagonal scaling matrices applied to\nthe fixed random matrices. This allows us to effectively overcome the low-rank\nlimitations while maintaining parameter and memory efficiency during training.\nThrough extensive experimentation across vision, language, and vision-language\nbenchmarks, we systematically evaluate the limitations of LoRA and existing\nrandom basis methods. Our findings reveal that full-rank updates are beneficial\nacross vision and language tasks individually, and even more so for\nvision-language tasks, where RandLoRA significantly reduces -- and sometimes\neliminates -- the performance gap between standard fine-tuning and LoRA,\ndemonstrating its efficacy.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Feb 2025 01:59:45 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 00:43:45 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Albert', 'Paul', ''], ['Zhang', 'Frederic Z.', ''], ['Saratchandran', 'Hemanth', ''], ['Rodriguez-Opazo', 'Cristian', ''], ['Hengel', 'Anton van den', ''], ['Abbasnejad', 'Ehsan', '']]","extracted_entities":"[{'text': 'standard fine-tuning', 'label': 'Fine-tuning'}, {'text': 'diagonal scaling matrices', 'label': 'Scaling law'}, {'text': 'standard fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"standard fine-tuning","similarity_score":0.8863296509}
{"id":2502.06987,"submitter":"Bo Wen","authors":"Bo Wen and Anna Heinke and Akshay Agnihotri and Dirk-Uwe Bartsch and\n  William Freeman and Truong Nguyen and Cheolhong An","title":"Universal Vessel Segmentation for Multi-Modality Retinal Images","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We identify two major limitations in the existing studies on retinal vessel\nsegmentation: (1) Most existing works are restricted to one modality, i.e, the\nColor Fundus (CF). However, multi-modality retinal images are used every day in\nthe study of retina and retinal diseases, and the study of vessel segmentation\non the other modalities is scarce; (2) Even though a small amount of works\nextended their experiments to limited new modalities such as the Multi-Color\nScanning Laser Ophthalmoscopy (MC), these works still require finetuning a\nseparate model for the new modality. The finetuning will require extra training\ndata, which is difficult to acquire. In this work, we present a foundational\nuniversal vessel segmentation model (UVSM) for multi-modality retinal images.\nNot only do we perform the study on a much wider range of modalities, but we\nalso propose a universal model to segment the vessels in all these\ncommonly-used modalities. Despite being much more versatile comparing with\nexisting methods, our universal model still demonstrates comparable performance\nwith the state-of-the-art finetuned methods. To the best of our knowledge, this\nis the first work that achieves cross-modality retinal vessel segmentation and\nalso the first work to study retinal vessel segmentation in some novel\nmodalities.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Feb 2025 19:28:20 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 00:59:47 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Wen', 'Bo', ''], ['Heinke', 'Anna', ''], ['Agnihotri', 'Akshay', ''], ['Bartsch', 'Dirk-Uwe', ''], ['Freeman', 'William', ''], ['Nguyen', 'Truong', ''], ['An', 'Cheolhong', '']]","extracted_entities":"[{'text': 'finetuning', 'label': 'Fine-tuning'}, {'text': 'finetuning', 'label': 'Fine-tuning'}, {'text': 'universal vessel segmentation model', 'label': 'Foundation Model'}]","assigned_concept":"Fine-tuning","matched_keyword":"finetuning","similarity_score":0.5753726363}
{"id":2502.0746,"submitter":"Heyang Zhao","authors":"Heyang Zhao, Chenlu Ye, Wei Xiong, Quanquan Gu, Tong Zhang","title":"Logarithmic Regret for Online KL-Regularized Reinforcement Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ML","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advances in Reinforcement Learning from Human Feedback (RLHF) have\nshown that KL-regularization plays a pivotal role in improving the efficiency\nof RL fine-tuning for large language models (LLMs). Despite its empirical\nadvantage, the theoretical difference between KL-regularized RL and standard RL\nremains largely under-explored. While there is a recent line of work on the\ntheoretical analysis of KL-regularized objective in decision making\n\\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses\neither reduce to the traditional RL setting or rely on strong coverage\nassumptions. In this paper, we propose an optimism-based KL-regularized online\ncontextual bandit algorithm, and provide a novel analysis of its regret. By\ncarefully leveraging the benign optimization landscape induced by the\nKL-regularization and the optimistic reward estimation, our algorithm achieves\nan $\\mathcal{O}\\big(\\eta\\log (N_{\\mathcal R} T)\\cdot d_{\\mathcal R}\\big)$\nlogarithmic regret bound, where $\\eta, N_{\\mathcal R},T,d_{\\mathcal R}$ denote\nthe KL-regularization parameter, the cardinality of the reward function class,\nnumber of rounds, and the complexity of the reward function class. Furthermore,\nwe extend our algorithm and analysis to reinforcement learning by developing a\nnovel decomposition over transition steps and also obtain a similar logarithmic\nregret bound.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Feb 2025 11:11:05 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Feb 2025 13:55:04 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 07:24:46 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Zhao', 'Heyang', ''], ['Ye', 'Chenlu', ''], ['Xiong', 'Wei', ''], ['Gu', 'Quanquan', ''], ['Zhang', 'Tong', '']]","extracted_entities":"[{'text': 'RL fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"RL fine-tuning","similarity_score":0.7450102568}
{"id":2502.11492,"submitter":"Kung-Hsiang Huang","authors":"Kung-Hsiang Huang, Can Qin, Haoyi Qiu, Philippe Laban, Shafiq Joty,\n  Caiming Xiong, Chien-Sheng Wu","title":"Why Vision Language Models Struggle with Visual Arithmetic? Towards\n  Enhanced Chart and Geometry Understanding","comments":"Code and data are available at\n  https:\/\/github.com\/SalesforceAIResearch\/CogAlign","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Vision Language Models (VLMs) have achieved remarkable progress in multimodal\ntasks, yet they often struggle with visual arithmetic, seemingly simple\ncapabilities like object counting or length comparison, which are essential for\nrelevant complex tasks like chart understanding and geometric reasoning. In\nthis work, we first investigate the root causes of this deficiency through a\nsuite of probing tasks focusing on basic visual arithmetic. Our analysis\nreveals that while pre-trained vision encoders typically capture sufficient\ninformation, the text decoder often fails to decode it correctly for arithmetic\nreasoning. To address this, we propose CogAlign, a novel post-training strategy\ninspired by Piaget's theory of cognitive development. CogAlign trains VLMs to\nrecognize invariant properties under visual transformations. We demonstrate\nthat this approach significantly improves the performance of three diverse VLMs\non our proposed probing tasks. Furthermore, CogAlign enhances performance by an\naverage of 4.6% on CHOCOLATE and 2.9% on MATH-VISION, outperforming or matching\nsupervised fine-tuning methods while requiring only 60% less training data.\nThese results highlight the effectiveness and generalizability of CogAlign in\nimproving fundamental visual arithmetic capabilities and their transfer to\ndownstream tasks.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Feb 2025 06:54:49 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 02:13:57 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Huang', 'Kung-Hsiang', ''], ['Qin', 'Can', ''], ['Qiu', 'Haoyi', ''], ['Laban', 'Philippe', ''], ['Joty', 'Shafiq', ''], ['Xiong', 'Caiming', ''], ['Wu', 'Chien-Sheng', '']]","extracted_entities":"[{'text': 'supervised fine-tuning methods', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"supervised fine-tuning methods","similarity_score":0.6933436394}
{"id":2502.12292,"submitter":"Sally Zhu","authors":"Sally Zhu, Ahmed Ahmed, Rohith Kuditipudi, Percy Liang","title":"Independence Tests for Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We consider the following problem: given the weights of two models, can we\ntest whether they were trained independently -- i.e., from independent random\ninitializations? We consider two settings: constrained and unconstrained. In\nthe constrained setting, we make assumptions about model architecture and\ntraining and propose a family of statistical tests that yield exact p-values\nwith respect to the null hypothesis that the models are trained from\nindependent random initializations. These p-values are valid regardless of the\ncomposition of either model's training data; we compute them by simulating\nexchangeable copies of each model under our assumptions and comparing various\nsimilarity measures of weights and activations between the original two models\nversus these copies. We report the p-values from these tests on pairs of 21\nopen-weight models (210 total pairs) and correctly identify all pairs of\nnon-independent models. Our tests remain effective even if one model was\nfine-tuned for many tokens. In the unconstrained setting, where we make no\nassumptions about training procedures, can change model architecture, and allow\nfor adversarial evasion attacks, the previous tests no longer work. Instead, we\npropose a new test which matches hidden activations between two models, and\nwhich is robust to adversarial transformations and to changes in model\narchitecture. The test can also do localized testing: identifying specific\nnon-independent components of models. Though we no longer obtain exact p-values\nfrom this, empirically we find it behaves as one and reliably identifies\nnon-independent models. Notably, we can use the test to identify specific parts\nof one model that are derived from another (e.g., how Llama 3.1-8B was pruned\nto initialize Llama 3.2-3B, or shared layers between Mistral-7B and\nStripedHyena-7B), and it is even robust to retraining individual layers of\neither model from scratch.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Feb 2025 20:01:08 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 15:58:01 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Zhu', 'Sally', ''], ['Ahmed', 'Ahmed', ''], ['Kuditipudi', 'Rohith', ''], ['Liang', 'Percy', '']]","extracted_entities":"[{'text': 'fine-tuned', 'label': 'Fine-tuning'}, {'text': 'Llama 3.1-8B', 'label': 'Llama'}, {'text': 'Llama 3.2-3B', 'label': 'Llama'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuned","similarity_score":0.8707774282}
{"id":2502.17664,"submitter":"Tsan Tsai Chan","authors":"Tsan Tsai Chan, Xin Tong, Thi Thu Uyen Hoang, Barbare Tepnadze,\n  Wojciech Stempniak","title":"Towards Typologically Aware Rescoring to Mitigate Unfaithfulness in\n  Lower-Resource Languages","comments":"ISCA\/ITG Workshop on Diversity in Large Speech and Language Models","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multilingual large language models (LLMs) are known to more frequently\ngenerate non-faithful output in resource-constrained languages (Guerreiro et\nal., 2023 - arXiv:2303.16104), potentially because these typologically diverse\nlanguages are underrepresented in their training data. To mitigate\nunfaithfulness in such settings, we propose using computationally light\nauxiliary models to rescore the outputs of larger architectures. As proof of\nthe feasibility of such an approach, we show that monolingual 4-layer BERT\nmodels pretrained from scratch on less than 700 MB of data without fine-tuning\nare able to identify faithful summaries with a mean accuracy of 88.33% in three\ngenetically unrelated languages that differ in their morphological complexity -\nVietnamese, Polish and Georgian. The same hyperparameter combination moreover\ngeneralises well to three other tasks, suggesting applications for rescoring\nbeyond improving faithfulness. In order to inform typologically aware model\nselection, we also investigate how morphological complexity interacts with\nregularisation, model depth and training objectives, ultimately demonstrating\nthat morphologically complex languages are more likely to benefit from dropout,\nwhile across languages downstream performance is enhanced most by shallow\narchitectures as well as training using the standard BERT objectives.\n","versions":"[{'version': 'v1', 'created': 'Mon, 24 Feb 2025 21:22:19 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 08:17:58 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Chan', 'Tsan Tsai', ''], ['Tong', 'Xin', ''], ['Hoang', 'Thi Thu Uyen', ''], ['Tepnadze', 'Barbare', ''], ['Stempniak', 'Wojciech', '']]","extracted_entities":"[{'text': 'Multilingual large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'BERT', 'label': 'BERT'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'regularisation', 'label': 'Few-shot Learning'}, {'text': 'BERT', 'label': 'BERT'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2502.18008,"submitter":"Yashan Wang","authors":"Yashan Wang, Shangda Wu, Jianhuai Hu, Xingjian Du, Yueqi Peng, Yongxin\n  Huang, Shuai Fan, Xiaobing Li, Feng Yu, Maosong Sun","title":"NotaGen: Advancing Musicality in Symbolic Music Generation with Large\n  Language Model Training Paradigms","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.AI eess.AS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce NotaGen, a symbolic music generation model aiming to explore the\npotential of producing high-quality classical sheet music. Inspired by the\nsuccess of Large Language Models (LLMs), NotaGen adopts pre-training,\nfine-tuning, and reinforcement learning paradigms (henceforth referred to as\nthe LLM training paradigms). It is pre-trained on 1.6M pieces of music in ABC\nnotation, and then fine-tuned on approximately 9K high-quality classical\ncompositions conditioned on \"period-composer-instrumentation\" prompts. For\nreinforcement learning, we propose the CLaMP-DPO method, which further enhances\ngeneration quality and controllability without requiring human annotations or\npredefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in\nsymbolic music generation models with different architectures and encoding\nschemes. Furthermore, subjective A\/B tests show that NotaGen outperforms\nbaseline models against human compositions, greatly advancing musical\naesthetics in symbolic music generation.\n","versions":"[{'version': 'v1', 'created': 'Tue, 25 Feb 2025 09:12:07 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 08:18:41 GMT'}, {'version': 'v3', 'created': 'Thu, 27 Feb 2025 07:02:39 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 13:50:00 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Yashan', ''], ['Wu', 'Shangda', ''], ['Hu', 'Jianhuai', ''], ['Du', 'Xingjian', ''], ['Peng', 'Yueqi', ''], ['Huang', 'Yongxin', ''], ['Fan', 'Shuai', ''], ['Li', 'Xiaobing', ''], ['Yu', 'Feng', ''], ['Sun', 'Maosong', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'reinforcement learning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuned', 'label': 'Fine-tuning'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'reinforcement learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.01905,"submitter":"Sunghyeon Woo","authors":"Sunghyeon Woo, Sol Namkung, Sunwoo Lee, Inho Jeong, Beomseok Kim,\n  Dongsuk Jeon","title":"PaCA: Partial Connection Adaptation for Efficient Fine-Tuning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Prior parameter-efficient fine-tuning (PEFT) algorithms reduce memory usage\nand computational costs of fine-tuning large neural network models by training\nonly a few additional adapter parameters, rather than the entire model.\nHowever, the reduction in computational costs due to PEFT does not necessarily\ntranslate to a reduction in training time; although the computational costs of\nthe adapter layers are much smaller than the pretrained layers, it is well\nknown that those two types of layers are processed sequentially on GPUs,\nresulting in significant latency overhead. LoRA and its variants merge low-rank\nadapter matrices with pretrained weights during inference to avoid latency\noverhead, but during training, the pretrained weights remain frozen while the\nadapter matrices are continuously updated, preventing such merging. To mitigate\nthis issue, we propose Partial Connection Adaptation (PaCA), which fine-tunes\nrandomly selected partial connections within the pretrained weights instead of\nintroducing adapter layers in the model. PaCA not only enhances training speed\nby eliminating the time overhead due to the sequential processing of the\nadapter and pretrained layers but also reduces activation memory since only\npartial activations, rather than full activations, need to be stored for\ngradient computation. Compared to LoRA, PaCA reduces training time by 22% and\ntotal memory usage by 16%, while maintaining comparable accuracy across various\nfine-tuning scenarios, such as fine-tuning on the MMLU dataset and instruction\ntuning on the Oasst1 dataset. PaCA can also be combined with quantization,\nenabling the fine-tuning of large models such as LLaMA3.1-70B. In addition,\nPaCA enables training with 23% longer sequence and improves throughput by 16%\non both NVIDIA A100 GPU and INTEL Gaudi2 HPU compared to LoRA. The code is\navailable at https:\/\/github.com\/WooSunghyeon\/paca.\n","versions":"[{'version': 'v1', 'created': 'Fri, 28 Feb 2025 13:30:10 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 15:24:13 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Woo', 'Sunghyeon', ''], ['Namkung', 'Sol', ''], ['Lee', 'Sunwoo', ''], ['Jeong', 'Inho', ''], ['Kim', 'Beomseok', ''], ['Jeon', 'Dongsuk', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'quantization', 'label': 'quantisation'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.028,"submitter":"Alicia Russell-Gilbert","authors":"Alicia Russell-Gilbert, Sudip Mittal, Shahram Rahimi, Maria Seale,\n  Joseph Jabour, Thomas Arnold, Joshua Church","title":"RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration","comments":"arXiv admin note: substantial text overlap with arXiv:2411.00914","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CE","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Anomaly detection in complex industrial environments poses unique challenges,\nparticularly in contexts characterized by data sparsity and evolving\noperational conditions. Predictive maintenance (PdM) in such settings demands\nmethodologies that are adaptive, transferable, and capable of integrating\ndomain-specific knowledge. In this paper, we present RAAD-LLM, a novel\nframework for adaptive anomaly detection, leveraging large language models\n(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach\naddresses the aforementioned PdM challenges. By effectively utilizing\ndomain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time\nseries data without requiring fine-tuning on specific datasets. The framework's\nadaptability mechanism enables it to adjust its understanding of normal\noperating conditions dynamically, thus increasing detection accuracy. We\nvalidate this methodology through a real-world application for a plastics\nmanufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show\nsignificant improvements over our previous model with an accuracy increase from\n70.7% to 88.6% on the real-world dataset. By allowing for the enriching of\ninput series data with semantics, RAAD-LLM incorporates multimodal capabilities\nthat facilitate more collaborative decision-making between the model and plant\noperators. Overall, our findings support RAAD-LLM's ability to revolutionize\nanomaly detection methodologies in PdM, potentially leading to a paradigm shift\nin how anomaly detection is implemented across various industries.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 17:20:43 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Mar 2025 18:30:45 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 15:47:37 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Russell-Gilbert', 'Alicia', ''], ['Mittal', 'Sudip', ''], ['Rahimi', 'Shahram', ''], ['Seale', 'Maria', ''], ['Jabour', 'Joseph', ''], ['Arnold', 'Thomas', ''], ['Church', 'Joshua', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'Retrieval-Augmented Generation (RAG)', 'label': 'RAG'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.02854,"submitter":"Belinda Z. Li","authors":"Belinda Z. Li, Zifan Carl Guo, Jacob Andreas","title":"(How) Do Language Models Track State?","comments":"21 pages, 17 figures, 1 table. Code:\n  http:\/\/github.com\/belindal\/state-tracking","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Transformer language models (LMs) exhibit behaviors -- from storytelling to\ncode generation -- that appear to require tracking the unobserved state of an\nevolving world. How do they do so? We study state tracking in LMs trained or\nfine-tuned to compose permutations (i.e., to compute the order of a set of\nobjects after a sequence of swaps). Despite the simple algebraic structure of\nthis problem, many other tasks (e.g., simulation of finite automata and\nevaluation of boolean expressions) can be reduced to permutation composition,\nmaking it a natural model for state tracking in general. We show that LMs\nconsistently learn one of two state tracking mechanisms for this task. The\nfirst closely resembles the \"associative scan\" construction used in recent\ntheoretical work by Liu et al. (2023) and Merrill et al. (2024). The second\nuses an easy-to-compute feature (permutation parity) to partially prune the\nspace of outputs, then refines this with an associative scan. The two\nmechanisms exhibit markedly different robustness properties, and we show how to\nsteer LMs toward one or the other with intermediate training tasks that\nencourage or suppress the heuristics. Our results demonstrate that transformer\nLMs, whether pretrained or fine-tuned, can learn to implement efficient and\ninterpretable state tracking mechanisms, and the emergence of these mechanisms\ncan be predicted and controlled.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 18:31:02 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 15:36:40 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Li', 'Belinda Z.', ''], ['Guo', 'Zifan Carl', ''], ['Andreas', 'Jacob', '']]","extracted_entities":"[{'text': 'fine-tuned', 'label': 'Fine-tuning'}, {'text': 'fine-tuned', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuned","similarity_score":0.8707774282}
{"id":2503.03059,"submitter":"Tomoi Koide","authors":"T. Koide and F. Nicacio","title":"Unification of Stochastic and Quantum Thermodynamics in Scalar Field\n  Theory via a Model with Brownian Thermostat","comments":"30 pages, no figure, new references were added","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.stat-mech hep-th","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We present a systematic procedure to derive a quantum master equation for\nthermal relaxation in real scalar field theory, expanding on the method\nproposed in [Koide and Nicacio, Phys. Lett. A494, 129277 (2024)]. We begin by\nintroducing a generalized model for a classical scalar field interacting with a\nBrownian thermostat, consistent with stochastic thermodynamics. Applying\ncanonical quantization to this model, we derive the corresponding quantum\nmaster equation, that is applicable to any form of the scalar field\nHamiltonian. While its evolution is generally non-CPTP (Completely Positive and\nTrace-Preserving), it can be adjusted to describe a CPTP evolution, such as\nthose found in the GKSL (Gorini-Kossakowski-Sudarshan-Lindblad) equation by\nappropriately tuning the parameters of the model. In this framework, we define\nheat, work, and entropy in a way that satisfies the first and second laws of\nquantum thermodynamics. This suggests that the quantum-classical correspondence\nextends beyond closed systems governed by unitary time evolution to open\nsystems as well. We further investigate the relation between the second law in\nquantum thermodynamics and relative entropy, providing insights into the study\nof quantum fluctuations through information-theoretical techniques in quantum\nfield theory.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 23:48:29 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 14:16:50 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Koide', 'T.', ''], ['Nicacio', 'F.', '']]","extracted_entities":"[{'text': 'canonical quantization', 'label': 'quantisation'}, {'text': 'appropriately tuning', 'label': 'Fine-tuning'}, {'text': 'second law', 'label': 'Scaling law'}]","assigned_concept":"Fine-tuning","matched_keyword":"appropriately tuning","similarity_score":0.8882499933}
{"id":2503.03135,"submitter":"Runze Wang","authors":"Runze Wang, Mingqi Yang, Yanming Shen","title":"Bridging Molecular Graphs and Large Language Models","comments":"AAAI 2025 camera ready version","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  While Large Language Models (LLMs) have shown exceptional generalization\ncapabilities, their ability to process graph data, such as molecular\nstructures, remains limited. To bridge this gap, this paper proposes\nGraph2Token, an efficient solution that aligns graph tokens to LLM tokens. The\nkey idea is to represent a graph token with the LLM token vocabulary, without\nfine-tuning the LLM backbone. To achieve this goal, we first construct a\nmolecule-text paired dataset from multisources, including CHEBI and HMDB, to\ntrain a graph structure encoder, which reduces the distance between graphs and\ntexts representations in the feature space. Then, we propose a novel alignment\nstrategy that associates a graph token with LLM tokens. To further unleash the\npotential of LLMs, we collect molecular IUPAC name identifiers, which are\nincorporated into the LLM prompts. By aligning molecular graphs as special\ntokens, we can activate LLM generalization ability to molecular few-shot\nlearning. Extensive experiments on molecular classification and regression\ntasks demonstrate the effectiveness of our proposed Graph2Token.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 03:15:38 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 09:51:05 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Wang', 'Runze', ''], ['Yang', 'Mingqi', ''], ['Shen', 'Yanming', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'multisources', 'label': 'Open-source LLMs'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'molecular few-shot\\nlearning', 'label': 'Few-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.04036,"submitter":"Xinyue Cui","authors":"Xinyue Cui, Johnny Tian-Zheng Wei, Swabha Swayamdipta, Robin Jia","title":"Robust Data Watermarking in Language Models by Injecting Fictitious\n  Knowledge","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Data watermarking in language models injects traceable signals, such as\nspecific token sequences or stylistic patterns, into copyrighted text, allowing\ncopyright holders to track and verify training data ownership. Previous data\nwatermarking techniques primarily focus on effective memorization after\npretraining, while overlooking challenges that arise in other stages of the LLM\npipeline, such as the risk of watermark filtering during data preprocessing, or\npotential forgetting through post-training, or verification difficulties due to\nAPI-only access. We propose a novel data watermarking approach that injects\ncoherent and plausible yet fictitious knowledge into training data using\ngenerated passages describing a fictitious entity and its associated\nattributes. Our watermarks are designed to be memorized by the LLM through\nseamlessly integrating in its training data, making them harder to detect\nlexically during preprocessing. We demonstrate that our watermarks can be\neffectively memorized by LLMs, and that increasing our watermarks' density,\nlength, and diversity of attributes strengthens their memorization. We further\nshow that our watermarks remain robust throughout LLM development, maintaining\ntheir effectiveness after continual pretraining and supervised finetuning.\nFinally, we show that our data watermarks can be evaluated even under API-only\naccess via question answering.\n","versions":"[{'version': 'v1', 'created': 'Thu, 6 Mar 2025 02:40:51 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 06:10:02 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Cui', 'Xinyue', ''], ['Wei', 'Johnny Tian-Zheng', ''], ['Swayamdipta', 'Swabha', ''], ['Jia', 'Robin', '']]","extracted_entities":"[{'text': 'supervised finetuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"supervised finetuning","similarity_score":0.5287704468}
{"id":2503.04807,"submitter":"Hyeonseok Moon","authors":"Hyeonseok Moon, Jaehyung Seo, Heuiseok Lim","title":"Call for Rigor in Reporting Quality of Instruction Tuning Data","comments":"10 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Instruction tuning is crucial for adapting large language models (LLMs) to\nalign with user intentions. Numerous studies emphasize the significance of the\nquality of instruction tuning (IT) data, revealing a strong correlation between\nIT data quality and the alignment performance of LLMs. In these studies, the\nquality of IT data is typically assessed by evaluating the performance of LLMs\ntrained with that data. However, we identified a prevalent issue in such\npractice: hyperparameters for training models are often selected arbitrarily\nwithout adequate justification. We observed significant variations in\nhyperparameters applied across different studies, even when training the same\nmodel with the same data. In this study, we demonstrate the potential problems\narising from this practice and emphasize the need for careful consideration in\nverifying data quality. Through our experiments on the quality of LIMA data and\na selected set of 1,000 Alpaca data points, we demonstrate that arbitrary\nhyperparameter decisions can make any arbitrary conclusion.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 02:04:58 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 07:10:07 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Moon', 'Hyeonseok', ''], ['Seo', 'Jaehyung', ''], ['Lim', 'Heuiseok', '']]","extracted_entities":"[{'text': 'Instruction tuning', 'label': 'Fine-tuning'}, {'text': 'instruction tuning', 'label': 'Fine-tuning'}, {'text': 'Alpaca', 'label': 'Llama'}]","assigned_concept":"Fine-tuning","matched_keyword":"Instruction tuning","similarity_score":0.6964834929}
{"id":2503.05168,"submitter":"Yu Feng","authors":"Xiaotong Huang, He Zhu, Zihan Liu, Weikai Lin, Xiaohong Liu, Zhezhi\n  He, Jingwen Leng, Minyi Guo, Yu Feng","title":"SeeLe: A Unified Acceleration Framework for Real-Time Gaussian Splatting","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  3D Gaussian Splatting (3DGS) has become a crucial rendering technique for\nmany real-time applications. However, the limited hardware resources on today's\nmobile platforms hinder these applications, as they struggle to achieve\nreal-time performance. In this paper, we propose SeeLe, a general framework\ndesigned to accelerate the 3DGS pipeline for resource-constrained mobile\ndevices.\n  Specifically, we propose two GPU-oriented techniques: hybrid preprocessing\nand contribution-aware rasterization. Hybrid preprocessing alleviates the GPU\ncompute and memory pressure by reducing the number of irrelevant Gaussians\nduring rendering. The key is to combine our view-dependent scene representation\nwith online filtering. Meanwhile, contribution-aware rasterization improves the\nGPU utilization at the rasterization stage by prioritizing Gaussians with high\ncontributions while reducing computations for those with low contributions.\nBoth techniques can be seamlessly integrated into existing 3DGS pipelines with\nminimal fine-tuning. Collectively, our framework achieves 2.6$\\times$ speedup\nand 32.3\\% model reduction while achieving superior rendering quality compared\nto existing methods.\n","versions":"[{'version': 'v1', 'created': 'Fri, 7 Mar 2025 06:23:58 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 05:49:03 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Huang', 'Xiaotong', ''], ['Zhu', 'He', ''], ['Liu', 'Zihan', ''], ['Lin', 'Weikai', ''], ['Liu', 'Xiaohong', ''], ['He', 'Zhezhi', ''], ['Leng', 'Jingwen', ''], ['Guo', 'Minyi', ''], ['Feng', 'Yu', '']]","extracted_entities":"[{'text': 'minimal fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"minimal fine-tuning","similarity_score":0.8483207226}
{"id":2503.05403,"submitter":"Verena H\\\"aberle","authors":"Verena H\\\"aberle, Xiuqiang He, Linbin Huang, Florian D\\\"orfler, Steven\n  Low","title":"Quantitative Decentralized Stability Certificates for Grid-Forming\n  Converter Control","comments":"12 pages, 13 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We propose a decentralized framework for guaranteeing the small-signal\nstability of future power systems with grid-forming converters. Our approach\nleverages dynamic loop-shifting techniques to compensate for the lack of\npassivity in the network dynamics and establishes decentralized parametric\nstability certificates, depending on the local device-level controls and\nincorporating the effects of the network dynamics. By following practical\ntuning rules, we are able to ensure plug-and-play operation without centralized\ncoordination. Unlike prior works, our approach accommodates coupled frequency\nand voltage dynamics, incorporates network dynamics, and does not rely on\nspecific network configurations or operating points, offering a general and\nscalable solution for the integration of power-electronics-based devices into\nfuture power systems. We validate our theoretical stability results through\nnumerical case studies in a high-fidelity simulation model.\n","versions":"[{'version': 'v1', 'created': 'Fri, 7 Mar 2025 13:26:55 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 13:51:36 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['H\u00e4berle', 'Verena', ''], ['He', 'Xiuqiang', ''], ['Huang', 'Linbin', ''], ['D\u00f6rfler', 'Florian', ''], ['Low', 'Steven', '']]","extracted_entities":"[{'text': 'practical\\ntuning rules', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"practical\ntuning rules","similarity_score":0.691578269}
{"id":2503.06316,"submitter":"Tieqiao Wang","authors":"Tieqiao Wang, Sinisa Todorovic","title":"End-to-End Action Segmentation Transformer","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV eess.IV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Existing approaches to action segmentation use pre-computed frame features\nextracted by methods which have been trained on tasks that are different from\naction segmentation. Also, recent approaches typically use deep framewise\nrepresentations that lack explicit modeling of action segments. To address\nthese shortcomings, we introduce the first end-to-end solution to action\nsegmentation -- End-to-End Action Segmentation Transformer (EAST). Our key\ncontributions include: (1) a simple and efficient adapter design for effective\nbackbone fine-tuning; (2) a segmentation-by-detection framework for leveraging\naction proposals initially predicted over a coarsely downsampled video toward\nlabeling of all frames; and (3) a new action-proposal based data augmentation\nfor robust training. EAST achieves state-of-the-art performance on standard\nbenchmarks, including GTEA, 50Salads, Breakfast, and Assembly-101. The model\nand corresponding code will be released.\n","versions":"[{'version': 'v1', 'created': 'Sat, 8 Mar 2025 19:25:16 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 04:33:47 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Wang', 'Tieqiao', ''], ['Todorovic', 'Sinisa', '']]","extracted_entities":"[{'text': 'effective\\nbackbone fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"effective\nbackbone fine-tuning","similarity_score":0.6030130982}
{"id":2503.06399,"submitter":"Haisheng Fu","authors":"Haisheng Fu, Jie Liang, Zhenman Fang, Jingning Han","title":"FEDS: Feature and Entropy-Based Distillation Strategy for Efficient\n  Learned Image Compression","comments":"16 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV stat.ML","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Learned image compression (LIC) methods have recently outperformed\ntraditional codecs such as VVC in rate-distortion performance. However, their\nlarge models and high computational costs have limited their practical\nadoption. In this paper, we first construct a high-capacity teacher model by\nintegrating Swin-Transformer V2-based attention modules, additional residual\nblocks, and expanded latent channels, thus achieving enhanced compression\nperformance. Building on this foundation, we propose a \\underline{F}eature and\n\\underline{E}ntropy-based \\underline{D}istillation \\underline{S}trategy\n(\\textbf{FEDS}) that transfers key knowledge from the teacher to a lightweight\nstudent model. Specifically, we align intermediate feature representations and\nemphasize the most informative latent channels through an entropy-based loss. A\nstaged training scheme refines this transfer in three phases: feature\nalignment, channel-level distillation, and final fine-tuning. Our student model\nnearly matches the teacher across Kodak (1.24\\% BD-Rate increase), Tecnick\n(1.17\\%), and CLIC (0.55\\%) while cutting parameters by about 63\\% and\naccelerating encoding\/decoding by around 73\\%. Moreover, ablation studies\nindicate that FEDS generalizes effectively to transformer-based networks. The\nexperimental results demonstrate our approach strikes a compelling balance\namong compression performance, speed, and model parameters, making it\nwell-suited for real-time or resource-limited scenarios.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 02:39:39 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 20:13:20 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Fu', 'Haisheng', ''], ['Liang', 'Jie', ''], ['Fang', 'Zhenman', ''], ['Han', 'Jingning', '']]","extracted_entities":"[{'text': 'channel-level distillation', 'label': 'Knowledge distillation'}, {'text': 'final fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"final fine-tuning","similarity_score":0.904732883}
{"id":2503.06414,"submitter":"Shanya Baghel","authors":"Shanya Baghel and Shuvashree Mondal","title":"Exponential-polynomial divergence based inference for nondestructive\n  one-shot devices under progressive stress model","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.AP","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Nondestructive one-shot device (NOSD) testing plays a crucial role in\nengineering, particularly in the reliability assessment of high-stakes systems\nsuch as aerospace components, medical devices, and semiconductor technologies.\nAccurate reliability prognosis of NOSD testing data is essential for ensuring\nproduct durability, safety, and performance optimization. The conventional\nestimation methods like maximum likelihood estimation (MLE) are sensitive to\ndata contamination, leading to biased results. Consequently, this study\ndevelops robust inferential analysis for NOSD testing data under a progressive\nstress model. The lifetime of NOSD is assumed to follow Log-logistic\ndistribution. The estimation procedure addresses robustness by incorporating\nExponential-polynomial divergence (EPD). Equipped with three tuning parameters,\nEPD based estimation is proven to be more flexible than density power\ndivergence estimation frequently used for one-shot device testing data\nanalysis. Further, we explore the asymptotic behaviour of minimum EPD estimator\n(MEPDE) for large sample size. The robustness of MEPDE is analytically studied\nthrough influence function. Since tradeoff between efficiency and robustness of\nEPD based estimation is governed by three tuning parameters, a novel approach\nleveraging Concrete Score Matching (CSM) is introduced to optimize the tuning\nparameters of MEPDE. Moreover, a comparative study with the existing methods of\nfinding tuning parameters is conducted through extensive simulation experiment\nand data analysis. Another aspect of this study is determining an optimal plan\nto ensure a successful ALT experiment within specified budget and time\nconstraints. It is designed on A-optimality criteria subject to the given\nconstraints and is executed using the constraint particle swarm optimization\n(CPSO) algorithm.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 03:16:21 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Baghel', 'Shanya', ''], ['Mondal', 'Shuvashree', '']]","extracted_entities":"[{'text': 'tuning\\nparameters', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"tuning\nparameters","similarity_score":0.6904253364}
{"id":2503.06472,"submitter":"Yuxuan Luo","authors":"Yuxuan Luo, Jiaqi Tang, Chenyi Huang, Feiyang Hao, Zhouhui Lian","title":"CalliReader: Contextualizing Chinese Calligraphy via an\n  Embedding-Aligned Vision-Language Model","comments":"11 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.MM","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Chinese calligraphy, a UNESCO Heritage, remains computationally challenging\ndue to visual ambiguity and cultural complexity. Existing AI systems fail to\ncontextualize their intricate scripts, because of limited annotated data and\npoor visual-semantic alignment. We propose CalliReader, a vision-language model\n(VLM) that solves the Chinese Calligraphy Contextualization (CC$^2$) problem\nthrough three innovations: (1) character-wise slicing for precise character\nextraction and sorting, (2) CalliAlign for visual-text token compression and\nalignment, (3) embedding instruction tuning (e-IT) for improving alignment and\naddressing data scarcity. We also build CalliBench, the first benchmark for\nfull-page calligraphic contextualization, addressing three critical issues in\nprevious OCR and VQA approaches: fragmented context, shallow reasoning, and\nhallucination. Extensive experiments including user studies have been conducted\nto verify our CalliReader's \\textbf{superiority to other state-of-the-art\nmethods and even human professionals in page-level calligraphy recognition and\ninterpretation}, achieving higher accuracy while reducing hallucination.\nComparisons with reasoning models highlight the importance of accurate\nrecognition as a prerequisite for reliable comprehension. Quantitative analyses\nvalidate CalliReader's efficiency; evaluations on document and real-world\nbenchmarks confirm its robust generalization ability.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 06:19:32 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 01:50:14 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Luo', 'Yuxuan', ''], ['Tang', 'Jiaqi', ''], ['Huang', 'Chenyi', ''], ['Hao', 'Feiyang', ''], ['Lian', 'Zhouhui', '']]","extracted_entities":"[{'text': 'character-wise slicing', 'label': 'Embedding'}, {'text': 'CalliAlign', 'label': 'contextual Embedding'}, {'text': 'embedding instruction tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"embedding instruction tuning","similarity_score":0.507078886}
{"id":2503.06491,"submitter":"Jean Seo","authors":"Jean Seo, Jaeyoon Kim, Hyopil Shin","title":"MoFE: Mixture of Frozen Experts Architecture","comments":"NAACL 2025 Industry","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We propose the Mixture of Frozen Experts (MoFE) architecture, which\nintegrates Parameter-efficient Fine-tuning (PEFT) and the Mixture of Experts\n(MoE) architecture to enhance both training efficiency and model scalability.\nBy freezing the Feed Forward Network (FFN) layers within the MoE framework,\nMoFE significantly reduces the number of trainable parameters, improving\ntraining efficiency while still allowing for effective knowledge transfer from\nthe expert models. This facilitates the creation of models proficient in\nmultiple domains. We conduct experiments to evaluate the trade-offs between\nperformance and efficiency, compare MoFE with other PEFT methodologies, assess\nthe impact of domain expertise in the constituent models, and determine the\noptimal training strategy. The results show that, although there may be some\ntrade-offs in performance, the efficiency gains are substantial, making MoFE a\nreasonable solution for real-world, resource-constrained environments.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 07:24:36 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Seo', 'Jean', ''], ['Kim', 'Jaeyoon', ''], ['Shin', 'Hyopil', '']]","extracted_entities":"[{'text': 'Parameter-efficient Fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"Parameter-efficient Fine-tuning","similarity_score":0.7545099258}
{"id":2503.06504,"submitter":"Nancy Ghangas","authors":"Nancy Ghangas, Ghanasyam Remesh, Venu Gopal Achanta, Shubhrangshu\n  Dasgupta","title":"Dynamics of Light Localization via Coherent Control: The Interplay of\n  Transmission, Absorption and Disorder in Photonic Crystals","comments":"13 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.optics","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  This study examines the dynamic relationship between the Lyapunov exponent,\nabsorption, and structural disorder to exploit localization phenomena in\nphotonic crystals. We study systems where random variations in the refractive\nindex of one of the bilayers introduce disorder, while a defect layer features\nnon-uniform doping with $\\Lambda$-type atoms and enables coherent modulation of\neffective refractive index. The coherent control permits the active tuning of\nabsorption, Lyapunov exponent, and localization characteristics in disordered\nregimes. A striking contrast in the absorption and Lyapunov spectra is observed\nfor band gap and band edge frequencies, highlighting distinct localization\nbehaviors. These findings advance understanding of light-matter interactions\nand field localization in disordered systems, offering pathways for tailored\nphotonic devices.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 08:09:53 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Ghangas', 'Nancy', ''], ['Remesh', 'Ghanasyam', ''], ['Achanta', 'Venu Gopal', ''], ['Dasgupta', 'Shubhrangshu', '']]","extracted_entities":"[{'text': 'active tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"active tuning","similarity_score":0.715613246}
{"id":2503.0652,"submitter":"Yuqi Liu","authors":"Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu,\n  Jiaya Jia","title":"Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive\n  Reinforcement","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.MM","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Traditional methods for reasoning segmentation rely on supervised fine-tuning\nwith categorical labels and simple descriptions, limiting its out-of-domain\ngeneralization and lacking explicit reasoning processes. To address these\nlimitations, we propose Seg-Zero, a novel framework that demonstrates\nremarkable generalizability and derives explicit chain-of-thought reasoning\nthrough cognitive reinforcement. Seg-Zero introduces a decoupled architecture\nconsisting of a reasoning model and a segmentation model. The reasoning model\ninterprets user intentions, generates explicit reasoning chains, and produces\npositional prompts, which are subsequently used by the segmentation model to\ngenerate precious pixel-level masks. We design a sophisticated reward mechanism\nthat integrates both format and accuracy rewards to effectively guide\noptimization directions. Trained exclusively via reinforcement learning with\nGRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot\ngeneralization and exhibits emergent test-time reasoning capabilities.\nExperiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on\nthe ReasonSeg benchmark, surpassing the prior LISA-7B by 18\\%. This significant\nimprovement highlights Seg-Zero's ability to generalize across domains while\npresenting an explicit reasoning process. Code is available at\nhttps:\/\/github.com\/dvlab-research\/Seg-Zero.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 08:48:51 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Liu', 'Yuqi', ''], ['Peng', 'Bohao', ''], ['Zhong', 'Zhisheng', ''], ['Yue', 'Zihao', ''], ['Lu', 'Fanbin', ''], ['Yu', 'Bei', ''], ['Jia', 'Jiaya', '']]","extracted_entities":"[{'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'reasoning model', 'label': 'Neural Language Model'}, {'text': 'reasoning model', 'label': 'Neural Language Model'}, {'text': 'explicit reasoning chains', 'label': 'Chain of thought'}, {'text': 'positional prompts', 'label': 'Prompting'}, {'text': 'segmentation model', 'label': 'Neural Language Model'}, {'text': 'reinforcement learning', 'label': 'Few-shot Learning'}, {'text': 'GRPO', 'label': 'Generative Pre-trained Transformer (GPT)'}]","assigned_concept":"Fine-tuning","matched_keyword":"supervised fine-tuning","similarity_score":0.7449287176}
{"id":2503.06648,"submitter":"Hender Lin","authors":"Hender Lin","title":"Enhancing NLP Robustness and Generalization through LLM-Generated\n  Contrast Sets: A Scalable Framework for Systematic Evaluation and Adversarial\n  Training","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Standard NLP benchmarks often fail to capture vulnerabilities stemming from\ndataset artifacts and spurious correlations. Contrast sets address this gap by\nchallenging models near decision boundaries but are traditionally\nlabor-intensive to create and limited in diversity. This study leverages large\nlanguage models to automate the generation of diverse contrast sets. Using the\nSNLI dataset, we created a 3,000-example contrast set to evaluate and improve\nmodel robustness. Fine-tuning on these contrast sets enhanced performance on\nsystematically perturbed examples, maintained standard test accuracy, and\nmodestly improved generalization to novel perturbations. This automated\napproach offers a scalable solution for evaluating and improving NLP models,\naddressing systematic generalization challenges, and advancing robustness in\nreal-world applications.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 14:52:53 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Lin', 'Hender', '']]","extracted_entities":"[{'text': 'large\\nlanguage models', 'label': 'Large Language Model'}, {'text': 'Fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"Fine-tuning","similarity_score":1.0000001192}
{"id":2407.13766,"submitter":"Tsung-Han Wu","authors":"Tsung-Han Wu, Giscard Biamby, Jerome Quenum, Ritwik Gupta, Joseph E.\n  Gonzalez, Trevor Darrell, David M. Chan","title":"Visual Haystacks: A Vision-Centric Needle-In-A-Haystack Benchmark","comments":"Accepted to ICLR 2025; Project page:\n  https:\/\/visual-haystacks.github.io","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Multimodal Models (LMMs) have made significant strides in visual\nquestion-answering for single images. Recent advancements like long-context\nLMMs have allowed them to ingest larger, or even multiple, images. However, the\nability to process a large number of visual tokens does not guarantee effective\nretrieval and reasoning for multi-image question answering (MIQA), especially\nin real-world applications like photo album searches or satellite imagery\nanalysis. In this work, we first assess the limitations of current benchmarks\nfor long-context LMMs. We address these limitations by introducing a new\nvision-centric, long-context benchmark, \"Visual Haystacks (VHs)\". We\ncomprehensively evaluate both open-source and proprietary models on VHs, and\ndemonstrate that these models struggle when reasoning across potentially\nunrelated images, perform poorly on cross-image reasoning, as well as exhibit\nbiases based on the placement of key information within the context window.\nTowards a solution, we introduce MIRAGE (Multi-Image Retrieval Augmented\nGeneration), an open-source, lightweight visual-RAG framework that processes up\nto 10k images on a single 40G A100 GPU -- far surpassing the 1k-image limit of\ncontemporary models. MIRAGE demonstrates up to 13% performance improvement over\nexisting open-source LMMs on VHs, sets a new state-of-the-art on the RetVQA\nmulti-image QA benchmark, and achieves competitive performance on single-image\nQA with state-of-the-art LMMs. Our dataset, model, and code are available at:\nhttps:\/\/visual-haystacks.github.io.\n","versions":"[{'version': 'v1', 'created': 'Thu, 18 Jul 2024 17:59:30 GMT'}, {'version': 'v2', 'created': 'Thu, 10 Oct 2024 21:03:15 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Feb 2025 17:56:16 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Mar 2025 17:31:27 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Wu', 'Tsung-Han', ''], ['Biamby', 'Giscard', ''], ['Quenum', 'Jerome', ''], ['Gupta', 'Ritwik', ''], ['Gonzalez', 'Joseph E.', ''], ['Darrell', 'Trevor', ''], ['Chan', 'David M.', '']]","extracted_entities":"[{'text': 'Large Multimodal Models', 'label': 'Large Language Model'}, {'text': 'Visual Haystacks', 'label': 'RAG'}, {'text': 'visual-RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"visual-RAG","similarity_score":0.713988781}
{"id":2411.13163,"submitter":"Nabeel Seedat","authors":"Nabeel Seedat, Caterina Tozzi, Andrea Hita Ardiaca, Mihaela van der\n  Schaar, James Weatherall, Adam Taylor","title":"Unlocking Historical Clinical Trial Data with ALIGN: A Compositional\n  Large Language Model System for Medical Coding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The reuse of historical clinical trial data has significant potential to\naccelerate medical research and drug development. However, interoperability\nchallenges, particularly with missing medical codes, hinders effective data\nintegration across studies. While Large Language Models (LLMs) offer a\npromising solution for automated coding without labeled data, current\napproaches face challenges on complex coding tasks. We introduce ALIGN, a novel\ncompositional LLM-based system for automated, zero-shot medical coding. ALIGN\nfollows a three-step process: (1) diverse candidate code generation; (2)\nself-evaluation of codes and (3) confidence scoring and uncertainty estimation\nenabling human deferral to ensure reliability. We evaluate ALIGN on harmonizing\nmedication terms into Anatomical Therapeutic Chemical (ATC) and medical history\nterms into Medical Dictionary for Regulatory Activities (MedDRA) codes\nextracted from 22 immunology trials. ALIGN outperformed the LLM baselines,\nwhile also providing capabilities for trustworthy deployment. For MedDRA\ncoding, ALIGN achieved high accuracy across all levels, matching RAG and\nexcelling at the most specific levels (87-90% for HLGT). For ATC coding, ALIGN\ndemonstrated superior performance, particularly at lower hierarchy levels (ATC\nLevel 4), with 72-73% overall accuracy and 86-89% accuracy for common\nmedications, outperforming baselines by 7-22%. ALIGN's uncertainty-based\ndeferral improved accuracy by 17% to 90% accuracy with 30% deferral, notably\nenhancing performance on uncommon medications. ALIGN achieves this\ncost-efficiently at \\$0.0007 and \\$0.02 per code for GPT-4o-mini and GPT-4o,\nreducing barriers to clinical adoption. ALIGN advances automated medical coding\nfor clinical trial data, contributing to enhanced data interoperability and\nreusability, positioning it as a promising tool to improve clinical research\nand accelerate drug development.\n","versions":"[{'version': 'v1', 'created': 'Wed, 20 Nov 2024 09:59:12 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 13:39:09 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Seedat', 'Nabeel', ''], ['Tozzi', 'Caterina', ''], ['Ardiaca', 'Andrea Hita', ''], ['van der Schaar', 'Mihaela', ''], ['Weatherall', 'James', ''], ['Taylor', 'Adam', '']]","extracted_entities":"[{'text': 'RAG', 'label': 'RAG'}, {'text': 'HLGT', 'label': 'GPT'}, {'text': 'GPT-4o-mini', 'label': 'GPT'}, {'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2412.02592,"submitter":"Junyuan Zhang","authors":"Junyuan Zhang, Qintong Zhang, Bin Wang, Linke Ouyang, Zichen Wen, Ying\n  Li, Ka-Ho Chow, Conghui He, Wentao Zhang","title":"OCR Hinders RAG: Evaluating the Cascading Impact of OCR on\n  Retrieval-Augmented Generation","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external knowledge to reduce hallucinations and incorporate\nup-to-date information without retraining. As an essential part of RAG,\nexternal knowledge bases are commonly built by extracting structured data from\nunstructured PDF documents using Optical Character Recognition (OCR). However,\ngiven the imperfect prediction of OCR and the inherent non-uniform\nrepresentation of structured data, knowledge bases inevitably contain various\nOCR noises. In this paper, we introduce OHRBench, the first benchmark for\nunderstanding the cascading impact of OCR on RAG systems. OHRBench includes\n8,561 carefully selected unstructured document images from seven real-world RAG\napplication domains, along with 8,498 Q&A pairs derived from multimodal\nelements in documents, challenging existing OCR solutions used for RAG. To\nbetter understand OCR's impact on RAG systems, we identify two primary types of\nOCR noise: Semantic Noise and Formatting Noise and apply perturbation to\ngenerate a set of structured data with varying degrees of each OCR noise. Using\nOHRBench, we first conduct a comprehensive evaluation of current OCR solutions\nand reveal that none is competent for constructing high-quality knowledge bases\nfor RAG systems. We then systematically evaluate the impact of these two noise\ntypes and demonstrate the trend relationship between the degree of OCR noise\nand RAG performance. Our OHRBench, including PDF documents, Q&As, and the\nground truth structured data are released at:\nhttps:\/\/github.com\/opendatalab\/OHR-Bench\n","versions":"[{'version': 'v1', 'created': 'Tue, 3 Dec 2024 17:23:47 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 06:46:18 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Zhang', 'Junyuan', ''], ['Zhang', 'Qintong', ''], ['Wang', 'Bin', ''], ['Ouyang', 'Linke', ''], ['Wen', 'Zichen', ''], ['Li', 'Ying', ''], ['Chow', 'Ka-Ho', ''], ['He', 'Conghui', ''], ['Zhang', 'Wentao', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2412.06099,"submitter":"Yiwen Zhu","authors":"Yiwen Zhu, Mathieu Demarne, Kai Deng, Wenjing Wang, Nutan Sahoo, Divya\n  Vermareddy, Hannah Lerner, Yunlei Lu, Swati Bararia, Anjali Bhavan, William\n  Zhang, Xia Li, Katherine Lin, Miso Cilimdzic, and Subru Krishnan","title":"DECO: Life-Cycle Management of Enterprise-Grade Copilots","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI","license":"http:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/","abstract":"  Software engineers frequently grapple with the challenge of accessing\ndisparate documentation and telemetry data, including TroubleShooting Guides\n(TSGs), incident reports, code repositories, and various internal tools\ndeveloped by multiple stakeholders. While on-call duties are inevitable,\nincident resolution becomes even more daunting due to the obscurity of legacy\nsources and the pressures of strict time constraints. To enhance the efficiency\nof on-call engineers (OCEs) and streamline their daily workflows, we introduced\nDECO-a comprehensive framework for developing, deploying, and managing\nenterprise-grade copilots tailored to improve productivity in engineering\nroutines. This paper details the design and implementation of the DECO\nframework, emphasizing its innovative NL2SearchQuery functionality and a\nlightweight agentic framework. These features support efficient and customized\nretrieval-augmented-generation (RAG) algorithms that not only extract relevant\ninformation from diverse sources but also select the most pertinent skills in\nresponse to user queries. This enables the addressing of complex technical\nquestions and provides seamless, automated access to internal resources.\nAdditionally, DECO incorporates a robust mechanism for converting unstructured\nincident logs into user-friendly, structured guides, effectively bridging the\ndocumentation gap.\n  Since its launch in September 2023, DECO has demonstrated its effectiveness\nthrough widespread adoption, enabling tens of thousands of interactions and\nengaging hundreds of monthly active users (MAU) across dozens of organizations\nwithin the company.\n","versions":"[{'version': 'v1', 'created': 'Sun, 8 Dec 2024 23:00:06 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 05:24:19 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhu', 'Yiwen', ''], ['Demarne', 'Mathieu', ''], ['Deng', 'Kai', ''], ['Wang', 'Wenjing', ''], ['Sahoo', 'Nutan', ''], ['Vermareddy', 'Divya', ''], ['Lerner', 'Hannah', ''], ['Lu', 'Yunlei', ''], ['Bararia', 'Swati', ''], ['Bhavan', 'Anjali', ''], ['Zhang', 'William', ''], ['Li', 'Xia', ''], ['Lin', 'Katherine', ''], ['Cilimdzic', 'Miso', ''], ['Krishnan', 'Subru', '']]","extracted_entities":"[{'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2501.1334,"submitter":"Hao Fang","authors":"Hao Fang, Xiaohang Sui, Hongyao Yu, Kuofeng Gao, Jiawei Kong, Sijin\n  Yu, Bin Chen, Hao Wu, Shu-Tao Xia","title":"Retrievals Can Be Detrimental: A Contrastive Backdoor Attack Paradigm on\n  Retrieval-Augmented Diffusion Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Diffusion models (DMs) have recently demonstrated remarkable generation\ncapability. However, their training generally requires huge computational\nresources and large-scale datasets. To solve these, recent studies empower DMs\nwith the advanced Retrieval-Augmented Generation (RAG) technique and propose\nretrieval-augmented diffusion models (RDMs). By incorporating rich knowledge\nfrom an auxiliary database, RAG enhances diffusion models' generation and\ngeneralization ability while significantly reducing model parameters. Despite\nthe great success, RAG may introduce novel security issues that warrant further\ninvestigation. In this paper, we reveal that the RDM is susceptible to backdoor\nattacks by proposing a multimodal contrastive attack approach named BadRDM. Our\nframework fully considers RAG's characteristics and is devised to manipulate\nthe retrieved items for given text triggers, thereby further controlling the\ngenerated contents. Specifically, we first insert a tiny portion of images into\nthe retrieval database as target toxicity surrogates. Subsequently, a malicious\nvariant of contrastive learning is adopted to inject backdoors into the\nretriever, which builds shortcuts from triggers to the toxicity surrogates.\nFurthermore, we enhance the attacks through novel entropy-based selection and\ngenerative augmentation strategies that can derive better toxicity surrogates.\nExtensive experiments on two mainstream tasks demonstrate the proposed BadRDM\nachieves outstanding attack effects while preserving the model's benign\nutility.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 Jan 2025 02:42:28 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 06:55:26 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Fang', 'Hao', ''], ['Sui', 'Xiaohang', ''], ['Yu', 'Hongyao', ''], ['Gao', 'Kuofeng', ''], ['Kong', 'Jiawei', ''], ['Yu', 'Sijin', ''], ['Chen', 'Bin', ''], ['Wu', 'Hao', ''], ['Xia', 'Shu-Tao', '']]","extracted_entities":"[{'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2502.14614,"submitter":"Mingyi Jia","authors":"Mingyi Jia and Junwen Duan and Yan Song and Jianxin Wang","title":"FIND: Fine-grained Information Density Guided Adaptive\n  Retrieval-Augmented Generation for Disease Diagnosis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Retrieval-Augmented Large Language Models (LLMs), which integrate external\nknowledge into LLMs, have shown remarkable performance in various medical\ndomains, including clinical diagnosis. However, existing RAG methods struggle\nto effectively assess task difficulty to make retrieval decisions, thereby\nfailing to meet the clinical requirements for balancing efficiency and\naccuracy. So in this paper, we propose FIND (\\textbf{F}ine-grained\n\\textbf{In}formation \\textbf{D}ensity Guided Adaptive RAG), a novel framework\nthat improves the reliability of RAG in disease diagnosis scenarios. FIND\nincorporates a fine-grained adaptive control module to determine whether\nretrieval is necessary based on the information density of the input. By\noptimizing the retrieval process and implementing a knowledge filtering module,\nFIND ensures that the retrieval is better suited to clinical scenarios.\nExperiments on three Chinese electronic medical record datasets demonstrate\nthat FIND significantly outperforms various baseline methods, highlighting its\neffectiveness in clinical diagnosis tasks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Feb 2025 14:52:36 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 13:13:07 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Jia', 'Mingyi', ''], ['Duan', 'Junwen', ''], ['Song', 'Yan', ''], ['Wang', 'Jianxin', '']]","extracted_entities":"[{'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2502.15723,"submitter":"Anjali Dharmik","authors":"Prakhar Gurawa and Anjali Dharmik","title":"Balancing Content Size in RAG-Text2SQL System","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR cs.AI cs.DB","license":"http:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/","abstract":"  Large Language Models (LLMs) have emerged as a promising solution for\nconverting natural language queries into SQL commands, enabling seamless\ndatabase interaction. However, these Text-to-SQL (Text2SQL) systems face\ninherent limitations, hallucinations, outdated knowledge, and untraceable\nreasoning. To address these challenges, the integration of retrieval-augmented\ngeneration (RAG) with Text2SQL models has gained traction. RAG serves as a\nretrieval mechanism, providing essential contextual information, such as table\nschemas and metadata, to enhance the query generation process. Despite their\npotential, RAG + Text2SQL systems are susceptible to the quality and size of\nretrieved documents. While richer document content can improve schema relevance\nand retrieval accuracy, it also introduces noise, increasing the risk of\nhallucinations and reducing query fidelity as the prompt size of the Text2SQL\nmodel increases. This research investigates the nuanced trade-off between\ndocument size and quality, aiming to strike a balance that optimizes system\nperformance. Key thresholds are identified where performance degradation\noccurs, along with actionable strategies to mitigate these challenges.\nAdditionally, we explore the phenomenon of hallucinations in Text2SQL models,\nemphasizing the critical role of curated document presentation in minimizing\nerrors. Our findings provide a roadmap for enhancing the robustness of RAG +\nText2SQL systems, offering practical insights for real-world applications.\n","versions":"[{'version': 'v1', 'created': 'Tue, 28 Jan 2025 06:06:28 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 03:53:50 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Gurawa', 'Prakhar', ''], ['Dharmik', 'Anjali', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'prompt size', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2502.17506,"submitter":"Namkyeong Lee","authors":"Namkyeong Lee, Edward De Brouwer, Ehsan Hajiramezanali, Tommaso\n  Biancalani, Chanyoung Park, Gabriele Scalia","title":"RAG-Enhanced Collaborative LLM Agents for Drug Discovery","comments":"Machine Learning, Drug Discovery","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advances in large language models (LLMs) have shown great potential to\naccelerate drug discovery. However, the specialized nature of biochemical data\noften necessitates costly domain-specific fine-tuning, posing critical\nchallenges. First, it hinders the application of more flexible general-purpose\nLLMs in cutting-edge drug discovery tasks. More importantly, it impedes the\nrapid integration of the vast amounts of scientific data continuously generated\nthrough experiments and research. To investigate these challenges, we propose\nCLADD, a retrieval-augmented generation (RAG)-empowered agentic system tailored\nto drug discovery tasks. Through the collaboration of multiple LLM agents,\nCLADD dynamically retrieves information from biomedical knowledge bases,\ncontextualizes query molecules, and integrates relevant evidence to generate\nresponses -- all without the need for domain-specific fine-tuning. Crucially,\nwe tackle key obstacles in applying RAG workflows to biochemical data,\nincluding data heterogeneity, ambiguity, and multi-source integration. We\ndemonstrate the flexibility and effectiveness of this framework across a\nvariety of drug discovery tasks, showing that it outperforms general-purpose\nand domain-specific LLMs as well as traditional deep learning approaches.\n","versions":"[{'version': 'v1', 'created': 'Sat, 22 Feb 2025 00:12:52 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 12:11:58 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Lee', 'Namkyeong', ''], ['De Brouwer', 'Edward', ''], ['Hajiramezanali', 'Ehsan', ''], ['Biancalani', 'Tommaso', ''], ['Park', 'Chanyoung', ''], ['Scalia', 'Gabriele', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'domain-specific fine-tuning', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'domain-specific fine-tuning', 'label': 'Fine-tuning'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2502.17832,"submitter":"Hyeonjeong Ha","authors":"Hyeonjeong Ha, Qiusi Zhan, Jeonghwan Kim, Dimitrios Bralios,\n  Saikrishna Sanniboina, Nanyun Peng, Kai-Wei Chang, Daniel Kang, Heng Ji","title":"MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning\n  Attacks","comments":"Code is available at https:\/\/github.com\/HyeonjeongHa\/MM-PoisonRAG","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CR cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multimodal large language models (MLLMs) equipped with Retrieval Augmented\nGeneration (RAG) leverage both their rich parametric knowledge and the dynamic,\nexternal knowledge to excel in tasks such as Question Answering. While RAG\nenhances MLLMs by grounding responses in query-relevant external knowledge,\nthis reliance poses a critical yet underexplored safety risk: knowledge\npoisoning attacks, where misinformation or irrelevant knowledge is\nintentionally injected into external knowledge bases to manipulate model\noutputs to be incorrect and even harmful. To expose such vulnerabilities in\nmultimodal RAG, we propose MM-PoisonRAG, a novel knowledge poisoning attack\nframework with two attack strategies: Localized Poisoning Attack (LPA), which\ninjects query-specific misinformation in both text and images for targeted\nmanipulation, and Globalized Poisoning Attack (GPA) to provide false guidance\nduring MLLM generation to elicit nonsensical responses across all queries. We\nevaluate our attacks across multiple tasks, models, and access settings,\ndemonstrating that LPA successfully manipulates the MLLM to generate\nattacker-controlled answers, with a success rate of up to 56% on MultiModalQA.\nMoreover, GPA completely disrupts model generation to 0% accuracy with just a\nsingle irrelevant knowledge injection. Our results highlight the urgent need\nfor robust defenses against knowledge poisoning to safeguard multimodal RAG\nframeworks.\n","versions":"[{'version': 'v1', 'created': 'Tue, 25 Feb 2025 04:23:59 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 02:52:43 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Ha', 'Hyeonjeong', ''], ['Zhan', 'Qiusi', ''], ['Kim', 'Jeonghwan', ''], ['Bralios', 'Dimitrios', ''], ['Sanniboina', 'Saikrishna', ''], ['Peng', 'Nanyun', ''], ['Chang', 'Kai-Wei', ''], ['Kang', 'Daniel', ''], ['Ji', 'Heng', '']]","extracted_entities":"[{'text': 'Multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'MLLM', 'label': 'Large Language Model'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.01478,"submitter":"Yijie Xu","authors":"Lu Dai, Yijie Xu, Jinhui Ye, Hao Liu, Hui Xiong","title":"SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity\n  Reduction","comments":"ICLR 2025 Spotlight","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Large Language Models (LLMs) have demonstrated improved generation\nperformance by incorporating externally retrieved knowledge, a process known as\nretrieval-augmented generation (RAG). Despite the potential of this approach,\nexisting studies evaluate RAG effectiveness by 1) assessing retrieval and\ngeneration components jointly, which obscures retrieval's distinct\ncontribution, or 2) examining retrievers using traditional metrics such as\nNDCG, which creates a gap in understanding retrieval's true utility in the\noverall generation process. To address the above limitations, in this work, we\nintroduce an automatic evaluation method that measures retrieval quality\nthrough the lens of information gain within the RAG framework. Specifically, we\npropose Semantic Perplexity (SePer), a metric that captures the LLM's internal\nbelief about the correctness of the retrieved information. We quantify the\nutility of retrieval by the extent to which it reduces semantic perplexity\npost-retrieval. Extensive experiments demonstrate that SePer not only aligns\nclosely with human preferences but also offers a more precise and efficient\nevaluation of retrieval utility across diverse RAG scenarios.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 12:37:34 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Mar 2025 07:51:56 GMT'}, {'version': 'v3', 'created': 'Wed, 5 Mar 2025 05:24:54 GMT'}, {'version': 'v4', 'created': 'Wed, 12 Mar 2025 08:49:58 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Dai', 'Lu', ''], ['Xu', 'Yijie', ''], ['Ye', 'Jinhui', ''], ['Liu', 'Hao', ''], ['Xiong', 'Hui', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'retrieval-augmented generation', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.04789,"submitter":"Hwanjun Song","authors":"Hwanjun Song and Jeonghwan Choi and Minseok Kim","title":"Ext2Gen: Alignment through Unified Extraction and Generation for Robust\n  Retrieval-Augmented Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Retrieval-augmented generation (RAG) enhances LLMs by integrating external\nknowledge, but generation remains fragile due to the uncertain placement of\nrelevant chunks and retrieval-induced information overload, leading to\nhallucinations. We propose Ext2Gen, a novel extract-then-generate model that\nenhances RAG robustness by first extracting query-relevant sentences before\ngenerating answers. To optimize this model, we employ preference alignment\nthrough pairwise feedback learning, enabling the model to generate robust\nanswers regardless of variations in retrieval results. Extensive experiments\ndemonstrate that Ext2Gen effectively identifies query-relevant sentences with\nhigh precision and recall, leading to highly reliable answers. Furthermore,\ndeploying our model in a RAG environment reveals that it not only boosts the\nperformance of the base LLM but also synergizes with advanced retrieval\nstrategies like query expansion. The model is available at\nhttps:\/\/huggingface.co\/DISLab\/Ext2Gen-8B-R2.\n","versions":"[{'version': 'v1', 'created': 'Fri, 28 Feb 2025 06:46:53 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 14:42:18 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Song', 'Hwanjun', ''], ['Choi', 'Jeonghwan', ''], ['Kim', 'Minseok', '']]","extracted_entities":"[{'text': 'Retrieval-augmented generation', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'pairwise feedback learning', 'label': 'Few-shot Learning'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.06567,"submitter":"Yao Cheng","authors":"Yao Cheng, Yibo Zhao, Jiapeng Zhu, Yao Liu, Xing Sun, Xiang Li","title":"Human Cognition Inspired RAG with Knowledge Graph for Complex Problem\n  Solving","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models (LLMs) have demonstrated transformative potential\nacross various domains, yet they face significant challenges in knowledge\nintegration and complex problem reasoning, often leading to hallucinations and\nunreliable outputs. Retrieval-Augmented Generation (RAG) has emerged as a\npromising solution to enhance LLMs accuracy by incorporating external\nknowledge. However, traditional RAG systems struggle with processing complex\nrelational information and multi-step reasoning, limiting their effectiveness\nin advanced problem-solving tasks. To address these limitations, we propose\nCogGRAG, a cognition inspired graph-based RAG framework, designed to improve\nLLMs performance in Knowledge Graph Question Answering (KGQA). Inspired by the\nhuman cognitive process of decomposing complex problems and performing\nself-verification, our framework introduces a three-stage methodology:\ndecomposition, retrieval, and reasoning with self-verification. By integrating\nthese components, CogGRAG enhances the accuracy of LLMs in complex problem\nsolving. We conduct systematic experiments with three LLM backbones on four\nbenchmark datasets, where CogGRAG outperforms the baselines.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 11:50:39 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Cheng', 'Yao', ''], ['Zhao', 'Yibo', ''], ['Zhu', 'Jiapeng', ''], ['Liu', 'Yao', ''], ['Sun', 'Xing', ''], ['Li', 'Xiang', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2311.14747,"submitter":"Po-Yuan Mao","authors":"Do Huu Dat, Po Yuan Mao, Tien Hoang Nguyen, Wray Buntine, Mohammed\n  Bennamoun","title":"HOPE: A Memory-Based and Composition-Aware Framework for Zero-Shot\n  Learning with Hopfield Network and Soft Mixture of Experts","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Compositional Zero-Shot Learning (CZSL) has emerged as an essential paradigm\nin machine learning, aiming to overcome the constraints of traditional\nzero-shot learning by incorporating compositional thinking into its\nmethodology. Conventional zero-shot learning has difficulty managing unfamiliar\ncombinations of seen and unseen classes because it depends on pre-defined class\nembeddings. In contrast, Compositional Zero-Shot Learning leverages the\ninherent hierarchies and structural connections among classes, creating new\nclass representations by combining attributes, components, or other semantic\nelements. In our paper, we propose a novel framework that for the first time\ncombines the Modern \\underline{H}opfield Network with a Mixture \\underline{o}f\n\\underline{E}x\\underline{p}erts (HOPE) to classify the compositions of\npreviously unseen objects. Specifically, the Modern Hopfield Network creates a\nmemory that stores label prototypes and identifies relevant labels for a given\ninput image. Subsequently, the Mixture of Expert models integrates the image\nwith the appropriate prototype to produce the final composition classification.\nOur approach achieves SOTA performance on several benchmarks, including\nMIT-States and UT-Zappos. We also examine how each component contributes to\nimproved generalization.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 Nov 2023 07:32:20 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 09:42:36 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Dat', 'Do Huu', ''], ['Mao', 'Po Yuan', ''], ['Nguyen', 'Tien Hoang', ''], ['Buntine', 'Wray', ''], ['Bennamoun', 'Mohammed', '']]","extracted_entities":"[{'text': 'Compositional Zero-Shot Learning', 'label': 'Zero-shot Learning'}, {'text': 'zero-shot learning', 'label': 'Zero-shot Learning'}, {'text': 'Conventional zero-shot learning', 'label': 'Zero-shot Learning'}, {'text': 'pre-defined class\\nembeddings', 'label': 'Zero-shot Learning'}, {'text': 'Compositional Zero-Shot Learning', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot learning","similarity_score":1.0000001192}
{"id":2403.1732,"submitter":"Giulio Turrisi","authors":"Zhi Su, Xiaoyu Huang, Daniel Ordo\\~nez-Apraez, Yunfei Li, Zhongyu Li,\n  Qiayuan Liao, Giulio Turrisi, Massimiliano Pontil, Claudio Semini, Yi Wu,\n  Koushil Sreenath","title":"Leveraging Symmetry in RL-based Legged Locomotion Control","comments":null,"journal-ref":"2024 IEEE\/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), pp. 6899-6906","doi":"10.1109\/IROS58592.2024.10802439","report-no":null,"categories":"cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Model-free reinforcement learning is a promising approach for autonomously\nsolving challenging robotics control problems, but faces exploration difficulty\nwithout information of the robot's kinematics and dynamics morphology. The\nunder-exploration of multiple modalities with symmetric states leads to\nbehaviors that are often unnatural and sub-optimal. This issue becomes\nparticularly pronounced in the context of robotic systems with morphological\nsymmetries, such as legged robots for which the resulting asymmetric and\naperiodic behaviors compromise performance, robustness, and transferability to\nreal hardware. To mitigate this challenge, we can leverage symmetry to guide\nand improve the exploration in policy learning via equivariance\/invariance\nconstraints. In this paper, we investigate the efficacy of two approaches to\nincorporate symmetry: modifying the network architectures to be strictly\nequivariant\/invariant, and leveraging data augmentation to approximate\nequivariant\/invariant actor-critics. We implement the methods on challenging\nloco-manipulation and bipedal locomotion tasks and compare with an\nunconstrained baseline. We find that the strictly equivariant policy\nconsistently outperforms other methods in sample efficiency and task\nperformance in simulation. In addition, symmetry-incorporated approaches\nexhibit better gait quality, higher robustness and can be deployed zero-shot in\nreal-world experiments.\n","versions":"[{'version': 'v1', 'created': 'Tue, 26 Mar 2024 02:02:35 GMT'}, {'version': 'v2', 'created': 'Wed, 27 Mar 2024 02:39:30 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 12:43:09 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Su', 'Zhi', ''], ['Huang', 'Xiaoyu', ''], ['Ordo\u00f1ez-Apraez', 'Daniel', ''], ['Li', 'Yunfei', ''], ['Li', 'Zhongyu', ''], ['Liao', 'Qiayuan', ''], ['Turrisi', 'Giulio', ''], ['Pontil', 'Massimiliano', ''], ['Semini', 'Claudio', ''], ['Wu', 'Yi', ''], ['Sreenath', 'Koushil', '']]","extracted_entities":"[{'text': 'zero-shot', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot","similarity_score":0.7800109386}
{"id":2404.03906,"submitter":"Nimrod Shabtay","authors":"Nimrod Shabtay, Eli Schwartz, and Raja Giryes","title":"Deep Phase Coded Image Prior","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Phase-coded imaging is a computational imaging method designed to tackle\ntasks such as passive depth estimation and extended depth of field (EDOF) using\ndepth cues inserted during image capture. Most of the current deep\nlearning-based methods for depth estimation or all-in-focus imaging require a\ntraining dataset with high-quality depth maps and an optimal focus point at\ninfinity for all-in-focus images. Such datasets are difficult to create,\nusually synthetic, and require external graphic programs. We propose a new\nmethod named \"Deep Phase Coded Image Prior\" (DPCIP) for jointly recovering the\ndepth map and all-in-focus image from a coded-phase image using solely the\ncaptured image and the optical information of the imaging system. Our approach\ndoes not depend on any specific dataset and surpasses prior supervised\ntechniques utilizing the same imaging system. This improvement is achieved\nthrough the utilization of a problem formulation based on implicit neural\nrepresentation (INR) and deep image prior (DIP). Due to our zero-shot method,\nwe overcome the barrier of acquiring accurate ground-truth data of depth maps\nand all-in-focus images for each new phase-coded system introduced. This allows\nfocusing mainly on developing the imaging system, and not on ground-truth data\ncollection.\n","versions":"[{'version': 'v1', 'created': 'Fri, 5 Apr 2024 05:58:40 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 09:34:49 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Shabtay', 'Nimrod', ''], ['Schwartz', 'Eli', ''], ['Giryes', 'Raja', '']]","extracted_entities":"[{'text': 'zero-shot method', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot method","similarity_score":0.7310518026}
{"id":2406.03032,"submitter":"Man Liu","authors":"Man Liu, Huihui Bai, Feng Li, Chunjie Zhang, Yunchao Wei, Tat-Seng\n  Chua, Yao Zhao","title":"Attend and Enrich: Enhanced Visual Prompt for Zero-Shot Learning","comments":"Accepted by AAAI 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Zero-shot learning (ZSL) endeavors to transfer knowledge from seen categories\nto recognize unseen categories, which mostly relies on the semantic-visual\ninteractions between image and attribute tokens. Recently, prompt learning has\nemerged in ZSL and demonstrated significant potential as it allows the\nzero-shot transfer of diverse visual concepts to downstream tasks. However,\ncurrent methods explore the fixed adaption of learnable prompt on seen domains,\nwhich makes them over-emphasize the primary visual features observed during\ntraining, limiting their generalization capabilities to unseen domains. In this\nwork, we propose AENet, which endows semantic information into the visual\nprompt to distill semantic-enhanced prompt for visual representation\nenrichment, enabling effective knowledge transfer for ZSL. AENet comprises two\nkey steps: 1) exploring the concept-harmonized tokens for the visual and\nattribute modalities, grounded on the modal-sharing token that represents\nconsistent visual-semantic concepts; and 2) yielding semantic-enhanced prompt\nvia the visual residual refinement unit with attribute consistency supervision.\nThese are further integrated with primary visual features to attend to\nsemantic-related information for visual enhancement, thus strengthening\ntransferable ability. Experimental results on three benchmarks show that our\nAENet outperforms existing state-of-the-art ZSL methods. The code is provided\nin the zip file of supplementary materials.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Jun 2024 07:59:48 GMT'}, {'version': 'v2', 'created': 'Tue, 10 Dec 2024 04:37:06 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 03:48:20 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Liu', 'Man', ''], ['Bai', 'Huihui', ''], ['Li', 'Feng', ''], ['Zhang', 'Chunjie', ''], ['Wei', 'Yunchao', ''], ['Chua', 'Tat-Seng', ''], ['Zhao', 'Yao', '']]","extracted_entities":"[{'text': 'Zero-shot learning', 'label': 'Zero-shot Learning'}, {'text': 'ZSL', 'label': 'Zero-shot Learning'}, {'text': 'visual\\nprompt', 'label': 'Prompting'}, {'text': 'semantic-enhanced prompt', 'label': 'Prompting'}, {'text': 'ZSL', 'label': 'Few-shot Learning'}, {'text': 'semantic-enhanced prompt', 'label': 'Prompting'}, {'text': 'ZSL', 'label': 'Few-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"Zero-shot learning","similarity_score":1.0000001192}
{"id":2411.10745,"submitter":"Jeonghyeok Do","authors":"Jeonghyeok Do, Munchurl Kim","title":"TDSM: Triplet Diffusion for Skeleton-Text Matching in Zero-Shot Action\n  Recognition","comments":"Please visit our project page at\n  https:\/\/kaist-viclab.github.io\/TDSM_site\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We firstly present a diffusion-based action recognition with zero-shot\nlearning for skeleton inputs. In zero-shot skeleton-based action recognition,\naligning skeleton features with the text features of action labels is essential\nfor accurately predicting unseen actions. Previous methods focus on direct\nalignment between skeleton and text latent spaces, but the modality gaps\nbetween these spaces hinder robust generalization learning. Motivated from the\nremarkable performance of text-to-image diffusion models, we leverage their\nalignment capabilities between different modalities mostly by focusing on the\ntraining process during reverse diffusion rather than using their generative\npower. Based on this, our framework is designed as a Triplet Diffusion for\nSkeleton-Text Matching (TDSM) method which aligns skeleton features with text\nprompts through reverse diffusion, embedding the prompts into the unified\nskeleton-text latent space to achieve robust matching. To enhance\ndiscriminative power, we introduce a novel triplet diffusion (TD) loss that\nencourages our TDSM to correct skeleton-text matches while pushing apart\nincorrect ones. Our TDSM significantly outperforms the very recent\nstate-of-the-art methods with large margins of 2.36%-point to 13.05%-point,\ndemonstrating superior accuracy and scalability in zero-shot settings through\neffective skeleton-text matching.\n","versions":"[{'version': 'v1', 'created': 'Sat, 16 Nov 2024 08:55:18 GMT'}, {'version': 'v2', 'created': 'Fri, 22 Nov 2024 15:49:47 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 04:40:07 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Do', 'Jeonghyeok', ''], ['Kim', 'Munchurl', '']]","extracted_entities":"[{'text': 'zero-shot\\nlearning', 'label': 'Zero-shot Learning'}, {'text': 'text\\nprompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot\nlearning","similarity_score":1.0000001192}
{"id":2411.15933,"submitter":"Klara Janouskova","authors":"Klara Janouskova, Cristian Gavrus, Jiri Matas","title":"Bringing the Context Back into Object Recognition, Robustly","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In object recognition, both the subject of interest (referred to as\nforeground, FG, for simplicity) and its surrounding context (background, BG)\nmay play an important role. However, standard supervised learning often leads\nto unintended over-reliance on the BG, limiting model robustness in real-world\ndeployment settings. The problem is mainly addressed by suppressing the BG,\nsacrificing context information for improved generalization.\n  We propose \"Localize to Recognize Robustly\" (L2R2), a novel recognition\napproach which exploits the benefits of context-aware classification while\nmaintaining robustness to distribution shifts. L2R2 leverages advances in\nzero-shot detection to localize the FG before recognition. It improves the\nperformance of both standard recognition with supervised training, as well as\nmultimodal zero-shot recognition with VLMs, while being robust to long-tail BGs\nand distribution shifts. The results confirm localization before recognition is\npossible for a wide range of datasets and they highlight the limits of object\ndetection on others\n","versions":"[{'version': 'v1', 'created': 'Sun, 24 Nov 2024 17:39:39 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 12:08:58 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Janouskova', 'Klara', ''], ['Gavrus', 'Cristian', ''], ['Matas', 'Jiri', '']]","extracted_entities":"[{'text': 'zero-shot detection', 'label': 'Zero-shot Learning'}, {'text': 'multimodal zero-shot recognition', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"multimodal zero-shot recognition","similarity_score":0.798302412}
{"id":2411.19418,"submitter":"Siddhant Agarwal","authors":"Siddhant Agarwal, Harshit Sikchi, Peter Stone, Amy Zhang","title":"Proto Successor Measure: Representing the Behavior Space of an RL Agent","comments":"Under submission, 20 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Having explored an environment, intelligent agents should be able to transfer\ntheir knowledge to most downstream tasks within that environment without\nadditional interactions. Referred to as \"zero-shot learning\", this ability\nremains elusive for general-purpose reinforcement learning algorithms. While\nrecent works have attempted to produce zero-shot RL agents, they make\nassumptions about the nature of the tasks or the structure of the MDP. We\npresent Proto Successor Measure: the basis set for all possible behaviors of a\nReinforcement Learning Agent in a dynamical system. We prove that any possible\nbehavior (represented using visitation distributions) can be represented using\nan affine combination of these policy-independent basis functions. Given a\nreward function at test time, we simply need to find the right set of linear\nweights to combine these bases corresponding to the optimal policy. We derive a\npractical algorithm to learn these basis functions using reward-free\ninteraction data from the environment and show that our approach can produce\nthe optimal policy at test time for any given reward function without\nadditional environmental interactions. Project page:\nhttps:\/\/agarwalsiddhant10.github.io\/projects\/psm.html.\n","versions":"[{'version': 'v1', 'created': 'Fri, 29 Nov 2024 00:09:39 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 17:41:54 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Agarwal', 'Siddhant', ''], ['Sikchi', 'Harshit', ''], ['Stone', 'Peter', ''], ['Zhang', 'Amy', '']]","extracted_entities":"[{'text': 'zero-shot learning', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot learning","similarity_score":1.0000001192}
{"id":2412.02837,"submitter":"Sarthak Kumar Maharana","authors":"Sarthak Kumar Maharana, Baoming Zhang, Leonid Karlinsky, Rogerio\n  Feris, Yunhui Guo","title":"$\\texttt{BATCLIP}$: Bimodal Online Test-Time Adaptation for CLIP","comments":"Preprint. Under review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Although open-vocabulary classification models like Contrastive Language\nImage Pretraining (CLIP) have demonstrated strong zero-shot learning\ncapabilities, their robustness to common image corruptions remains poorly\nunderstood. Through extensive experiments, we show that zero-shot CLIP lacks\nrobustness to common image corruptions during test-time, necessitating the\nadaptation of CLIP to unlabeled corrupted images using test-time adaptation\n(TTA). However, we found that existing TTA methods have severe limitations in\nadapting CLIP due to their unimodal nature. To address these limitations, we\npropose $\\texttt{BATCLIP}$, a bimodal $\\textbf{online}$ TTA method designed to\nimprove CLIP's robustness to common image corruptions. The key insight of our\napproach is not only to adapt the visual encoders for improving image features\nbut also to strengthen the alignment between image and text features by\npromoting a stronger association between the image class prototype, computed\nusing pseudo-labels, and the corresponding text feature. We evaluate our\napproach on benchmark image corruption datasets and achieve state-of-the-art\nresults in online TTA for CLIP. Furthermore, we evaluate our proposed TTA\napproach on various domain generalization datasets to demonstrate its\ngeneralization capabilities.\n","versions":"[{'version': 'v1', 'created': 'Tue, 3 Dec 2024 21:02:14 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 06:10:48 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Maharana', 'Sarthak Kumar', ''], ['Zhang', 'Baoming', ''], ['Karlinsky', 'Leonid', ''], ['Feris', 'Rogerio', ''], ['Guo', 'Yunhui', '']]","extracted_entities":"[{'text': 'zero-shot learning', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot learning","similarity_score":1.0000001192}
{"id":2501.13859,"submitter":"Shiyu Zhang","authors":"Shiyu Zhang, Cheng Yan, Yang Liu, Chenchen Jing, Lei Zhou, Wenjun Wang","title":"Learning Visual Proxy for Compositional Zero-Shot Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Compositional Zero-Shot Learning (CZSL) aims to recognize novel\nattribute-object compositions by leveraging knowledge from seen compositions.\nExisting methods align textual prototypes with visual features through\nVision-Language Models (VLMs), but they face two key limitations: (1) modality\ngaps hinder the discrimination of semantically similar composition pairs, and\n(2) single-modal textual prototypes lack fine-grained visual cues, creating\nbottlenecks in VLM-based CZSL. In this paper, we introduce Visual Proxy\nLearning, a novel approach that facilitates the learning of distinct visual\ndistributions, effectively reducing the modality gap and improving\ncompositional generalization performance. Specifically, we initialize visual\nproxies for various attributes, objects, and their compositions using text\nrepresentations. By optimizing the visual space, we capture fine-grained visual\ncues and guide the learning of more discriminative visual representations for\nattributes, objects and compositions. Furthermore, we propose an effective\nCross-Modal Joint Learning (CMJL) strategy that imposes cross-modal constraints\nbetween the original text-image space and the fine-grained visual space. This\napproach not only boosts generalization for previously unseen composition pairs\nbut also sharpens the discrimination of similar pairs, fostering more robust\nand precise learning. Extensive experiments demonstrate state-of-the-art\nperformance in closed-world scenarios and competitive open-world results across\nfour established CZSL benchmarks, validating the effectiveness of our approach\nin advancing compositional generalization.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 Jan 2025 17:30:27 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 05:46:59 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 04:04:32 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhang', 'Shiyu', ''], ['Yan', 'Cheng', ''], ['Liu', 'Yang', ''], ['Jing', 'Chenchen', ''], ['Zhou', 'Lei', ''], ['Wang', 'Wenjun', '']]","extracted_entities":"[{'text': 'Compositional Zero-Shot Learning', 'label': 'Zero-shot Learning'}, {'text': 'CZSL', 'label': 'Zero-shot Learning'}, {'text': 'Visual Proxy\\nLearning', 'label': 'Zero-shot Learning'}, {'text': 'CZSL', 'label': 'Few-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"Compositional Zero-Shot Learning","similarity_score":0.8895157576}
{"id":2501.15211,"submitter":"Yuanze Hu","authors":"Siqi Wang, Yuanze Hu, Xinwang Liu, Siwei Wang, Guangpu Wang, Chuanfu\n  Xu, Jie Liu, Ping Chen","title":"\"Stones from Other Hills can Polish Jade\": Zero-shot Anomaly Image\n  Synthesis via Cross-domain Anomaly Injection","comments":"10 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Industrial image anomaly detection (IAD) is a pivotal topic with huge value.\nDue to anomaly's nature, real anomalies in a specific modern industrial domain\n(i.e. domain-specific anomalies) are usually too rare to collect, which\nseverely hinders IAD. Thus, zero-shot anomaly synthesis (ZSAS), which\nsynthesizes pseudo anomaly images without any domain-specific anomaly, emerges\nas a vital technique for IAD. However, existing solutions are either unable to\nsynthesize authentic pseudo anomalies, or require cumbersome training. Thus, we\nfocus on ZSAS and propose a brand-new paradigm that can realize both authentic\nand training-free ZSAS. It is based on a chronically-ignored fact: Although\ndomain-specific anomalies are rare, real anomalies from other domains (i.e.\ncross-domain anomalies) are actually abundant and directly applicable to ZSAS.\nSpecifically, our new ZSAS paradigm makes three-fold contributions: First, we\npropose a novel method named Cross-domain Anomaly Injection (CAI), which\ndirectly exploits cross-domain anomalies to enable highly authentic ZSAS in a\ntraining-free manner. Second, to supply CAI with sufficient cross-domain\nanomalies, we build the first Domain-agnostic Anomaly Dataset within our best\nknowledge, which provides ZSAS with abundant real anomaly patterns. Third, we\npropose a CAI-guided Diffusion Mechanism, which further breaks the quantity\nlimit of real anomalies and enable unlimited anomaly synthesis. Our\nhead-to-head comparison with existing ZSAS solutions justifies our paradigm's\nsuperior performance for IAD and demonstrates it as an effective and pragmatic\nZSAS solution.\n","versions":"[{'version': 'v1', 'created': 'Sat, 25 Jan 2025 13:30:03 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 12:58:44 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Wang', 'Siqi', ''], ['Hu', 'Yuanze', ''], ['Liu', 'Xinwang', ''], ['Wang', 'Siwei', ''], ['Wang', 'Guangpu', ''], ['Xu', 'Chuanfu', ''], ['Liu', 'Jie', ''], ['Chen', 'Ping', '']]","extracted_entities":"[{'text': 'zero-shot anomaly synthesis', 'label': 'Zero-shot Learning'}, {'text': 'ZSAS', 'label': 'Zero-shot Learning'}, {'text': 'ZSAS', 'label': 'Zero-shot Learning'}, {'text': 'ZSAS', 'label': 'Zero-shot Learning'}, {'text': 'ZSAS', 'label': 'Zero-shot Learning'}, {'text': 'ZSAS', 'label': 'Zero-shot Learning'}, {'text': 'Cross-domain Anomaly Injection', 'label': 'Zero-shot Learning'}, {'text': 'ZSAS', 'label': 'Zero-shot Learning'}, {'text': 'CAI', 'label': 'Zero-shot Learning'}, {'text': 'ZSAS', 'label': 'Zero-shot Learning'}, {'text': 'ZSAS', 'label': 'Zero-shot Learning'}, {'text': 'ZSAS', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot anomaly synthesis","similarity_score":0.6106444597}
{"id":2503.00723,"submitter":"Yiyang Liu","authors":"Yiyang Liu, James Chenhao Liang, Ruixiang Tang, Yugyung Lee, Majid\n  Rabbani, Sohail Dianat, Raghuveer Rao, Lifu Huang, Dongfang Liu, Qifan Wang,\n  Cheng Han","title":"Re-Imagining Multimodal Instruction Tuning: A Representation View","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multimodal instruction tuning has proven to be an effective strategy for\nachieving zero-shot generalization by fine-tuning pre-trained Large Multimodal\nModels (LMMs) with instruction-following data. However, as the scale of LMMs\ncontinues to grow, fully fine-tuning these models has become highly\nparameter-intensive. Although Parameter-Efficient Fine-Tuning (PEFT) methods\nhave been introduced to reduce the number of tunable parameters, a significant\nperformance gap remains compared to full fine-tuning. Furthermore, existing\nPEFT approaches are often highly parameterized, making them difficult to\ninterpret and control. In light of this, we introduce Multimodal Representation\nTuning (MRT), a novel approach that focuses on directly editing semantically\nrich multimodal representations to achieve strong performance and provide\nintuitive control over LMMs. Empirical results show that our method surpasses\ncurrent state-of-the-art baselines with significant performance gains (e.g.,\n1580.40 MME score) while requiring substantially fewer tunable parameters\n(e.g., 0.03% parameters). Additionally, we conduct experiments on editing\ninstrumental tokens within multimodal representations, demonstrating that\ndirect manipulation of these representations enables simple yet effective\ncontrol over network behavior.\n","versions":"[{'version': 'v1', 'created': 'Sun, 2 Mar 2025 04:11:03 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 22:44:30 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Liu', 'Yiyang', ''], ['Liang', 'James Chenhao', ''], ['Tang', 'Ruixiang', ''], ['Lee', 'Yugyung', ''], ['Rabbani', 'Majid', ''], ['Dianat', 'Sohail', ''], ['Rao', 'Raghuveer', ''], ['Huang', 'Lifu', ''], ['Liu', 'Dongfang', ''], ['Wang', 'Qifan', ''], ['Han', 'Cheng', '']]","extracted_entities":"[{'text': 'Multimodal instruction tuning', 'label': 'Fine-tuning'}, {'text': 'zero-shot generalization', 'label': 'Zero-shot Learning'}, {'text': 'Multimodal Representation\\nTuning', 'label': 'Fine-tuning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot generalization","similarity_score":0.8053859472}
{"id":2503.03734,"submitter":"Letian Fu","authors":"Huang Huang, Fangchen Liu, Letian Fu, Tingfan Wu, Mustafa Mukadam,\n  Jitendra Malik, Ken Goldberg, Pieter Abbeel","title":"OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature\n  Extraction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Vision-Language-Action (VLA) models aim to predict robotic actions based on\nvisual observations and language instructions. Existing approaches require\nfine-tuning pre-trained visionlanguage models (VLMs) as visual and language\nfeatures are independently fed into downstream policies, degrading the\npre-trained semantic alignments. We propose OTTER, a novel VLA architecture\nthat leverages these existing alignments through explicit, text-aware visual\nfeature extraction. Instead of processing all visual features, OTTER\nselectively extracts and passes only task-relevant visual features that are\nsemantically aligned with the language instruction to the policy transformer.\nThis allows OTTER to keep the pre-trained vision-language encoders frozen.\nThereby, OTTER preserves and utilizes the rich semantic understanding learned\nfrom large-scale pre-training, enabling strong zero-shot generalization\ncapabilities. In simulation and real-world experiments, OTTER significantly\noutperforms existing VLA models, demonstrating strong zeroshot generalization\nto novel objects and environments. Video, code, checkpoints, and dataset:\nhttps:\/\/ottervla.github.io\/.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 18:44:48 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 03:17:25 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Huang', 'Huang', ''], ['Liu', 'Fangchen', ''], ['Fu', 'Letian', ''], ['Wu', 'Tingfan', ''], ['Mukadam', 'Mustafa', ''], ['Malik', 'Jitendra', ''], ['Goldberg', 'Ken', ''], ['Abbeel', 'Pieter', '']]","extracted_entities":"[{'text': 'zero-shot generalization', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot generalization","similarity_score":0.8053859472}
{"id":2503.06442,"submitter":"Hao Tang","authors":"Yu Liu, Hao Tang, Haiqi Zhang, Jing Qin, Zechao Li","title":"OT-DETECTOR: Delving into Optimal Transport for Zero-shot\n  Out-of-Distribution Detection","comments":"The first two authors contributed equally to this work","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.MM","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Out-of-distribution (OOD) detection is crucial for ensuring the reliability\nand safety of machine learning models in real-world applications. While\nzero-shot OOD detection, which requires no training on in-distribution (ID)\ndata, has become feasible with the emergence of vision-language models like\nCLIP, existing methods primarily focus on semantic matching and fail to fully\ncapture distributional discrepancies. To address these limitations, we propose\nOT-DETECTOR, a novel framework that employs Optimal Transport (OT) to quantify\nboth semantic and distributional discrepancies between test samples and ID\nlabels. Specifically, we introduce cross-modal transport mass and transport\ncost as semantic-wise and distribution-wise OOD scores, respectively, enabling\nmore robust detection of OOD samples. Additionally, we present a semantic-aware\ncontent refinement (SaCR) module, which utilizes semantic cues from ID labels\nto amplify the distributional discrepancy between ID and hard OOD samples.\nExtensive experiments on several benchmarks demonstrate that OT-DETECTOR\nachieves state-of-the-art performance across various OOD detection tasks,\nparticularly in challenging hard-OOD scenarios.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 04:47:19 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Liu', 'Yu', ''], ['Tang', 'Hao', ''], ['Zhang', 'Haiqi', ''], ['Qin', 'Jing', ''], ['Li', 'Zechao', '']]","extracted_entities":"[{'text': 'zero-shot OOD detection', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot OOD detection","similarity_score":0.5483837128}
{"id":2503.06467,"submitter":"Qiming Xia","authors":"Shijia Zhao, Qiming Xia, Xusheng Guo, Pufan Zou, Maoji Zheng, Hai Wu,\n  Chenglu Wen, Cheng Wang","title":"SP3D: Boosting Sparsely-Supervised 3D Object Detection via Accurate\n  Cross-Modal Semantic Prompts","comments":"11 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recently, sparsely-supervised 3D object detection has gained great attention,\nachieving performance close to fully-supervised 3D objectors while requiring\nonly a few annotated instances. Nevertheless, these methods suffer challenges\nwhen accurate labels are extremely absent. In this paper, we propose a boosting\nstrategy, termed SP3D, explicitly utilizing the cross-modal semantic prompts\ngenerated from Large Multimodal Models (LMMs) to boost the 3D detector with\nrobust feature discrimination capability under sparse annotation settings.\nSpecifically, we first develop a Confident Points Semantic Transfer (CPST)\nmodule that generates accurate cross-modal semantic prompts through\nboundary-constrained center cluster selection. Based on these accurate semantic\nprompts, which we treat as seed points, we introduce a Dynamic Cluster\nPseudo-label Generation (DCPG) module to yield pseudo-supervision signals from\nthe geometry shape of multi-scale neighbor points. Additionally, we design a\nDistribution Shape score (DS score) that chooses high-quality supervision\nsignals for the initial training of the 3D detector. Experiments on the KITTI\ndataset and Waymo Open Dataset (WOD) have validated that SP3D can enhance the\nperformance of sparsely supervised detectors by a large margin under meager\nlabeling conditions. Moreover, we verified SP3D in the zero-shot setting, where\nits performance exceeded that of the state-of-the-art methods. The code is\navailable at https:\/\/github.com\/xmuqimingxia\/SP3D.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 06:08:04 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhao', 'Shijia', ''], ['Xia', 'Qiming', ''], ['Guo', 'Xusheng', ''], ['Zou', 'Pufan', ''], ['Zheng', 'Maoji', ''], ['Wu', 'Hai', ''], ['Wen', 'Chenglu', ''], ['Wang', 'Cheng', '']]","extracted_entities":"[{'text': 'cross-modal semantic prompts', 'label': 'Prompting'}, {'text': 'Large Multimodal Models', 'label': 'Large Language Model'}, {'text': 'zero-shot setting', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot setting","similarity_score":0.6837423444}
{"id":2412.08099,"submitter":"Sicong Jiang","authors":"Fuqiang Liu, Sicong Jiang, Luis Miranda-Moreno, Seongjin Choi, Lijun\n  Sun","title":"Adversarial Vulnerabilities in Large Language Models for Time Series\n  Forecasting","comments":"AISTATS 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL cs.CR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) have recently demonstrated significant potential\nin time series forecasting, offering impressive capabilities in handling\ncomplex temporal data. However, their robustness and reliability in real-world\napplications remain under-explored, particularly concerning their\nsusceptibility to adversarial attacks. In this paper, we introduce a targeted\nadversarial attack framework for LLM-based time series forecasting. By\nemploying both gradient-free and black-box optimization methods, we generate\nminimal yet highly effective perturbations that significantly degrade the\nforecasting accuracy across multiple datasets and LLM architectures. Our\nexperiments, which include models like LLMTime with GPT-3.5, GPT-4, LLaMa, and\nMistral, TimeGPT, and TimeLLM show that adversarial attacks lead to much more\nsevere performance degradation than random noise, and demonstrate the broad\neffectiveness of our attacks across different LLMs. The results underscore the\ncritical vulnerabilities of LLMs in time series forecasting, highlighting the\nneed for robust defense mechanisms to ensure their reliable deployment in\npractical applications. The code repository can be found at\nhttps:\/\/github.com\/JohnsonJiang1996\/AdvAttack_LLM4TS.\n","versions":"[{'version': 'v1', 'created': 'Wed, 11 Dec 2024 04:53:15 GMT'}, {'version': 'v2', 'created': 'Mon, 6 Jan 2025 20:32:48 GMT'}, {'version': 'v3', 'created': 'Tue, 28 Jan 2025 17:33:40 GMT'}, {'version': 'v4', 'created': 'Wed, 12 Mar 2025 21:35:52 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Liu', 'Fuqiang', ''], ['Jiang', 'Sicong', ''], ['Miranda-Moreno', 'Luis', ''], ['Choi', 'Seongjin', ''], ['Sun', 'Lijun', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GPT-3', 'label': 'GPT'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'Mistral', 'label': 'Mistral'}, {'text': 'TimeLLM', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Mistral","matched_keyword":"Mistral","similarity_score":1.0}
{"id":2102.11196,"submitter":"Frederic Faure","authors":"Fr\\'ed\\'eric Faure and Masato Tsujii","title":"Micro-local analysis of contact Anosov flows and band structure of the\n  Ruelle spectrum","comments":"100 pages","journal-ref":"Comm. Amer. Math. Soc. 4 (2024), 641-745","doi":null,"report-no":null,"categories":"math.DS math-ph math.MP math.SG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We develop a geometrical micro-local analysis of contact Anosov flow, such as\ngeodesic flow on negatively curved manifold. This micro-local analysis is based\non wave-packet transform discussed in arXiv:1706.09307. The main result is that\nthe transfer operator is well approximated (in the high frequency limit) by the\nquantization of the Hamiltonian flow naturally defined from the contact Anosov\nflow and extended to some vector bundle over the symplectization set. This\ngives a few important consequences: the discrete eigenvalues of the generator\nof transfer operators, called Ruelle spectrum, are structured into vertical\nbands. If the right-most band is isolated from the others, most of the Ruelle\nspectrum in it concentrate along a line parallel to the imaginary axis and,\nfurther, the density satisfies a Weyl law as the imaginary part tend to\ninfinity. Some of these results were announced in arXiv:1301.5525.\n","versions":"[{'version': 'v1', 'created': 'Mon, 22 Feb 2021 17:19:33 GMT'}, {'version': 'v2', 'created': 'Tue, 15 Feb 2022 19:23:19 GMT'}, {'version': 'v3', 'created': 'Sun, 24 Sep 2023 11:07:11 GMT'}, {'version': 'v4', 'created': 'Sun, 9 Mar 2025 16:26:57 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Faure', 'Fr\u00e9d\u00e9ric', ''], ['Tsujii', 'Masato', '']]","extracted_entities":"[{'text': 'quantization', 'label': 'quantisation'}, {'text': 'contact Anosov\\nflow', 'label': 'Mistral'}, {'text': 'Ruelle spectrum', 'label': 'Mistral'}, {'text': 'Ruelle\\nspectrum', 'label': 'Mistral'}, {'text': 'Weyl law', 'label': 'Scaling law'}]","assigned_concept":"quantisation","matched_keyword":"quantization","similarity_score":0.8134455085}
{"id":2304.07839,"submitter":"Akaki Tikaradze","authors":"Akaki Tikaradze","title":"Rigidity of quantum algebras","comments":"Final version, to appear in the Journal of the London Mathematical\n  Society, 27 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.QA math.RT","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Given an associative $\\mathbb{C}$-algebra $A$, we call $A$ strongly rigid if\nfor any pair of finite subgroups of its automorphism groups $G, H,$ such that\n$A^G\\cong A^H$, then $G$ and $H$ must be isomorphic. In this paper we show that\na large class of filtered quantizations are strongly rigid. We also prove\nseveral other rigidity type results for various quantum algebras. For example,\nwe show that given two non-isomorphic complex semi-simple Lie algebras\n$\\mathfrak{g}_1, \\mathfrak{g}_2$ of equal dimension, there are no injective\n$\\mathbb{C}$-algebra homomorphisms between their enveloping algebras. We also\nshow that any finite subgroup of automorphisms of a central reduction of a\nfinite $W$-algebra $W_{\\chi}(\\mathfrak{g}, e)$ must be isomorphic to a subgroup\nof $Aut(\\mathfrak{g}(e)).$ We solve the inverse Galois problem for a wide class\nof rational Cherednik algebras that includes all (simple) classical generalized\nWeyl algebras, and also for quantum tori. Finally, we show that the Picard\ngroup of an $n$-dimensional quantum torus $A_q$ (with $q$ not a root of unity)\nis isomorphic to the group of outer automorphisms of $A_q.$\n","versions":"[{'version': 'v1', 'created': 'Sun, 16 Apr 2023 17:27:52 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 21:44:26 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Tikaradze', 'Akaki', '']]","extracted_entities":"[{'text': 'filtered quantizations', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"filtered quantizations","similarity_score":0.6282494664}
{"id":2308.14815,"submitter":"Souradeep Dutta","authors":"Souradeep Dutta, Michele Caprio, Vivian Lin, Matthew Cleaveland, Kuk\n  Jin Jang, Ivan Ruchkin, Oleg Sokolsky, Insup Lee","title":"Distributionally Robust Statistical Verification with Imprecise Neural\n  Networks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.LG cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  A particularly challenging problem in AI safety is providing guarantees on\nthe behavior of high-dimensional autonomous systems. Verification approaches\ncentered around reachability analysis fail to scale, and purely statistical\napproaches are constrained by the distributional assumptions about the sampling\nprocess. Instead, we pose a distributionally robust version of the statistical\nverification problem for black-box systems, where our performance guarantees\nhold over a large family of distributions. This paper proposes a novel approach\nbased on uncertainty quantification using concepts from imprecise\nprobabilities. A central piece of our approach is an ensemble technique called\nImprecise Neural Networks, which provides the uncertainty quantification.\nAdditionally, we solve the allied problem of exploring the input set using\nactive learning. The active learning uses an exhaustive neural-network\nverification tool Sherlock to collect samples. An evaluation on multiple\nphysical simulators in the openAI gym Mujoco environments with\nreinforcement-learned controllers demonstrates that our approach can provide\nuseful and scalable guarantees for high-dimensional systems.\n","versions":"[{'version': 'v1', 'created': 'Mon, 28 Aug 2023 18:06:24 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Aug 2023 16:31:53 GMT'}, {'version': 'v3', 'created': 'Mon, 11 Dec 2023 23:57:50 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Mar 2025 04:10:08 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Dutta', 'Souradeep', ''], ['Caprio', 'Michele', ''], ['Lin', 'Vivian', ''], ['Cleaveland', 'Matthew', ''], ['Jang', 'Kuk Jin', ''], ['Ruchkin', 'Ivan', ''], ['Sokolsky', 'Oleg', ''], ['Lee', 'Insup', '']]","extracted_entities":"[{'text': 'uncertainty quantification', 'label': 'quantisation'}, {'text': 'uncertainty quantification', 'label': 'quantisation'}, {'text': 'active learning', 'label': 'Few-shot Learning'}, {'text': 'active learning', 'label': 'Few-shot Learning'}]","assigned_concept":"quantisation","matched_keyword":"uncertainty quantification","similarity_score":0.5714546442}
{"id":2403.08859,"submitter":"Lewis Anderson","authors":"Lewis W. Anderson, Martin Kiffner, Tom O'Leary, Jason Crain, Dieter\n  Jaksch","title":"Solving lattice gauge theories using the quantum Krylov algorithm and\n  qubitization","comments":"22+22 pages, 7+9 figures, 0+5 tables. Published version","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph hep-lat hep-th","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Computing vacuum states of lattice gauge theories (LGTs) containing fermionic\ndegrees of freedom can present significant challenges for classical computation\nusing Monte-Carlo methods. Quantum algorithms may offer a pathway towards more\nscalable computation of groundstate properties of LGTs. However, a\ncomprehensive understanding of the quantum computational resources required for\nsuch a problem is thus far lacking. In this work, we investigate using the\nquantum subspace expansion (QSE) algorithm to compute the groundstate of the\nSchwinger model, an archetypal LGT describing quantum electrodynamics in one\nspatial dimension. We perform numerical simulations, including the effect of\nmeasurement noise, to extrapolate the resources required for the QSE algorithm\nto achieve a desired accuracy for a range of system sizes. Using this, we\npresent a full analysis of the resources required to compute LGT vacuum states\nusing a quantum algorithm using qubitization within a fault tolerant framework.\nWe develop of a novel method for performing qubitization of a LGT Hamiltonian\nbased on a 'linear combination of unitaries' (LCU) approach. The cost of the\ncorresponding block encoding operation scales as $\\tilde{O}(N)$ with system\nsize $N$. Including the corresponding prefactors, our method reduces the gate\ncost by multiple orders of magnitude when compared to previous LCU methods for\nthe QSE algorithm, which scales as $\\tilde{O}(N^2)$ when applied to the\nSchwinger model. While the qubit and single circuit T-gate cost resulting from\nour resource analysis is appealing to early fault-tolerant implementation, we\nfind that the number of shots required to avoid numerical instability within\nthe QSE procedure must be significantly reduced in order to improve the\nfeasibility of the methodology we consider and discuss how this might be\nachieved.\n","versions":"[{'version': 'v1', 'created': 'Wed, 13 Mar 2024 18:00:01 GMT'}, {'version': 'v2', 'created': 'Mon, 25 Mar 2024 14:09:49 GMT'}, {'version': 'v3', 'created': 'Thu, 9 May 2024 16:51:56 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Mar 2025 11:27:33 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Anderson', 'Lewis W.', ''], ['Kiffner', 'Martin', ''], [\"O'Leary\", 'Tom', ''], ['Crain', 'Jason', ''], ['Jaksch', 'Dieter', '']]","extracted_entities":"[{'text': 'Schwinger model', 'label': 'AI model'}, {'text': 'qubitization', 'label': 'quantisation'}, {'text': 'qubitization', 'label': 'quantisation'}, {'text': 'Schwinger model', 'label': 'AI model'}]","assigned_concept":"quantisation","matched_keyword":"qubitization","similarity_score":0.508687675}
{"id":2404.11788,"submitter":"Rachid Karami","authors":"Rachid Karami, Sheng-Chun Kao, Hyoukjun Kwon","title":"NonGEMM Bench: Understanding the Performance Horizon of the Latest ML\n  Workloads with NonGEMM Workloads","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AR cs.LG cs.PF","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Among ML operators today, GEneralMatrix Multiplication (GEMM)-based operators\nare known to be key operators that build the main backbone of ML models. As\ntheir computational overhead dominates the overall execution time (e.g., 42.8%\n- 96.6% in our results), GEMM operators have been the prime optimization\ntargets for fast ML inference. This led to advanced GPUs and accelerators\navailable today, which provided significant boost in the GEMM performance\ncompared to CPUs, aligned with the lesson from Amdahl's law. However,\naccelerating GEMM has significantly shifted the Amdahl's law's landscape for ML\ninference; due to the decreased GEMM execution time, the relative execution\ntime of non-GEMM operators is not dominant. Although the importance of non-GEMM\nperformance is increasing, we have little knowledge about the non-GEMM\nperformance horizon in the latest hardware platforms and models. Therefore, to\nguide non-GEMM-oriented optimizations, we conduct a thorough performance\nanalysis of 16 widely adopted ML models in Hugging Face and Torchvision on\nworkstation and data center platforms with\/without GPUs. We discover that\nnon-GEMM performance bottleneck is a considerable issue across all the\nplatforms and models, accounting for 11.3% to 73.6% of total latency, on\naverage. The challenge significantly aggravates when we apply quantization,\nwhich is a common model compression technique, due to the boosted GEMM\nperformance and extra non-GEMM operators for dequantization and requantization.\nTo provide insights into non-GEMM optimization targets, we demystify the most\ndominant non-GEMM operators for each model and deployment software.We also show\nthat widely adopted optimizations such as operator fusion do not completely\naddress the non-GEMM performance bottleneck, where non-GEMM operators still\naccount for 15% to 48% of total latency.\n","versions":"[{'version': 'v1', 'created': 'Wed, 17 Apr 2024 22:44:22 GMT'}, {'version': 'v2', 'created': 'Wed, 24 Apr 2024 17:58:45 GMT'}, {'version': 'v3', 'created': 'Fri, 22 Nov 2024 01:54:26 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Mar 2025 04:51:22 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Karami', 'Rachid', ''], ['Kao', 'Sheng-Chun', ''], ['Kwon', 'Hyoukjun', '']]","extracted_entities":"[{'text': \"Amdahl's law\", 'label': 'Scaling law'}, {'text': \"Amdahl's law\", 'label': 'Scaling law'}, {'text': 'Hugging Face', 'label': 'Open-source LLMs'}, {'text': 'quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization","similarity_score":0.8134455085}
{"id":2404.19248,"submitter":"Junghyup Lee","authors":"Junghyup Lee, Jeimin Jeon, Dohyung Kim, Bumsub Ham","title":"Scheduling Weight Transitions for Quantization-Aware Training","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Quantization-aware training (QAT) simulates a quantization process during\ntraining to lower bit-precision of weights\/activations. It learns quantized\nweights indirectly by updating latent weights,i.e., full-precision inputs to a\nquantizer, using gradient-based optimizers. We claim that coupling a\nuser-defined learning rate (LR) with these optimizers is sub-optimal for QAT.\nQuantized weights transit discrete levels of a quantizer, only if corresponding\nlatent weights pass transition points, where the quantizer changes discrete\nstates. This suggests that the changes of quantized weights are affected by\nboth the LR for latent weights and their distributions. It is thus difficult to\ncontrol the degree of changes for quantized weights by scheduling the LR\nmanually. We conjecture that the degree of parameter changes in QAT is related\nto the number of quantized weights transiting discrete levels. Based on this,\nwe introduce a transition rate (TR) scheduling technique that controls the\nnumber of transitions of quantized weights explicitly. Instead of scheduling a\nLR for latent weights, we schedule a target TR of quantized weights, and update\nthe latent weights with a novel transition-adaptive LR (TALR), enabling\nconsidering the degree of changes for the quantized weights during QAT.\nExperimental results demonstrate the effectiveness of our approach on standard\nbenchmarks.\n","versions":"[{'version': 'v1', 'created': 'Tue, 30 Apr 2024 04:12:36 GMT'}, {'version': 'v2', 'created': 'Fri, 6 Dec 2024 12:02:37 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 02:29:37 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Lee', 'Junghyup', ''], ['Jeon', 'Jeimin', ''], ['Kim', 'Dohyung', ''], ['Ham', 'Bumsub', '']]","extracted_entities":"[{'text': 'Quantization-aware training', 'label': 'Zero-shot Learning'}, {'text': 'quantization', 'label': 'quantisation'}, {'text': 'Quantized weights', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization","similarity_score":0.8134455085}
{"id":2407.19547,"submitter":"Yushi Huang","authors":"Yushi Huang, Ruihao Gong, Xianglong Liu, Jing Liu, Yuhang Li, Jiwen\n  Lu, Dacheng Tao","title":"Temporal Feature Matters: A Framework for Diffusion Model Quantization","comments":"arXiv admin note: substantial text overlap with arXiv:2311.16503","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.\n","versions":"[{'version': 'v1', 'created': 'Sun, 28 Jul 2024 17:46:15 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Aug 2024 20:43:10 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 17:43:28 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Huang', 'Yushi', ''], ['Gong', 'Ruihao', ''], ['Liu', 'Xianglong', ''], ['Liu', 'Jing', ''], ['Li', 'Yuhang', ''], ['Lu', 'Jiwen', ''], ['Tao', 'Dacheng', '']]","extracted_entities":"[{'text': 'Efficient Post-Training Quantization', 'label': 'quantisation'}, {'text': 'fine-grained selection', 'label': 'Fine-tuning'}]","assigned_concept":"quantisation","matched_keyword":"Efficient Post-Training Quantization","similarity_score":0.5989352465}
{"id":2408.03145,"submitter":"Aleksei Ivanov","authors":"Timothy N. Georges, Marius Bothe, Christoph S\\\"underhauf, Bjorn K.\n  Berntson, R\\'obert Izs\\'ak, Aleksei V. Ivanov","title":"Quantum Simulations of Chemistry in First Quantization with any Basis\n  Set","comments":"Added more detailed comparison with previous pw algo","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph physics.chem-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Quantum computation of the energy of molecules and materials is one of the\nmost promising applications of fault-tolerant quantum computers. Practical\napplications require development of quantum algorithms with reduced resource\nrequirements. Previous work has mainly focused on quantum algorithms where the\nHamiltonian is represented in second quantization with compact basis sets while\nexisting methods in first quantization are limited to a grid-based basis. In\nthis work, we present a new method to solve the generic ground-state chemistry\nproblem in first quantization using any basis set. We achieve asymptotic\nspeedup in Toffoli count for molecular orbitals, and orders of magnitude\nimprovement using dual plane waves as compared to the second quantization\ncounterparts. In some instances, our approach provides similar or even lower\nresources compared to previous first quantization plane wave algorithms that,\nunlike our approach, avoids the loading of the classical data. The developed\nmethodology can be applied to variety of applications, where the matrix\nelements of a first quantized Hamiltonian lack simple circuit representation.\n","versions":"[{'version': 'v1', 'created': 'Tue, 6 Aug 2024 12:40:32 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Aug 2024 15:58:52 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 22:06:09 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Georges', 'Timothy N.', ''], ['Bothe', 'Marius', ''], ['S\u00fcnderhauf', 'Christoph', ''], ['Berntson', 'Bjorn K.', ''], ['Izs\u00e1k', 'R\u00f3bert', ''], ['Ivanov', 'Aleksei V.', '']]","extracted_entities":"[{'text': 'second quantization', 'label': 'quantisation'}, {'text': 'first quantization', 'label': 'quantisation'}, {'text': 'first quantization', 'label': 'quantisation'}, {'text': 'second quantization', 'label': 'quantisation'}, {'text': 'first quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"first quantization","similarity_score":0.7858427763}
{"id":2409.02066,"submitter":"Anton Kozyriev","authors":"Anton Kozyriev, Vladimir Norkin","title":"Robust Clustering on High-Dimensional Data with Stochastic Quantization","comments":"22 pages, 5 figures, published in the International Scientific\n  Technical Journal \"Problems of Control and Informatics\"","journal-ref":"International Scientific Technical Journal \"Problems of Control\n  and Informatics\" 70 (2025) 32-48","doi":"10.34229\/1028-0979-2025-1-3","report-no":null,"categories":"cs.LG math.OC","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  This paper addresses the limitations of conventional vector quantization\nalgorithms, particularly K-Means and its variant K-Means++, and investigates\nthe Stochastic Quantization (SQ) algorithm as a scalable alternative for\nhigh-dimensional unsupervised and semi-supervised learning tasks. Traditional\nclustering algorithms often suffer from inefficient memory utilization during\ncomputation, necessitating the loading of all data samples into memory, which\nbecomes impractical for large-scale datasets. While variants such as Mini-Batch\nK-Means partially mitigate this issue by reducing memory usage, they lack\nrobust theoretical convergence guarantees due to the non-convex nature of\nclustering problems. In contrast, the Stochastic Quantization algorithm\nprovides strong theoretical convergence guarantees, making it a robust\nalternative for clustering tasks. We demonstrate the computational efficiency\nand rapid convergence of the algorithm on an image classification problem with\npartially labeled data, comparing model accuracy across various ratios of\nlabeled to unlabeled data. To address the challenge of high dimensionality, we\nemploy a Triplet Network to encode images into low-dimensional representations\nin a latent space, which serve as a basis for comparing the efficiency of both\nthe Stochastic Quantization algorithm and traditional quantization algorithms.\nFurthermore, we enhance the algorithm's convergence speed by introducing\nmodifications with an adaptive learning rate.\n","versions":"[{'version': 'v1', 'created': 'Tue, 3 Sep 2024 17:13:55 GMT'}, {'version': 'v2', 'created': 'Thu, 5 Sep 2024 15:35:53 GMT'}, {'version': 'v3', 'created': 'Fri, 11 Oct 2024 14:21:22 GMT'}, {'version': 'v4', 'created': 'Tue, 12 Nov 2024 09:50:15 GMT'}, {'version': 'v5', 'created': 'Sun, 9 Mar 2025 16:53:00 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Kozyriev', 'Anton', ''], ['Norkin', 'Vladimir', '']]","extracted_entities":"[{'text': 'K-Means', 'label': 'quantisation'}, {'text': 'Stochastic Quantization', 'label': 'quantisation'}, {'text': 'Stochastic Quantization algorithm', 'label': 'quantisation'}, {'text': 'Stochastic Quantization algorithm', 'label': 'quantisation'}, {'text': 'traditional quantization algorithms', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"traditional quantization algorithms","similarity_score":0.5870589614}
{"id":2410.02665,"submitter":"Mahathi Vempati","authors":"Joseph Carolan, Amin Shiraz Gilani, Mahathi Vempati","title":"Quantum advantage and lower bounds in parallel query complexity","comments":"Changed pdf title","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  It is well known that quantum, randomized and deterministic (sequential)\nquery complexities are polynomially related for total boolean functions. We\nfind that significantly larger separations between the parallel generalizations\nof these measures are possible. In particular,\n  (1) We employ the cheatsheet framework to obtain an unbounded parallel\nquantum query advantage over its randomized analogue for a total function,\nfalsifying a conjecture of Jeffery et al. 2017 (arXiv:1309.6116).\n  (2) We strengthen (1) by constructing a total function which exhibits an\nunbounded parallel quantum query advantage despite having no sequential\nadvantage, suggesting that genuine quantum advantage could occur entirely due\nto parallelism.\n  (3) We construct a total function that exhibits a polynomial separation\nbetween 2-round quantum and randomized query complexities, contrasting a result\nof Montanaro in 2010 (arXiv:1001.0018) that there is at most a constant\nseparation for 1-round (nonadaptive) algorithms.\n  (4) We develop a new technique for deriving parallel quantum lower bounds\nfrom sequential upper bounds. We employ this technique to give lower bounds for\nBoolean symmetric functions and read-once formulas, ruling out large parallel\nquery advantages for them.\n  We also provide separations between randomized and deterministic parallel\nquery complexities analogous to items (1)-(3).\n","versions":"[{'version': 'v1', 'created': 'Thu, 3 Oct 2024 16:50:00 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 16:08:24 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Carolan', 'Joseph', ''], ['Gilani', 'Amin Shiraz', ''], ['Vempati', 'Mahathi', '']]","extracted_entities":"[{'text': 'quantum', 'label': 'quantisation'}, {'text': 'quantum', 'label': 'quantisation'}, {'text': 'quantum', 'label': 'quantisation'}, {'text': 'quantum', 'label': 'quantisation'}, {'text': 'quantum', 'label': 'quantisation'}, {'text': 'quantum', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantum","similarity_score":0.5509136915}
{"id":2410.15721,"submitter":"Raphael Carpintero Perez","authors":"Rapha\\\"el Carpintero Perez (CMAP), S\\'ebastien da Veiga (ENSAI,\n  CREST), Josselin Garnier (CMAP, ASCII), Brian Staber","title":"Learning signals defined on graphs with optimal transport and Gaussian\n  process regression","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In computational physics, machine learning has now emerged as a powerful\ncomplementary tool to explore efficiently candidate designs in engineering\nstudies. Outputs in such supervised problems are signals defined on meshes, and\na natural question is the extension of general scalar output regression models\nto such complex outputs. Changes between input geometries in terms of both size\nand adjacency structure in particular make this transition non-trivial. In this\nwork, we propose an innovative strategy for Gaussian process regression where\ninputs are large and sparse graphs with continuous node attributes and outputs\nare signals defined on the nodes of the associated inputs. The methodology\nrelies on the combination of regularized optimal transport, dimension reduction\ntechniques, and the use of Gaussian processes indexed by graphs. In addition to\nenabling signal prediction, the main point of our proposal is to come with\nconfidence intervals on node values, which is crucial for uncertainty\nquantification and active learning. Numerical experiments highlight the\nefficiency of the method to solve real problems in fluid dynamics and solid\nmechanics.\n","versions":"[{'version': 'v1', 'created': 'Mon, 21 Oct 2024 07:39:44 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 10:11:26 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Perez', 'Rapha\u00ebl Carpintero', '', 'CMAP'], ['da Veiga', 'S\u00e9bastien', '', 'ENSAI,\\n  CREST'], ['Garnier', 'Josselin', '', 'CMAP, ASCII'], ['Staber', 'Brian', '']]","extracted_entities":"[{'text': 'uncertainty\\nquantification', 'label': 'quantisation'}, {'text': 'active learning', 'label': 'Few-shot Learning'}]","assigned_concept":"quantisation","matched_keyword":"uncertainty\nquantification","similarity_score":0.5714546442}
{"id":2410.17786,"submitter":"Elisabeth Richter","authors":"Elisabeth Richter, Michael Barth, Dmitriy A. Kozlov, Angelika Knothe,\n  Nikolay N. Mikhailov, Juliane Steidl, Cosimo Gorini, Stefan Hartl, Wolfgang\n  Himmler, Klaus Richter, Dieter Weiss","title":"Anomalous conductance steps in three-dimensional topological insulator\n  HgTe-based quantum point contacts","comments":"Submitted to PRR on 10\/21\/2024, re-submitted with minor changes to\n  PRR on 01\/14\/2025. Accepted on 02\/04\/2025 and published in PRR on 03\/11\/2025.\n  8 pages, 7 figures","journal-ref":"Phys. Rev. Research 7, 013260 (2025)","doi":"10.1103\/PhysRevResearch.7.013260","report-no":null,"categories":"cond-mat.mes-hall","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We explore electrical transport through a point contact in strained HgTe, a\nthree-dimensional topological insulator. In the absence of a magnetic field\n$B$, there is no quantization. However, under higher magnetic fields, we\nobserve distinct non-integer conductance steps. Based on numerical\ntight-binding calculations and a phenomenological Landauer-B\\\"uttiker approach,\nwe attribute these atypical, non-integer quantized plateaus to significant\nscattering effects at the point contact.\n","versions":"[{'version': 'v1', 'created': 'Wed, 23 Oct 2024 11:36:59 GMT'}, {'version': 'v2', 'created': 'Thu, 24 Oct 2024 12:17:30 GMT'}, {'version': 'v3', 'created': 'Tue, 14 Jan 2025 16:30:03 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Mar 2025 20:26:20 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Richter', 'Elisabeth', ''], ['Barth', 'Michael', ''], ['Kozlov', 'Dmitriy A.', ''], ['Knothe', 'Angelika', ''], ['Mikhailov', 'Nikolay N.', ''], ['Steidl', 'Juliane', ''], ['Gorini', 'Cosimo', ''], ['Hartl', 'Stefan', ''], ['Himmler', 'Wolfgang', ''], ['Richter', 'Klaus', ''], ['Weiss', 'Dieter', '']]","extracted_entities":"[{'text': 'quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization","similarity_score":0.8134455085}
{"id":2411.01681,"submitter":"Nezhla Aghaei","authors":"Nezhla Aghaei and M. K. Pawelkiewicz","title":"Graded discrete Heisenberg and Drinfeld doubles","comments":"41 pages. arXiv admin note: text overlap with arXiv:1909.04565","journal-ref":null,"doi":null,"report-no":null,"categories":"math.QA hep-th math-ph math.MP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We study the Heisenberg double and the Drinfeld double of the Z2-graded Hopf\nalgebras. To present the constructions, we consider in detail the Borel half of\nUq(sl(2)) and two super Hopf algebra examples: the Borel half of Uq(osp(1|2))\nand the Borel half of Uq(gl(1|1)) for q being a root of unity. We prove the\nisomorphism between the Heisenberg doubles and the handle algebras, which is\nmissing in the literature, and extend the isomorphism to the graded Heisenberg\ndoubles and the handle algebras in the context of the Z2-graded generalisation\nof Alekseev-Schomerus combinatorial quantisation of Chern-Simons theory [1, 2],\nas well as illustrate it on the example of the Heisenberg double of the full\nUq(gl(1|1)) Hopf algebra. In addition, we generalise an isomorphism between the\nDrinfeld double and the loop algebra from the combinatorial quantisation to the\ngraded setting.\n","versions":"[{'version': 'v1', 'created': 'Sun, 3 Nov 2024 20:45:09 GMT'}, {'version': 'v2', 'created': 'Sun, 12 Jan 2025 17:27:39 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 20:33:58 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Aghaei', 'Nezhla', ''], ['Pawelkiewicz', 'M. K.', '']]","extracted_entities":"[{'text': 'Alekseev-Schomerus combinatorial quantisation', 'label': 'quantisation'}, {'text': 'combinatorial quantisation', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"combinatorial quantisation","similarity_score":0.7041225433}
{"id":2411.1637,"submitter":"Amaan Valiuddin","authors":"M.M.A. Valiuddin, R.J.G. van Sloun, C.G.A. Viviers, P.H.N. de With, F.\n  van der Sommen","title":"A Review of Bayesian Uncertainty Quantification in Deep Probabilistic\n  Image Segmentation","comments":"20 pages, revised","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG eess.IV stat.ML","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Advancements in image segmentation play an integral role within the broad\nscope of Deep Learning-based Computer Vision. Furthermore, their widespread\napplicability in critical real-world tasks has resulted in challenges related\nto the reliability of such algorithms. Hence, uncertainty quantification has\nbeen extensively studied within this context, enabling the expression of model\nignorance (epistemic uncertainty) or data ambiguity (aleatoric uncertainty) to\nprevent uninformed decision-making. Due to the rapid adoption of Convolutional\nNeural Network (CNN)-based segmentation models in high-stake applications, a\nsubstantial body of research has been published on this very topic, causing its\nswift expansion into a distinct field. This work provides a comprehensive\noverview of probabilistic segmentation, by discussing fundamental concepts of\nuncertainty quantification, governing advancements in the field as well as the\napplication to various tasks. Moreover, literature on both types of\nuncertainties trace back to four key applications: (1) to quantify statistical\ninconsistencies in the annotation process due ambiguous images, (2) correlating\nprediction error with uncertainty, (3) expanding the model hypothesis space for\nbetter generalization, and (4) Active Learning. An extensive discussion follows\nthat includes an overview of utilized datasets for each of the applications and\nevaluation of the available methods. We also highlight challenges related to\narchitectures, uncertainty quantification methods, standardization and\nbenchmarking, and finally end with recommendations for future work such as\nmethods based on single forward passes and models that appropriately leverage\nvolumetric data.\n","versions":"[{'version': 'v1', 'created': 'Mon, 25 Nov 2024 13:26:09 GMT'}, {'version': 'v2', 'created': 'Tue, 7 Jan 2025 09:34:51 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 09:51:17 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Valiuddin', 'M. M. A.', ''], ['van Sloun', 'R. J. G.', ''], ['Viviers', 'C. G. A.', ''], ['de With', 'P. H. N.', ''], ['van der Sommen', 'F.', '']]","extracted_entities":"[{'text': 'uncertainty quantification', 'label': 'quantisation'}, {'text': 'uncertainty quantification', 'label': 'quantisation'}, {'text': 'Active Learning', 'label': 'Few-shot Learning'}]","assigned_concept":"quantisation","matched_keyword":"uncertainty quantification","similarity_score":0.5714546442}
{"id":2412.10319,"submitter":"Huiqiang Jiang","authors":"Yucheng Li, Huiqiang Jiang, Qianhui Wu, Xufang Luo, Surin Ahn,\n  Chengruidong Zhang, Amir H. Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang,\n  Lili Qiu","title":"SCBench: A KV Cache-Centric Analysis of Long-Context Methods","comments":"Accepted at ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https:\/\/aka.ms\/SCBench.\n","versions":"[{'version': 'v1', 'created': 'Fri, 13 Dec 2024 17:59:52 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 14:02:04 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Li', 'Yucheng', ''], ['Jiang', 'Huiqiang', ''], ['Wu', 'Qianhui', ''], ['Luo', 'Xufang', ''], ['Ahn', 'Surin', ''], ['Zhang', 'Chengruidong', ''], ['Abdi', 'Amir H.', ''], ['Li', 'Dongsheng', ''], ['Gao', 'Jianfeng', ''], ['Yang', 'Yuqing', ''], ['Qiu', 'Lili', '']]","extracted_entities":"[{'text': 'Long-context LLMs', 'label': 'LLM-based'}, {'text': 'vLLM', 'label': 'Open-source LLMs'}, {'text': 'SGLang', 'label': 'Open-source LLMs'}, {'text': 'OpenAI', 'label': 'Open-source LLMs'}, {'text': 'Anthropic', 'label': 'Open-source LLMs'}, {'text': 'sparse attention', 'label': 'Attention mechanism'}, {'text': 'quantization', 'label': 'quantisation'}, {'text': 'prompt compression', 'label': 'Prompting'}, {'text': 'long-context LLMs', 'label': 'LLM-based'}]","assigned_concept":"quantisation","matched_keyword":"quantization","similarity_score":0.8134455085}
{"id":2412.10663,"submitter":"Jingyang Li","authors":"Jingyang Li, Kuangyu Ding, Kim-Chuan Toh, Pan Zhou","title":"Memory-Efficient 4-bit Preconditioned Stochastic Optimization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV math.OC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Preconditioned stochastic optimization algorithms, exemplified by Shampoo,\noutperform first-order optimizers by offering theoretical convergence benefits\nand practical gains in large-scale neural network training. However, they incur\nsubstantial memory overhead due to the storage demands of non-diagonal\npreconditioning matrices. To address this, we introduce 4-bit quantization for\nShampoo's preconditioners. We introduce two key methods: First, we apply\nCholesky decomposition followed by quantization of the Cholesky factors,\nreducing memory usage by leveraging their lower triangular structure while\nbetter preserving spectral properties to minimize information loss. To our\nknowledge, this is the first quantization approach applied to Cholesky factors\nof preconditioners. Second, we incorporate error feedback in the quantization\nprocess, efficiently storing Cholesky factor and error state in the lower and\nupper triangular parts of the same matrix. Through extensive experiments, we\ndemonstrate that combining Cholesky quantization with error feedback enhances\nmemory efficiency and algorithm performance in large-scale deep-learning tasks.\nTheoretically, we also provide convergence proofs for quantized Shampoo under\nboth smooth and non-smooth stochastic optimization settings.\n","versions":"[{'version': 'v1', 'created': 'Sat, 14 Dec 2024 03:32:54 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 09:19:31 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Li', 'Jingyang', ''], ['Ding', 'Kuangyu', ''], ['Toh', 'Kim-Chuan', ''], ['Zhou', 'Pan', '']]","extracted_entities":"[{'text': '4-bit quantization', 'label': 'quantisation'}, {'text': 'quantization', 'label': 'quantisation'}, {'text': 'Cholesky quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization","similarity_score":0.8134455085}
{"id":2501.16622,"submitter":"Arkajit Mandal","authors":"Logan Blackham and Arshath Manjalingal and Saeed R. Koshkaki and\n  Arkajit Mandal","title":"Microscopic Theory of Polaron-Polariton Dispersion and Propagation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mes-hall cond-mat.mtrl-sci physics.chem-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We develop an analytical microscopic theory to describe the polaron-polariton\ndispersion, formed by hybridizing excitons, photons, and phonons, and their\ncoherent dynamics inside optical cavities. Starting from a microscopic\nlight-matter Hamiltonian, we derive a simple analytical model by pursuing a\nnon-perturbative treatment of the phonon and photon couplings to excitons.\nWithin our theoretical framework, the phonons are treated as classical fields\nthat are then quantized via the Floquet formalism. We show that, to a good\napproximation, the entire polaron-polariton system can be described using a\nband picture despite the phonons breaking translational symmetry. Our theory\nalso sheds light on the long-lived coherent ballistic motion of\nexciton-polaritons with high excitonic character that propagate with group\nvelocities lower than is expected from pure exciton-polariton bands, offering a\nmicroscopic explanation for these puzzling experimental observations.\n","versions":"[{'version': 'v1', 'created': 'Tue, 28 Jan 2025 01:47:57 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 16:01:01 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Blackham', 'Logan', ''], ['Manjalingal', 'Arshath', ''], ['Koshkaki', 'Saeed R.', ''], ['Mandal', 'Arkajit', '']]","extracted_entities":"[{'text': 'quantized', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantized","similarity_score":0.7305599451}
{"id":2502.07842,"submitter":"Jiyoon Kim","authors":"Jiyoon Kim, Kang Eun Jeon, Yulhwa Kim, and Jong Hwan Ko","title":"Column-wise Quantization of Weights and Partial Sums for Accurate and\n  Efficient Compute-In-Memory Accelerators","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AR cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Compute-in-memory (CIM) is an efficient method for implementing deep neural\nnetworks (DNNs) but suffers from substantial overhead from analog-to-digital\nconverters (ADCs), especially as ADC precision increases. Low-precision ADCs\ncan reduce this overhead but introduce partial-sum quantization errors\ndegrading accuracy. Additionally, low-bit weight constraints, imposed by cell\nlimitations and the need for multiple cells for higher-bit weights, present\nfurther challenges. While fine-grained partial-sum quantization has been\nstudied to lower ADC resolution effectively, weight granularity, which limits\noverall partial-sum quantized accuracy, remains underexplored. This work\naddresses these challenges by aligning weight and partial-sum quantization\ngranularities at the column-wise level. Our method improves accuracy while\nmaintaining dequantization overhead, simplifies training by removing two-stage\nprocesses, and ensures robustness to memory cell variations via independent\ncolumn-wise scale factors. We also propose an open-source CIM-oriented\nconvolution framework to handle fine-grained weights and partial-sums\nefficiently, incorporating a novel tiling method and group convolution.\nExperimental results on ResNet-20 (CIFAR-10, CIFAR-100) and ResNet-18\n(ImageNet) show accuracy improvements of 0.99%, 2.69%, and 1.01%, respectively,\ncompared to the best-performing related works. Additionally, variation analysis\nreveals the robustness of our method against memory cell variations. These\nfindings highlight the effectiveness of our quantization scheme in enhancing\naccuracy and robustness while maintaining hardware efficiency in CIM-based DNN\nimplementations. Our code is available at\nhttps:\/\/github.com\/jiyoonkm\/ColumnQuant.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Feb 2025 05:32:14 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 11:32:19 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Kim', 'Jiyoon', ''], ['Jeon', 'Kang Eun', ''], ['Kim', 'Yulhwa', ''], ['Ko', 'Jong Hwan', '']]","extracted_entities":"[{'text': 'partial-sum quantization', 'label': 'quantisation'}, {'text': 'partial-sum quantization', 'label': 'quantisation'}, {'text': 'partial-sum quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"partial-sum quantization","similarity_score":0.5884458423}
{"id":2502.19547,"submitter":"Zhengdi Sun","authors":"Seolhwa Kim, Per Kraus, Zhengdi Sun","title":"Codimension one defects in free scalar field theory","comments":"62 pages; v2: reference added","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-th","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We study various aspects of codimension one defects in free scalar field\ntheory, with particular emphasis on line defects in two-dimensions. These\ndefects are generically non-conformal, but include conformal and topological\ndefects as special cases. Our analysis is based on the interplay between two\ncomplementary descriptions, the first involving matching conditions imposed on\nfields and their derivatives across the defect, and the second on the\nresummation of perturbation theory in terms of renormalized defect couplings.\nUsing either description as appropriate we compute a variety of observables:\ncorrelators of fields in the presence of such defects; the defect anomalous\ndimension; multiple defects and their fusion; canonical quantization and\ninstabilities; ring shaped defects with application to the g-theorem and the\nentanglement entropy of accelerating defects; defects on the torus and Cardy\nformulas for the asymptotic density of states of the defect Hilbert space; and\nquenches produced by spacelike defects. The simplicity of the model allows for\nexplicit computation of all these quantities, and provides a starting point for\nmore complicated theories involving interactions.\n","versions":"[{'version': 'v1', 'created': 'Wed, 26 Feb 2025 20:40:31 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 20:45:43 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Kim', 'Seolhwa', ''], ['Kraus', 'Per', ''], ['Sun', 'Zhengdi', '']]","extracted_entities":"[{'text': 'canonical quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"canonical quantization","similarity_score":0.6431959867}
{"id":2503.01261,"submitter":"Guotao Liang","authors":"Guotao Liang, Baoquan Zhang, Zhiyuan Wen, Junteng Zhao, Yunming Ye,\n  Kola Ye, Yao He","title":"Towards Improved Text-Aligned Codebook Learning: Multi-Hierarchical\n  Codebook-Text Alignment with Long Text","comments":"Accepted by CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Image quantization is a crucial technique in image generation, aimed at\nlearning a codebook that encodes an image into a discrete token sequence.\nRecent advancements have seen researchers exploring learning multi-modal\ncodebook (i.e., text-aligned codebook) by utilizing image caption semantics,\naiming to enhance codebook performance in cross-modal tasks. However, existing\nimage-text paired datasets exhibit a notable flaw in that the text descriptions\ntend to be overly concise, failing to adequately describe the images and\nprovide sufficient semantic knowledge, resulting in limited alignment of text\nand codebook at a fine-grained level. In this paper, we propose a novel\nText-Augmented Codebook Learning framework, named TA-VQ, which generates longer\ntext for each image using the visual-language model for improved text-aligned\ncodebook learning. However, the long text presents two key challenges: how to\nencode text and how to align codebook and text. To tackle two challenges, we\npropose to split the long text into multiple granularities for encoding, i.e.,\nword, phrase, and sentence, so that the long text can be fully encoded without\nlosing any key semantic knowledge. Following this, a hierarchical encoder and\nnovel sampling-based alignment strategy are designed to achieve fine-grained\ncodebook-text alignment. Additionally, our method can be seamlessly integrated\ninto existing VQ models. Extensive experiments in reconstruction and various\ndownstream tasks demonstrate its effectiveness compared to previous\nstate-of-the-art approaches.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 07:38:18 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 06:09:18 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Liang', 'Guotao', ''], ['Zhang', 'Baoquan', ''], ['Wen', 'Zhiyuan', ''], ['Zhao', 'Junteng', ''], ['Ye', 'Yunming', ''], ['Ye', 'Kola', ''], ['He', 'Yao', '']]","extracted_entities":"[{'text': 'Image quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"Image quantization","similarity_score":0.620505929}
{"id":2503.02244,"submitter":"Yixuan Huang","authors":"Yixuan Huang, Jie Yang, Chao-Kai Wen, Shi Jin","title":"Integrated Communication and Learned Recognizer with Customized RIS\n  Phases and Sensing Durations","comments":"17 pages, 16 figures, 8 tables, accepted by IEEE Transactions on\n  Communications","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IT eess.SP math.IT","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Future wireless communication networks are expected to be smarter and more\naware of their surroundings, enabling a wide range of context-aware\napplications. Reconfigurable intelligent surfaces (RISs) are set to play a\ncritical role in supporting various sensing tasks, such as target recognition.\nHowever, current methods typically use RIS configurations optimized once and\napplied over fixed sensing durations, limiting their ability to adapt to\ndifferent targets and reducing sensing accuracy. To overcome these limitations,\nthis study proposes an advanced wireless communication system that multiplexes\ndownlink signals for environmental sensing and introduces an intelligent\nrecognizer powered by deep learning techniques. Specifically, we design a novel\nneural network based on the long short-term memory architecture and the\nphysical channel model. This network iteratively captures and fuses information\nfrom previous measurements, adaptively customizing RIS phases to gather the\nmost relevant information for the recognition task at subsequent moments. These\nconfigurations are dynamically adjusted according to scene, task, target, and\nquantization priors. Furthermore, the recognizer includes a decision-making\nmodule that dynamically allocates different sensing durations, determining\nwhether to continue or terminate the sensing process based on the collected\nmeasurements. This approach maximizes resource utilization efficiency.\nSimulation results demonstrate that the proposed method significantly\noutperforms state-of-the-art techniques while minimizing the impact on\ncommunication performance, even when sensing and communication occur\nsimultaneously. Part of the source code for this paper can be accessed at\nhttps:\/\/github.com\/kiwi1944\/CRISense.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 03:43:01 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 05:10:49 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Huang', 'Yixuan', ''], ['Yang', 'Jie', ''], ['Wen', 'Chao-Kai', ''], ['Jin', 'Shi', '']]","extracted_entities":"[{'text': 'deep learning techniques', 'label': 'Few-shot Learning'}, {'text': 'physical channel model', 'label': 'Foundation Model'}, {'text': 'quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization","similarity_score":0.8134455085}
{"id":2503.04832,"submitter":"Alaa Eddine Mazouz","authors":"Alaa Mazouz, Sumanta Chaudhuri, Marco Cagnanzzo, Mihai Mitrea, Enzo\n  Tartaglione, Attilio Fiandrotti","title":"RD Efficient FPGA Deployment of Learned Image Compression: Knowledge\n  Distillation and Hybrid Quantization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Learnable Image Compression (LIC) has shown the potential to outperform\nstandardized video codecs in RD efficiency, prompting the research for\nhardware-friendly implementations. Most existing LIC hardware implementations\nprioritize latency to RD-efficiency and through an extensive exploration of the\nhardware design space. We present a novel design paradigm where the burden of\ntuning the design for a specific hardware platform is shifted towards model\ndimensioning and without compromising on RD-efficiency. First, we design a\nframework for distilling a leaner student LIC model from a reference teacher:\nby tuning a single model hyperparameters, we can meet the constraints of\ndifferent hardware platforms without a complex hardware design exploration.\nSecond, we propose a hardware-friendly implementation of the Generalized\nDivisive Normalization (GDN) activation that preserves RD efficiency even post\nparameter quantization. Third, we design a pipelined FPGA configuration which\ntakes full advantage of available FPGA resources by leveraging parallel\nprocessing and optimizing resource allocation. Our experiments with a state of\nthe art LIC model show that we outperform all existing FPGA implementations\nwhile performing very close to the original model in terms of RD efficiency.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 10:59:32 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 08:47:03 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Mazouz', 'Alaa', ''], ['Chaudhuri', 'Sumanta', ''], ['Cagnanzzo', 'Marco', ''], ['Mitrea', 'Mihai', ''], ['Tartaglione', 'Enzo', ''], ['Fiandrotti', 'Attilio', '']]","extracted_entities":"[{'text': 'parameter quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"parameter quantization","similarity_score":0.6211687922}
{"id":2503.06384,"submitter":"Jasel Berra-Montiel","authors":"Jasel Berra-Montiel, Daniel Contreras-Bear, Alberto Molgado, Mar\n  Sanchez-Cordova","title":"Star exponentials and Wigner functions for time-dependent harmonic\n  oscillators","comments":"14 pages, no figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph math-ph math.MP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper, we address the Wigner distribution and the star exponential\nfunction for a time-dependent harmonic oscillator for which the mass and the\nfrequency terms are considered explicitly depending on time. To such an end, we\nexplore the connection between the star exponential, naturally emerging within\nthe context of deformation quantization, and the propagators constructed\nthrough the path integral formalism. In particular, the Fourier-Dirichlet\nexpansion of the star exponential implies a distinctive quantization of the\nLewis-Riesenfeld invariant. Further, by introducing a judicious time variable,\nwe recovered a time-dependent phase function associated with the\nLewis-Riesenfeld construction of the standard Schr\\\"odinger picture. In\nparticular, we applied our results to the cases of the Caldirola-Kanai and the\ntime-dependent frequency harmonic oscillators, recovering relevant results\npreviously reported in the literature.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 01:43:07 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Berra-Montiel', 'Jasel', ''], ['Contreras-Bear', 'Daniel', ''], ['Molgado', 'Alberto', ''], ['Sanchez-Cordova', 'Mar', '']]","extracted_entities":"[{'text': 'deformation quantization', 'label': 'quantisation'}, {'text': 'distinctive quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"distinctive quantization","similarity_score":0.706843853}
{"id":2503.06518,"submitter":"Feng Zhang","authors":"Feng Zhang, Yanbin Liu, Weihua Li, Jie Lv, Xiaodan Wang, Quan Bai","title":"Towards Superior Quantization Accuracy: A Layer-sensitive Approach","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Large Vision and Language Models have exhibited remarkable human-like\nintelligence in tasks such as natural language comprehension, problem-solving,\nlogical reasoning, and knowledge retrieval. However, training and serving these\nmodels require substantial computational resources, posing a significant\nbarrier to their widespread application and further research. To mitigate this\nchallenge, various model compression techniques have been developed to reduce\ncomputational requirements. Nevertheless, existing methods often employ uniform\nquantization configurations, failing to account for the varying difficulties\nacross different layers in quantizing large neural network models. This paper\ntackles this issue by leveraging layer-sensitivity features, such as activation\nsensitivity and weight distribution Kurtosis, to identify layers that are\nchallenging to quantize accurately and allocate additional memory budget. The\nproposed methods, named SensiBoost and KurtBoost, respectively, demonstrate\nnotable improvement in quantization accuracy, achieving up to 9% lower\nperplexity with only a 2% increase in memory budget on LLama models compared to\nthe baseline.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 08:45:03 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhang', 'Feng', ''], ['Liu', 'Yanbin', ''], ['Li', 'Weihua', ''], ['Lv', 'Jie', ''], ['Wang', 'Xiaodan', ''], ['Bai', 'Quan', '']]","extracted_entities":"[{'text': 'quantization accuracy', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization accuracy","similarity_score":0.6343355179}
{"id":2503.06545,"submitter":"Junyi Wu","authors":"Junyi Wu, Zhiteng Li, Zheng Hui, Yulun Zhang, Linghe Kong, Xiaokang\n  Yang","title":"QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation","comments":"The code and models will be available at\n  https:\/\/github.com\/JunyiWuCode\/QuantCache","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps:\/\/github.com\/JunyiWuCode\/QuantCache.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 10:31:51 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Wu', 'Junyi', ''], ['Li', 'Zhiteng', ''], ['Hui', 'Zheng', ''], ['Zhang', 'Yulun', ''], ['Kong', 'Linghe', ''], ['Yang', 'Xiaokang', '']]","extracted_entities":"[{'text': 'Diffusion Transformers', 'label': 'Transformers'}, {'text': 'DiTs', 'label': 'Transformers'}, {'text': 'U-Net-based models', 'label': 'Transformers'}, {'text': 'DiTs', 'label': 'Transformers'}, {'text': 'quantization', 'label': 'quantisation'}, {'text': 'adaptive importance-guided quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization","similarity_score":0.8134455085}
{"id":2503.06557,"submitter":"Olivier Klein","authors":"Thierry Valet, Kei Yamamoto, Benjamin Pigeau, Gr\\'egoire de Loubens,\n  and Olivier Klein","title":"Field Theory of Linear Spin-Waves in Finite Textured Ferromagnets","comments":"18 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mes-hall","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In the context of an ever-expanding experimental and theoretical interest in\nthe magnetization dynamics of mesoscopic magnetic structures, both in the\nclassical and quantum regimes, we formulate a low energy field theory for the\nlinear spin-waves in finite and textured ferromagnets and we perform its\nconstrained canonical quantization. The introduction of a manifestly gauge\ninvariant Lagrangian enables a straightforward application of the Noether's\ntheorem. Taking advantage of this in the context of a broad class of\naxisymmetric ferromagnets of special conceptual and experimental relevance, a\ngeneral expression of the conserved and quantized spin-wave total angular\nmomentum is rigorously derived, while separate conservation and quantization of\nits orbital and spin components are established for a more restricted class of\nuniaxial exchange ferromagnets. Further particularizing this general framework\nto the case of axially saturated magnetic thin disks, we develop a\nsemi-analytic theory of the low frequency part of the exchange-dipole azimuthal\nspin wave spectrum, providing a powerful theoretical platform for the analysis\nand interpretation of magnetic resonance experiments on magnetic microdots as\nfurther demonstrated in a joint paper [arxiv The Orbital Angular Momentum of\nAzimuthal Spin-Waves]\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 11:05:33 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 16:55:42 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Valet', 'Thierry', ''], ['Yamamoto', 'Kei', ''], ['Pigeau', 'Benjamin', ''], ['de Loubens', 'Gr\u00e9goire', ''], ['Klein', 'Olivier', '']]","extracted_entities":"[{'text': 'constrained canonical quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"constrained canonical quantization","similarity_score":0.5522534251}
{"id":2503.06564,"submitter":"Ziyang Yan","authors":"Yihua Shao, Deyang Lin, Fanhu Zeng, Minxi Yan, Muyang Zhang, Siyu\n  Chen, Yuxuan Fan, Ziyang Yan, Haozhe Wang, Jingcai Guo, Yan Wang, Haotong\n  Qin, Hao Tang","title":"TR-DQ: Time-Rotation Diffusion Quantization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Diffusion models have been widely adopted in image and video generation.\nHowever, their complex network architecture leads to high inference overhead\nfor its generation process. Existing diffusion quantization methods primarily\nfocus on the quantization of the model structure while ignoring the impact of\ntime-steps variation during sampling. At the same time, most current approaches\nfail to account for significant activations that cannot be eliminated,\nresulting in substantial performance degradation after quantization. To address\nthese issues, we propose Time-Rotation Diffusion Quantization (TR-DQ), a novel\nquantization method incorporating time-step and rotation-based optimization.\nTR-DQ first divides the sampling process based on time-steps and applies a\nrotation matrix to smooth activations and weights dynamically. For different\ntime-steps, a dedicated hyperparameter is introduced for adaptive timing\nmodeling, which enables dynamic quantization across different time steps.\nAdditionally, we also explore the compression potential of Classifier-Free\nGuidance (CFG-wise) to establish a foundation for subsequent work. TR-DQ\nachieves state-of-the-art (SOTA) performance on image generation and video\ngeneration tasks and a 1.38-1.89x speedup and 1.97-2.58x memory reduction in\ninference compared to existing quantization methods.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 11:37:11 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Shao', 'Yihua', ''], ['Lin', 'Deyang', ''], ['Zeng', 'Fanhu', ''], ['Yan', 'Minxi', ''], ['Zhang', 'Muyang', ''], ['Chen', 'Siyu', ''], ['Fan', 'Yuxuan', ''], ['Yan', 'Ziyang', ''], ['Wang', 'Haozhe', ''], ['Guo', 'Jingcai', ''], ['Wang', 'Yan', ''], ['Qin', 'Haotong', ''], ['Tang', 'Hao', '']]","extracted_entities":"[{'text': 'Time-Rotation Diffusion Quantization', 'label': 'quantisation'}, {'text': 'dynamic quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"dynamic quantization","similarity_score":0.6581095457}
{"id":2407.12863,"submitter":"June Yong Yang","authors":"Jung Hyun Lee, June Yong Yang, Byeongho Heo, Dongyoon Han, Kyungsu\n  Kim, Eunho Yang, Kang Min Yoo","title":"Token-Supervised Value Models for Enhancing Mathematical Problem-Solving\n  Capabilities of Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  With the rapid advancement of test-time compute search strategies to improve\nthe mathematical problem-solving capabilities of large language models (LLMs),\nthe need for building robust verifiers has become increasingly important.\nHowever, all these inference strategies rely on existing verifiers originally\ndesigned for Best-of-N search, which makes them sub-optimal for tree search\ntechniques at test time. During tree search, existing verifiers can only offer\nindirect and implicit assessments of partial solutions or under-value\nprospective intermediate steps, thus resulting in the premature pruning of\npromising intermediate steps. To overcome these limitations, we propose\ntoken-supervised value models (TVMs) - a new class of verifiers that assign\neach token a probability that reflects the likelihood of reaching the correct\nfinal answer. This new token-level supervision enables TVMs to directly and\nexplicitly evaluate partial solutions, effectively distinguishing between\npromising and incorrect intermediate steps during tree search at test time.\nExperimental results demonstrate that combining tree-search-based inference\nstrategies with TVMs significantly improves the accuracy of LLMs in\nmathematical problem-solving tasks, surpassing the performance of existing\nverifiers.\n","versions":"[{'version': 'v1', 'created': 'Fri, 12 Jul 2024 13:16:50 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 14:24:29 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Lee', 'Jung Hyun', ''], ['Yang', 'June Yong', ''], ['Heo', 'Byeongho', ''], ['Han', 'Dongyoon', ''], ['Kim', 'Kyungsu', ''], ['Yang', 'Eunho', ''], ['Yoo', 'Kang Min', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'TVMs', 'label': 'LLM-based'}, {'text': 'TVMs', 'label': 'LLM-based'}]","assigned_concept":"LLM-based","matched_keyword":"TVMs","similarity_score":0.524304986}
{"id":2408.17267,"submitter":"Baichuan Zhou","authors":"Baichuan Zhou, Haote Yang, Dairong Chen, Junyan Ye, Tianyi Bai, Jinhua\n  Yu, Songyang Zhang, Dahua Lin, Conghui He, Weijia Li","title":"UrBench: A Comprehensive Benchmark for Evaluating Large Multimodal\n  Models in Multi-View Urban Scenarios","comments":"9 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent evaluations of Large Multimodal Models (LMMs) have explored their\ncapabilities in various domains, with only few benchmarks specifically focusing\non urban environments. Moreover, existing urban benchmarks have been limited to\nevaluating LMMs with basic region-level urban tasks under singular views,\nleading to incomplete evaluations of LMMs' abilities in urban environments. To\naddress these issues, we present UrBench, a comprehensive benchmark designed\nfor evaluating LMMs in complex multi-view urban scenarios. UrBench contains\n11.6K meticulously curated questions at both region-level and role-level that\ncover 4 task dimensions: Geo-Localization, Scene Reasoning, Scene\nUnderstanding, and Object Understanding, totaling 14 task types. In\nconstructing UrBench, we utilize data from existing datasets and additionally\ncollect data from 11 cities, creating new annotations using a cross-view\ndetection-matching method. With these images and annotations, we then integrate\nLMM-based, rule-based, and human-based methods to construct large-scale\nhigh-quality questions. Our evaluations on 21 LMMs show that current LMMs\nstruggle in the urban environments in several aspects. Even the best performing\nGPT-4o lags behind humans in most tasks, ranging from simple tasks such as\ncounting to complex tasks such as orientation, localization and object\nattribute recognition, with an average performance gap of 17.4%. Our benchmark\nalso reveals that LMMs exhibit inconsistent behaviors with different urban\nviews, especially with respect to understanding cross-view relations.\n","versions":"[{'version': 'v1', 'created': 'Fri, 30 Aug 2024 13:13:35 GMT'}, {'version': 'v2', 'created': 'Mon, 23 Dec 2024 07:25:51 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 09:48:31 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhou', 'Baichuan', ''], ['Yang', 'Haote', ''], ['Chen', 'Dairong', ''], ['Ye', 'Junyan', ''], ['Bai', 'Tianyi', ''], ['Yu', 'Jinhua', ''], ['Zhang', 'Songyang', ''], ['Lin', 'Dahua', ''], ['He', 'Conghui', ''], ['Li', 'Weijia', '']]","extracted_entities":"[{'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMM-based', 'label': 'LLM-based'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}]","assigned_concept":"LLM-based","matched_keyword":"LMM-based","similarity_score":0.5798506737}
{"id":2409.03946,"submitter":"Kowshik Thopalli","authors":"Banooqa Banday, Kowshik Thopalli, Tanzima Z. Islam, and Jayaraman J.\n  Thiagarajan","title":"On The Role of Prompt Construction In Enhancing Efficacy and Efficiency\n  of LLM-Based Tabular Data Generation","comments":"Accepted to IEEE ICASSP 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  LLM-based data generation for real-world tabular data can be challenged by\nthe lack of sufficient semantic context in feature names used to describe\ncolumns. We hypothesize that enriching prompts with domain-specific insights\ncan improve both the quality and efficiency of data generation. To test this\nhypothesis, we explore three prompt construction protocols: Expert-guided,\nLLM-guided, and Novel-Mapping. Through empirical studies with the recently\nproposed GReaT framework, we find that context-enriched prompts lead to\nsignificantly improved data generation quality and training efficiency.\n","versions":"[{'version': 'v1', 'created': 'Fri, 6 Sep 2024 00:02:09 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 09:52:25 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Banday', 'Banooqa', ''], ['Thopalli', 'Kowshik', ''], ['Islam', 'Tanzima Z.', ''], ['Thiagarajan', 'Jayaraman J.', '']]","extracted_entities":"[{'text': 'Expert-guided', 'label': 'LLM-based'}, {'text': 'LLM-guided', 'label': 'LLM-based'}, {'text': 'Novel-Mapping', 'label': 'LLM-based'}]","assigned_concept":"LLM-based","matched_keyword":"LLM-guided","similarity_score":0.6705395579}
{"id":2410.0407,"submitter":"Ruizhe Chen","authors":"Ruizhe Chen, Xiaotian Zhang, Meng Luo, Wenhao Chai, and Zuozhu Liu","title":"PAD: Personalized Alignment of LLMs at Decoding-Time","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment.\n","versions":"[{'version': 'v1', 'created': 'Sat, 5 Oct 2024 08:00:55 GMT'}, {'version': 'v2', 'created': 'Mon, 14 Oct 2024 13:27:36 GMT'}, {'version': 'v3', 'created': 'Wed, 16 Oct 2024 06:15:35 GMT'}, {'version': 'v4', 'created': 'Tue, 29 Oct 2024 12:51:33 GMT'}, {'version': 'v5', 'created': 'Thu, 7 Nov 2024 06:21:14 GMT'}, {'version': 'v6', 'created': 'Tue, 4 Mar 2025 13:51:14 GMT'}, {'version': 'v7', 'created': 'Thu, 13 Mar 2025 13:37:57 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chen', 'Ruizhe', ''], ['Zhang', 'Xiaotian', ''], ['Luo', 'Meng', ''], ['Chai', 'Wenhao', ''], ['Liu', 'Zuozhu', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM-based'}]","assigned_concept":"LLM-based","matched_keyword":"LLMs","similarity_score":0.8177113533}
{"id":2411.08932,"submitter":"Saikat Barua","authors":"Saikat Barua, Mostafizur Rahman, Md Jafor Sadek, Rafiul Islam,\n  Shehenaz Khaled, Md. Shohrab Hossain","title":"PyGen: A Collaborative Human-AI Approach to Python Package Creation","comments":"33 pages, 13 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The principles of automation and innovation serve as foundational elements\nfor advancement in contemporary science and technology. Here, we introduce\nPygen, an automation platform designed to empower researchers, technologists,\nand hobbyists to bring abstract ideas to life as core, usable software tools\nwritten in Python. Pygen leverages the immense power of autoregressive large\nlanguage models to augment human creativity during the ideation, iteration, and\ninnovation process. By combining state-of-the-art language models with\nopen-source code generation technologies, Pygen has significantly reduced the\nmanual overhead of tool development. From a user prompt, Pygen automatically\ngenerates Python packages for a complete workflow from concept to package\ngeneration and documentation. The findings of our work show that Pygen\nconsiderably enhances the researcher's productivity by enabling the creation of\nresilient, modular, and well-documented packages for various specialized\npurposes. We employ a prompt enhancement approach to distill the user's package\ndescription into increasingly specific and actionable. While being inherently\nan open-ended task, we have evaluated the generated packages and the\ndocumentation using Human Evaluation, LLM-based evaluation, and CodeBLEU, with\ndetailed results in the results section. Furthermore, we documented our\nresults, analyzed the limitations, and suggested strategies to alleviate them.\nPygen is our vision of ethical automation, a framework that promotes\ninclusivity, accessibility, and collaborative development. This project marks\nthe beginning of a large-scale effort towards creating tools where intelligent\nagents collaborate with humans to improve scientific and technological\ndevelopment substantially.\n  Our code and generated examples are open-sourced at\n[https:\/\/github.com\/GitsSaikat\/Pygen]\n","versions":"[{'version': 'v1', 'created': 'Wed, 13 Nov 2024 03:16:18 GMT'}, {'version': 'v2', 'created': 'Wed, 5 Mar 2025 17:11:13 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 09:05:50 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Barua', 'Saikat', ''], ['Rahman', 'Mostafizur', ''], ['Sadek', 'Md Jafor', ''], ['Islam', 'Rafiul', ''], ['Khaled', 'Shehenaz', ''], ['Hossain', 'Md. Shohrab', '']]","extracted_entities":"[{'text': 'user prompt', 'label': 'Prompting'}, {'text': 'LLM-based evaluation', 'label': 'LLM-based'}]","assigned_concept":"LLM-based","matched_keyword":"LLM-based evaluation","similarity_score":0.7232331038}
{"id":2412.12478,"submitter":"Xi Cao","authors":"Xi Cao, Yuan Sun, Jiajun Li, Quzong Gesang, Nuo Qun, Tashi Nyima","title":"Human-in-the-Loop Generation of Adversarial Texts: A Case Study on\n  Tibetan Script","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CR cs.HC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  DNN-based language models perform excellently on various tasks, but even SOTA\nLLMs are susceptible to textual adversarial attacks. Adversarial texts play\ncrucial roles in multiple subfields of NLP. However, current research has the\nfollowing issues. (1) Most textual adversarial attack methods target\nrich-resourced languages. How do we generate adversarial texts for less-studied\nlanguages? (2) Most textual adversarial attack methods are prone to generating\ninvalid or ambiguous adversarial texts. How do we construct high-quality\nadversarial robustness benchmarks? (3) New language models may be immune to\npart of previously generated adversarial texts. How do we update adversarial\nrobustness benchmarks? To address the above issues, we introduce HITL-GAT, a\nsystem based on a general approach to human-in-the-loop generation of\nadversarial texts. HITL-GAT contains four stages in one pipeline: victim model\nconstruction, adversarial example generation, high-quality benchmark\nconstruction, and adversarial robustness evaluation. Additionally, we utilize\nHITL-GAT to make a case study on Tibetan script which can be a reference for\nthe adversarial research of other less-studied languages.\n","versions":"[{'version': 'v1', 'created': 'Tue, 17 Dec 2024 02:29:54 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 00:50:50 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Cao', 'Xi', ''], ['Sun', 'Yuan', ''], ['Li', 'Jiajun', ''], ['Gesang', 'Quzong', ''], ['Qun', 'Nuo', ''], ['Nyima', 'Tashi', '']]","extracted_entities":"[{'text': 'SOTA\\nLLMs', 'label': 'LLM-based'}, {'text': 'HITL-GAT', 'label': 'LLM-based'}, {'text': 'HITL-GAT', 'label': 'LLM-based'}, {'text': 'HITL-GAT', 'label': 'LLM-based'}]","assigned_concept":"LLM-based","matched_keyword":"SOTA\nLLMs","similarity_score":0.5861214399}
{"id":2212.06975,"submitter":"Ernest Y.-Z. Tan","authors":"Mikka Stasiuk, Norbert L\\\"utkenhaus, Ernest Y.-Z. Tan","title":"Quantum Chernoff divergence in advantage distillation for quantum key\n  distribution and device-independent quantum key distribution","comments":"Close to published version","journal-ref":null,"doi":"10.1103\/PhysRevA.111.022631","report-no":null,"categories":"quant-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Device-independent quantum key distribution (DIQKD) aims to mitigate\nadversarial exploitation of imperfections in quantum devices, by providing an\napproach for secret key distillation with modest security assumptions.\nAdvantage distillation, a two-way communication procedure in error correction,\nhas proven effective in raising noise tolerances in both device-dependent and\ndevice-independent QKD. Previously, device-independent security proofs against\nIID collective attacks were developed for an advantage distillation protocol\nknown as the repetition-code protocol, based on security conditions involving\nthe fidelity between some states in the protocol. However, there exists a gap\nbetween the sufficient and necessary security conditions, which hinders the\ncalculation of tight noise-tolerance bounds based on the fidelity. We close\nthis gap by presenting an alternative proof structure that replaces the\nfidelity with the quantum Chernoff divergence, a distinguishability measure\nthat arises in symmetric hypothesis testing. Working in the IID collective\nattacks model, we derive matching sufficient and necessary conditions for the\nrepetition-code protocol to be secure (up to a natural conjecture regarding the\nlatter case) in terms of the quantum Chernoff divergence, hence indicating that\nthis serves as the relevant quantity of interest for this protocol.\nFurthermore, using this security condition we obtain some improvements over\nprevious results on the noise tolerance thresholds for DIQKD. Our results\nprovide insight into a fundamental question in quantum information theory\nregarding the circumstances under which DIQKD is possible.\n","versions":"[{'version': 'v1', 'created': 'Wed, 14 Dec 2022 01:44:23 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Dec 2022 01:15:39 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 19:24:27 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Stasiuk', 'Mikka', ''], ['L\u00fctkenhaus', 'Norbert', ''], ['Tan', 'Ernest Y. -Z.', '']]","extracted_entities":"[{'text': 'secret key distillation', 'label': 'Knowledge distillation'}, {'text': 'Advantage distillation', 'label': 'Knowledge distillation'}, {'text': 'advantage distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Advantage distillation","similarity_score":0.6923582554}
{"id":2303.02278,"submitter":"Chun-Yin Huang","authors":"Chun-Yin Huang, Ruinan Jin, Can Zhao, Daguang Xu, and Xiaoxiao Li","title":"Federated Learning on Virtual Heterogeneous Data with Local-global\n  Distillation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  While Federated Learning (FL) is gaining popularity for training machine\nlearning models in a decentralized fashion, numerous challenges persist, such\nas asynchronization, computational expenses, data heterogeneity, and gradient\nand membership privacy attacks. Lately, dataset distillation has emerged as a\npromising solution for addressing the aforementioned challenges by generating a\ncompact synthetic dataset that preserves a model's training efficacy. However,\nwe discover that using distilled local datasets can amplify the heterogeneity\nissue in FL. To address this, we propose Federated Learning on Virtual\nHeterogeneous Data with Local-Global Dataset Distillation (FedLGD), where we\nseamlessly integrate dataset distillation algorithms into FL pipeline and train\nFL using a smaller synthetic dataset (referred as virtual data). Specifically,\nto harmonize the domain shifts, we propose iterative distribution matching to\ninpaint global information to local virtual data and use federated gradient\nmatching to distill global virtual data that serve as anchor points to rectify\nheterogeneous local training, without compromising data privacy. We experiment\non both benchmark and real-world datasets that contain heterogeneous data from\ndifferent sources, and further scale up to an FL scenario that contains a large\nnumber of clients with heterogeneous and class-imbalanced data. Our method\noutperforms state-of-the-art heterogeneous FL algorithms under various\nsettings. Our code is available at https:\/\/github.com\/ubc-tea\/FedLGD.\n","versions":"[{'version': 'v1', 'created': 'Sat, 4 Mar 2023 00:35:29 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Jun 2023 18:43:26 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 01:01:17 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Huang', 'Chun-Yin', ''], ['Jin', 'Ruinan', ''], ['Zhao', 'Can', ''], ['Xu', 'Daguang', ''], ['Li', 'Xiaoxiao', '']]","extracted_entities":"[{'text': 'Federated Learning', 'label': 'Few-shot Learning'}, {'text': 'dataset distillation', 'label': 'Knowledge distillation'}, {'text': 'Federated Learning', 'label': 'Few-shot Learning'}, {'text': 'Local-Global Dataset Distillation', 'label': 'Knowledge distillation'}, {'text': 'dataset distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"dataset distillation","similarity_score":0.6805129051}
{"id":2303.10318,"submitter":"Shengqin Jiang","authors":"Shengqin Jiang, Yuan Gao, Bowen Li, Fengna Cheng, Renlong Hang,\n  Qingshan Liu","title":"Remote Sensing Object Counting with Online Knowledge Learning","comments":"Accepted by IEEE Transactions on Geoscience and Remote Sensing, 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Efficient models for remote sensing object counting are urgently required for\napplications in scenarios with limited computing resources, such as drones or\nembedded systems. A straightforward yet powerful technique to achieve this is\nknowledge distillation, which steers the learning of student networks by\nleveraging the experience of already-trained teacher networks. However, it\nfaces a pair of challenges: Firstly, due to its two-stage training nature, a\nlonger training period is essential, especially as the training samples\nincrease. Secondly, despite the proficiency of teacher networks in transmitting\nassimilated knowledge, they tend to overlook the latent insights gained during\ntheir learning process. To address these challenges, we introduce an online\ndistillation learning method for remote sensing object counting. It builds an\nend-to-end training framework that seamlessly integrates two distinct networks\ninto a unified one. It comprises a shared shallow module, a teacher branch, and\na student branch. The shared module serving as the foundation for both branches\nis dedicated to learning some primitive information. The teacher branch\nutilizes prior knowledge to reduce the difficulty of learning and guides the\nstudent branch in online learning. In parallel, the student branch achieves\nparameter reduction and rapid inference capabilities by means of channel\nreduction. This design empowers the student branch not only to receive\nprivileged insights from the teacher branch but also to tap into the latent\nreservoir of knowledge held by the teacher branch during the learning process.\nMoreover, we propose a relation-in-relation distillation method that allows the\nstudent branch to effectively comprehend the evolution of the relationship of\nintra-layer teacher features among different inter-layer features. Extensive\nexperiments demonstrate the effectiveness of our method.\n","versions":"[{'version': 'v1', 'created': 'Sat, 18 Mar 2023 03:27:57 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 01:17:37 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Jiang', 'Shengqin', ''], ['Gao', 'Yuan', ''], ['Li', 'Bowen', ''], ['Cheng', 'Fengna', ''], ['Hang', 'Renlong', ''], ['Liu', 'Qingshan', '']]","extracted_entities":"[{'text': 'knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'online\\ndistillation', 'label': 'Knowledge distillation'}, {'text': 'shared shallow module', 'label': 'Foundation Model'}, {'text': 'shared module', 'label': 'Foundation Model'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge distillation","similarity_score":1.0000002384}
{"id":2310.12214,"submitter":"Meng Tong","authors":"Meng Tong and Kejiang Chen and Jie Zhang and Yuang Qi and Weiming\n  Zhang and Nenghai Yu and Tianwei Zhang and Zhikun Zhang","title":"InferDPT: Privacy-Preserving Inference for Black-box Large Language\n  Model","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs), like ChatGPT, have greatly simplified text\ngeneration tasks. However, they have also raised concerns about privacy risks\nsuch as data leakage and unauthorized data collection. Existing solutions for\nprivacy-preserving inference face practical challenges related to computation\ntime and communication costs. In this paper, we propose InferDPT, the first\npractical framework for the privacy-preserving Inference of black-box LLMs,\nimplementing Differential Privacy in Text generation. InferDPT comprises two\nkey modules: the \"perturbation module\" utilizes the exponential mechanism to\ngenerate a perturbed prompt, facilitating privacy-preserving inference with\nblack-box LLMs, and the \"extraction module\", inspired by knowledge distillation\nand retrieval-augmented generation, extracts coherent and consistent text from\nthe perturbed generation result, ensuring successful text generation\ncompletion. To address privacy concerns related to previous exponential\nmechanisms' susceptibility to embedding revision attacks, we introduce RANTEXT,\na novel differential privacy mechanism integrated into the perturbation module\nof InferDPT, which introduces the concept of \"RANdom adjacency\" for TEXT\nperturbation within the prompt. Experimental results across three datasets\ndemonstrate that the text generation quality of InferDPT is comparable to that\nof non-private GPT-4, and RANTEXT surpasses existing state-of-the-art\nmechanisms, namely, SANTEXT+ and CUSTEXT+ in the trade-off between privacy and\nutility. Even with an privacy parameter epsilon value of 6.0, RANTEXT achieves\nan average privacy protection rate exceeding 90% against embedding revision\nattacks, which is 0.58 times higher than that of SANTEXT+ and 3.35 times higher\nthan that of CUSTEXT+.\n","versions":"[{'version': 'v1', 'created': 'Wed, 18 Oct 2023 18:00:11 GMT'}, {'version': 'v2', 'created': 'Sun, 22 Oct 2023 07:34:36 GMT'}, {'version': 'v3', 'created': 'Tue, 24 Oct 2023 03:25:14 GMT'}, {'version': 'v4', 'created': 'Fri, 8 Dec 2023 05:14:40 GMT'}, {'version': 'v5', 'created': 'Mon, 11 Dec 2023 09:59:09 GMT'}, {'version': 'v6', 'created': 'Wed, 27 Mar 2024 09:19:01 GMT'}, {'version': 'v7', 'created': 'Mon, 10 Mar 2025 06:52:58 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Tong', 'Meng', ''], ['Chen', 'Kejiang', ''], ['Zhang', 'Jie', ''], ['Qi', 'Yuang', ''], ['Zhang', 'Weiming', ''], ['Yu', 'Nenghai', ''], ['Zhang', 'Tianwei', ''], ['Zhang', 'Zhikun', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'GPT-4', 'label': 'GPT'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge distillation","similarity_score":1.0000002384}
{"id":2402.13785,"submitter":"Florent Delgrange","authors":"Florent Delgrange, Guy Avni, Anna Lukina, Christian Schilling, Ann\n  Now\\'e, and Guillermo A. P\\'erez","title":"Composing Reinforcement Learning Policies, with Formal Guarantees","comments":"AAMAS 2025, 8 pages main text, 19 pages Appendix (excluding\n  references)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We propose a novel framework to controller design in environments with a\ntwo-level structure: a known high-level graph (\"map\") in which each vertex is\npopulated by a Markov decision process, called a \"room\". The framework\n\"separates concerns\" by using different design techniques for low- and\nhigh-level tasks. We apply reactive synthesis for high-level tasks: given a\nspecification as a logical formula over the high-level graph and a collection\nof low-level policies obtained together with \"concise\" latent structures, we\nconstruct a \"planner\" that selects which low-level policy to apply in each\nroom. We develop a reinforcement learning procedure to train low-level policies\non latent structures, which unlike previous approaches, circumvents a model\ndistillation step. We pair the policy with probably approximately correct\nguarantees on its performance and on the abstraction quality, and lift these\nguarantees to the high-level task. These formal guarantees are the main\nadvantage of the framework. Other advantages include scalability (rooms are\nlarge and their dynamics are unknown) and reusability of low-level policies. We\ndemonstrate feasibility in challenging case studies where an agent navigates\nenvironments with moving obstacles and visual inputs.\n","versions":"[{'version': 'v1', 'created': 'Wed, 21 Feb 2024 13:10:58 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 11:38:38 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Delgrange', 'Florent', ''], ['Avni', 'Guy', ''], ['Lukina', 'Anna', ''], ['Schilling', 'Christian', ''], ['Now\u00e9', 'Ann', ''], ['P\u00e9rez', 'Guillermo A.', '']]","extracted_entities":"[{'text': 'model\\ndistillation step', 'label': 'Knowledge distillation'}, {'text': 'scalability', 'label': 'Scaling law'}]","assigned_concept":"Knowledge distillation","matched_keyword":"model\ndistillation step","similarity_score":0.6324329376}
{"id":2404.07199,"submitter":"Alex Trevithick","authors":"Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi","title":"RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth\n  Diffusion","comments":"Published at 3DV 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.GR cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce RealmDreamer, a technique for generating forward-facing 3D\nscenes from text descriptions. Our method optimizes a 3D Gaussian Splatting\nrepresentation to match complex text prompts using pretrained diffusion models.\nOur key insight is to leverage 2D inpainting diffusion models conditioned on an\ninitial scene estimate to provide low variance supervision for unknown regions\nduring 3D distillation. In conjunction, we imbue high-fidelity geometry with\ngeometric distillation from a depth diffusion model, conditioned on samples\nfrom the inpainting model. We find that the initialization of the optimization\nis crucial, and provide a principled methodology for doing so. Notably, our\ntechnique doesn't require video or multi-view data and can synthesize various\nhigh-quality 3D scenes in different styles with complex layouts. Further, the\ngenerality of our method allows 3D synthesis from a single image. As measured\nby a comprehensive user study, our method outperforms all existing approaches,\npreferred by 88-95%. Project Page: https:\/\/realmdreamer.github.io\/\n","versions":"[{'version': 'v1', 'created': 'Wed, 10 Apr 2024 17:57:41 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 17:06:18 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Shriram', 'Jaidev', ''], ['Trevithick', 'Alex', ''], ['Liu', 'Lingjie', ''], ['Ramamoorthi', 'Ravi', '']]","extracted_entities":"[{'text': 'RealmDreamer', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'complex text prompts', 'label': 'Prompting'}, {'text': '3D distillation', 'label': 'Knowledge distillation'}, {'text': 'geometric distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"3D distillation","similarity_score":0.6011003256}
{"id":2405.11525,"submitter":"Chun-Yin Huang","authors":"Chun-Yin Huang and Kartik Srinivas and Xin Zhang and Xiaoxiao Li","title":"Overcoming Data and Model Heterogeneities in Decentralized Federated\n  Learning via Synthetic Anchors","comments":"Paper Accepted at ICML 2024, 23 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Conventional Federated Learning (FL) involves collaborative training of a\nglobal model while maintaining user data privacy. One of its branches,\ndecentralized FL, is a serverless network that allows clients to own and\noptimize different local models separately, which results in saving management\nand communication resources. Despite the promising advancements in\ndecentralized FL, it may reduce model generalizability due to lacking a global\nmodel. In this scenario, managing data and model heterogeneity among clients\nbecomes a crucial problem, which poses a unique challenge that must be\novercome: How can every client's local model learn generalizable representation\nin a decentralized manner? To address this challenge, we propose a novel\nDecentralized FL technique by introducing Synthetic Anchors, dubbed as DeSA.\nBased on the theory of domain adaptation and Knowledge Distillation (KD), we\ntheoretically and empirically show that synthesizing global anchors based on\nraw data distribution facilitates mutual knowledge transfer. We further design\ntwo effective regularization terms for local training: 1) REG loss that\nregularizes the distribution of the client's latent embedding with the anchors\nand 2) KD loss that enables clients to learn from others. Through extensive\nexperiments on diverse client data distributions, we showcase the effectiveness\nof DeSA in enhancing both inter- and intra-domain accuracy of each client.\n","versions":"[{'version': 'v1', 'created': 'Sun, 19 May 2024 11:36:45 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 04:39:54 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Huang', 'Chun-Yin', ''], ['Srinivas', 'Kartik', ''], ['Zhang', 'Xin', ''], ['Li', 'Xiaoxiao', '']]","extracted_entities":"[{'text': 'Knowledge Distillation', 'label': 'Knowledge distillation'}, {'text': 'latent embedding', 'label': 'Embedding'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Knowledge Distillation","similarity_score":1.0000002384}
{"id":2405.15644,"submitter":"Martijn de Vos","authors":"Akash Dhasade, Anne-Marie Kermarrec, Tuan-Anh Nguyen, Rafael Pires,\n  Martijn de Vos","title":"Harnessing Increased Client Participation with Cohort-Parallel Federated\n  Learning","comments":"To appear in the proceedings of EuroMLSys'25","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.DC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Federated learning (FL) is a machine learning approach where nodes\ncollaboratively train a global model. As more nodes participate in a round of\nFL, the effectiveness of individual model updates by nodes also diminishes. In\nthis study, we increase the effectiveness of client updates by dividing the\nnetwork into smaller partitions, or cohorts. We introduce Cohort-Parallel\nFederated Learning (CPFL): a novel learning approach where each cohort\nindependently trains a global model using FL, until convergence, and the\nproduced models by each cohort are then unified using knowledge distillation.\nThe insight behind CPFL is that smaller, isolated networks converge quicker\nthan in a one-network setting where all nodes participate. Through exhaustive\nexperiments involving realistic traces and non-IID data distributions on the\nCIFAR-10 and FEMNIST image classification tasks, we investigate the balance\nbetween the number of cohorts, model accuracy, training time, and compute\nresources. Compared to traditional FL, CPFL with four cohorts, non-IID data\ndistribution, and CIFAR-10 yields a 1.9x reduction in train time and a 1.3x\nreduction in resource usage, with a minimal drop in test accuracy.\n","versions":"[{'version': 'v1', 'created': 'Fri, 24 May 2024 15:34:09 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 20:38:37 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Dhasade', 'Akash', ''], ['Kermarrec', 'Anne-Marie', ''], ['Nguyen', 'Tuan-Anh', ''], ['Pires', 'Rafael', ''], ['de Vos', 'Martijn', '']]","extracted_entities":"[{'text': 'Federated learning', 'label': 'Few-shot Learning'}, {'text': 'FL', 'label': 'Zero-shot Learning'}, {'text': 'FL', 'label': 'Zero-shot Learning'}, {'text': 'knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'FL', 'label': 'Zero-shot Learning'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge distillation","similarity_score":1.0000002384}
{"id":2406.08226,"submitter":"Sanket Biswas","authors":"Jordy Van Landeghem, Subhajit Maity, Ayan Banerjee, Matthew Blaschko,\n  Marie-Francine Moens, Josep Llad\\'os, Sanket Biswas","title":"DistilDoc: Knowledge Distillation for Visually-Rich Document\n  Applications","comments":"Accepted to ICDAR 2024 (Athens, Greece)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  This work explores knowledge distillation (KD) for visually-rich document\n(VRD) applications such as document layout analysis (DLA) and document image\nclassification (DIC). While VRD research is dependent on increasingly\nsophisticated and cumbersome models, the field has neglected to study\nefficiency via model compression. Here, we design a KD experimentation\nmethodology for more lean, performant models on document understanding (DU)\ntasks that are integral within larger task pipelines. We carefully selected KD\nstrategies (response-based, feature-based) for distilling knowledge to and from\nbackbones with different architectures (ResNet, ViT, DiT) and capacities (base,\nsmall, tiny). We study what affects the teacher-student knowledge gap and find\nthat some methods (tuned vanilla KD, MSE, SimKD with an apt projector) can\nconsistently outperform supervised student training. Furthermore, we design\ndownstream task setups to evaluate covariate shift and the robustness of\ndistilled DLA models on zero-shot layout-aware document visual question\nanswering (DocVQA). DLA-KD experiments result in a large mAP knowledge gap,\nwhich unpredictably translates to downstream robustness, accentuating the need\nto further explore how to efficiently obtain more semantic document layout\nawareness.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Jun 2024 13:55:12 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 11:58:36 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Van Landeghem', 'Jordy', ''], ['Maity', 'Subhajit', ''], ['Banerjee', 'Ayan', ''], ['Blaschko', 'Matthew', ''], ['Moens', 'Marie-Francine', ''], ['Llad\u00f3s', 'Josep', ''], ['Biswas', 'Sanket', '']]","extracted_entities":"[{'text': 'knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'base', 'label': 'Foundation Model'}, {'text': 'supervised student training', 'label': 'Few-shot Learning'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge distillation","similarity_score":1.0000002384}
{"id":2407.13911,"submitter":"Qifan Zhang","authors":"Qifan Zhang, Yunhui Guo, Yu Xiang","title":"Continual Distillation Learning: Knowledge Distillation in Prompt-based\n  Continual Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  We introduce the problem of continual distillation learning (CDL) in order to\nuse knowledge distillation (KD) to improve prompt-based continual learning (CL)\nmodels. The CDL problem is valuable to study since the use of a larger vision\ntransformer (ViT) leads to better performance in prompt-based continual\nlearning. The distillation of knowledge from a large ViT to a small ViT can\nimprove the inference efficiency for prompt-based CL models. We empirically\nfound that existing KD methods such as logit distillation and feature\ndistillation cannot effectively improve the student model in the CDL setup. To\nthis end, we introduce a novel method named Knowledge Distillation based on\nPrompts (KDP), in which globally accessible prompts specifically designed for\nknowledge distillation are inserted into the frozen ViT backbone of the student\nmodel. We demonstrate that our KDP method effectively enhances the distillation\nperformance in comparison to existing KD methods in the CDL setup.\n","versions":"[{'version': 'v1', 'created': 'Thu, 18 Jul 2024 21:52:57 GMT'}, {'version': 'v2', 'created': 'Fri, 13 Dec 2024 23:49:45 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 22:12:13 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Zhang', 'Qifan', ''], ['Guo', 'Yunhui', ''], ['Xiang', 'Yu', '']]","extracted_entities":"[{'text': 'knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'CDL', 'label': 'Few-shot Learning'}, {'text': 'prompt-based continual\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'logit distillation', 'label': 'Knowledge distillation'}, {'text': 'feature\\ndistillation', 'label': 'Knowledge distillation'}, {'text': 'CDL', 'label': 'Few-shot Learning'}, {'text': 'Knowledge Distillation', 'label': 'Knowledge distillation'}, {'text': 'Prompts', 'label': 'Prompting'}, {'text': 'globally accessible prompts', 'label': 'Prompting'}, {'text': 'knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'ViT', 'label': 'Large Language Model'}, {'text': 'KDP', 'label': 'Prompting'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge distillation","similarity_score":1.0000002384}
{"id":2409.10362,"submitter":"Amin Karimi Monsefi","authors":"Amin Karimi Monsefi, Mengxi Zhou, Nastaran Karimi Monsefi, Ser-Nam\n  Lim, Wei-Lun Chao, Rajiv Ramnath","title":"Frequency-Guided Masking for Enhanced Vision Self-Supervised Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We present a novel frequency-based Self-Supervised Learning (SSL) approach\nthat significantly enhances its efficacy for pre-training. Prior work in this\ndirection masks out pre-defined frequencies in the input image and employs a\nreconstruction loss to pre-train the model. While achieving promising results,\nsuch an implementation has two fundamental limitations as identified in our\npaper. First, using pre-defined frequencies overlooks the variability of image\nfrequency responses. Second, pre-trained with frequency-filtered images, the\nresulting model needs relatively more data to adapt to naturally looking images\nduring fine-tuning. To address these drawbacks, we propose FOurier transform\ncompression with seLf-Knowledge distillation (FOLK), integrating two dedicated\nideas. First, inspired by image compression, we adaptively select the\nmasked-out frequencies based on image frequency responses, creating more\nsuitable SSL tasks for pre-training. Second, we employ a two-branch framework\nempowered by knowledge distillation, enabling the model to take both the\nfiltered and original images as input, largely reducing the burden of\ndownstream tasks. Our experimental results demonstrate the effectiveness of\nFOLK in achieving competitive performance to many state-of-the-art SSL methods\nacross various downstream tasks, including image classification, few-shot\nlearning, and semantic segmentation.\n","versions":"[{'version': 'v1', 'created': 'Mon, 16 Sep 2024 15:10:07 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 23:26:49 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Monsefi', 'Amin Karimi', ''], ['Zhou', 'Mengxi', ''], ['Monsefi', 'Nastaran Karimi', ''], ['Lim', 'Ser-Nam', ''], ['Chao', 'Wei-Lun', ''], ['Ramnath', 'Rajiv', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'seLf-Knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'FOLK', 'label': 'Knowledge distillation'}, {'text': 'few-shot\\nlearning', 'label': 'Few-shot Learning'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge distillation","similarity_score":1.0000002384}
{"id":2410.04224,"submitter":"Jianze Li","authors":"Jianze Li, Jiezhang Cao, Zichen Zou, Xiongfei Su, Xin Yuan, Yulun\n  Zhang, Yong Guo, Xiaokang Yang","title":"Unleashing the Power of One-Step Diffusion based Image Super-Resolution\n  via a Large-Scale Diffusion Discriminator","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Diffusion models have demonstrated excellent performance for real-world image\nsuper-resolution (Real-ISR), albeit at high computational costs. Most existing\nmethods are trying to derive one-step diffusion models from multi-step\ncounterparts through knowledge distillation (KD) or variational score\ndistillation (VSD). However, these methods are limited by the capabilities of\nthe teacher model, especially if the teacher model itself is not sufficiently\nstrong. To tackle these issues, we propose a new One-Step \\textbf{D}iffusion\nmodel with a larger-scale \\textbf{D}iffusion \\textbf{D}iscriminator for SR,\ncalled D$^3$SR. Our discriminator is able to distill noisy features from any\ntime step of diffusion models in the latent space. In this way, our diffusion\ndiscriminator breaks through the potential limitations imposed by the presence\nof a teacher model. Additionally, we improve the perceptual loss with\nedge-aware DISTS (EA-DISTS) to enhance the model's ability to generate fine\ndetails. Our experiments demonstrate that, compared with previous\ndiffusion-based methods requiring dozens or even hundreds of steps, our D$^3$SR\nattains comparable or even superior results in both quantitative metrics and\nqualitative evaluations. Moreover, compared with other methods, D$^3$SR\nachieves at least $3\\times$ faster inference speed and reduces parameters by at\nleast 30\\%. We will release code and models at\nhttps:\/\/github.com\/JianzeLi-114\/D3SR.\n","versions":"[{'version': 'v1', 'created': 'Sat, 5 Oct 2024 16:41:36 GMT'}, {'version': 'v2', 'created': 'Thu, 10 Oct 2024 06:00:35 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 16:37:34 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Li', 'Jianze', ''], ['Cao', 'Jiezhang', ''], ['Zou', 'Zichen', ''], ['Su', 'Xiongfei', ''], ['Yuan', 'Xin', ''], ['Zhang', 'Yulun', ''], ['Guo', 'Yong', ''], ['Yang', 'Xiaokang', '']]","extracted_entities":"[{'text': 'knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'variational score\\ndistillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge distillation","similarity_score":1.0000002384}
{"id":2410.07663,"submitter":"Sohwi Kim","authors":"Sohwi Kim, Tae-Kyun Kim","title":"Co-learning Single-Step Diffusion Upsampler and Downsampler with Two\n  Discriminators and Distillation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Super-resolution (SR) aims to reconstruct high-resolution (HR) images from\ntheir low-resolution (LR) counterparts, often relying on effective downsampling\nto generate diverse and realistic training pairs. In this work, we propose a\nco-learning framework that jointly optimizes a single-step diffusion-based\nupsampler and a learnable downsampler, enhanced by two discriminators and a\ncyclic distillation strategy. Our learnable downsampler is designed to better\ncapture realistic degradation patterns while preserving structural details in\nthe LR domain, which is crucial for enhancing SR performance. By leveraging a\ndiffusion-based approach, our model generates diverse LR-HR pairs during\ntraining, enabling robust learning across varying degradations. We demonstrate\nthe effectiveness of our method on both general real-world and domain-specific\nface SR tasks, achieving state-of-the-art performance in both fidelity and\nperceptual quality. Our approach not only improves efficiency with a single\ninference step but also ensures high-quality image reconstruction, bridging the\ngap between synthetic and real-world SR scenarios.\n","versions":"[{'version': 'v1', 'created': 'Thu, 10 Oct 2024 07:12:46 GMT'}, {'version': 'v2', 'created': 'Wed, 4 Dec 2024 08:47:23 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 10:53:12 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Kim', 'Sohwi', ''], ['Kim', 'Tae-Kyun', '']]","extracted_entities":"[{'text': 'cyclic distillation strategy', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"cyclic distillation strategy","similarity_score":0.6092345715}
{"id":2410.21186,"submitter":"Damiano Andreghetti","authors":"Damiano Andreghetti, Luca Dall'Asta, Andrea Gamba, Igor Kolokolov,\n  Vladimir Lebedev","title":"Molecular sorting on a fluctuating membrane","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.bio-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Molecular sorting in biological membranes is essential for proper cellular\nfunction. It also plays a crucial role in the budding of enveloped viruses from\nhost cells. We recently proposed that this process is driven by phase\nseparation, where the formation and growth of sorting domains depend primarily\non direct intermolecular interactions. In addition to these, Casimir-like\nforces -- arising from entropic effects in fluctuating membranes -- may also\nplay a significant role in the molecular distillation process. Here, using a\ncombination of theoretical analysis and numerical simulations, we explore how\nCasimir-like forces between rigid membrane inclusions contribute to sorting,\nparticularly in the biologically relevant regime where direct intermolecular\ninteractions are weak. Our results show that these forces enhance molecular\ndistillation by reducing the critical radius for the formation of new sorting\ndomains and facilitating the capture of molecules within these domains. We\nidentify the relative rigidity of the membrane and supermolecular domains as a\nkey parameter controlling molecular sorting efficiency, offering new insights\ninto the physical principles underlying molecular sorting in biological\nsystems.\n","versions":"[{'version': 'v1', 'created': 'Mon, 28 Oct 2024 16:25:57 GMT'}, {'version': 'v2', 'created': 'Fri, 1 Nov 2024 15:02:49 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 13:49:47 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Andreghetti', 'Damiano', ''], [\"Dall'Asta\", 'Luca', ''], ['Gamba', 'Andrea', ''], ['Kolokolov', 'Igor', ''], ['Lebedev', 'Vladimir', '']]","extracted_entities":"[{'text': 'molecular distillation', 'label': 'Knowledge distillation'}, {'text': 'molecular\\ndistillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"molecular distillation","similarity_score":0.6359279156}
{"id":2411.13383,"submitter":"Chen Bin","authors":"Bin Chen, Gehui Li, Rongyuan Wu, Xindong Zhang, Jie Chen, Jian Zhang,\n  Lei Zhang","title":"Adversarial Diffusion Compression for Real-World Image Super-Resolution","comments":"Accepted by CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Real-world image super-resolution (Real-ISR) aims to reconstruct\nhigh-resolution images from low-resolution inputs degraded by complex, unknown\nprocesses. While many Stable Diffusion (SD)-based Real-ISR methods have\nachieved remarkable success, their slow, multi-step inference hinders practical\ndeployment. Recent SD-based one-step networks like OSEDiff and S3Diff alleviate\nthis issue but still incur high computational costs due to their reliance on\nlarge pretrained SD models. This paper proposes a novel Real-ISR method, AdcSR,\nby distilling the one-step diffusion network OSEDiff into a streamlined\ndiffusion-GAN model under our Adversarial Diffusion Compression (ADC)\nframework. We meticulously examine the modules of OSEDiff, categorizing them\ninto two types: (1) Removable (VAE encoder, prompt extractor, text encoder,\netc.) and (2) Prunable (denoising UNet and VAE decoder). Since direct removal\nand pruning can degrade the model's generation capability, we pretrain our\npruned VAE decoder to restore its ability to decode images and employ\nadversarial distillation to compensate for performance loss. This ADC-based\ndiffusion-GAN hybrid design effectively reduces complexity by 73% in inference\ntime, 78% in computation, and 74% in parameters, while preserving the model's\ngeneration capability. Experiments manifest that our proposed AdcSR achieves\ncompetitive recovery quality on both synthetic and real-world datasets,\noffering up to 9.3$\\times$ speedup over previous one-step diffusion-based\nmethods. Code and models are available at\nhttps:\/\/github.com\/Guaishou74851\/AdcSR.\n","versions":"[{'version': 'v1', 'created': 'Wed, 20 Nov 2024 15:13:36 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 09:31:57 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Chen', 'Bin', ''], ['Li', 'Gehui', ''], ['Wu', 'Rongyuan', ''], ['Zhang', 'Xindong', ''], ['Chen', 'Jie', ''], ['Zhang', 'Jian', ''], ['Zhang', 'Lei', '']]","extracted_entities":"[{'text': 'adversarial distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"adversarial distillation","similarity_score":0.6295915246}
{"id":2411.13753,"submitter":"Ola Shorinw","authors":"Ola Shorinwa, Jiankai Sun, Mac Schwager","title":"FAST-Splat: Fast, Ambiguity-Free Semantics Transfer in Gaussian\n  Splatting","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We present FAST-Splat for fast, ambiguity-free semantic Gaussian Splatting,\nwhich seeks to address the main limitations of existing semantic Gaussian\nSplatting methods, namely: slow training and rendering speeds; high memory\nusage; and ambiguous semantic object localization. We take a bottom-up approach\nin deriving FAST-Splat, dismantling the limitations of closed-set semantic\ndistillation to enable open-set (open-vocabulary) semantic distillation.\nUltimately, this key approach enables FAST-Splat to provide precise semantic\nobject localization results, even when prompted with ambiguous user-provided\nnatural-language queries. Further, by exploiting the explicit form of the\nGaussian Splatting scene representation to the fullest extent, FAST-Splat\nretains the remarkable training and rendering speeds of Gaussian Splatting.\nPrecisely, while existing semantic Gaussian Splatting methods distill semantics\ninto a separate neural field or utilize neural models for dimensionality\nreduction, FAST-Splat directly augments each Gaussian with specific semantic\ncodes, preserving the training, rendering, and memory-usage advantages of\nGaussian Splatting over neural field methods. These Gaussian-specific semantic\ncodes, together with a hash-table, enable semantic similarity to be measured\nwith open-vocabulary user prompts and further enable FAST-Splat to respond with\nunambiguous semantic object labels and $3$D masks, unlike prior methods. In\nexperiments, we demonstrate that FAST-Splat is 6x to 8x faster to train,\nachieves between 18x to 51x faster rendering speeds, and requires about 6x\nsmaller GPU memory, compared to the best-competing semantic Gaussian Splatting\nmethods. Further, FAST-Splat achieves relatively similar or better semantic\nsegmentation performance compared to existing methods. After the review period,\nwe will provide links to the project website and the codebase.\n","versions":"[{'version': 'v1', 'created': 'Wed, 20 Nov 2024 23:36:46 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 02:17:03 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Shorinwa', 'Ola', ''], ['Sun', 'Jiankai', ''], ['Schwager', 'Mac', '']]","extracted_entities":"[{'text': 'closed-set semantic\\ndistillation', 'label': 'Knowledge distillation'}, {'text': 'semantic distillation', 'label': 'Knowledge distillation'}, {'text': 'user prompts', 'label': 'Prompting'}]","assigned_concept":"Knowledge distillation","matched_keyword":"semantic distillation","similarity_score":0.790204823}
{"id":2411.15232,"submitter":"Taha Koleilat","authors":"Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao","title":"BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models","comments":"Accepted to CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Recent advancements in vision-language models (VLMs), such as CLIP, have\ndemonstrated substantial success in self-supervised representation learning for\nvision tasks. However, effectively adapting VLMs to downstream applications\nremains challenging, as their accuracy often depends on time-intensive and\nexpertise-demanding prompt engineering, while full model fine-tuning is costly.\nThis is particularly true for biomedical images, which, unlike natural images,\ntypically suffer from limited annotated datasets, unintuitive image contrasts,\nand nuanced visual features. Recent prompt learning techniques, such as Context\nOptimization (CoOp) intend to tackle these issues, but still fall short in\ngeneralizability. Meanwhile, explorations in prompt learning for biomedical\nimage analysis are still highly limited. In this work, we propose BiomedCoOp, a\nnovel prompt learning framework that enables efficient adaptation of BiomedCLIP\nfor accurate and highly generalizable few-shot biomedical image classification.\nOur approach achieves effective prompt context learning by leveraging semantic\nconsistency with average prompt ensembles from Large Language Models (LLMs) and\nknowledge distillation with a statistics-based prompt selection strategy. We\nconducted comprehensive validation of our proposed framework on 11 medical\ndatasets across 9 modalities and 10 organs against existing state-of-the-art\nmethods, demonstrating significant improvements in both accuracy and\ngeneralizability. The code is publicly available at\nhttps:\/\/github.com\/HealthX-Lab\/BiomedCoOp.\n","versions":"[{'version': 'v1', 'created': 'Thu, 21 Nov 2024 19:13:04 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 03:28:09 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Koleilat', 'Taha', ''], ['Asgariandehkordi', 'Hojat', ''], ['Rivaz', 'Hassan', ''], ['Xiao', 'Yiming', '']]","extracted_entities":"[{'text': 'full model fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'knowledge distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge distillation","similarity_score":1.0000002384}
{"id":2412.06244,"submitter":"Yunheng Li","authors":"Yunheng Li, Yuxuan Li, Quansheng Zeng, Wenhai Wang, Qibin Hou,\n  Ming-Ming Cheng","title":"Unbiased Region-Language Alignment for Open-Vocabulary Dense Prediction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated\nimpressive zero-shot recognition capability, but still underperform in dense\nprediction tasks. Self-distillation recently is emerging as a promising\napproach for fine-tuning VLMs to better adapt to local regions without\nrequiring extensive annotations. However, previous state-of-the-art approaches\noften suffer from significant `foreground bias', where models tend to wrongly\nidentify background regions as foreground objects. To alleviate this issue, we\npropose DenseVLM, a framework designed to learn unbiased region-language\nalignment from powerful pre-trained VLM representations. To alleviate this\nissue, we propose DenseVLM, a framework designed to learn unbiased\nregion-language alignment from powerful pre-trained VLM representations.\nDenseVLM leverages the pre-trained VLM to retrieve categories for unlabeled\nregions and then decouples the interference between foreground and background\nfeatures. We show that DenseVLM can directly replace the original VLM in\nopen-vocabulary object detection and image segmentation methods, leading to\nnotable performance improvements. Furthermore, it exhibits promising zero-shot\nscalability when training on more extensive and diverse datasets. Our code is\navailable at https:\/\/github.com\/HVision-NKU\/DenseVLM.\n","versions":"[{'version': 'v1', 'created': 'Mon, 9 Dec 2024 06:34:23 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 07:19:10 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Li', 'Yunheng', ''], ['Li', 'Yuxuan', ''], ['Zeng', 'Quansheng', ''], ['Wang', 'Wenhai', ''], ['Hou', 'Qibin', ''], ['Cheng', 'Ming-Ming', '']]","extracted_entities":"[{'text': 'Self-distillation', 'label': 'Knowledge distillation'}, {'text': 'foreground bias', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Self-distillation","similarity_score":0.6929070354}
{"id":2412.09959,"submitter":"Xinhao Zhong","authors":"Xinhao Zhong, Shuoyang Sun, Xulin Gu, Zhaoyang Xu, Yaowei Wang, Min\n  Zhang, Bin Chen","title":"Efficient Dataset Distillation via Diffusion-Driven Patch Selection for\n  Improved Generalization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Dataset distillation offers an efficient way to reduce memory and\ncomputational costs by optimizing a smaller dataset with performance comparable\nto the full-scale original. However, for large datasets and complex deep\nnetworks (e.g., ImageNet-1K with ResNet-101), the extensive optimization space\nlimits performance, reducing its practicality. Recent approaches employ\npre-trained diffusion models to generate informative images directly, avoiding\npixel-level optimization and achieving notable results. However, these methods\noften face challenges due to distribution shifts between pre-trained models and\ntarget datasets, along with the need for multiple distillation steps across\nvarying settings. To address these issues, we propose a novel framework\northogonal to existing diffusion-based distillation methods, leveraging\ndiffusion models for selection rather than generation. Our method starts by\npredicting noise generated by the diffusion model based on input images and\ntext prompts (with or without label text), then calculates the corresponding\nloss for each pair. With the loss differences, we identify distinctive regions\nof the original images. Additionally, we perform intra-class clustering and\nranking on selected patches to maintain diversity constraints. This streamlined\nframework enables a single-step distillation process, and extensive experiments\ndemonstrate that our approach outperforms state-of-the-art methods across\nvarious metrics.\n","versions":"[{'version': 'v1', 'created': 'Fri, 13 Dec 2024 08:34:46 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Feb 2025 16:11:13 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 09:32:43 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhong', 'Xinhao', ''], ['Sun', 'Shuoyang', ''], ['Gu', 'Xulin', ''], ['Xu', 'Zhaoyang', ''], ['Wang', 'Yaowei', ''], ['Zhang', 'Min', ''], ['Chen', 'Bin', '']]","extracted_entities":"[{'text': 'Dataset distillation', 'label': 'Knowledge distillation'}, {'text': 'text prompts', 'label': 'Prompting'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Dataset distillation","similarity_score":0.6805129051}
{"id":2412.15341,"submitter":"Reza Shirkavand","authors":"Reza Shirkavand, Peiran Yu, Shangqian Gao, Gowthami Somepalli, Tom\n  Goldstein, Heng Huang","title":"Efficient Fine-Tuning and Concept Suppression for Pruned Diffusion\n  Models","comments":"CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent advances in diffusion generative models have yielded remarkable\nprogress. While the quality of generated content continues to improve, these\nmodels have grown considerably in size and complexity. This increasing\ncomputational burden poses significant challenges, particularly in\nresource-constrained deployment scenarios such as mobile devices. The\ncombination of model pruning and knowledge distillation has emerged as a\npromising solution to reduce computational demands while preserving generation\nquality. However, this technique inadvertently propagates undesirable\nbehaviors, including the generation of copyrighted content and unsafe concepts,\neven when such instances are absent from the fine-tuning dataset. In this\npaper, we propose a novel bilevel optimization framework for pruned diffusion\nmodels that consolidates the fine-tuning and unlearning processes into a\nunified phase. Our approach maintains the principal advantages of\ndistillation-namely, efficient convergence and style transfer\ncapabilities-while selectively suppressing the generation of unwanted content.\nThis plug-in framework is compatible with various pruning and concept\nunlearning methods, facilitating efficient, safe deployment of diffusion models\nin controlled environments.\n","versions":"[{'version': 'v1', 'created': 'Thu, 19 Dec 2024 19:13:18 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 20:52:10 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Shirkavand', 'Reza', ''], ['Yu', 'Peiran', ''], ['Gao', 'Shangqian', ''], ['Somepalli', 'Gowthami', ''], ['Goldstein', 'Tom', ''], ['Huang', 'Heng', '']]","extracted_entities":"[{'text': 'knowledge distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge distillation","similarity_score":1.0000002384}
{"id":2412.21197,"submitter":"Yang Chen","authors":"Yang Chen, Sheng Guo, Bo Zheng and Limin Wang","title":"A Large-Scale Study on Video Action Dataset Condensation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recently, dataset condensation has made significant progress in the image\ndomain. Unlike images, videos possess an additional temporal dimension, which\nharbors considerable redundant information, making condensation even more\ncrucial. However, video dataset condensation still remains an underexplored\narea. We aim to bridge this gap by providing a large-scale study with\nsystematic design and fair comparison. Specifically, our work delves into three\nkey aspects to provide valuable empirical insights: (1) temporal processing of\nvideo data, (2) the evaluation protocol for video dataset condensation, and (3)\nadaptation of condensation algorithms to the space-time domain. From this\nstudy, we derive several intriguing observations: (i) labeling methods greatly\ninfluence condensation performance, (ii) simple sliding-window sampling is\neffective for temporal processing, and (iii) dataset distillation methods\nperform better in challenging scenarios, while sample selection methods excel\nin easier ones. Furthermore, we propose a unified evaluation protocol for the\nfair comparison of different condensation algorithms and achieve\nstate-of-the-art results on four widely-used action recognition datasets:\nHMDB51, UCF101, SSv2 and K400. Our code is available at\nhttps:\/\/github.com\/MCG-NJU\/Video-DC.\n","versions":"[{'version': 'v1', 'created': 'Mon, 30 Dec 2024 18:58:29 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 03:28:28 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Chen', 'Yang', ''], ['Guo', 'Sheng', ''], ['Zheng', 'Bo', ''], ['Wang', 'Limin', '']]","extracted_entities":"[{'text': 'dataset condensation', 'label': 'Knowledge distillation'}, {'text': 'dataset distillation methods', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"dataset distillation methods","similarity_score":0.6553401351}
{"id":2501.10459,"submitter":"Qianru Zhang","authors":"Qianru Zhang, Xinyi Gao, Haixin Wang, Siu-Ming Yiu and Hongzhi Yin","title":"Efficient Traffic Prediction Through Spatio-Temporal Distillation","comments":"9 pages","journal-ref":"AAAI'2025","doi":null,"report-no":null,"categories":"cs.LG cs.CE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Graph neural networks (GNNs) have gained considerable attention in recent\nyears for traffic flow prediction due to their ability to learn spatio-temporal\npattern representations through a graph-based message-passing framework.\nAlthough GNNs have shown great promise in handling traffic datasets, their\ndeployment in real-life applications has been hindered by scalability\nconstraints arising from high-order message passing. Additionally, the\nover-smoothing problem of GNNs may lead to indistinguishable region\nrepresentations as the number of layers increases, resulting in performance\ndegradation. To address these challenges, we propose a new knowledge\ndistillation paradigm termed LightST that transfers spatial and temporal\nknowledge from a high-capacity teacher to a lightweight student. Specifically,\nwe introduce a spatio-temporal knowledge distillation framework that helps\nstudent MLPs capture graph-structured global spatio-temporal patterns while\nalleviating the over-smoothing effect with adaptive knowledge distillation.\nExtensive experiments verify that LightST significantly speeds up traffic flow\npredictions by 5X to 40X compared to state-of-the-art spatio-temporal GNNs, all\nwhile maintaining superior accuracy.\n","versions":"[{'version': 'v1', 'created': 'Wed, 15 Jan 2025 04:23:10 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 06:38:35 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Zhang', 'Qianru', ''], ['Gao', 'Xinyi', ''], ['Wang', 'Haixin', ''], ['Yiu', 'Siu-Ming', ''], ['Yin', 'Hongzhi', '']]","extracted_entities":"[{'text': 'Graph neural networks', 'label': 'Neural Language Model'}, {'text': 'GNNs', 'label': 'Neural Language Model'}, {'text': 'scalability\\nconstraints', 'label': 'Scaling law'}, {'text': 'GNNs', 'label': 'Neural Language Model'}, {'text': 'LightST', 'label': 'Knowledge distillation'}, {'text': 'spatio-temporal knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'adaptive knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'LightST', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"adaptive knowledge distillation","similarity_score":0.9114365578}
{"id":2502.07938,"submitter":"Andrianos Michail","authors":"Andrianos Michail, Corina Julia Racl\\'e, Juri Opitz, Simon Clematide","title":"Adapting Multilingual Embedding Models to Historical Luxembourgish","comments":"To appear in LaTeCH-CLfL 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The growing volume of digitized historical texts requires effective semantic\nsearch using text embeddings. However, pre-trained multilingual models face\nchallenges with historical content due to OCR noise and outdated spellings.\nThis study examines multilingual embeddings for cross-lingual semantic search\nin historical Luxembourgish (LB), a low-resource language. We collect\nhistorical Luxembourgish news articles from various periods and use GPT-4o for\nsentence segmentation and translation, generating 20,000 parallel training\nsentences per language pair. Additionally, we create a semantic search\n(Historical LB Bitext Mining) evaluation set and find that existing models\nperform poorly on cross-lingual search for historical Luxembourgish. Using our\nhistorical and additional modern parallel training data, we adapt several\nmultilingual embedding models through contrastive learning or knowledge\ndistillation and increase accuracy significantly for all models. We release our\nadapted models and historical Luxembourgish-German\/French\/English bitexts to\nsupport further research.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Feb 2025 20:35:29 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Feb 2025 10:38:40 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 13:19:30 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Michail', 'Andrianos', ''], ['Racl\u00e9', 'Corina Julia', ''], ['Opitz', 'Juri', ''], ['Clematide', 'Simon', '']]","extracted_entities":"[{'text': 'text embeddings', 'label': 'Embedding'}, {'text': 'multilingual embeddings', 'label': 'Embedding'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'contrastive learning', 'label': 'Few-shot Learning'}, {'text': 'knowledge\\ndistillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge\ndistillation","similarity_score":1.0000002384}
{"id":2502.11253,"submitter":"Avimita Chatterjee","authors":"Avimita Chatterjee, Archisman Ghosh and Swaroop Ghosh","title":"The Q-Spellbook: Crafting Surface Code Layouts and Magic State Protocols\n  for Large-Scale Quantum Computing","comments":"11 pages, 8 figures, 5 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Quantum error correction is a cornerstone of reliable quantum computing, with\nsurface codes emerging as a prominent method for protecting quantum\ninformation. Surface codes are efficient for Clifford gates but require magic\nstate distillation protocols to process non-Clifford gates, such as T gates,\nessential for universal quantum computation. In large-scale quantum\narchitectures capable of correcting arbitrary circuits, specialized surface\ncodes for data qubits and distinct codes for magic state distillation are\nneeded. These architectures can be organized into data blocks and distillation\nblocks. The system works by having distillation blocks produce magic states and\ndata blocks consume them, causing stalls due to either a shortage or excess of\nmagic states. This bottleneck presents an opportunity to optimize quantum space\nby balancing data and distillation blocks. While prior research offers insights\ninto selecting distillation protocols and estimating qubit requirements, it\nlacks a tailored optimization approach. We present a framework for optimizing\nlarge-scale quantum architectures, focusing on data block layouts and magic\nstate distillation protocols. We evaluate three data block layouts and four\ndistillation protocols under three optimization strategies: minimizing tiles,\nminimizing steps, and achieving a balanced trade-off. Through a comparative\nanalysis of brute force, dynamic programming, greedy, and random algorithms, we\nfind that brute force delivers optimal results, while greedy deviates by 7% for\nminimizing steps and dynamic programming matches brute force in tile\nminimization. We observe that total steps increase with columns, while total\ntiles scale with qubits. Finally, we propose a heuristic to help users select\nalgorithms suited to their objectives, enabling scalable and efficient quantum\narchitectures.\n","versions":"[{'version': 'v1', 'created': 'Sun, 16 Feb 2025 20:13:51 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 19:11:02 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Chatterjee', 'Avimita', ''], ['Ghosh', 'Archisman', ''], ['Ghosh', 'Swaroop', '']]","extracted_entities":"[{'text': 'Clifford gates', 'label': 'quantisation'}, {'text': 'magic\\nstate distillation protocols', 'label': 'Knowledge distillation'}, {'text': 'magic state distillation', 'label': 'Knowledge distillation'}, {'text': 'distillation blocks', 'label': 'Knowledge distillation'}, {'text': 'distillation protocols', 'label': 'Knowledge distillation'}, {'text': 'magic\\nstate distillation protocols', 'label': 'Knowledge distillation'}, {'text': 'minimizing steps', 'label': 'Fine-tuning'}]","assigned_concept":"Knowledge distillation","matched_keyword":"distillation protocols","similarity_score":0.7241358161}
{"id":2502.15681,"submitter":"Yilun Xu","authors":"Yilun Xu, Weili Nie, Arash Vahdat","title":"One-step Diffusion Models with $f$-Divergence Distribution Matching","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Sampling from diffusion models involves a slow iterative process that hinders\ntheir practical deployment, especially for interactive applications. To\naccelerate generation speed, recent approaches distill a multi-step diffusion\nmodel into a single-step student generator via variational score distillation,\nwhich matches the distribution of samples generated by the student to the\nteacher's distribution. However, these approaches use the reverse\nKullback-Leibler (KL) divergence for distribution matching which is known to be\nmode seeking. In this paper, we generalize the distribution matching approach\nusing a novel $f$-divergence minimization framework, termed $f$-distill, that\ncovers different divergences with different trade-offs in terms of mode\ncoverage and training variance. We derive the gradient of the $f$-divergence\nbetween the teacher and student distributions and show that it is expressed as\nthe product of their score differences and a weighting function determined by\ntheir density ratio. This weighting function naturally emphasizes samples with\nhigher density in the teacher distribution, when using a less mode-seeking\ndivergence. We observe that the popular variational score distillation approach\nusing the reverse-KL divergence is a special case within our framework.\nEmpirically, we demonstrate that alternative $f$-divergences, such as\nforward-KL and Jensen-Shannon divergences, outperform the current best\nvariational score distillation methods across image generation tasks. In\nparticular, when using Jensen-Shannon divergence, $f$-distill achieves current\nstate-of-the-art one-step generation performance on ImageNet64 and zero-shot\ntext-to-image generation on MS-COCO. Project page:\nhttps:\/\/research.nvidia.com\/labs\/genair\/f-distill\n","versions":"[{'version': 'v1', 'created': 'Fri, 21 Feb 2025 18:59:20 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 22:53:27 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Xu', 'Yilun', ''], ['Nie', 'Weili', ''], ['Vahdat', 'Arash', '']]","extracted_entities":"[{'text': 'variational score distillation', 'label': 'Knowledge distillation'}, {'text': 'zero-shot\\ntext-to-image generation', 'label': 'Few-shot Learning'}]","assigned_concept":"Knowledge distillation","matched_keyword":"variational score distillation","similarity_score":0.5865233541}
{"id":2503.05005,"submitter":"Benyamin Jamialahmadi","authors":"Benyamin Jamialahmadi, Parsa Kavehzadeh, Mehdi Rezagholizadeh, Parsa\n  Farinneya, Hossein Rajabzadeh, Aref Jafari, Boxing Chen, and Marzieh S.Tahaei","title":"Balcony: A Lightweight Approach to Dynamic Inference of Generative\n  Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Deploying large language models (LLMs) in real-world applications is often\nhindered by strict computational and latency constraints. While dynamic\ninference offers the flexibility to adjust model behavior based on varying\nresource budgets, existing methods are frequently limited by hardware\ninefficiencies or performance degradation. In this paper, we introduce Balcony,\na simple yet highly effective framework for depth-based dynamic inference. By\nfreezing the pretrained LLM and inserting additional transformer layers at\nselected exit points, Balcony maintains the full model's performance while\nenabling real-time adaptation to different computational budgets. These\nadditional layers are trained using a straightforward self-distillation loss,\naligning the sub-model outputs with those of the full model. This approach\nrequires significantly fewer training tokens and tunable parameters,\ndrastically reducing computational costs compared to prior methods. When\napplied to the LLaMA3-8B model, using only 0.2% of the original pretraining\ndata, Balcony achieves minimal performance degradation while enabling\nsignificant speedups. Remarkably, we show that Balcony outperforms\nstate-of-the-art methods such as Flextron and Layerskip as well as other\nleading compression techniques on multiple models and at various scales, across\na variety of benchmarks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 6 Mar 2025 22:09:55 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 18:52:15 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Jamialahmadi', 'Benyamin', ''], ['Kavehzadeh', 'Parsa', ''], ['Rezagholizadeh', 'Mehdi', ''], ['Farinneya', 'Parsa', ''], ['Rajabzadeh', 'Hossein', ''], ['Jafari', 'Aref', ''], ['Chen', 'Boxing', ''], ['Tahaei', 'Marzieh S.', '']]","extracted_entities":"[{'text': 'Balcony', 'label': 'LLM'}, {'text': 'self-distillation loss', 'label': 'Knowledge distillation'}, {'text': 'Balcony', 'label': 'LLM'}]","assigned_concept":"Knowledge distillation","matched_keyword":"self-distillation loss","similarity_score":0.5811356306}
{"id":2503.06045,"submitter":"Avimita Chatterjee","authors":"Avimita Chatterjee, Archisman Ghosh and Swaroop Ghosh","title":"The Art of Optimizing T-Depth for Quantum Error Correction in\n  Large-Scale Quantum Computing","comments":"6 pages, 5 figures, 3 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Quantum Error Correction (QEC), combined with magic state distillation,\nensures fault tolerance in large-scale quantum computation. To apply QEC, a\ncircuit must first be transformed into a non-Clifford (or T) gate set. T-depth,\nthe number of sequential T-gate layers, determines the magic state cost,\nimpacting both spatial and temporal overhead. Minimizing T-depth is crucial for\noptimizing resource efficiency in fault-tolerant quantum computing. While QEC\nscalability has been widely studied, T-depth reduction remains an overlooked\nchallenge. We establish that T-depth reduction is an NP-hard problem and\nsystematically evaluate multiple approximation techniques: greedy,\ndivide-and-conquer, Lookahead-based brute force, and graph-based. The\nLookahead-based brute-force algorithm (partition size 4) performs best,\noptimizing 90\\% of reducible cases (i.e., circuits where at least one algorithm\nachieved optimization) with an average T-depth reduction of around 51\\%.\nAdditionally, we introduce an expansion factor-based identity gate insertion\nstrategy, leveraging controlled redundancy to achieve deeper reductions in\ncircuits initially classified as non-reducible. With this approach, we\nsuccessfully convert up to 25\\% of non-reducible circuits into reducible ones,\nwhile achieving an additional average reduction of up to 11.8\\%. Furthermore,\nwe analyze the impact of different expansion factor values and explore how\nvarying the partition size in the Lookahead-based brute-force algorithm\ninfluences the quality of T-depth reduction.\n","versions":"[{'version': 'v1', 'created': 'Sat, 8 Mar 2025 03:48:21 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 19:08:09 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Chatterjee', 'Avimita', ''], ['Ghosh', 'Archisman', ''], ['Ghosh', 'Swaroop', '']]","extracted_entities":"[{'text': 'magic state distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"magic state distillation","similarity_score":0.6210391521}
{"id":2503.06398,"submitter":"Tao Feng","authors":"Tao Feng, Yunke Zhang, Huandong Wang, Yong Li","title":"Causality Enhanced Origin-Destination Flow Prediction in Data-Scarce\n  Cities","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Accurate origin-destination (OD) flow prediction is of great importance to\ndeveloping cities, as it can contribute to optimize urban structures and\nlayouts. However, with the common issues of missing regional features and\nlacking OD flow data, it is quite daunting to predict OD flow in developing\ncities. To address this challenge, we propose a novel Causality-Enhanced OD\nFlow Prediction (CE-OFP), a unified framework that aims to transfer urban\nknowledge between cities and achieve accuracy improvements in OD flow\npredictions across data-scarce cities. In specific, we propose a novel\nreinforcement learning model to discover universal causalities among urban\nfeatures in data-rich cities and build corresponding causal graphs. Then, we\nfurther build Causality-Enhanced Variational Auto-Encoder (CE-VAE) to\nincorporate causal graphs for effective feature reconstruction in data-scarce\ncities. Finally, with the reconstructed features, we devise a knowledge\ndistillation method with a graph attention network to migrate the OD prediction\nmodel from data-rich cities to data-scare cities. Extensive experiments on two\npairs of real-world datasets validate that the proposed CE-OFP remarkably\noutperforms state-of-the-art baselines, which can reduce the RMSE of OD flow\nprediction for data-scarce cities by up to 11%.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 02:36:36 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Feng', 'Tao', ''], ['Zhang', 'Yunke', ''], ['Wang', 'Huandong', ''], ['Li', 'Yong', '']]","extracted_entities":"[{'text': 'knowledge\\ndistillation method', 'label': 'Knowledge distillation'}, {'text': 'graph attention network', 'label': 'Attention mechanism'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge\ndistillation method","similarity_score":0.9219750166}
{"id":2503.06499,"submitter":"Xukun Zhou","authors":"Xukun Zhou, Fengxin Li, Ming Chen, Yan Zhou, Pengfei Wan, Di Zhang,\n  Hongyan Liu, Jun He, Zhaoxin Fan","title":"ExGes: Expressive Human Motion Retrieval and Modulation for Audio-Driven\n  Gesture Synthesis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Audio-driven human gesture synthesis is a crucial task with broad\napplications in virtual avatars, human-computer interaction, and creative\ncontent generation. Despite notable progress, existing methods often produce\ngestures that are coarse, lack expressiveness, and fail to fully align with\naudio semantics. To address these challenges, we propose ExGes, a novel\nretrieval-enhanced diffusion framework with three key designs: (1) a Motion\nBase Construction, which builds a gesture library using training dataset; (2) a\nMotion Retrieval Module, employing constrative learning and momentum\ndistillation for fine-grained reference poses retreiving; and (3) a Precision\nControl Module, integrating partial masking and stochastic masking to enable\nflexible and fine-grained control. Experimental evaluations on BEAT2\ndemonstrate that ExGes reduces Fr\\'echet Gesture Distance by 6.2\\% and improves\nmotion diversity by 5.3\\% over EMAGE, with user studies revealing a 71.3\\%\npreference for its naturalness and semantic relevance. Code will be released\nupon acceptance.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 07:59:39 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhou', 'Xukun', ''], ['Li', 'Fengxin', ''], ['Chen', 'Ming', ''], ['Zhou', 'Yan', ''], ['Wan', 'Pengfei', ''], ['Zhang', 'Di', ''], ['Liu', 'Hongyan', ''], ['He', 'Jun', ''], ['Fan', 'Zhaoxin', '']]","extracted_entities":"[{'text': 'Motion\\nBase Construction', 'label': 'Embedding'}, {'text': 'Motion Retrieval Module', 'label': 'Embedding'}, {'text': 'momentum\\ndistillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"momentum\ndistillation","similarity_score":0.5743275285}
{"id":2503.06511,"submitter":"Bohan Lin","authors":"Yiting Zheng, Bohan Lin, Jinqian Chen, Jihua Zhu","title":"HFedCKD: Toward Robust Heterogeneous Federated Learning via Data-free\n  Knowledge Distillation and Two-way Contrast","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Most current federated learning frameworks are modeled as static processes,\nignoring the dynamic characteristics of the learning system. Under the limited\ncommunication budget of the central server, the flexible model architecture of\na large number of clients participating in knowledge transfer requires a lower\nparticipation rate, active clients have uneven contributions, and the client\nscale seriously hinders the performance of FL. We consider a more general and\npractical federation scenario and propose a system heterogeneous federation\nmethod based on data-free knowledge distillation and two-way contrast\n(HFedCKD). We apply the Inverse Probability Weighted Distillation (IPWD)\nstrategy to the data-free knowledge transfer framework. The generator completes\nthe data features of the nonparticipating clients. IPWD implements a dynamic\nevaluation of the prediction contribution of each client under different data\ndistributions. Based on the antibiased weighting of its prediction loss, the\nweight distribution of each client is effectively adjusted to fairly integrate\nthe knowledge of participating clients. At the same time, the local model is\nsplit into a feature extractor and a classifier. Through differential contrast\nlearning, the feature extractor is aligned with the global model in the feature\nspace, while the classifier maintains personalized decision-making\ncapabilities. HFedCKD effectively alleviates the knowledge offset caused by a\nlow participation rate under data-free knowledge distillation and improves the\nperformance and stability of the model. We conduct extensive experiments on\nimage and IoT datasets to comprehensively evaluate and verify the\ngeneralization and robustness of the proposed HFedCKD framework.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 08:32:57 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zheng', 'Yiting', ''], ['Lin', 'Bohan', ''], ['Chen', 'Jinqian', ''], ['Zhu', 'Jihua', '']]","extracted_entities":"[{'text': 'data-free knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'Inverse Probability Weighted Distillation', 'label': 'Knowledge distillation'}, {'text': 'differential contrast\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'HFedCKD', 'label': 'Few-shot Learning'}, {'text': 'data-free knowledge distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"data-free knowledge distillation","similarity_score":0.8578619361}
{"id":2503.06554,"submitter":"Chengcheng Zhu","authors":"Chengcheng Zhu, Jiale Zhang, Di Wu, Guodong Long","title":"BDPFL: Backdoor Defense for Personalized Federated Learning via\n  Explainable Distillation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Federated learning is a distributed learning paradigm that facilitates the\ncollaborative training of a global model across multiple clients while\npreserving the privacy of local datasets. To address inherent challenges\nrelated to data heterogeneity and satisfy personalized needs, a new direction\nwithin FL, known as personalized Federated Learning (pFL), has gradually\nevolved. Extensive attention has been directed toward developing novel\nframeworks and methods to enhance the performance of pFL. Regrettably, the\naspect of security in pFL has been largely overlooked. Our objective is to fill\nthis gap. Similar to FL, pFL is susceptible to backdoor attacks. However,\nexisting backdoor defense strategies are primarily tailored to general FL\nframeworks, and pFL lacks robustness against backdoor attacks. We propose a\nnovel, backdoor-robust pFL framework named BDPFL to address these challenges.\nFirst, BDPFL introduces layer-wise mutual distillation that enables clients to\nlearn their personalized local models while mitigating potential backdoors.\nThen, BDPFL employs explanation heatmap to learn high-quality intermediate\nrepresentations and enhance the effect of eliminating deeper and more\nentrenched backdoors. Moreover, we perform empirical evaluations of BDPFL's\nperformance on three datasets and compare BDPFL with four backdoor defense\nmethods. The experiments demonstrate that BDPFL outperforms baseline methods\nand is effective under various settings.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 10:59:18 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhu', 'Chengcheng', ''], ['Zhang', 'Jiale', ''], ['Wu', 'Di', ''], ['Long', 'Guodong', '']]","extracted_entities":"[{'text': 'Federated learning', 'label': 'Few-shot Learning'}, {'text': 'pFL', 'label': 'Few-shot Learning'}, {'text': 'layer-wise mutual distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"layer-wise mutual distillation","similarity_score":0.6190372109}
{"id":2503.06559,"submitter":"Yuzheng Wang","authors":"Yuzheng Wang, Zhaoyu Chen, Dingkang Yang, Yuanhang Wang, Lizhe Qi","title":"MMARD: Improving the Min-Max Optimization Process in Adversarial\n  Robustness Distillation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Adversarial Robustness Distillation (ARD) is a promising task to boost the\nrobustness of small-capacity models with the guidance of the pre-trained robust\nteacher. The ARD can be summarized as a min-max optimization process, i.e.,\nsynthesizing adversarial examples (inner) & training the student (outer).\nAlthough competitive robustness performance, existing ARD methods still have\nissues. In the inner process, the synthetic training examples are far from the\nteacher's decision boundary leading to important robust information missing. In\nthe outer process, the student model is decoupled from learning natural and\nrobust scenarios, leading to the robustness saturation, i.e., student\nperformance is highly susceptible to customized teacher selection. To tackle\nthese issues, this paper proposes a general Min-Max optimization Adversarial\nRobustness Distillation (MMARD) method. For the inner process, we introduce the\nteacher's robust predictions, which drive the training examples closer to the\nteacher's decision boundary to explore more robust knowledge. For the outer\nprocess, we propose a structured information modeling method based on\ntriangular relationships to measure the mutual information of the model in\nnatural and robust scenarios and enhance the model's ability to understand\nmulti-scenario mapping relationships. Experiments show our MMARD achieves\nstate-of-the-art performance on multiple benchmarks. Besides, MMARD is\nplug-and-play and convenient to combine with existing methods.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 11:15:02 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Wang', 'Yuzheng', ''], ['Chen', 'Zhaoyu', ''], ['Yang', 'Dingkang', ''], ['Wang', 'Yuanhang', ''], ['Qi', 'Lizhe', '']]","extracted_entities":"[{'text': 'Adversarial Robustness Distillation', 'label': 'Knowledge distillation'}, {'text': 'ARD', 'label': 'Knowledge distillation'}, {'text': 'Adversarial\\nRobustness Distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Adversarial Robustness Distillation","similarity_score":0.5289990902}
{"id":2503.06598,"submitter":"Hao Xu","authors":"Hao Xu, Tengfei Xue, Dongnan Liu, Yuqian Chen, Fan Zhang, Carl-Fredrik\n  Westin, Ron Kikinis, Lauren J. O'Donnell, Weidong Cai","title":"MultiCo3D: Multi-Label Voxel Contrast for One-Shot Incremental\n  Segmentation of 3D Neuroimages","comments":"13 pages, 6 figures, 6 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  3D neuroimages provide a comprehensive view of brain structure and function,\naiding in precise localization and functional connectivity analysis.\nSegmentation of white matter (WM) tracts using 3D neuroimages is vital for\nunderstanding the brain's structural connectivity in both healthy and diseased\nstates. One-shot Class Incremental Semantic Segmentation (OCIS) refers to\neffectively segmenting new (novel) classes using only a single sample while\nretaining knowledge of old (base) classes without forgetting. Voxel-contrastive\nOCIS methods adjust the feature space to alleviate the feature overlap problem\nbetween the base and novel classes. However, since WM tract segmentation is a\nmulti-label segmentation task, existing single-label voxel contrastive-based\nmethods may cause inherent contradictions. To address this, we propose a new\nmulti-label voxel contrast framework called MultiCo3D for one-shot class\nincremental tract segmentation. Our method utilizes uncertainty distillation to\npreserve base tract segmentation knowledge while adjusting the feature space\nwith multi-label voxel contrast to alleviate feature overlap when learning\nnovel tracts and dynamically weighting multi losses to balance overall loss. We\ncompare our method against several state-of-the-art (SOTA) approaches. The\nexperimental results show that our method significantly enhances one-shot class\nincremental tract segmentation accuracy across five different experimental\nsetups on HCP and Preto datasets.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 13:06:20 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Xu', 'Hao', ''], ['Xue', 'Tengfei', ''], ['Liu', 'Dongnan', ''], ['Chen', 'Yuqian', ''], ['Zhang', 'Fan', ''], ['Westin', 'Carl-Fredrik', ''], ['Kikinis', 'Ron', ''], [\"O'Donnell\", 'Lauren J.', ''], ['Cai', 'Weidong', '']]","extracted_entities":"[{'text': 'uncertainty distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"uncertainty distillation","similarity_score":0.6661061049}
{"id":2503.06601,"submitter":"Yanqing Shen","authors":"Yanqing Shen, Sanping Zhou, Jingwen Fu, Ruotong Wang, Shitao Chen, and\n  Nanning Zheng","title":"StructVPR++: Distill Structural and Semantic Knowledge with Weighting\n  Samples for Visual Place Recognition","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Visual place recognition is a challenging task for autonomous driving and\nrobotics, which is usually considered as an image retrieval problem. A commonly\nused two-stage strategy involves global retrieval followed by re-ranking using\npatch-level descriptors. Most deep learning-based methods in an end-to-end\nmanner cannot extract global features with sufficient semantic information from\nRGB images. In contrast, re-ranking can utilize more explicit structural and\nsemantic information in one-to-one matching process, but it is time-consuming.\nTo bridge the gap between global retrieval and re-ranking and achieve a good\ntrade-off between accuracy and efficiency, we propose StructVPR++, a framework\nthat embeds structural and semantic knowledge into RGB global representations\nvia segmentation-guided distillation. Our key innovation lies in decoupling\nlabel-specific features from global descriptors, enabling explicit semantic\nalignment between image pairs without requiring segmentation during deployment.\nFurthermore, we introduce a sample-wise weighted distillation strategy that\nprioritizes reliable training pairs while suppressing noisy ones. Experiments\non four benchmarks demonstrate that StructVPR++ surpasses state-of-the-art\nglobal methods by 5-23% in Recall@1 and even outperforms many two-stage\napproaches, achieving real-time efficiency with a single RGB input.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 13:12:34 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Shen', 'Yanqing', ''], ['Zhou', 'Sanping', ''], ['Fu', 'Jingwen', ''], ['Wang', 'Ruotong', ''], ['Chen', 'Shitao', ''], ['Zheng', 'Nanning', '']]","extracted_entities":"[{'text': 'segmentation-guided distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"segmentation-guided distillation","similarity_score":0.5341175795}
{"id":2503.06652,"submitter":"Yihong Luo","authors":"Yihong Luo, Tianyang Hu, Yifan Song, Jiacheng Sun, Zhenguo Li, Jing\n  Tang","title":"Adding Additional Control to One-Step Diffusion with Joint Distribution\n  Matching","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  While diffusion distillation has enabled one-step generation through methods\nlike Variational Score Distillation, adapting distilled models to emerging new\ncontrols -- such as novel structural constraints or latest user preferences --\nremains challenging. Conventional approaches typically requires modifying the\nbase diffusion model and redistilling it -- a process that is both\ncomputationally intensive and time-consuming. To address these challenges, we\nintroduce Joint Distribution Matching (JDM), a novel approach that minimizes\nthe reverse KL divergence between image-condition joint distributions. By\nderiving a tractable upper bound, JDM decouples fidelity learning from\ncondition learning. This asymmetric distillation scheme enables our one-step\nstudent to handle controls unknown to the teacher model and facilitates\nimproved classifier-free guidance (CFG) usage and seamless integration of human\nfeedback learning (HFL). Experimental results demonstrate that JDM surpasses\nbaseline methods such as multi-step ControlNet by mere one-step in most cases,\nwhile achieving state-of-the-art performance in one-step text-to-image\nsynthesis through improved usage of CFG or HFL integration.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 15:06:50 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 12:24:26 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Luo', 'Yihong', ''], ['Hu', 'Tianyang', ''], ['Song', 'Yifan', ''], ['Sun', 'Jiacheng', ''], ['Li', 'Zhenguo', ''], ['Tang', 'Jing', '']]","extracted_entities":"[{'text': 'diffusion distillation', 'label': 'Knowledge distillation'}, {'text': 'Variational Score Distillation', 'label': 'Knowledge distillation'}, {'text': 'fidelity learning', 'label': 'Zero-shot Learning'}, {'text': 'condition learning', 'label': 'Zero-shot Learning'}, {'text': 'human\\nfeedback learning', 'label': 'Few-shot Learning'}, {'text': 'HFL', 'label': 'Few-shot Learning'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Variational Score Distillation","similarity_score":0.5865233541}
{"id":2303.00055,"submitter":"Kangjie Zhou","authors":"Rapha\\\"el Berthier, Andrea Montanari, Kangjie Zhou","title":"Learning time-scales in two-layers neural networks","comments":"64 pages, 15 figures","journal-ref":null,"doi":"10.1007\/s10208-024-09664-9","report-no":null,"categories":"cs.LG math.OC stat.ML","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Gradient-based learning in multi-layer neural networks displays a number of\nstriking features. In particular, the decrease rate of empirical risk is\nnon-monotone even after averaging over large batches. Long plateaus in which\none observes barely any progress alternate with intervals of rapid decrease.\nThese successive phases of learning often take place on very different time\nscales. Finally, models learnt in an early phase are typically `simpler' or\n`easier to learn' although in a way that is difficult to formalize.\n  Although theoretical explanations of these phenomena have been put forward,\neach of them captures at best certain specific regimes. In this paper, we study\nthe gradient flow dynamics of a wide two-layer neural network in\nhigh-dimension, when data are distributed according to a single-index model\n(i.e., the target function depends on a one-dimensional projection of the\ncovariates). Based on a mixture of new rigorous results, non-rigorous\nmathematical derivations, and numerical simulations, we propose a scenario for\nthe learning dynamics in this setting. In particular, the proposed evolution\nexhibits separation of timescales and intermittency. These behaviors arise\nnaturally because the population gradient flow can be recast as a singularly\nperturbed dynamical system.\n","versions":"[{'version': 'v1', 'created': 'Tue, 28 Feb 2023 19:52:26 GMT'}, {'version': 'v2', 'created': 'Thu, 16 Mar 2023 21:31:45 GMT'}, {'version': 'v3', 'created': 'Wed, 17 Apr 2024 18:36:27 GMT'}, {'version': 'v4', 'created': 'Sun, 9 Mar 2025 05:50:54 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Berthier', 'Rapha\u00ebl', ''], ['Montanari', 'Andrea', ''], ['Zhou', 'Kangjie', '']]","extracted_entities":"[{'text': 'Gradient-based learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"Gradient-based learning","similarity_score":0.5312626958}
{"id":2305.10397,"submitter":"Yifan Zhang","authors":"Yifan Zhang, Jingqin Yang, Zhiquan Tan, Yang Yuan","title":"RelationMatch: Matching In-batch Relationships for Semi-supervised\n  Learning","comments":"21 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Semi-supervised learning has emerged as a pivotal approach for leveraging\nscarce labeled data alongside abundant unlabeled data. Despite significant\nprogress, prevailing SSL methods predominantly enforce consistency between\ndifferent augmented views of individual samples, thereby overlooking the rich\nrelational structure inherent within a mini-batch. In this paper, we present\nRelationMatch, a novel SSL framework that explicitly enforces in-batch\nrelational consistency through a Matrix Cross-Entropy (MCE) loss function. The\nproposed MCE loss is rigorously derived from both matrix analysis and\ninformation geometry perspectives, ensuring theoretical soundness and practical\nefficacy. Extensive empirical evaluations on standard benchmarks, including a\nnotable 15.21% accuracy improvement over FlexMatch on STL-10, demonstrate that\nRelationMatch not only advances state-of-the-art performance but also provides\na principled foundation for incorporating relational cues in SSL.\n","versions":"[{'version': 'v1', 'created': 'Wed, 17 May 2023 17:37:48 GMT'}, {'version': 'v2', 'created': 'Tue, 30 May 2023 14:55:06 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 02:45:16 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Zhang', 'Yifan', ''], ['Yang', 'Jingqin', ''], ['Tan', 'Zhiquan', ''], ['Yuan', 'Yang', '']]","extracted_entities":"[{'text': 'Semi-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'RelationMatch', 'label': 'Foundation Model'}, {'text': 'FlexMatch', 'label': 'Foundation Model'}, {'text': 'RelationMatch', 'label': 'Foundation Model'}]","assigned_concept":"Few-shot Learning","matched_keyword":"Semi-supervised learning","similarity_score":0.5018399954}
{"id":2403.05158,"submitter":"Zuguang Li","authors":"Zuguang Li, Wen Wu, Shaohua Wu, and Wei Wang","title":"Adaptive Split Learning over Energy-Constrained Wireless Edge Networks","comments":"6 pages, 5 figures, 20 conferences","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.NI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Split learning (SL) is a promising approach for training artificial\nintelligence (AI) models, in which devices collaborate with a server to train\nan AI model in a distributed manner, based on a same fixed split point.\nHowever, due to the device heterogeneity and variation of channel conditions,\nthis way is not optimal in training delay and energy consumption. In this\npaper, we design an adaptive split learning (ASL) scheme which can dynamically\nselect split points for devices and allocate computing resource for the server\nin wireless edge networks. We formulate an optimization problem to minimize the\naverage training latency subject to long-term energy consumption constraint.\nThe difficulties in solving this problem are the lack of future information and\nmixed integer programming (MIP). To solve it, we propose an online algorithm\nleveraging the Lyapunov theory, named OPEN, which decomposes it into a new MIP\nproblem only with the current information. Then, a two-layer optimization\nmethod is proposed to solve the MIP problem. Extensive simulation results\ndemonstrate that the ASL scheme can reduce the average training delay and\nenergy consumption by 53.7% and 22.1%, respectively, as compared to the\nexisting SL schemes.\n","versions":"[{'version': 'v1', 'created': 'Fri, 8 Mar 2024 08:51:37 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 13:27:47 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Zuguang', ''], ['Wu', 'Wen', ''], ['Wu', 'Shaohua', ''], ['Wang', 'Wei', '']]","extracted_entities":"[{'text': 'Split learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"Split learning","similarity_score":0.5144664049}
{"id":2404.02493,"submitter":"Kai Jiang","authors":"Chen Cui, Kai Jiang and Shi Shu","title":"A Neural Multigrid Solver for Helmholtz Equations with High Wavenumber\n  and Heterogeneous Media","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.NA","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper, we propose a deep learning-enhanced multigrid solver for\nhigh-frequency and heterogeneous Helmholtz equations. By applying spectral\nanalysis, we categorize the iteration error into characteristic and\nnon-characteristic components. We eliminate the non-characteristic components\nby a multigrid wave cycle, which employs carefully selected smoothers on each\ngrid. We diminish the characteristic components by a learned phase function and\nthe approximate solution of an advection-diffusion-reaction (ADR) equation,\nwhich is solved using another multigrid V-cycle on a coarser scale, referred to\nas the ADR cycle. The resulting solver, termed Wave-ADR-NS, enables the\nhandling of error components with varying frequencies and overcomes constraints\non the number of grid points per wavelength on coarse grids. Furthermore, we\nprovide an efficient implementation using differentiable programming, making\nWave-ADR-NS an end-to-end Helmholtz solver that incorporates parameters learned\nthrough a semi-supervised training. Wave-ADR-NS demonstrates robust\ngeneralization capabilities for both in-distribution and out-of-distribution\nvelocity fields of varying difficulty. Comparative experiments with other\nmultigrid methods validate its superior performance in solving heterogeneous 2D\nHelmholtz equations with wavenumbers exceeding 2000.\n","versions":"[{'version': 'v1', 'created': 'Wed, 3 Apr 2024 06:08:00 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 07:00:39 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Cui', 'Chen', ''], ['Jiang', 'Kai', ''], ['Shu', 'Shi', '']]","extracted_entities":"[{'text': 'semi-supervised training', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"semi-supervised training","similarity_score":0.5018717051}
{"id":2408.10007,"submitter":"Xuechao Chen","authors":"Xuechao Chen, Ying Chen, Jialin Li, Qiang Nie, Hanqiu Deng, Yong Liu,\n  Qixing Huang, Yang Li","title":"P3P: Pseudo-3D Pre-training for Scaling 3D Masked Autoencoders","comments":"Under review. Pre-print","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Pre-training in 3D is pivotal for advancing 3D perception tasks. However, the\nscarcity of clean 3D data poses significant challenges for scaling 3D\npre-training efforts. Drawing inspiration from semi-supervised learning, which\neffectively combines limited labeled data with abundant unlabeled data, we\nintroduce an innovative self-supervised pre-training framework. This framework\nleverages both authentic 3D data and pseudo-3D data generated from images using\na robust depth estimation model. Another critical challenge is the efficiency\nof the pre-training process. Existing approaches, such as Point-BERT and\nPoint-MAE, utilize k-nearest neighbors for 3D token embedding, resulting in\nquadratic time complexity. To address this, we propose a novel token embedding\nstrategy with linear time complexity, coupled with a training-efficient 2D\nreconstruction target. Our method not only achieves state-of-the-art\nperformance in 3D classification, detection, and few-shot learning but also\nensures high efficiency in both pre-training and downstream fine-tuning\nprocesses.\n","versions":"[{'version': 'v1', 'created': 'Mon, 19 Aug 2024 13:59:53 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 14:13:37 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Chen', 'Xuechao', ''], ['Chen', 'Ying', ''], ['Li', 'Jialin', ''], ['Nie', 'Qiang', ''], ['Deng', 'Hanqiu', ''], ['Liu', 'Yong', ''], ['Huang', 'Qixing', ''], ['Li', 'Yang', '']]","extracted_entities":"[{'text': 'semi-supervised learning', 'label': 'Zero-shot Learning'}, {'text': 'Point-BERT', 'label': 'BERT'}, {'text': 'Point-MAE', 'label': 'BERT'}, {'text': '3D token embedding', 'label': 'Embedding'}, {'text': 'few-shot learning', 'label': 'Few-shot Learning'}, {'text': 'downstream fine-tuning\\nprocesses', 'label': 'Fine-tuning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"few-shot learning","similarity_score":1.0}
{"id":2409.19583,"submitter":"Jun Liu","authors":"Jun Liu, Geng Yuan, Weihao Zeng, Hao Tang, Wenbin Zhang, Xue Lin,\n  XiaoLin Xu, Dong Huang and Yanzhi Wang","title":"Brain Tumor Classification on MRI in Light of Molecular Markers","comments":"ICAI'22 - The 24th International Conference on Artificial\n  Intelligence, The 2022 World Congress in Computer Science, Computer\n  Engineering, & Applied Computing (CSCE'22), Las Vegas, USA. The paper\n  acceptance rate 17% for regular papers. The publication of the CSCE 2022\n  conference proceedings has been delayed due to the pandemic","journal-ref":"Springer Nature - Book Series: Transactions on Computational\n  Science & Computational Intelligence, 2022","doi":null,"report-no":null,"categories":"eess.IV cs.CV cs.LG q-bio.QM","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In research findings, co-deletion of the 1p\/19q gene is associated with\nclinical outcomes in low-grade gliomas. The ability to predict 1p19q status is\ncritical for treatment planning and patient follow-up. This study aims to\nutilize a specially MRI-based convolutional neural network for brain cancer\ndetection. Although public networks such as RestNet and AlexNet can effectively\ndiagnose brain cancers using transfer learning, the model includes quite a few\nweights that have nothing to do with medical images. As a result, the\ndiagnostic results are unreliable by the transfer learning model. To deal with\nthe problem of trustworthiness, we create the model from the ground up, rather\nthan depending on a pre-trained model. To enable flexibility, we combined\nconvolution stacking with a dropout and full connect operation, it improved\nperformance by reducing overfitting. During model training, we also supplement\nthe given dataset and inject Gaussian noise. We use three--fold\ncross-validation to train the best selection model. Comparing InceptionV3,\nVGG16, and MobileNetV2 fine-tuned with pre-trained models, our model produces\nbetter results. On an validation set of 125 codeletion vs. 31 not codeletion\nimages, the proposed network achieves 96.37\\% percent F1-score, 97.46\\% percent\nprecision, and 96.34\\% percent recall when classifying 1p\/19q codeletion and\nnot codeletion images.\n","versions":"[{'version': 'v1', 'created': 'Sun, 29 Sep 2024 07:04:26 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 17:01:47 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Liu', 'Jun', ''], ['Yuan', 'Geng', ''], ['Zeng', 'Weihao', ''], ['Tang', 'Hao', ''], ['Zhang', 'Wenbin', ''], ['Lin', 'Xue', ''], ['Xu', 'XiaoLin', ''], ['Huang', 'Dong', ''], ['Wang', 'Yanzhi', '']]","extracted_entities":"[{'text': 'transfer learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"transfer learning","similarity_score":0.5694054365}
{"id":2410.10663,"submitter":"Zhengwei Yang","authors":"Zhengwei Yang, Yuke Li, Qiang Sun, Basura Fernando, Heng Huang, Zheng\n  Wang","title":"Cross-Modal Few-Shot Learning: a Generative Transfer Learning Framework","comments":"15 pages, 9 figures, 7 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Most existing studies on few-shot learning focus on unimodal settings, where\nmodels are trained to generalize to unseen data using a limited amount of\nlabeled examples from a single modality. However, real-world data are\ninherently multi-modal, and such unimodal approaches limit the practical\napplications of few-shot learning. To bridge this gap, this paper introduces\nthe Cross-modal Few-Shot Learning (CFSL) task, which aims to recognize\ninstances across multiple modalities while relying on scarce labeled data. This\ntask presents unique challenges compared to classical few-shot learning arising\nfrom the distinct visual attributes and structural disparities inherent to each\nmodality. To tackle these challenges, we propose a Generative Transfer Learning\n(GTL) framework by simulating how humans abstract and generalize concepts.\nSpecifically, the GTL jointly estimates the latent shared concept across\nmodalities and the in-modality disturbance through a generative structure.\nEstablishing the relationship between latent concepts and visual content among\nabundant unimodal data enables GTL to effectively transfer knowledge from\nunimodal to novel multimodal data, as humans did. Comprehensive experiments\ndemonstrate that the GTL achieves state-of-the-art performance across seven\nmulti-modal datasets across RGB-Sketch, RGB-Infrared, and RGB-Depth.\n","versions":"[{'version': 'v1', 'created': 'Mon, 14 Oct 2024 16:09:38 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 08:58:21 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Yang', 'Zhengwei', ''], ['Li', 'Yuke', ''], ['Sun', 'Qiang', ''], ['Fernando', 'Basura', ''], ['Huang', 'Heng', ''], ['Wang', 'Zheng', '']]","extracted_entities":"[{'text': 'few-shot learning', 'label': 'Few-shot Learning'}, {'text': 'few-shot learning', 'label': 'Few-shot Learning'}, {'text': 'Cross-modal Few-Shot Learning', 'label': 'Few-shot Learning'}, {'text': 'few-shot learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"few-shot learning","similarity_score":1.0}
{"id":2411.011,"submitter":"Xinran Miao","authors":"Xinran Miao, Jiwei Zhao, Hyunseung Kang","title":"Transfer Learning Between U.S. Presidential Elections: How Should We\n  Learn From A 2020 Ad Campaign To Inform 2024 Ad Campaigns?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.AP stat.ME","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  For the 2024 U.S. presidential election, would negative, digital ads against\nDonald Trump impact voter turnout in Pennsylvania (PA), a key \"tipping point''\nstate? The gold standard to address this question, a randomized experiment\nwhere voters get randomized to different ads, yields unbiased estimates of the\nad effect, but is very expensive. Instead, we propose a less-than-ideal, but\nsignificantly cheaper and faster framework based on transfer learning, where we\ntransfer knowledge from a past ad experiment in 2020 to evaluate ads for 2024.\nA key component of our framework is a sensitivity analysis that quantifies the\nunobservable differences between 2020 and 2024 elections, where sensitivity\nparameters can be calibrated in a data-driven manner. We propose two estimators\nof the 2024 ad effect: a simple regression estimator with bootstrap, which we\nrecommend for practitioners in this field, and an estimator based on the\nefficient influence function for broader applications. Using our framework, we\nestimate the effect of running a negative, digital ad campaign against Trump on\nvoter turnout in PA for the 2024 election. Our findings indicate effect\nheterogeneity across counties of PA and among important subgroups stratified by\ngender, urbanicity, and education attainment.\n","versions":"[{'version': 'v1', 'created': 'Sat, 2 Nov 2024 01:35:58 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 01:23:02 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Miao', 'Xinran', ''], ['Zhao', 'Jiwei', ''], ['Kang', 'Hyunseung', '']]","extracted_entities":"[{'text': 'transfer learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"transfer learning","similarity_score":0.5694054365}
{"id":2411.14871,"submitter":"Dingyuan Shi","authors":"Dingyuan Shi, Yong Wang, Hangyu Li, Xiangxiang Chu","title":"Preference Alignment for Diffusion Model via Explicit Denoised\n  Distribution Estimation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Diffusion models have shown remarkable success in text-to-image generation,\nmaking preference alignment for these models increasingly important. The\npreference labels are typically available only at the terminal of denoising\ntrajectories, which poses challenges in optimizing the intermediate denoising\nsteps. In this paper, we propose to conduct Denoised Distribution Estimation\n(DDE) that explicitly connects intermediate steps to the terminal denoised\ndistribution. Therefore, preference labels can be used for the entire\ntrajectory optimization. To this end, we design two estimation strategies for\nour DDE. The first is stepwise estimation, which utilizes the conditional\ndenoised distribution to estimate the model denoised distribution. The second\nis single-shot estimation, which converts the model output into the terminal\ndenoised distribution via DDIM modeling. Analytically and empirically, we\nreveal that DDE equipped with two estimation strategies naturally derives a\nnovel credit assignment scheme that prioritizes optimizing the middle part of\nthe denoising trajectory. Extensive experiments demonstrate that our approach\nachieves superior performance, both quantitatively and qualitatively.\n","versions":"[{'version': 'v1', 'created': 'Fri, 22 Nov 2024 11:45:33 GMT'}, {'version': 'v2', 'created': 'Wed, 25 Dec 2024 14:55:08 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 02:36:28 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Shi', 'Dingyuan', ''], ['Wang', 'Yong', ''], ['Li', 'Hangyu', ''], ['Chu', 'Xiangxiang', '']]","extracted_entities":"[{'text': 'single-shot estimation', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"single-shot estimation","similarity_score":0.6626640558}
{"id":2412.12902,"submitter":"Nikitha Sr","authors":"Nikitha SR, Tarun Ram Menta, Mausoom Sarkar","title":"DoPTA: Improving Document Layout Analysis using Patch-Text Alignment","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  The advent of multimodal learning has brought a significant improvement in\ndocument AI. Documents are now treated as multimodal entities, incorporating\nboth textual and visual information for downstream analysis. However, works in\nthis space are often focused on the textual aspect, using the visual space as\nauxiliary information. While some works have explored pure vision based\ntechniques for document image understanding, they require OCR identified text\nas input during inference, or do not align with text in their learning\nprocedure. Therefore, we present a novel image-text alignment technique\nspecially designed for leveraging the textual information in document images to\nimprove performance on visual tasks. Our document encoder model DoPTA - trained\nwith this technique demonstrates strong performance on a wide range of document\nimage understanding tasks, without requiring OCR during inference. Combined\nwith an auxiliary reconstruction objective, DoPTA consistently outperforms\nlarger models, while using significantly lesser pre-training compute. DoPTA\nalso sets new state-of-the art results on D4LA, and FUNSD, two challenging\ndocument visual analysis benchmarks.\n","versions":"[{'version': 'v1', 'created': 'Tue, 17 Dec 2024 13:26:31 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 14:17:02 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['SR', 'Nikitha', ''], ['Menta', 'Tarun Ram', ''], ['Sarkar', 'Mausoom', '']]","extracted_entities":"[{'text': 'multimodal learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"multimodal learning","similarity_score":0.5063463449}
{"id":2412.14957,"submitter":"Leonardo Barcellona","authors":"Leonardo Barcellona, Andrii Zadaianchuk, Davide Allegro, Samuele Papa,\n  Stefano Ghidoni, Efstratios Gavves","title":"Dream to Manipulate: Compositional World Models Empowering Robot\n  Imitation Learning with Imagination","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  A world model provides an agent with a representation of its environment,\nenabling it to predict the causal consequences of its actions. Current world\nmodels typically cannot directly and explicitly imitate the actual environment\nin front of a robot, often resulting in unrealistic behaviors and\nhallucinations that make them unsuitable for real-world robotics applications.\nTo overcome those challenges, we propose to rethink robot world models as\nlearnable digital twins. We introduce DreMa, a new approach for constructing\ndigital twins automatically using learned explicit representations of the real\nworld and its dynamics, bridging the gap between traditional digital twins and\nworld models. DreMa replicates the observed world and its structure by\nintegrating Gaussian Splatting and physics simulators, allowing robots to\nimagine novel configurations of objects and to predict the future consequences\nof robot actions thanks to its compositionality. We leverage this capability to\ngenerate new data for imitation learning by applying equivariant\ntransformations to a small set of demonstrations. Our evaluations across\nvarious settings demonstrate significant improvements in accuracy and\nrobustness by incrementing actions and object distributions, reducing the data\nneeded to learn a policy and improving the generalization of the agents. As a\nhighlight, we show that a real Franka Emika Panda robot, powered by DreMa's\nimagination, can successfully learn novel physical tasks from just a single\nexample per task variation (one-shot policy learning). Our project page can be\nfound in: https:\/\/dreamtomanipulate.github.io\/.\n","versions":"[{'version': 'v1', 'created': 'Thu, 19 Dec 2024 15:38:15 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 09:40:42 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Barcellona', 'Leonardo', ''], ['Zadaianchuk', 'Andrii', ''], ['Allegro', 'Davide', ''], ['Papa', 'Samuele', ''], ['Ghidoni', 'Stefano', ''], ['Gavves', 'Efstratios', '']]","extracted_entities":"[{'text': 'one-shot policy learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"one-shot policy learning","similarity_score":0.677072525}
{"id":2412.1995,"submitter":"Christian Friedrich","authors":"Eric Hirsch and Christian Friedrich","title":"Data-driven tool wear prediction in milling, based on a\n  process-integrated single-sensor approach","comments":"This preprint has been submitted to Robotics and Computer-Integrated\n  Manufacturing for possible publication ,14 pages, 12 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.RO eess.SP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Accurate tool wear prediction is essential for maintaining productivity and\nminimizing costs in machining. However, the complex nature of the tool wear\nprocess poses significant challenges to achieving reliable predictions. This\nstudy explores data-driven methods, in particular deep learning, for tool wear\nprediction. Traditional data-driven approaches often focus on a single process,\nrelying on multi-sensor setups and extensive data generation, which limits\ngeneralization to new settings. Moreover, multi-sensor integration is often\nimpractical in industrial environments. To address these limitations, this\nresearch investigates the transferability of predictive models using minimal\ntraining data, validated across two processes. Furthermore, it uses a simple\nsetup with a single acceleration sensor to establish a low-cost data generation\napproach that facilitates the generalization of models to other processes via\ntransfer learning. The study evaluates several machine learning models,\nincluding transformer-inspired convolutional neural networks (CNN), long\nshort-term memory networks (LSTM), support vector machines (SVM), and decision\ntrees, trained on different input formats such as feature vectors and\nshort-time Fourier transform (STFT). The performance of the models is evaluated\non two machines and on different amounts of training data, including scenarios\nwith significantly reduced datasets, providing insight into their effectiveness\nunder constrained data conditions. The results demonstrate the potential of\nspecific models and configurations for effective tool wear prediction,\ncontributing to the development of more adaptable and efficient predictive\nmaintenance strategies in machining. Notably, the ConvNeXt model has an\nexceptional performance, achieving 99.1\\% accuracy in identifying tool wear\nusing data from only four milling tools operated until they are worn.\n","versions":"[{'version': 'v1', 'created': 'Fri, 27 Dec 2024 23:10:32 GMT'}, {'version': 'v2', 'created': 'Tue, 7 Jan 2025 14:35:01 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 18:20:38 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Hirsch', 'Eric', ''], ['Friedrich', 'Christian', '']]","extracted_entities":"[{'text': 'deep learning', 'label': 'Zero-shot Learning'}, {'text': 'transfer learning', 'label': 'Few-shot Learning'}, {'text': 'decision\\ntrees', 'label': 'AI model'}]","assigned_concept":"Few-shot Learning","matched_keyword":"transfer learning","similarity_score":0.5694054365}
{"id":2501.05017,"submitter":"Xiaojie Li","authors":"Xiaojie Li, Yibo Yang, Jianlong Wu, Jie Liu, Yue Yu, Liqiang Nie, Min\n  Zhang","title":"Continuous Knowledge-Preserving Decomposition for Few-Shot Continual\n  Learning","comments":"Code: https:\/\/github.com\/xiaojieli0903\/CKPD-FSCIL","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Few-shot class-incremental learning (FSCIL) involves learning new classes\nfrom limited data while retaining prior knowledge, and often results in\ncatastrophic forgetting. Existing methods either freeze backbone networks to\npreserve knowledge, which limits adaptability, or rely on additional modules or\nprompts, introducing inference overhead. To this end, we propose Continuous\nKnowledge-Preserving Decomposition for FSCIL (CKPD-FSCIL), a framework that\ndecomposes a model's weights into two parts: one that compacts existing\nknowledge (knowledge-sensitive components) and another that carries redundant\ncapacity to accommodate new abilities (redundant-capacity components). The\ndecomposition is guided by a covariance matrix from replay samples, ensuring\nprincipal components align with classification abilities. During adaptation, we\nfreeze the knowledge-sensitive components and only adapt the redundant-capacity\ncomponents, fostering plasticity while minimizing interference without changing\nthe architecture or increasing overhead. Additionally, CKPD introduces an\nadaptive layer selection strategy to identify layers with redundant capacity,\ndynamically allocating adapters. Experiments on multiple benchmarks show that\nCKPD-FSCIL outperforms state-of-the-art methods.\n","versions":"[{'version': 'v1', 'created': 'Thu, 9 Jan 2025 07:18:48 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 05:21:19 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Li', 'Xiaojie', ''], ['Yang', 'Yibo', ''], ['Wu', 'Jianlong', ''], ['Liu', 'Jie', ''], ['Yu', 'Yue', ''], ['Nie', 'Liqiang', ''], ['Zhang', 'Min', '']]","extracted_entities":"[{'text': 'Few-shot class-incremental learning', 'label': 'Few-shot Learning'}, {'text': 'FSCIL', 'label': 'Few-shot Learning'}, {'text': 'prompts', 'label': 'Prompting'}]","assigned_concept":"Few-shot Learning","matched_keyword":"Few-shot class-incremental learning","similarity_score":0.793258667}
{"id":2501.08109,"submitter":"Wenjie Huang","authors":"Xinye Qu, Longxiao Liu, Wenjie Huang","title":"Data-driven inventory management for new products: An adjusted Dyna-$Q$\n  approach with transfer learning","comments":"7 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper, we propose a novel reinforcement learning algorithm for\ninventory management of newly launched products with no historical demand\ninformation. The algorithm follows the classic Dyna-$Q$ structure, balancing\nthe model-free and model-based approaches, while accelerating the training\nprocess of Dyna-$Q$ and mitigating the model discrepancy generated by the\nmodel-based feedback. Based on the idea of transfer learning, warm-start\ninformation from the demand data of existing similar products can be\nincorporated into the algorithm to further stabilize the early-stage training\nand reduce the variance of the estimated optimal policy. Our approach is\nvalidated through a case study of bakery inventory management with real data.\nThe adjusted Dyna-$Q$ shows up to a 23.7\\% reduction in average daily cost\ncompared with $Q$-learning, and up to a 77.5\\% reduction in training time\nwithin the same horizon compared with classic Dyna-$Q$. By using transfer\nlearning, it can be found that the adjusted Dyna-$Q$ has the lowest total cost,\nlowest variance in total cost, and relatively low shortage percentages among\nall the benchmarking algorithms under a 30-day testing.\n","versions":"[{'version': 'v1', 'created': 'Tue, 14 Jan 2025 13:40:08 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Jan 2025 02:48:33 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 06:43:36 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Qu', 'Xinye', ''], ['Liu', 'Longxiao', ''], ['Huang', 'Wenjie', '']]","extracted_entities":"[{'text': 'Dyna-$Q$', 'label': 'AI model'}, {'text': 'transfer learning', 'label': 'Few-shot Learning'}, {'text': 'transfer\\nlearning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"transfer learning","similarity_score":0.5694054365}
{"id":2501.17568,"submitter":"Ehsan Aminian","authors":"Ehsan Aminian, Rita P. Ribeiro, Joao Gama","title":"Histogram Approaches for Imbalanced Data Streams Regression","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Imbalanced domains pose a significant challenge in real-world predictive\nanalytics, particularly in the context of regression. While existing research\nhas primarily focused on batch learning from static datasets, limited attention\nhas been given to imbalanced regression in online learning scenarios. Intending\nto address this gap, in prior work, we proposed sampling strategies based on\nChebyshevs inequality as the first methodologies designed explicitly for data\nstreams. However, these approaches operated under the restrictive assumption\nthat rare instances exclusively reside at distribution extremes. This study\nintroduces histogram-based sampling strategies to overcome this constraint,\nproposing flexible solutions for imbalanced regression in evolving data\nstreams. The proposed techniques -- Histogram-based Undersampling (HistUS) and\nHistogram-based Oversampling (HistOS) -- employ incremental online histograms\nto dynamically detect and prioritize rare instances across arbitrary regions of\nthe target distribution to improve predictions in the rare cases. Comprehensive\nexperiments on synthetic and real-world benchmarks demonstrate that HistUS and\nHistOS substantially improve rare-case prediction accuracy, outperforming\nbaseline models while maintaining competitiveness with Chebyshev-based\napproaches.\n","versions":"[{'version': 'v1', 'created': 'Wed, 29 Jan 2025 11:03:02 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 11:38:47 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Aminian', 'Ehsan', ''], ['Ribeiro', 'Rita P.', ''], ['Gama', 'Joao', '']]","extracted_entities":"[{'text': 'batch learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"batch learning","similarity_score":0.5028504729}
{"id":2502.0709,"submitter":"Xinyu Tian","authors":"Xinyu Tian and Xiaotong Shen","title":"Generative Distribution Prediction: A Unified Approach to Multimodal\n  Learning","comments":"31 pages 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Accurate prediction with multimodal data-encompassing tabular, textual, and\nvisual inputs or outputs-is fundamental to advancing analytics in diverse\napplication domains. Traditional approaches often struggle to integrate\nheterogeneous data types while maintaining high predictive accuracy. We\nintroduce Generative Distribution Prediction (GDP), a novel framework that\nleverages multimodal synthetic data generation-such as conditional diffusion\nmodels-to enhance predictive performance across structured and unstructured\nmodalities. GDP is model-agnostic, compatible with any high-fidelity generative\nmodel, and supports transfer learning for domain adaptation. We establish a\nrigorous theoretical foundation for GDP, providing statistical guarantees on\nits predictive accuracy when using diffusion models as the generative backbone.\nBy estimating the data-generating distribution and adapting to various loss\nfunctions for risk minimization, GDP enables accurate point predictions across\nmultimodal settings. We empirically validate GDP on four supervised learning\ntasks-tabular data prediction, question answering, image captioning, and\nadaptive quantile regression-demonstrating its versatility and effectiveness\nacross diverse domains.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Feb 2025 22:30:35 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 17:40:18 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Tian', 'Xinyu', ''], ['Shen', 'Xiaotong', '']]","extracted_entities":"[{'text': 'conditional diffusion\\nmodels-to', 'label': 'Foundation Model'}, {'text': 'transfer learning', 'label': 'Few-shot Learning'}, {'text': 'diffusion models', 'label': 'Foundation Model'}]","assigned_concept":"Few-shot Learning","matched_keyword":"transfer learning","similarity_score":0.5694054365}
{"id":2502.18636,"submitter":"Chenhao Chu","authors":"Chenhao Chu, Yuhao Mao, Hua Wang","title":"Transfer Learning Assisted Fast Design Migration Over Technology Nodes:\n  A Study on Transformer Matching Network","comments":"Publihsed and Presented at IEEE MTT-S International Microwave\n  Symposium (IMS 2024), Washington, DC, USA","journal-ref":null,"doi":"10.1109\/IMS40175.2024.10600344","report-no":null,"categories":"eess.SP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this study, we introduce an innovative methodology for the design of\nmm-Wave passive networks that leverages knowledge transfer from a pre-trained\nsynthesis neural network (NN) model in one technology node and achieves swift\nand reliable design adaptation across different integrated circuit (IC)\ntechnologies, operating frequencies, and metal options. We prove this concept\nthrough simulation-based demonstrations focusing on the training and comparison\nof the coefficient of determination (R2) of synthesis NNs for 1:1 on-chip\ntransformers in GlobalFoundries(GF) 22nm FDX+ (target domain), with and without\ntransfer learning from a model trained in GF 45nm SOI (source domain). In the\nexperiments, we explore varying target data densities of 0.5%, 1%, 5%, and 100%\nwith a complete dataset of 0.33 million in GF 22FDX+, and for comparative\nanalysis, apply source data densities of 25%, 50%, 75%, and 100% with a\ncomplete dataset of 2.5 million in GF 45SOI. With the source data only at\n30GHz, the experiments span target data from two metal options in GF 22FDX+ at\nfrequencies of 30 and 39 GHz. The results prove that the transfer learning with\nthe source domain knowledge (GF 45SOI) can both accelerate the training process\nin the target domain (GF 22FDX+) and improve the R2 values compared to models\nwithout knowledge transfer. Furthermore, it is observed that a model trained\nwith just 5% of target data and augmented by transfer learning achieves R2\nvalues superior to a model trained with 20% of the data without transfer,\nvalidating the advantage seen from 1% to 5% data density. This demonstrates a\nnotable reduction of 4X in the necessary dataset size highlighting the efficacy\nof utilizing transfer learning to mm-Wave passive network design. The PyTorch\nlearning and testing code is publicly available at\nhttps:\/\/github.com\/ChenhaoChu\/RFIC-TL.\n","versions":"[{'version': 'v1', 'created': 'Tue, 25 Feb 2025 20:53:53 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 14:27:10 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Chu', 'Chenhao', ''], ['Mao', 'Yuhao', ''], ['Wang', 'Hua', '']]","extracted_entities":"[{'text': '1:1 on-chip\\ntransformers', 'label': 'Transformers'}, {'text': 'transfer learning', 'label': 'Few-shot Learning'}, {'text': 'transfer learning', 'label': 'Few-shot Learning'}, {'text': 'transfer learning', 'label': 'Few-shot Learning'}, {'text': 'transfer learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"transfer learning","similarity_score":0.5694054365}
{"id":2503.02162,"submitter":"Jianzhong You","authors":"Jianzhong You, Yuan Gao, Sangwook Kim, Chris Mcintosh","title":"X2CT-CLIP: Enable Multi-Abnormality Detection in Computed Tomography\n  from Chest Radiography via Tri-Modal Contrastive Learning","comments":"11 pages, 1 figure, 5 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Computed tomography (CT) is a key imaging modality for diagnosis, yet its\nclinical utility is marred by high radiation exposure and long turnaround\ntimes, restricting its use for larger-scale screening. Although chest\nradiography (CXR) is more accessible and safer, existing CXR foundation models\nfocus primarily on detecting diseases that are readily visible on the CXR.\nRecently, works have explored training disease classification models on\nsimulated CXRs, but they remain limited to recognizing a single disease type\nfrom CT. CT foundation models have also emerged with significantly improved\ndetection of pathologies in CT. However, the generalized application of\nCT-derived labels on CXR has remained illusive. In this study, we propose\nX2CT-CLIP, a tri-modal knowledge transfer learning framework that bridges the\nmodality gap between CT and CXR while reducing the computational burden of\nmodel training. Our approach is the first work to enable multi-abnormality\nclassification in CT, using CXR, by transferring knowledge from 3D CT volumes\nand associated radiology reports to a CXR encoder via a carefully designed\ntri-modal alignment mechanism in latent space. Extensive evaluations on three\nmulti-label CT datasets demonstrate that our method outperforms\nstate-of-the-art baselines in cross-modal retrieval, few-shot adaptation, and\nexternal validation. These results highlight the potential of CXR, enriched\nwith knowledge derived from CT, as a viable efficient alternative for disease\ndetection in resource-limited settings.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 00:48:09 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 00:50:53 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['You', 'Jianzhong', ''], ['Gao', 'Yuan', ''], ['Kim', 'Sangwook', ''], ['Mcintosh', 'Chris', '']]","extracted_entities":"[{'text': 'CT foundation models', 'label': 'Foundation Model'}, {'text': 'X2CT-CLIP', 'label': 'Foundation Model'}, {'text': 'few-shot adaptation', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"few-shot adaptation","similarity_score":0.7985853553}
{"id":2503.0483,"submitter":"Jingying Zeng","authors":"Jingying Zeng, Hui Liu, Zhenwei Dai, Xianfeng Tang, Chen Luo, Samarth\n  Varshney, Zhen Li, Qi He","title":"Cite Before You Speak: Enhancing Context-Response Grounding in\n  E-commerce Conversational LLM-Agents","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  With the advancement of conversational large language models (LLMs), several\nLLM-based Conversational Shopping Agents (CSA) have been developed to help\ncustomers answer questions and smooth their shopping journey in e-commerce\ndomain. The primary objective in building a trustworthy CSA is to ensure the\nagent's responses are accurate and factually grounded, which is essential for\nbuilding customer trust and encouraging continuous engagement. However, two\nchallenges remain. First, LLMs produce hallucinated or unsupported claims. Such\ninaccuracies risk spreading misinformation and diminishing customer trust.\nSecond, without providing knowledge source attribution in CSA response,\ncustomers struggle to verify LLM-generated information. To address these\nchallenges, we present an easily productionized solution that enables a\n\"citation experience\" utilizing In-context Learning (ICL) and\nMulti-UX-Inference (MUI) to generate responses with citations to attribute its\noriginal sources without interfering other existing UX features. With proper UX\ndesign, these citation marks can be linked to the related product information\nand display the source to our customers. In this work, we also build\nauto-metrics and scalable benchmarks to holistically evaluate LLM's grounding\nand attribution capabilities. Our experiments demonstrate that incorporating\nthis citation generation paradigm can substantially enhance the grounding of\nLLM responses by 13.83% on the real-world data. As such, our solution not only\naddresses the immediate challenges of LLM grounding issues but also adds\ntransparency to conversational AI.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 08:58:35 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 01:47:04 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zeng', 'Jingying', ''], ['Liu', 'Hui', ''], ['Dai', 'Zhenwei', ''], ['Tang', 'Xianfeng', ''], ['Luo', 'Chen', ''], ['Varshney', 'Samarth', ''], ['Li', 'Zhen', ''], ['He', 'Qi', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'In-context Learning', 'label': 'Few-shot Learning'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]","assigned_concept":"Few-shot Learning","matched_keyword":"In-context Learning","similarity_score":0.5115564466}
{"id":2503.05491,"submitter":"Lo\\\"ic Fosse","authors":"Lo\\\"ic Fosse and Fr\\'ed\\'eric B\\'echet and Beno\\^it Favre and\n  G\\'eraldine Damnati and Gw\\'enol\\'e Lecorv\\'e and Maxime Darrin and Philippe\n  Formont and Pablo Piantanida","title":"Statistical Deficiency for Task Inclusion Estimation","comments":"34 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Tasks are central in machine learning, as they are the most natural objects\nto assess the capabilities of current models. The trend is to build general\nmodels able to address any task. Even though transfer learning and multitask\nlearning try to leverage the underlying task space, no well-founded tools are\navailable to study its structure. This study proposes a theoretically grounded\nsetup to define the notion of task and to compute the {\\bf inclusion} between\ntwo tasks from a statistical deficiency point of view. We propose a tractable\nproxy as information sufficiency to estimate the degree of inclusion between\ntasks, show its soundness on synthetic data, and use it to reconstruct\nempirically the classic NLP pipeline.\n","versions":"[{'version': 'v1', 'created': 'Fri, 7 Mar 2025 15:00:28 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 08:41:29 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Fosse', 'Lo\u00efc', ''], ['B\u00e9chet', 'Fr\u00e9d\u00e9ric', ''], ['Favre', 'Beno\u00eet', ''], ['Damnati', 'G\u00e9raldine', ''], ['Lecorv\u00e9', 'Gw\u00e9nol\u00e9', ''], ['Darrin', 'Maxime', ''], ['Formont', 'Philippe', ''], ['Piantanida', 'Pablo', '']]","extracted_entities":"[{'text': 'transfer learning', 'label': 'Few-shot Learning'}, {'text': 'multitask\\nlearning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"transfer learning","similarity_score":0.5694054365}
{"id":2503.06456,"submitter":"Chengxuan Qian","authors":"Chengxuan Qian, Kai Han, Jingchao Wang, Zhenlong Yuan, Rui Qian,\n  Chongwen Lyu, Jun Chen, Zhe Liu","title":"DynCIM: Dynamic Curriculum for Imbalanced Multimodal Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multimodal learning integrates complementary information from diverse\nmodalities to enhance the decision-making process. However, the potential of\nmultimodal collaboration remains under-exploited due to disparities in data\nquality and modality representation capabilities. To address this, we introduce\nDynCIM, a novel dynamic curriculum learning framework designed to quantify the\ninherent imbalances from both sample and modality perspectives. DynCIM employs\na sample-level curriculum to dynamically assess each sample's difficulty\naccording to prediction deviation, consistency, and stability, while a\nmodality-level curriculum measures modality contributions from global and\nlocal. Furthermore, a gating-based dynamic fusion mechanism is introduced to\nadaptively adjust modality contributions, minimizing redundancy and optimizing\nfusion effectiveness. Extensive experiments on six multimodal benchmarking\ndatasets, spanning both bimodal and trimodal scenarios, demonstrate that DynCIM\nconsistently outperforms state-of-the-art methods. Our approach effectively\nmitigates modality and sample imbalances while enhancing adaptability and\nrobustness in multimodal learning tasks. Our code is available at\nhttps:\/\/github.com\/Raymond-Qiancx\/DynCIM.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 05:30:15 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Qian', 'Chengxuan', ''], ['Han', 'Kai', ''], ['Wang', 'Jingchao', ''], ['Yuan', 'Zhenlong', ''], ['Qian', 'Rui', ''], ['Lyu', 'Chongwen', ''], ['Chen', 'Jun', ''], ['Liu', 'Zhe', '']]","extracted_entities":"[{'text': 'Multimodal learning', 'label': 'Few-shot Learning'}, {'text': 'multimodal learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"Multimodal learning","similarity_score":0.5063463449}
{"id":2503.06531,"submitter":"Jie He","authors":"Jie He, Yu Fu","title":"MetaXCR: Reinforcement-Based Meta-Transfer Learning for Cross-Lingual\n  Commonsense Reasoning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Commonsense reasoning (CR) has been studied in many pieces of domain and has\nachieved great progress with the aid of large datasets. Unfortunately, most\nexisting CR datasets are built in English, so most previous work focus on\nEnglish. Furthermore, as the annotation of commonsense reasoning is costly, it\nis impossible to build a large dataset for every novel task. Therefore, there\nare growing appeals for Cross-lingual Low-Resource Commonsense Reasoning, which\naims to leverage diverse existed English datasets to help the model adapt to\nnew cross-lingual target datasets with limited labeled data. In this paper, we\npropose a multi-source adapter for cross-lingual low-resource Commonsense\nReasoning (MetaXCR). In this framework, we first extend meta learning by\nincorporating multiple training datasets to learn a generalized task adapters\nacross different tasks. Then, we further introduce a reinforcement-based\nsampling strategy to help the model sample the source task that is the most\nhelpful to the target task. Finally, we introduce two types of cross-lingual\nmeta-adaption methods to enhance the performance of models on target languages.\nExtensive experiments demonstrate MetaXCR is superior over state-of-the-arts,\nwhile being trained with fewer parameters than other work.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 09:27:57 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['He', 'Jie', ''], ['Fu', 'Yu', '']]","extracted_entities":"[{'text': 'meta learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"meta learning","similarity_score":0.5217227936}
{"id":2311.17643,"submitter":"Alexander Becker","authors":"Alexander Becker, Rodrigo Caye Daudt, Dominik Narnhofer, Torben\n  Peters, Nando Metzger, Jan Dirk Wegner, Konrad Schindler","title":"Thera: Aliasing-Free Arbitrary-Scale Super-Resolution with Neural Heat\n  Fields","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent approaches to arbitrary-scale single image super-resolution (ASR) use\nneural fields to represent continuous signals that can be sampled at arbitrary\nresolutions. However, point-wise queries of neural fields do not naturally\nmatch the point spread function (PSF) of pixels, which may cause aliasing in\nthe super-resolved image. Existing methods attempt to mitigate this by\napproximating an integral version of the field at each scaling factor,\ncompromising both fidelity and generalization. In this work, we introduce\nneural heat fields, a novel neural field formulation that inherently models a\nphysically exact PSF. Our formulation enables analytically correct\nanti-aliasing at any desired output resolution, and -- unlike supersampling --\nat no additional cost. Building on this foundation, we propose Thera, an\nend-to-end ASR method that substantially outperforms existing approaches, while\nbeing more parameter-efficient and offering strong theoretical guarantees. The\nproject page is at https:\/\/therasr.github.io.\n","versions":"[{'version': 'v1', 'created': 'Wed, 29 Nov 2023 14:01:28 GMT'}, {'version': 'v2', 'created': 'Thu, 14 Mar 2024 19:17:34 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 12:22:00 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Becker', 'Alexander', ''], ['Daudt', 'Rodrigo Caye', ''], ['Narnhofer', 'Dominik', ''], ['Peters', 'Torben', ''], ['Metzger', 'Nando', ''], ['Wegner', 'Jan Dirk', ''], ['Schindler', 'Konrad', '']]","extracted_entities":"[{'text': 'neural fields', 'label': 'Neural Language Model'}, {'text': 'neural heat fields', 'label': 'Neural Language Model'}, {'text': 'Thera', 'label': 'Foundation Model'}]","assigned_concept":"Neural Language Model","matched_keyword":"neural fields","similarity_score":0.5002090335}
{"id":2412.19054,"submitter":"Ngoc Hai Trinh","authors":"Pham Ky Anh and Trinh Ngoc Hai and Nguyen Van Manh","title":"Regularized neural network for general variational inequalities\n  involving monotone couples of operators in Hilbert spaces","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper, based on the Tikhonov regularization technique, we study a\nmonotone general variational inequality (GVI) by considering an associated\nstrongly monotone GVI, depending on a regularization parameter $\\alpha,$ such\nthat the latter admits a unique solution $x_\\alpha$ which tends to some\nsolution of the initial GVI, as $\\alpha \\to 0.$ However, instead of solving the\nregularized GVI for each $\\alpha$, which may be very expensive, we consider a\nneural network (also known as a dynamical system) associated with the\nregularized GVI and establish the existence and the uniqueness of the strong\nglobal solution to the corresponding Cauchy problem. An explicit discretization\nof this neural network leads to strongly convergent iterative regularization\nalgorithms for monotone general variational inequality. Numerical tests are\nperformed to show the effectiveness of the proposed methods.\n  This work extends our recent results in [Anh, Hai, Optim. Eng. 25 (2024)\n2295-2313] to more general setting.\n","versions":"[{'version': 'v1', 'created': 'Thu, 26 Dec 2024 04:39:37 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 01:47:34 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Anh', 'Pham Ky', ''], ['Hai', 'Trinh Ngoc', ''], ['Van Manh', 'Nguyen', '']]","extracted_entities":"[{'text': 'neural network', 'label': 'Neural Language Model'}, {'text': 'neural network', 'label': 'Neural Language Model'}]","assigned_concept":"Neural Language Model","matched_keyword":"neural network","similarity_score":0.5788917542}
{"id":2503.02376,"submitter":"Guo Chen","authors":"Guo Chen, Ling Lin, Chengfeng Zhang, Jie Zhang, Gilles Frapper, and\n  Xianlong Wang","title":"Unsaturated Dinitrogen Difluoride under Pressure: toward high-Energy\n  Density Polymerized NF Chains","comments":"High-energy-density material, First-principles calculation, Ab initio\n  molecular dynamics, NF compound","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci physics.chem-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Based on first-principles calculations and ab initio molecular dynamics\nsimulations, the polymerisation of the unsaturated cis dinitrogen-difluoride\n(cis-N2F2) molecular compound is investigated. The thermodynamic, dynamical and\nthermal stabilities of the nitrogen fluorine NF system are investigated at\nconditions of 0-3000 K and 0-200 GPa. The cis-N2F2 molecule is a suitable\nprecursor to obtain one-dimensional polymerized nitrogen-fluorine (poly-NF)\nchains at a pressure above 90 GPa and at a temperature around 1900 K.\nImportantly, these poly-NF chains can be quenched to room conditions, and\npotentially serve as a High-energy-density materials (HEDM). It has been\nestablished that when Al is utilised as a reducing agent, poly-NF chains\nexhibit a gravimetric energy density of 13.55 kJ\/g, which exceeds that of cubic\ngauche nitrogen (cg-N, 9.70 kJ\/g). This is attributable to the presence of both\npolymerised nitrogen and strong oxidising F atoms.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 08:05:17 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 05:40:22 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chen', 'Guo', ''], ['Lin', 'Ling', ''], ['Zhang', 'Chengfeng', ''], ['Zhang', 'Jie', ''], ['Frapper', 'Gilles', ''], ['Wang', 'Xianlong', '']]","extracted_entities":"[{'text': 'Al', 'label': 'ALBERT'}]","assigned_concept":"ALBERT","matched_keyword":"Al","similarity_score":0.5163906813}
{"id":2211.15072,"submitter":"Puheng Li","authors":"Puheng Li, James Zou, Linjun Zhang","title":"FaiREE: Fair Classification with Finite-Sample and Distribution-Free\n  Guarantee","comments":"Accepted at ICLR 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Algorithmic fairness plays an increasingly critical role in machine learning\nresearch. Several group fairness notions and algorithms have been proposed.\nHowever, the fairness guarantee of existing fair classification methods mainly\ndepends on specific data distributional assumptions, often requiring large\nsample sizes, and fairness could be violated when there is a modest number of\nsamples, which is often the case in practice. In this paper, we propose FaiREE,\na fair classification algorithm that can satisfy group fairness constraints\nwith finite-sample and distribution-free theoretical guarantees. FaiREE can be\nadapted to satisfy various group fairness notions (e.g., Equality of\nOpportunity, Equalized Odds, Demographic Parity, etc.) and achieve the optimal\naccuracy. These theoretical guarantees are further supported by experiments on\nboth synthetic and real data. FaiREE is shown to have favorable performance\nover state-of-the-art algorithms.\n","versions":"[{'version': 'v1', 'created': 'Mon, 28 Nov 2022 05:16:20 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Apr 2023 04:29:13 GMT'}, {'version': 'v3', 'created': 'Fri, 14 Apr 2023 17:36:38 GMT'}, {'version': 'v4', 'created': 'Mon, 9 Oct 2023 05:19:22 GMT'}, {'version': 'v5', 'created': 'Wed, 12 Mar 2025 07:17:23 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Li', 'Puheng', ''], ['Zou', 'James', ''], ['Zhang', 'Linjun', '']]","extracted_entities":"[{'text': 'Algorithmic fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'Equality of\\nOpportunity', 'label': 'Model Bias and Fairness'}, {'text': 'Equalized Odds', 'label': 'Model Bias and Fairness'}, {'text': 'Demographic Parity', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"Algorithmic fairness","similarity_score":0.7059931755}
{"id":2406.19256,"submitter":"Kaveen Hiniduma","authors":"Kaveen Hiniduma, Suren Byna, Jean Luca Bez, Ravi Madduri","title":"AI Data Readiness Inspector (AIDRIN) for Quantitative Assessment of Data\n  Readiness for AI","comments":"12 pages, 9 figures, Accepted to SSDBM 2024","journal-ref":null,"doi":"10.1145\/3676288.3676296","report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  \"Garbage In Garbage Out\" is a universally agreed quote by computer scientists\nfrom various domains, including Artificial Intelligence (AI). As data is the\nfuel for AI, models trained on low-quality, biased data are often ineffective.\nComputer scientists who use AI invest a considerable amount of time and effort\nin preparing the data for AI. However, there are no standard methods or\nframeworks for assessing the \"readiness\" of data for AI. To provide a\nquantifiable assessment of the readiness of data for AI processes, we define\nparameters of AI data readiness and introduce AIDRIN (AI Data Readiness\nInspector). AIDRIN is a framework covering a broad range of readiness\ndimensions available in the literature that aid in evaluating the readiness of\ndata quantitatively and qualitatively. AIDRIN uses metrics in traditional data\nquality assessment such as completeness, outliers, and duplicates for data\nevaluation. Furthermore, AIDRIN uses metrics specific to assess data for AI,\nsuch as feature importance, feature correlations, class imbalance, fairness,\nprivacy, and FAIR (Findability, Accessibility, Interoperability, and\nReusability) principle compliance. AIDRIN provides visualizations and reports\nto assist data scientists in further investigating the readiness of data. The\nAIDRIN framework enhances the efficiency of the machine learning pipeline to\nmake informed decisions on data readiness for AI applications.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Jun 2024 15:26:39 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 15:58:48 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Hiniduma', 'Kaveen', ''], ['Byna', 'Suren', ''], ['Bez', 'Jean Luca', ''], ['Madduri', 'Ravi', '']]","extracted_entities":"[{'text': 'feature importance', 'label': 'Model Bias and Fairness'}, {'text': 'feature correlations', 'label': 'Model Bias and Fairness'}, {'text': 'class imbalance', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'privacy', 'label': 'AI Ethics'}, {'text': 'FAIR', 'label': 'AI Ethics'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"fairness","similarity_score":0.6551788449}
{"id":2407.19114,"submitter":"Saige Rutherford","authors":"Saige Rutherford, Thomas Wolfers, Charlotte Fraza, Nathaniel G.\n  Harnett, Christian F. Beckmann, Henricus G. Ruhe, Andre F. Marquand","title":"To which reference class do you belong? Measuring racial fairness of\n  reference classes with normative modeling","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV cs.CY","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Reference classes in healthcare establish healthy norms, such as pediatric\ngrowth charts of height and weight, and are used to chart deviations from these\nnorms which represent potential clinical risk. How the demographics of the\nreference class influence clinical interpretation of deviations is unknown.\nUsing normative modeling, a method for building reference classes, we evaluate\nthe fairness (racial bias) in reference models of structural brain images that\nare widely used in psychiatry and neurology. We test whether including race in\nthe model creates fairer models. We predict self-reported race using the\ndeviation scores from three different reference class normative models, to\nbetter understand bias in an integrated, multivariate sense. Across all of\nthese tasks, we uncover racial disparities that are not easily addressed with\nexisting data or commonly used modeling techniques. Our work suggests that\ndeviations from the norm could be due to demographic mismatch with the\nreference class, and assigning clinical meaning to these deviations should be\ndone with caution. Our approach also suggests that acquiring more\nrepresentative samples is an urgent research priority.\n","versions":"[{'version': 'v1', 'created': 'Fri, 26 Jul 2024 22:34:05 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 11:15:58 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Rutherford', 'Saige', ''], ['Wolfers', 'Thomas', ''], ['Fraza', 'Charlotte', ''], ['Harnett', 'Nathaniel G.', ''], ['Beckmann', 'Christian F.', ''], ['Ruhe', 'Henricus G.', ''], ['Marquand', 'Andre F.', '']]","extracted_entities":"[{'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'racial bias', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"fairness","similarity_score":0.6551788449}
{"id":2410.19314,"submitter":"Leander Girrbach","authors":"Leander Girrbach, Stephan Alaniz, Yiran Huang, Trevor Darrell, Zeynep\n  Akata","title":"Revealing and Reducing Gender Biases in Vision and Language Assistants\n  (VLAs)","comments":"Accepted at ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Pre-trained large language models (LLMs) have been reliably integrated with\nvisual input for multimodal tasks. The widespread adoption of instruction-tuned\nimage-to-text vision-language assistants (VLAs) like LLaVA and InternVL\nnecessitates evaluating gender biases. We study gender bias in 22 popular\nopen-source VLAs with respect to personality traits, skills, and occupations.\nOur results show that VLAs replicate human biases likely present in the data,\nsuch as real-world occupational imbalances. Similarly, they tend to attribute\nmore skills and positive personality traits to women than to men, and we see a\nconsistent tendency to associate negative personality traits with men. To\neliminate the gender bias in these models, we find that fine-tuning-based\ndebiasing methods achieve the best trade-off between debiasing and retaining\nperformance on downstream tasks. We argue for pre-deploying gender bias\nassessment in VLAs and motivate further development of debiasing strategies to\nensure equitable societal outcomes. Code is available at\nhttps:\/\/github.com\/ExplainableML\/vla-gender-bias.\n","versions":"[{'version': 'v1', 'created': 'Fri, 25 Oct 2024 05:59:44 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 18:00:00 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Girrbach', 'Leander', ''], ['Alaniz', 'Stephan', ''], ['Huang', 'Yiran', ''], ['Darrell', 'Trevor', ''], ['Akata', 'Zeynep', '']]","extracted_entities":"[{'text': 'gender bias', 'label': 'Model Bias and Fairness'}, {'text': 'skills', 'label': 'Model Bias and Fairness'}, {'text': 'gender bias', 'label': 'Model Bias and Fairness'}, {'text': 'gender bias', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"gender bias","similarity_score":0.5324533582}
{"id":2411.05448,"submitter":"Takuro Niitsuma","authors":"Takuro Niitsuma and Mitsuo Yoshida, Hideaki Tamori, Yo Nakawake","title":"Prestige bias drives the viral spread of content reposted by influencers\n  in online communities","comments":"22pages, 8Figure (+ Supplementary)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SI cs.CY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Cultural evolution theory suggests that prestige bias - whereby individuals\npreferentially learn from prestigious figures - has played a key role in human\necological success. However, its impact within online environments remains\nunclear, particularly with respect to whether reposts by prestigious\nindividuals amplify diffusion more effectively than reposts by noninfluential\nusers. We analyzed over 55 million posts and 520 million reposts on Twitter\n(currently X) to examine whether users with high influence scores (hg indices)\nmore effectively amplified the reach of others' content. Our findings indicate\nthat posts shared by influencers are more likely to be further shared than\nthose shared by non-influencers. This effect persisted over time, especially in\nviral posts. Moreover, a small group of highly influential users accounted for\napproximately half of the information flow within repost cascades. These\nfindings demonstrate a prestige bias in information diffusion within the\ndigital society, suggesting that cognitive biases shape content spread through\nreposting.\n","versions":"[{'version': 'v1', 'created': 'Fri, 8 Nov 2024 09:48:21 GMT'}, {'version': 'v2', 'created': 'Tue, 26 Nov 2024 02:22:46 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 23:18:13 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Niitsuma', 'Takuro', ''], ['Yoshida', 'Mitsuo', ''], ['Tamori', 'Hideaki', ''], ['Nakawake', 'Yo', '']]","extracted_entities":"[{'text': 'prestige bias', 'label': 'Model Bias and Fairness'}, {'text': 'prestige bias', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"prestige bias","similarity_score":0.5518395901}
{"id":2411.12021,"submitter":"Hector Gil-Marin","authors":"DESI Collaboration: A. G. Adame, J. Aguilar, S. Ahlen, S. Alam, D. M.\n  Alexander, M. Alvarez, O. Alves, A. Anand, U. Andrade, E. Armengaud, S.\n  Avila, A. Aviles, H. Awan, S. Bailey, C. Baltay, A. Bault, J. Behera, S.\n  BenZvi, F. Beutler, D. Bianchi, C. Blake, R. Blum, S. Brieden, A. Brodzeller,\n  D. Brooks, E. Buckley-Geer, E. Burtin, R. Calderon, R. Canning, A. Carnero\n  Rosell, R. Cereskaite, J. L. Cervantes-Cota, S. Chabanier, E. Chaussidon, J.\n  Chaves-Montero, S. Chen, X. Chen, T. Claybaugh, S. Cole, A. Cuceu, T. M.\n  Davis, K. Dawson, A. de la Macorra, A. de Mattia, N. Deiosso, A. Dey, B. Dey,\n  Z. Ding, P. Doel, J. Edelstein, S. Eftekharzadeh, D. J. Eisenstein, A.\n  Elliott, P. Fagrelius, K. Fanning, S. Ferraro, J. Ereza, N. Findlay, B.\n  Flaugher, A. Font-Ribera, D. Forero-S\\'anchez, J. E. Forero-Romero, C.\n  Garcia-Quintero, L. H. Garrison, E. Gazta\\~naga, H. Gil-Mar\\'in, S. Gontcho A\n  Gontcho, A. X. Gonzalez-Morales, V. Gonzalez-Perez, C. Gordon, D. Green, D.\n  Gruen, R. Gsponer, G. Gutierrez, J. Guy, B. Hadzhiyska, C. Hahn, M. M. S\n  Hanif, H. K. Herrera-Alcantar, K. Honscheid, C. Howlett, D. Huterer, V.\n  Ir\\v{s}i\\v{c}, M. Ishak, S. Juneau, N. G. Kara\\c{c}ayl{\\i}, R. Kehoe, S.\n  Kent, D. Kirkby, H. Kong, S. E. Koposov, A. Kremin, A. Krolewski, Y. Lai,\n  T.-W. Lan, M. Landriau, D. Lang, J. Lasker, J.M. Le Goff, L. Le Guillou, A.\n  Leauthaud, M. E. Levi, T. S. Li, K. Lodha, C. Magneville, M. Manera, D.\n  Margala, P. Martini, M. Maus, P. McDonald, L. Medina-Varela, A. Meisner, J.\n  Mena-Fern\\'andez, R. Miquel, J. Moon, S. Moore, J. Moustakas, E. Mueller, A.\n  Mu\\~noz-Guti\\'errez, A. D. Myers, S. Nadathur, L. Napolitano, R. Neveux, J.\n  A. Newman, N. M. Nguyen, J. Nie, G. Niz, H. E. Noriega, N. Padmanabhan, E.\n  Paillas, N. Palanque-Delabrouille, J. Pan, S. Penmetsa, W. J. Percival, M. M.\n  Pieri, M. Pinon, C. Poppett, A. Porredon, F. Prada, A. P\\'erez-Fern\\'andez,\n  I. P\\'erez-R\\`afols, D. Rabinowitz, A. Raichoor, C. Ram\\'irez-P\\'erez, S.\n  Ramirez-Solano, M. Rashkovetskyi, C. Ravoux, M. Rezaie, J. Rich, A. Rocher,\n  C. Rockosi, F. Rodr\\'iguez-Mart\\'inez, N.A. Roe, A. Rosado-Marin, A. J. Ross,\n  G. Rossi, R. Ruggeri, V. Ruhlmann-Kleider, L. Samushia, E. Sanchez, C.\n  Saulder, E. F. Schlafly, D. Schlegel, M. Schubnell, H. Seo, R. Sharples, J.\n  Silber, A. Slosar, A. Smith, D. Sprayberry, T. Tan, G. Tarl\\'e, S. Trusov, R.\n  Vaisakh, D. Valcin, F. Valdes, M. Vargas-Maga\\~na, L. Verde, M. Walther, B.\n  Wang, M. S. Wang, B. A. Weaver, N. Weaverdyck, R. H. Wechsler, D. H.\n  Weinberg, M. White, M. J. Wilson, J. Yu, Y. Yu, S. Yuan, C. Y\\`eche, E. A.\n  Zaborowski, P. Zarrouk, H. Zhang, C. Zhao, R. Zhao, R. Zhou, H. Zou","title":"DESI 2024 V: Full-Shape Galaxy Clustering from Galaxies and Quasars","comments":"This DESI Collaboration Key Publication is part of the 2024\n  publication series using the first year of observations (see\n  https:\/\/data.desi.lbl.gov\/doc\/papers\/). 81 pages, 24 figures. This version\n  matches the revision after the referee report","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.CO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We present the measurements and cosmological implications of the galaxy\ntwo-point clustering using over 4.7 million unique galaxy and quasar redshifts\nin the range $0.1<z<2.1$ divided into six redshift bins over a $\\sim 7,500$\nsquare degree footprint, from the first year of observations with the Dark\nEnergy Spectroscopic Instrument (DESI Data Release 1). By fitting the full\npower spectrum, we extend previous DESI DR1 baryon acoustic oscillation (BAO)\nmeasurements to include redshift-space distortions and signals from the\nmatter-radiation equality scale. For the first time, this Full-Shape analysis\nis blinded at the catalogue-level to avoid confirmation bias and the systematic\nerrors are accounted for at the two-point clustering level, which automatically\npropagates them into any cosmological parameter. When analysing the data in\nterms of compressed model-agnostic variables, we obtain a combined precision of\n4.7\\% on the amplitude of the redshift space distortion signal reaching similar\nprecision with just one year of DESI data than with 20 years of observation\nfrom previous generation surveys. We analyse the data to directly constrain the\ncosmological parameters within the $\\Lambda$CDM model using perturbation theory\nand combine this information with the reconstructed DESI DR1 galaxy BAO. Using\na Big Bang Nucleosynthesis Gaussian prior on the baryon density parameter, and\na Gaussian prior on the spectral index, we constrain the matter density is\n$\\Omega_m=0.296\\pm 0.010 $ and the Hubble constant $H_0=(68.63 \\pm 0.79)[{\\rm\nkm\\, s^{-1}Mpc^{-1}}]$. Additionally, we measure the amplitude of clustering\n$\\sigma_8=0.841 \\pm 0.034$. The DESI DR1 results are in agreement with the\n$\\Lambda$CDM model based on general relativity with parameters consistent with\nthose from Planck. The cosmological interpretation of these results in\ncombination with external datasets are presented in a companion paper.\n","versions":"[{'version': 'v1', 'created': 'Mon, 18 Nov 2024 20:03:34 GMT'}, {'version': 'v2', 'created': 'Tue, 10 Dec 2024 08:31:15 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 18:31:18 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['DESI Collaboration', '', ''], ['Adame', 'A. G.', ''], ['Aguilar', 'J.', ''], ['Ahlen', 'S.', ''], ['Alam', 'S.', ''], ['Alexander', 'D. M.', ''], ['Alvarez', 'M.', ''], ['Alves', 'O.', ''], ['Anand', 'A.', ''], ['Andrade', 'U.', ''], ['Armengaud', 'E.', ''], ['Avila', 'S.', ''], ['Aviles', 'A.', ''], ['Awan', 'H.', ''], ['Bailey', 'S.', ''], ['Baltay', 'C.', ''], ['Bault', 'A.', ''], ['Behera', 'J.', ''], ['BenZvi', 'S.', ''], ['Beutler', 'F.', ''], ['Bianchi', 'D.', ''], ['Blake', 'C.', ''], ['Blum', 'R.', ''], ['Brieden', 'S.', ''], ['Brodzeller', 'A.', ''], ['Brooks', 'D.', ''], ['Buckley-Geer', 'E.', ''], ['Burtin', 'E.', ''], ['Calderon', 'R.', ''], ['Canning', 'R.', ''], ['Rosell', 'A. Carnero', ''], ['Cereskaite', 'R.', ''], ['Cervantes-Cota', 'J. L.', ''], ['Chabanier', 'S.', ''], ['Chaussidon', 'E.', ''], ['Chaves-Montero', 'J.', ''], ['Chen', 'S.', ''], ['Chen', 'X.', ''], ['Claybaugh', 'T.', ''], ['Cole', 'S.', ''], ['Cuceu', 'A.', ''], ['Davis', 'T. M.', ''], ['Dawson', 'K.', ''], ['de la Macorra', 'A.', ''], ['de Mattia', 'A.', ''], ['Deiosso', 'N.', ''], ['Dey', 'A.', ''], ['Dey', 'B.', ''], ['Ding', 'Z.', ''], ['Doel', 'P.', ''], ['Edelstein', 'J.', ''], ['Eftekharzadeh', 'S.', ''], ['Eisenstein', 'D. J.', ''], ['Elliott', 'A.', ''], ['Fagrelius', 'P.', ''], ['Fanning', 'K.', ''], ['Ferraro', 'S.', ''], ['Ereza', 'J.', ''], ['Findlay', 'N.', ''], ['Flaugher', 'B.', ''], ['Font-Ribera', 'A.', ''], ['Forero-S\u00e1nchez', 'D.', ''], ['Forero-Romero', 'J. E.', ''], ['Garcia-Quintero', 'C.', ''], ['Garrison', 'L. H.', ''], ['Gazta\u00f1aga', 'E.', ''], ['Gil-Mar\u00edn', 'H.', ''], ['Gontcho', 'S. Gontcho A', ''], ['Gonzalez-Morales', 'A. X.', ''], ['Gonzalez-Perez', 'V.', ''], ['Gordon', 'C.', ''], ['Green', 'D.', ''], ['Gruen', 'D.', ''], ['Gsponer', 'R.', ''], ['Gutierrez', 'G.', ''], ['Guy', 'J.', ''], ['Hadzhiyska', 'B.', ''], ['Hahn', 'C.', ''], ['Hanif', 'M. M. S', ''], ['Herrera-Alcantar', 'H. K.', ''], ['Honscheid', 'K.', ''], ['Howlett', 'C.', ''], ['Huterer', 'D.', ''], ['Ir\u0161i\u010d', 'V.', ''], ['Ishak', 'M.', ''], ['Juneau', 'S.', ''], ['Kara\u00e7ayl\u0131', 'N. G.', ''], ['Kehoe', 'R.', ''], ['Kent', 'S.', ''], ['Kirkby', 'D.', ''], ['Kong', 'H.', ''], ['Koposov', 'S. E.', ''], ['Kremin', 'A.', ''], ['Krolewski', 'A.', ''], ['Lai', 'Y.', ''], ['Lan', 'T. -W.', ''], ['Landriau', 'M.', ''], ['Lang', 'D.', ''], ['Lasker', 'J.', ''], ['Goff', 'J. M. Le', ''], ['Guillou', 'L. Le', ''], ['Leauthaud', 'A.', ''], ['Levi', 'M. E.', ''], ['Li', 'T. S.', ''], ['Lodha', 'K.', ''], ['Magneville', 'C.', ''], ['Manera', 'M.', ''], ['Margala', 'D.', ''], ['Martini', 'P.', ''], ['Maus', 'M.', ''], ['McDonald', 'P.', ''], ['Medina-Varela', 'L.', ''], ['Meisner', 'A.', ''], ['Mena-Fern\u00e1ndez', 'J.', ''], ['Miquel', 'R.', ''], ['Moon', 'J.', ''], ['Moore', 'S.', ''], ['Moustakas', 'J.', ''], ['Mueller', 'E.', ''], ['Mu\u00f1oz-Guti\u00e9rrez', 'A.', ''], ['Myers', 'A. D.', ''], ['Nadathur', 'S.', ''], ['Napolitano', 'L.', ''], ['Neveux', 'R.', ''], ['Newman', 'J. A.', ''], ['Nguyen', 'N. M.', ''], ['Nie', 'J.', ''], ['Niz', 'G.', ''], ['Noriega', 'H. E.', ''], ['Padmanabhan', 'N.', ''], ['Paillas', 'E.', ''], ['Palanque-Delabrouille', 'N.', ''], ['Pan', 'J.', ''], ['Penmetsa', 'S.', ''], ['Percival', 'W. J.', ''], ['Pieri', 'M. M.', ''], ['Pinon', 'M.', ''], ['Poppett', 'C.', ''], ['Porredon', 'A.', ''], ['Prada', 'F.', ''], ['P\u00e9rez-Fern\u00e1ndez', 'A.', ''], ['P\u00e9rez-R\u00e0fols', 'I.', ''], ['Rabinowitz', 'D.', ''], ['Raichoor', 'A.', ''], ['Ram\u00edrez-P\u00e9rez', 'C.', ''], ['Ramirez-Solano', 'S.', ''], ['Rashkovetskyi', 'M.', ''], ['Ravoux', 'C.', ''], ['Rezaie', 'M.', ''], ['Rich', 'J.', ''], ['Rocher', 'A.', ''], ['Rockosi', 'C.', ''], ['Rodr\u00edguez-Mart\u00ednez', 'F.', ''], ['Roe', 'N. A.', ''], ['Rosado-Marin', 'A.', ''], ['Ross', 'A. J.', ''], ['Rossi', 'G.', ''], ['Ruggeri', 'R.', ''], ['Ruhlmann-Kleider', 'V.', ''], ['Samushia', 'L.', ''], ['Sanchez', 'E.', ''], ['Saulder', 'C.', ''], ['Schlafly', 'E. F.', ''], ['Schlegel', 'D.', ''], ['Schubnell', 'M.', ''], ['Seo', 'H.', ''], ['Sharples', 'R.', ''], ['Silber', 'J.', ''], ['Slosar', 'A.', ''], ['Smith', 'A.', ''], ['Sprayberry', 'D.', ''], ['Tan', 'T.', ''], ['Tarl\u00e9', 'G.', ''], ['Trusov', 'S.', ''], ['Vaisakh', 'R.', ''], ['Valcin', 'D.', ''], ['Valdes', 'F.', ''], ['Vargas-Maga\u00f1a', 'M.', ''], ['Verde', 'L.', ''], ['Walther', 'M.', ''], ['Wang', 'B.', ''], ['Wang', 'M. S.', ''], ['Weaver', 'B. A.', ''], ['Weaverdyck', 'N.', ''], ['Wechsler', 'R. H.', ''], ['Weinberg', 'D. H.', ''], ['White', 'M.', ''], ['Wilson', 'M. J.', ''], ['Yu', 'J.', ''], ['Yu', 'Y.', ''], ['Yuan', 'S.', ''], ['Y\u00e8che', 'C.', ''], ['Zaborowski', 'E. A.', ''], ['Zarrouk', 'P.', ''], ['Zhang', 'H.', ''], ['Zhao', 'C.', ''], ['Zhao', 'R.', ''], ['Zhou', 'R.', ''], ['Zou', 'H.', '']]","extracted_entities":"[{'text': 'confirmation bias', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"confirmation bias","similarity_score":0.5562574863}
{"id":2412.01383,"submitter":"Ivan DeAndres-Tame","authors":"Ivan DeAndres-Tame, Ruben Tolosana, Pietro Melzi, Ruben\n  Vera-Rodriguez, Minchul Kim, Christian Rathgeb, Xiaoming Liu, Luis F. Gomez,\n  Aythami Morales, Julian Fierrez, Javier Ortega-Garcia, Zhizhou Zhong, Yuge\n  Huang, Yuxi Mi, Shouhong Ding, Shuigeng Zhou, Shuai He, Lingzhi Fu, Heng\n  Cong, Rongyu Zhang, Zhihong Xiao, Evgeny Smirnov, Anton Pimenov, Aleksei\n  Grigorev, Denis Timoshenko, Kaleb Mesfin Asfaw, Cheng Yaw Low, Hao Liu, Chuyi\n  Wang, Qing Zuo, Zhixiang He, Hatef Otroshi Shahreza, Anjith George, Alexander\n  Unnervik, Parsa Rahimi, S\\'ebastien Marcel, Pedro C. Neto, Marco Huber, Jan\n  Niklas Kolf, Naser Damer, Fadi Boutros, Jaime S. Cardoso, Ana F. Sequeira,\n  Andrea Atzori, Gianni Fenu, Mirko Marras, Vitomir \\v{S}truc, Jiang Yu,\n  Zhangjie Li, Jichun Li, Weisong Zhao, Zhen Lei, Xiangyu Zhu, Xiao-Yu Zhang,\n  Bernardo Biesseck, Pedro Vidal, Luiz Coelho, Roger Granada, David Menotti","title":"Second FRCSyn-onGoing: Winning Solutions and Post-Challenge Analysis to\n  Improve Face Recognition with Synthetic Data","comments":"Accepted in Information Fusion","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CY cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Synthetic data is gaining increasing popularity for face recognition\ntechnologies, mainly due to the privacy concerns and challenges associated with\nobtaining real data, including diverse scenarios, quality, and demographic\ngroups, among others. It also offers some advantages over real data, such as\nthe large amount of data that can be generated or the ability to customize it\nto adapt to specific problem-solving needs. To effectively use such data, face\nrecognition models should also be specifically designed to exploit synthetic\ndata to its fullest potential. In order to promote the proposal of novel\nGenerative AI methods and synthetic data, and investigate the application of\nsynthetic data to better train face recognition systems, we introduce the 2nd\nFRCSyn-onGoing challenge, based on the 2nd Face Recognition Challenge in the\nEra of Synthetic Data (FRCSyn), originally launched at CVPR 2024. This is an\nongoing challenge that provides researchers with an accessible platform to\nbenchmark i) the proposal of novel Generative AI methods and synthetic data,\nand ii) novel face recognition systems that are specifically proposed to take\nadvantage of synthetic data. We focus on exploring the use of synthetic data\nboth individually and in combination with real data to solve current challenges\nin face recognition such as demographic bias, domain adaptation, and\nperformance constraints in demanding situations, such as age disparities\nbetween training and testing, changes in the pose, or occlusions. Very\ninteresting findings are obtained in this second edition, including a direct\ncomparison with the first one, in which synthetic databases were restricted to\nDCFace and GANDiffFace.\n","versions":"[{'version': 'v1', 'created': 'Mon, 2 Dec 2024 11:12:01 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 09:29:33 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['DeAndres-Tame', 'Ivan', ''], ['Tolosana', 'Ruben', ''], ['Melzi', 'Pietro', ''], ['Vera-Rodriguez', 'Ruben', ''], ['Kim', 'Minchul', ''], ['Rathgeb', 'Christian', ''], ['Liu', 'Xiaoming', ''], ['Gomez', 'Luis F.', ''], ['Morales', 'Aythami', ''], ['Fierrez', 'Julian', ''], ['Ortega-Garcia', 'Javier', ''], ['Zhong', 'Zhizhou', ''], ['Huang', 'Yuge', ''], ['Mi', 'Yuxi', ''], ['Ding', 'Shouhong', ''], ['Zhou', 'Shuigeng', ''], ['He', 'Shuai', ''], ['Fu', 'Lingzhi', ''], ['Cong', 'Heng', ''], ['Zhang', 'Rongyu', ''], ['Xiao', 'Zhihong', ''], ['Smirnov', 'Evgeny', ''], ['Pimenov', 'Anton', ''], ['Grigorev', 'Aleksei', ''], ['Timoshenko', 'Denis', ''], ['Asfaw', 'Kaleb Mesfin', ''], ['Low', 'Cheng Yaw', ''], ['Liu', 'Hao', ''], ['Wang', 'Chuyi', ''], ['Zuo', 'Qing', ''], ['He', 'Zhixiang', ''], ['Shahreza', 'Hatef Otroshi', ''], ['George', 'Anjith', ''], ['Unnervik', 'Alexander', ''], ['Rahimi', 'Parsa', ''], ['Marcel', 'S\u00e9bastien', ''], ['Neto', 'Pedro C.', ''], ['Huber', 'Marco', ''], ['Kolf', 'Jan Niklas', ''], ['Damer', 'Naser', ''], ['Boutros', 'Fadi', ''], ['Cardoso', 'Jaime S.', ''], ['Sequeira', 'Ana F.', ''], ['Atzori', 'Andrea', ''], ['Fenu', 'Gianni', ''], ['Marras', 'Mirko', ''], ['\u0160truc', 'Vitomir', ''], ['Yu', 'Jiang', ''], ['Li', 'Zhangjie', ''], ['Li', 'Jichun', ''], ['Zhao', 'Weisong', ''], ['Lei', 'Zhen', ''], ['Zhu', 'Xiangyu', ''], ['Zhang', 'Xiao-Yu', ''], ['Biesseck', 'Bernardo', ''], ['Vidal', 'Pedro', ''], ['Coelho', 'Luiz', ''], ['Granada', 'Roger', ''], ['Menotti', 'David', '']]","extracted_entities":"[{'text': 'demographic bias', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"demographic bias","similarity_score":0.540956676}
{"id":2501.00658,"submitter":"Peihao Wang","authors":"Peihao Wang, Ruisi Cai, Yuehao Wang, Jiajun Zhu, Pragya Srivastava,\n  Zhangyang Wang, Pan Li","title":"Understanding and Mitigating Bottlenecks of State Space Models through\n  the Lens of Recency and Over-smoothing","comments":"International Conference on Learning Representations (ICLR), 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Structured State Space Models (SSMs) have emerged as alternatives to\ntransformers. While SSMs are often regarded as effective in capturing\nlong-sequence dependencies, we rigorously demonstrate that they are inherently\nlimited by strong recency bias. Our empirical studies also reveal that this\nbias impairs the models' ability to recall distant information and introduces\nrobustness issues. Our scaling experiments then discovered that deeper\nstructures in SSMs can facilitate the learning of long contexts. However,\nsubsequent theoretical analysis reveals that as SSMs increase in depth, they\nexhibit another inevitable tendency toward over-smoothing, e.g., token\nrepresentations becoming increasingly indistinguishable. This fundamental\ndilemma between recency and over-smoothing hinders the scalability of existing\nSSMs. Inspired by our theoretical findings, we propose to polarize two channels\nof the state transition matrices in SSMs, setting them to zero and one,\nrespectively, simultaneously addressing recency bias and over-smoothing.\nExperiments demonstrate that our polarization technique consistently enhances\nthe associative recall accuracy of long-range tokens and unlocks SSMs to\nbenefit further from deeper architectures. All source codes are released at\nhttps:\/\/github.com\/VITA-Group\/SSM-Bottleneck.\n","versions":"[{'version': 'v1', 'created': 'Tue, 31 Dec 2024 22:06:39 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 03:58:57 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Wang', 'Peihao', ''], ['Cai', 'Ruisi', ''], ['Wang', 'Yuehao', ''], ['Zhu', 'Jiajun', ''], ['Srivastava', 'Pragya', ''], ['Wang', 'Zhangyang', ''], ['Li', 'Pan', '']]","extracted_entities":"[{'text': 'recency bias', 'label': 'Model Bias and Fairness'}, {'text': 'recency bias', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"recency bias","similarity_score":0.5899209976}
{"id":2503.04626,"submitter":"Yu Pan","authors":"Yu Pan, Chaozheng Wang, Zekai Wu, Qifan Wang, Min Zhang, Zenglin Xu","title":"IDInit: A Universal and Stable Initialization Method for Neural Network\n  Training","comments":"Accepted in ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Deep neural networks have achieved remarkable accomplishments in practice.\nThe success of these networks hinges on effective initialization methods, which\nare vital for ensuring stable and rapid convergence during training. Recently,\ninitialization methods that maintain identity transition within layers have\nshown good efficiency in network training. These techniques (e.g., Fixup) set\nspecific weights to zero to achieve identity control. However, settings of\nremaining weight (e.g., Fixup uses random values to initialize non-zero\nweights) will affect the inductive bias that is achieved only by a zero weight,\nwhich may be harmful to training. Addressing this concern, we introduce fully\nidentical initialization (IDInit), a novel method that preserves identity in\nboth the main and sub-stem layers of residual networks. IDInit employs a padded\nidentity-like matrix to overcome rank constraints in non-square weight\nmatrices. Furthermore, we show the convergence problem of an identity matrix\ncan be solved by stochastic gradient descent. Additionally, we enhance the\nuniversality of IDInit by processing higher-order weights and addressing dead\nneuron problems. IDInit is a straightforward yet effective initialization\nmethod, with improved convergence, stability, and performance across various\nsettings, including large-scale datasets and deep models.\n","versions":"[{'version': 'v1', 'created': 'Thu, 6 Mar 2025 17:12:46 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 16:31:31 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Pan', 'Yu', ''], ['Wang', 'Chaozheng', ''], ['Wu', 'Zekai', ''], ['Wang', 'Qifan', ''], ['Zhang', 'Min', ''], ['Xu', 'Zenglin', '']]","extracted_entities":"[{'text': 'inductive bias', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"inductive bias","similarity_score":0.5208441019}
{"id":2503.06431,"submitter":"Mingrui Zhang","authors":"Mingrui Zhang, Xiaowu Dai, Lexin Li","title":"Fairness-aware organ exchange and kidney paired donation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ME cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The kidney paired donation (KPD) program provides an innovative solution to\novercome incompatibility challenges in kidney transplants by matching\nincompatible donor-patient pairs and facilitating kidney exchanges. To address\nunequal access to transplant opportunities, there are two widely used fairness\ncriteria: group fairness and individual fairness. However, these criteria do\nnot consider protected patient features, which refer to characteristics legally\nor ethically recognized as needing protection from discrimination, such as race\nand gender. Motivated by the calibration principle in machine learning, we\nintroduce a new fairness criterion: the matching outcome should be\nconditionally independent of the protected feature, given the sensitization\nlevel. We integrate this fairness criterion as a constraint within the KPD\noptimization framework and propose a computationally efficient solution.\nTheoretically, we analyze the associated price of fairness using random graph\nmodels. Empirically, we compare our fairness criterion with group fairness and\nindividual fairness through both simulations and a real-data example.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 04:01:08 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhang', 'Mingrui', ''], ['Dai', 'Xiaowu', ''], ['Li', 'Lexin', '']]","extracted_entities":"[{'text': 'group fairness', 'label': 'Model Bias and Fairness'}, {'text': 'individual fairness', 'label': 'Model Bias and Fairness'}, {'text': 'group fairness', 'label': 'Model Bias and Fairness'}, {'text': 'individual fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"individual fairness","similarity_score":0.7425557375}
{"id":2503.06523,"submitter":"Gilad Abiri","authors":"Gilad Abiri","title":"Generative AI as Digital Media","comments":null,"journal-ref":"Harv. J. Sports & Ent. L. 15 (2024): 279","doi":null,"report-no":null,"categories":"cs.CY cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Generative AI is frequently portrayed as revolutionary or even apocalyptic,\nprompting calls for novel regulatory approaches. This essay argues that such\nviews are misguided. Instead, generative AI should be understood as an\nevolutionary step in the broader algorithmic media landscape, alongside search\nengines and social media. Like these platforms, generative AI centralizes\ninformation control, relies on complex algorithms to shape content, and\nextensively uses user data, thus perpetuating common problems: unchecked\ncorporate power, echo chambers, and weakened traditional gatekeepers.\nRegulation should therefore share a consistent objective: ensuring media\ninstitutions remain trustworthy. Without trust, public discourse risks\nfragmenting into isolated communities dominated by comforting, tribal beliefs\n-- a threat intensified by generative AI's capacity to bypass gatekeepers and\npersonalize truth. Current governance frameworks, such as the EU's AI Act and\nthe US Executive Order 14110, emphasize reactive risk mitigation, addressing\nmeasurable threats like national security, public health, and algorithmic bias.\nWhile effective for novel technological risks, this reactive approach fails to\nadequately address broader issues of trust and legitimacy inherent to digital\nmedia. Proactive regulation fostering transparency, accountability, and public\nconfidence is essential. Viewing generative AI exclusively as revolutionary\nrisks repeating past regulatory failures that left social media and search\nengines insufficiently regulated. Instead, regulation must proactively shape an\nalgorithmic media environment serving the public good, supporting quality\ninformation and robust civic discourse.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 08:58:17 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Abiri', 'Gilad', '']]","extracted_entities":"[{'text': 'algorithmic bias', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"algorithmic bias","similarity_score":0.6063995361}
{"id":2503.06579,"submitter":"George Chacko","authors":"George Chacko and Minhyuk Park and Vikram Ramavarapu and Ananth Grama\n  and Pablo Robles-Granda and Tandy Warnow","title":"An Agent-based Model of Citation Behavior","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Whether citations can be objectively and reliably used to measure\nproductivity and scientific quality of articles and researchers can, and\nshould, be vigorously questioned. However, citations are widely used to\nestimate the productivity of researchers and institutions, effectively creating\na 'grubby' motivation to be well-cited. We model citation growth, and this\ngrubby interest using an agent-based model (ABM) of network growth. In this\nmodel, each new node (article) in a citation network is an autonomous agent\nthat cites other nodes based on a 'citation personality' consisting of a\ncomposite bias for locality, preferential attachment, recency, and fitness. We\nask whether strategic citation behavior (reference selection) by the author of\na scientific article can boost subsequent citations to it. Our study suggests\nthat fitness and, to a lesser extent, out_degree and locality effects are\ninfluential in capturing citations, which raises questions about similar\neffects in the real world.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 12:19:26 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Chacko', 'George', ''], ['Park', 'Minhyuk', ''], ['Ramavarapu', 'Vikram', ''], ['Grama', 'Ananth', ''], ['Robles-Granda', 'Pablo', ''], ['Warnow', 'Tandy', '']]","extracted_entities":"[{'text': 'composite bias', 'label': 'Model Bias and Fairness'}, {'text': 'locality', 'label': 'Model Bias and Fairness'}, {'text': 'preferential attachment', 'label': 'Model Bias and Fairness'}, {'text': 'recency', 'label': 'Model Bias and Fairness'}, {'text': 'fitness', 'label': 'Model Bias and Fairness'}, {'text': 'fitness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"composite bias","similarity_score":0.6256318688}
{"id":2503.06635,"submitter":"Zaitian Wang","authors":"Zhiyuan Ning, Zaitian Wang, Ran Zhang, Ping Xu, Kunpeng Liu, Pengyang\n  Wang, Chong Chen, Pengfei Wang, Yuanchun Zhou, and Erik Cambria","title":"Deep Cut-informed Graph Embedding and Clustering","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Graph clustering aims to divide the graph into different clusters. The\nrecently emerging deep graph clustering approaches are largely built on graph\nneural networks (GNN). However, GNN is designed for general graph encoding and\nthere is a common issue of representation collapse in existing GNN-based deep\ngraph clustering algorithms. We attribute two main reasons for such issue: (i)\nthe inductive bias of GNN models: GNNs tend to generate similar representations\nfor proximal nodes. Since graphs often contain a non-negligible amount of\ninter-cluster links, the bias results in error message passing and leads to\nbiased clustering; (ii) the clustering guided loss function: most traditional\napproaches strive to make all samples closer to pre-learned cluster centers,\nwhich cause a degenerate solution assigning all data points to a single label\nthus make all samples and less discriminative. To address these challenges, we\ninvestigate graph clustering from a graph cut perspective and propose an\ninnovative and non-GNN-based Deep Cut-informed Graph embedding and Clustering\nframework, namely DCGC. This framework includes two modules: (i) cut-informed\ngraph encoding; (ii) self-supervised graph clustering via optimal transport.\nFor the encoding module, we derive a cut-informed graph embedding objective to\nfuse graph structure and attributes by minimizing their joint normalized cut.\nFor the clustering module, we utilize the optimal transport theory to obtain\nthe clustering assignments, which can balance the guidance of proximity to the\npre-learned cluster center. With the above two tailored designs, DCGC is more\nsuitable for the graph clustering task, which can effectively alleviate the\nproblem of representation collapse and achieve better performance. We conduct\nextensive experiments to demonstrate that our method is simple but effective\ncompared with benchmarks.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 14:24:09 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Ning', 'Zhiyuan', ''], ['Wang', 'Zaitian', ''], ['Zhang', 'Ran', ''], ['Xu', 'Ping', ''], ['Liu', 'Kunpeng', ''], ['Wang', 'Pengyang', ''], ['Chen', 'Chong', ''], ['Wang', 'Pengfei', ''], ['Zhou', 'Yuanchun', ''], ['Cambria', 'Erik', '']]","extracted_entities":"[{'text': 'GNN', 'label': 'AI model'}, {'text': 'inductive bias', 'label': 'Model Bias and Fairness'}, {'text': 'GNN', 'label': 'AI model'}, {'text': 'cut-informed\\ngraph encoding', 'label': 'Embedding'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"inductive bias","similarity_score":0.5208441019}
{"id":2404.11681,"submitter":"Cheng Ren","authors":"Xin Chen and Cheng Ren and Timothy A Thomas","title":"Evaluating Tenant-Landlord Tensions Using Generative AI on Online Tenant\n  Forums","comments":null,"journal-ref":"J Comput Soc Sc 8, 50 (2025)","doi":"10.1007\/s42001-025-00378-8","report-no":null,"categories":"cs.HC cs.CY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Tenant-landlord relationships exhibit a power asymmetry where landlords'\npower to evict the tenants at a low-cost results in their dominating status in\nsuch relationships. Tenant concerns are thus often unspoken, unresolved, or\nignored and this could lead to blatant conflicts as suppressed tenant concerns\naccumulate. Modern machine learning methods and Large Language Models (LLM)\nhave demonstrated immense abilities to perform language tasks. In this study,\nwe incorporate Latent Dirichlet Allocation (LDA) with GPT-4 to classify Reddit\npost data scraped from the subreddit r\/Tenant, aiming to unveil trends in\ntenant concerns while exploring the adoption of LLMs and machine learning\nmethods in social science research. We find that tenant concerns in topics like\nfee dispute and utility issues are consistently dominant in all four states\nanalyzed while each state has other common tenant concerns special to itself.\nMoreover, we discover temporal trends in tenant concerns that provide important\nimplications regarding the impact of the pandemic and the Eviction Moratorium.\n","versions":"[{'version': 'v1', 'created': 'Wed, 17 Apr 2024 18:20:31 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 16:41:52 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Chen', 'Xin', ''], ['Ren', 'Cheng', ''], ['Thomas', 'Timothy A', '']]","extracted_entities":"[{'text': 'GPT-4', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4","similarity_score":0.8572875857}
{"id":2406.11402,"submitter":"Neelabh Sinha","authors":"Neelabh Sinha, Vinija Jain, Aman Chadha","title":"Are Small Language Models Ready to Compete with Large Language Models\n  for Practical Applications?","comments":"Accepted at The Fifth Workshop on Trustworthy Natural Language\n  Processing (TrustNLP 2025) in Annual Conference of the Nations of the\n  Americas Chapter of the Association for Computational Linguistics (NAACL),\n  2025. 8 pages + references + Appendix","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The rapid rise of Language Models (LMs) has expanded their use in several\napplications. Yet, due to constraints of model size, associated cost, or\nproprietary restrictions, utilizing state-of-the-art (SOTA) LLMs is not always\nfeasible. With open, smaller LMs emerging, more applications can leverage their\ncapabilities, but selecting the right LM can be challenging as smaller LMs do\nnot perform well universally. This work tries to bridge this gap by proposing a\nframework to experimentally evaluate small, open LMs in practical settings\nthrough measuring semantic correctness of outputs across three practical\naspects: task types, application domains, and reasoning types, using diverse\nprompt styles. It also conducts an in-depth comparison of 10 small, open LMs to\nidentify the best LM and prompt style depending on specific application\nrequirements using the proposed framework. We also show that if selected\nappropriately, they can outperform SOTA LLMs like DeepSeek-v2, GPT-4o,\nGPT-4o-mini, Gemini-1.5-Pro, and even compete with GPT-4o.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Jun 2024 10:45:36 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Aug 2024 19:24:29 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 04:37:42 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Sinha', 'Neelabh', ''], ['Jain', 'Vinija', ''], ['Chadha', 'Aman', '']]","extracted_entities":"[{'text': 'prompt styles', 'label': 'Prompting'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'GPT-4o-mini', 'label': 'GPT-4'}, {'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2406.16855,"submitter":"Yuang Peng","authors":"Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai,\n  Chunrui Han, Zheng Ge, Xiangyu Zhang, Shu-Tao Xia","title":"DreamBench++: A Human-Aligned Benchmark for Personalized Image\n  Generation","comments":"ICLR 2025, Project page: https:\/\/dreambenchplus.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Personalized image generation holds great promise in assisting humans in\neveryday work and life due to its impressive ability to creatively generate\npersonalized content across various contexts. However, current evaluations\neither are automated but misalign with humans or require human evaluations that\nare time-consuming and expensive. In this work, we present DreamBench++, a\nhuman-aligned benchmark that advanced multimodal GPT models automate.\nSpecifically, we systematically design the prompts to let GPT be both\nhuman-aligned and self-aligned, empowered with task reinforcement. Further, we\nconstruct a comprehensive dataset comprising diverse images and prompts. By\nbenchmarking 7 modern generative models, we demonstrate that DreamBench++\nresults in significantly more human-aligned evaluation, helping boost the\ncommunity with innovative findings.\n","versions":"[{'version': 'v1', 'created': 'Mon, 24 Jun 2024 17:58:47 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 02:57:28 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Peng', 'Yuang', ''], ['Cui', 'Yuxin', ''], ['Tang', 'Haomiao', ''], ['Qi', 'Zekun', ''], ['Dong', 'Runpei', ''], ['Bai', 'Jing', ''], ['Han', 'Chunrui', ''], ['Ge', 'Zheng', ''], ['Zhang', 'Xiangyu', ''], ['Xia', 'Shu-Tao', '']]","extracted_entities":"[{'text': 'GPT', 'label': 'GPT'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'GPT', 'label': 'GPT'}, {'text': 'prompts', 'label': 'Prompting'}]","assigned_concept":"GPT","matched_keyword":"GPT","similarity_score":1.0000001192}
{"id":2410.02651,"submitter":"Maxence Faldor","authors":"Maxence Faldor, Antoine Cully","title":"CAX: Cellular Automata Accelerated in JAX","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Cellular automata have become a cornerstone for investigating emergence and\nself-organization across diverse scientific disciplines. However, the absence\nof a hardware-accelerated cellular automata library limits the exploration of\nnew research directions, hinders collaboration, and impedes reproducibility. In\nthis work, we introduce CAX (Cellular Automata Accelerated in JAX), a\nhigh-performance and flexible open-source library designed to accelerate\ncellular automata research. CAX delivers cutting-edge performance through\nhardware acceleration while maintaining flexibility through its modular\narchitecture, intuitive API, and support for both discrete and continuous\ncellular automata in arbitrary dimensions. We demonstrate CAX's performance and\nflexibility through a wide range of benchmarks and applications. From classic\nmodels like elementary cellular automata and Conway's Game of Life to advanced\napplications such as growing neural cellular automata and self-classifying\nMNIST digits, CAX speeds up simulations up to 2,000 times faster. Furthermore,\nwe demonstrate CAX's potential to accelerate research by presenting a\ncollection of three novel cellular automata experiments, each implemented in\njust a few lines of code thanks to the library's modular architecture. Notably,\nwe show that a simple one-dimensional cellular automaton can outperform GPT-4\non the 1D-ARC challenge.\n","versions":"[{'version': 'v1', 'created': 'Thu, 3 Oct 2024 16:36:05 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 11:34:10 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Faldor', 'Maxence', ''], ['Cully', 'Antoine', '']]","extracted_entities":"[{'text': 'CAX', 'label': 'Open-source LLMs'}, {'text': 'CAX', 'label': 'Open-source LLMs'}, {'text': 'CAX', 'label': 'Open-source LLMs'}, {'text': 'CAX', 'label': 'Open-source LLMs'}, {'text': 'CAX', 'label': 'Open-source LLMs'}, {'text': 'GPT-4', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4","similarity_score":0.8572875857}
{"id":2411.07521,"submitter":"Sina Bagheri Nezhad","authors":"Sina Bagheri Nezhad, Sayan Bandyapadhyay, Ameeta Agrawal","title":"Fair Summarization: Bridging Quality and Diversity in Extractive\n  Summaries","comments":"Accepted at AFLME@NeurIPS 2024 & C3NLP@NAACL 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Fairness in multi-document summarization of user-generated content remains a\ncritical challenge in natural language processing (NLP). Existing summarization\nmethods often fail to ensure equitable representation across different social\ngroups, leading to biased outputs. In this paper, we introduce two novel\nmethods for fair extractive summarization: FairExtract, a clustering-based\napproach, and FairGPT, which leverages GPT-3.5-turbo with fairness constraints.\nWe evaluate these methods using Divsumm summarization dataset of White-aligned,\nHispanic, and African-American dialect tweets and compare them against relevant\nbaselines. The results obtained using a comprehensive set of summarization\nquality metrics such as SUPERT, BLANC, SummaQA, BARTScore, and UniEval, as well\nas a fairness metric F, demonstrate that FairExtract and FairGPT achieve\nsuperior fairness while maintaining competitive summarization quality.\nAdditionally, we introduce composite metrics (e.g., SUPERT+F, BLANC+F) that\nintegrate quality and fairness into a single evaluation framework, offering a\nmore nuanced understanding of the trade-offs between these objectives. Our code\nis available online.\n","versions":"[{'version': 'v1', 'created': 'Tue, 12 Nov 2024 03:37:53 GMT'}, {'version': 'v2', 'created': 'Wed, 13 Nov 2024 04:03:54 GMT'}, {'version': 'v3', 'created': 'Wed, 5 Feb 2025 23:34:44 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Mar 2025 16:55:48 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Nezhad', 'Sina Bagheri', ''], ['Bandyapadhyay', 'Sayan', ''], ['Agrawal', 'Ameeta', '']]","extracted_entities":"[{'text': 'Fairness', 'label': 'Model Bias and Fairness'}, {'text': 'FairGPT', 'label': 'ChatGPT'}, {'text': 'GPT-3', 'label': 'GPT'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'FairGPT', 'label': 'ChatGPT'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"GPT","matched_keyword":"GPT-3","similarity_score":0.8771116138}
{"id":2411.18203,"submitter":"Junxian Li","authors":"Di Zhang, Junxian Li, Jingdi Lei, Xunzhi Wang, Yujie Liu, Zonglin\n  Yang, Jiatong Li, Weida Wang, Suorong Yang, Jianbo Wu, Peng Ye, Wanli Ouyang,\n  Dongzhan Zhou","title":"Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning","comments":"16 pages, 11 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Vision-language models (VLMs) have shown remarkable advancements in\nmultimodal reasoning tasks. However, they still often generate inaccurate or\nirrelevant responses due to issues like hallucinated image understandings or\nunrefined reasoning paths. To address these challenges, we introduce Critic-V,\na novel framework inspired by the Actor-Critic paradigm to boost the reasoning\ncapability of VLMs. This framework decouples the reasoning process and critic\nprocess by integrating two independent components: the Reasoner, which\ngenerates reasoning paths based on visual and textual inputs, and the Critic,\nwhich provides constructive critique to refine these paths. In this approach,\nthe Reasoner generates reasoning responses according to text prompts, which can\nevolve iteratively as a policy based on feedback from the Critic. This\ninteraction process was theoretically driven by a reinforcement learning\nframework where the Critic offers natural language critiques instead of scalar\nrewards, enabling more nuanced feedback to boost the Reasoner's capability on\ncomplex reasoning tasks. The Critic model is trained using Direct Preference\nOptimization (DPO), leveraging a preference dataset of critiques ranked by\nRule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results\nshow that the Critic-V framework significantly outperforms existing methods,\nincluding GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning\naccuracy and efficiency. Combining a dynamic text-based policy for the Reasoner\nand constructive feedback from the preference-optimized Critic enables a more\nreliable and context-sensitive multimodal reasoning process. Our approach\nprovides a promising solution to enhance the reliability of VLMs, improving\ntheir performance in real-world reasoning-heavy multimodal applications such as\nautonomous driving and embodied intelligence.\n","versions":"[{'version': 'v1', 'created': 'Wed, 27 Nov 2024 10:28:57 GMT'}, {'version': 'v2', 'created': 'Mon, 2 Dec 2024 05:00:19 GMT'}, {'version': 'v3', 'created': 'Mon, 16 Dec 2024 08:12:17 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Mar 2025 15:46:15 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Zhang', 'Di', ''], ['Li', 'Junxian', ''], ['Lei', 'Jingdi', ''], ['Wang', 'Xunzhi', ''], ['Liu', 'Yujie', ''], ['Yang', 'Zonglin', ''], ['Li', 'Jiatong', ''], ['Wang', 'Weida', ''], ['Yang', 'Suorong', ''], ['Wu', 'Jianbo', ''], ['Ye', 'Peng', ''], ['Ouyang', 'Wanli', ''], ['Zhou', 'Dongzhan', '']]","extracted_entities":"[{'text': 'text prompts', 'label': 'Prompting'}, {'text': 'GPT-4V', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4V","similarity_score":0.7028234005}
{"id":2411.19325,"submitter":"Muhammad Sohail Danish","authors":"Muhammad Sohail Danish, Muhammad Akhtar Munir, Syed Roshaan Ali Shah,\n  Kartik Kuckreja, Fahad Shahbaz Khan, Paolo Fraccaro, Alexandre Lacoste,\n  Salman Khan","title":"GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks","comments":"This updated version includes revisions and additional analysis","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While numerous recent benchmarks focus on evaluating generic Vision-Language\nModels (VLMs), they do not effectively address the specific challenges of\ngeospatial applications. Generic VLM benchmarks are not designed to handle the\ncomplexities of geospatial data, an essential component for applications such\nas environmental monitoring, urban planning, and disaster management. Key\nchallenges in the geospatial domain include temporal change detection,\nlarge-scale object counting, tiny object detection, and understanding\nrelationships between entities in remote sensing imagery. To bridge this gap,\nwe present GEOBench-VLM, a comprehensive benchmark specifically designed to\nevaluate VLMs on geospatial tasks, including scene understanding, object\ncounting, localization, fine-grained categorization, segmentation, and temporal\nanalysis. Our benchmark features over 10,000 manually verified instructions and\nspanning diverse visual conditions, object types, and scales. We evaluate\nseveral state-of-the-art VLMs to assess performance on geospatial-specific\nchallenges. The results indicate that although existing VLMs demonstrate\npotential, they face challenges when dealing with geospatial-specific tasks,\nhighlighting the room for further improvements. Notably, the best-performing\nLLaVa-OneVision achieves only 41.7% accuracy on MCQs, slightly more than\nGPT-4o, which is approximately double the random guess performance. Our\nbenchmark is publicly available at\nhttps:\/\/github.com\/The-AI-Alliance\/GEO-Bench-VLM .\n","versions":"[{'version': 'v1', 'created': 'Thu, 28 Nov 2024 18:59:56 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 19:28:05 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Danish', 'Muhammad Sohail', ''], ['Munir', 'Muhammad Akhtar', ''], ['Shah', 'Syed Roshaan Ali', ''], ['Kuckreja', 'Kartik', ''], ['Khan', 'Fahad Shahbaz', ''], ['Fraccaro', 'Paolo', ''], ['Lacoste', 'Alexandre', ''], ['Khan', 'Salman', '']]","extracted_entities":"[{'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2412.16359,"submitter":"Nilanjana Das","authors":"Nilanjana Das, Edward Raff, Manas Gaur","title":"Human-Readable Adversarial Prompts: An Investigation into LLM\n  Vulnerabilities Using Situational Context","comments":"arXiv admin note: text overlap with arXiv:2407.14644","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Previous studies that uncovered vulnerabilities in large language models\n(LLMs) frequently employed nonsensical adversarial prompts. However, such\nprompts can now be readily identified using automated detection techniques. To\nfurther strengthen adversarial attacks, we focus on human-readable adversarial\nprompts, which are more realistic and potent threats. Our key contributions are\n(1) situation-driven attacks leveraging movie scripts as context to create\nhuman-readable prompts that successfully deceive LLMs, (2) adversarial suffix\nconversion to transform nonsensical adversarial suffixes into independent\nmeaningful text, and (3) AdvPrompter with p-nucleus sampling, a method to\ngenerate diverse, human-readable adversarial suffixes, improving attack\nefficacy in models like GPT-3.5 and Gemma 7B.\n","versions":"[{'version': 'v1', 'created': 'Fri, 20 Dec 2024 21:43:52 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 21:41:19 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Das', 'Nilanjana', ''], ['Raff', 'Edward', ''], ['Gaur', 'Manas', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'nonsensical adversarial prompts', 'label': 'Prompting'}, {'text': 'human-readable adversarial\\nprompts', 'label': 'Prompting'}, {'text': 'human-readable prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GPT-3.5', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-3.5","similarity_score":0.8113200665}
{"id":2501.01428,"submitter":"Zhangyang Qi","authors":"Zhangyang Qi, Zhixiong Zhang, Ye Fang, Jiaqi Wang, Hengshuang Zhao","title":"GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models","comments":"Project page: https:\/\/gpt4scene.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In recent years, 2D Vision-Language Models (VLMs) have made significant\nstrides in image-text understanding tasks. However, their performance in 3D\nspatial comprehension, which is critical for embodied intelligence, remains\nlimited. Recent advances have leveraged 3D point clouds and multi-view images\nas inputs, yielding promising results. However, we propose exploring a purely\nvision-based solution inspired by human perception, which merely relies on\nvisual cues for 3D spatial understanding. This paper empirically investigates\nthe limitations of VLMs in 3D spatial knowledge, revealing that their primary\nshortcoming lies in the lack of global-local correspondence between the scene\nand individual frames. To address this, we introduce GPT4Scene, a novel visual\nprompting paradigm in VLM training and inference that helps build the\nglobal-local relationship, significantly improving the 3D spatial understanding\nof indoor scenes. Specifically, GPT4Scene constructs a Bird's Eye View (BEV)\nimage from the video and marks consistent object IDs across both frames and the\nBEV image. The model then inputs the concatenated BEV image and video frames\nwith markers. In zero-shot evaluations, GPT4Scene improves performance over\nclosed-source VLMs like GPT-4o. Additionally, we prepare a processed video\ndataset consisting of 165K text annotation to fine-tune open-source VLMs,\nachieving state-of-the-art performance on all 3D understanding tasks.\nSurprisingly, after training with the GPT4Scene paradigm, VLMs consistently\nimprove during inference, even without object marker prompting and BEV image as\nexplicit correspondence. It demonstrates that the proposed paradigm helps VLMs\ndevelop an intrinsic ability to understand 3D scenes, which paves the way for a\nseamless approach to extending pre-trained VLMs for 3D scene understanding.\n","versions":"[{'version': 'v1', 'created': 'Thu, 2 Jan 2025 18:59:59 GMT'}, {'version': 'v2', 'created': 'Fri, 3 Jan 2025 12:30:16 GMT'}, {'version': 'v3', 'created': 'Thu, 9 Jan 2025 16:41:07 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Mar 2025 07:54:04 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Qi', 'Zhangyang', ''], ['Zhang', 'Zhixiong', ''], ['Fang', 'Ye', ''], ['Wang', 'Jiaqi', ''], ['Zhao', 'Hengshuang', '']]","extracted_entities":"[{'text': 'zero-shot evaluations', 'label': 'Zero-shot Learning'}, {'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2501.09307,"submitter":"Zhen Luo","authors":"Zhen Luo, Yixuan Yang, Yanfu Zhang and Feng Zheng","title":"RoboReflect: A Robotic Reflective Reasoning Framework for Grasping\n  Ambiguous-Condition Objects","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  As robotic technology rapidly develops, robots are being employed in an\nincreasing number of fields. However, due to the complexity of deployment\nenvironments or the prevalence of ambiguous-condition objects, the practical\napplication of robotics still faces many challenges, leading to frequent\nerrors. Traditional methods and some LLM-based approaches, although improved,\nstill require substantial human intervention and struggle with autonomous error\ncorrection in complex scenarios. In this work, we propose RoboReflect, a novel\nframework leveraging large vision-language models (LVLMs) to enable\nself-reflection and autonomous error correction in robotic grasping tasks.\nRoboReflect allows robots to automatically adjust their strategies based on\nunsuccessful attempts until successful execution is achieved. The corrected\nstrategies are saved in the memory for future task reference. We evaluate\nRoboReflect through extensive testing on eight common objects prone to\nambiguous conditions of three categories. Our results demonstrate that\nRoboReflect not only outperforms existing grasp pose estimation methods like\nAnyGrasp and high-level action planning techniques ReKep with GPT-4V but also\nsignificantly enhances the robot's capability to adapt and correct errors\nindependently. These findings underscore the critical importance of autonomous\nself-reflection in robotic systems while effectively addressing the challenges\nposed by ambiguous-condition environments.\n","versions":"[{'version': 'v1', 'created': 'Thu, 16 Jan 2025 05:40:37 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 08:46:11 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Luo', 'Zhen', ''], ['Yang', 'Yixuan', ''], ['Zhang', 'Yanfu', ''], ['Zheng', 'Feng', '']]","extracted_entities":"[{'text': 'GPT-4V', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4V","similarity_score":0.7028234005}
{"id":2501.13802,"submitter":"Mowafak Allaham","authors":"Mowafak Allaham, Ayse D. Lokmanoglu, P. Sol Hart, Erik C. Nisbet","title":"Enhancing LLMs for Governance with Human Oversight: Evaluating and\n  Aligning LLMs on Expert Classification of Climate Misinformation for\n  Detecting False or Misleading Claims about Climate Change","comments":"International Workshop on AI Governance: Alignment, Morality and Law\n  (AIGOV) 2025. AAAI Conference on Artificial Intelligence","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Climate misinformation is a problem that has the potential to be\nsubstantially aggravated by the development of Large Language Models (LLMs). In\nthis study we evaluate the potential for LLMs to be part of the solution for\nmitigating online dis\/misinformation rather than the problem. Employing a\npublic expert annotated dataset and a curated sample of social media content we\nevaluate the performance of proprietary vs. open source LLMs on climate\nmisinformation classification task, comparing them to existing climate-focused\ncomputer-assisted tools and expert assessments. Results show (1) open-source\nmodels substantially under-perform in classifying climate misinformation\ncompared to proprietary models, (2) existing climate-focused computer-assisted\ntools leveraging expert-annotated datasets continues to outperform many of\nproprietary models, including GPT-4o, and (3) demonstrate the efficacy and\ngeneralizability of fine-tuning GPT-3.5-turbo on expert annotated dataset in\nclassifying claims about climate change at the equivalency of climate change\nexperts with over 20 years of experience in climate communication. These\nfindings highlight 1) the importance of incorporating human-oversight, such as\nincorporating expert-annotated datasets in training LLMs, for governance tasks\nthat require subject-matter expertise like classifying climate misinformation,\nand 2) the potential for LLMs in facilitating civil society organizations to\nengage in various governance tasks such as classifying false or misleading\nclaims in domains beyond climate change such as politics and health science.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 Jan 2025 16:21:15 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 16:39:06 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Allaham', 'Mowafak', ''], ['Lokmanoglu', 'Ayse D.', ''], ['Hart', 'P. Sol', ''], ['Nisbet', 'Erik C.', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'GPT-3', 'label': 'GPT'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"GPT","matched_keyword":"GPT-3","similarity_score":0.8771116138}
{"id":2501.14225,"submitter":"Rong Ye","authors":"Rong Ye, Yongxin Zhang, Yikai Zhang, Haoyu Kuang, Zhongyu Wei, Peng\n  Sun","title":"Multi-agent KTO: Reinforcing Strategic Interactions of Large Language\n  Model in Language Game","comments":"Preprint. Code and data will be available at\n  https:\/\/reneeye.github.io\/MaKTO.html","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Achieving Artificial General Intelligence (AGI) requires AI agents that can\nnot only make stratigic decisions but also engage in flexible and meaningful\ncommunication. Inspired by Wittgenstein's language game theory in Philosophical\nInvestigations, we propose that language agents can learn through in-context\ninteraction rather than traditional multi-stage frameworks that separate\ndecision-making from language expression. Using Werewolf, a social deduction\ngame that tests language understanding, strategic interaction, and\nadaptability, we develop the Multi-agent Kahneman & Tversky's Optimization\n(MaKTO). MaKTO engages diverse models in extensive gameplay to generate\nunpaired desirable and unacceptable responses, then employs KTO to refine the\nmodel's decision-making process. In 9-player Werewolf games, MaKTO achieves a\n61% average win rate across various models, outperforming GPT-4o and two-stage\nRL agents by relative improvements of 23.0% and 10.9%, respectively. Notably,\nMaKTO also demonstrates human-like performance, winning 60% against expert\nplayers and showing only 49% detectability in Turing-style blind tests.\n","versions":"[{'version': 'v1', 'created': 'Fri, 24 Jan 2025 04:09:03 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 03:55:17 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Ye', 'Rong', ''], ['Zhang', 'Yongxin', ''], ['Zhang', 'Yikai', ''], ['Kuang', 'Haoyu', ''], ['Wei', 'Zhongyu', ''], ['Sun', 'Peng', '']]","extracted_entities":"[{'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2502.07072,"submitter":"Sayem Mohammad Imtiaz","authors":"Sayem Mohammad Imtiaz, Astha Singh, Fraol Batole, Hridesh Rajan","title":"IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large\n  Language Models","comments":"Accepted as full research paper at FSE'2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Not a day goes by without hearing about the impressive feats of large\nlanguage models (LLMs), and equally, not a day passes without hearing about\ntheir challenges. LLMs are notoriously vulnerable to biases in their dataset,\nleading to issues such as toxicity. While domain-adaptive training has been\nemployed to mitigate these issues, these techniques often address all model\nparameters indiscriminately during the repair process, resulting in poor repair\nquality and reduced model versatility. In this paper, we introduce a novel\ndynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach\nselectively targets the most error-prone sections of the model for repair.\nSpecifically, we propose dynamically slicing the model's most sensitive layers\nthat require immediate attention, concentrating repair efforts on those areas.\nThis method enables more effective repairs with potentially less impact on the\nmodel's overall performance by altering a smaller portion of the model. We\nevaluated our technique on three models from the GPT2 and GPT-Neo families,\nwith parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our\nresults show that IRepair repairs errors 43.6% more effectively while causing\n46% less disruption to general performance compared to the closest baseline,\ndirect preference optimization. Our empirical analysis also reveals that errors\nare more concentrated in a smaller section of the model, with the top 20% of\nlayers exhibiting 773% more error density than the remaining 80\\%. This\nhighlights the need for selective repair. Additionally, we demonstrate that a\ndynamic selection approach is essential for addressing errors dispersed\nthroughout the model, ensuring a robust and efficient repair.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Feb 2025 22:07:02 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Feb 2025 05:14:41 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 17:08:05 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Imtiaz', 'Sayem Mohammad', ''], ['Singh', 'Astha', ''], ['Batole', 'Fraol', ''], ['Rajan', 'Hridesh', '']]","extracted_entities":"[{'text': 'domain-adaptive training', 'label': 'Few-shot Learning'}, {'text': 'GPT2', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT2","similarity_score":0.8684533238}
{"id":2502.15969,"submitter":"William Rudman Jr","authors":"William Rudman, Michal Golovanesky, Amir Bar, Vedant Palit, Yann\n  LeCun, Carsten Eickhoff, Ritambhara Singh","title":"Forgotten Polygons: Multimodal Large Language Models are Shape-Blind","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Despite strong performance on vision-language tasks, Multimodal Large\nLanguage Models (MLLMs) struggle with mathematical problem-solving, with both\nopen-source and state-of-the-art models falling short of human performance on\nvisual-math benchmarks. To systematically examine visual-mathematical reasoning\nin MLLMs, we (1) evaluate their understanding of geometric primitives, (2) test\nmulti-step reasoning, and (3) explore a potential solution to improve visual\nreasoning capabilities. Our findings reveal fundamental shortcomings in shape\nrecognition, with top models achieving under 50% accuracy in identifying\nregular polygons. We analyze these failures through the lens of dual-process\ntheory and show that MLLMs rely on System 1 (intuitive, memorized associations)\nrather than System 2 (deliberate reasoning). Consequently, MLLMs fail to count\nthe sides of both familiar and novel shapes, suggesting they have neither\nlearned the concept of sides nor effectively process visual inputs. Finally, we\npropose Visually Cued Chain-of-Thought (VC-CoT) prompting, which enhances\nmulti-step mathematical reasoning by explicitly referencing visual annotations\nin diagrams, boosting GPT-4o's accuracy on an irregular polygon side-counting\ntask from 7% to 93%. Our findings suggest that System 2 reasoning in MLLMs\nremains an open problem, and visually-guided prompting is essential for\nsuccessfully engaging visual reasoning. Code available at:\nhttps:\/\/github.com\/rsinghlab\/Shape-Blind.\n","versions":"[{'version': 'v1', 'created': 'Fri, 21 Feb 2025 22:04:09 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 15:28:50 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Rudman', 'William', ''], ['Golovanesky', 'Michal', ''], ['Bar', 'Amir', ''], ['Palit', 'Vedant', ''], ['LeCun', 'Yann', ''], ['Eickhoff', 'Carsten', ''], ['Singh', 'Ritambhara', '']]","extracted_entities":"[{'text': 'Multimodal Large\\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'Visually Cued Chain-of-Thought (VC-CoT) prompting', 'label': 'Prompting'}, {'text': 'multi-step mathematical reasoning', 'label': 'Chain of thought'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'visually-guided prompting', 'label': 'Prompting'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2502.18485,"submitter":"Jiaqi Xu","authors":"Jiaqi Xu, Cuiling Lan, Xuejin Chen, Yan Lu","title":"Deciphering Functions of Neurons in Vision-Language Models","comments":"22 pages, 23 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"q-bio.NC cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The burgeoning growth of open-sourced vision-language models (VLMs) has\ncatalyzed a plethora of applications across diverse domains. Ensuring the\ntransparency and interpretability of these models is critical for fostering\ntrustworthy and responsible AI systems. In this study, our objective is to\ndelve into the internals of VLMs to interpret the functions of individual\nneurons. We observe the activations of neurons with respects to the input\nvisual tokens and text tokens, and reveal some interesting findings.\nParticularly, we found that there are neurons responsible for only visual or\ntext information, or both, respectively, which we refer to them as visual\nneurons, text neurons, and multi-modal neurons, respectively. We build a\nframework that automates the explanation of neurons with the assistant of\nGPT-4o. Meanwhile, for visual neurons, we propose an activation simulator to\nassess the reliability of the explanations for visual neurons. System\nstatistical analyses on top of one representative VLM of LLaVA, uncover the\nbehaviors\/characteristics of different categories of neurons.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Feb 2025 10:00:06 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Feb 2025 06:32:05 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 07:13:38 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Xu', 'Jiaqi', ''], ['Lan', 'Cuiling', ''], ['Chen', 'Xuejin', ''], ['Lu', 'Yan', '']]","extracted_entities":"[{'text': 'VLMs', 'label': 'Open-source LLMs'}, {'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2502.18778,"submitter":"Kaiyou Song","authors":"Qingpei Guo and Kaiyou Song and Zipeng Feng and Ziping Ma and Qinglong\n  Zhang and Sirui Gao and Xuzheng Yu and Yunxiao Sun and Tai-Wei Chang and\n  Jingdong Chen and Ming Yang and Jun Zhou","title":"M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with\n  Competitive Performance","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We present M2-omni, a cutting-edge, open-source omni-MLLM that achieves\ncompetitive performance to GPT-4o. M2-omni employs a unified multimodal\nsequence modeling framework, which empowers Large Language Models(LLMs) to\nacquire comprehensive cross-modal understanding and generation capabilities.\nSpecifically, M2-omni can process arbitrary combinations of audio, video,\nimage, and text modalities as input, generating multimodal sequences\ninterleaving with audio, image, or text outputs, thereby enabling an advanced\nand interactive real-time experience. The training of such an omni-MLLM is\nchallenged by significant disparities in data quantity and convergence rates\nacross modalities. To address these challenges, we propose a step balance\nstrategy during pre-training to handle the quantity disparities in\nmodality-specific data. Additionally, a dynamically adaptive balance strategy\nis introduced during the instruction tuning stage to synchronize the\nmodality-wise training progress, ensuring optimal convergence. Notably, we\nprioritize preserving strong performance on pure text tasks to maintain the\nrobustness of M2-omni's language understanding capability throughout the\ntraining process. To our best knowledge, M2-omni is currently a very\ncompetitive open-source model to GPT-4o, characterized by its comprehensive\nmodality and task support, as well as its exceptional performance. We expect\nM2-omni will advance the development of omni-MLLMs, thus facilitating future\nresearch in this domain.\n","versions":"[{'version': 'v1', 'created': 'Wed, 26 Feb 2025 03:21:12 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 04:11:38 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Guo', 'Qingpei', ''], ['Song', 'Kaiyou', ''], ['Feng', 'Zipeng', ''], ['Ma', 'Ziping', ''], ['Zhang', 'Qinglong', ''], ['Gao', 'Sirui', ''], ['Yu', 'Xuzheng', ''], ['Sun', 'Yunxiao', ''], ['Chang', 'Tai-Wei', ''], ['Chen', 'Jingdong', ''], ['Yang', 'Ming', ''], ['Zhou', 'Jun', '']]","extracted_entities":"[{'text': 'M2-omni', 'label': 'Open-source LLMs'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'M2-omni', 'label': 'Open-source LLMs'}, {'text': 'M2-omni', 'label': 'Open-source LLMs'}, {'text': 'instruction tuning stage', 'label': 'Fine-tuning'}, {'text': 'M2-omni', 'label': 'Open-source LLMs'}, {'text': 'M2-omni', 'label': 'Open-source LLMs'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'M2-omni', 'label': 'Open-source LLMs'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2503.06424,"submitter":"Alexander Scarlatos","authors":"Alexander Scarlatos, Naiming Liu, Jaewook Lee, Richard Baraniuk,\n  Andrew Lan","title":"Training LLM-based Tutors to Improve Student Learning Outcomes in\n  Dialogues","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CY","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Generative artificial intelligence (AI) has the potential to scale up\npersonalized tutoring through large language models (LLMs). Recent AI tutors\nare adapted for the tutoring task by training or prompting LLMs to follow\neffective pedagogical principles, though they are not trained to maximize\nstudent learning throughout the course of a dialogue. Therefore, they may\nengage with students in a suboptimal way. We address this limitation by\nintroducing an approach to train LLMs to generate tutor utterances that\nmaximize the likelihood of student correctness, while still encouraging the\nmodel to follow good pedagogical practice. Specifically, we generate a set of\ncandidate tutor utterances and score them using (1) an LLM-based student model\nto predict the chance of correct student responses and (2) a pedagogical rubric\nevaluated by GPT-4o. We then use the resulting data to train an open-source\nLLM, Llama 3.1 8B, using direct preference optimization. We show that tutor\nutterances generated by our model lead to significantly higher chances of\ncorrect student responses while maintaining the pedagogical quality of GPT-4o.\nWe also conduct qualitative analyses and a human evaluation to demonstrate that\nour model generates high quality tutor utterances.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 03:38:55 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Scarlatos', 'Alexander', ''], ['Liu', 'Naiming', ''], ['Lee', 'Jaewook', ''], ['Baraniuk', 'Richard', ''], ['Lan', 'Andrew', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'Llama 3.1 8B', 'label': 'Llama'}, {'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2503.06492,"submitter":"Yanling Wang","authors":"Yanling Wang, Yihan Zhao, Xiaodong Chen, Shasha Guo, Lixin Liu,\n  Haoyang Li, Yong Xiao, Jing Zhang, Qi Li, Ke Xu","title":"VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large\n  Vision-Language Models in Fact-Seeking Question Answering","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large vision-language models (LVLMs) have demonstrated remarkable\nachievements, yet the generation of non-factual responses remains prevalent in\nfact-seeking question answering (QA). Current multimodal fact-seeking\nbenchmarks primarily focus on comparing model outputs to ground truth answers,\nproviding limited insights into the performance of modality-specific modules.\nTo bridge this gap, we introduce VisualSimpleQA, a multimodal fact-seeking\nbenchmark with two key features. First, it enables streamlined and decoupled\nevaluation of LVLMs in visual and linguistic modalities. Second, it\nincorporates well-defined difficulty criteria to guide human annotation and\nfacilitates the extraction of a challenging subset, VisualSimpleQA-hard.\nExperiments on 15 LVLMs show that even state-of-the-art models such as GPT-4o\nachieve merely 60%+ correctness in multimodal fact-seeking QA on VisualSimpleQA\nand 30%+ on VisualSimpleQA-hard. Furthermore, the decoupled evaluation across\nthese models highlights substantial opportunities for improvement in both\nvisual and linguistic modules. The dataset is available at\nhttps:\/\/huggingface.co\/datasets\/WYLing\/VisualSimpleQA.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 07:25:32 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Wang', 'Yanling', ''], ['Zhao', 'Yihan', ''], ['Chen', 'Xiaodong', ''], ['Guo', 'Shasha', ''], ['Liu', 'Lixin', ''], ['Li', 'Haoyang', ''], ['Xiao', 'Yong', ''], ['Zhang', 'Jing', ''], ['Li', 'Qi', ''], ['Xu', 'Ke', '']]","extracted_entities":"[{'text': 'Large vision-language models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2305.18226,"submitter":"Christoforos Vasilatos","authors":"Christoforos Vasilatos, Manaar Alam, Talal Rahwan, Yasir Zaki and\n  Michail Maniatakos","title":"HowkGPT: Investigating the Detection of ChatGPT-generated University\n  Student Homework through Context-Aware Perplexity Analysis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  As the use of Large Language Models (LLMs) in text generation tasks\nproliferates, concerns arise over their potential to compromise academic\nintegrity. The education sector currently tussles with distinguishing\nstudent-authored homework assignments from AI-generated ones. This paper\naddresses the challenge by introducing HowkGPT, designed to identify homework\nassignments generated by AI. HowkGPT is built upon a dataset of academic\nassignments and accompanying metadata [17] and employs a pretrained LLM to\ncompute perplexity scores for student-authored and ChatGPT-generated responses.\nThese scores then assist in establishing a threshold for discerning the origin\nof a submitted assignment. Given the specificity and contextual nature of\nacademic work, HowkGPT further refines its analysis by defining\ncategory-specific thresholds derived from the metadata, enhancing the precision\nof the detection. This study emphasizes the critical need for effective\nstrategies to uphold academic integrity amidst the growing influence of LLMs\nand provides an approach to ensuring fair and accurate grading in educational\ninstitutions.\n","versions":"[{'version': 'v1', 'created': 'Fri, 26 May 2023 11:07:25 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Jun 2023 11:43:44 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 08:08:05 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Vasilatos', 'Christoforos', ''], ['Alam', 'Manaar', ''], ['Rahwan', 'Talal', ''], ['Zaki', 'Yasir', ''], ['Maniatakos', 'Michail', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'academic\\nintegrity', 'label': 'AI Ethics'}, {'text': 'academic integrity', 'label': 'AI Ethics'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2310.11829,"submitter":"Jiawei Liu","authors":"Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei\n  Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S. Yu, Chuan Shi","title":"Graph Foundation Models: Concepts, Opportunities and Challenges","comments":"This is the author's version of the accepted paper (not the\n  IEEE-published version). Citation information: DOI\n  10.1109\/TPAMI.2025.3548729. For access to the final edited and published\n  article, please follow the link provided:\n  https:\/\/ieeexplore.ieee.org\/document\/10915556","journal-ref":null,"doi":"10.1109\/TPAMI.2025.3548729","report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Foundation models have emerged as critical components in a variety of\nartificial intelligence applications, and showcase significant success in\nnatural language processing and several other domains. Meanwhile, the field of\ngraph machine learning is witnessing a paradigm transition from shallow methods\nto more sophisticated deep learning approaches. The capabilities of foundation\nmodels in generalization and adaptation motivate graph machine learning\nresearchers to discuss the potential of developing a new graph learning\nparadigm. This paradigm envisions models that are pre-trained on extensive\ngraph data and can be adapted for various graph tasks. Despite this burgeoning\ninterest, there is a noticeable lack of clear definitions and systematic\nanalyses pertaining to this new domain. To this end, this article introduces\nthe concept of Graph Foundation Models (GFMs), and offers an exhaustive\nexplanation of their key characteristics and underlying technologies. We\nproceed to classify the existing work related to GFMs into three distinct\ncategories, based on their dependence on graph neural networks and large\nlanguage models. In addition to providing a thorough review of the current\nstate of GFMs, this article also outlooks potential avenues for future research\nin this rapidly evolving domain.\n","versions":"[{'version': 'v1', 'created': 'Wed, 18 Oct 2023 09:31:21 GMT'}, {'version': 'v2', 'created': 'Sat, 2 Dec 2023 08:36:17 GMT'}, {'version': 'v3', 'created': 'Mon, 1 Jul 2024 02:06:42 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Mar 2025 16:14:30 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Liu', 'Jiawei', ''], ['Yang', 'Cheng', ''], ['Lu', 'Zhiyuan', ''], ['Chen', 'Junze', ''], ['Li', 'Yibo', ''], ['Zhang', 'Mengmei', ''], ['Bai', 'Ting', ''], ['Fang', 'Yuan', ''], ['Sun', 'Lichao', ''], ['Yu', 'Philip S.', ''], ['Shi', 'Chuan', '']]","extracted_entities":"[{'text': 'Foundation models', 'label': 'Foundation Model'}, {'text': 'foundation\\nmodels', 'label': 'Foundation Model'}, {'text': 'Graph Foundation Models', 'label': 'Foundation Model'}, {'text': 'GFMs', 'label': 'Foundation Model'}, {'text': 'large\\nlanguage models', 'label': 'Large Language Model'}, {'text': 'GFMs', 'label': 'Foundation Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large\nlanguage models","similarity_score":0.9664971828}
{"id":2312.10321,"submitter":"Fuheng Zhao","authors":"Fuheng Zhao, Jiayue Chen, Lawrence Lim, Ishtiyaque Ahmad, Divyakant\n  Agrawal, Amr El Abbadi","title":"LLM-SQL-Solver: Can LLMs Determine SQL Equivalence?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DB cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Judging the equivalence between two SQL queries is a fundamental problem with\nmany practical applications in data management and SQL generation (i.e.,\nevaluating the quality of generated SQL queries in text-to-SQL task). While the\nresearch community has reasoned about SQL equivalence for decades, it poses\nconsiderable difficulties and no complete solutions exist. Recently, Large\nLanguage Models (LLMs) have shown strong reasoning capability in conversation,\nquestion answering and solving mathematics challenges. In this paper, we study\nif LLMs can be used to determine the equivalence between SQL queries under two\nnotions of SQL equivalence (semantic equivalence and relaxed equivalence). To\nassist LLMs in generating high quality responses, we present two prompting\ntechniques: Miniature & Mull and Explain & Compare. The former technique is\nused to evaluate the semantic equivalence in which it asks LLMs to execute a\nquery on a simple database instance and then explore if a counterexample exists\nby modifying the database. The latter technique is used to evaluate the relaxed\nequivalence in which it asks LLMs to explain the queries and then compare if\nthey contain significant logical differences. Our experiments demonstrate using\nour techniques, LLMs is a promising tool to help data engineers in writing\nsemantically equivalent SQL queries, however challenges still persist, and is a\nbetter metric for evaluating SQL generation than the popular execution\naccuracy.\n","versions":"[{'version': 'v1', 'created': 'Sat, 16 Dec 2023 05:01:23 GMT'}, {'version': 'v2', 'created': 'Wed, 17 Jan 2024 20:11:38 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Jun 2024 20:19:00 GMT'}, {'version': 'v4', 'created': 'Wed, 12 Mar 2025 03:16:27 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Zhao', 'Fuheng', ''], ['Chen', 'Jiayue', ''], ['Lim', 'Lawrence', ''], ['Ahmad', 'Ishtiyaque', ''], ['Agrawal', 'Divyakant', ''], ['Abbadi', 'Amr El', '']]","extracted_entities":"[{'text': 'Large\\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Miniature & Mull', 'label': 'Prompting'}, {'text': 'Explain & Compare', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large\nLanguage Models","similarity_score":0.9664971828}
{"id":2312.1596,"submitter":"Jingyao Li","authors":"Jingyao Li, Pengguang Chen, Bin Xia, Hong Xu, Jiaya Jia","title":"MoTCoder: Elevating Large Language Models with Modular of Thought for\n  Challenging Programming Tasks","comments":"Model: https:\/\/huggingface.co\/JingyaoLi\/MoTCoder-15B-v1.0. Code:\n  https:\/\/github.com\/dvlab-research\/MoTCoder","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.PL cs.SE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) have showcased impressive capabilities in\nhandling straightforward programming tasks. However, their performance tends to\nfalter when confronted with more challenging programming problems. We observe\nthat conventional models often generate solutions as monolithic code blocks,\nrestricting their effectiveness in tackling intricate questions. To overcome\nthis limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a\npioneering framework for MoT instruction tuning, designed to promote the\ndecomposition of tasks into logical sub-tasks and sub-modules. Our\ninvestigations reveal that, through the cultivation and utilization of\nsub-modules, MoTCoder significantly improves both the modularity and\ncorrectness of the generated solutions, leading to substantial relative pass@1\nimprovements of 12.9% on APPS and 9.43% on CodeContests. Our codes are\navailable at https:\/\/github.com\/dvlab-research\/MoTCoder.\n","versions":"[{'version': 'v1', 'created': 'Tue, 26 Dec 2023 08:49:57 GMT'}, {'version': 'v2', 'created': 'Fri, 5 Jan 2024 10:33:32 GMT'}, {'version': 'v3', 'created': 'Thu, 22 Aug 2024 06:24:12 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 05:36:12 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Jingyao', ''], ['Chen', 'Pengguang', ''], ['Xia', 'Bin', ''], ['Xu', 'Hong', ''], ['Jia', 'Jiaya', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2312.16893,"submitter":"Zhecheng Sheng","authors":"Zhecheng Sheng, Tianhao Zhang, Chen Jiang, Dongyeop Kang","title":"BBScore: A Brownian Bridge Based Metric for Assessing Text Coherence","comments":"Accepted to the 38th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-24)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Measuring the coherence of text is a vital aspect of evaluating the quality\nof written content. Recent advancements in neural coherence modeling have\ndemonstrated their efficacy in capturing entity coreference and discourse\nrelations, thereby enhancing coherence evaluation. However, many existing\nmethods heavily depend on static embeddings or focus narrowly on nearby\ncontext, constraining their capacity to measure the overarching coherence of\nlong texts. In this paper, we posit that coherent texts inherently manifest a\nsequential and cohesive interplay among sentences, effectively conveying the\ncentral theme, purpose, or standpoint. To explore this abstract relationship,\nwe introduce the \"BBScore,\" a novel reference-free metric grounded in Brownian\nbridge theory for assessing text coherence. Our findings showcase that when\nsynergized with a simple additional classification component, this metric\nattains a performance level comparable to state-of-the-art techniques on\nstandard artificial discrimination tasks. We also establish in downstream tasks\nthat this metric effectively differentiates between human-written documents and\ntext generated by large language models under a specific domain. Furthermore,\nwe illustrate the efficacy of this approach in detecting written styles\nattributed to diverse large language models, underscoring its potential for\ngeneralizability. In summary, we present a novel Brownian bridge coherence\nmetric capable of measuring both local and global text coherence, while\ncircumventing the need for end-to-end model training. This flexibility allows\nfor its application in various downstream tasks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 28 Dec 2023 08:34:17 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 19:00:39 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Sheng', 'Zhecheng', ''], ['Zhang', 'Tianhao', ''], ['Jiang', 'Chen', ''], ['Kang', 'Dongyeop', '']]","extracted_entities":"[{'text': 'static embeddings', 'label': 'contextual Embedding'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2402.03848,"submitter":"David Peer","authors":"David Peer, Philemon Sch\\\"opf, Volckmar Nebendahl, Alexander Rietzler,\n  Sebastian Stabinger","title":"ANLS* -- A Universal Document Processing Metric for Generative Large\n  Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Traditionally, discriminative models have been the predominant choice for\ntasks like document classification and information extraction. These models\nmake predictions that fall into a limited number of predefined classes,\nfacilitating a binary true or false evaluation and enabling the direct\ncalculation of metrics such as the F1 score. However, recent advancements in\ngenerative large language models (GLLMs) have prompted a shift in the field due\nto their enhanced zero-shot capabilities, which eliminate the need for a\ndownstream dataset and computationally expensive fine-tuning. However,\nevaluating GLLMs presents a challenge as the binary true or false evaluation\nused for discriminative models is not applicable to the predictions made by\nGLLMs.\n  This paper introduces a new metric for generative models called ANLS* for\nevaluating a wide variety of tasks, including information extraction and\nclassification tasks. The ANLS* metric extends existing ANLS metrics as a\ndrop-in-replacement and is still compatible with previously reported ANLS\nscores. An evaluation of 7 different datasets, and more than 20 different GLLMs\ntogether with 3 different prompting methods using the ANLS* metric is also\nprovided, demonstrating the importance of the proposed metric.\n  We also benchmark a novel approach to generate prompts for documents, called\nSFT, against other prompting techniques such as LATIN. In almost all cases, SFT\noutperforms other techniques and improves the state-of-the-art, sometimes by as\nmuch as $10$ percentage points.\n  Sources are available at https:\/\/github.com\/deepopinion\/anls_star_metric\n","versions":"[{'version': 'v1', 'created': 'Tue, 6 Feb 2024 09:50:08 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Feb 2024 13:14:28 GMT'}, {'version': 'v3', 'created': 'Thu, 21 Mar 2024 05:58:10 GMT'}, {'version': 'v4', 'created': 'Tue, 16 Apr 2024 09:14:46 GMT'}, {'version': 'v5', 'created': 'Sat, 25 May 2024 06:31:45 GMT'}, {'version': 'v6', 'created': 'Fri, 28 Jun 2024 06:49:39 GMT'}, {'version': 'v7', 'created': 'Tue, 27 Aug 2024 08:33:29 GMT'}, {'version': 'v8', 'created': 'Mon, 3 Mar 2025 12:50:31 GMT'}, {'version': 'v9', 'created': 'Wed, 12 Mar 2025 08:02:54 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Peer', 'David', ''], ['Sch\u00f6pf', 'Philemon', ''], ['Nebendahl', 'Volckmar', ''], ['Rietzler', 'Alexander', ''], ['Stabinger', 'Sebastian', '']]","extracted_entities":"[{'text': 'generative large language models', 'label': 'Large Language Model'}, {'text': 'GLLMs', 'label': 'Large Language Model'}, {'text': 'computationally expensive fine-tuning', 'label': 'Fine-tuning'}, {'text': 'GLLMs', 'label': 'Large Language Model'}, {'text': 'GLLMs', 'label': 'Large Language Model'}, {'text': 'GLLMs', 'label': 'Large Language Model'}, {'text': 'SFT', 'label': 'BERT'}, {'text': 'LATIN', 'label': 'Prompting'}, {'text': 'SFT', 'label': 'BERT'}]","assigned_concept":"Large Language Model","matched_keyword":"generative large language models","similarity_score":0.8079476953}
{"id":2402.04863,"submitter":"Yingjie Mao","authors":"Xiaoqi Li, Yingjie Mao, Zexin Lu, Wenkai Li, Zongwei Li","title":"SCLA: Automated Smart Contract Summarization via LLMs and Control Flow\n  Prompt","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Smart contract code summarization is crucial for efficient maintenance and\nvulnerability mitigation. While many studies use Large Language Models (LLMs)\nfor summarization, their performance still falls short compared to fine-tuned\nmodels like CodeT5+ and CodeBERT. Some approaches combine LLMs with data flow\nanalysis but fail to fully capture the hierarchy and control structures of the\ncode, leading to information loss and degraded summarization quality. We\npropose SCLA, an LLM-based method that enhances summarization by integrating a\nControl Flow Graph (CFG) and semantic facts from the code's control flow into a\nsemantically enriched prompt. SCLA uses a control flow extraction algorithm to\nderive control flows from semantic nodes in the Abstract Syntax Tree (AST) and\nconstructs the corresponding CFG. Code semantic facts refer to both explicit\nand implicit information within the AST that is relevant to smart contracts.\nThis method enables LLMs to better capture the structural and contextual\ndependencies of the code. We validate the effectiveness of SCLA through\ncomprehensive experiments on a dataset of 40,000 real-world smart contracts.\nThe experiment shows that SCLA significantly improves summarization quality,\noutperforming the SOTA baselines with improvements of 26.7%, 23.2%, 16.7%, and\n14.7% in BLEU-4, METEOR, ROUGE-L, and BLEURT scores, respectively.\n","versions":"[{'version': 'v1', 'created': 'Wed, 7 Feb 2024 13:58:26 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Feb 2024 06:09:16 GMT'}, {'version': 'v3', 'created': 'Wed, 21 Feb 2024 14:18:32 GMT'}, {'version': 'v4', 'created': 'Sat, 17 Aug 2024 03:41:42 GMT'}, {'version': 'v5', 'created': 'Tue, 20 Aug 2024 02:34:56 GMT'}, {'version': 'v6', 'created': 'Thu, 13 Mar 2025 07:05:15 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Xiaoqi', ''], ['Mao', 'Yingjie', ''], ['Lu', 'Zexin', ''], ['Li', 'Wenkai', ''], ['Li', 'Zongwei', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'CodeT5+', 'label': 'Transformer-based model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'SCLA', 'label': 'LLM-based'}, {'text': 'semantically enriched prompt', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'SCLA', 'label': 'LLM-based'}, {'text': 'SCLA', 'label': 'LLM-based'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2402.15131,"submitter":"Guanming Xiong","authors":"Guanming Xiong, Junwei Bao, Wen Zhao","title":"Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question\n  Answering with Large Language Models","comments":"This work has been accepted by the ACL 2024 main conference. Code and\n  data are available at: https:\/\/github.com\/JimXiongGM\/Interactive-KBQA","journal-ref":null,"doi":"10.18653\/v1\/2024.acl-long.569","report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This study explores the realm of knowledge base question answering (KBQA).\nKBQA is considered a challenging task, particularly in parsing intricate\nquestions into executable logical forms. Traditional semantic parsing\n(SP)-based methods require extensive data annotations, which result in\nsignificant costs. Recently, the advent of few-shot in-context learning,\npowered by large language models (LLMs), has showcased promising capabilities.\nHowever, fully leveraging LLMs to parse questions into logical forms in\nlow-resource scenarios poses a substantial challenge. To tackle these hurdles,\nwe introduce Interactive-KBQA, a framework designed to generate logical forms\nthrough direct interaction with knowledge bases (KBs). Within this framework,\nwe have developed three generic APIs for KB interaction. For each category of\ncomplex question, we devised exemplars to guide LLMs through the reasoning\nprocesses. Our method achieves competitive results on the WebQuestionsSP,\nComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of\nexamples (shots). Importantly, our approach supports manual intervention,\nallowing for the iterative refinement of LLM outputs. By annotating a dataset\nwith step-wise reasoning processes, we showcase our model's adaptability and\nhighlight its potential for contributing significant enhancements to the field.\n","versions":"[{'version': 'v1', 'created': 'Fri, 23 Feb 2024 06:32:18 GMT'}, {'version': 'v2', 'created': 'Fri, 19 Jul 2024 06:14:20 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 06:15:34 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Xiong', 'Guanming', ''], ['Bao', 'Junwei', ''], ['Zhao', 'Wen', '']]","extracted_entities":"[{'text': 'few-shot in-context learning', 'label': 'Few-shot Learning'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2402.15183,"submitter":"Zirui Guo","authors":"Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Kangkang Lu, Zhiyong\n  Huang, Chao Huang","title":"GraphEdit: Large Language Models for Graph Structure Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies\nand interactions among nodes in graph-structured data by generating novel graph\nstructures. Graph Neural Networks (GNNs) have emerged as promising GSL\nsolutions, utilizing recursive message passing to encode node-wise\ninter-dependencies. However, many existing GSL methods heavily depend on\nexplicit graph structural information as supervision signals, leaving them\nsusceptible to challenges such as data noise and sparsity. In this work, we\npropose GraphEdit, an approach that leverages large language models (LLMs) to\nlearn complex node relationships in graph-structured data. By enhancing the\nreasoning capabilities of LLMs through instruction-tuning over graph\nstructures, we aim to overcome the limitations associated with explicit graph\nstructural information and enhance the reliability of graph structure learning.\nOur approach not only effectively denoises noisy connections but also\nidentifies node-wise dependencies from a global perspective, providing a\ncomprehensive understanding of the graph structure. We conduct extensive\nexperiments on multiple benchmark datasets to demonstrate the effectiveness and\nrobustness of GraphEdit across various settings. We have made our model\nimplementation available at: https:\/\/github.com\/HKUDS\/GraphEdit.\n","versions":"[{'version': 'v1', 'created': 'Fri, 23 Feb 2024 08:29:42 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Feb 2024 08:22:11 GMT'}, {'version': 'v3', 'created': 'Thu, 29 Feb 2024 04:15:44 GMT'}, {'version': 'v4', 'created': 'Tue, 5 Mar 2024 05:22:00 GMT'}, {'version': 'v5', 'created': 'Mon, 10 Mar 2025 14:04:39 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Guo', 'Zirui', ''], ['Xia', 'Lianghao', ''], ['Yu', 'Yanhua', ''], ['Wang', 'Yuling', ''], ['Lu', 'Kangkang', ''], ['Huang', 'Zhiyong', ''], ['Huang', 'Chao', '']]","extracted_entities":"[{'text': 'Graph Structure Learning', 'label': 'Few-shot Learning'}, {'text': 'GraphEdit', 'label': 'LLM'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'graph structure learning', 'label': 'Few-shot Learning'}, {'text': 'GraphEdit', 'label': 'LLM'}, {'text': 'GraphEdit', 'label': 'LLM'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2403.09905,"submitter":"Vishnu Sashank Dorbala","authors":"Vishnu Sashank Dorbala, Bhrij Patel, Amrit Singh Bedi, Dinesh Manocha","title":"Right Place, Right Time! Dynamizing Topological Graphs for Embodied\n  Navigation","comments":"18","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.CV","license":"http:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/","abstract":"  Embodied Navigation tasks often involve constructing topological graphs of a\nscene during exploration to facilitate high-level planning and decision-making\nfor execution in continuous environments. Prior literature makes the assumption\nof static graphs with stationary targets, which does not hold in many\nreal-world environments with moving objects. To address this, we present a\nnovel formulation generalizing navigation to dynamic environments by\nintroducing structured object transitions to dynamize static topological graphs\ncalled Object Transition Graphs (OTGs). OTGs simulate portable targets\nfollowing structured routes inspired by human habits. We apply this technique\nto Matterport3D (MP3D), a popular simulator for evaluating embodied tasks. On\nthese dynamized OTGs, we establish a navigation benchmark by evaluating\nOracle-based, Reinforcement Learning, and Large Language Model (LLM)-based\napproaches on a multi-object finding task. Further, we quantify agent\nadaptability, and make key inferences such as agents employing learned\ndecision-making strategies generalize better than those relying on privileged\noracle knowledge. To the best of our knowledge, ours is the first work to\nintroduce structured temporal dynamism on topological graphs for studying\ngeneralist embodied navigation policies. The code and dataset for our OTGs will\nbe made publicly available to foster research on embodied navigation in dynamic\nscenes.\n","versions":"[{'version': 'v1', 'created': 'Thu, 14 Mar 2024 22:33:22 GMT'}, {'version': 'v2', 'created': 'Sun, 1 Dec 2024 21:42:37 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 22:26:37 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Dorbala', 'Vishnu Sashank', ''], ['Patel', 'Bhrij', ''], ['Bedi', 'Amrit Singh', ''], ['Manocha', 'Dinesh', '']]","extracted_entities":"[{'text': 'OTGs', 'label': 'LLMs'}, {'text': 'OTGs', 'label': 'LLMs'}, {'text': 'Oracle-based', 'label': 'LLM-based'}, {'text': 'Large Language Model', 'label': 'Large Language Model'}, {'text': 'OTGs', 'label': 'LLMs'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Model","similarity_score":1.0}
{"id":2403.14362,"submitter":"Jiaqi Yue","authors":"Jiaqi Yue, Chunhui Zhao, Jiancheng Zhao, Biao Huang","title":"Enabling Generalized Zero-shot Learning Towards Unseen Domains by\n  Intrinsic Learning from Redundant LLM Semantics","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen\nclasses against domain shift problem where data of unseen classes may be\nmisclassified as seen classes. However, existing GZSL is still limited to seen\ndomains. In the current work, we study cross-domain GZSL (CDGZSL) which\naddresses GZSL towards unseen domains. Different from existing GZSL methods,\nCDGZSL constructs a common feature space across domains and acquires the\ncorresponding intrinsic semantics shared among domains to transfer from seen to\nunseen domains. Considering the information asymmetry problem caused by\nredundant class semantics annotated with large language models (LLMs), we\npresent Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR\nconsists of two parts: Inter-class similarity alignment, which eliminates the\nnon-intrinsic semantics not shared across all domains under the guidance of\ninter-class feature relationships, and unseen-class meta generation, which\npreserves intrinsic semantics to maintain connectivity between seen and unseen\nclasses by simulating feature generation. MDASR effectively aligns the\nredundant semantic space with the common feature space, mitigating the\ninformation asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on\ntwo datasets, Office-Home and Mini-DomainNet, and we have shared the LLM-based\nsemantics for these datasets as a benchmark.\n","versions":"[{'version': 'v1', 'created': 'Thu, 21 Mar 2024 12:45:01 GMT'}, {'version': 'v2', 'created': 'Thu, 23 May 2024 07:50:31 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Aug 2024 07:32:46 GMT'}, {'version': 'v4', 'created': 'Mon, 19 Aug 2024 12:28:55 GMT'}, {'version': 'v5', 'created': 'Mon, 10 Mar 2025 09:35:20 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Yue', 'Jiaqi', ''], ['Zhao', 'Chunhui', ''], ['Zhao', 'Jiancheng', ''], ['Huang', 'Biao', '']]","extracted_entities":"[{'text': 'Generalized zero-shot learning', 'label': 'Zero-shot Learning'}, {'text': 'GZSL', 'label': 'Zero-shot Learning'}, {'text': 'GZSL', 'label': 'Zero-shot Learning'}, {'text': 'GZSL', 'label': 'Zero-shot Learning'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2403.14743,"submitter":"Ahmad Mahmood","authors":"Ahmad Mahmood, Ashmal Vayani, Muzammal Naseer, Salman Khan, Fahad\n  Shahbaz Khan","title":"VURF: A General-purpose Reasoning and Self-refinement Framework for\n  Video Understanding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent studies have demonstrated the effectiveness of Large Language Models\n(LLMs) as reasoning modules that can deconstruct complex tasks into more\nmanageable sub-tasks, particularly when applied to visual reasoning tasks for\nimages. In contrast, this paper introduces a Video Understanding and Reasoning\nFramework (VURF) based on the reasoning power of LLMs. Ours is a novel approach\nto extend the utility of LLMs in the context of video tasks, leveraging their\ncapacity to generalize from minimal input and output demonstrations within a\ncontextual framework. We harness their contextual learning capabilities by\npresenting LLMs with pairs of instructions and their corresponding high-level\nprograms to generate executable visual programs for video understanding. To\nenhance the program's accuracy and robustness, we implement two important\nstrategies. \\emph{Firstly,} we employ a feedback-generation approach, powered\nby GPT-3.5, to rectify errors in programs utilizing unsupported functions.\n\\emph{Secondly}, taking motivation from recent works on self-refinement of LLM\noutputs, we introduce an iterative procedure for improving the quality of the\nin-context examples by aligning the initial outputs to the outputs that would\nhave been generated had the LLM not been bound by the structure of the\nin-context examples. Our results on several video-specific tasks, including\nvisual QA, video anticipation, pose estimation, and multi-video QA, illustrate\nthese enhancements' efficacy in improving the performance of visual programming\napproaches for video tasks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 21 Mar 2024 18:00:00 GMT'}, {'version': 'v2', 'created': 'Mon, 25 Mar 2024 01:18:37 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 03:53:38 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Mahmood', 'Ahmad', ''], ['Vayani', 'Ashmal', ''], ['Naseer', 'Muzammal', ''], ['Khan', 'Salman', ''], ['Khan', 'Fahad Shahbaz', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'contextual framework', 'label': 'contextual Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GPT-3.5', 'label': 'GPT'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2403.16812,"submitter":"Shuai Ma","authors":"Shuai Ma, Qiaoyi Chen, Xinru Wang, Chengbo Zheng, Zhenhui Peng, Ming\n  Yin, Xiaojuan Ma","title":"Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered\n  Deliberative AI for AI-Assisted Decision-Making","comments":"23 pages, ACM CHI 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In AI-assisted decision-making, humans often passively review AI's suggestion\nand decide whether to accept or reject it as a whole. In such a paradigm,\nhumans are found to rarely trigger analytical thinking and face difficulties in\ncommunicating the nuances of conflicting opinions to the AI when disagreements\noccur. To tackle this challenge, we propose Human-AI Deliberation, a novel\nframework to promote human reflection and discussion on conflicting human-AI\nopinions in decision-making. Based on theories in human deliberation, this\nframework engages humans and AI in dimension-level opinion elicitation,\ndeliberative discussion, and decision updates. To empower AI with deliberative\ncapabilities, we designed Deliberative AI, which leverages large language\nmodels (LLMs) as a bridge between humans and domain-specific models to enable\nflexible conversational interactions and faithful information provision. An\nexploratory evaluation on a graduate admissions task shows that Deliberative AI\noutperforms conventional explainable AI (XAI) assistants in improving humans'\nappropriate reliance and task performance. Based on a mixed-methods analysis of\nparticipant behavior, perception, user experience, and open-ended feedback, we\ndraw implications for future AI-assisted decision tool design.\n","versions":"[{'version': 'v1', 'created': 'Mon, 25 Mar 2024 14:34:06 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 19:23:23 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Ma', 'Shuai', ''], ['Chen', 'Qiaoyi', ''], ['Wang', 'Xinru', ''], ['Zheng', 'Chengbo', ''], ['Peng', 'Zhenhui', ''], ['Yin', 'Ming', ''], ['Ma', 'Xiaojuan', '']]","extracted_entities":"[{'text': 'large language\\nmodels', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language\nmodels","similarity_score":0.9664971828}
{"id":2405.05966,"submitter":"Juri Opitz","authors":"Juri Opitz and Shira Wein and Nathan Schneider","title":"Natural Language Processing RELIES on Linguistics","comments":"To appear in Computational Linguistics. This is a pre-MIT Press\n  publication version","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) have become capable of generating highly fluent\ntext in certain languages, without modules specially designed to capture\ngrammar or semantic coherence. What does this mean for the future of linguistic\nexpertise in NLP? We highlight several aspects in which NLP (still) relies on\nlinguistics, or where linguistic thinking can illuminate new directions. We\nargue our case around the acronym RELIES that encapsulates six major facets\nwhere linguistics contributes to NLP: Resources, Evaluation, Low-resource\nsettings, Interpretability, Explanation, and the Study of language. This list\nis not exhaustive, nor is linguistics the main point of reference for every\neffort under these themes; but at a macro level, these facets highlight the\nenduring importance of studying machine systems vis-\\`a-vis systems of human\nlanguage.\n","versions":"[{'version': 'v1', 'created': 'Thu, 9 May 2024 17:59:32 GMT'}, {'version': 'v2', 'created': 'Mon, 9 Sep 2024 08:21:13 GMT'}, {'version': 'v3', 'created': 'Fri, 22 Nov 2024 15:36:32 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Mar 2025 15:07:49 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Opitz', 'Juri', ''], ['Wein', 'Shira', ''], ['Schneider', 'Nathan', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2405.16918,"submitter":"Nils Philipp Walter","authors":"Nils Philipp Walter, Linara Adilova, Jilles Vreeken, Michael Kamp","title":"The Uncanny Valley: Exploring Adversarial Robustness from a Flatness\n  Perspective","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Flatness of the loss surface not only correlates positively with\ngeneralization, but is also related to adversarial robustness since\nperturbations of inputs relate non-linearly to perturbations of weights. In\nthis paper, we empirically analyze the relation between adversarial examples\nand relative flatness with respect to the parameters of one layer. We observe a\npeculiar property of adversarial examples in the context of relative flatness:\nduring an iterative first-order white-box attack, the flatness of the loss\nsurface measured around the adversarial example first becomes sharper until the\nlabel is flipped, but if we keep the attack running, it runs into a flat\nuncanny valley where the label remains flipped. In extensive experiments, we\nobserve this phenomenon across various model architectures and datasets, even\nfor adversarially trained models. Our results also extend to large language\nmodels (LLMs), but due to the discrete nature of the input space and\ncomparatively weak attacks, adversarial examples rarely reach truly flat\nregions. Most importantly, this phenomenon shows that flatness alone cannot\nexplain adversarial robustness unless we can also guarantee the behavior of the\nfunction around the examples. We, therefore theoretically connect relative\nflatness to adversarial robustness by bounding the third derivative of the loss\nsurface, underlining the need for flatness in combination with a low global\nLipschitz constant for a robust model.\n","versions":"[{'version': 'v1', 'created': 'Mon, 27 May 2024 08:10:46 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 14:47:37 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Walter', 'Nils Philipp', ''], ['Adilova', 'Linara', ''], ['Vreeken', 'Jilles', ''], ['Kamp', 'Michael', '']]","extracted_entities":"[{'text': 'large language\\nmodels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language\nmodels","similarity_score":0.9664971828}
{"id":2406.04443,"submitter":"Eduard Gorbunov","authors":"Savelii Chezhegov, Yaroslav Klyukin, Andrei Semenov, Aleksandr\n  Beznosikov, Alexander Gasnikov, Samuel Horv\\'ath, Martin Tak\\'a\\v{c}, Eduard\n  Gorbunov","title":"Clipping Improves Adam-Norm and AdaGrad-Norm when the Noise Is\n  Heavy-Tailed","comments":"63 pages, 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG math.OC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Methods with adaptive stepsizes, such as AdaGrad and Adam, are essential for\ntraining modern Deep Learning models, especially Large Language Models.\nTypically, the noise in the stochastic gradients is heavy-tailed for the later\nones. Gradient clipping provably helps to achieve good high-probability\nconvergence for such noises. However, despite the similarity between\nAdaGrad\/Adam and Clip-SGD, the current understanding of the high-probability\nconvergence of AdaGrad\/Adam-type methods is limited in this case. In this work,\nwe prove that AdaGrad\/Adam (and their delayed version) can have provably bad\nhigh-probability convergence if the noise is heavy-tailed. We also show that\ngradient clipping fixes this issue, i.e., we derive new high-probability\nconvergence bounds with polylogarithmic dependence on the confidence level for\nAdaGrad-Norm and Adam-Norm with clipping and with\/without delay for smooth\nconvex\/non-convex stochastic optimization with heavy-tailed noise. Our\nempirical evaluations highlight the superiority of clipped versions of\nAdaGrad\/Adam-Norm in handling the heavy-tailed noise.\n","versions":"[{'version': 'v1', 'created': 'Thu, 6 Jun 2024 18:49:10 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 10:26:57 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chezhegov', 'Savelii', ''], ['Klyukin', 'Yaroslav', ''], ['Semenov', 'Andrei', ''], ['Beznosikov', 'Aleksandr', ''], ['Gasnikov', 'Alexander', ''], ['Horv\u00e1th', 'Samuel', ''], ['Tak\u00e1\u010d', 'Martin', ''], ['Gorbunov', 'Eduard', '']]","extracted_entities":"[{'text': 'Adam', 'label': 'ALBERT'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'Adam', 'label': 'ALBERT'}, {'text': 'Adam', 'label': 'ALBERT'}, {'text': 'Adam-Norm', 'label': 'ALBERT'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2406.08426,"submitter":"Zijin Hong","authors":"Zijin Hong, Zheng Yuan, Qinggang Zhang, Hao Chen, Junnan Dong, Feiran\n  Huang, Xiao Huang","title":"Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.DB","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Generating accurate SQL from users' natural language questions (text-to-SQL)\nremains a long-standing challenge due to the complexities involved in user\nquestion understanding, database schema comprehension, and SQL generation.\nTraditional text-to-SQL systems, which combine human engineering and deep\nneural networks, have made significant progress. Subsequently, pre-trained\nlanguage models (PLMs) have been developed for text-to-SQL tasks, achieving\npromising results. However, as modern databases and user questions grow more\ncomplex, PLMs with a limited parameter size often produce incorrect SQL. This\nnecessitates more sophisticated and tailored optimization methods, which\nrestricts the application of PLM-based systems. Recently, large language models\n(LLMs) have shown significant capabilities in natural language understanding as\nmodel scale increases. Thus, integrating LLM-based solutions can bring unique\nopportunities, improvements, and solutions to text-to-SQL research. In this\nsurvey, we provide a comprehensive review of existing LLM-based text-to-SQL\nstudies. Specifically, we offer a brief overview of the technical challenges\nand evolutionary process of text-to-SQL. Next, we introduce the datasets and\nmetrics designed to evaluate text-to-SQL systems. Subsequently, we present a\nsystematic analysis of recent advances in LLM-based text-to-SQL. Finally, we\nmake a summarization and discuss the remaining challenges in this field and\nsuggest expectations for future research directions.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Jun 2024 17:13:17 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Jun 2024 13:51:30 GMT'}, {'version': 'v3', 'created': 'Tue, 16 Jul 2024 08:06:57 GMT'}, {'version': 'v4', 'created': 'Sun, 23 Feb 2025 22:22:20 GMT'}, {'version': 'v5', 'created': 'Thu, 13 Mar 2025 08:45:35 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Hong', 'Zijin', ''], ['Yuan', 'Zheng', ''], ['Zhang', 'Qinggang', ''], ['Chen', 'Hao', ''], ['Dong', 'Junnan', ''], ['Huang', 'Feiran', ''], ['Huang', 'Xiao', '']]","extracted_entities":"[{'text': 'PLMs', 'label': 'Large Language Model'}, {'text': 'PLMs', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2406.1681,"submitter":"William F. Shen","authors":"Xinchi Qiu, William F. Shen, Yihong Chen, Meghdad Kurmanji, Nicola\n  Cancedda, Pontus Stenetorp, Nicholas D. Lane","title":"How Data Inter-connectivity Shapes LLMs Unlearning: A Structural\n  Unlearning Perspective","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While unlearning knowledge from large language models (LLMs) is receiving\nincreasing attention, one important aspect remains unexplored. Existing\napproaches and benchmarks assume data points to-be-forgotten are independent,\nignoring their inter-connectivity - a fundamental characteristic of real-world\ndata structures. In this paper, we propose PISTOL, a method for compiling\nstructural datasets. PISTOL leverages the inherently structured nature of\ncontractual relationships, offering several key benefits. First, it enables\ninsights into the impact of structural data on unlearning effectiveness.\nSecond, it provides precise and concise ground truths for clearer evaluation.\nThird, its attribute generation does not require input from pre-trained LLMs,\nmitigating confounding risks. Leveraging datasets synthesized using PISTOL, we\ndemonstrate how data inter-connectivity impacts LLM unlearning. Specifically,\n(a) in both the pre-trained and fine-tuned models, unlearning difficulty\nincreases as data inter-connectivity grows, (b) there is a positive correlation\nbetween the density of the knowledge graph and unlearning difficulty, and (c)\nwhen the to-be-forgotten data is skewed towards one domain, balancing retaining\nperformance across all domains is challenging.\n","versions":"[{'version': 'v1', 'created': 'Mon, 24 Jun 2024 17:22:36 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 21:33:53 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Qiu', 'Xinchi', ''], ['Shen', 'William F.', ''], ['Chen', 'Yihong', ''], ['Kurmanji', 'Meghdad', ''], ['Cancedda', 'Nicola', ''], ['Stenetorp', 'Pontus', ''], ['Lane', 'Nicholas D.', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'PISTOL', 'label': 'Generative Pre-trained Transformer (GPT)'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2406.17055,"submitter":"Ryan Liu","authors":"Ryan Liu, Jiayi Geng, Joshua C. Peterson, Ilia Sucholutsky, Thomas L.\n  Griffiths","title":"Large Language Models Assume People are More Rational than We Really are","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CY cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  In order for AI systems to communicate effectively with people, they must\nunderstand how we make decisions. However, people's decisions are not always\nrational, so the implicit internal models of human decision-making in Large\nLanguage Models (LLMs) must account for this. Previous empirical evidence seems\nto suggest that these implicit models are accurate -- LLMs offer believable\nproxies of human behavior, acting how we expect humans would in everyday\ninteractions. However, by comparing LLM behavior and predictions to a large\ndataset of human decisions, we find that this is actually not the case: when\nboth simulating and predicting people's choices, a suite of cutting-edge LLMs\n(GPT-4o & 4-Turbo, Llama-3-8B & 70B, Claude 3 Opus) assume that people are more\nrational than we really are. Specifically, these models deviate from human\nbehavior and align more closely with a classic model of rational choice --\nexpected value theory. Interestingly, people also tend to assume that other\npeople are rational when interpreting their behavior. As a consequence, when we\ncompare the inferences that LLMs and people draw from the decisions of others\nusing another psychological dataset, we find that these inferences are highly\ncorrelated. Thus, the implicit decision-making models of LLMs appear to be\naligned with the human expectation that other people will act rationally,\nrather than with how people actually act.\n","versions":"[{'version': 'v1', 'created': 'Mon, 24 Jun 2024 18:15:27 GMT'}, {'version': 'v2', 'created': 'Mon, 1 Jul 2024 17:29:54 GMT'}, {'version': 'v3', 'created': 'Tue, 30 Jul 2024 14:22:26 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Mar 2025 17:42:37 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Liu', 'Ryan', ''], ['Geng', 'Jiayi', ''], ['Peterson', 'Joshua C.', ''], ['Sucholutsky', 'Ilia', ''], ['Griffiths', 'Thomas L.', '']]","extracted_entities":"[{'text': 'Large\\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large\nLanguage Models","similarity_score":0.9664971828}
{"id":2406.18113,"submitter":"Boris Meinardus","authors":"Boris Meinardus, Hector Rodriguez, Anil Batra, Anna Rohrbach, Marcus\n  Rohrbach","title":"Chrono: A Simple Blueprint for Representing Time in MLLMs","comments":"Code: https:\/\/github.com\/sudo-Boris\/mr-Blip","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The recent success of Large Language Models (LLMs) has prompted the extension\nto the multimodal domain developing image-text Multimodal LLMs (MLLMs) and then\nvideo-text models. In this work, we investigate the challenge of contextual and\ntemporal comprehension in video-language models by exploring the task of\ntemporal localization in videos. To address this problem, prior works have\ndeveloped complex task-specific architectures, novel modules to embed time into\nMLLMs, or leveraged additional input signals such as video transcripts to best\nencode contextual and temporal information. Interestingly, we find that most of\nthese efforts are surpassed by a much simpler design. We introduce Chrono, a\nuniversal sequence blueprint that can be applied to an image-text pretrained\nMLLM. Through extensive ablations across different MLLM architectures,\nfinetuning and zero-shot settings, and different datasets, we achieve a new\nSOTA in moment retrieval on the most widely used benchmarks Charades-STA,\nQVHighlights, ActivityNet Captions, and grounded video question answering on\nNeXT-GQA.\n","versions":"[{'version': 'v1', 'created': 'Wed, 26 Jun 2024 06:59:09 GMT'}, {'version': 'v2', 'created': 'Wed, 24 Jul 2024 06:43:07 GMT'}, {'version': 'v3', 'created': 'Mon, 14 Oct 2024 06:50:19 GMT'}, {'version': 'v4', 'created': 'Fri, 21 Feb 2025 00:49:07 GMT'}, {'version': 'v5', 'created': 'Tue, 11 Mar 2025 10:03:46 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Meinardus', 'Boris', ''], ['Rodriguez', 'Hector', ''], ['Batra', 'Anil', ''], ['Rohrbach', 'Anna', ''], ['Rohrbach', 'Marcus', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'finetuning', 'label': 'Fine-tuning'}, {'text': 'zero-shot settings', 'label': 'Zero-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2407.00936,"submitter":"Zirui Chen","authors":"Xin Wang, Zirui Chen, Haofen Wang, Leong Hou U, Zhao Li, Wenbin Guo","title":"Large Language Model Enhanced Knowledge Representation Learning: A\n  Survey","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Knowledge Representation Learning (KRL) is crucial for enabling applications\nof symbolic knowledge from Knowledge Graphs (KGs) to downstream tasks by\nprojecting knowledge facts into vector spaces. Despite their effectiveness in\nmodeling KG structural information, KRL methods are suffering from the\nsparseness of KGs. The rise of Large Language Models (LLMs) built on the\nTransformer architecture presents promising opportunities for enhancing KRL by\nincorporating textual information to address information sparsity in KGs.\nLLM-enhanced KRL methods, including three key approaches, encoder-based methods\nthat leverage detailed contextual information, encoder-decoder-based methods\nthat utilize a unified Seq2Seq model for comprehensive encoding and decoding,\nand decoder-based methods that utilize extensive knowledge from large corpora,\nhave significantly advanced the effectiveness and generalization of KRL in\naddressing a wide range of downstream tasks. This work provides a broad\noverview of downstream tasks while simultaneously identifying emerging research\ndirections in these evolving domains.\n","versions":"[{'version': 'v1', 'created': 'Mon, 1 Jul 2024 03:37:35 GMT'}, {'version': 'v2', 'created': 'Thu, 18 Jul 2024 02:19:34 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Feb 2025 02:38:25 GMT'}, {'version': 'v4', 'created': 'Wed, 12 Mar 2025 05:48:32 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Wang', 'Xin', ''], ['Chen', 'Zirui', ''], ['Wang', 'Haofen', ''], ['U', 'Leong Hou', ''], ['Li', 'Zhao', ''], ['Guo', 'Wenbin', '']]","extracted_entities":"[{'text': 'Knowledge Representation Learning', 'label': 'Few-shot Learning'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'KRL', 'label': 'Few-shot Learning'}, {'text': 'decoder-based methods', 'label': 'LLM-powered'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2407.08952,"submitter":"Ye Liu","authors":"Ye Liu, Jiajun Zhu, Xukai Liu, Haoyu Tang, Yanghai Zhang, Kai Zhang,\n  Xiaofang Zhou, Enhong Chen","title":"Detect, Investigate, Judge and Determine: A Knowledge-guided Framework\n  for Few-shot Fake News Detection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Knowledge-guided Fake News\nDetection (DKFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DKFND first identifies the knowledge concepts of\neach news article through a Detection Module. Subsequently, DKFND creatively\ndesigns an Investigation Module to retrieve inside and outside valuable\ninformation concerning to the current news, followed by another Judge Module to\nevaluate the relevance and confidence of them. Finally, a Determination Module\nfurther derives two respective predictions and obtain the final result.\nExtensive experiments on two public datasets show the efficacy of our proposed\nmethod, particularly in low-resource settings.\n","versions":"[{'version': 'v1', 'created': 'Fri, 12 Jul 2024 03:15:01 GMT'}, {'version': 'v2', 'created': 'Fri, 14 Feb 2025 04:56:16 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Feb 2025 05:25:32 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Mar 2025 13:06:04 GMT'}, {'version': 'v5', 'created': 'Wed, 12 Mar 2025 04:46:47 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Liu', 'Ye', ''], ['Zhu', 'Jiajun', ''], ['Liu', 'Xukai', ''], ['Tang', 'Haoyu', ''], ['Zhang', 'Yanghai', ''], ['Zhang', 'Kai', ''], ['Zhou', 'Xiaofang', ''], ['Chen', 'Enhong', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Detection Module', 'label': 'Embedding'}, {'text': 'Investigation Module', 'label': 'Embedding'}, {'text': 'Determination Module', 'label': 'Embedding'}, {'text': 'public datasets', 'label': 'Open-source LLMs'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2407.12358,"submitter":"Chuwei Luo","authors":"Yufan Shen, Chuwei Luo, Zhaoqing Zhu, Yang Chen, Qi Zheng, Zhi Yu,\n  Jiajun Bu, Cong Yao","title":"ProcTag: Process Tagging for Assessing the Efficacy of Document\n  Instruction Data","comments":"AAAI 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recently, large language models (LLMs) and multimodal large language models\n(MLLMs) have demonstrated promising results on document visual question\nanswering (VQA) task, particularly after training on document instruction\ndatasets. An effective evaluation method for document instruction data is\ncrucial in constructing instruction data with high efficacy, which, in turn,\nfacilitates the training of LLMs and MLLMs for document VQA. However, most\nexisting evaluation methods for instruction data are limited to the textual\ncontent of the instructions themselves, thereby hindering the effective\nassessment of document instruction datasets and constraining their\nconstruction. In this paper, we propose ProcTag, a data-oriented method that\nassesses the efficacy of document instruction data. ProcTag innovatively\nperforms tagging on the execution process of instructions rather than the\ninstruction text itself. By leveraging the diversity and complexity of these\ntags to assess the efficacy of the given dataset, ProcTag enables selective\nsampling or filtering of document instructions. Furthermore, DocLayPrompt, a\nnovel semi-structured layout-aware document prompting strategy, is proposed for\neffectively representing documents. Experiments demonstrate that sampling\nexisting open-sourced and generated document VQA\/instruction datasets with\nProcTag significantly outperforms current methods for evaluating instruction\ndata. Impressively, with ProcTag-based sampling in the generated document\ndatasets, only 30.5\\% of the document instructions are required to achieve\n100\\% efficacy compared to the complete dataset. The code is publicly available\nat\nhttps:\/\/github.com\/AlibabaResearch\/AdvancedLiterateMachinery\/tree\/main\/DocumentUnderstanding\/ProcTag.\n","versions":"[{'version': 'v1', 'created': 'Wed, 17 Jul 2024 07:29:59 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 02:20:28 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Shen', 'Yufan', ''], ['Luo', 'Chuwei', ''], ['Zhu', 'Zhaoqing', ''], ['Chen', 'Yang', ''], ['Zheng', 'Qi', ''], ['Yu', 'Zhi', ''], ['Bu', 'Jiajun', ''], ['Yao', 'Cong', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'ProcTag', 'label': 'Prompting'}, {'text': 'ProcTag', 'label': 'Prompting'}, {'text': 'ProcTag', 'label': 'Prompting'}, {'text': 'DocLayPrompt', 'label': 'Prompting'}, {'text': 'ProcTag', 'label': 'Prompting'}, {'text': 'ProcTag', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2407.17417,"submitter":"Michael-Andrei Panaitescu-Liess","authors":"Michael-Andrei Panaitescu-Liess, Zora Che, Bang An, Yuancheng Xu,\n  Pankayaraj Pathmanathan, Souradip Chakraborty, Sicheng Zhu, Tom Goldstein,\n  Furong Huang","title":"Can Watermarking Large Language Models Prevent Copyrighted Text\n  Generation and Hide Training Data?","comments":"19 pages, 7 figures. Published at AAAI 2025. Code will be available\n  at https:\/\/github.com\/michael-panaitescu\/watermark_copyright_aaai25","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) have demonstrated impressive capabilities in\ngenerating diverse and contextually rich text. However, concerns regarding\ncopyright infringement arise as LLMs may inadvertently produce copyrighted\nmaterial. In this paper, we first investigate the effectiveness of watermarking\nLLMs as a deterrent against the generation of copyrighted texts. Through\ntheoretical analysis and empirical evaluation, we demonstrate that\nincorporating watermarks into LLMs significantly reduces the likelihood of\ngenerating copyrighted content, thereby addressing a critical concern in the\ndeployment of LLMs. However, we also find that watermarking can have unintended\nconsequences on Membership Inference Attacks (MIAs), which aim to discern\nwhether a sample was part of the pretraining dataset and may be used to detect\ncopyright violations. Surprisingly, we find that watermarking adversely affects\nthe success rate of MIAs, complicating the task of detecting copyrighted text\nin the pretraining dataset. These results reveal the complex interplay between\ndifferent regulatory measures, which may impact each other in unforeseen ways.\nFinally, we propose an adaptive technique to improve the success rate of a\nrecent MIA under watermarking. Our findings underscore the importance of\ndeveloping adaptive methods to study critical problems in LLMs with potential\nlegal implications.\n","versions":"[{'version': 'v1', 'created': 'Wed, 24 Jul 2024 16:53:09 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 06:18:24 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Panaitescu-Liess', 'Michael-Andrei', ''], ['Che', 'Zora', ''], ['An', 'Bang', ''], ['Xu', 'Yuancheng', ''], ['Pathmanathan', 'Pankayaraj', ''], ['Chakraborty', 'Souradip', ''], ['Zhu', 'Sicheng', ''], ['Goldstein', 'Tom', ''], ['Huang', 'Furong', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2407.21227,"submitter":"Florian Tambon","authors":"Florian Tambon, Amin Nikanjam, Cyrine Zid, Foutse Khomh, Giuliano\n  Antoniol","title":"TaskEval: Assessing Difficulty of Code Generation Tasks for Large\n  Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI","license":"http:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/","abstract":"  Large Language Models (LLMs) excel in code-related tasks like code\ngeneration, but benchmark evaluations often overlook task characteristics, such\nas difficulty. Moreover, benchmarks are usually built using tasks described\nwith one single prompt, despite the formulation of prompts having a profound\nimpact on the outcome. This paper introduces a generalist approach, TaskEval, a\nframework using diverse prompts and Item Response Theory (IRT) to efficiently\nassess LLMs' capabilities and benchmark task characteristics, improving the\nunderstanding of their performance.\n  Using two code generation benchmarks, HumanEval+ and ClassEval, as well as 5\ncode generation LLMs, we show that TaskEval is capable of characterizing the\nproperties of tasks. Using topic analysis, we identify and analyze the tasks of\nrespectively 17 and 21 topics within the benchmarks. We also cross-analyze\ntasks' characteristics with programming constructs (e.g., variable assignment,\nconditions, etc.) used by LLMs, emphasizing some patterns with tasks'\ndifficulty. Finally, we conduct a comparison between the difficulty assessment\nof tasks by human-annotators and LLMs. Orthogonal to current benchmarking\nevaluation efforts, TaskEval can assist researchers and practitioners in\nfostering better assessments of LLMs. The tasks' characteristics can be used to\nidentify shortcomings within existing benchmarks. This could be used to\ngenerate additional related tasks for the evaluation or improvement of LLM.\n","versions":"[{'version': 'v1', 'created': 'Tue, 30 Jul 2024 22:31:19 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 17:41:17 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Tambon', 'Florian', ''], ['Nikanjam', 'Amin', ''], ['Zid', 'Cyrine', ''], ['Khomh', 'Foutse', ''], ['Antoniol', 'Giuliano', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2408.08158,"submitter":"Mar Gonzalez-Franco","authors":"Riccardo Bovo, Steven Abreu, Karan Ahuja, Eric J Gonzalez, Li-Te\n  Cheng, Mar Gonzalez-Franco","title":"EmBARDiment: an Embodied AI Agent for Productivity in XR","comments":null,"journal-ref":"IEEE Virtual Reality Conference 2025","doi":null,"report-no":null,"categories":"cs.HC cs.MA","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  XR devices running chat-bots powered by Large Language Models (LLMs) have the\nto become always-on agents that enable much better productivity scenarios.\nCurrent screen based chat-bots do not take advantage of the the full-suite of\nnatural inputs available in XR, including inward facing sensor data, instead\nthey over-rely on explicit voice or text prompts, sometimes paired with\nmulti-modal data dropped as part of the query. We propose a solution that\nleverages an attention framework that derives context implicitly from user\nactions, eye-gaze, and contextual memory within the XR environment. Our work\nminimizes the need for engineered explicit prompts, fostering grounded and\nintuitive interactions that glean user insights for the chat-bot.\n","versions":"[{'version': 'v1', 'created': 'Thu, 15 Aug 2024 13:48:44 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 21:54:08 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Bovo', 'Riccardo', ''], ['Abreu', 'Steven', ''], ['Ahuja', 'Karan', ''], ['Gonzalez', 'Eric J', ''], ['Cheng', 'Li-Te', ''], ['Gonzalez-Franco', 'Mar', '']]","extracted_entities":"[{'text': 'chat-bots', 'label': 'ChatGPT'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'chat-bots', 'label': 'ChatGPT'}, {'text': 'explicit voice or text prompts', 'label': 'Prompting'}, {'text': 'attention framework', 'label': 'Attention mechanism'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2408.10883,"submitter":"Xinqi Su","authors":"Xinqi Su, Zitong Yu, Yawen Cui, Ajian Liu, Xun Lin, Yuhao Wang,\n  Haochen Liang, Wenhui Li, Li Shen, Xiaochun Cao","title":"Dynamic Analysis and Adaptive Discriminator for Fake News Detection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In current web environment, fake news spreads rapidly across online social\nnetworks, posing serious threats to society. Existing multimodal fake news\ndetection methods can generally be classified into knowledge-based and\nsemantic-based approaches. However, these methods are heavily rely on human\nexpertise and feedback, lacking flexibility. To address this challenge, we\npropose a Dynamic Analysis and Adaptive Discriminator (DAAD) approach for fake\nnews detection. For knowledge-based methods, we introduce the Monte Carlo Tree\nSearch algorithm to leverage the self-reflective capabilities of large language\nmodels (LLMs) for prompt optimization, providing richer, domain-specific\ndetails and guidance to the LLMs, while enabling more flexible integration of\nLLM comment on news content. For semantic-based methods, we define four typical\ndeceit patterns: emotional exaggeration, logical inconsistency, image\nmanipulation, and semantic inconsistency, to reveal the mechanisms behind fake\nnews creation. To detect these patterns, we carefully design four\ndiscriminators and expand them in depth and breadth, using the soft-routing\nmechanism to explore optimal detection models. Experimental results on three\nreal-world datasets demonstrate the superiority of our approach. The code will\nbe available at: https:\/\/github.com\/SuXinqi\/DAAD.\n","versions":"[{'version': 'v1', 'created': 'Tue, 20 Aug 2024 14:13:54 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 03:05:45 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Su', 'Xinqi', ''], ['Yu', 'Zitong', ''], ['Cui', 'Yawen', ''], ['Liu', 'Ajian', ''], ['Lin', 'Xun', ''], ['Wang', 'Yuhao', ''], ['Liang', 'Haochen', ''], ['Li', 'Wenhui', ''], ['Shen', 'Li', ''], ['Cao', 'Xiaochun', '']]","extracted_entities":"[{'text': 'large language\\nmodels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompt optimization', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"large language\nmodels","similarity_score":0.9664971828}
{"id":2409.11863,"submitter":"Kejia Chen","authors":"Kejia Chen, Zheng Shen, Yue Zhang, Lingyun Chen, Fan Wu, Zhenshan\n  Bing, Sami Haddadin, Alois Knoll","title":"LEMMo-Plan: LLM-Enhanced Learning from Multi-Modal Demonstration for\n  Planning Sequential Contact-Rich Manipulation Tasks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Large Language Models (LLMs) have gained popularity in task planning for\nlong-horizon manipulation tasks. To enhance the validity of LLM-generated\nplans, visual demonstrations and online videos have been widely employed to\nguide the planning process. However, for manipulation tasks involving subtle\nmovements but rich contact interactions, visual perception alone may be\ninsufficient for the LLM to fully interpret the demonstration. Additionally,\nvisual data provides limited information on force-related parameters and\nconditions, which are crucial for effective execution on real robots. In this\npaper, we introduce an in-context learning framework that incorporates tactile\nand force-torque information from human demonstrations to enhance LLMs' ability\nto generate plans for new task scenarios. We propose a bootstrapped reasoning\npipeline that sequentially integrates each modality into a comprehensive task\nplan. This task plan is then used as a reference for planning in new task\nconfigurations. Real-world experiments on two different sequential manipulation\ntasks demonstrate the effectiveness of our framework in improving LLMs'\nunderstanding of multi-modal demonstrations and enhancing the overall planning\nperformance.\n","versions":"[{'version': 'v1', 'created': 'Wed, 18 Sep 2024 10:36:47 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 18:24:51 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Chen', 'Kejia', ''], ['Shen', 'Zheng', ''], ['Zhang', 'Yue', ''], ['Chen', 'Lingyun', ''], ['Wu', 'Fan', ''], ['Bing', 'Zhenshan', ''], ['Haddadin', 'Sami', ''], ['Knoll', 'Alois', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2409.14572,"submitter":"Hongchen Wang","authors":"Hongchen Wang, Kangming Li, Scott Ramsay, Yao Fehlis, Edward Kim, and\n  Jason Hattrick-Simpers","title":"Evaluating the Performance and Robustness of LLMs in Materials Science\n  Q&A and Property Predictions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cond-mat.mtrl-sci cs.AI cs.LG","license":"http:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/","abstract":"  Large Language Models (LLMs) have the potential to revolutionize scientific\nresearch, yet their robustness and reliability in domain-specific applications\nremain insufficiently explored. In this study, we evaluate the performance and\nrobustness of LLMs for materials science, focusing on domain-specific question\nanswering and materials property prediction across diverse real-world and\nadversarial conditions. Three distinct datasets are used in this study: 1) a\nset of multiple-choice questions from undergraduate-level materials science\ncourses, 2) a dataset including various steel compositions and yield strengths,\nand 3) a band gap dataset, containing textual descriptions of material crystal\nstructures and band gap values. The performance of LLMs is assessed using\nvarious prompting strategies, including zero-shot chain-of-thought, expert\nprompting, and few-shot in-context learning. The robustness of these models is\ntested against various forms of 'noise', ranging from realistic disturbances to\nintentionally adversarial manipulations, to evaluate their resilience and\nreliability under real-world conditions. Additionally, the study showcases\nunique phenomena of LLMs during predictive tasks, such as mode collapse\nbehavior when the proximity of prompt examples is altered and performance\nrecovery from train\/test mismatch. The findings aim to provide informed\nskepticism for the broad use of LLMs in materials science and to inspire\nadvancements that enhance their robustness and reliability for practical\napplications.\n","versions":"[{'version': 'v1', 'created': 'Sun, 22 Sep 2024 19:31:16 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 22:03:26 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Wang', 'Hongchen', ''], ['Li', 'Kangming', ''], ['Ramsay', 'Scott', ''], ['Fehlis', 'Yao', ''], ['Kim', 'Edward', ''], ['Hattrick-Simpers', 'Jason', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'zero-shot chain-of-thought', 'label': 'Chain of thought'}, {'text': 'expert\\nprompting', 'label': 'Prompting'}, {'text': 'few-shot in-context learning', 'label': 'Few-shot Learning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2409.17954,"submitter":"Jian Gao","authors":"Jian Gao, Xiao Zhang, Ji Wu, Miao Li","title":"Enhancing elusive clues in knowledge learning by contrasting attention\n  of language models","comments":"Oral presentation in AAAI 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Causal language models acquire vast amount of knowledge from general text\ncorpus during pretraining, but the efficiency of knowledge learning is known to\nbe unsatisfactory, especially when learning from knowledge-dense and\nsmall-sized corpora. The deficiency can come from long-distance dependencies\nwhich are hard to capture by language models, and overfitting to co-occurrence\npatterns and distracting clues in the training text. To address these issues,\nthe paper proposes a method to enhance knowledge learning during language model\npretraining, by enhancing elusive but important clues in text discovered by the\nlanguage model themselves. We found that larger language models pay more\nattention to non-obvious but important clues, which are often overlooked by\nsmaller language models. Therefore, we can identify these clues by contrasting\nthe attention weights of large and small language models. We use the identified\nclues as a guide to perform token-dropout data augmentation on the training\ntext, and observed a significant boost in both small and large models'\nperformance in fact memorization. This shows that the behavior contrast between\nmore and less-performant language models contains important clues for knowledge\nlearning, and it can be ``amplified\" for a straight-forward improvement in\nknowledge learning efficiency.\n","versions":"[{'version': 'v1', 'created': 'Thu, 26 Sep 2024 15:30:54 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 09:42:19 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Gao', 'Jian', ''], ['Zhang', 'Xiao', ''], ['Wu', 'Ji', ''], ['Li', 'Miao', '']]","extracted_entities":"[{'text': 'knowledge learning', 'label': 'Few-shot Learning'}, {'text': 'knowledge learning', 'label': 'Few-shot Learning'}, {'text': 'language models', 'label': 'Large Language Model'}, {'text': 'language models', 'label': 'Large Language Model'}, {'text': 'knowledge\\nlearning', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"language models","similarity_score":0.8378260732}
{"id":2409.18042,"submitter":"Kai Chen","authors":"Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu,\n  Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, Dingdong Wang, Kun Xiang, Haoyuan\n  Li, Haoli Bai, Jianhua Han, Xiaohui Li, Weike Jin, Nian Xie, Yu Zhang, James\n  T. Kwok, Hengshuang Zhao, Xiaodan Liang, Dit-Yan Yeung, Xiao Chen, Zhenguo\n  Li, Wei Zhang, Qun Liu, Jun Yao, Lanqing Hong, Lu Hou, Hang Xu","title":"EMOVA: Empowering Language Models to See, Hear and Speak with Vivid\n  Emotions","comments":"Accepted by CVPR 2025. Project Page: https:\/\/emova-ollm.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  GPT-4o, an omni-modal model that enables vocal conversations with diverse\nemotions and tones, marks a milestone for omni-modal foundation models.\nHowever, empowering Large Language Models to perceive and generate images,\ntexts, and speeches end-to-end with publicly available data remains challenging\nfor the open-source community. Existing vision-language models rely on external\ntools for speech processing, while speech-language models still suffer from\nlimited or totally without vision-understanding capabilities. To address this\ngap, we propose the EMOVA (EMotionally Omni-present Voice Assistant), to enable\nLarge Language Models with end-to-end speech abilities while maintaining the\nleading vision-language performance. With a semantic-acoustic disentangled\nspeech tokenizer, we surprisingly notice that omni-modal alignment can further\nenhance vision-language and speech abilities compared with the bi-modal aligned\ncounterparts. Moreover, a lightweight style module is introduced for the\nflexible speech style controls including emotions and pitches. For the first\ntime, EMOVA achieves state-of-the-art performance on both the vision-language\nand speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue\nwith vivid emotions.\n","versions":"[{'version': 'v1', 'created': 'Thu, 26 Sep 2024 16:44:02 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Oct 2024 06:25:52 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 14:51:04 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chen', 'Kai', ''], ['Gou', 'Yunhao', ''], ['Huang', 'Runhui', ''], ['Liu', 'Zhili', ''], ['Tan', 'Daxin', ''], ['Xu', 'Jing', ''], ['Wang', 'Chunwei', ''], ['Zhu', 'Yi', ''], ['Zeng', 'Yihan', ''], ['Yang', 'Kuo', ''], ['Wang', 'Dingdong', ''], ['Xiang', 'Kun', ''], ['Li', 'Haoyuan', ''], ['Bai', 'Haoli', ''], ['Han', 'Jianhua', ''], ['Li', 'Xiaohui', ''], ['Jin', 'Weike', ''], ['Xie', 'Nian', ''], ['Zhang', 'Yu', ''], ['Kwok', 'James T.', ''], ['Zhao', 'Hengshuang', ''], ['Liang', 'Xiaodan', ''], ['Yeung', 'Dit-Yan', ''], ['Chen', 'Xiao', ''], ['Li', 'Zhenguo', ''], ['Zhang', 'Wei', ''], ['Liu', 'Qun', ''], ['Yao', 'Jun', ''], ['Hong', 'Lanqing', ''], ['Hou', 'Lu', ''], ['Xu', 'Hang', '']]","extracted_entities":"[{'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'open-source community', 'label': 'Open-source LLMs'}, {'text': 'vision-language models', 'label': 'Large Language Model'}, {'text': 'speech-language models', 'label': 'Large Language Model'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2409.20548,"submitter":"Anxing Xiao","authors":"Anxing Xiao, Nuwan Janaka, Tianrun Hu, Anshul Gupta, Kaixin Li, Cunjun\n  Yu, David Hsu","title":"Robi Butler: Multimodal Remote Interaction with a Household Robot\n  Assistant","comments":"Accepted to ICRA 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI cs.HC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Imagine a future when we can Zoom-call a robot to manage household chores\nremotely. This work takes one step in this direction. Robi Butler is a new\nhousehold robot assistant that enables seamless multimodal remote interaction.\nIt allows the human user to monitor its environment from a first-person view,\nissue voice or text commands, and specify target objects through hand-pointing\ngestures. At its core, a high-level behavior module, powered by Large Language\nModels (LLMs), interprets multimodal instructions to generate multistep action\nplans. Each plan consists of open-vocabulary primitives supported by\nvision-language models, enabling the robot to process both textual and gestural\ninputs. Zoom provides a convenient interface to implement remote interactions\nbetween the human and the robot. The integration of these components allows\nRobi Butler to ground remote multimodal instructions in real-world home\nenvironments in a zero-shot manner. We evaluated the system on various\nhousehold tasks, demonstrating its ability to execute complex user commands\nwith multimodal inputs. We also conducted a user study to examine how\nmultimodal interaction influences user experiences in remote human-robot\ninteraction. These results suggest that with the advances in robot foundation\nmodels, we are moving closer to the reality of remote household robot\nassistants.\n","versions":"[{'version': 'v1', 'created': 'Mon, 30 Sep 2024 17:49:09 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 06:00:08 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Xiao', 'Anxing', ''], ['Janaka', 'Nuwan', ''], ['Hu', 'Tianrun', ''], ['Gupta', 'Anshul', ''], ['Li', 'Kaixin', ''], ['Yu', 'Cunjun', ''], ['Hsu', 'David', '']]","extracted_entities":"[{'text': 'Large Language\\nModels', 'label': 'Large Language Model'}, {'text': 'robot foundation\\nmodels', 'label': 'Foundation Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language\nModels","similarity_score":0.9664971828}
{"id":2410.00263,"submitter":"Kun Yuan","authors":"Kun Yuan, Vinkle Srivastav, Nassir Navab, Nicolas Padoy","title":"Procedure-Aware Surgical Video-language Pretraining with Hierarchical\n  Knowledge Augmentation","comments":"Accepted at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024 Spolight)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Surgical video-language pretraining (VLP) faces unique challenges due to the\nknowledge domain gap and the scarcity of multi-modal data. This study aims to\nbridge the gap by addressing issues regarding textual information loss in\nsurgical lecture videos and the spatial-temporal challenges of surgical VLP. We\npropose a hierarchical knowledge augmentation approach and a novel\nProcedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining\n(PeskaVLP) framework to tackle these issues. The knowledge augmentation uses\nlarge language models (LLM) for refining and enriching surgical concepts, thus\nproviding comprehensive language supervision and reducing the risk of\noverfitting. PeskaVLP combines language supervision with visual\nself-supervision, constructing hard negative samples and employing a Dynamic\nTime Warping (DTW) based loss function to effectively comprehend the\ncross-modal procedural alignment. Extensive experiments on multiple public\nsurgical scene understanding and cross-modal retrieval datasets show that our\nproposed method significantly improves zero-shot transferring performance and\noffers a generalist visual representation for further advancements in surgical\nscene understanding.The code is available at\nhttps:\/\/github.com\/CAMMA-public\/SurgVLP\n","versions":"[{'version': 'v1', 'created': 'Mon, 30 Sep 2024 22:21:05 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 15:21:36 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Yuan', 'Kun', ''], ['Srivastav', 'Vinkle', ''], ['Navab', 'Nassir', ''], ['Padoy', 'Nicolas', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2410.01727,"submitter":"Yilmazcan Ozyurt","authors":"Yilmazcan Ozyurt, Stefan Feuerriegel, Mrinmaya Sachan","title":"Automated Knowledge Concept Annotation and Question Representation\n  Learning for Knowledge Tracing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Knowledge tracing (KT) is a popular approach for modeling students' learning\nprogress over time, which can enable more personalized and adaptive learning.\nHowever, existing KT approaches face two major limitations: (1) they rely\nheavily on expert-defined knowledge concepts (KCs) in questions, which is\ntime-consuming and prone to errors; and (2) KT methods tend to overlook the\nsemantics of both questions and the given KCs. In this work, we address these\nchallenges and present KCQRL, a framework for automated knowledge concept\nannotation and question representation learning that can improve the\neffectiveness of any existing KT model. First, we propose an automated KC\nannotation process using large language models (LLMs), which generates question\nsolutions and then annotates KCs in each solution step of the questions.\nSecond, we introduce a contrastive learning approach to generate semantically\nrich embeddings for questions and solution steps, aligning them with their\nassociated KCs via a tailored false negative elimination approach. These\nembeddings can be readily integrated into existing KT models, replacing their\nrandomly initialized embeddings. We demonstrate the effectiveness of KCQRL\nacross 15 KT algorithms on two large real-world Math learning datasets, where\nwe achieve consistent performance improvements.\n","versions":"[{'version': 'v1', 'created': 'Wed, 2 Oct 2024 16:37:19 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 13:09:14 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Ozyurt', 'Yilmazcan', ''], ['Feuerriegel', 'Stefan', ''], ['Sachan', 'Mrinmaya', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2410.01824,"submitter":"Alexander Wuttke","authors":"Alexander Wuttke, Matthias A{\\ss}enmacher, Christopher Klamm, Max M.\n  Lang, Quirin W\\\"urschinger, Frauke Kreuter","title":"AI Conversational Interviewing: Transforming Surveys with LLMs as\n  Adaptive Interviewers","comments":null,"journal-ref":"LaTeCH-CLfL2025","doi":null,"report-no":null,"categories":"cs.HC cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Traditional methods for eliciting people's opinions face a trade-off between\ndepth and scale: structured surveys enable large-scale data collection but\nlimit respondents' ability to voice their opinions in their own words, while\nconversational interviews provide deeper insights but are resource-intensive.\nThis study explores the potential of replacing human interviewers with large\nlanguage models (LLMs) to conduct scalable conversational interviews. Our goal\nis to assess the performance of AI Conversational Interviewing and to identify\nopportunities for improvement in a controlled environment. We conducted a\nsmall-scale, in-depth study with university students who were randomly assigned\nto a conversational interview by either AI or human interviewers, both\nemploying identical questionnaires on political topics. Various quantitative\nand qualitative measures assessed interviewer adherence to guidelines, response\nquality, participant engagement, and overall interview efficacy. The findings\nindicate the viability of AI Conversational Interviewing in producing quality\ndata comparable to traditional methods, with the added benefit of scalability.\nWe publish our data and materials for re-use and present specific\nrecommendations for effective implementation.\n","versions":"[{'version': 'v1', 'created': 'Mon, 16 Sep 2024 16:03:08 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 09:55:22 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Wuttke', 'Alexander', ''], ['A\u00dfenmacher', 'Matthias', ''], ['Klamm', 'Christopher', ''], ['Lang', 'Max M.', ''], ['W\u00fcrschinger', 'Quirin', ''], ['Kreuter', 'Frauke', '']]","extracted_entities":"[{'text': 'large\\nlanguage models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large\nlanguage models","similarity_score":0.9664971828}
{"id":2410.02056,"submitter":"Sreyan Ghosh","authors":"Sreyan Ghosh and Sonal Kumar and Zhifeng Kong and Rafael Valle and\n  Bryan Catanzaro and Dinesh Manocha","title":"Synthio: Augmenting Small-Scale Audio Classification Datasets with\n  Synthetic Data","comments":"Accepted at ICLR 2025. Code and Checkpoints available here:\n  https:\/\/github.com\/Sreyan88\/Synthio","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.AS cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We present Synthio, a novel approach for augmenting small-scale audio\nclassification datasets with synthetic data. Our goal is to improve audio\nclassification accuracy with limited labeled data. Traditional data\naugmentation techniques, which apply artificial transformations (e.g., adding\nrandom noise or masking segments), struggle to create data that captures the\ntrue diversity present in real-world audios. To address this shortcoming, we\npropose to augment the dataset with synthetic audio generated from\ntext-to-audio (T2A) diffusion models. However, synthesizing effective\naugmentations is challenging because not only should the generated data be\nacoustically consistent with the underlying small-scale dataset, but they\nshould also have sufficient compositional diversity. To overcome the first\nchallenge, we align the generations of the T2A model with the small-scale\ndataset using preference optimization. This ensures that the acoustic\ncharacteristics of the generated data remain consistent with the small-scale\ndataset. To address the second challenge, we propose a novel caption generation\ntechnique that leverages the reasoning capabilities of Large Language Models to\n(1) generate diverse and meaningful audio captions and (2) iteratively refine\ntheir quality. The generated captions are then used to prompt the aligned T2A\nmodel. We extensively evaluate Synthio on ten datasets and four simulated\nlimited-data settings. Results indicate our method consistently outperforms all\nbaselines by 0.1%-39% using a T2A model trained only on weakly-captioned\nAudioSet.\n","versions":"[{'version': 'v1', 'created': 'Wed, 2 Oct 2024 22:05:36 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 00:25:08 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Ghosh', 'Sreyan', ''], ['Kumar', 'Sonal', ''], ['Kong', 'Zhifeng', ''], ['Valle', 'Rafael', ''], ['Catanzaro', 'Bryan', ''], ['Manocha', 'Dinesh', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'prompt', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2410.02191,"submitter":"Qianru Zhang","authors":"Qianru Zhang, Peng Yang, Junliang Yu, Haixin Wang, Xingwei He,\n  Siu-Ming Yiu, Hongzhi Yin","title":"A Survey on Point-of-Interest Recommendation: Models, Architectures, and\n  Security","comments":"20 pages","journal-ref":"TKDE 2025","doi":null,"report-no":"20 pages","categories":"cs.IR cs.AI cs.CE cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The widespread adoption of smartphones and Location-Based Social Networks has\nled to a massive influx of spatio-temporal data, creating unparalleled\nopportunities for enhancing Point-of-Interest (POI) recommendation systems.\nThese advanced POI systems are crucial for enriching user experiences, enabling\npersonalized interactions, and optimizing decision-making processes in the\ndigital landscape. However, existing surveys tend to focus on traditional\napproaches and few of them delve into cutting-edge developments, emerging\narchitectures, as well as security considerations in POI recommendations. To\naddress this gap, our survey stands out by offering a comprehensive, up-to-date\nreview of POI recommendation systems, covering advancements in models,\narchitectures, and security aspects. We systematically examine the transition\nfrom traditional models to advanced techniques such as large language models.\nAdditionally, we explore the architectural evolution from centralized to\ndecentralized and federated learning systems, highlighting the improvements in\nscalability and privacy. Furthermore, we address the increasing importance of\nsecurity, examining potential vulnerabilities and privacy-preserving\napproaches. Our taxonomy provides a structured overview of the current state of\nPOI recommendation, while we also identify promising directions for future\nresearch in this rapidly advancing field.\n","versions":"[{'version': 'v1', 'created': 'Thu, 3 Oct 2024 04:11:42 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 02:57:32 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhang', 'Qianru', ''], ['Yang', 'Peng', ''], ['Yu', 'Junliang', ''], ['Wang', 'Haixin', ''], ['He', 'Xingwei', ''], ['Yiu', 'Siu-Ming', ''], ['Yin', 'Hongzhi', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'scalability', 'label': 'AI Ethics'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2410.02705,"submitter":"Tianheng Cheng","authors":"Zongming Li, Tianheng Cheng, Shoufa Chen, Peize Sun, Haocheng Shen,\n  Longjin Ran, Xiaoxin Chen, Wenyu Liu, Xinggang Wang","title":"ControlAR: Controllable Image Generation with Autoregressive Models","comments":"To appear in ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Autoregressive (AR) models have reformulated image generation as next-token\nprediction, demonstrating remarkable potential and emerging as strong\ncompetitors to diffusion models. However, control-to-image generation, akin to\nControlNet, remains largely unexplored within AR models. Although a natural\napproach, inspired by advancements in Large Language Models, is to tokenize\ncontrol images into tokens and prefill them into the autoregressive model\nbefore decoding image tokens, it still falls short in generation quality\ncompared to ControlNet and suffers from inefficiency. To this end, we introduce\nControlAR, an efficient and effective framework for integrating spatial\ncontrols into autoregressive image generation models. Firstly, we explore\ncontrol encoding for AR models and propose a lightweight control encoder to\ntransform spatial inputs (e.g., canny edges or depth maps) into control tokens.\nThen ControlAR exploits the conditional decoding method to generate the next\nimage token conditioned on the per-token fusion between control and image\ntokens, similar to positional encodings. Compared to prefilling tokens, using\nconditional decoding significantly strengthens the control capability of AR\nmodels but also maintains the model's efficiency. Furthermore, the proposed\nControlAR surprisingly empowers AR models with arbitrary-resolution image\ngeneration via conditional decoding and specific controls. Extensive\nexperiments can demonstrate the controllability of the proposed ControlAR for\nthe autoregressive control-to-image generation across diverse inputs, including\nedges, depths, and segmentation masks. Furthermore, both quantitative and\nqualitative results indicate that ControlAR surpasses previous state-of-the-art\ncontrollable diffusion models, e.g., ControlNet++. Code, models, and demo will\nsoon be available at https:\/\/github.com\/hustvl\/ControlAR.\n","versions":"[{'version': 'v1', 'created': 'Thu, 3 Oct 2024 17:28:07 GMT'}, {'version': 'v2', 'created': 'Fri, 24 Jan 2025 05:25:24 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 06:33:37 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Li', 'Zongming', ''], ['Cheng', 'Tianheng', ''], ['Chen', 'Shoufa', ''], ['Sun', 'Peize', ''], ['Shen', 'Haocheng', ''], ['Ran', 'Longjin', ''], ['Chen', 'Xiaoxin', ''], ['Liu', 'Wenyu', ''], ['Wang', 'Xinggang', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2410.03735,"submitter":"David Grangier","authors":"David Grangier, Simin Fan, Skyler Seto, Pierre Ablin","title":"Task-Adaptive Pretrained Language Models via Clustered-Importance\n  Sampling","comments":"23 pages, presented at the International Conference on Learning\n  Representation (ICLR), 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Specialist language models (LMs) focus on a specific task or domain on which\nthey often outperform generalist LMs of the same size. However, the specialist\ndata needed to pretrain these models is only available in limited amount for\nmost tasks. In this work, we build specialist models from large generalist\ntraining sets instead. We propose a novel method, ClusteRed Importance SamPling\n(CRISP). CRISP clusters the generalist dataset and samples from these clusters\nbased on their frequencies in the smaller specialist dataset. It is scalable,\nsuitable for both pretraining and continued pretraining, and works well in\nmulti-task settings. CRISP performs favorably compared to other methods that\nadjust the training distribution of the generalist data with guidance from the\nlimited domain-specific data. Our findings demonstrate improvements across\ndifferent domains in terms of language modeling perplexity and accuracy on\nmultiple-choice question tasks. We also present ablation studies that examine\nthe impact of dataset sizes, clustering configurations, and model sizes.\n","versions":"[{'version': 'v1', 'created': 'Mon, 30 Sep 2024 20:49:54 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 00:20:30 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Grangier', 'David', ''], ['Fan', 'Simin', ''], ['Seto', 'Skyler', ''], ['Ablin', 'Pierre', '']]","extracted_entities":"[{'text': 'Specialist language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Specialist language models","similarity_score":0.7475558519}
{"id":2410.04759,"submitter":"Yifan Liu","authors":"Tianhui Cai, Yifan Liu, Zewei Zhou, Haoxuan Ma, Seth Z. Zhao, Zhiwen\n  Wu and Jiaqi Ma","title":"Driving with Regulation: Interpretable Decision-Making for Autonomous\n  Vehicles with Retrieval-Augmented Reasoning via LLM","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This work presents an interpretable decision-making framework for autonomous\nvehicles that integrates traffic regulations, norms, and safety guidelines\ncomprehensively and enables seamless adaptation to different regions. While\ntraditional rule-based methods struggle to incorporate the full scope of\ntraffic rules, we develop a Traffic Regulation Retrieval (TRR) Agent based on\nRetrieval-Augmented Generation (RAG) to automatically retrieve relevant traffic\nrules and guidelines from extensive regulation documents and relevant records\nbased on the ego vehicle's situation. Given the semantic complexity of the\nretrieved rules, we also design a reasoning module powered by a Large Language\nModel (LLM) to interpret these rules, differentiate between mandatory rules and\nsafety guidelines, and assess actions on legal compliance and safety.\nAdditionally, the reasoning is designed to be interpretable, enhancing both\ntransparency and reliability. The framework demonstrates robust performance on\nboth hypothesized and real-world cases across diverse scenarios, along with the\nability to adapt to different regions with ease.\n","versions":"[{'version': 'v1', 'created': 'Mon, 7 Oct 2024 05:27:22 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 04:00:16 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Cai', 'Tianhui', ''], ['Liu', 'Yifan', ''], ['Zhou', 'Zewei', ''], ['Ma', 'Haoxuan', ''], ['Zhao', 'Seth Z.', ''], ['Wu', 'Zhiwen', ''], ['Ma', 'Jiaqi', '']]","extracted_entities":"[{'text': 'Large Language\\nModel', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language\nModel","similarity_score":1.0}
{"id":2410.04949,"submitter":"Yongming Chen","authors":"Yongming Chen, Miner Chen, Ye Zhu, Juan Pei, Siyu Chen, Yu Zhou, Yi\n  Wang, Yifan Zhou, Hao Li, Songan Zhang","title":"Leverage Knowledge Graph and Large Language Model for Law Article\n  Recommendation: A Case Study of Chinese Criminal Law","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Court efficiency is vital for social stability. However, in most countries\naround the world, the grassroots courts face case backlogs, with decisions\nrelying heavily on judicial personnel's cognitive labor, lacking intelligent\ntools to improve efficiency. To address this issue, we propose an efficient law\narticle recommendation approach utilizing a Knowledge Graph (KG) and a Large\nLanguage Model (LLM). Firstly, we propose a Case-Enhanced Law Article Knowledge\nGraph (CLAKG) as a database to store current law statutes, historical case\ninformation, and correspondence between law articles and historical cases.\nAdditionally, we introduce an automated CLAKG construction method based on LLM.\nOn this basis, we propose a closed-loop law article recommendation method.\nFinally, through a series of experiments using judgment documents from the\nwebsite \"China Judgements Online\", we have improved the accuracy of law article\nrecommendation in cases from 0.549 to 0.694, demonstrating that our proposed\nmethod significantly outperforms baseline approaches.\n","versions":"[{'version': 'v1', 'created': 'Mon, 7 Oct 2024 11:45:04 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 05:10:23 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Chen', 'Yongming', ''], ['Chen', 'Miner', ''], ['Zhu', 'Ye', ''], ['Pei', 'Juan', ''], ['Chen', 'Siyu', ''], ['Zhou', 'Yu', ''], ['Wang', 'Yi', ''], ['Zhou', 'Yifan', ''], ['Li', 'Hao', ''], ['Zhang', 'Songan', '']]","extracted_entities":"[{'text': 'Large\\nLanguage Model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"Large Language Model","matched_keyword":"Large\nLanguage Model","similarity_score":1.0}
{"id":2410.0544,"submitter":"Zihao Zhou","authors":"Zihao Zhou, Rose Yu","title":"Can LLMs Understand Time Series Anomalies?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) have gained popularity in time series\nforecasting, but their potential for anomaly detection remains largely\nunexplored. Our study investigates whether LLMs can understand and detect\nanomalies in time series data, focusing on zero-shot and few-shot scenarios.\nInspired by conjectures about LLMs' behavior from time series forecasting\nresearch, we formulate key hypotheses about LLMs' capabilities in time series\nanomaly detection. We design and conduct principled experiments to test each of\nthese hypotheses. Our investigation reveals several surprising findings about\nLLMs for time series: (1) LLMs understand time series better as images rather\nthan as text, (2) LLMs do not demonstrate enhanced performance when prompted to\nengage in explicit reasoning about time series analysis. (3) Contrary to common\nbeliefs, LLMs' understanding of time series does not stem from their repetition\nbiases or arithmetic abilities. (4) LLMs' behaviors and performance in time\nseries analysis vary significantly across different models. This study provides\nthe first comprehensive analysis of contemporary LLM capabilities in time\nseries anomaly detection. Our results suggest that while LLMs can understand\ntrivial time series anomalies, we have no evidence that they can understand\nmore subtle real-world anomalies. Many common conjectures based on their\nreasoning capabilities do not hold. All synthetic dataset generators, final\nprompts, and evaluation scripts have been made available in\nhttps:\/\/github.com\/rose-stl-lab\/anomllm.\n","versions":"[{'version': 'v1', 'created': 'Mon, 7 Oct 2024 19:16:02 GMT'}, {'version': 'v2', 'created': 'Mon, 14 Oct 2024 23:32:50 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 18:04:52 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Zhou', 'Zihao', ''], ['Yu', 'Rose', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'zero-shot and few-shot scenarios', 'label': 'Zero-shot Learning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'final\\nprompts', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2410.10855,"submitter":"Hokin Deng","authors":"Yijiang Li, Qingying Gao, Tianwei Zhao, Bingyang Wang, Haoran Sun,\n  Haiyun Lyu, Dezhi Luo, Hokin Deng","title":"Core Knowledge Deficits in Multi-Modal Language Models","comments":"Website with this\n  $\\href{https:\/\/growing-ai-like-a-child.github.io\/}{link}$","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While Multimodal Large Language Models (MLLMs) demonstrate impressive\nabilities over high level perception and reasoning, their robustness in the\nwild still lags behind humans and exhibits diminished efficacy on simple tasks\nthat are intuitive for humans. We examine the hypothesis that these\ndeficiencies stem from the absence of core knowledge, rudimentary cognitive\nabilities innate to humans from early childhood. To probe core knowledge\nrepresentation in MLLMs, we draw from developmental cognitive sciences and\ndevelop a large-scale benchmark, CoreCognition dataset, encompassing 12 core\ncognitive concepts. We evaluate 219 models with 10 different prompts, leading\nto a total of 2409 data points for analysis. Our findings reveal core knowledge\ndeficits in early developed core abilities while models demonstrate human\ncomparable performance in high level cognition. Moreover, we find that low\nlevel abilities show little to no scaling, in stark contrast to high level\nabilities. Finally, we introduce an evaluation technique, Concept Hacking,\nthrough which we demonstrate that MLLMs do not genuinely advance toward core\nknowledge but instead rely on illusory understanding and shortcut learning as\nthey scale. Website with this\n$\\href{https:\/\/growing-ai-like-a-child.github.io\/}{link}$.\n","versions":"[{'version': 'v1', 'created': 'Sun, 6 Oct 2024 20:13:11 GMT'}, {'version': 'v2', 'created': 'Sat, 2 Nov 2024 21:07:54 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 04:39:42 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Li', 'Yijiang', ''], ['Gao', 'Qingying', ''], ['Zhao', 'Tianwei', ''], ['Wang', 'Bingyang', ''], ['Sun', 'Haoran', ''], ['Lyu', 'Haiyun', ''], ['Luo', 'Dezhi', ''], ['Deng', 'Hokin', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': '10 different prompts', 'label': 'Prompting'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2410.11996,"submitter":"Seiji Maekawa","authors":"Seiji Maekawa, Hayate Iso, Nikita Bhutani","title":"Holistic Reasoning with Long-Context LMs: A Benchmark for Database\n  Operations on Massive Textual Data","comments":"ICLR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The rapid increase in textual information means we need more efficient\nmethods to sift through, organize, and understand it all. While\nretrieval-augmented generation (RAG) models excel in accessing information from\nlarge document collections, they struggle with complex tasks that require\naggregation and reasoning over information spanning across multiple\ndocuments--what we call holistic reasoning. Long-context language models\n(LCLMs) have great potential for managing large-scale documents, but their\nholistic reasoning capabilities remain unclear. In this work, we introduce\nHoloBench, a novel framework that brings database reasoning operations into\ntext-based contexts, making it easier to systematically evaluate how LCLMs\nhandle holistic reasoning across large documents. Our approach adjusts key\nfactors such as context length, information density, distribution of\ninformation, and query complexity to evaluate LCLMs comprehensively. Our\nexperiments show that the amount of information in the context has a bigger\ninfluence on LCLM performance than the actual context length. Furthermore, the\ncomplexity of queries affects performance more than the amount of information,\nparticularly for different types of queries. Interestingly, queries that\ninvolve finding maximum or minimum values are easier for LCLMs and are less\naffected by context length, even though they pose challenges for RAG systems.\nHowever, tasks requiring the aggregation of multiple pieces of information show\na noticeable drop in accuracy as context length increases. Additionally, we\nfind that while grouping relevant information generally improves performance,\nthe optimal positioning varies across models. Our findings surface both the\nadvancements and the ongoing challenges in achieving a holistic understanding\nof long contexts.\n","versions":"[{'version': 'v1', 'created': 'Tue, 15 Oct 2024 19:04:13 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Feb 2025 01:36:03 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 18:21:04 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Maekawa', 'Seiji', ''], ['Iso', 'Hayate', ''], ['Bhutani', 'Nikita', '']]","extracted_entities":"[{'text': 'Long-context language models', 'label': 'Large Language Model'}, {'text': 'LCLMs', 'label': 'Large Language Model'}, {'text': 'text-based contexts', 'label': 'contextual Embedding'}, {'text': 'LCLMs', 'label': 'Large Language Model'}, {'text': 'LCLMs', 'label': 'Large Language Model'}, {'text': 'LCLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Long-context language models","similarity_score":0.7303164601}
{"id":2410.14211,"submitter":"Xingyu Tan","authors":"Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Wenjie Zhang","title":"Paths-over-Graph: Knowledge Graph Empowered Large Language Model\n  Reasoning","comments":"Accepted by The Web Conference 2025 (WWW, 2025)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) have achieved impressive results in various\ntasks but struggle with hallucination problems and lack of relevant knowledge,\nespecially in deep complex reasoning and knowledge-intensive tasks. Knowledge\nGraphs (KGs), which capture vast amounts of facts in a structured format, offer\na reliable source of knowledge for reasoning. However, existing KG-based LLM\nreasoning methods face challenges like handling multi-hop reasoning,\nmulti-entity questions, and effectively utilizing graph structures. To address\nthese issues, we propose Paths-over-Graph (PoG), a novel method that enhances\nLLM reasoning by integrating knowledge reasoning paths from KGs, improving the\ninterpretability and faithfulness of LLM outputs. PoG tackles multi-hop and\nmulti-entity questions through a three-phase dynamic multi-hop path\nexploration, which combines the inherent knowledge of LLMs with factual\nknowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant\ninformation from the graph exploration first and introduces efficient\nthree-step pruning techniques that incorporate graph structures, LLM prompting,\nand a pre-trained language model (e.g., SBERT) to effectively narrow down the\nexplored candidate paths. This ensures all reasoning paths contain highly\nrelevant information captured from KGs, making the reasoning faithful and\ninterpretable in problem-solving. PoG innovatively utilizes graph structure to\nprune the irrelevant noise and represents the first method to implement\nmulti-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive\nexperiments on five benchmark KGQA datasets demonstrate PoG outperforms the\nstate-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an\naverage accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo\nsurpasses ToG with GPT-4 by up to 23.9%.\n","versions":"[{'version': 'v1', 'created': 'Fri, 18 Oct 2024 06:57:19 GMT'}, {'version': 'v2', 'created': 'Mon, 21 Oct 2024 01:22:16 GMT'}, {'version': 'v3', 'created': 'Tue, 28 Jan 2025 04:31:11 GMT'}, {'version': 'v4', 'created': 'Wed, 12 Mar 2025 23:45:13 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Tan', 'Xingyu', ''], ['Wang', 'Xiaoyang', ''], ['Liu', 'Qing', ''], ['Xu', 'Xiwei', ''], ['Yuan', 'Xin', ''], ['Zhang', 'Wenjie', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'PoG', 'label': 'LLM-based'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'SBERT', 'label': 'BERT'}, {'text': 'GPT-3.5-Turbo', 'label': 'GPT'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'GPT-3.5-Turbo', 'label': 'GPT'}, {'text': 'GPT-4', 'label': 'GPT'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2410.18469,"submitter":"Chung En Sun","authors":"Chung-En Sun, Xiaodong Liu, Weiwei Yang, Tsui-Wei Weng, Hao Cheng,\n  Aidan San, Michel Galley, Jianfeng Gao","title":"Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities","comments":"Accepted to NAACL 2025 Main (oral)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent research has shown that Large Language Models (LLMs) are vulnerable to\nautomated jailbreak attacks, where adversarial suffixes crafted by algorithms\nappended to harmful queries bypass safety alignment and trigger unintended\nresponses. Current methods for generating these suffixes are computationally\nexpensive and have low Attack Success Rates (ASR), especially against\nwell-aligned models like Llama2 and Llama3. To overcome these limitations, we\nintroduce ADV-LLM, an iterative self-tuning process that crafts adversarial\nLLMs with enhanced jailbreak ability. Our framework significantly reduces the\ncomputational cost of generating adversarial suffixes while achieving nearly\n100\\% ASR on various open-source LLMs. Moreover, it exhibits strong attack\ntransferability to closed-source models, achieving 99\\% ASR on GPT-3.5 and 49\\%\nASR on GPT-4, despite being optimized solely on Llama3. Beyond improving\njailbreak ability, ADV-LLM provides valuable insights for future safety\nalignment research through its ability to generate large datasets for studying\nLLM safety.\n","versions":"[{'version': 'v1', 'created': 'Thu, 24 Oct 2024 06:36:12 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Oct 2024 23:05:59 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 23:26:25 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Sun', 'Chung-En', ''], ['Liu', 'Xiaodong', ''], ['Yang', 'Weiwei', ''], ['Weng', 'Tsui-Wei', ''], ['Cheng', 'Hao', ''], ['San', 'Aidan', ''], ['Galley', 'Michel', ''], ['Gao', 'Jianfeng', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Llama2', 'label': 'Llama'}, {'text': 'Llama3', 'label': 'Llama'}, {'text': 'ADV-LLM', 'label': 'Open-source LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GPT-3', 'label': 'GPT'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'Llama3', 'label': 'Llama'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2410.19482,"submitter":"Jamie Hayes","authors":"Jamie Hayes, Marika Swanberg, Harsh Chaudhari, Itay Yona, Ilia\n  Shumailov, Milad Nasr, Christopher A. Choquette-Choo, Katherine Lee, A. Feder\n  Cooper","title":"Measuring memorization in language models via probabilistic extraction","comments":"NAACL 25","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs) are susceptible to memorizing training data,\nraising concerns about the potential extraction of sensitive information at\ngeneration time. Discoverable extraction is the most common method for\nmeasuring this issue: split a training example into a prefix and suffix, then\nprompt the LLM with the prefix, and deem the example extractable if the LLM\ngenerates the matching suffix using greedy sampling. This definition yields a\nyes-or-no determination of whether extraction was successful with respect to a\nsingle query. Though efficient to compute, we show that this definition is\nunreliable because it does not account for non-determinism present in more\nrealistic (non-greedy) sampling schemes, for which LLMs produce a range of\noutputs for the same prompt. We introduce probabilistic discoverable\nextraction, which, without additional cost, relaxes discoverable extraction by\nconsidering multiple queries to quantify the probability of extracting a target\nsequence. We evaluate our probabilistic measure across different models,\nsampling schemes, and training-data repetitions, and find that this measure\nprovides more nuanced information about extraction risk compared to traditional\ndiscoverable extraction.\n","versions":"[{'version': 'v1', 'created': 'Fri, 25 Oct 2024 11:37:04 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 14:25:10 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Hayes', 'Jamie', ''], ['Swanberg', 'Marika', ''], ['Chaudhari', 'Harsh', ''], ['Yona', 'Itay', ''], ['Shumailov', 'Ilia', ''], ['Nasr', 'Milad', ''], ['Choquette-Choo', 'Christopher A.', ''], ['Lee', 'Katherine', ''], ['Cooper', 'A. Feder', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2410.20215,"submitter":"Xinyu Tang","authors":"Xinyu Tang, Xiaolei Wang, Wayne Xin Zhao, Ji-Rong Wen","title":"DAWN-ICL: Strategic Planning of Problem-solving Trajectories for\n  Zero-Shot In-Context Learning","comments":"NAACL 2025 Main Conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Zero-shot in-context learning (ZS-ICL) aims to conduct in-context learning\n(ICL) without using human-annotated demonstrations. Most ZS-ICL methods use\nlarge language models (LLMs) to generate (input, label) pairs as\npseudo-demonstrations and leverage historical pseudo-demonstrations to help\nsolve the current problem. They assume that problems are from the same task and\ntraverse them in a random order. However, in real-world scenarios, problems\nusually come from diverse tasks, and only a few belong to the same task. The\nrandom traversing order may generate unreliable pseudo-demonstrations and lead\nto error accumulation. To address this problem, we reformulate ZS-ICL as a\nplanning problem and propose a Demonstration-aware Monte Carlo Tree Search\n(MCTS) approach (DAWN-ICL), which leverages MCTS to strategically plan the\nproblem-solving trajectories for ZS-ICL. In addition, to achieve effective and\nefficient Q value estimation, we propose a novel demonstration-aware Q-value\nfunction and use it to enhance the selection phase and accelerate the expansion\nand simulation phases in MCTS. Extensive experiments demonstrate the\neffectiveness and efficiency of DAWN-ICL on in-domain and cross-domain\nscenarios, and it even outperforms ICL using human-annotated labels. The code\nis available at https:\/\/github.com\/RUCAIBox\/MCTS4ZSICL.\n","versions":"[{'version': 'v1', 'created': 'Sat, 26 Oct 2024 16:17:02 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 04:20:07 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Tang', 'Xinyu', ''], ['Wang', 'Xiaolei', ''], ['Zhao', 'Wayne Xin', ''], ['Wen', 'Ji-Rong', '']]","extracted_entities":"[{'text': 'Zero-shot in-context learning', 'label': 'Few-shot Learning'}, {'text': 'ZS-ICL', 'label': 'Zero-shot Learning'}, {'text': 'in-context learning', 'label': 'Few-shot Learning'}, {'text': 'ICL', 'label': 'Zero-shot Learning'}, {'text': 'ZS-ICL', 'label': 'Few-shot Learning'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ZS-ICL', 'label': 'Few-shot Learning'}, {'text': 'ZS-ICL', 'label': 'Few-shot Learning'}, {'text': 'ICL', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2410.20666,"submitter":"Sangmim Song Mr","authors":"Sangmim Song, Sarath Kodagoda, Amal Gunatilake, Marc G. Carmichael,\n  Karthick Thiyagarajan, Jodi Martin","title":"Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for\n  Robotic Guidance of People with Visual Impairments","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Navigation presents a significant challenge for persons with visual\nimpairments (PVI). While traditional aids such as white canes and guide dogs\nare invaluable, they fall short in delivering detailed spatial information and\nprecise guidance to desired locations. Recent developments in large language\nmodels (LLMs) and vision-language models (VLMs) offer new avenues for enhancing\nassistive navigation. In this paper, we introduce Guide-LLM, an embodied\nLLM-based agent designed to assist PVI in navigating large indoor environments.\nOur approach features a novel text-based topological map that enables the LLM\nto plan global paths using a simplified environmental representation, focusing\non straight paths and right-angle turns to facilitate navigation. Additionally,\nwe utilize the LLM's commonsense reasoning for hazard detection and\npersonalized path planning based on user preferences. Simulated experiments\ndemonstrate the system's efficacy in guiding PVI, underscoring its potential as\na significant advancement in assistive technology. The results highlight\nGuide-LLM's ability to offer efficient, adaptive, and personalized navigation\nassistance, pointing to promising advancements in this field.\n","versions":"[{'version': 'v1', 'created': 'Mon, 28 Oct 2024 01:58:21 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 23:45:58 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Song', 'Sangmim', ''], ['Kodagoda', 'Sarath', ''], ['Gunatilake', 'Amal', ''], ['Carmichael', 'Marc G.', ''], ['Thiyagarajan', 'Karthick', ''], ['Martin', 'Jodi', '']]","extracted_entities":"[{'text': 'large language\\nmodels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'Guide-LLM', 'label': 'LLM'}, {'text': 'Guide-LLM', 'label': 'LLM'}]","assigned_concept":"Large Language Model","matched_keyword":"large language\nmodels","similarity_score":0.9664971828}
{"id":2410.22592,"submitter":"Royi Rassin","authors":"Royi Rassin, Aviv Slobodkin, Shauli Ravfogel, Yanai Elazar, Yoav\n  Goldberg","title":"GRADE: Quantifying Sample Diversity in Text-to-Image Models","comments":"For project page and code see https:\/\/royira.github.io\/GRADE","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce GRADE, an automatic method for quantifying sample diversity in\ntext-to-image models. Our method leverages the world knowledge embedded in\nlarge language models and visual question-answering systems to identify\nrelevant concept-specific axes of diversity (e.g., ``shape'' for the concept\n``cookie''). It then estimates frequency distributions of concepts and their\nattributes and quantifies diversity using entropy. We use GRADE to measure the\ndiversity of 12 models over a total of 720K images, revealing that all models\ndisplay limited variation, with clear deterioration in stronger models.\nFurther, we find that models often exhibit default behaviors, a phenomenon\nwhere a model consistently generates concepts with the same attributes (e.g.,\n98% of the cookies are round). Lastly, we show that a key reason for low\ndiversity is underspecified captions in training data. Our work proposes an\nautomatic, semantically-driven approach to measure sample diversity and\nhighlights the stunning homogeneity in text-to-image models.\n","versions":"[{'version': 'v1', 'created': 'Tue, 29 Oct 2024 23:10:28 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 07:44:10 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Rassin', 'Royi', ''], ['Slobodkin', 'Aviv', ''], ['Ravfogel', 'Shauli', ''], ['Elazar', 'Yanai', ''], ['Goldberg', 'Yoav', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2411.00915,"submitter":"Liang Mi","authors":"Liang Mi, Weijun Wang, Wenming Tu, Qingfeng He, Rui Kong, Xinyu Fang,\n  Yazhu Dong, Yikang Zhang, Yunchun Li, Meng Li, Haipeng Dai, Guihai Chen,\n  Yunxin Liu","title":"V-LoRA: An Efficient and Flexible System Boosts Vision Applications with\n  LoRA LMM","comments":"EuroSys'2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Multimodal Models (LMMs) have shown significant progress in various\ncomplex vision tasks with the solid linguistic and reasoning capacity inherited\nfrom large language models (LMMs). Low-rank adaptation (LoRA) offers a\npromising method to integrate external knowledge into LMMs, compensating for\ntheir limitations on domain-specific tasks. However, the existing LoRA model\nserving is excessively computationally expensive and causes extremely high\nlatency. In this paper, we present an end-to-end solution that empowers diverse\nvision tasks and enriches vision applications with LoRA LMMs. Our system,\nVaLoRA, enables accurate and efficient vision tasks by 1) an accuracy-aware\nLoRA adapter generation approach that generates LoRA adapters rich in\ndomain-specific knowledge to meet application-specific accuracy requirements,\n2) an adaptive-tiling LoRA adapters batching operator that efficiently computes\nconcurrent heterogeneous LoRA adapters, and 3) a flexible LoRA adapter\norchestration mechanism that manages application requests and LoRA adapters to\nachieve the lowest average response latency. We prototype VaLoRA on five\npopular vision tasks on three LMMs. Experiment results reveal that VaLoRA\nimproves 24-62% of the accuracy compared to the original LMMs and reduces\n20-89% of the latency compared to the state-of-the-art LoRA model serving\nsystems.\n","versions":"[{'version': 'v1', 'created': 'Fri, 1 Nov 2024 13:43:33 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Feb 2025 05:57:42 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 13:26:38 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 08:38:15 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Mi', 'Liang', ''], ['Wang', 'Weijun', ''], ['Tu', 'Wenming', ''], ['He', 'Qingfeng', ''], ['Kong', 'Rui', ''], ['Fang', 'Xinyu', ''], ['Dong', 'Yazhu', ''], ['Zhang', 'Yikang', ''], ['Li', 'Yunchun', ''], ['Li', 'Meng', ''], ['Dai', 'Haipeng', ''], ['Chen', 'Guihai', ''], ['Liu', 'Yunxin', '']]","extracted_entities":"[{'text': 'Large Multimodal Models', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Multimodal Models","similarity_score":0.5739125013}
{"id":2411.02348,"submitter":"Claire Stevenson","authors":"Claire E. Stevenson, Alexandra Pafford, Han L. J. van der Maas,\n  Melanie Mitchell","title":"Can Large Language Models generalize analogy solving like people can?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  When we solve an analogy we transfer information from a known context to a\nnew one through abstract rules and relational similarity. In people, the\nability to solve analogies such as \"body : feet :: table : ?\" emerges in\nchildhood, and appears to transfer easily to other domains, such as the visual\ndomain \"( : ) :: < : ?\". Recent research shows that large language models\n(LLMs) can solve various forms of analogies. However, can LLMs generalize\nanalogy solving to new domains like people can? To investigate this, we had\nchildren, adults, and LLMs solve a series of letter-string analogies (e.g., a b\n: a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek\nalphabet), and a far transfer domain (list of symbols). As expected, children\nand adults easily generalized their knowledge to unfamiliar domains, whereas\nLLMs did not. This key difference between human and AI performance is evidence\nthat these LLMs still struggle with robust human-like analogical transfer.\n","versions":"[{'version': 'v1', 'created': 'Mon, 4 Nov 2024 18:18:38 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 19:51:32 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Stevenson', 'Claire E.', ''], ['Pafford', 'Alexandra', ''], ['van der Maas', 'Han L. J.', ''], ['Mitchell', 'Melanie', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2411.08127,"submitter":"SangHyun Park","authors":"Shih-Ying Yeh, Sang-Hyun Park, Yi Li, Giyeong Oh, Xuehai Wang, Min\n  Song, Youngjae Yu","title":"TIPO: Text to Image with Text Presampling for Prompt Optimization","comments":"41 pages, 32 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  TIPO (Text-to-Image Prompt Optimization) introduces an efficient approach for\nautomatic prompt refinement in text-to-image (T2I) generation. Starting from\nsimple user prompts, TIPO leverages a lightweight pre-trained model to expand\nthese prompts into richer, detailed versions. Conceptually, TIPO samples\nrefined prompts from a targeted sub-distribution within the broader semantic\nspace, preserving the original intent while significantly improving visual\nquality, coherence, and detail. Unlike resource-intensive methods based on\nlarge language models (LLMs) or reinforcement learning (RL), TIPO provides\ncomputational efficiency and scalability, opening new possibilities for\neffective, automated prompt engineering in T2I tasks.\n  We provide visual results, human preference report to investigate TIPO's\neffectiveness. Experimental evaluations on benchmark datasets demonstrate\nsubstantial improvements in aesthetic quality, significant reduction of visual\nartifacts, and enhanced alignment with target distributions along with\nsignificant human preference proficiency. These results highlight the\nimportance of targeted prompt engineering in text-to-image tasks and indicate\nbroader opportunities for automated prompt refinement.\n","versions":"[{'version': 'v1', 'created': 'Tue, 12 Nov 2024 19:09:45 GMT'}, {'version': 'v2', 'created': 'Fri, 22 Nov 2024 14:58:31 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 18:21:57 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Yeh', 'Shih-Ying', ''], ['Park', 'Sang-Hyun', ''], ['Li', 'Yi', ''], ['Oh', 'Giyeong', ''], ['Wang', 'Xuehai', ''], ['Song', 'Min', ''], ['Yu', 'Youngjae', '']]","extracted_entities":"[{'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'scalability', 'label': 'Scaling law'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2411.12279,"submitter":"Ziyang Zong","authors":"Ziyang Zong, Guanying Chen, Zhaohuan Zhan, Fengcheng Yu, Guang Tan","title":"HouseTune: Two-Stage Floorplan Generation with LLM Assistance","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper proposes a two-stage text-to-floorplan generation framework that\ncombines the reasoning capability of Large Language Models (LLMs) with the\ngenerative power of diffusion models. In the first stage, we leverage a\nChain-of-Thought (CoT) prompting strategy to guide an LLM in generating an\ninitial layout (Layout-Init) from natural language descriptions, which ensures\na user-friendly and intuitive design process. However, Layout-Init may lack\nprecise geometric alignment and fine-grained structural details. To address\nthis, the second stage employs a conditional diffusion model to refine\nLayout-Init into a final floorplan (Layout-Final) that better adheres to\nphysical constraints and user requirements. Unlike prior methods, our approach\neffectively reduces the difficulty of floorplan generation learning without the\nneed for extensive domain-specific training data. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance across all\nmetrics, which validates its effectiveness in practical home design\napplications.\n","versions":"[{'version': 'v1', 'created': 'Tue, 19 Nov 2024 06:57:45 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Nov 2024 05:05:48 GMT'}, {'version': 'v3', 'created': 'Sun, 1 Dec 2024 02:12:08 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Mar 2025 11:08:17 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zong', 'Ziyang', ''], ['Chen', 'Guanying', ''], ['Zhan', 'Zhaohuan', ''], ['Yu', 'Fengcheng', ''], ['Tan', 'Guang', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2411.12789,"submitter":"Haoyu Zhao","authors":"Haoyu Zhao, Hao Wang, Xingyue Zhao, Hao Fei, Hongqiu Wang, Chengjiang\n  Long, Hua Zou","title":"Efficient Physics Simulation for 3D Scenes via MLLM-Guided Gaussian\n  Splatting","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advancements in 3D generation models have opened new possibilities for\nsimulating dynamic 3D object movements and customizing behaviors, yet creating\nthis content remains challenging. Current methods often require manual\nassignment of precise physical properties for simulations or rely on video\ngeneration models to predict them, which is computationally intensive. In this\npaper, we rethink the usage of multi-modal large language model (MLLM) in\nphysics-based simulation, and present Sim Anything, a physics-based approach\nthat endows static 3D objects with interactive dynamics. We begin with detailed\nscene reconstruction and object-level 3D open-vocabulary segmentation,\nprogressing to multi-view image in-painting. Inspired by human visual\nreasoning, we propose MLLM-based Physical Property Perception (MLLM-P3) to\npredict mean physical properties of objects in a zero-shot manner. Based on the\nmean values and the object's geometry, the Material Property Distribution\nPrediction model (MPDP) model then estimates the full distribution,\nreformulating the problem as probability distribution estimation to reduce\ncomputational costs. Finally, we simulate objects in an open-world scene with\nparticles sampled via the Physical-Geometric Adaptive Sampling (PGAS) strategy,\nefficiently capturing complex deformations and significantly reducing\ncomputational costs. Extensive experiments and user studies demonstrate our Sim\nAnything achieves more realistic motion than state-of-the-art methods within 2\nminutes on a single GPU.\n","versions":"[{'version': 'v1', 'created': 'Tue, 19 Nov 2024 12:52:21 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 04:46:09 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Zhao', 'Haoyu', ''], ['Wang', 'Hao', ''], ['Zhao', 'Xingyue', ''], ['Fei', 'Hao', ''], ['Wang', 'Hongqiu', ''], ['Long', 'Chengjiang', ''], ['Zou', 'Hua', '']]","extracted_entities":"[{'text': 'multi-modal large language model', 'label': 'Large Language Model'}, {'text': 'MLLM', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"multi-modal large language model","similarity_score":0.8085874915}
{"id":2411.15594,"submitter":"Xuhui Jiang","authors":"Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai,\n  Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang,\n  Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni, Jian Guo","title":"A Survey on LLM-as-a-Judge","comments":"Project Page: https:\/\/awesome-llm-as-a-judge.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/","abstract":"  Accurate and consistent evaluation is crucial for decision-making across\nnumerous fields, yet it remains a challenging task due to inherent\nsubjectivity, variability, and scale. Large Language Models (LLMs) have\nachieved remarkable success across diverse domains, leading to the emergence of\n\"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With\ntheir ability to process diverse data types and provide scalable,\ncost-effective, and consistent assessments, LLMs present a compelling\nalternative to traditional expert-driven evaluations. However, ensuring the\nreliability of LLM-as-a-Judge systems remains a significant challenge that\nrequires careful design and standardization. This paper provides a\ncomprehensive survey of LLM-as-a-Judge, addressing the core question: How can\nreliable LLM-as-a-Judge systems be built? We explore strategies to enhance\nreliability, including improving consistency, mitigating biases, and adapting\nto diverse assessment scenarios. Additionally, we propose methodologies for\nevaluating the reliability of LLM-as-a-Judge systems, supported by a novel\nbenchmark designed for this purpose. To advance the development and real-world\ndeployment of LLM-as-a-Judge systems, we also discussed practical applications,\nchallenges, and future directions. This survey serves as a foundational\nreference for researchers and practitioners in this rapidly evolving field.\n","versions":"[{'version': 'v1', 'created': 'Sat, 23 Nov 2024 16:03:35 GMT'}, {'version': 'v2', 'created': 'Mon, 16 Dec 2024 15:00:53 GMT'}, {'version': 'v3', 'created': 'Thu, 9 Jan 2025 03:08:17 GMT'}, {'version': 'v4', 'created': 'Sat, 1 Feb 2025 08:55:51 GMT'}, {'version': 'v5', 'created': 'Sun, 9 Mar 2025 05:21:22 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Gu', 'Jiawei', ''], ['Jiang', 'Xuhui', ''], ['Shi', 'Zhichao', ''], ['Tan', 'Hexiang', ''], ['Zhai', 'Xuehao', ''], ['Xu', 'Chengjin', ''], ['Li', 'Wei', ''], ['Shen', 'Yinghan', ''], ['Ma', 'Shengjie', ''], ['Liu', 'Honghao', ''], ['Wang', 'Saizhuo', ''], ['Zhang', 'Kun', ''], ['Wang', 'Yuanzhuo', ''], ['Gao', 'Wen', ''], ['Ni', 'Lionel', ''], ['Guo', 'Jian', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM-as-a-Judge', 'label': 'Large Language Model'}, {'text': 'LLM-as-a-Judge', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2411.17017,"submitter":"Zhenchen Wan","authors":"Zhenchen Wan, Yanwu Xu, Zhaoqing Wang, Feng Liu, Tongliang Liu,\n  Mingming Gong","title":"TED-VITON: Transformer-Empowered Diffusion Models for Virtual Try-On","comments":"Project page: https:\/\/github.com\/ZhenchenWan\/TED-VITON","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advancements in Virtual Try-On (VTO) have demonstrated exceptional\nefficacy in generating realistic images and preserving garment details, largely\nattributed to the robust generative capabilities of text-to-image (T2I)\ndiffusion backbones. However, the T2I models that underpin these methods have\nbecome outdated, thereby limiting the potential for further improvement in VTO.\nAdditionally, current methods face notable challenges in accurately rendering\ntext on garments without distortion and preserving fine-grained details, such\nas textures and material fidelity. The emergence of Diffusion Transformer (DiT)\nbased T2I models has showcased impressive performance and offers a promising\nopportunity for advancing VTO. Directly applying existing VTO techniques to\ntransformer-based T2I models is ineffective due to substantial architectural\ndifferences, which hinder their ability to fully leverage the models' advanced\ncapabilities for improved text generation. To address these challenges and\nunlock the full potential of DiT-based T2I models for VTO, we propose\nTED-VITON, a novel framework that integrates a Garment Semantic (GS) Adapter\nfor enhancing garment-specific features, a Text Preservation Loss to ensure\naccurate and distortion-free text rendering, and a constraint mechanism to\ngenerate prompts by optimizing Large Language Model (LLM). These innovations\nenable state-of-the-art (SOTA) performance in visual quality and text fidelity,\nestablishing a new benchmark for VTO task. Project page:\nhttps:\/\/zhenchenwan.github.io\/TED-VITON\/\n","versions":"[{'version': 'v1', 'created': 'Tue, 26 Nov 2024 01:00:09 GMT'}, {'version': 'v2', 'created': 'Sun, 1 Dec 2024 14:37:22 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 17:42:55 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Wan', 'Zhenchen', ''], ['Xu', 'Yanwu', ''], ['Wang', 'Zhaoqing', ''], ['Liu', 'Feng', ''], ['Liu', 'Tongliang', ''], ['Gong', 'Mingming', '']]","extracted_entities":"[{'text': 'prompts', 'label': 'Prompting'}, {'text': 'Large Language Model', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Model","similarity_score":1.0}
{"id":2411.17237,"submitter":"Zheng Chen","authors":"Zheng Chen, Xun Zhang, Wenbo Li, Renjing Pei, Fenglong Song, Xiongkuo\n  Min, Xiaohong Liu, Xin Yuan, Yong Guo, Yulun Zhang","title":"Grounding-IQA: Multimodal Language Grounding Model for Image Quality\n  Assessment","comments":"Code is available at: https:\/\/github.com\/zhengchen1999\/Grounding-IQA","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The development of multimodal large language models (MLLMs) enables the\nevaluation of image quality through natural language descriptions. This\nadvancement allows for more detailed assessments. However, these MLLM-based IQA\nmethods primarily rely on general contextual descriptions, sometimes limiting\nfine-grained quality assessment. To address this limitation, we introduce a new\nimage quality assessment (IQA) task paradigm, grounding-IQA. This paradigm\nintegrates multimodal referring and grounding with IQA to realize more\nfine-grained quality perception. Specifically, grounding-IQA comprises two\nsubtasks: grounding-IQA-description (GIQA-DES) and visual question answering\n(GIQA-VQA). GIQA-DES involves detailed descriptions with precise locations\n(e.g., bounding boxes), while GIQA-VQA focuses on quality QA for local regions.\nTo realize grounding-IQA, we construct a corresponding dataset, GIQA-160K,\nthrough our proposed automated annotation pipeline. Furthermore, we develop a\nwell-designed benchmark, GIQA-Bench. The benchmark comprehensively evaluates\nthe model grounding-IQA performance from three perspectives: description\nquality, VQA accuracy, and grounding precision. Experiments demonstrate that\nour proposed task paradigm, dataset, and benchmark facilitate the more\nfine-grained IQA application. Code:\nhttps:\/\/github.com\/zhengchen1999\/Grounding-IQA.\n","versions":"[{'version': 'v1', 'created': 'Tue, 26 Nov 2024 09:03:16 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 02:18:29 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Chen', 'Zheng', ''], ['Zhang', 'Xun', ''], ['Li', 'Wenbo', ''], ['Pei', 'Renjing', ''], ['Song', 'Fenglong', ''], ['Min', 'Xiongkuo', ''], ['Liu', 'Xiaohong', ''], ['Yuan', 'Xin', ''], ['Guo', 'Yong', ''], ['Zhang', 'Yulun', '']]","extracted_entities":"[{'text': 'multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"multimodal large language models","similarity_score":0.7649828196}
{"id":2411.19038,"submitter":"Alon Zolfi","authors":"Ben Ganon, Alon Zolfi, Omer Hofman, Inderjeet Singh, Hisashi Kojima,\n  Yuval Elovici, Asaf Shabtai","title":"DIESEL -- Dynamic Inference-Guidance via Evasion of Semantic Embeddings\n  in LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In recent years, large language models (LLMs) have had great success in tasks\nsuch as casual conversation, contributing to significant advancements in\ndomains like virtual assistance. However, they often generate responses that\nare not aligned with human values (e.g., ethical standards, safety), leading to\npotentially unsafe or inappropriate outputs. While several techniques have been\nproposed to address this problem, they come with a cost, requiring\ncomputationally expensive training or dramatically increasing the inference\ntime. In this paper, we present DIESEL, a lightweight inference-guidance\ntechnique that can be seamlessly integrated into any autoregressive LLM to\nsemantically filter undesired concepts from the response. DIESEL can function\neither as a standalone safeguard or as an additional layer of defense,\nenhancing response safety by reranking the LLM's proposed tokens based on their\nsimilarity to predefined negative concepts in the latent space. Our evaluation\ndemonstrates DIESEL's effectiveness on state-of-the-art conversational models,\neven in adversarial jailbreaking scenarios that challenge response safety. We\nalso highlight DIESEL's generalization capabilities, showing that it can be\nused in use cases other than safety, providing general-purpose response\nfiltering.\n","versions":"[{'version': 'v1', 'created': 'Thu, 28 Nov 2024 10:33:11 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 09:54:02 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Ganon', 'Ben', ''], ['Zolfi', 'Alon', ''], ['Hofman', 'Omer', ''], ['Singh', 'Inderjeet', ''], ['Kojima', 'Hisashi', ''], ['Elovici', 'Yuval', ''], ['Shabtai', 'Asaf', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'ethical standards', 'label': 'AI Ethics'}, {'text': 'safety', 'label': 'AI Ethics'}, {'text': 'DIESEL', 'label': 'LLM'}, {'text': 'DIESEL', 'label': 'LLM'}, {'text': 'safety', 'label': 'AI Ethics'}, {'text': 'DIESEL', 'label': 'LLM'}, {'text': 'safety', 'label': 'AI Ethics'}, {'text': 'DIESEL', 'label': 'LLM'}, {'text': 'safety', 'label': 'AI Ethics'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2411.19275,"submitter":"Merlijn Sevenhuijsen","authors":"Merlijn Sevenhuijsen, Khashayar Etemadi, Mattias Nyberg","title":"VeCoGen: Automating Generation of Formally Verified C Code with Large\n  Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models have demonstrated impressive capabilities in generating\ncode, yet they often produce programs with flaws or deviations from intended\nbehavior, limiting their suitability for safety-critical applications. To\naddress this limitation, this paper introduces VECOGEN, a novel tool that\ncombines large language models with formal verification to automate the\ngeneration of formally verified C programs. VECOGEN takes a formal\nspecification in ANSI\/ISO C Specification Language, a natural language\nspecification, and a set of test cases to attempt to generate a verified\nprogram. This program-generation process consists of two steps. First, VECOGEN\ngenerates an initial set of candidate programs. Secondly, the tool iteratively\nimproves on previously generated candidates. If a candidate program meets the\nformal specification, then we are sure the program.is correct. We evaluate\nVECOGEN on 15 problems presented in Codeforces competitions. On these problems,\nVECOGEN solves 13 problems. This work shows the potential of combining large\nlanguage models with formal verification to automate program generation.\n","versions":"[{'version': 'v1', 'created': 'Thu, 28 Nov 2024 17:12:21 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 09:40:22 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Sevenhuijsen', 'Merlijn', ''], ['Etemadi', 'Khashayar', ''], ['Nyberg', 'Mattias', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'large\\nlanguage models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2412.11934,"submitter":"Jingyu Peng","authors":"Jingyu Peng, Maolin Wang, Xiangyu Zhao, Kai Zhang, Wanyu Wang, Pengyue\n  Jia, Qidong Liu, Ruocheng Guo, Qi Liu","title":"Stepwise Reasoning Error Disruption Attack of LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models (LLMs) have made remarkable strides in complex\nreasoning tasks, but their safety and robustness in reasoning processes remain\nunderexplored. Existing attacks on LLM reasoning are constrained by specific\nsettings or lack of imperceptibility, limiting their feasibility and\ngeneralizability. To address these challenges, we propose the Stepwise\nrEasoning Error Disruption (SEED) attack, which subtly injects errors into\nprior reasoning steps to mislead the model into producing incorrect subsequent\nreasoning and final answers. Unlike previous methods, SEED is compatible with\nzero-shot and few-shot settings, maintains the natural reasoning flow, and\nensures covert execution without modifying the instruction. Extensive\nexperiments on four datasets across four different models demonstrate SEED's\neffectiveness, revealing the vulnerabilities of LLMs to disruptions in\nreasoning processes. These findings underscore the need for greater attention\nto the robustness of LLM reasoning to ensure safety in practical applications.\n","versions":"[{'version': 'v1', 'created': 'Mon, 16 Dec 2024 16:20:41 GMT'}, {'version': 'v2', 'created': 'Tue, 24 Dec 2024 03:55:40 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 06:22:15 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Peng', 'Jingyu', ''], ['Wang', 'Maolin', ''], ['Zhao', 'Xiangyu', ''], ['Zhang', 'Kai', ''], ['Wang', 'Wanyu', ''], ['Jia', 'Pengyue', ''], ['Liu', 'Qidong', ''], ['Guo', 'Ruocheng', ''], ['Liu', 'Qi', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'zero-shot', 'label': 'Zero-shot Learning'}, {'text': 'natural reasoning flow', 'label': 'Chain of thought'}, {'text': 'SEED', 'label': 'BERT'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2412.12932,"submitter":"Zihui Cheng","authors":"Zihui Cheng, Qiguang Chen, Jin Zhang, Hao Fei, Xiaocheng Feng,\n  Wanxiang Che, Min Li, Libo Qin","title":"CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large\n  Vision-Language Models","comments":"Accepted at AAAI 2025; Project Page: https:\/\/github.com\/czhhzc\/CoMT","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Large Vision-Language Models (LVLMs) have recently demonstrated amazing\nsuccess in multi-modal tasks, including advancements in Multi-modal\nChain-of-Thought (MCoT) reasoning. Despite these successes, current benchmarks\nstill follow a traditional paradigm with multi-modal input and text-modal\noutput, which leads to significant drawbacks such as missing visual operations\nand vague expressions. Motivated by this, we introduce a novel Chain of\nMulti-modal Thought (CoMT) benchmark to address these limitations. Different\nfrom the traditional MCoT benchmark, CoMT requires both multi-modal input and\nmulti-modal reasoning output, aiming to mimic human-like reasoning that\ninherently integrates visual operation. Specifically, CoMT consists of four\ncategories: (1) Visual Creation, (2) Visual Deletion, (3) Visual Update, and\n(4) Visual Selection to comprehensively explore complex visual operations and\nconcise expression in real scenarios. We evaluate various LVLMs and strategies\non CoMT, revealing some key insights into the capabilities and limitations of\nthe current approaches. We hope that CoMT can inspire more research on\nintroducing multi-modal generation into the reasoning process.\n","versions":"[{'version': 'v1', 'created': 'Tue, 17 Dec 2024 14:10:16 GMT'}, {'version': 'v2', 'created': 'Sat, 15 Feb 2025 06:32:55 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 08:47:34 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Cheng', 'Zihui', ''], ['Chen', 'Qiguang', ''], ['Zhang', 'Jin', ''], ['Fei', 'Hao', ''], ['Feng', 'Xiaocheng', ''], ['Che', 'Wanxiang', ''], ['Li', 'Min', ''], ['Qin', 'Libo', '']]","extracted_entities":"[{'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'Multi-modal\\nChain-of-Thought', 'label': 'Chain of thought'}, {'text': 'Visual Creation', 'label': 'Chain of thought'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Vision-Language Models","similarity_score":0.7742220759}
{"id":2412.16833,"submitter":"Kaiwen Zuo","authors":"Kaiwen Zuo, Yirui Jiang, Fan Mo, Pietro Lio","title":"KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge\n  Graph Enhancement for Medical Diagnosis","comments":"10 pages,5 figures,published to AAAI-25 Bridge Program","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Integrating Large Language Models (LLMs) in healthcare diagnosis demands\nsystematic frameworks that can handle complex medical scenarios while\nmaintaining specialized expertise. We present KG4Diagnosis, a novel\nhierarchical multi-agent framework that combines LLMs with automated knowledge\ngraph construction, encompassing 362 common diseases across medical\nspecialties. Our framework mirrors real-world medical systems through a\ntwo-tier architecture: a general practitioner (GP) agent for initial assessment\nand triage, coordinating with specialized agents for in-depth diagnosis in\nspecific domains. The core innovation lies in our end-to-end knowledge graph\ngeneration methodology, incorporating: (1) semantic-driven entity and relation\nextraction optimized for medical terminology, (2) multi-dimensional decision\nrelationship reconstruction from unstructured medical texts, and (3)\nhuman-guided reasoning for knowledge expansion. KG4Diagnosis serves as an\nextensible foundation for specialized medical diagnosis systems, with\ncapabilities to incorporate new diseases and medical knowledge. The framework's\nmodular design enables seamless integration of domain-specific enhancements,\nmaking it valuable for developing targeted medical diagnosis systems. We\nprovide architectural guidelines and protocols to facilitate adoption across\nmedical contexts.\n","versions":"[{'version': 'v1', 'created': 'Sun, 22 Dec 2024 02:40:59 GMT'}, {'version': 'v2', 'created': 'Fri, 3 Jan 2025 00:07:09 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 03:05:30 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zuo', 'Kaiwen', ''], ['Jiang', 'Yirui', ''], ['Mo', 'Fan', ''], ['Lio', 'Pietro', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'KG4Diagnosis', 'label': 'Foundation Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'KG4Diagnosis', 'label': 'Foundation Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2412.17574,"submitter":"Daoyuan Chen","authors":"Ting Zhou, Daoyuan Chen, Qirui Jiao, Bolin Ding, Yaliang Li, Ying Shen","title":"HumanVBench: Exploring Human-Centric Video Understanding Capabilities of\n  MLLMs with Synthetic Benchmark Data","comments":"22 pages, 23 figures, 7 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In the domain of Multimodal Large Language Models (MLLMs), achieving\nhuman-centric video understanding remains a formidable challenge. Existing\nbenchmarks primarily emphasize object and action recognition, often neglecting\nthe intricate nuances of human emotions, behaviors, and speech-visual alignment\nwithin video content. We present HumanVBench, an innovative benchmark\nmeticulously crafted to bridge these gaps in the evaluation of video MLLMs.\nHumanVBench comprises 16 carefully designed tasks that explore two primary\ndimensions: inner emotion and outer manifestations, spanning static and\ndynamic, basic and complex, as well as single-modal and cross-modal aspects.\nWith two advanced automated pipelines for video annotation and\ndistractor-included QA generation, HumanVBench utilizes diverse\nstate-of-the-art (SOTA) techniques to streamline benchmark data synthesis and\nquality assessment, minimizing human annotation dependency tailored to\nhuman-centric multimodal attributes. A comprehensive evaluation across 22 SOTA\nvideo MLLMs reveals notable limitations in current performance, especially in\ncross-modal and emotion perception, underscoring the necessity for further\nrefinement toward achieving more human-like understanding. HumanVBench is\nopen-sourced to facilitate future advancements and real-world applications in\nvideo MLLMs.\n","versions":"[{'version': 'v1', 'created': 'Mon, 23 Dec 2024 13:45:56 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 03:42:48 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Zhou', 'Ting', ''], ['Chen', 'Daoyuan', ''], ['Jiao', 'Qirui', ''], ['Ding', 'Bolin', ''], ['Li', 'Yaliang', ''], ['Shen', 'Ying', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'HumanVBench', 'label': 'Open-source LLMs'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2412.18084,"submitter":"Long Chen","authors":"Xuan Lin, Long Chen, Yile Wang, Xiangxiang Zeng, Philip S. Yu","title":"Property Enhanced Instruction Tuning for Multi-task Molecule Generation\n  with Large Language Models","comments":"9","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models (LLMs) are widely applied in various natural language\nprocessing tasks such as question answering and machine translation. However,\ndue to the lack of labeled data and the difficulty of manual annotation for\nbiochemical properties, the performance for molecule generation tasks is still\nlimited, especially for tasks involving multi-properties constraints. In this\nwork, we present a two-step framework PEIT (Property Enhanced Instruction\nTuning) to improve LLMs for molecular-related tasks. In the first step, we use\ntextual descriptions, SMILES, and biochemical properties as multimodal inputs\nto pre-train a model called PEIT-GEN, by aligning multi-modal representations\nto synthesize instruction data. In the second step, we fine-tune existing\nopen-source LLMs with the synthesized data, the resulting PEIT-LLM can handle\nmolecule captioning, text-based molecule generation, molecular property\nprediction, and our newly proposed multi-constraint molecule generation tasks.\nExperimental results show that our pre-trained PEIT-GEN outperforms MolT5 and\nBioT5 in molecule captioning, demonstrating modalities align well between\ntextual descriptions, structures, and biochemical properties. Furthermore,\nPEIT-LLM shows promising improvements in multi-task molecule generation,\nproving the scalability of the PEIT framework for various molecular tasks. We\nrelease the code, constructed instruction data, and model checkpoints in\nhttps:\/\/github.com\/chenlong164\/PEIT.\n","versions":"[{'version': 'v1', 'created': 'Tue, 24 Dec 2024 01:48:07 GMT'}, {'version': 'v2', 'created': 'Wed, 5 Mar 2025 02:08:32 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 04:25:11 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Lin', 'Xuan', ''], ['Chen', 'Long', ''], ['Wang', 'Yile', ''], ['Zeng', 'Xiangxiang', ''], ['Yu', 'Philip S.', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'PEIT-GEN', 'label': 'Open-source LLMs'}, {'text': 'PEIT-LLM', 'label': 'Open-source LLMs'}, {'text': 'PEIT-GEN', 'label': 'Open-source LLMs'}, {'text': 'PEIT-LLM', 'label': 'Open-source LLMs'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2412.18947,"submitter":"Kaiwen Zuo","authors":"Kaiwen Zuo, Yirui Jiang","title":"MedHallBench: A New Benchmark for Assessing Hallucination in Medical\n  Large Language Models","comments":"Published to AAAI-25 Bridge Program","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Medical Large Language Models (MLLMs) have demonstrated potential in\nhealthcare applications, yet their propensity for hallucinations -- generating\nmedically implausible or inaccurate information -- presents substantial risks\nto patient care. This paper introduces MedHallBench, a comprehensive benchmark\nframework for evaluating and mitigating hallucinations in MLLMs. Our\nmethodology integrates expert-validated medical case scenarios with established\nmedical databases to create a robust evaluation dataset. The framework employs\na sophisticated measurement system that combines automated ACHMI (Automatic\nCaption Hallucination Measurement in Medical Imaging) scoring with rigorous\nclinical expert evaluations and utilizes reinforcement learning methods to\nachieve automatic annotation. Through an optimized reinforcement learning from\nhuman feedback (RLHF) training pipeline specifically designed for medical\napplications, MedHallBench enables thorough evaluation of MLLMs across diverse\nclinical contexts while maintaining stringent accuracy standards. We conducted\ncomparative experiments involving various models, utilizing the benchmark to\nestablish a baseline for widely adopted large language models (LLMs). Our\nfindings indicate that ACHMI provides a more nuanced understanding of the\neffects of hallucinations compared to traditional metrics, thereby highlighting\nits advantages in hallucination assessment. This research establishes a\nfoundational framework for enhancing MLLMs' reliability in healthcare settings\nand presents actionable strategies for addressing the critical challenge of AI\nhallucinations in medical applications.\n","versions":"[{'version': 'v1', 'created': 'Wed, 25 Dec 2024 16:51:29 GMT'}, {'version': 'v2', 'created': 'Fri, 3 Jan 2025 00:16:52 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 02:29:47 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zuo', 'Kaiwen', ''], ['Jiang', 'Yirui', '']]","extracted_entities":"[{'text': 'Medical Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MedHallBench', 'label': 'Foundation Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MedHallBench', 'label': 'Foundation Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Medical Large Language Models","similarity_score":0.7974728346}
{"id":2412.19496,"submitter":"Xiangkui Cao","authors":"Jie Zhang, Xiangkui Cao, Zhouyu Han, Shiguang Shan, Xilin Chen","title":"Multi-P$^2$A: A Multi-perspective Benchmark on Privacy Assessment for\n  Large Vision-Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Vision-Language Models (LVLMs) exhibit impressive potential across\nvarious tasks but also face significant privacy risks, limiting their practical\napplications. Current researches on privacy assessment for LVLMs is limited in\nscope, with gaps in both assessment dimensions and privacy categories. To\nbridge this gap, we propose Multi-P$^2$A, a comprehensive benchmark for\nevaluating the privacy preservation capabilities of LVLMs in terms of privacy\nawareness and leakage. Privacy awareness measures the model's ability to\nrecognize the privacy sensitivity of input data, while privacy leakage assesses\nthe risk of the model unintentionally disclosing privacy information in its\noutput. We design a range of sub-tasks to thoroughly evaluate the model's\nprivacy protection offered by LVLMs. Multi-P$^2$A covers 26 categories of\npersonal privacy, 15 categories of trade secrets, and 18 categories of state\nsecrets, totaling 31,962 samples. Based on Multi-P$^2$A, we evaluate the\nprivacy preservation capabilities of 21 open-source and 2 closed-source LVLMs.\nOur results reveal that current LVLMs generally pose a high risk of\nfacilitating privacy breaches, with vulnerabilities varying across personal\nprivacy, trade secret, and state secret.\n","versions":"[{'version': 'v1', 'created': 'Fri, 27 Dec 2024 07:33:39 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 04:32:32 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Zhang', 'Jie', ''], ['Cao', 'Xiangkui', ''], ['Han', 'Zhouyu', ''], ['Shan', 'Shiguang', ''], ['Chen', 'Xilin', '']]","extracted_entities":"[{'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Vision-Language Models","similarity_score":0.7742220759}
{"id":2412.20504,"submitter":"Xiao Wang","authors":"Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, and Liqiang Nie","title":"ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding","comments":"Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL cs.MM","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps:\/\/github.com\/SCZwangxiao\/video-ReTaKe\n","versions":"[{'version': 'v1', 'created': 'Sun, 29 Dec 2024 15:42:24 GMT'}, {'version': 'v2', 'created': 'Sun, 5 Jan 2025 14:11:48 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 16:35:59 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Wang', 'Xiao', ''], ['Si', 'Qingyi', ''], ['Wu', 'Jianlong', ''], ['Zhu', 'Shiyu', ''], ['Cao', 'Li', ''], ['Nie', 'Liqiang', '']]","extracted_entities":"[{'text': 'Video Large Language Models', 'label': 'Large Language Model'}, {'text': 'VideoLLMs', 'label': 'Large Language Model'}, {'text': 'VideoLLMs', 'label': 'Large Language Model'}, {'text': 'low attention scores', 'label': 'Attention mechanism'}, {'text': 'VideoLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Video Large Language Models","similarity_score":0.76164186}
{"id":2501.01926,"submitter":"Jiaming Li","authors":"Jiaming Li, Jiacheng Zhang, Zequn Jie, Lin Ma, Guanbin Li","title":"Mitigating Hallucination for Large Vision Language Model by\n  Inter-Modality Correlation Calibration Decoding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large vision-language models (LVLMs) have shown remarkable capabilities in\nvisual-language understanding for downstream multi-modal tasks. Despite their\nsuccess, LVLMs still suffer from generating hallucinations in complex\ngeneration tasks, leading to inconsistencies between visual inputs and\ngenerated content. To address this issue, some approaches have introduced\ninference-time interventions, such as contrastive decoding and attention\nrectification, to reduce overreliance on language priors. However, these\napproaches overlook hallucinations stemming from spurious inter-modality\ncorrelations. In this paper, we propose an Inter-Modality Correlation\nCalibration Decoding (IMCCD) method to mitigate hallucinations in LVLMs in a\ntraining-free manner. In this method, we design a Cross-Modal Value-Enhanced\nDecoding(CMVED) module to alleviate hallucination by a novel contrastive\ndecoding mechanism. During the estimation of distorted distribution, CMVED\nmasks the value vectors associated with significant cross-modal attention\nweights, which address both uni-modality overreliance and misleading\ninter-modality correlations. Additionally, a Content-Driven Attention\nRefinement(CDAR) module refines cross-modal attention weights, guiding LVLMs to\nfocus on important visual content. Experimental results on diverse\nhallucination benchmarks validate the superiority of our method over existing\nstate-of-the-art techniques in reducing hallucinations in LVLM text generation.\nOur code will be available at https:\/\/github.com\/lijm48\/IMCCD.\n","versions":"[{'version': 'v1', 'created': 'Fri, 3 Jan 2025 17:56:28 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 18:21:46 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Li', 'Jiaming', ''], ['Zhang', 'Jiacheng', ''], ['Jie', 'Zequn', ''], ['Ma', 'Lin', ''], ['Li', 'Guanbin', '']]","extracted_entities":"[{'text': 'Large vision-language models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'contrastive decoding', 'label': 'Attention mechanism'}, {'text': 'attention\\nrectification', 'label': 'Attention mechanism'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large vision-language models","similarity_score":0.7742220759}
{"id":2501.05031,"submitter":"Ronghao Dang","authors":"Ronghao Dang, Yuqian Yuan, Wenqi Zhang, Yifei Xin, Boqiang Zhang, Long\n  Li, Liuyi Wang, Qinyang Zeng, Xin Li, Lidong Bing","title":"ECBench: Can Multi-modal Foundation Models Understand the Egocentric\n  World? A Holistic Embodied Cognition Benchmark","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The enhancement of generalization in robots by large vision-language models\n(LVLMs) is increasingly evident. Therefore, the embodied cognitive abilities of\nLVLMs based on egocentric videos are of great interest. However, current\ndatasets for embodied video question answering lack comprehensive and\nsystematic evaluation frameworks. Critical embodied cognitive issues, such as\nrobotic self-cognition, dynamic scene perception, and hallucination, are rarely\naddressed. To tackle these challenges, we propose ECBench, a high-quality\nbenchmark designed to systematically evaluate the embodied cognitive abilities\nof LVLMs. ECBench features a diverse range of scene video sources, open and\nvaried question formats, and 30 dimensions of embodied cognition. To ensure\nquality, balance, and high visual dependence, ECBench uses class-independent\nmeticulous human annotation and multi-round question screening strategies.\nAdditionally, we introduce ECEval, a comprehensive evaluation system that\nensures the fairness and rationality of the indicators. Utilizing ECBench, we\nconduct extensive evaluations of proprietary, open-source, and task-specific\nLVLMs. ECBench is pivotal in advancing the embodied cognitive capabilities of\nLVLMs, laying a solid foundation for developing reliable core models for\nembodied agents. All data and code are available at\nhttps:\/\/github.com\/Rh-Dang\/ECBench.\n","versions":"[{'version': 'v1', 'created': 'Thu, 9 Jan 2025 07:43:49 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 07:45:55 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Dang', 'Ronghao', ''], ['Yuan', 'Yuqian', ''], ['Zhang', 'Wenqi', ''], ['Xin', 'Yifei', ''], ['Zhang', 'Boqiang', ''], ['Li', 'Long', ''], ['Wang', 'Liuyi', ''], ['Zeng', 'Qinyang', ''], ['Li', 'Xin', ''], ['Bing', 'Lidong', '']]","extracted_entities":"[{'text': 'large vision-language models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large vision-language models","similarity_score":0.7742220759}
{"id":2501.05179,"submitter":"Xuyang Liu","authors":"Xuyang Liu, Ziming Wang, Yuhang Han, Yingyao Wang, Jiale Yuan, Jun\n  Song, Bo Zheng, Linfeng Zhang, Siteng Huang, Honggang Chen","title":"Global Compression Commander: Plug-and-Play Inference Acceleration for\n  High-Resolution Large Vision-Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large vision-language models (LVLMs) excel at visual understanding and\nreasoning, but face efficiency challenges due to quadratic complexity in\nprocessing long multimodal contexts. While token compression techniques can\nreduce computational costs, existing approaches are designed for single-view\nLVLMs and fail to consider the unique multi-view characteristics of recent\nhigh-resolution LVLMs with dynamic tiling. While existing methods treat all\ntokens uniformly, our analysis reveals that global thumbnails can naturally\nguide the compression of local crops by providing holistic context for\ninformativeness evaluation. In this paper, we first analyze dynamic tiling\nstrategy comprehensively, revealing both the complementary nature between\nthumbnails and crops, and the distinctive characteristics across different\ncrops. Based on our observations, we propose \"Global Compression Commander\"\n(i.e., GlobalCom$^2$), a novel plug-and-play token compression framework for\nHR-LVLMs. GlobalCom$^2$ leverages thumbnail as the \"commander\" to guide the\ncompression process of local crops, adaptively preserving informative details\nwhile eliminating redundancy. Extensive experiments show that GlobalCom$^2$\nmaintains over 90\\% performance while compressing 90\\% visual tokens, reducing\nFLOPs and peak memory to 9.1\\% and 60\\% respectively across multiple\nbenchmarks. Our code is available at\nhttps:\/\/github.com\/xuyang-liu16\/GlobalCom2.\n","versions":"[{'version': 'v1', 'created': 'Thu, 9 Jan 2025 11:57:58 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Jan 2025 17:34:26 GMT'}, {'version': 'v3', 'created': 'Sun, 16 Feb 2025 18:33:57 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Mar 2025 05:18:12 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Liu', 'Xuyang', ''], ['Wang', 'Ziming', ''], ['Han', 'Yuhang', ''], ['Wang', 'Yingyao', ''], ['Yuan', 'Jiale', ''], ['Song', 'Jun', ''], ['Zheng', 'Bo', ''], ['Zhang', 'Linfeng', ''], ['Huang', 'Siteng', ''], ['Chen', 'Honggang', '']]","extracted_entities":"[{'text': 'Large vision-language models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large vision-language models","similarity_score":0.7742220759}
{"id":2501.06828,"submitter":"Ruizhe Ou","authors":"Ruizhe Ou, Yuan Hu, Fan Zhang, Jiaxin Chen, Yu Liu","title":"GeoPix: Multi-Modal Large Language Model for Pixel-level Image\n  Understanding in Remote Sensing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multi-modal large language models (MLLMs) have achieved remarkable success in\nimage- and region-level remote sensing (RS) image understanding tasks, such as\nimage captioning, visual question answering, and visual grounding. However,\nexisting RS MLLMs lack the pixel-level dialogue capability, which involves\nresponding to user instructions with segmentation masks for specific instances.\nIn this paper, we propose GeoPix, a RS MLLM that extends image understanding\ncapabilities to the pixel level. This is achieved by equipping the MLLM with a\nmask predictor, which transforms visual features from the vision encoder into\nmasks conditioned on the LLM's segmentation token embeddings. To facilitate the\nsegmentation of multi-scale objects in RS imagery, a class-wise learnable\nmemory module is integrated into the mask predictor to capture and store\nclass-wise geo-context at the instance level across the entire dataset. In\naddition, to address the absence of large-scale datasets for training\npixel-level RS MLLMs, we construct the GeoPixInstruct dataset, comprising\n65,463 images and 140,412 instances, with each instance annotated with text\ndescriptions, bounding boxes, and masks. Furthermore, we develop a two-stage\ntraining strategy to balance the distinct requirements of text generation and\nmasks prediction in multi-modal multi-task optimization. Extensive experiments\nverify the effectiveness and superiority of GeoPix in pixel-level segmentation\ntasks, while also maintaining competitive performance in image- and\nregion-level benchmarks.\n","versions":"[{'version': 'v1', 'created': 'Sun, 12 Jan 2025 14:45:27 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 08:16:01 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Ou', 'Ruizhe', ''], ['Hu', 'Yuan', ''], ['Zhang', 'Fan', ''], ['Chen', 'Jiaxin', ''], ['Liu', 'Yu', '']]","extracted_entities":"[{'text': 'Multi-modal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'segmentation token embeddings', 'label': 'Embedding'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multi-modal large language models","similarity_score":0.7925285697}
{"id":2501.13652,"submitter":"Yizheng Sun","authors":"Yizheng Sun, Yanze Xin, Hao Li, Jingyuan Sun, Chenghua Lin, Riza\n  Batista-Navarro","title":"LVPruning: An Effective yet Simple Language-Guided Vision Token Pruning\n  Approach for Multi-modal Large Language Models","comments":"Accepted to NAACL 2025 Findings","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multi-modal Large Language Models (MLLMs) have achieved remarkable success by\nintegrating visual and textual modalities. However, they incur significant\ncomputational overhead due to the large number of vision tokens processed,\nlimiting their practicality in resource-constrained environments. We introduce\nLanguage-Guided Vision Token Pruning (LVPruning) for MLLMs, an effective yet\nsimple method that significantly reduces the computational burden while\npreserving model performance. LVPruning employs cross-attention modules to\ncompute the importance of vision tokens based on their interaction with\nlanguage tokens, determining which to prune. Importantly, LVPruning can be\nintegrated without modifying the original MLLM parameters, which makes\nLVPruning simple to apply or remove. Our experiments show that LVPruning can\neffectively reduce up to 90% of vision tokens by the middle layer of LLaVA-1.5,\nresulting in a 62.1% decrease in inference Tera Floating-Point Operations Per\nSecond (TFLOPs), with an average performance loss of just 0.45% across nine\nmulti-modal benchmarks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 Jan 2025 13:31:51 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 21:32:52 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Sun', 'Yizheng', ''], ['Xin', 'Yanze', ''], ['Li', 'Hao', ''], ['Sun', 'Jingyuan', ''], ['Lin', 'Chenghua', ''], ['Batista-Navarro', 'Riza', '']]","extracted_entities":"[{'text': 'Multi-modal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'cross-attention modules', 'label': 'Attention mechanism'}]","assigned_concept":"Large Language Model","matched_keyword":"Multi-modal Large Language Models","similarity_score":0.7925285697}
{"id":2501.13778,"submitter":"Yoonsang Kim","authors":"Yoonsang Kim, Zainab Aamir, Mithilesh Singh, Saeed Boorboor, Klaus\n  Mueller, Arie E. Kaufman","title":"Explainable XR: Understanding User Behaviors of XR Environments using\n  LLM-assisted Analytics Framework","comments":"11 pages, 8 figures. This is the author's version of the article that\n  has been accepted for publication in IEEE Transactions on Visualization and\n  Computer Graphics","journal-ref":null,"doi":"10.1109\/TVCG.2025.3549537","report-no":null,"categories":"cs.HC cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present Explainable XR, an end-to-end framework for analyzing user\nbehavior in diverse eXtended Reality (XR) environments by leveraging Large\nLanguage Models (LLMs) for data interpretation assistance. Existing XR user\nanalytics frameworks face challenges in handling cross-virtuality - AR, VR, MR\n- transitions, multi-user collaborative application scenarios, and the\ncomplexity of multimodal data. Explainable XR addresses these challenges by\nproviding a virtuality-agnostic solution for the collection, analysis, and\nvisualization of immersive sessions. We propose three main components in our\nframework: (1) A novel user data recording schema, called User Action\nDescriptor (UAD), that can capture the users' multimodal actions, along with\ntheir intents and the contexts; (2) a platform-agnostic XR session recorder,\nand (3) a visual analytics interface that offers LLM-assisted insights tailored\nto the analysts' perspectives, facilitating the exploration and analysis of the\nrecorded XR session data. We demonstrate the versatility of Explainable XR by\ndemonstrating five use-case scenarios, in both individual and collaborative XR\napplications across virtualities. Our technical evaluation and user studies\nshow that Explainable XR provides a highly usable analytics solution for\nunderstanding user actions and delivering multifaceted, actionable insights\ninto user behaviors in immersive environments.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 Jan 2025 15:55:07 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 22:15:10 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Kim', 'Yoonsang', ''], ['Aamir', 'Zainab', ''], ['Singh', 'Mithilesh', ''], ['Boorboor', 'Saeed', ''], ['Mueller', 'Klaus', ''], ['Kaufman', 'Arie E.', '']]","extracted_entities":"[{'text': 'Large\\nLanguage Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large\nLanguage Models","similarity_score":0.9664971828}
{"id":2501.17202,"submitter":"Chen Chen","authors":"Chen Chen, Yuchen Hu, Siyin Wang, Helin Wang, Zhehuai Chen, Chao\n  Zhang, Chao-Han Huck Yang, and Eng Siong Chng","title":"Audio Large Language Models Can Be Descriptive Speech Quality Evaluators","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.CL eess.AS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  An ideal multimodal agent should be aware of the quality of its input\nmodalities. Recent advances have enabled large language models (LLMs) to\nincorporate auditory systems for handling various speech-related tasks.\nHowever, most audio LLMs remain unaware of the quality of the speech they\nprocess. This limitation arises because speech quality evaluation is typically\nexcluded from multi-task training due to the lack of suitable datasets. To\naddress this, we introduce the first natural language-based speech evaluation\ncorpus, generated from authentic human ratings. In addition to the overall Mean\nOpinion Score (MOS), this corpus offers detailed analysis across multiple\ndimensions and identifies causes of quality degradation. It also enables\ndescriptive comparisons between two speech samples (A\/B tests) with human-like\njudgment. Leveraging this corpus, we propose an alignment approach with LLM\ndistillation (ALLD) to guide the audio LLM in extracting relevant information\nfrom raw speech and generating meaningful responses. Experimental results\ndemonstrate that ALLD outperforms the previous state-of-the-art regression\nmodel in MOS prediction, with a mean square error of 0.17 and an A\/B test\naccuracy of 98.6%. Additionally, the generated responses achieve BLEU scores of\n25.8 and 30.2 on two tasks, surpassing the capabilities of task-specific\nmodels. This work advances the comprehensive perception of speech signals by\naudio LLMs, contributing to the development of real-world auditory and sensory\nintelligent agents.\n","versions":"[{'version': 'v1', 'created': 'Mon, 27 Jan 2025 22:47:51 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 02:01:46 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Chen', 'Chen', ''], ['Hu', 'Yuchen', ''], ['Wang', 'Siyin', ''], ['Wang', 'Helin', ''], ['Chen', 'Zhehuai', ''], ['Zhang', 'Chao', ''], ['Yang', 'Chao-Han Huck', ''], ['Chng', 'Eng Siong', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM\\ndistillation', 'label': 'Knowledge distillation'}, {'text': 'ALLD', 'label': 'Knowledge distillation'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2502.01403,"submitter":"Zhiteng Li","authors":"Zhiteng Li, Mingyuan Xia, Jingyuan Zhang, Zheng Hui, Linghe Kong,\n  Yulun Zhang, Xiaokang Yang","title":"AdaSVD: Adaptive Singular Value Decomposition for Large Language Models","comments":"The code and models will be available at\n  https:\/\/github.com\/ZHITENGLI\/AdaSVD","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP) tasks, yet their substantial memory requirements\npresent significant challenges for deployment on resource-constrained devices.\nSingular Value Decomposition (SVD) has emerged as a promising compression\ntechnique for LLMs, offering considerable reductions in memory overhead.\nHowever, existing SVD-based methods often struggle to effectively mitigate the\nerrors introduced by SVD truncation, leading to a noticeable performance gap\nwhen compared to the original models. Furthermore, applying a uniform\ncompression ratio across all transformer layers fails to account for the\nvarying importance of different layers. To address these challenges, we propose\nAdaSVD, an adaptive SVD-based LLM compression approach. Specifically, AdaSVD\nintroduces adaComp, which adaptively compensates for SVD truncation errors by\nalternately updating the singular matrices $\\mathcal{U}$ and\n$\\mathcal{V}^\\top$. Additionally, AdaSVD introduces adaCR, which adaptively\nassigns layer-specific compression ratios based on the relative importance of\neach layer. Extensive experiments across multiple LLM\/VLM families and\nevaluation metrics demonstrate that AdaSVD consistently outperforms\nstate-of-the-art (SOTA) SVD-based methods, achieving superior performance with\nsignificantly reduced memory requirements. Code and models of AdaSVD will be\navailable at https:\/\/github.com\/ZHITENGLI\/AdaSVD.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Feb 2025 14:34:37 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Feb 2025 03:51:28 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 09:04:18 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Li', 'Zhiteng', ''], ['Xia', 'Mingyuan', ''], ['Zhang', 'Jingyuan', ''], ['Hui', 'Zheng', ''], ['Kong', 'Linghe', ''], ['Zhang', 'Yulun', ''], ['Yang', 'Xiaokang', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'AdaSVD', 'label': 'LLM-based'}, {'text': 'AdaSVD', 'label': 'LLM-based'}, {'text': 'AdaSVD', 'label': 'LLM-based'}, {'text': 'AdaSVD', 'label': 'LLM-based'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2502.01821,"submitter":"Runxiang Cheng","authors":"Runxiang Cheng, Michele Tufano, J\\\"urgen Cito, Jos\\'e Cambronero, Pat\n  Rondon, Renyao Wei, Aaron Sun, Satish Chandra","title":"Agentic Bug Reproduction for Effective Automated Program Repair at\n  Google","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Bug reports often lack sufficient detail for developers to reproduce and fix\nthe underlying defects. Bug Reproduction Tests (BRTs), tests that fail when the\nbug is present and pass when it has been resolved, are crucial for debugging,\nbut they are rarely included in bug reports, both in open-source and in\nindustrial settings. Thus, automatically generating BRTs from bug reports has\nthe potential to accelerate the debugging process and lower time to repair.\nThis paper investigates automated BRT generation within an industry setting,\nspecifically at Google, focusing on the challenges of a large-scale,\nproprietary codebase and considering real-world industry bugs extracted from\nGoogle's internal issue tracker. We adapt and evaluate a state-of-the-art BRT\ngeneration technique, LIBRO, and present our agent-based approach, BRT Agent,\nwhich makes use of a fine-tuned Large Language Model (LLM) for code editing.\nOur BRT Agent significantly outperforms LIBRO, achieving a 28% plausible BRT\ngeneration rate, compared to 10% by LIBRO, on 80 human-reported bugs from\nGoogle's internal issue tracker. We further investigate the practical value of\ngenerated BRTs by integrating them with an Automated Program Repair (APR)\nsystem at Google. Our results show that providing BRTs to the APR system\nresults in 30% more bugs with plausible fixes. Additionally, we introduce\nEnsemble Pass Rate (EPR), a metric which leverages the generated BRTs to select\nthe most promising fixes from all fixes generated by APR system. Our evaluation\non EPR for Top-K and threshold-based fix selections demonstrates promising\nresults and trade-offs. For example, EPR correctly selects a plausible fix from\na pool of 20 candidates in 70% of cases, based on its top-1 ranking.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Feb 2025 20:57:17 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 02:30:46 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Cheng', 'Runxiang', ''], ['Tufano', 'Michele', ''], ['Cito', 'J\u00fcrgen', ''], ['Cambronero', 'Jos\u00e9', ''], ['Rondon', 'Pat', ''], ['Wei', 'Renyao', ''], ['Sun', 'Aaron', ''], ['Chandra', 'Satish', '']]","extracted_entities":"[{'text': 'Large Language Model', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Model","similarity_score":1.0}
{"id":2502.02088,"submitter":"Xiaomeng Yang","authors":"Xiaomeng Yang, Zhiyu Tan, and Hao Li","title":"IPO: Iterative Preference Optimization for Text-to-Video Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Video foundation models have achieved significant advancement with the help\nof network upgrade as well as model scale-up. However, they are still hard to\nmeet requirements of applications due to unsatisfied generation quality. To\nsolve this problem, we propose to align video foundation models with human\npreferences from the perspective of post-training in this paper. Consequently,\nwe introduce an Iterative Preference Optimization strategy to enhance generated\nvideo quality by incorporating human feedback. Specifically, IPO exploits a\ncritic model to justify video generations for pairwise ranking as in Direct\nPreference Optimization or point-wise scoring as in Kahneman-Tversky\nOptimization. Given this, IPO optimizes video foundation models with guidance\nof signals from preference feedback, which helps improve generated video\nquality in subject consistency, motion smoothness and aesthetic quality, etc.\nIn addition, IPO incorporates the critic model with the multi-modality large\nlanguage model, which enables it to automatically assign preference labels\nwithout need of retraining or relabeling. In this way, IPO can efficiently\nperform multi-round preference optimization in an iterative manner, without the\nneed of tediously manual labeling. Comprehensive experiments demonstrate that\nthe proposed IPO can effectively improve the video generation quality of a\npretrained model and help a model with only 2B parameters surpass the one with\n5B parameters. Besides, IPO achieves new state-of-the-art performance on VBench\nbenchmark.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Feb 2025 08:14:34 GMT'}, {'version': 'v2', 'created': 'Wed, 5 Feb 2025 06:18:12 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 12:30:00 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Yang', 'Xiaomeng', ''], ['Tan', 'Zhiyu', ''], ['Li', 'Hao', '']]","extracted_entities":"[{'text': 'critic model', 'label': 'Foundation Model'}, {'text': 'IPO', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'critic model', 'label': 'Foundation Model'}, {'text': 'multi-modality large\\nlanguage model', 'label': 'Large Language Model'}, {'text': 'IPO', 'label': 'Generative Pre-trained Transformer (GPT)'}]","assigned_concept":"Large Language Model","matched_keyword":"multi-modality large\nlanguage model","similarity_score":0.7886222601}
{"id":2502.03678,"submitter":"Zeyu Tang","authors":"Zeyu Tang, Zhenhao Chen, Loka Li, Xiangchen Song, Yunlong Deng, Yifan\n  Shen, Guangyi Chen, Peter Spirtes, Kun Zhang","title":"Reflection-Window Decoding: Text Generation with Selective Refinement","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The autoregressive decoding for text generation in large language models\n(LLMs), while widely used, is inherently suboptimal due to the lack of a\nbuilt-in mechanism to perform refinement and\/or correction of the generated\ncontent. In this paper, we consider optimality in terms of the joint\nprobability over the generated response, when jointly considering all tokens at\nthe same time. We theoretically characterize the potential deviation of the\nautoregressively generated response from its globally optimal counterpart that\nis of the same length. Our analysis suggests that we need to be cautious when\nnoticeable uncertainty arises during text generation, which may signal the\nsub-optimality of the generation history. To address the pitfall of\nautoregressive decoding for text generation, we propose an approach that\nincorporates a sliding reflection window and a pausing criterion, such that\nrefinement and generation can be carried out interchangeably as the decoding\nproceeds. Our selective refinement framework strikes a balance between\nefficiency and optimality, and our extensive experimental results demonstrate\nthe effectiveness of our approach.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Feb 2025 23:53:08 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 19:34:32 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Tang', 'Zeyu', ''], ['Chen', 'Zhenhao', ''], ['Li', 'Loka', ''], ['Song', 'Xiangchen', ''], ['Deng', 'Yunlong', ''], ['Shen', 'Yifan', ''], ['Chen', 'Guangyi', ''], ['Spirtes', 'Peter', ''], ['Zhang', 'Kun', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2502.06759,"submitter":"Gaetano Rossiello","authors":"Gaetano Rossiello, Nhan Pham, Michael Glass, Junkyu Lee, Dharmashankar\n  Subramanian","title":"Rationalization Models for Text-to-SQL","comments":"Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.DB","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Feb 2025 18:38:57 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Feb 2025 17:12:34 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 17:37:30 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Rossiello', 'Gaetano', ''], ['Pham', 'Nhan', ''], ['Glass', 'Michael', ''], ['Lee', 'Junkyu', ''], ['Subramanian', 'Dharmashankar', '']]","extracted_entities":"[{'text': 'prompt', 'label': 'Prompting'}, {'text': 'large language model', 'label': 'Large Language Model'}, {'text': 'few-shot knowledge distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Large Language Model","matched_keyword":"large language model","similarity_score":1.0}
{"id":2502.07972,"submitter":"Zach Nussbaum","authors":"Zach Nussbaum, Brandon Duderstadt","title":"Training Sparse Mixture Of Experts Text Embedding Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.IR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Transformer-based text embedding models have improved their performance on\nbenchmarks like MIRACL and BEIR by increasing their parameter counts. However,\nthis scaling approach introduces significant deployment challenges, including\nincreased inference latency and memory usage. These challenges are particularly\nsevere in retrieval-augmented generation (RAG) applications, where large\nmodels' increased memory requirements constrain dataset ingestion capacity, and\ntheir higher latency directly impacts query-time performance. While causal\nlanguage models have addressed similar efficiency challenges using Mixture of\nExperts (MoE) architectures, this approach hasn't been successfully adapted to\nthe general text embedding setting. In this paper, we introduce Nomic Embed v2,\nthe first general purpose MoE text embedding model. Our model outperforms\nmodels in the same parameter class on both monolingual and multilingual\nbenchmarks while also maintaining competitive performance with models twice its\nsize. We open-source all code, models, and evaluation data to ensure full\nreproducibility of our training pipeline at\n\\href{https:\/\/github.com\/nomic-ai\/contrastors}{https:\/\/github.com\/nomic-ai\/contrastors}.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Feb 2025 21:36:31 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Feb 2025 01:23:29 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 19:39:00 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Nussbaum', 'Zach', ''], ['Duderstadt', 'Brandon', '']]","extracted_entities":"[{'text': 'retrieval-augmented generation (RAG)', 'label': 'RAG'}, {'text': 'large\\nmodels', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large\nmodels","similarity_score":0.6642580628}
{"id":2502.11418,"submitter":"Geon Lee","authors":"Geon Lee, Wenchao Yu, Kijung Shin, Wei Cheng, Haifeng Chen","title":"TimeCAP: Learning to Contextualize, Augment, and Predict Time Series\n  Events with Large Language Model Agents","comments":"AAAI 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Time series data is essential in various applications, including climate\nmodeling, healthcare monitoring, and financial analytics. Understanding the\ncontextual information associated with real-world time series data is often\nessential for accurate and reliable event predictions. In this paper, we\nintroduce TimeCAP, a time-series processing framework that creatively employs\nLarge Language Models (LLMs) as contextualizers of time series data, extending\ntheir typical usage as predictors. TimeCAP incorporates two independent LLM\nagents: one generates a textual summary capturing the context of the time\nseries, while the other uses this enriched summary to make more informed\npredictions. In addition, TimeCAP employs a multi-modal encoder that synergizes\nwith the LLM agents, enhancing predictive performance through mutual\naugmentation of inputs with in-context examples. Experimental results on\nreal-world datasets demonstrate that TimeCAP outperforms state-of-the-art\nmethods for time series event prediction, including those utilizing LLMs as\npredictors, achieving an average improvement of 28.75% in F1 score.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Feb 2025 04:17:27 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 04:15:20 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Lee', 'Geon', ''], ['Yu', 'Wenchao', ''], ['Shin', 'Kijung', ''], ['Cheng', 'Wei', ''], ['Chen', 'Haifeng', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2502.11649,"submitter":"Mehwish Nasim","authors":"Amin Qasmi and Usman Naseem and Mehwish Nasim","title":"Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.SI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce a novel non-cooperative game to analyse opinion formation and\nresistance, incorporating principles from social psychology such as\nconfirmation bias, resource constraints, and influence penalties. Our\nsimulation features Large Language Model (LLM) agents competing to influence a\npopulation, with penalties imposed for generating messages that propagate or\ncounter misinformation. This framework integrates resource optimisation into\nthe agents' decision-making process. Our findings demonstrate that while higher\nconfirmation bias strengthens opinion alignment within groups, it also\nexacerbates overall polarisation. Conversely, lower confirmation bias leads to\nfragmented opinions and limited shifts in individual beliefs. Investing heavily\nin a high-resource debunking strategy can initially align the population with\nthe debunking agent, but risks rapid resource depletion and diminished\nlong-term influence.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Feb 2025 10:41:55 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 02:14:41 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Qasmi', 'Amin', ''], ['Naseem', 'Usman', ''], ['Nasim', 'Mehwish', '']]","extracted_entities":"[{'text': 'confirmation bias', 'label': 'Model Bias and Fairness'}, {'text': 'Large Language Model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'confirmation bias', 'label': 'Model Bias and Fairness'}, {'text': 'confirmation bias', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Model","similarity_score":1.0}
{"id":2502.12029,"submitter":"Qi Zhao","authors":"Qi Zhao, Hongyu Yang, Qi Song, Xinwei Yao, Xiangyang Li","title":"KnowPath: Knowledge-enhanced Reasoning via LLM-generated Inference Paths\n  over Knowledge Graphs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious complex tasks, yet they still suffer from hallucinations. Introducing\nexternal knowledge, such as knowledge graph, can enhance the LLMs' ability to\nprovide factual answers. LLMs have the ability to interactively explore\nknowledge graphs. However, most approaches have been affected by insufficient\ninternal knowledge excavation in LLMs, limited generation of trustworthy\nknowledge reasoning paths, and a vague integration between internal and\nexternal knowledge. Therefore, we propose KnowPath, a knowledge-enhanced large\nmodel framework driven by the collaboration of internal and external knowledge.\nIt relies on the internal knowledge of the LLM to guide the exploration of\ninterpretable directed subgraphs in external knowledge graphs, better\nintegrating the two knowledge sources for more accurate reasoning. Extensive\nexperiments on multiple real-world datasets confirm the superiority of\nKnowPath.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Feb 2025 17:02:01 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 13:22:46 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhao', 'Qi', ''], ['Yang', 'Hongyu', ''], ['Song', 'Qi', ''], ['Yao', 'Xinwei', ''], ['Li', 'Xiangyang', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'KnowPath', 'label': 'LLM-based'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2502.12455,"submitter":"Minxuan Lv","authors":"Minxuan Lv, Zhenpeng Su, Leiyu Pan, Yizhe Xiong, Zijia Lin, Hui Chen,\n  Wei Zhou, Jungong Han, Guiguang Ding, Cheng Luo, Di Zhang, Kun Gai, Songlin\n  Hu","title":"DSMoE: Matrix-Partitioned Experts with Dynamic Routing for\n  Computation-Efficient Dense LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  As large language models continue to scale, computational costs and resource\nconsumption have emerged as significant challenges. While existing\nsparsification methods like pruning reduce computational overhead, they risk\nlosing model knowledge through parameter removal. This paper proposes DSMoE\n(Dynamic Sparse Mixture-of-Experts), a novel approach that achieves\nsparsification by partitioning pre-trained FFN layers into computational\nblocks. We implement adaptive expert routing using sigmoid activation and\nstraight-through estimators, enabling tokens to flexibly access different\naspects of model knowledge based on input complexity. Additionally, we\nintroduce a sparsity loss term to balance performance and computational\nefficiency. Extensive experiments on LLaMA models demonstrate that under\nequivalent computational constraints, DSMoE achieves superior performance\ncompared to existing pruning and MoE approaches across language modeling and\ndownstream tasks, particularly excelling in generation tasks. Analysis reveals\nthat DSMoE learns distinctive layerwise activation patterns, providing new\ninsights for future MoE architecture design.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Feb 2025 02:37:26 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 10:40:09 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Lv', 'Minxuan', ''], ['Su', 'Zhenpeng', ''], ['Pan', 'Leiyu', ''], ['Xiong', 'Yizhe', ''], ['Lin', 'Zijia', ''], ['Chen', 'Hui', ''], ['Zhou', 'Wei', ''], ['Han', 'Jungong', ''], ['Ding', 'Guiguang', ''], ['Luo', 'Cheng', ''], ['Zhang', 'Di', ''], ['Gai', 'Kun', ''], ['Hu', 'Songlin', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2502.12509,"submitter":"Kangda Wei","authors":"Kangda Wei, Xi Shi, Jonathan Tong, Sai Ramana Reddy, Anandhavelu\n  Natarajan, Rajiv Jain, Aparna Garimella, Ruihong Huang","title":"LegalCore: A Dataset for Event Coreference Resolution in Legal Documents","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Recognizing events and their coreferential mentions in a document is\nessential for understanding semantic meanings of text. The existing research on\nevent coreference resolution is mostly limited to news articles. In this paper,\nwe present the first dataset for the legal domain, LegalCore, which has been\nannotated with comprehensive event and event coreference information. The legal\ncontract documents we annotated in this dataset are several times longer than\nnews articles, with an average length of around 25k tokens per document. The\nannotations show that legal documents have dense event mentions and feature\nboth short-distance and super long-distance coreference links between event\nmentions. We further benchmark mainstream Large Language Models (LLMs) on this\ndataset for both event detection and event coreference resolution tasks, and\nfind that this dataset poses significant challenges for state-of-the-art\nopen-source and proprietary LLMs, which perform significantly worse than a\nsupervised baseline. We will publish the dataset as well as the code.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Feb 2025 03:47:53 GMT'}, {'version': 'v2', 'created': 'Mon, 3 Mar 2025 19:36:00 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 16:53:11 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Wei', 'Kangda', ''], ['Shi', 'Xi', ''], ['Tong', 'Jonathan', ''], ['Reddy', 'Sai Ramana', ''], ['Natarajan', 'Anandhavelu', ''], ['Jain', 'Rajiv', ''], ['Garimella', 'Aparna', ''], ['Huang', 'Ruihong', '']]","extracted_entities":"[{'text': 'mainstream Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"mainstream Large Language Models","similarity_score":0.8480705023}
{"id":2502.12558,"submitter":"Huaying Yuan","authors":"Huaying Yuan, Jian Ni, Yueze Wang, Junjie Zhou, Zhengyang Liang, Zheng\n  Liu, Zhao Cao, Zhicheng Dou, and Ji-Rong Wen","title":"MomentSeeker: A Comprehensive Benchmark and A Strong Baseline For Moment\n  Retrieval Within Long Videos","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Retrieval augmented generation (RAG) holds great promise in addressing\nchallenges associated with long video understanding. These methods retrieve\nuseful moments from long videos for their presented tasks, thereby enabling\nmultimodal large language models (MLLMs) to generate high-quality answers in a\ncost-effective way. In this work, we present MomentSeeker, a comprehensive\nbenchmark to evaluate retrieval models' performance in handling general\nlong-video moment retrieval (LVMR) tasks. MomentSeeker offers three key\nadvantages. First, it incorporates long videos of over 500 seconds on average,\nmaking it the first benchmark specialized for long-video moment retrieval.\nSecond, it covers a wide range of task categories (including Moment Search,\nCaption Alignment, Image-conditioned Moment Search, and Video-conditioned\nMoment Search) and diverse application scenarios (e.g., sports, movies,\ncartoons, and ego), making it a comprehensive tool for assessing retrieval\nmodels' general LVMR performance. Additionally, the evaluation tasks are\ncarefully curated through human annotation, ensuring the reliability of\nassessment. We further fine-tune an MLLM-based LVMR retriever on synthetic\ndata, which demonstrates strong performance on our benchmark. We perform\nextensive experiments with various popular multimodal retrievers based on our\nbenchmark, whose results highlight the challenges of LVMR and limitations for\nexisting methods. Our created resources will be shared with community to\nadvance future research in this field.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Feb 2025 05:50:23 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 05:34:20 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Yuan', 'Huaying', ''], ['Ni', 'Jian', ''], ['Wang', 'Yueze', ''], ['Zhou', 'Junjie', ''], ['Liang', 'Zhengyang', ''], ['Liu', 'Zheng', ''], ['Cao', 'Zhao', ''], ['Dou', 'Zhicheng', ''], ['Wen', 'Ji-Rong', '']]","extracted_entities":"[{'text': 'multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MomentSeeker', 'label': 'RAG'}, {'text': 'MomentSeeker', 'label': 'RAG'}]","assigned_concept":"Large Language Model","matched_keyword":"multimodal large language models","similarity_score":0.7649828196}
{"id":2502.14856,"submitter":"Weilin Zhao","authors":"Weilin Zhao, Tengyu Pan, Xu Han, Yudi Zhang, Ao Sun, Yuxiang Huang,\n  Kaihuo Zhang, Weilun Zhao, Yuxuan Li, Jianyong Wang, Zhiyuan Liu, Maosong Sun","title":"FR-Spec: Accelerating Large-Vocabulary Language Models via\n  Frequency-Ranked Speculative Sampling","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Speculative sampling has emerged as an important technique for accelerating\nthe auto-regressive generation process of large language models (LLMs) by\nutilizing a draft-then-verify mechanism to produce multiple tokens per forward\npass. While state-of-the-art speculative sampling methods use only a single\nlayer and a language modeling (LM) head as the draft model to achieve\nimpressive layer compression, their efficiency gains are substantially reduced\nfor large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens.\nTo address this, we present FR-Spec, a frequency-ranked speculative sampling\nframework that optimizes draft candidate selection through vocabulary space\ncompression. By constraining the draft search to a frequency-prioritized token\nsubset, our method reduces LM Head computation overhead by 75% while ensuring\nthe equivalence of the final output distribution. Experiments across multiple\ndatasets demonstrate an average of 1.12$\\times$ speedup over the\nstate-of-the-art speculative sampling method EAGLE-2. Code available at\nhttps:\/\/github.com\/thunlp\/FR-Spec.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Feb 2025 18:58:10 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 08:54:55 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Zhao', 'Weilin', ''], ['Pan', 'Tengyu', ''], ['Han', 'Xu', ''], ['Zhang', 'Yudi', ''], ['Sun', 'Ao', ''], ['Huang', 'Yuxiang', ''], ['Zhang', 'Kaihuo', ''], ['Zhao', 'Weilun', ''], ['Li', 'Yuxuan', ''], ['Wang', 'Jianyong', ''], ['Liu', 'Zhiyuan', ''], ['Sun', 'Maosong', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Llama-3-8B', 'label': 'Llama'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2502.15844,"submitter":"Borui Yang","authors":"Borui Yang, Md Afif Al Mamun, Jie M. Zhang, Gias Uddin","title":"Hallucination Detection in Large Language Models with Metamorphic\n  Relations","comments":"Accepted to the ACM Joint European Software Engineering Conference\n  and Symposium on the Foundations of Software Engineering (ESEC\/FSE 2025)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) are prone to hallucinations, e.g., factually\nincorrect information, in their responses. These hallucinations present\nchallenges for LLM-based applications that demand high factual accuracy.\nExisting hallucination detection methods primarily depend on external\nresources, which can suffer from issues such as low availability, incomplete\ncoverage, privacy concerns, high latency, low reliability, and poor\nscalability. There are also methods depending on output probabilities, which\nare often inaccessible for closed-source LLMs like GPT models. This paper\npresents MetaQA, a self-contained hallucination detection approach that\nleverages metamorphic relation and prompt mutation. Unlike existing methods,\nMetaQA operates without any external resources and is compatible with both\nopen-source and closed-source LLMs. MetaQA is based on the hypothesis that if\nan LLM's response is a hallucination, the designed metamorphic relations will\nbe violated. We compare MetaQA with the state-of-the-art zero-resource\nhallucination detection method, SelfCheckGPT, across multiple datasets, and on\ntwo open-source and two closed-source LLMs. Our results reveal that MetaQA\noutperforms SelfCheckGPT in terms of precision, recall, and f1 score. For the\nfour LLMs we study, MetaQA outperforms SelfCheckGPT with a superiority margin\nranging from 0.041 - 0.113 (for precision), 0.143 - 0.430 (for recall), and\n0.154 - 0.368 (for F1-score). For instance, with Mistral-7B, MetaQA achieves an\naverage F1-score of 0.435, compared to SelfCheckGPT's F1-score of 0.205,\nrepresenting an improvement rate of 112.2%. MetaQA also demonstrates\nsuperiority across all different categories of questions.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Feb 2025 19:44:33 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 18:28:18 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Yang', 'Borui', ''], ['Mamun', 'Md Afif Al', ''], ['Zhang', 'Jie M.', ''], ['Uddin', 'Gias', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2502.17591,"submitter":"Martin Kuo","authors":"Martin Kuo, Jingyang Zhang, Jianyi Zhang, Minxue Tang, Louis\n  DiValentin, Aolin Ding, Jingwei Sun, William Chen, Amin Hass, Tianlong Chen,\n  Yiran Chen, Hai Li","title":"Proactive Privacy Amnesia for Large Language Models: Safeguarding PII\n  with Negligible Impact on Model Utility","comments":"ICLR'25 Poster. Project page and code is available at\n  https:\/\/ppa-iclr2025.my.canva.site\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  With the rise of large language models (LLMs), increasing research has\nrecognized their risk of leaking personally identifiable information (PII)\nunder malicious attacks. Although efforts have been made to protect PII in\nLLMs, existing methods struggle to balance privacy protection with maintaining\nmodel utility. In this paper, inspired by studies of amnesia in cognitive\nscience, we propose a novel approach, Proactive Privacy Amnesia (PPA), to\nsafeguard PII in LLMs while preserving their utility. This mechanism works by\nactively identifying and forgetting key memories most closely associated with\nPII in sequences, followed by a memory implanting using suitable substitute\nmemories to maintain the LLM's functionality. We conduct evaluations across\nmultiple models to protect common PII, such as phone numbers and physical\naddresses, against prevalent PII-targeted attacks, demonstrating the\nsuperiority of our method compared with other existing defensive techniques.\nThe results show that our PPA method completely eliminates the risk of phone\nnumber exposure by 100% and significantly reduces the risk of physical address\nexposure by 9.8% - 87.6%, all while maintaining comparable model utility\nperformance.\n","versions":"[{'version': 'v1', 'created': 'Mon, 24 Feb 2025 19:16:39 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 17:32:22 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Kuo', 'Martin', ''], ['Zhang', 'Jingyang', ''], ['Zhang', 'Jianyi', ''], ['Tang', 'Minxue', ''], ['DiValentin', 'Louis', ''], ['Ding', 'Aolin', ''], ['Sun', 'Jingwei', ''], ['Chen', 'William', ''], ['Hass', 'Amin', ''], ['Chen', 'Tianlong', ''], ['Chen', 'Yiran', ''], ['Li', 'Hai', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2502.17599,"submitter":"Zhongwei Wan","authors":"Zhongwei Wan, Hui Shen, Xin Wang, Che Liu, Zheda Mai, Mi Zhang","title":"MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference","comments":"NAACL 2025 Main","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps:\/\/github.com\/AIoT-MLSys-Lab\/MEDA.\n","versions":"[{'version': 'v1', 'created': 'Mon, 24 Feb 2025 19:34:52 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 04:04:08 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wan', 'Zhongwei', ''], ['Shen', 'Hui', ''], ['Wang', 'Xin', ''], ['Liu', 'Che', ''], ['Mai', 'Zheda', ''], ['Zhang', 'Mi', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MEDA', 'label': 'LLM'}, {'text': 'cross-modal attention entropy', 'label': 'Attention mechanism'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2502.19263,"submitter":"Arnavi Chheda-Kothary","authors":"Arnavi Chheda-Kothary, Ritesh Kanchi, Chris Sanders, Kevin Xiao,\n  Aditya Sengupta, Melanie Kneitmix, Jacob O. Wobbrock, Jon E. Froehlich","title":"ArtInsight: Enabling AI-Powered Artwork Engagement for Mixed\n  Visual-Ability Families","comments":"21 pages, 30th International Conference on Intelligent User\n  Interfaces (IUI 2025)","journal-ref":"30th International Conference on Intelligent User Interfaces 2025","doi":"10.1145\/3708359.3712082","report-no":null,"categories":"cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce ArtInsight, a novel AI-powered system to facilitate deeper\nengagement with child-created artwork in mixed visual-ability families.\nArtInsight leverages large language models (LLMs) to craft a respectful and\nthorough initial description of a child's artwork, and provides: creative\nAI-generated descriptions for a vivid overview, audio recording to capture the\nchild's own description of their artwork, and a set of AI-generated questions\nto facilitate discussion between blind or low-vision (BLV) family members and\ntheir children. Alongside ArtInsight, we also contribute a new rubric to score\nAI-generated descriptions of child-created artwork and an assessment of\nstate-of-the-art LLMs. We evaluated ArtInsight with five groups of BLV family\nmembers and their children, and as a case study with one BLV child therapist.\nOur findings highlight a preference for ArtInsight's longer,\nartistically-tailored descriptions over those generated by existing BLV AI\ntools. Participants highlighted the creative description and audio recording\ncomponents as most beneficial, with the former helping ``bring a picture to\nlife'' and the latter centering the child's narrative to generate context-aware\nAI responses. Our findings reveal different ways that AI can be used to support\nart engagement, including before, during, and after interaction with the child\nartist, as well as expectations that BLV adults and their sighted children have\nabout AI-powered tools.\n","versions":"[{'version': 'v1', 'created': 'Wed, 26 Feb 2025 16:17:15 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 16:33:23 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Chheda-Kothary', 'Arnavi', ''], ['Kanchi', 'Ritesh', ''], ['Sanders', 'Chris', ''], ['Xiao', 'Kevin', ''], ['Sengupta', 'Aditya', ''], ['Kneitmix', 'Melanie', ''], ['Wobbrock', 'Jacob O.', ''], ['Froehlich', 'Jon E.', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2502.19679,"submitter":"Linzhuo Li","authors":"Linzhuo li","title":"Old Experience Helps: Leveraging Survey Methodology to Improve AI Text\n  Annotation Reliability in Social Sciences","comments":"8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DL cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper introduces a framework for assessing the reliability of Large\nLanguage Model (LLM) text annotations in social science research by adapting\nestablished survey methodology principles. Drawing parallels between survey\nrespondent behavior and LLM outputs, the study implements three key\ninterventions: option randomization, position randomization, and reverse\nvalidation. While traditional accuracy metrics may mask model instabilities,\nparticularly in edge cases, the framework provides a more comprehensive\nreliability assessment. Using the F1000 dataset in biomedical science and three\nsizes of Llama models (8B, 70B, and 405B parameters), the paper demonstrates\nthat these survey-inspired interventions can effectively identify unreliable\nannotations that might otherwise go undetected through accuracy metrics alone.\nThe results show that 5-25% of LLM annotations change under these\ninterventions, with larger models exhibiting greater stability. Notably, for\nrare categories approximately 50% of \"correct\" annotations demonstrate low\nreliability when subjected to this framework. The paper then introduce an\ninformation-theoretic reliability score (R-score) based on Kullback-Leibler\ndivergence that quantifies annotation confidence and distinguishes between\nrandom guessing and meaningful annotations at the case level. This approach\ncomplements existing expert validation methods by providing a scalable way to\nassess internal annotation reliability and offers practical guidance for prompt\ndesign and downstream analysis.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 01:42:10 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 03:06:47 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['li', 'Linzhuo', '']]","extracted_entities":"[{'text': 'Large\\nLanguage Model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'prompt\\ndesign', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Large\nLanguage Model","similarity_score":1.0}
{"id":2502.19834,"submitter":"Guanzhou Ke","authors":"Guanzhou Ke, Shengfeng He, Xiao Li Wang, Bo Wang, Guoqing Chao,\n  Yuanyang Zhang, Yi Xie, and HeXing Su","title":"Knowledge Bridger: Towards Training-free Missing Multi-modality\n  Completion","comments":"Accepted to CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV cs.MM","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Previous successful approaches to missing modality completion rely on\ncarefully designed fusion techniques and extensive pre-training on complete\ndata, which can limit their generalizability in out-of-domain (OOD) scenarios.\nIn this study, we pose a new challenge: can we develop a missing modality\ncompletion model that is both resource-efficient and robust to OOD\ngeneralization? To address this, we present a training-free framework for\nmissing modality completion that leverages large multimodal models (LMMs). Our\napproach, termed the \"Knowledge Bridger\", is modality-agnostic and integrates\ngeneration and ranking of missing modalities. By defining domain-specific\npriors, our method automatically extracts structured information from available\nmodalities to construct knowledge graphs. These extracted graphs connect the\nmissing modality generation and ranking modules through the LMM, resulting in\nhigh-quality imputations of missing modalities. Experimental results across\nboth general and medical domains show that our approach consistently\noutperforms competing methods, including in OOD generalization. Additionally,\nour knowledge-driven generation and ranking techniques demonstrate superiority\nover variants that directly employ LMMs for generation and ranking, offering\ninsights that may be valuable for applications in other domains.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 07:14:11 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 04:45:52 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 01:45:10 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Ke', 'Guanzhou', ''], ['He', 'Shengfeng', ''], ['Wang', 'Xiao Li', ''], ['Wang', 'Bo', ''], ['Chao', 'Guoqing', ''], ['Zhang', 'Yuanyang', ''], ['Xie', 'Yi', ''], ['Su', 'HeXing', '']]","extracted_entities":"[{'text': 'large multimodal models', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large multimodal models","similarity_score":0.5739125013}
{"id":2502.19844,"submitter":"Xiangyan Qu","authors":"Xiangyan Qu, Gaopeng Gou, Jiamin Zhuang, Jing Yu, Kun Song, Qihao\n  Wang, Yili Li, Gang Xiong","title":"ProAPO: Progressively Automatic Prompt Optimization for Visual\n  Classification","comments":"Accepted to the IEEE\/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Vision-language models (VLMs) have made significant progress in image\nclassification by training with large-scale paired image-text data. Their\nperformances largely depend on the prompt quality. While recent methods show\nthat visual descriptions generated by large language models (LLMs) enhance the\ngeneralization of VLMs, class-specific prompts may be inaccurate or lack\ndiscrimination due to the hallucination in LLMs. In this paper, we aim to find\nvisually discriminative prompts for fine-grained categories with minimal\nsupervision and no human-in-the-loop. An evolution-based algorithm is proposed\nto progressively optimize language prompts from task-specific templates to\nclass-specific descriptions. Unlike optimizing templates, the search space\nshows an explosion in class-specific candidate prompts. This increases prompt\ngeneration costs, iterative times, and the overfitting problem. To this end, we\nfirst introduce several simple yet effective edit-based and evolution-based\noperations to generate diverse candidate prompts by one-time query of LLMs.\nThen, two sampling strategies are proposed to find a better initial search\npoint and reduce traversed categories, saving iteration costs. Moreover, we\napply a novel fitness score with entropy constraints to mitigate overfitting.\nIn a challenging one-shot image classification setting, our method outperforms\nexisting textual prompt-based methods and improves LLM-generated description\nmethods across 13 datasets. Meanwhile, we demonstrate that our optimal prompts\nimprove adapter-based methods and transfer effectively across different\nbackbones.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 07:39:23 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Mar 2025 01:18:01 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 08:56:58 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Qu', 'Xiangyan', ''], ['Gou', 'Gaopeng', ''], ['Zhuang', 'Jiamin', ''], ['Yu', 'Jing', ''], ['Song', 'Kun', ''], ['Wang', 'Qihao', ''], ['Li', 'Yili', ''], ['Xiong', 'Gang', '']]","extracted_entities":"[{'text': 'Vision-language models', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'class-specific prompts', 'label': 'Prompting'}, {'text': 'visually discriminative prompts', 'label': 'Prompting'}, {'text': 'evolution-based algorithm', 'label': 'LLM-based'}, {'text': 'language prompts', 'label': 'Prompting'}, {'text': 'task-specific templates', 'label': 'Embedding'}, {'text': 'class-specific candidate prompts', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2502.19902,"submitter":"Zaijing Li","authors":"Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, Liqiang\n  Nie","title":"Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action\n  Conditioned Policy","comments":"Accept to CVPR 2025, Project page:\n  https:\/\/cybertronagent.github.io\/Optimus-2.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Building an agent that can mimic human behavior patterns to accomplish\nvarious open-world tasks is a long-term goal. To enable agents to effectively\nlearn behavioral patterns across diverse tasks, a key challenge lies in\nmodeling the intricate relationships among observations, actions, and language.\nTo this end, we propose Optimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-level planning, alongside a\nGoal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP\ncontains (1) an Action-guided Behavior Encoder that models causal relationships\nbetween observations and actions at each timestep, then dynamically interacts\nwith the historical observation-action sequence, consolidating it into\nfixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with\nopen-ended language instructions to predict actions auto-regressively.\nMoreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)}\ndataset, which contains 25,000 videos across 8 atomic tasks, providing about\n30M goal-observation-action pairs. The automated construction method, along\nwith the MGOA dataset, can contribute to the community's efforts to train\nMinecraft agents. Extensive experimental results demonstrate that Optimus-2\nexhibits superior performance across atomic tasks, long-horizon tasks, and\nopen-ended instruction tasks in Minecraft. Please see the project page at\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 09:18:04 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 07:51:05 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Li', 'Zaijing', ''], ['Xie', 'Yuquan', ''], ['Shao', 'Rui', ''], ['Chen', 'Gongwei', ''], ['Jiang', 'Dongmei', ''], ['Nie', 'Liqiang', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Model', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Model","similarity_score":0.7924776673}
{"id":2502.1992,"submitter":"Marco Pleines","authors":"Marco Pleines, Daniel Addis, David Rubinstein, Frank Zimmer, Mike\n  Preuss, Peter Whidden","title":"Pokemon Red via Reinforcement Learning","comments":"8 pages, 3 figures, 3 tables, under review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Pok\\'emon Red, a classic Game Boy JRPG, presents significant challenges as a\ntestbed for agents, including multi-tasking, long horizons of tens of thousands\nof steps, hard exploration, and a vast array of potential policies. We\nintroduce a simplistic environment and a Deep Reinforcement Learning (DRL)\ntraining methodology, demonstrating a baseline agent that completes an initial\nsegment of the game up to completing Cerulean City. Our experiments include\nvarious ablations that reveal vulnerabilities in reward shaping, where agents\nexploit specific reward signals. We also discuss limitations and argue that\ngames like Pok\\'emon hold strong potential for future research on Large\nLanguage Model agents, hierarchical training algorithms, and advanced\nexploration methods. Source Code:\nhttps:\/\/github.com\/MarcoMeter\/neroRL\/tree\/poke_red\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 09:42:23 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 05:44:11 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Pleines', 'Marco', ''], ['Addis', 'Daniel', ''], ['Rubinstein', 'David', ''], ['Zimmer', 'Frank', ''], ['Preuss', 'Mike', ''], ['Whidden', 'Peter', '']]","extracted_entities":"[{'text': 'Large\\nLanguage Model', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large\nLanguage Model","similarity_score":1.0}
{"id":2502.19973,"submitter":"Luning Zhang","authors":"Chao Wang, Luning Zhang, Zheng Wang, Yang Zhou","title":"Can Large Language Models Unveil the Mysteries? An Exploration of Their\n  Ability to Unlock Information in Complex Scenarios","comments":"11pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Combining multiple perceptual inputs and performing combinatorial reasoning\nin complex scenarios is a sophisticated cognitive function in humans. With\nadvancements in multi-modal large language models, recent benchmarks tend to\nevaluate visual understanding across multiple images. However, they often\noverlook the necessity of combinatorial reasoning across multiple perceptual\ninformation. To explore the ability of advanced models to integrate multiple\nperceptual inputs for combinatorial reasoning in complex scenarios, we\nintroduce two benchmarks: Clue-Visual Question Answering (CVQA), with three\ntask types to assess visual comprehension and synthesis, and Clue of\nPassword-Visual Question Answering (CPVQA), with two task types focused on\naccurate interpretation and application of visual data. For our benchmarks, we\npresent three plug-and-play approaches: utilizing model input for reasoning,\nenhancing reasoning through minimum margin decoding with randomness generation,\nand retrieving semantically relevant visual information for effective data\nintegration. The combined results reveal current models' poor performance on\ncombinatorial reasoning benchmarks, even the state-of-the-art (SOTA)\nclosed-source model achieves only 33.04% accuracy on CVQA, and drops to 7.38%\non CPVQA. Notably, our approach improves the performance of models on\ncombinatorial reasoning, with a 22.17% boost on CVQA and 9.40% on CPVQA over\nthe SOTA closed-source model, demonstrating its effectiveness in enhancing\ncombinatorial reasoning with multiple perceptual inputs in complex scenarios.\nThe code will be publicly available.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 10:58:27 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 05:35:07 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Wang', 'Chao', ''], ['Zhang', 'Luning', ''], ['Wang', 'Zheng', ''], ['Zhou', 'Yang', '']]","extracted_entities":"[{'text': 'multi-modal large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"multi-modal large language models","similarity_score":0.7925285697}
{"id":2502.20489,"submitter":"Linying Lv","authors":"Linying Lv","title":"Do Sell-side Analyst Reports Have Investment Value?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"q-fin.PR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper documents new investment value in analyst reports. Analyst\nnarratives embedded with large language models strongly forecast future stock\nreturns, generating significant alpha beyond established analyst-based and\nfundamental-based factors. The return predictability arises primarily from\nreports that convey negative sentiment but forecast favorable long-term\nprospects, suggesting systematic market overreaction to near-term negative\nnews. The effect is more pronounced for large, mature firms and for reports\nauthored by skilled, experienced analysts. A Shapley value decomposition\nreveals that analysts' strategic outlook contributes the most to portfolio\nperformance, especially forward-looking discussions on fundamentals. Beyond\ndemonstrating untapped value in qualitative information, this paper illustrates\nthe broader potential of artificial intelligence to augment, rather than\nreplace, expert human judgment in financial markets.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 19:53:59 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 17:02:27 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Lv', 'Linying', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2502.21037,"submitter":"Mingmin Feng","authors":"Eric Hitz, Mingmin Feng, Radu Tanase, Ren\\'e Algesheimer, Manuel S.\n  Mariani","title":"The amplifier effect of artificial agents in social contagion","comments":"Main text pp. 1-4; Supplementary Material pp. 5-10","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SI econ.GN physics.soc-ph q-fin.EC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent advances in artificial intelligence have led to the proliferation of\nartificial agents in social contexts, ranging from education to online social\nmedia and financial markets, among many others. The increasing rate at which\nartificial and human agents interact makes it urgent to understand the\nconsequences of human-machine interactions for the propagation of new ideas,\nproducts, and behaviors in society. Across two distinct empirical contexts, we\nfind here that artificial agents lead to significantly faster and wider social\ncontagion. To this end, we replicate a choice experiment previously conducted\nwith human subjects by using artificial agents powered by large language models\n(LLMs). We use the experiment's results to measure the adoption thresholds of\nartificial agents and their impact on the spread of social contagion. We find\nthat artificial agents tend to exhibit lower adoption thresholds than humans,\nwhich leads to wider network-based social contagions. Our findings suggest that\nthe increased presence of artificial agents in real-world networks may\naccelerate behavioral shifts, potentially in unforeseen ways.\n","versions":"[{'version': 'v1', 'created': 'Fri, 28 Feb 2025 13:29:52 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 13:02:48 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Hitz', 'Eric', ''], ['Feng', 'Mingmin', ''], ['Tanase', 'Radu', ''], ['Algesheimer', 'Ren\u00e9', ''], ['Mariani', 'Manuel S.', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.00399,"submitter":"Lijie Yang","authors":"Juan Song, Lijie Yang, Mingtao Feng","title":"Taming Large Multimodal Agents for Ultra-low Bitrate Semantically\n  Disentangled Image Compression","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  It remains a significant challenge to compress images at ultra-low bitrate\nwhile achieving both semantic consistency and high perceptual quality. We\npropose a novel image compression framework, Semantically Disentangled Image\nCompression (SEDIC) in this paper. Our proposed SEDIC leverages large\nmultimodal models (LMMs) to disentangle the image into several essential\nsemantic information, including an extremely compressed reference image,\noverall and object-level text descriptions, and the semantic masks. A\nmulti-stage semantic decoder is designed to progressively restore the\ntransmitted reference image object-by-object, ultimately producing high-quality\nand perceptually consistent reconstructions. In each decoding stage, a\npre-trained controllable diffusion model is utilized to restore the object\ndetails on the reference image conditioned by the text descriptions and\nsemantic masks. Experimental results demonstrate that SEDIC significantly\noutperforms state-of-the-art approaches, achieving superior perceptual quality\nand semantic consistency at ultra-low bitrates ($\\le$ 0.05 bpp).\n","versions":"[{'version': 'v1', 'created': 'Sat, 1 Mar 2025 08:27:11 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 02:03:22 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Song', 'Juan', ''], ['Yang', 'Lijie', ''], ['Feng', 'Mingtao', '']]","extracted_entities":"[{'text': 'large\\nmultimodal models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large\nmultimodal models","similarity_score":0.5739125013}
{"id":2503.01001,"submitter":"Allen Lin","authors":"Allen Lin, Renqin Cai, Yun He, Hanchao Yu, Jing Qian, Rui Li, Qifan\n  Wang, James Caverlee","title":"Towards An Efficient LLM Training Paradigm for CTR Prediction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) have demonstrated tremendous potential as the\nnext-generation ranking-based recommendation system. Many recent works have\nshown that LLMs can significantly outperform conventional click-through-rate\n(CTR) prediction approaches. Despite such promising results, the computational\ninefficiency inherent in the current training paradigm makes it particularly\nchallenging to train LLMs for ranking-based recommendation tasks on large\ndatasets. To train LLMs for CTR prediction, most existing studies adopt the\nprevalent ''sliding-window'' paradigm. Given a sequence of $m$ user\ninteractions, a unique training prompt is constructed for each interaction by\ndesignating it as the prediction target along with its preceding $n$\ninteractions serving as context. In turn, the sliding-window paradigm results\nin an overall complexity of $O(mn^2)$ that scales linearly with the length of\nuser interactions. Consequently, a direct adoption to train LLMs with such\nstrategy can result in prohibitively high training costs as the length of\ninteractions grows. To alleviate the computational inefficiency, we propose a\nnovel training paradigm, namely Dynamic Target Isolation (DTI), that\nstructurally parallelizes the training of $k$ (where $k >> 1$) target\ninteractions. Furthermore, we identify two major bottlenecks - hidden-state\nleakage and positional bias overfitting - that limit DTI to only scale up to a\nsmall value of $k$ (e.g., 5) then propose a computationally light solution to\neffectively tackle each. Through extensive experiments on three widely adopted\npublic CTR datasets, we empirically show that DTI reduces training time by an\naverage of $\\textbf{92%}$ (e.g., from $70.5$ hrs to $5.31$ hrs), without\ncompromising CTR prediction performance.\n","versions":"[{'version': 'v1', 'created': 'Sun, 2 Mar 2025 19:43:35 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 21:50:37 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Lin', 'Allen', ''], ['Cai', 'Renqin', ''], ['He', 'Yun', ''], ['Yu', 'Hanchao', ''], ['Qian', 'Jing', ''], ['Li', 'Rui', ''], ['Wang', 'Qifan', ''], ['Caverlee', 'James', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'unique training prompt', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.01564,"submitter":"Darya Frolova","authors":"Eli Sason, Darya Frolova, Boris Nazarov, Felix Goldberd","title":"Attention Condensation via Sparsity Induced Regularized Training","comments":"The loss described in the section 3 (pg 4, expression (2)) has an\n  error and needs to be corrected. The experiments should be re-run according\n  to the modified loss. This loss correction doesn't affect the general idea of\n  the paper, and the paper will be resubmitted after the new corrected\n  experimental results are obtained","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  As the context window expands, self-attention increasingly dominates the\ntransformer's inference time. Therefore, accelerating attention computation\nwhile minimizing performance degradation is essential for the efficient\ndeployment of Large Language Models (LLMs). In this study we extend a\ntheoretical framework of attention sparsity in LLMs. A customized loss function\nis designed to enforce the sparsity by restricting the number of top elements\nin the attention matrix. We perform an initial set of evaluations with GPT-2 to\nshow the effectiveness of our sparsification approach. The attention matrices\nof the models trained with the proposed loss are both sparse and effective in\ncapturing relevant input dependencies. We now continue working to demonstrate\nthe value of our approach on larger models and different architectures.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 14:09:13 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 18:12:59 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Sason', 'Eli', ''], ['Frolova', 'Darya', ''], ['Nazarov', 'Boris', ''], ['Goldberd', 'Felix', '']]","extracted_entities":"[{'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'GPT-2', 'label': 'GPT'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.02233,"submitter":"Hang Zheng","authors":"Hang Zheng, Hongshen Xu, Yuncong Liu, Lu Chen, Pascale Fung, Kai Yu","title":"Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models (LLMs) frequently hallucinate due to misaligned\nself-awareness, generating erroneous outputs when addressing queries beyond\ntheir knowledge boundaries. While existing approaches mitigate hallucinations\nvia uncertainty estimation or query rejection, they suffer from computational\ninefficiency or sacrificed helpfulness. To address these issues, we propose the\nExplicit Knowledge Boundary Modeling (EKBM) framework, integrating fast and\nslow reasoning systems to harmonize reliability and usability. The framework\nfirst employs a fast-thinking model to generate confidence-labeled responses,\nenabling immediate use of high-confidence outputs. For uncertain predictions, a\nslow refinement model conducts targeted reasoning to improve accuracy. To align\nmodel behavior with our proposed object, we propose a hybrid training pipeline,\nenhancing self-awareness without degrading task performance. Evaluations on\ndialogue state tracking tasks demonstrate that EKBM achieves superior model\nreliability over uncertainty-based baselines. Further analysis reveals that\nrefinement substantially boosts accuracy while maintaining low computational\noverhead. Our work establishes a scalable paradigm for advancing LLM\nreliability and balancing accuracy and practical utility in error-sensitive\napplications.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 03:16:02 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 07:42:04 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Zheng', 'Hang', ''], ['Xu', 'Hongshen', ''], ['Liu', 'Yuncong', ''], ['Chen', 'Lu', ''], ['Fung', 'Pascale', ''], ['Yu', 'Kai', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'dialogue state tracking tasks', 'label': 'ChatGPT'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.03122,"submitter":"Zichao Li","authors":"Zichao Li, Xueru Wen, Jie Lou, Yuqiu Ji, Yaojie Lu, Xianpei Han,\n  Debing Zhang, Le Sun","title":"The Devil Is in the Details: Tackling Unimodal Spurious Correlations for\n  Generalizable Multimodal Reward Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language\nModels (LLMs) with human preferences, particularly as LLMs increasingly\ninteract with multimodal data. However, we find that MM-RMs trained on existing\ndatasets often struggle to generalize to out-of-distribution data due to their\nreliance on unimodal spurious correlations, primarily text-only shortcuts\nwithin the training distribution, which prevents them from leveraging true\nmultimodal reward functions. To address this, we introduce a Shortcut-aware\nMM-RM learning algorithm that mitigates this issue by dynamically reweighting\ntraining samples, shifting the distribution toward better multimodal\nunderstanding, and reducing dependence on unimodal spurious correlations. Our\nexperiments demonstrate significant improvements in generalization, downstream\ntask performance, and scalability, establishing a more robust framework for\nmultimodal reward modeling.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 02:37:41 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 02:34:53 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Li', 'Zichao', ''], ['Wen', 'Xueru', ''], ['Lou', 'Jie', ''], ['Ji', 'Yuqiu', ''], ['Lu', 'Yaojie', ''], ['Han', 'Xianpei', ''], ['Zhang', 'Debing', ''], ['Sun', 'Le', '']]","extracted_entities":"[{'text': 'Multimodal Reward Models', 'label': 'Large Language Model'}, {'text': 'Large Language\\nModels', 'label': 'Large Language Model'}, {'text': 'scalability', 'label': 'Scaling law'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language\nModels","similarity_score":0.9664971828}
{"id":2503.04299,"submitter":"Henry Papadatos","authors":"Malcolm Murray, Henry Papadatos, Otter Quarks, Pierre-Fran\\c{c}ois\n  Gimenez, Simeon Campos","title":"Mapping AI Benchmark Data to Quantitative Risk Estimates Through Expert\n  Elicitation","comments":"23 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The literature and multiple experts point to many potential risks from large\nlanguage models (LLMs), but there are still very few direct measurements of the\nactual harms posed. AI risk assessment has so far focused on measuring the\nmodels' capabilities, but the capabilities of models are only indicators of\nrisk, not measures of risk. Better modeling and quantification of AI risk\nscenarios can help bridge this disconnect and link the capabilities of LLMs to\ntangible real-world harm. This paper makes an early contribution to this field\nby demonstrating how existing AI benchmarks can be used to facilitate the\ncreation of risk estimates. We describe the results of a pilot study in which\nexperts use information from Cybench, an AI benchmark, to generate probability\nestimates. We show that the methodology seems promising for this purpose, while\nnoting improvements that can be made to further strengthen its application in\nquantitative AI risk assessment.\n","versions":"[{'version': 'v1', 'created': 'Thu, 6 Mar 2025 10:39:47 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 13:00:00 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Murray', 'Malcolm', ''], ['Papadatos', 'Henry', ''], ['Quarks', 'Otter', ''], ['Gimenez', 'Pierre-Fran\u00e7ois', ''], ['Campos', 'Simeon', '']]","extracted_entities":"[{'text': 'large\\nlanguage models', 'label': 'Large Language Model'}, {'text': 'AI risk assessment', 'label': 'AI Ethics'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Cybench', 'label': 'Open-source LLMs'}, {'text': 'AI risk assessment', 'label': 'AI Ethics'}]","assigned_concept":"Large Language Model","matched_keyword":"large\nlanguage models","similarity_score":0.9664971828}
{"id":2503.04773,"submitter":"Bingbing Fan","authors":"Bingbing Fan, Lin Chen, Songwei Li, Jian Yuan, Fengli Xu, Pan Hui,\n  Yong Li","title":"Invisible Walls in Cities: Leveraging Large Language Models to Predict\n  Urban Segregation Experience with Social Media Content","comments":"11 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CY cs.SI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Understanding experienced segregation in urban daily life is crucial for\naddressing societal inequalities and fostering inclusivity. The abundance of\nuser-generated reviews on social media encapsulates nuanced perceptions and\nfeelings associated with different places, offering rich insights into\nsegregation. However, leveraging this data poses significant challenges due to\nits vast volume, ambiguity, and confluence of diverse perspectives. To tackle\nthese challenges, we propose using Large Language Models (LLMs) to automate\nonline review mining for segregation prediction. We design a Reflective LLM\nCoder to digest social media content into insights consistent with real-world\nfeedback, and eventually produce a codebook capturing key dimensions that\nsignal segregation experience, such as cultural resonance and appeal,\naccessibility and convenience, and community engagement and local involvement.\nGuided by the codebook, LLMs can generate both informative review summaries and\nratings for segregation prediction. Moreover, we design a\nREasoning-and-EMbedding (RE'EM) framework, which combines the reasoning and\nembedding capabilities of language models to integrate multi-channel features\nfor segregation prediction. Experiments on real-world data demonstrate that our\nframework greatly improves prediction accuracy, with a 22.79% elevation in R2\nand a 9.33% reduction in MSE. The derived codebook is generalizable across\nthree different cities, consistently improving prediction accuracy. Moreover,\nour user study confirms that the codebook-guided summaries provide cognitive\ngains for human participants in perceiving POIs' social inclusiveness. Our\nstudy marks an important step toward understanding implicit social barriers and\ninequalities, demonstrating the great potential of promoting social\ninclusiveness with AI.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Feb 2025 09:52:17 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 06:15:16 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Fan', 'Bingbing', ''], ['Chen', 'Lin', ''], ['Li', 'Songwei', ''], ['Yuan', 'Jian', ''], ['Xu', 'Fengli', ''], ['Hui', 'Pan', ''], ['Li', 'Yong', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.04779,"submitter":"Thanh Le-Cong Le-Cong Thanh","authors":"Thanh Le-Cong, Bach Le, Toby Murray","title":"Can LLMs Reason About Program Semantics? A Comprehensive Evaluation of\n  LLMs on Formal Specification Inference","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.PL cs.AI cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) are increasingly being used to automate\nprogramming tasks. Yet, LLMs' capabilities in reasoning about program semantics\nare still inadequately studied, leaving significant potential for further\nexploration. This paper introduces FormalBench, a comprehensive benchmark\ndesigned to evaluate LLMs' reasoning abilities on program semantics,\nparticularly via the task of synthesizing formal program specifications to\nassist verifying program correctness. This task requires both comprehensive\nreasoning over all possible program executions and the generation of precise,\nsyntactically correct expressions that adhere to formal syntax and semantics.\nUsing this benchmark, we evaluated the ability of LLMs in synthesizing\nconsistent and complete specifications. Our findings show that LLMs perform\nwell with simple control flows but struggle with more complex structures,\nespecially loops, even with advanced prompting. Additionally, LLMs exhibit\nlimited robustness against semantic-preserving transformations. We also\nhighlight common failure patterns and design self-repair prompts, improving\nsuccess rates by 25%.\n","versions":"[{'version': 'v1', 'created': 'Sat, 22 Feb 2025 13:27:31 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 07:41:37 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Le-Cong', 'Thanh', ''], ['Le', 'Bach', ''], ['Murray', 'Toby', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'advanced prompting', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'design self-repair prompts', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.04784,"submitter":"Jiexiong Liu","authors":"Cheng Li, Jiexiong Liu, Yixuan Chen, Yanqin Jia, Zhepeng Li","title":"KunlunBaize: LLM with Multi-Scale Convolution and Multi-Token Prediction\n  Under TransformerX Framework","comments":"21 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models have demonstrated remarkable performance across various\ntasks, yet they face challenges such as low computational efficiency, gradient\nvanishing, and difficulties in capturing complex feature interactions. To\naddress these limitations, a novel framework has been proposed. This framework\nincorporates a learnable dense residual skip connection mechanism, a\nTransformerX module a transformer based component integrating multiscale\nconvolution and adaptive activation functions and a multitoken prediction\ninteraction module. The learnable dense residual connections enhance\ninformation flow and feature capture across layers. Within the TransformerX\nmodule, large convolutional kernels aggregate semantic information from\nextensive text segments, while smaller convolutions focus on local word order\nand syntactic structures. The adaptive activation function dynamically adjusts\nits parameters based on the semantic features of the input text, improving the\nmodel's ability to handle diverse semantic expressions and complex\nrelationships. The multitoken prediction module boosts data utilization and\naccelerates inference by predicting multiple future tokens. These components\nsignificantly enhance the performance and efficiency of large language models.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 01:56:09 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 01:59:26 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Li', 'Cheng', ''], ['Liu', 'Jiexiong', ''], ['Chen', 'Yixuan', ''], ['Jia', 'Yanqin', ''], ['Li', 'Zhepeng', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.04809,"submitter":"Lang Mei","authors":"Lang Mei, Chong Chen, Jiaxin Mao","title":"PanguIR Technical Report for NTCIR-18 AEOLLM Task","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  As large language models (LLMs) gain widespread attention in both academia\nand industry, it becomes increasingly critical and challenging to effectively\nevaluate their capabilities. Existing evaluation methods can be broadly\ncategorized into two types: manual evaluation and automatic evaluation. Manual\nevaluation, while comprehensive, is often costly and resource-intensive.\nConversely, automatic evaluation offers greater scalability but is constrained\nby the limitations of its evaluation criteria (dominated by reference-based\nanswers). To address these challenges, NTCIR-18 introduced the AEOLLM\n(Automatic Evaluation of LLMs) task, aiming to encourage reference-free\nevaluation methods that can overcome the limitations of existing approaches. In\nthis paper, to enhance the evaluation performance of the AEOLLM task, we\npropose three key methods to improve the reference-free evaluation: 1)\nMulti-model Collaboration: Leveraging multiple LLMs to approximate human\nratings across various subtasks; 2) Prompt Auto-optimization: Utilizing LLMs to\niteratively refine the initial task prompts based on evaluation feedback from\ntraining samples; and 3) In-context Learning (ICL) Optimization: Based on the\nmulti-task evaluation feedback, we train a specialized in-context example\nretrieval model, combined with a semantic relevance retrieval model, to jointly\nidentify the most effective in-context learning examples. Experiments conducted\non the final dataset demonstrate that our approach achieves superior\nperformance on the AEOLLM task.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 07:40:02 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 06:49:01 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Mei', 'Lang', ''], ['Chen', 'Chong', ''], ['Mao', 'Jiaxin', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Prompt Auto-optimization', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'initial task prompts', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.06362,"submitter":"Umberto Cappellazzo","authors":"Umberto Cappellazzo, Minsu Kim, Stavros Petridis","title":"Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal\n  LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.MM cs.SD eess.AS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Audio-Visual Speech Recognition (AVSR) leverages both audio and visual\nmodalities to enhance speech recognition robustness, particularly in noisy\nenvironments. Recent advancements in Large Language Models (LLMs) have\ndemonstrated their effectiveness in speech recognition, including AVSR.\nHowever, due to the significant length of speech representations, direct\nintegration with LLMs imposes substantial computational costs. Prior approaches\naddress this by compressing speech representations before feeding them into\nLLMs. However, higher compression ratios often lead to performance degradation,\nnecessitating a trade-off between computational efficiency and recognition\naccuracy. To address this challenge, we propose Llama-MTSK, the first\nMatryoshka-based Multimodal LLM for AVSR, which enables flexible adaptation of\nthe audio-visual token allocation based on specific computational constraints\nwhile preserving high performance. Our approach, inspired by Matryoshka\nRepresentation Learning, encodes audio-visual representations at multiple\ngranularities within a single model, eliminating the need to train separate\nmodels for different compression levels. Moreover, to efficiently fine-tune the\nLLM, we introduce three LoRA-based Matryoshka strategies using global and\nscale-specific LoRA modules. Extensive evaluations on the two largest AVSR\ndatasets demonstrate that Llama-MTSK achieves state-of-the-art results,\nmatching or surpassing models trained independently at fixed compression\nlevels.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 00:02:10 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Cappellazzo', 'Umberto', ''], ['Kim', 'Minsu', ''], ['Petridis', 'Stavros', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Llama-MTSK', 'label': 'Llama'}, {'text': 'Matryoshka\\nRepresentation Learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.06387,"submitter":"Joseph Wilson","authors":"Anurag Swarnim Yadav and Joseph N. Wilson","title":"R+R: Security Vulnerability Dataset Quality Is Critical","comments":"15 pages, 1 figure, 35 tables. To be published in Proceedings of the\n  2024 Annual Computer Security Applications Conference (ACSAC)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.CR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) are of great interest in vulnerability detection\nand repair. The effectiveness of these models hinges on the quality of the\ndatasets used for both training and evaluation. Our investigation reveals that\na number of studies featured in prominent software engineering conferences have\nemployed datasets that are plagued by high duplication rates, questionable\nlabel accuracy, and incomplete samples. Using these datasets for\nexperimentation will yield incorrect results that are significantly different\nfrom actual expected behavior. For example, the state-of-the-art VulRepair\nModel, which is reported to have 44% accuracy, on average yielded 9% accuracy\nwhen test-set duplicates were removed from its training set and 13% accuracy\nwhen training-set duplicates were removed from its test set. In an effort to\ntackle these data quality concerns, we have retrained models from several\npapers without duplicates and conducted an accuracy assessment of labels for\nthe top ten most hazardous Common Weakness Enumerations (CWEs). Our findings\nindicate that 56% of the samples had incorrect labels and 44% comprised\nincomplete samples--only 31% were both accurate and complete. Finally, we\nemploy transfer learning using a large deduplicated bugfix corpus to show that\nthese models can exhibit better performance if given larger amounts of\nhigh-quality pre-training data, leading us to conclude that while previous\nstudies have over-estimated performance due to poor dataset quality, this does\nnot demonstrate that better performance is not possible.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 01:49:30 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Yadav', 'Anurag Swarnim', ''], ['Wilson', 'Joseph N.', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'transfer learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.06394,"submitter":"Tatsuro Inaba","authors":"Tatsuro Inaba, Kentaro Inui, Yusuke Miyao, Yohei Oseki, Benjamin\n  Heinzerling, Yu Takagi","title":"How LLMs Learn: Tracing Internal Representations with Sparse\n  Autoencoders","comments":"Our code, demo, SAE weights are available at:\n  https:\/\/github.com\/llm-jp\/llm-jp-sae","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) demonstrate remarkable multilingual capabilities\nand broad knowledge. However, the internal mechanisms underlying the\ndevelopment of these capabilities remain poorly understood. To investigate\nthis, we analyze how the information encoded in LLMs' internal representations\nevolves during the training process. Specifically, we train sparse autoencoders\nat multiple checkpoints of the model and systematically compare the\ninterpretative results across these stages. Our findings suggest that LLMs\ninitially acquire language-specific knowledge independently, followed by\ncross-linguistic correspondences. Moreover, we observe that after mastering\ntoken-level knowledge, the model transitions to learning higher-level, abstract\nconcepts, indicating the development of more conceptual understanding.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 02:13:44 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Inaba', 'Tatsuro', ''], ['Inui', 'Kentaro', ''], ['Miyao', 'Yusuke', ''], ['Oseki', 'Yohei', ''], ['Heinzerling', 'Benjamin', ''], ['Takagi', 'Yu', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.0641,"submitter":"Haohan Wang","authors":"Alex Casella, Wayne Wang","title":"Performant LLM Agentic Framework for Conversational AI","comments":"6 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The rise of Agentic applications and automation in the Voice AI industry has\nled to an increased reliance on Large Language Models (LLMs) to navigate\ngraph-based logic workflows composed of nodes and edges. However, existing\nmethods face challenges such as alignment errors in complex workflows and\nhallucinations caused by excessive context size. To address these limitations,\nwe introduce the Performant Agentic Framework (PAF), a novel system that\nassists LLMs in selecting appropriate nodes and executing actions in order when\ntraversing complex graphs. PAF combines LLM-based reasoning with a\nmathematically grounded vector scoring mechanism, achieving both higher\naccuracy and reduced latency. Our approach dynamically balances strict\nadherence to predefined paths with flexible node jumps to handle various user\ninputs efficiently. Experiments demonstrate that PAF significantly outperforms\nbaseline methods, paving the way for scalable, real-time Conversational AI\nsystems in complex business environments.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 02:58:34 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Casella', 'Alex', ''], ['Wang', 'Wayne', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.06421,"submitter":"Weihao Cui","authors":"Weihao Cui, Ziyi Xu, Han Zhao, Quan Chen, Zijun Li, Bingsheng He,\n  Minyi Guo","title":"Efficient Function-as-a-Service for Large Language Models with TIDAL","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.OS","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Model (LLM) applications have emerged as a prominent use case\nfor Function-as-a-Service (FaaS) due to their high computational demands and\nsporadic invocation patterns. However, serving LLM functions within FaaS\nframeworks faces significant GPU-side cold start. A fundamental approach\ninvolves leveraging a template with function state saved on GPUs to bypass the\ncold start for new invocations. Yet, this approach struggles with the high GPU\nfootprint, dynamic initialization behaviors, and lazy GPU kernel loading\ninherent in LLM functions, primarily due to a lack of insight into the\nunderlying execution details. In this paper, we introduce TIDAL, an optimized\nFaaS framework for LLM applications that achieves fast startups by tracing\nfine-grained execution paths. By utilizing the traced execution details, TIDAL\ngenerates adaptive function templates, effectively breaking startup barriers\nfor LLM functions. Extensive evaluations demonstrate that TIDAL reduces cold\nstart latency by $1.79\\times\\text{\\textasciitilde}2.11\\times$ and improves the\n$95\\%$-ile time-to-first-token by $76.0\\%$, surpassing state-of-the-art\nmethods.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 03:33:15 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Cui', 'Weihao', ''], ['Xu', 'Ziyi', ''], ['Zhao', 'Han', ''], ['Chen', 'Quan', ''], ['Li', 'Zijun', ''], ['He', 'Bingsheng', ''], ['Guo', 'Minyi', '']]","extracted_entities":"[{'text': 'Large Language Model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Model","similarity_score":1.0}
{"id":2503.0643,"submitter":"Zhangchi Qiu","authors":"Zhangchi Qiu, Linhao Luo, Zicheng Zhao, Shirui Pan and Alan Wee-Chung\n  Liew","title":"Graph Retrieval-Augmented LLM for Conversational Recommendation Systems","comments":"Accepted by PAKDD 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.IR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Conversational Recommender Systems (CRSs) have emerged as a transformative\nparadigm for offering personalized recommendations through natural language\ndialogue. However, they face challenges with knowledge sparsity, as users often\nprovide brief, incomplete preference statements. While recent methods have\nintegrated external knowledge sources to mitigate this, they still struggle\nwith semantic understanding and complex preference reasoning. Recent Large\nLanguage Models (LLMs) demonstrate promising capabilities in natural language\nunderstanding and reasoning, showing significant potential for CRSs.\nNevertheless, due to the lack of domain knowledge, existing LLM-based CRSs\neither produce hallucinated recommendations or demand expensive domain-specific\ntraining, which largely limits their applicability. In this work, we present\nG-CRS (Graph Retrieval-Augmented Large Language Model for Conversational\nRecommender Systems), a novel training-free framework that combines graph\nretrieval-augmented generation and in-context learning to enhance LLMs'\nrecommendation capabilities. Specifically, G-CRS employs a two-stage\nretrieve-and-recommend architecture, where a GNN-based graph reasoner first\nidentifies candidate items, followed by Personalized PageRank exploration to\njointly discover potential items and similar user interactions. These retrieved\ncontexts are then transformed into structured prompts for LLM reasoning,\nenabling contextually grounded recommendations without task-specific training.\nExtensive experiments on two public datasets show that G-CRS achieves superior\nrecommendation performance compared to existing methods without requiring\ntask-specific training.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 03:56:22 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Qiu', 'Zhangchi', ''], ['Luo', 'Linhao', ''], ['Zhao', 'Zicheng', ''], ['Pan', 'Shirui', ''], ['Liew', 'Alan Wee-Chung', '']]","extracted_entities":"[{'text': 'Recent Large\\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'structured prompts', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Recent Large\nLanguage Models","similarity_score":0.9028562307}
{"id":2503.06463,"submitter":"Tongze Zhang","authors":"Tongze Zhang, Tammy Chung, Anind Dey, Sang Won Bae","title":"AXAI-CDSS : An Affective Explainable AI-Driven Clinical Decision Support\n  System for Cannabis Use","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  As cannabis use has increased in recent years, researchers have come to rely\non sophisticated machine learning models to predict cannabis use behavior and\nits impact on health. However, many artificial intelligence (AI) models lack\ntransparency and interpretability due to their opaque nature, limiting their\ntrust and adoption in real-world medical applications, such as clinical\ndecision support systems (CDSS). To address this issue, this paper enhances\nalgorithm explainability underlying CDSS by integrating multiple Explainable\nArtificial Intelligence (XAI) methods and applying causal inference techniques\nto clarify the model' predictive decisions under various scenarios. By\nproviding deeper interpretability of the XAI outputs using Large Language\nModels (LLMs), we provide users with more personalized and accessible insights\nto overcome the challenges posed by AI's \"black box\" nature. Our system\ndynamically adjusts feedback based on user queries and emotional states,\ncombining text-based sentiment analysis with real-time facial emotion\nrecognition to ensure responses are empathetic, context-adaptive, and\nuser-centered. This approach bridges the gap between the learning demands of\ninterpretability and the need for intuitive understanding, enabling\nnon-technical users such as clinicians and clinical researchers to interact\neffectively with AI models.} Ultimately, this approach improves usability,\nenhances perceived trustworthiness, and increases the impact of CDSS in\nhealthcare applications.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 05:40:44 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhang', 'Tongze', ''], ['Chung', 'Tammy', ''], ['Dey', 'Anind', ''], ['Bae', 'Sang Won', '']]","extracted_entities":"[{'text': 'Large Language\\nModels', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language\nModels","similarity_score":0.9664971828}
{"id":2503.06474,"submitter":"Kong Huanjun","authors":"Huanjun Kong, Zhefan Wang, Chenyang Wang, Zhe Ma, Nanqing Dong","title":"HuixiangDou2: A Robustly Optimized GraphRAG Approach","comments":"11 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) perform well on familiar queries but struggle\nwith specialized or emerging topics. Graph-based Retrieval-Augmented Generation\n(GraphRAG) addresses this by structuring domain knowledge as a graph for\ndynamic retrieval. However, existing pipelines involve complex engineering\nworkflows, making it difficult to isolate the impact of individual components.\nEvaluating retrieval effectiveness is also challenging due to dataset overlap\nwith LLM pretraining data. In this work, we introduce HuixiangDou2, a robustly\noptimized GraphRAG framework. Specifically, we leverage the effectiveness of\ndual-level retrieval and optimize its performance in a 32k context for maximum\nprecision, and compare logic-based retrieval and dual-level retrieval to\nenhance overall functionality. Our implementation includes comparative\nexperiments on a test set, where Qwen2.5-7B-Instruct initially underperformed.\nWith our approach, the score improved significantly from 60 to 74.5, as\nillustrated in the Figure. Experiments on domain-specific datasets reveal that\ndual-level retrieval enhances fuzzy matching, while logic-form retrieval\nimproves structured reasoning. Furthermore, we propose a multi-stage\nverification mechanism to improve retrieval robustness without increasing\ncomputational cost. Empirical results show significant accuracy gains over\nbaselines, highlighting the importance of adaptive retrieval. To support\nresearch and adoption, we release HuixiangDou2 as an open-source resource\nhttps:\/\/github.com\/tpoisonooo\/huixiangdou2.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 06:20:24 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Kong', 'Huanjun', ''], ['Wang', 'Zhefan', ''], ['Wang', 'Chenyang', ''], ['Ma', 'Zhe', ''], ['Dong', 'Nanqing', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GraphRAG', 'label': 'RAG'}, {'text': 'GraphRAG', 'label': 'RAG'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.06486,"submitter":"Cong Chen","authors":"Cong Chen, Mingyu Liu, Chenchen Jing, Yizhou Zhou, Fengyun Rao, Hao\n  Chen, Bo Zhang, Chunhua Shen","title":"PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative\n  Visual Training","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper aims to address the challenge of hallucinations in Multimodal\nLarge Language Models (MLLMs) particularly for dense image captioning tasks. To\ntackle the challenge, we identify the current lack of a metric that finely\nmeasures the caption quality in concept level. We hereby introduce HalFscore, a\nnovel metric built upon the language graph and is designed to evaluate both the\naccuracy and completeness of dense captions at a granular level. Additionally,\nwe identify the root cause of hallucination as the model's over-reliance on its\nlanguage prior. To address this, we propose PerturboLLaVA, which reduces the\nmodel's reliance on the language prior by incorporating adversarially perturbed\ntext during training. This method enhances the model's focus on visual inputs,\neffectively reducing hallucinations and producing accurate, image-grounded\ndescriptions without incurring additional computational overhead. PerturboLLaVA\nsignificantly improves the fidelity of generated captions, outperforming\nexisting approaches in handling multimodal hallucinations and achieving\nimproved performance across general multimodal benchmarks.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 07:07:03 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Chen', 'Cong', ''], ['Liu', 'Mingyu', ''], ['Jing', 'Chenchen', ''], ['Zhou', 'Yizhou', ''], ['Rao', 'Fengyun', ''], ['Chen', 'Hao', ''], ['Zhang', 'Bo', ''], ['Shen', 'Chunhua', '']]","extracted_entities":"[{'text': 'Multimodal\\nLarge Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal\nLarge Language Models","similarity_score":0.7649828196}
{"id":2503.06497,"submitter":"Enming Zhang","authors":"Enming Zhang, Peizhe Gong, Xingyuan Dai, Yisheng Lv, Qinghai Miao","title":"Evaluation of Safety Cognition Capability in Vision-Language Models for\n  Autonomous Driving","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Assessing the safety of vision-language models (VLMs) in autonomous driving\nis particularly important; however, existing work mainly focuses on traditional\nbenchmark evaluations. As interactive components within autonomous driving\nsystems, VLMs must maintain strong safety cognition during interactions. From\nthis perspective, we propose a novel evaluation method: Safety Cognitive\nDriving Benchmark (SCD-Bench) . To address the large-scale annotation challenge\nfor SCD-Bench, we develop the Autonomous Driving Image-Text Annotation System\n(ADA) . Additionally, to ensure data quality in SCD-Bench, our dataset\nundergoes manual refinement by experts with professional knowledge in\nautonomous driving. We further develop an automated evaluation method based on\nlarge language models (LLMs). To verify its effectiveness, we compare its\nevaluation results with those of expert human evaluations, achieving a\nconsistency rate of 99.74%. Preliminary experimental results indicate that\nexisting open-source models still lack sufficient safety cognition, showing a\nsignificant gap compared to GPT-4o. Notably, lightweight models (1B-4B)\ndemonstrate minimal safety cognition. However, since lightweight models are\ncrucial for autonomous driving systems, this presents a significant challenge\nfor integrating VLMs into the field.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 07:53:19 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhang', 'Enming', ''], ['Gong', 'Peizhe', ''], ['Dai', 'Xingyuan', ''], ['Lv', 'Yisheng', ''], ['Miao', 'Qinghai', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.06519,"submitter":"Wenhui Zhang","authors":"Wenhui Zhang, Huiyu Xu, Zhibo Wang, Zeqing He, Ziqi Zhu, Kui Ren","title":"Can Small Language Models Reliably Resist Jailbreak Attacks? A\n  Comprehensive Evaluation","comments":"19 pages, 12 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Small language models (SLMs) have emerged as promising alternatives to large\nlanguage models (LLMs) due to their low computational demands, enhanced privacy\nguarantees and comparable performance in specific domains through light-weight\nfine-tuning. Deploying SLMs on edge devices, such as smartphones and smart\nvehicles, has become a growing trend. However, the security implications of\nSLMs have received less attention than LLMs, particularly regarding jailbreak\nattacks, which is recognized as one of the top threats of LLMs by the OWASP. In\nthis paper, we conduct the first large-scale empirical study of SLMs'\nvulnerabilities to jailbreak attacks. Through systematically evaluation on 63\nSLMs from 15 mainstream SLM families against 8 state-of-the-art jailbreak\nmethods, we demonstrate that 47.6% of evaluated SLMs show high susceptibility\nto jailbreak attacks (ASR > 40%) and 38.1% of them can not even resist direct\nharmful query (ASR > 50%). We further analyze the reasons behind the\nvulnerabilities and identify four key factors: model size, model architecture,\ntraining datasets and training techniques. Moreover, we assess the\neffectiveness of three prompt-level defense methods and find that none of them\nachieve perfect performance, with detection accuracy varying across different\nSLMs and attack methods. Notably, we point out that the inherent security\nawareness play a critical role in SLM security, and models with strong security\nawareness could timely terminate unsafe response with little reminder. Building\nupon the findings, we highlight the urgent need for security-by-design\napproaches in SLM development and provide valuable insights for building more\ntrustworthy SLM ecosystem.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 08:47:16 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhang', 'Wenhui', ''], ['Xu', 'Huiyu', ''], ['Wang', 'Zhibo', ''], ['He', 'Zeqing', ''], ['Zhu', 'Ziqi', ''], ['Ren', 'Kui', '']]","extracted_entities":"[{'text': 'large\\nlanguage models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'light-weight\\nfine-tuning', 'label': 'Fine-tuning'}, {'text': 'SLMs', 'label': 'Large Language Model'}, {'text': 'SLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'SLMs', 'label': 'Large Language Model'}, {'text': 'SLMs', 'label': 'Large Language Model'}, {'text': 'SLMs', 'label': 'Large Language Model'}, {'text': 'prompt-level defense methods', 'label': 'Prompting'}, {'text': 'SLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large\nlanguage models","similarity_score":0.9664971828}
{"id":2503.06525,"submitter":"Xian Gao","authors":"Xian Gao, Jiacheng Ruan, Jingsheng Gao, Mingye Xie, Zongyun Zhang,\n  Ting Liu and Yuzhuo Fu","title":"From Motion Signals to Insights: A Unified Framework for Student\n  Behavior Analysis and Feedback in Physical Education Classes","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Analyzing student behavior in educational scenarios is crucial for enhancing\nteaching quality and student engagement. Existing AI-based models often rely on\nclassroom video footage to identify and analyze student behavior. While these\nvideo-based methods can partially capture and analyze student actions, they\nstruggle to accurately track each student's actions in physical education\nclasses, which take place in outdoor, open spaces with diverse activities, and\nare challenging to generalize to the specialized technical movements involved\nin these settings. Furthermore, current methods typically lack the ability to\nintegrate specialized pedagogical knowledge, limiting their ability to provide\nin-depth insights into student behavior and offer feedback for optimizing\ninstructional design. To address these limitations, we propose a unified\nend-to-end framework that leverages human activity recognition technologies\nbased on motion signals, combined with advanced large language models, to\nconduct more detailed analyses and feedback of student behavior in physical\neducation classes. Our framework begins with the teacher's instructional\ndesigns and the motion signals from students during physical education\nsessions, ultimately generating automated reports with teaching insights and\nsuggestions for improving both learning and class instructions. This solution\nprovides a motion signal-based approach for analyzing student behavior and\noptimizing instructional design tailored to physical education classes.\nExperimental results demonstrate that our framework can accurately identify\nstudent behaviors and produce meaningful pedagogical insights.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 09:04:36 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Gao', 'Xian', ''], ['Ruan', 'Jiacheng', ''], ['Gao', 'Jingsheng', ''], ['Xie', 'Mingye', ''], ['Zhang', 'Zongyun', ''], ['Liu', 'Ting', ''], ['Fu', 'Yuzhuo', '']]","extracted_entities":"[{'text': 'advanced large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"advanced large language models","similarity_score":0.9208456278}
{"id":2503.06534,"submitter":"Xingwei Tan","authors":"Xingwei Tan, Chen Lyu, Hafiz Muhammad Umer, Sahrish Khan, Mahathi\n  Parvatham, Lois Arthurs, Simon Cullen, Shelley Wilson, Arshad Jhumka,\n  Gabriele Pergola","title":"SafeSpeech: A Comprehensive and Interactive Tool for Analysing Sexist\n  and Abusive Language in Conversations","comments":"NAACL 2025 system demonstration camera-ready","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Detecting toxic language including sexism, harassment and abusive behaviour,\nremains a critical challenge, particularly in its subtle and context-dependent\nforms. Existing approaches largely focus on isolated message-level\nclassification, overlooking toxicity that emerges across conversational\ncontexts. To promote and enable future research in this direction, we introduce\nSafeSpeech, a comprehensive platform for toxic content detection and analysis\nthat bridges message-level and conversation-level insights. The platform\nintegrates fine-tuned classifiers and large language models (LLMs) to enable\nmulti-granularity detection, toxic-aware conversation summarization, and\npersona profiling. SafeSpeech also incorporates explainability mechanisms, such\nas perplexity gain analysis, to highlight the linguistic elements driving\npredictions. Evaluations on benchmark datasets, including EDOS, OffensEval, and\nHatEval, demonstrate the reproduction of state-of-the-art performance across\nmultiple tasks, including fine-grained sexism detection.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 09:31:17 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Tan', 'Xingwei', ''], ['Lyu', 'Chen', ''], ['Umer', 'Hafiz Muhammad', ''], ['Khan', 'Sahrish', ''], ['Parvatham', 'Mahathi', ''], ['Arthurs', 'Lois', ''], ['Cullen', 'Simon', ''], ['Wilson', 'Shelley', ''], ['Jhumka', 'Arshad', ''], ['Pergola', 'Gabriele', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'toxic-aware conversation summarization', 'label': 'Knowledge distillation'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.0655,"submitter":"Fan Yin","authors":"Fan Yin and Philippe Laban and Xiangyu Peng and Yilun Zhou and Yixin\n  Mao and Vaibhav Vats and Linnea Ross and Divyansh Agarwal and Caiming Xiong\n  and Chien-Sheng Wu","title":"BingoGuard: LLM Content Moderation Tools with Risk Levels","comments":"10 pages, 4 figures, 4 tables. ICLR 2025 poster","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Malicious content generated by large language models (LLMs) can pose varying\ndegrees of harm. Although existing LLM-based moderators can detect harmful\ncontent, they struggle to assess risk levels and may miss lower-risk outputs.\nAccurate risk assessment allows platforms with different safety thresholds to\ntailor content filtering and rejection. In this paper, we introduce per-topic\nseverity rubrics for 11 harmful topics and build BingoGuard, an LLM-based\nmoderation system designed to predict both binary safety labels and severity\nlevels. To address the lack of annotations on levels of severity, we propose a\nscalable generate-then-filter framework that first generates responses across\ndifferent severity levels and then filters out low-quality responses. Using\nthis framework, we create BingoGuardTrain, a training dataset with 54,897\nexamples covering a variety of topics, response severity, styles, and\nBingoGuardTest, a test set with 988 examples explicitly labeled based on our\nseverity rubrics that enables fine-grained analysis on model behaviors on\ndifferent severity levels. Our BingoGuard-8B, trained on BingoGuardTrain,\nachieves the state-of-the-art performance on several moderation benchmarks,\nincluding WildGuardTest and HarmBench, as well as BingoGuardTest, outperforming\nbest public models, WildGuard, by 4.3\\%. Our analysis demonstrates that\nincorporating severity levels into training significantly enhances detection\nperformance and enables the model to effectively gauge the severity of harmful\nresponses.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 10:43:09 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Yin', 'Fan', ''], ['Laban', 'Philippe', ''], ['Peng', 'Xiangyu', ''], ['Zhou', 'Yilun', ''], ['Mao', 'Yixin', ''], ['Vats', 'Vaibhav', ''], ['Ross', 'Linnea', ''], ['Agarwal', 'Divyansh', ''], ['Xiong', 'Caiming', ''], ['Wu', 'Chien-Sheng', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.0662,"submitter":"Xiangyu Zhang","authors":"Xiangyu Zhang, Beena Ahmed, Julien Epps","title":"Why Pre-trained Models Fail: Feature Entanglement in Multi-modal\n  Depression Detection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.AS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Depression remains a pressing global mental health issue, driving\nconsiderable research into AI-driven detection approaches. While pre-trained\nmodels, particularly speech self-supervised models (SSL Models), have been\napplied to depression detection, they show unexpectedly poor performance\nwithout extensive data augmentation. Large Language Models (LLMs), despite\ntheir success across various domains, have not been explored in multi-modal\ndepression detection. In this paper, we first establish an LLM-based system to\ninvestigate its potential in this task, uncovering fundamental limitations in\nhandling multi-modal information. Through systematic analysis, we discover that\nthe poor performance of pre-trained models stems from the conflation of\nhigh-level information, where high-level features derived from both content and\nspeech are mixed within pre-trained models model representations, making it\nchallenging to establish effective decision boundaries. To address this, we\npropose an information separation framework that disentangles these features,\nsignificantly improving the performance of both SSL models and LLMs in\ndepression detection. Our experiments validate this finding and demonstrate\nthat the integration of separated features yields substantial improvements over\nexisting approaches, providing new insights for developing more effective\nmulti-modal depression detection systems.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 13:45:21 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhang', 'Xiangyu', ''], ['Ahmed', 'Beena', ''], ['Epps', 'Julien', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.06626,"submitter":"Hasan Abed Al Kader Hammoud","authors":"Hasan Abed Al Kader Hammoud, Bernard Ghanem","title":"DiffCLIP: Differential Attention Meets CLIP","comments":"Under review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We propose DiffCLIP, a novel vision-language model that extends the\ndifferential attention mechanism to CLIP architectures. Differential attention\nwas originally developed for large language models to amplify relevant context\nwhile canceling out noisy information. In this work, we integrate this\nmechanism into CLIP's dual encoder (image and text) framework. With minimal\nadditional parameters, DiffCLIP achieves superior performance on image-text\nunderstanding tasks. Across zero-shot classification, retrieval, and robustness\nbenchmarks, DiffCLIP consistently outperforms baseline CLIP models. Notably,\nthese gains come with negligible computational overhead, demonstrating that\ndifferential attention can significantly enhance multi-modal representations\nwithout sacrificing efficiency. Code can be found at\nhttps:\/\/github.com\/hammoudhasan\/DiffCLIP.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 14:04:09 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Hammoud', 'Hasan Abed Al Kader', ''], ['Ghanem', 'Bernard', '']]","extracted_entities":"[{'text': 'differential attention mechanism', 'label': 'Attention mechanism'}, {'text': 'Differential attention', 'label': 'Attention mechanism'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'zero-shot classification', 'label': 'Few-shot Learning'}, {'text': 'differential attention', 'label': 'Attention mechanism'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.06646,"submitter":"Jiaxin Liu","authors":"Jiaxin Liu, Yi Yang, Kar Yan Tam","title":"Evaluating and Aligning Human Economic Risk Preferences in LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"econ.GN cs.CL q-fin.EC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) are increasingly used in decision-making\nscenarios that involve risk assessment, yet their alignment with human economic\nrationality remains unclear. In this study, we investigate whether LLMs exhibit\nrisk preferences consistent with human expectations across different personas.\nSpecifically, we assess whether LLM-generated responses reflect appropriate\nlevels of risk aversion or risk-seeking behavior based on individual's persona.\nOur results reveal that while LLMs make reasonable decisions in simplified,\npersonalized risk contexts, their performance declines in more complex economic\ndecision-making tasks. To address this, we propose an alignment method designed\nto enhance LLM adherence to persona-specific risk preferences. Our approach\nimproves the economic rationality of LLMs in risk-related applications,\noffering a step toward more human-aligned AI decision-making.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 14:47:31 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Liu', 'Jiaxin', ''], ['Yang', 'Yi', ''], ['Tam', 'Kar Yan', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2411.13901,"submitter":"Sparsh Mittal","authors":"Gayatri Deshmukh, Somsubhra De, Chirag Sehgal, Jishu Sen Gupta, Sparsh\n  Mittal","title":"Dressing the Imagination: A Dataset for AI-Powered Translation of Text\n  into Fashion Outfits and A Novel KAN Adapter for Enhanced Feature Adaptation","comments":"Under review at a conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Specialized datasets that capture the fashion industry's rich language and\nstyling elements can boost progress in AI-driven fashion design. We present\nFLORA (Fashion Language Outfit Representation for Apparel Generation), the\nfirst comprehensive dataset containing 4,330 curated pairs of fashion outfits\nand corresponding textual descriptions. Each description utilizes\nindustry-specific terminology and jargon commonly used by professional fashion\ndesigners, providing precise and detailed insights into the outfits. Hence, the\ndataset captures the delicate features and subtle stylistic elements necessary\nto create high-fidelity fashion designs. We demonstrate that fine-tuning\ngenerative models on the FLORA dataset significantly enhances their capability\nto generate accurate and stylistically rich images from textual descriptions of\nfashion sketches. FLORA will catalyze the creation of advanced AI models\ncapable of comprehending and producing subtle, stylistically rich fashion\ndesigns. It will also help fashion designers and end-users to bring their ideas\nto life.\n  As a second orthogonal contribution, we introduce KAN Adapters, which\nleverage Kolmogorov-Arnold Networks (KAN) as adaptive modules. They serve as\nreplacements for traditional MLP-based LoRA adapters. With learnable\nspline-based activations, KAN Adapters excel in modeling complex, non-linear\nrelationships, achieving superior fidelity, faster convergence and semantic\nalignment. Extensive experiments and ablation studies on our proposed FLORA\ndataset validate the superiority of KAN Adapters over LoRA adapters. To foster\nfurther research and collaboration, we will open-source both the FLORA and our\nimplementation code.\n","versions":"[{'version': 'v1', 'created': 'Thu, 21 Nov 2024 07:27:45 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 09:55:48 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Deshmukh', 'Gayatri', ''], ['De', 'Somsubhra', ''], ['Sehgal', 'Chirag', ''], ['Gupta', 'Jishu Sen', ''], ['Mittal', 'Sparsh', '']]","extracted_entities":"[{'text': 'FLORA', 'label': 'Large Language Model'}, {'text': 'FLORA', 'label': 'Mistral'}, {'text': 'FLORA', 'label': 'Mistral'}, {'text': 'FLORA', 'label': 'Mistral'}, {'text': 'open-source', 'label': 'Open-source LLMs'}, {'text': 'FLORA', 'label': 'Mistral'}]","assigned_concept":"Open-source LLMs","matched_keyword":"open-source","similarity_score":0.5279436111}
{"id":2310.08849,"submitter":"Md. Tanzib Hosain","authors":"Md. Tanzib Hosain, Mehedi Hasan Anik, Sadman Rafi, Rana Tabassum,\n  Khaleque Insia, Md. Mehrab Siddiky","title":"Path To Gain Functional Transparency In Artificial Intelligence With\n  Meaningful Explainability","comments":"Hosain, M. T., Anik, M. H., Rafi, S., Tabassum, R., Insia, K., &\n  S{\\i}dd{\\i}ky, M. M. (2023). Path to gain functional transparency in\n  artificial intelligence with meaningful explainability. Journal of Metaverse,\n  3(2), 166-180","journal-ref":"Journal of Metaverse, 3(2), 166-180 (2023)","doi":"10.57019\/jmv.1306685","report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Artificial Intelligence (AI) is rapidly integrating into various aspects of\nour daily lives, influencing decision-making processes in areas such as\ntargeted advertising and matchmaking algorithms. As AI systems become\nincreasingly sophisticated, ensuring their transparency and explainability\nbecomes crucial. Functional transparency is a fundamental aspect of algorithmic\ndecision-making systems, allowing stakeholders to comprehend the inner workings\nof these systems and enabling them to evaluate their fairness and accuracy.\nHowever, achieving functional transparency poses significant challenges that\nneed to be addressed. In this paper, we propose a design for user-centered\ncompliant-by-design transparency in transparent systems. We emphasize that the\ndevelopment of transparent and explainable AI systems is a complex and\nmultidisciplinary endeavor, necessitating collaboration among researchers from\ndiverse fields such as computer science, artificial intelligence, ethics, law,\nand social science. By providing a comprehensive understanding of the\nchallenges associated with transparency in AI systems and proposing a\nuser-centered design framework, we aim to facilitate the development of AI\nsystems that are accountable, trustworthy, and aligned with societal values.\n","versions":"[{'version': 'v1', 'created': 'Fri, 13 Oct 2023 04:25:30 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 10:34:16 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Hosain', 'Md. Tanzib', ''], ['Anik', 'Mehedi Hasan', ''], ['Rafi', 'Sadman', ''], ['Tabassum', 'Rana', ''], ['Insia', 'Khaleque', ''], ['Siddiky', 'Md. Mehrab', '']]","extracted_entities":"[{'text': 'ethics', 'label': 'AI Ethics'}, {'text': 'law', 'label': 'AI Ethics'}]","assigned_concept":"AI Ethics","matched_keyword":"ethics","similarity_score":0.716448009}
{"id":2411.16531,"submitter":"Bahman Rostami-Tabar","authors":"Bahman Rostami-Tabar, Travis Greene, Galit Shmueli, and Rob J. Hyndman","title":"Good intentions, unintended consequences: exploring forecasting harms","comments":"42 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.OT","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Organizations worldwide that rely on data-driven approaches regularly employ\nforecasting methods to enhance their planning and decision-making processes.\nWhile extensive research has examined the harms associated with traditional\nmachine learning applications, relatively little attention has been given to\nthe ethical implications of time series forecasting. However, forecasting\npresents distinct ethical challenges due to its diverse organizational\napplications, varied objectives, and unique data processing, model development,\nand evaluation workflows. These distinctions complicate the direct application\nof existing machine learning harm taxonomies to common forecasting scenarios.\nTo address this gap, we conduct multiple interviews with industry experts and\nacademic researchers, systematically identifying and analyzing underexplored\ndomains, use cases, and potential risks associated with forecasting. Our\nobjective is to develop a novel taxonomy of forecasting-specific harms. Drawing\ninspiration from Microsoft Azure taxonomy for responsible innovation, we\nintegrate a human-led inductive coding approach with AI-driven analysis to\nextract key categories of harm in forecasting. This taxonomy aims to support\nresearchers and practitioners by fostering ethical reflection on their\ndecision-making throughout the forecasting process. Additionally, we seek to\nestablish a research agenda focused on identifying measures to mitigate\npotential harms in forecasting. By highlighting unique risks within\nforecasting, our work contributes to the broader discourse on machine learning\nethics.\n","versions":"[{'version': 'v1', 'created': 'Mon, 25 Nov 2024 16:18:02 GMT'}, {'version': 'v2', 'created': 'Thu, 16 Jan 2025 10:14:39 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 19:17:45 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Rostami-Tabar', 'Bahman', ''], ['Greene', 'Travis', ''], ['Shmueli', 'Galit', ''], ['Hyndman', 'Rob J.', '']]","extracted_entities":"[{'text': 'machine learning\\nethics', 'label': 'AI Ethics'}]","assigned_concept":"AI Ethics","matched_keyword":"machine learning\nethics","similarity_score":0.7529441118}
{"id":2412.16238,"submitter":"Andr\\'es Corrada-Emmanuel","authors":"Andr\\'es Corrada-Emmanuel","title":"Algebraic Evaluation Theorems","comments":"28 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Majority voting (MV) is the prototypical ``wisdom of the crowd'' algorithm.\nTheorems considering when MV is optimal for group decisions date back to\nCondorcet's 1785 jury \\emph{decision} theorem. The same error independence\nassumption underlying the theorem can be used to prove a jury \\emph{evaluation}\ntheorem that does purely algebraic evaluation (AE) of juror performance based\non a batch of their decisions. Three or more binary jurors are enough to obtain\nthe only two possible statistics of their correctness on a test they took. AE\nis superior to MV in three ways. First, its empirical assumptions are looser\nand can handle jurors less than 50\\% accurate in making decisions. Second, it\nhas point-like precision in evaluating them given its assumption of error\nindependence. This precision enables a multi-accuracy approach that has higher\nlabeling accuracy than MV and comes with empirical uncertainty bounds. And,\nthird, it is self-alarming about the failure of its error independence\nassumption. Experiments using demographic data from the American Community\nSurvey confirm the practical utility of AE over MV. Two implications of the\ntheorem for AI safety are discussed - a principled way to terminate infinite\nmonitoring chains (who grades the graders?) and the super-alignment problem\n(how do we evaluate agents doing tasks we do not understand?).\n","versions":"[{'version': 'v1', 'created': 'Thu, 19 Dec 2024 13:01:21 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 16:31:39 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Corrada-Emmanuel', 'Andr\u00e9s', '']]","extracted_entities":"[{'text': 'AI safety', 'label': 'AI Ethics'}, {'text': 'infinite\\nmonitoring chains', 'label': 'AI Ethics'}]","assigned_concept":"AI Ethics","matched_keyword":"AI safety","similarity_score":0.6269190311}
{"id":2412.16594,"submitter":"Mucahid Kutlu","authors":"Basak Demirok, Mucahid Kutlu","title":"AIGCodeSet: A New Annotated Dataset for AI Generated Code Detection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While large language models provide significant convenience for software\ndevelopment, they can lead to ethical issues in job interviews and student\nassignments. Therefore, determining whether a piece of code is written by a\nhuman or generated by an artificial intelligence (AI) model is a critical\nissue. In this study, we present AIGCodeSet, which consists of 2.828\nAI-generated and 4.755 human-written Python codes, created using CodeLlama 34B,\nCodestral 22B, and Gemini 1.5 Flash. In addition, we share the results of our\nexperiments conducted with baseline detection methods. Our experiments show\nthat a Bayesian classifier outperforms the other models.\n","versions":"[{'version': 'v1', 'created': 'Sat, 21 Dec 2024 11:53:49 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 10:31:29 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Demirok', 'Basak', ''], ['Kutlu', 'Mucahid', '']]","extracted_entities":"[{'text': 'ethical issues', 'label': 'AI Ethics'}, {'text': 'CodeLlama 34B', 'label': 'Llama'}, {'text': 'Codestral 22B', 'label': 'Llama'}]","assigned_concept":"AI Ethics","matched_keyword":"ethical issues","similarity_score":0.5417473316}
{"id":2502.07347,"submitter":"Takashi Izumo","authors":"Takashi Izumo","title":"Coarse Set Theory for AI Ethics and Decision-Making: A Mathematical\n  Framework for Granular Evaluations","comments":"28 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.IT math.IT math.LO math.PR","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  In artificial intelligence (AI) and decision-making systems, structured\napproximations play a crucial role in balancing model interpretability and\npredictive accuracy. Coarse Set Theory (CST) introduces a mathematical\nframework to formalize Coarse Ethics (CE), which models coarse-grained\ndecision-making processes commonly used in human evaluations and AI\nclassification systems. CST defines hierarchical relationships among sets using\ntotally ordered structures and coarse mappings, enabling us to adjust decision\ngranularity dynamically. Furthermore, coarse evaluations inherently involve a\ntrade-off between efficiency and information retention, as they simplify\ncomplex data representations at the cost of precision. To quantitatively assess\nthis trade-off, we introduce Kullback-Leibler (KL) Divergence as a measure of\ninformation loss in coarse evaluations, demonstrating the impact of coarse\npartitioning on decision accuracy. This study employs CST in grading systems,\nautomated recommendations, and risk assessments, demonstrating its potential to\nenhance fairness, reduce bias, and improve transparency in AI-driven\ndecision-making.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Feb 2025 08:18:37 GMT'}, {'version': 'v2', 'created': 'Sun, 23 Feb 2025 05:41:53 GMT'}, {'version': 'v3', 'created': 'Thu, 6 Mar 2025 23:24:47 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Mar 2025 10:04:26 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Izumo', 'Takashi', '']]","extracted_entities":"[{'text': 'Coarse Ethics', 'label': 'AI Ethics'}, {'text': 'fairness', 'label': 'AI Ethics'}]","assigned_concept":"AI Ethics","matched_keyword":"Coarse Ethics","similarity_score":0.5616875887}
{"id":2503.04804,"submitter":"Arturs Kanepajs","authors":"Arturs Kanepajs, Aditi Basu, Sankalpa Ghose, Constance Li, Akshat\n  Mehta, Ronak Mehta, Samuel David Tucker-Davis, Eric Zhou, Bob Fischer","title":"What do Large Language Models Say About Animals? Investigating Risks of\n  Animal Harm in Generated Text","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  As machine learning systems become increasingly embedded in human society,\ntheir impact on the natural world continues to escalate. Technical evaluations\nhave addressed a variety of potential harms from large language models (LLMs)\ntowards humans and the environment, but there is little empirical work\nregarding harms towards nonhuman animals. Following the growing recognition of\nanimal protection in regulatory and ethical AI frameworks, we present the\nAnimal Harm Assessment (AHA), a novel evaluation of risks of animal harm in\nLLM-generated text. Our dataset comprises 1,850 curated questions from Reddit\npost titles and 2,500 synthetic questions based on 50 animal categories (e.g.,\ncats, reptiles) and 50 ethical scenarios, with further 70-30 public-private\nsplit. Scenarios include open-ended questions about how to treat animals,\npractical scenarios with potential animal harm, and willingness-to-pay measures\nfor the prevention of animal harm. Using the LLM-as-a-judge framework, answers\nare evaluated for their potential to increase or decrease harm, and evaluations\nare debiased for the tendency to judge their own outputs more favorably. We\nshow that AHA produces meaningful evaluation results when applied to frontier\nLLMs, revealing significant differences between models, animal categories,\nscenarios, and subreddits. We conclude with future directions for technical\nresearch and the challenges of building evaluations on complex social and moral\ntopics.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 15:32:18 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 03:02:59 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Kanepajs', 'Arturs', ''], ['Basu', 'Aditi', ''], ['Ghose', 'Sankalpa', ''], ['Li', 'Constance', ''], ['Mehta', 'Akshat', ''], ['Mehta', 'Ronak', ''], ['Tucker-Davis', 'Samuel David', ''], ['Zhou', 'Eric', ''], ['Fischer', 'Bob', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'regulatory and ethical AI frameworks', 'label': 'AI Ethics'}]","assigned_concept":"AI Ethics","matched_keyword":"regulatory and ethical AI frameworks","similarity_score":0.752835393}
{"id":2503.06411,"submitter":"Krti Tallam","authors":"Krti Tallam","title":"Decoding the Black Box: Integrating Moral Imagination with Technical AI\n  Governance","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.AI cs.SY","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper examines the intricate interplay among AI safety, security, and\ngovernance by integrating technical systems engineering with principles of\nmoral imagination and ethical philosophy. Drawing on foundational insights from\nWeapons of Math Destruction and Thinking in Systems alongside contemporary\ndebates in AI ethics, we develop a comprehensive multi-dimensional framework\ndesigned to regulate AI technologies deployed in high-stakes domains such as\ndefense, finance, healthcare, and education. Our approach combines rigorous\ntechnical analysis, quantitative risk assessment, and normative evaluation to\nexpose systemic vulnerabilities inherent in opaque, black-box models. Detailed\ncase studies, including analyses of Microsoft Tay (2016) and the UK A-Level\nGrading Algorithm (2020), demonstrate how security lapses, bias amplification,\nand lack of accountability can precipitate cascading failures that undermine\npublic trust. We conclude by outlining targeted strategies for enhancing AI\nresilience through adaptive regulatory mechanisms, robust security protocols,\nand interdisciplinary oversight, thereby advancing the state of the art in\nethical and technical AI governance.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 03:11:32 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Tallam', 'Krti', '']]","extracted_entities":"[{'text': 'AI ethics', 'label': 'AI Ethics'}, {'text': 'security lapses', 'label': 'Model Bias and Fairness'}, {'text': 'bias amplification', 'label': 'Model Bias and Fairness'}]","assigned_concept":"AI Ethics","matched_keyword":"AI ethics","similarity_score":1.0}
{"id":2306.16965,"submitter":"Ren\\'e Romen","authors":"Martin Bullinger and Ren\\'e Romen","title":"Online Coalition Formation under Random Arrival or Coalition Dissolution","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GT cs.DS","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Coalition formation explores how to partition a set of $n$ agents into\ndisjoint coalitions according to their preferences. We consider a cardinal\nutility model with an additively separable aggregation of preferences and study\nthe online variant of coalition formation, where the agents arrive in sequence.\nThe goal is to achieve competitive social welfare. In the basic model, agents\narrive in an arbitrary order and have to be assigned to coalitions immediately\nand irrevocably. There, the natural greedy algorithm is known to achieve an\noptimal competitive ratio, which heavily relies on the range of utilities.\n  We complement this result by considering two related models. First, we study\na model where agents arrive in a random order. We find that the competitive\nratio of the greedy algorithm is $\\Theta\\left(\\frac{1}{n^2}\\right)$. In\ncontrast, an alternative algorithm, which is based on alternating between\nwaiting and greedy phases, can achieve a competitive ratio of\n$\\Theta\\left(\\frac{1}{n}\\right)$. Second, we relax the irrevocability of\ndecisions by allowing the dissolution of coalitions into singleton coalitions.\nWe achieve an asymptotically optimal competitive ratio of $\\Theta\\left(\\frac\n1n\\right)$ by drawing a close connection to a general model of online matching.\nHence, in both models, we obtain a competitive ratio that removes the\nunavoidable utility dependencies in the basic model and essentially matches the\nbest possible approximation ratio by polynomial-time algorithms.\n","versions":"[{'version': 'v1', 'created': 'Thu, 29 Jun 2023 14:14:52 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 10:14:31 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Bullinger', 'Martin', ''], ['Romen', 'Ren\u00e9', '']]","extracted_entities":"[{'text': 'basic model', 'label': 'Foundation Model'}, {'text': 'basic model', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"basic model","similarity_score":0.5132901669}
{"id":2403.09616,"submitter":"Chaoyang Wang","authors":"Chaoyang Wang, Xiangtai Li, Henghui Ding, Lu Qi, Jiangning Zhang,\n  Yunhai Tong, Chen Change Loy, Shuicheng Yan","title":"Explore In-Context Segmentation via Latent Diffusion Models","comments":"AAAI 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In-context segmentation has drawn increasing attention with the advent of\nvision foundation models. Its goal is to segment objects using given reference\nimages. Most existing approaches adopt metric learning or masked image modeling\nto build the correlation between visual prompts and input image queries. This\nwork approaches the problem from a fresh perspective - unlocking the capability\nof the latent diffusion model (LDM) for in-context segmentation and\ninvestigating different design choices. Specifically, we examine the problem\nfrom three angles: instruction extraction, output alignment, and\nmeta-architectures. We design a two-stage masking strategy to prevent\ninterfering information from leaking into the instructions. In addition, we\npropose an augmented pseudo-masking target to ensure the model predicts without\nforgetting the original images. Moreover, we build a new and fair in-context\nsegmentation benchmark that covers both image and video datasets. Experiments\nvalidate the effectiveness of our approach, demonstrating comparable or even\nstronger results than previous specialist or visual foundation models. We hope\nour work inspires others to rethink the unification of segmentation and\ngeneration.\n","versions":"[{'version': 'v1', 'created': 'Thu, 14 Mar 2024 17:52:31 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 11:58:01 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Wang', 'Chaoyang', ''], ['Li', 'Xiangtai', ''], ['Ding', 'Henghui', ''], ['Qi', 'Lu', ''], ['Zhang', 'Jiangning', ''], ['Tong', 'Yunhai', ''], ['Loy', 'Chen Change', ''], ['Yan', 'Shuicheng', '']]","extracted_entities":"[{'text': 'In-context segmentation', 'label': 'contextual Embedding'}, {'text': 'vision foundation models', 'label': 'Foundation Model'}, {'text': 'metric learning', 'label': 'Few-shot Learning'}, {'text': 'visual prompts', 'label': 'Prompting'}, {'text': 'in-context segmentation', 'label': 'contextual Embedding'}, {'text': 'output alignment', 'label': 'contextual Embedding'}]","assigned_concept":"Foundation Model","matched_keyword":"vision foundation models","similarity_score":0.6914944053}
{"id":2405.14297,"submitter":"Yongxin Guo","authors":"Yongxin Guo, Zhenglin Cheng, Xiaoying Tang, Zhaopeng Tu, Tao Lin","title":"Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient\n  Transformer Models","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The Sparse Mixture of Experts (SMoE) has been widely employed to enhance the\nefficiency of training and inference for Transformer-based foundational models,\nyielding promising results.However, the performance of SMoE heavily depends on\nthe choice of hyper-parameters, such as the number of experts and the number of\nexperts to be activated (referred to as top-k), resulting in significant\ncomputational overhead due to the extensive model training by searching over\nvarious hyper-parameter configurations. As a remedy, we introduce the Dynamic\nMixture of Experts (DynMoE) technique. DynMoE incorporates (1) a novel gating\nmethod that enables each token to automatically determine the number of experts\nto activate. (2) An adaptive process automatically adjusts the number of\nexperts during training. Extensive numerical results across Vision, Language,\nand Vision-Language tasks demonstrate the effectiveness of our approach to\nachieve competitive performance compared to GMoE for vision and language tasks,\nand MoE-LLaVA for vision-language tasks, while maintaining efficiency by\nactivating fewer parameters. Our code is available at\nhttps:\/\/github.com\/LINs-lab\/DynMoE.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 May 2024 08:18:30 GMT'}, {'version': 'v2', 'created': 'Tue, 8 Oct 2024 10:53:41 GMT'}, {'version': 'v3', 'created': 'Thu, 10 Oct 2024 03:47:04 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Mar 2025 09:17:56 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Guo', 'Yongxin', ''], ['Cheng', 'Zhenglin', ''], ['Tang', 'Xiaoying', ''], ['Tu', 'Zhaopeng', ''], ['Lin', 'Tao', '']]","extracted_entities":"[{'text': 'Transformer-based foundational models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"Transformer-based foundational models","similarity_score":0.6654277444}
{"id":2407.07464,"submitter":"Manjie Xu","authors":"Manjie Xu, Chenxing Li, Xinyi Tu, Yong Ren, Rilin Chen, Yu Gu, Wei\n  Liang, Dong Yu","title":"Video-to-Audio Generation with Hidden Alignment","comments":"https:\/\/sites.google.com\/view\/vta-ldm","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.CV cs.MM eess.AS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Generating semantically and temporally aligned audio content in accordance\nwith video input has become a focal point for researchers, particularly\nfollowing the remarkable breakthrough in text-to-video generation. In this\nwork, we aim to offer insights into the video-to-audio generation paradigm,\nfocusing on three crucial aspects: vision encoders, auxiliary embeddings, and\ndata augmentation techniques. Beginning with a foundational model built on a\nsimple yet surprisingly effective intuition, we explore various vision encoders\nand auxiliary embeddings through ablation studies. Employing a comprehensive\nevaluation pipeline that emphasizes generation quality and video-audio\nsynchronization alignment, we demonstrate that our model exhibits\nstate-of-the-art video-to-audio generation capabilities. Furthermore, we\nprovide critical insights into the impact of different data augmentation\nmethods on enhancing the generation framework's overall capacity. We showcase\npossibilities to advance the challenge of generating synchronized audio from\nsemantic and temporal perspectives. We hope these insights will serve as a\nstepping stone toward developing more realistic and accurate audio-visual\ngeneration models.\n","versions":"[{'version': 'v1', 'created': 'Wed, 10 Jul 2024 08:40:39 GMT'}, {'version': 'v2', 'created': 'Wed, 16 Oct 2024 03:44:41 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 15:57:51 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Xu', 'Manjie', ''], ['Li', 'Chenxing', ''], ['Tu', 'Xinyi', ''], ['Ren', 'Yong', ''], ['Chen', 'Rilin', ''], ['Gu', 'Yu', ''], ['Liang', 'Wei', ''], ['Yu', 'Dong', '']]","extracted_entities":"[{'text': 'vision encoders', 'label': 'Embedding'}, {'text': 'auxiliary embeddings', 'label': 'Embedding'}, {'text': 'foundational model', 'label': 'Foundation Model'}, {'text': 'vision encoders', 'label': 'Embedding'}, {'text': 'auxiliary embeddings', 'label': 'Embedding'}]","assigned_concept":"Foundation Model","matched_keyword":"foundational model","similarity_score":0.9140634537}
{"id":2407.13493,"submitter":"Giorgio Franceschelli","authors":"Giorgio Franceschelli, Claudia Cevenini, Mirco Musolesi","title":"Training Foundation Models as Data Compression: On Information, Model\n  Weights and Copyright Law","comments":"Spotlight presentation at GenLaw'24, see\n  https:\/\/www.genlaw.org\/2024-icml-papers#training-foundation-models-as-data-compression-on-information-model-weights-and-copyright-law","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The training process of foundation models as for other classes of deep\nlearning systems is based on minimizing the reconstruction error over a\ntraining set. For this reason, they are susceptible to the memorization and\nsubsequent reproduction of training samples. In this paper, we introduce a\ntraining-as-compressing perspective, wherein the model's weights embody a\ncompressed representation of the training data. From a copyright standpoint,\nthis point of view implies that the weights can be considered a reproduction\nor, more likely, a derivative work of a potentially protected set of works. We\ninvestigate the technical and legal challenges that emerge from this framing of\nthe copyright of outputs generated by foundation models, including their\nimplications for practitioners and researchers. We demonstrate that adopting an\ninformation-centric approach to the problem presents a promising pathway for\ntackling these emerging complex legal issues.\n","versions":"[{'version': 'v1', 'created': 'Thu, 18 Jul 2024 13:23:16 GMT'}, {'version': 'v2', 'created': 'Wed, 18 Sep 2024 13:41:20 GMT'}, {'version': 'v3', 'created': 'Mon, 7 Oct 2024 16:40:25 GMT'}, {'version': 'v4', 'created': 'Wed, 12 Mar 2025 14:54:13 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Franceschelli', 'Giorgio', ''], ['Cevenini', 'Claudia', ''], ['Musolesi', 'Mirco', '']]","extracted_entities":"[{'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2407.20143,"submitter":"Borui Wan","authors":"Borui Wan, Mingji Han, Yiyao Sheng, Yanghua Peng, Haibin Lin, Mofan\n  Zhang, Zhichao Lai, Menghan Yu, Junda Zhang, Zuquan Song, Xin Liu, Chuan Wu","title":"ByteCheckpoint: A Unified Checkpointing System for Large Foundation\n  Model Development","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Checkpointing to preserve training states is crucial during the development\nof Large Foundation Models (LFMs), for training resumption upon various\nfailures or changes in GPU resources and parallelism configurations. In\naddition, saved checkpoints are dispatched to evaluation tasks or transferred\nacross different training stages (e.g., from pre-training to post-training).\nAll these scenarios require resharding distributed checkpoints from one\nparallelism to another. In production environments, different LFMs are trained\nwith various frameworks and storage backends, depending on model sizes and\ntraining scales. A high-performance checkpointing system is needed to enable\nefficient checkpoint management at scale throughout the lifecycle of LFM\ndevelopment. We introduce ByteCheckpoint, an industrial-grade checkpointing\nsystem for large-scale LFM training. ByteCheckpoint features: a\nparallelism-agnostic checkpoint representation that enables efficient load-time\ncheckpoint resharding; a generic checkpoint saving\/loading workflow to\naccommodate multiple training frameworks and support different storage\nbackends; full-stack optimizations to ensure high I\/O efficiency and\nscalability; a suite of monitoring tools to streamline large-scale performance\nanalysis and bottleneck detection. Compared to existing open-source\ncheckpointing systems [52, 58], ByteCheckpoint significantly reduces runtime\ncheckpoint stalls, achieving an average reduction of 54.20x. For saving and\nloading times, ByteCheckpoint achieves improvements of up to 9.96x and 8.80x,\nrespectively.\n","versions":"[{'version': 'v1', 'created': 'Mon, 29 Jul 2024 16:18:20 GMT'}, {'version': 'v2', 'created': 'Thu, 10 Oct 2024 12:29:09 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 04:10:33 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Wan', 'Borui', ''], ['Han', 'Mingji', ''], ['Sheng', 'Yiyao', ''], ['Peng', 'Yanghua', ''], ['Lin', 'Haibin', ''], ['Zhang', 'Mofan', ''], ['Lai', 'Zhichao', ''], ['Yu', 'Menghan', ''], ['Zhang', 'Junda', ''], ['Song', 'Zuquan', ''], ['Liu', 'Xin', ''], ['Wu', 'Chuan', '']]","extracted_entities":"[{'text': 'Large Foundation Models', 'label': 'Foundation Model'}, {'text': 'ByteCheckpoint', 'label': 'Open-source LLMs'}, {'text': 'ByteCheckpoint', 'label': 'Open-source LLMs'}]","assigned_concept":"Foundation Model","matched_keyword":"Large Foundation Models","similarity_score":0.8749241829}
{"id":2409.16178,"submitter":"Dimitrije Anti\\'c","authors":"Dimitrije Anti\\'c, Georgios Paschalidis, Shashank Tripathi, Theo\n  Gevers, Sai Kumar Dwivedi, Dimitrios Tzionas","title":"SDFit: 3D Object Pose and Shape by Fitting a Morphable SDF to a Single\n  Image","comments":"12 pages, 10 figures, 5 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recovering 3D object pose and shape from a single image is a challenging and\nhighly ill-posed problem. This is due to strong (self-)occlusions, depth\nambiguities, the vast intra- and inter-class shape variance, and lack of 3D\nground truth for natural images. While existing methods train deep networks on\nsynthetic datasets to predict 3D shapes, they often struggle to generalize to\nreal-world scenarios, lack an explicit feedback loop for refining noisy\nestimates, and primarily focus on geometry without explicitly considering pixel\nalignment. To this end, we make two key observations: (1) a robust solution\nrequires a model that imposes a strong category-specific shape prior to\nconstrain the search space, and (2) foundational models embed 2D images and 3D\nshapes in joint spaces; both help resolve ambiguities. Hence, we propose SDFit,\na novel optimization framework that is built on three key innovations: First,\nwe use a learned morphable signed-distance-function (mSDF) model that acts as a\nstrong shape prior, thus constraining the shape space. Second, we use\nfoundational models to establish rich 2D-to-3D correspondences between image\nfeatures and the mSDF. Third, we develop a fitting pipeline that iteratively\nrefines both shape and pose, aligning the mSDF to the image. We evaluate SDFit\non the Pix3D, Pascal3D+, and COMIC image datasets. SDFit performs on par with\nSotA methods, while demonstrating exceptional robustness to occlusions and\nrequiring no retraining for unseen images. Therefore, SDFit contributes new\ninsights for generalizing in the wild, paving the way for future research. Code\nwill be released.\n","versions":"[{'version': 'v1', 'created': 'Tue, 24 Sep 2024 15:22:04 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 14:43:42 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Anti\u0107', 'Dimitrije', ''], ['Paschalidis', 'Georgios', ''], ['Tripathi', 'Shashank', ''], ['Gevers', 'Theo', ''], ['Dwivedi', 'Sai Kumar', ''], ['Tzionas', 'Dimitrios', '']]","extracted_entities":"[{'text': 'foundational models', 'label': 'Foundation Model'}, {'text': 'SDFit', 'label': 'Foundation Model'}, {'text': 'foundational models', 'label': 'Foundation Model'}, {'text': 'SDFit', 'label': 'Foundation Model'}, {'text': 'SDFit', 'label': 'Foundation Model'}, {'text': 'SDFit', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundational models","similarity_score":0.8593510985}
{"id":2410.02155,"submitter":"Wanpeng Zhang","authors":"Wanpeng Zhang, Zilong Xie, Yicheng Feng, Yijiang Li, Xingrun Xing,\n  Sipeng Zheng, Zongqing Lu","title":"From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multimodal Large Language Models have made significant strides in integrating\nvisual and textual information, yet they often struggle with effectively\naligning these modalities. We introduce a novel image tokenizer that bridges\nthis gap by applying the principle of Byte-Pair Encoding (BPE) to visual data.\nUnlike conventional approaches that rely on separate visual encoders, our\nmethod directly incorporates structural prior information into image tokens,\nmirroring the successful tokenization strategies used in text-only Large\nLanguage Models. This innovative approach enables Transformer models to more\neffectively learn and reason across modalities. Through theoretical analysis\nand extensive experiments, we demonstrate that our BPE Image Tokenizer\nsignificantly enhances MLLMs' multimodal understanding capabilities, even with\nlimited training data. Leveraging this method, we develop Being-VL-0, a model\nthat demonstrates superior performance across various benchmarks and shows\npromising scalability, potentially paving the way for more efficient and\ncapable multimodal foundation models.\n","versions":"[{'version': 'v1', 'created': 'Thu, 3 Oct 2024 02:34:31 GMT'}, {'version': 'v2', 'created': 'Fri, 4 Oct 2024 09:27:20 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 15:36:53 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhang', 'Wanpeng', ''], ['Xie', 'Zilong', ''], ['Feng', 'Yicheng', ''], ['Li', 'Yijiang', ''], ['Xing', 'Xingrun', ''], ['Zheng', 'Sipeng', ''], ['Lu', 'Zongqing', '']]","extracted_entities":"[{'text': 'Transformer models', 'label': 'Transformers'}, {'text': 'BPE', 'label': 'BERT'}, {'text': 'multimodal foundation models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"multimodal foundation models","similarity_score":0.7080713511}
{"id":2410.15218,"submitter":"Junyang He","authors":"Junyang He, Ying-Jung Chen, Alireza Jafari, Anushka Idamekorala,\n  Geoffrey Fox","title":"Deep Learning Foundation and Pattern Models: Challenges in Hydrological\n  Time Series","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  There has been active investigation into deep learning approaches for time\nseries analysis, including foundation models. However, most studies do not\naddress significant scientific applications. This paper aims to identify key\nfeatures in time series by examining hydrology data. Our work advances computer\nscience by emphasizing critical application features and contributes to\nhydrology and other scientific fields by identifying modeling approaches that\neffectively capture these features. Scientific time series data are inherently\ncomplex, involving observations from multiple locations, each with various\ntime-dependent data streams and exogenous factors that may be static or\ntime-varying and either application-dependent or purely mathematical. This\nresearch analyzes hydrology time series from the CAMELS and Caravan global\ndatasets, which encompass rainfall and runoff data across catchments, featuring\nup to six observed streams and 209 static parameters across approximately 8,000\nlocations. Our investigation assesses the impact of exogenous data through\neight different model configurations for key hydrology tasks. Results\ndemonstrate that integrating exogenous information enhances data\nrepresentation, reducing mean squared error by up to 40% in the largest\ndataset. Additionally, we present a detailed performance comparison of over 20\nstate-of-the-art pattern and foundation models. The analysis is fully\nopen-source, facilitated by Jupyter Notebook on Google Colab for LSTM-based\nmodeling, data preprocessing, and model comparisons. Preliminary findings using\nalternative deep learning architectures reveal that models incorporating\ncomprehensive observed and exogenous data outperform more limited approaches,\nincluding foundation models. Notably, natural annual periodic exogenous time\nseries contribute the most significant improvements, though static and other\nperiodic factors are also valuable.\n","versions":"[{'version': 'v1', 'created': 'Sat, 19 Oct 2024 21:23:48 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 21:54:42 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['He', 'Junyang', ''], ['Chen', 'Ying-Jung', ''], ['Jafari', 'Alireza', ''], ['Idamekorala', 'Anushka', ''], ['Fox', 'Geoffrey', '']]","extracted_entities":"[{'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2410.22332,"submitter":"Murtaza Dalal","authors":"Murtaza Dalal and Min Liu and Walter Talbott and Chen Chen and Deepak\n  Pathak and Jian Zhang and Ruslan Salakhutdinov","title":"Local Policies Enable Zero-shot Long-horizon Manipulation","comments":"ICRA 2025 accepted paper. Main Paper 7 pages, 3 tables, 3 figures.\n  Appendix 6 pages, 2 figures, 6 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Sim2real for robotic manipulation is difficult due to the challenges of\nsimulating complex contacts and generating realistic task distributions. To\ntackle the latter problem, we introduce ManipGen, which leverages a new class\nof policies for sim2real transfer: local policies. Locality enables a variety\nof appealing properties including invariances to absolute robot and object\npose, skill ordering, and global scene configuration. We combine these policies\nwith foundation models for vision, language and motion planning and demonstrate\nSOTA zero-shot performance of our method to Robosuite benchmark tasks in\nsimulation (97%). We transfer our local policies from simulation to reality and\nobserve they can solve unseen long-horizon manipulation tasks with up to 8\nstages with significant pose, object and scene configuration variation.\nManipGen outperforms SOTA approaches such as SayCan, OpenVLA, LLMTrajGen and\nVoxPoser across 50 real-world manipulation tasks by 36%, 76%, 62% and 60%\nrespectively. Video results at https:\/\/mihdalal.github.io\/manipgen\/\n","versions":"[{'version': 'v1', 'created': 'Tue, 29 Oct 2024 17:59:55 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 00:54:50 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Dalal', 'Murtaza', ''], ['Liu', 'Min', ''], ['Talbott', 'Walter', ''], ['Chen', 'Chen', ''], ['Pathak', 'Deepak', ''], ['Zhang', 'Jian', ''], ['Salakhutdinov', 'Ruslan', '']]","extracted_entities":"[{'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'SOTA', 'label': 'Zero-shot Learning'}, {'text': 'OpenVLA', 'label': 'Foundation Model'}, {'text': 'LLMTrajGen', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2411.14961,"submitter":"Jieming Bian","authors":"Jieming Bian, Lei Wang, Letian Zhang, Jie Xu","title":"LoRA-FAIR: Federated LoRA Fine-Tuning with Aggregation and\n  Initialization Refinement","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Foundation models (FMs) achieve strong performance across diverse tasks with\ntask-specific fine-tuning, yet full parameter fine-tuning is often\ncomputationally prohibitive for large models. Parameter-efficient fine-tuning\n(PEFT) methods like Low-Rank Adaptation (LoRA) reduce this cost by introducing\nlow-rank matrices for tuning fewer parameters. While LoRA allows for efficient\nfine-tuning, it requires significant data for adaptation, making Federated\nLearning (FL) an appealing solution due to its privacy-preserving collaborative\nframework. However, combining LoRA with FL introduces two key challenges: the\n\\textbf{Server-Side Aggregation Bias}, where server-side averaging of LoRA\nmatrices diverges from the ideal global update, and the \\textbf{Client-Side\nInitialization Lag}, emphasizing the need for consistent initialization across\nrounds. Existing approaches address these challenges individually, limiting\ntheir effectiveness. We propose LoRA-FAIR, a novel method that tackles both\nissues by introducing a correction term on the server, enhancing aggregation\nefficiency and accuracy. LoRA-FAIR maintains computational and communication\nefficiency, yielding superior performance over state-of-the-art methods.\nExperimental results on ViT and MLP-Mixer models across large-scale datasets\ndemonstrate that LoRA-FAIR consistently achieves performance improvements in FL\nsettings.\n","versions":"[{'version': 'v1', 'created': 'Fri, 22 Nov 2024 14:19:01 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 19:43:25 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Bian', 'Jieming', ''], ['Wang', 'Lei', ''], ['Zhang', 'Letian', ''], ['Xu', 'Jie', '']]","extracted_entities":"[{'text': 'Foundation models', 'label': 'Foundation Model'}, {'text': 'task-specific fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Federated\\nLearning', 'label': 'Few-shot Learning'}, {'text': 'FL', 'label': 'Few-shot Learning'}, {'text': 'Server-Side Aggregation Bias', 'label': 'Model Bias and Fairness'}, {'text': 'FL', 'label': 'Few-shot Learning'}]","assigned_concept":"Foundation Model","matched_keyword":"Foundation models","similarity_score":0.9628887177}
{"id":2412.01562,"submitter":"Miroslav Purkrabek","authors":"Miroslav Purkrabek and Jiri Matas","title":"Detection, Pose Estimation and Segmentation for Multiple Bodies: Closing\n  the Virtuous Circle","comments":"Code: https:\/\/mirapurkrabek.github.io\/BBox-Mask-Pose","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Human pose estimation methods work well on isolated people but struggle with\nmultiple-bodies-in-proximity scenarios. Previous work has addressed this\nproblem by conditioning pose estimation by detected bounding boxes or\nkeypoints, but overlooked instance masks. We propose to iteratively enforce\nmutual consistency of bounding boxes, instance masks, and poses. The introduced\nBBox-Mask-Pose (BMP) method uses three specialized models that improve each\nother's output in a closed loop. All models are adapted for mutual\nconditioning, which improves robustness in multi-body scenes. MaskPose, a new\nmask-conditioned pose estimation model, is the best among top-down approaches\non OCHuman. BBox-Mask-Pose pushes SOTA on OCHuman dataset in all three tasks -\ndetection, instance segmentation, and pose estimation. It also achieves SOTA\nperformance on COCO pose estimation. The method is especially good in scenes\nwith large instances overlap, where it improves detection by 39% over the\nbaseline detector. With small specialized models and faster runtime, BMP is an\neffective alternative to large human-centered foundational models. Code and\nmodels are available on https:\/\/MiraPurkrabek.github.io\/BBox-Mask-Pose.\n","versions":"[{'version': 'v1', 'created': 'Mon, 2 Dec 2024 14:50:15 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 14:28:25 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Purkrabek', 'Miroslav', ''], ['Matas', 'Jiri', '']]","extracted_entities":"[{'text': 'OCHuman', 'label': 'Large Language Model'}, {'text': 'OCHuman', 'label': 'Large Language Model'}, {'text': 'large human-centered foundational models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"large human-centered foundational models","similarity_score":0.6611192226}
{"id":2412.02193,"submitter":"Fan-Yun Sun","authors":"Fan-Yun Sun, Weiyu Liu, Siyi Gu, Dylan Lim, Goutam Bhat, Federico\n  Tombari, Manling Li, Nick Haber, Jiajun Wu","title":"LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language\n  Models","comments":"CVPR 2025, project website:\n  https:\/\/ai.stanford.edu\/~sunfanyun\/layoutvlm\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Spatial reasoning is a fundamental aspect of human cognition, enabling\nintuitive understanding and manipulation of objects in three-dimensional space.\nWhile foundation models demonstrate remarkable performance on some benchmarks,\nthey still struggle with 3D reasoning tasks like arranging objects in space\naccording to open-ended language instructions, particularly in dense and\nphysically constrained environments. We introduce LayoutVLM, a framework and\nscene layout representation that exploits the semantic knowledge of\nVision-Language Models (VLMs) and supports differentiable optimization to\nensure physical plausibility. LayoutVLM employs VLMs to generate two mutually\nreinforcing representations from visually marked images, and a self-consistent\ndecoding process to improve VLMs spatial planning. Our experiments show that\nLayoutVLM addresses the limitations of existing LLM and constraint-based\napproaches, producing physically plausible 3D layouts better aligned with the\nsemantic intent of input language instructions. We also demonstrate that\nfine-tuning VLMs with the proposed scene layout representation extracted from\nexisting scene datasets can improve their reasoning performance.\n","versions":"[{'version': 'v1', 'created': 'Tue, 3 Dec 2024 06:15:04 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 07:05:27 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 05:58:39 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Sun', 'Fan-Yun', ''], ['Liu', 'Weiyu', ''], ['Gu', 'Siyi', ''], ['Lim', 'Dylan', ''], ['Bhat', 'Goutam', ''], ['Tombari', 'Federico', ''], ['Li', 'Manling', ''], ['Haber', 'Nick', ''], ['Wu', 'Jiajun', '']]","extracted_entities":"[{'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'LayoutVLM', 'label': 'LLM'}, {'text': 'LayoutVLM', 'label': 'LLM'}, {'text': 'VLMs', 'label': 'Foundation Model'}, {'text': 'VLMs', 'label': 'Foundation Model'}, {'text': 'LayoutVLM', 'label': 'LLM'}, {'text': 'VLMs', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2412.02386,"submitter":"Blanca Lasheras-Hernandez","authors":"Blanca Lasheras-Hernandez, Klaus H. Strobl, Sergio Izquierdo, Tim\n  Bodenm\\\"uller, Rudolph Triebel, and Javier Civera","title":"Single-Shot Metric Depth from Focused Plenoptic Cameras","comments":"8 pages (6 for text + 2 for references), 6 figures, 2 tables.\n  Accepted at IEEE ICRA 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Metric depth estimation from visual sensors is crucial for robots to\nperceive, navigate, and interact with their environment. Traditional range\nimaging setups, such as stereo or structured light cameras, face hassles\nincluding calibration, occlusions, and hardware demands, with accuracy limited\nby the baseline between cameras. Single- and multi-view monocular depth offers\na more compact alternative, but is constrained by the unobservability of the\nmetric scale. Light field imaging provides a promising solution for estimating\nmetric depth by using a unique lens configuration through a single device.\nHowever, its application to single-view dense metric depth is under-addressed\nmainly due to the technology's high cost, the lack of public benchmarks, and\nproprietary geometrical models and software. Our work explores the potential of\nfocused plenoptic cameras for dense metric depth. We propose a novel pipeline\nthat predicts metric depth from a single plenoptic camera shot by first\ngenerating a sparse metric point cloud using machine learning, which is then\nused to scale and align a dense relative depth map regressed by a foundation\ndepth model, resulting in dense metric depth. To validate it, we curated the\nLight Field & Stereo Image Dataset (LFS) of real-world light field images with\nstereo depth labels, filling a current gap in existing resources. Experimental\nresults show that our pipeline produces accurate metric depth predictions,\nlaying a solid groundwork for future research in this field.\n","versions":"[{'version': 'v1', 'created': 'Tue, 3 Dec 2024 11:21:17 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 13:31:15 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Lasheras-Hernandez', 'Blanca', ''], ['Strobl', 'Klaus H.', ''], ['Izquierdo', 'Sergio', ''], ['Bodenm\u00fcller', 'Tim', ''], ['Triebel', 'Rudolph', ''], ['Civera', 'Javier', '']]","extracted_entities":"[{'text': 'foundation\\ndepth model', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation\ndepth model","similarity_score":0.8087540865}
{"id":2412.03342,"submitter":"Zhaopeng Gu","authors":"Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, Jinqiao\n  Wang","title":"UniVAD: A Training-free Unified Model for Few-shot Visual Anomaly\n  Detection","comments":"Accepted by CVPR 2025; Project page: https:\/\/uni-vad.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Visual Anomaly Detection (VAD) aims to identify abnormal samples in images\nthat deviate from normal patterns, covering multiple domains, including\nindustrial, logical, and medical fields. Due to the domain gaps between these\nfields, existing VAD methods are typically tailored to each domain, with\nspecialized detection techniques and model architectures that are difficult to\ngeneralize across different domains. Moreover, even within the same domain,\ncurrent VAD approaches often follow a \"one-category-one-model\" paradigm,\nrequiring large amounts of normal samples to train class-specific models,\nresulting in poor generalizability and hindering unified evaluation across\ndomains. To address this issue, we propose a generalized few-shot VAD method,\nUniVAD, capable of detecting anomalies across various domains, such as\nindustrial, logical, and medical anomalies, with a training-free unified model.\nUniVAD only needs few normal samples as references during testing to detect\nanomalies in previously unseen objects, without training on the specific\ndomain. Specifically, UniVAD employs a Contextual Component Clustering ($C^3$)\nmodule based on clustering and vision foundation models to segment components\nwithin the image accurately, and leverages Component-Aware Patch Matching\n(CAPM) and Graph-Enhanced Component Modeling (GECM) modules to detect anomalies\nat different semantic levels, which are aggregated to produce the final\ndetection result. We conduct experiments on nine datasets spanning industrial,\nlogical, and medical fields, and the results demonstrate that UniVAD achieves\nstate-of-the-art performance in few-shot anomaly detection tasks across\nmultiple domains, outperforming domain-specific anomaly detection models. Code\nis available at https:\/\/github.com\/FantasticGNU\/UniVAD.\n","versions":"[{'version': 'v1', 'created': 'Wed, 4 Dec 2024 14:20:27 GMT'}, {'version': 'v2', 'created': 'Thu, 5 Dec 2024 03:31:40 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 10:03:18 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Gu', 'Zhaopeng', ''], ['Zhu', 'Bingke', ''], ['Zhu', 'Guibo', ''], ['Chen', 'Yingying', ''], ['Tang', 'Ming', ''], ['Wang', 'Jinqiao', '']]","extracted_entities":"[{'text': 'Contextual Component Clustering', 'label': 'contextual Embedding'}, {'text': 'vision foundation models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"vision foundation models","similarity_score":0.6914944053}
{"id":2412.06082,"submitter":"Leo Fillioux","authors":"Leo Fillioux, Julio Silva-Rodr\\'iguez, Ismail Ben Ayed, Paul-Henry\n  Courn\\`ede, Maria Vakalopoulou, Stergios Christodoulidis, Jose Dolz","title":"Are foundation models for computer vision good conformal predictors?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent advances in self-supervision and contrastive learning have brought the\nperformance of foundation models to unprecedented levels in a variety of tasks.\nFueled by this progress, these models are becoming the prevailing approach for\na wide array of real-world vision problems, including risk-sensitive and\nhigh-stakes applications. However, ensuring safe deployment in these scenarios\nrequires a more comprehensive understanding of their uncertainty modeling\ncapabilities, which has been barely explored. In this work, we delve into the\nbehaviour of vision and vision-language foundation models under Conformal\nPrediction (CP), a statistical framework that provides theoretical guarantees\nof marginal coverage of the true class. Across extensive experiments including\npopular vision classification benchmarks, well-known foundation vision models,\nand three CP methods, our findings reveal that foundation models are\nwell-suited for conformalization procedures, particularly those integrating\nVision Transformers. We also show that calibrating the confidence predictions\nof these models, a popular strategy to improve their uncertainty\nquantification, actually leads to efficiency degradation of the conformal set\non adaptive CP methods. Furthermore, few-shot adaptation of Vision-Language\nModels (VLMs) to downstream tasks, whose popularity is surging, enhances\nconformal scores compared to zero-shot predictions. Last, our empirical study\nexposes APS as particularly promising in the context of vision foundation\nmodels, as it does not violate the marginal coverage guarantees across multiple\nchallenging, yet realistic scenarios.\n","versions":"[{'version': 'v1', 'created': 'Sun, 8 Dec 2024 22:05:38 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 12:55:06 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Fillioux', 'Leo', ''], ['Silva-Rodr\u00edguez', 'Julio', ''], ['Ayed', 'Ismail Ben', ''], ['Courn\u00e8de', 'Paul-Henry', ''], ['Vakalopoulou', 'Maria', ''], ['Christodoulidis', 'Stergios', ''], ['Dolz', 'Jose', '']]","extracted_entities":"[{'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'Vision Transformers', 'label': 'Transformers'}, {'text': 'few-shot adaptation', 'label': 'Few-shot Learning'}, {'text': 'Vision-Language\\nModels', 'label': 'Foundation Model'}, {'text': 'zero-shot predictions', 'label': 'Zero-shot Learning'}, {'text': 'foundation\\nmodels', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2412.10439,"submitter":"Yihan Cao","authors":"Yihan Cao, Jiazhao Zhang, Zhinan Yu, Shuzhen Liu, Zheng Qin, Qin Zou,\n  Bo Du, Kai Xu","title":"CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Object goal navigation (ObjectNav) is a fundamental task in embodied AI,\nrequiring an agent to locate a target object in previously unseen environments.\nThis task is particularly challenging because it requires both perceptual and\ncognitive processes, including object recognition and decision-making. While\nsubstantial advancements in perception have been driven by the rapid\ndevelopment of visual foundation models, progress on the cognitive aspect\nremains constrained, primarily limited to either implicit learning through\nsimulator rollouts or explicit reliance on predefined heuristic rules. Inspired\nby neuroscientific findings demonstrating that humans maintain and dynamically\nupdate fine-grained cognitive states during object search tasks in novel\nenvironments, we propose CogNav, a framework designed to mimic this cognitive\nprocess using large language models. Specifically, we model the cognitive\nprocess using a finite state machine comprising fine-grained cognitive states,\nranging from exploration to identification. Transitions between states are\ndetermined by a large language model based on a dynamically constructed\nheterogeneous cognitive map, which contains spatial and semantic information\nabout the scene being explored. Extensive evaluations on the HM3D, MP3D, and\nRoboTHOR benchmarks demonstrate that our cognitive process modeling\nsignificantly improves the success rate of ObjectNav at least by relative 14%\nover the state-of-the-arts.\n","versions":"[{'version': 'v1', 'created': 'Wed, 11 Dec 2024 09:50:35 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 12:19:09 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Cao', 'Yihan', ''], ['Zhang', 'Jiazhao', ''], ['Yu', 'Zhinan', ''], ['Liu', 'Shuzhen', ''], ['Qin', 'Zheng', ''], ['Zou', 'Qin', ''], ['Du', 'Bo', ''], ['Xu', 'Kai', '']]","extracted_entities":"[{'text': 'visual foundation models', 'label': 'Foundation Model'}, {'text': 'implicit learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Foundation Model","matched_keyword":"visual foundation models","similarity_score":0.7962188721}
{"id":2502.04981,"submitter":"Xiaoyu Zhou","authors":"Xiaoyu Zhou, Jingqi Wang, Yongtao Wang, Yufei Wei, Nan Dong,\n  Ming-Hsuan Yang","title":"AutoOcc: Automatic Open-Ended Semantic Occupancy Annotation via\n  Vision-Language Guided Gaussian Splatting","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Obtaining high-quality 3D semantic occupancy from raw sensor data remains an\nessential yet challenging task, often requiring extensive manual labeling. In\nthis work, we propose AutoOcc, an vision-centric automated pipeline for\nopen-ended semantic occupancy annotation that integrates differentiable\nGaussian splatting guided by vision-language models. We formulate the\nopen-ended semantic occupancy reconstruction task to automatically generate\nscene occupancy by combining attention maps from vision-language models and\nfoundation vision models. We devise semantic-aware Gaussians as intermediate\ngeometric descriptors and propose a cumulative Gaussian-to-voxel splatting\nalgorithm that enables effective and efficient occupancy annotation. Our\nframework outperforms existing automated occupancy annotation methods without\nhuman labels. AutoOcc also enables open-ended semantic occupancy auto-labeling,\nachieving robust performance in both static and dynamically complex scenarios.\nAll the source codes and trained models will be released.\n","versions":"[{'version': 'v1', 'created': 'Fri, 7 Feb 2025 14:58:59 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 03:12:18 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Zhou', 'Xiaoyu', ''], ['Wang', 'Jingqi', ''], ['Wang', 'Yongtao', ''], ['Wei', 'Yufei', ''], ['Dong', 'Nan', ''], ['Yang', 'Ming-Hsuan', '']]","extracted_entities":"[{'text': 'attention maps', 'label': 'Attention mechanism'}, {'text': 'foundation vision models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation vision models","similarity_score":0.7662065029}
{"id":2502.20256,"submitter":"Yancheng Cai","authors":"Yancheng Cai, Fei Yin, Dounia Hammou, Rafal Mantiuk","title":"Do computer vision foundation models learn the low-level characteristics\n  of the human visual system?","comments":"Accepted by CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Computer vision foundation models, such as DINO or OpenCLIP, are trained in a\nself-supervised manner on large image datasets. Analogously, substantial\nevidence suggests that the human visual system (HVS) is influenced by the\nstatistical distribution of colors and patterns in the natural world,\ncharacteristics also present in the training data of foundation models. The\nquestion we address in this paper is whether foundation models trained on\nnatural images mimic some of the low-level characteristics of the human visual\nsystem, such as contrast detection, contrast masking, and contrast constancy.\nSpecifically, we designed a protocol comprising nine test types to evaluate the\nimage encoders of 45 foundation and generative models. Our results indicate\nthat some foundation models (e.g., DINO, DINOv2, and OpenCLIP), share some of\nthe characteristics of human vision, but other models show little resemblance.\nFoundation models tend to show smaller sensitivity to low contrast and rather\nirregular responses to contrast across frequencies. The foundation models show\nthe best agreement with human data in terms of contrast masking. Our findings\nsuggest that human vision and computer vision may take both similar and\ndifferent paths when learning to interpret images of the real world. Overall,\nwhile differences remain, foundation models trained on vision tasks start to\nalign with low-level human vision, with DINOv2 showing the closest resemblance.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 16:43:56 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 21:52:23 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Cai', 'Yancheng', ''], ['Yin', 'Fei', ''], ['Hammou', 'Dounia', ''], ['Mantiuk', 'Rafal', '']]","extracted_entities":"[{'text': 'DINO', 'label': 'Foundation Model'}, {'text': 'OpenCLIP', 'label': 'Foundation Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'DINO', 'label': 'Foundation Model'}, {'text': 'DINOv2', 'label': 'Foundation Model'}, {'text': 'OpenCLIP', 'label': 'Foundation Model'}, {'text': 'Foundation models', 'label': 'Foundation Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'DINOv2', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2503.01115,"submitter":"Zhipeng Huang","authors":"Zhipeng Huang, Shaobin Zhuang, Canmiao Fu, Binxin Yang, Ying Zhang,\n  Chong Sun, Zhizheng Zhang, Yali Wang, Chen Li and Zheng-Jun Zha","title":"WeGen: A Unified Model for Interactive Multimodal Generation as We Chat","comments":"CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Existing multimodal generative models fall short as qualified design\ncopilots, as they often struggle to generate imaginative outputs once\ninstructions are less detailed or lack the ability to maintain consistency with\nthe provided references. In this work, we introduce WeGen, a model that unifies\nmultimodal generation and understanding, and promotes their interplay in\niterative generation. It can generate diverse results with high creativity for\nless detailed instructions. And it can progressively refine prior generation\nresults or integrating specific contents from references following the\ninstructions in its chat with users. During this process, it is capable of\npreserving consistency in the parts that the user is already satisfied with. To\nthis end, we curate a large-scale dataset, extracted from Internet videos,\ncontaining rich object dynamics and auto-labeled dynamics descriptions by\nadvanced foundation models to date. These two information are interleaved into\na single sequence to enable WeGen to learn consistency-aware generation where\nthe specified dynamics are generated while the consistency of unspecified\ncontent is preserved aligned with instructions. Besides, we introduce a prompt\nself-rewriting mechanism to enhance generation diversity. Extensive experiments\ndemonstrate the effectiveness of unifying multimodal understanding and\ngeneration in WeGen and show it achieves state-of-the-art performance across\nvarious visual generation benchmarks. These also demonstrate the potential of\nWeGen as a user-friendly design copilot as desired. The code and models will be\navailable at https:\/\/github.com\/hzphzp\/WeGen.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 02:50:07 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 02:12:53 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Huang', 'Zhipeng', ''], ['Zhuang', 'Shaobin', ''], ['Fu', 'Canmiao', ''], ['Yang', 'Binxin', ''], ['Zhang', 'Ying', ''], ['Sun', 'Chong', ''], ['Zhang', 'Zhizheng', ''], ['Wang', 'Yali', ''], ['Li', 'Chen', ''], ['Zha', 'Zheng-Jun', '']]","extracted_entities":"[{'text': 'advanced foundation models', 'label': 'Foundation Model'}, {'text': 'prompt\\nself-rewriting mechanism', 'label': 'Prompting'}]","assigned_concept":"Foundation Model","matched_keyword":"advanced foundation models","similarity_score":0.9149395227}
{"id":2503.02597,"submitter":"Wei-Yao Wang","authors":"Wei-Yao Wang, Zhao Wang, Helen Suzuki, Yoshiyuki Kobayashi","title":"Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual\n  Attention for Multimodal LLMs","comments":"Preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in perceiving and reasoning over multimodal inquiries, ushering in a\nnew research era for foundation models. However, vision-language misalignment\nin MLLMs has emerged as a critical challenge, where the textual responses\ngenerated by these models are not factually aligned with the given text-image\ninputs. Existing efforts to address vision-language misalignment have focused\non developing specialized vision-language connectors or leveraging visual\ninstruction tuning from diverse domains. In this paper, we tackle this issue\nfrom a fundamental yet unexplored perspective by revisiting the core\narchitecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs\nconsisting of a causal attention mechanism, which limits the ability of the\nearlier modalities (e.g., images) to incorporate information from the latter\nmodalities (e.g., text). To address this problem, we propose \\MapleLeaf AKI, a\nnovel MLLM that unlocks causal attention into modality-mutual attention (MMA)\nto enable image tokens to attend to text tokens. This simple yet effective\ndesign allows AKI to achieve superior performance in 12 multimodal\nunderstanding benchmarks (+7.2% on average) without introducing additional\nparameters and increasing training time. Our MMA design is intended to be\ngeneric, allowing for application across various modalities, and scalable to\naccommodate diverse multimodal scenarios. The code and model are publicly\navailable at https:\/\/github.com\/sony\/aki to encourage further advancements in\nMLLMs across various directions.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 13:18:33 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 01:48:08 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Wei-Yao', ''], ['Wang', 'Zhao', ''], ['Suzuki', 'Helen', ''], ['Kobayashi', 'Yoshiyuki', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'visual\\ninstruction tuning', 'label': 'Fine-tuning'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'decoder-only LLMs', 'label': 'LLMs'}, {'text': 'causal attention mechanism', 'label': 'Attention mechanism'}, {'text': 'causal attention', 'label': 'Attention mechanism'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2503.03464,"submitter":"Kun Zhang","authors":"Kun Zhang, Peng Yun, Jun Cen, Junhao Cai, Didi Zhu, Hangjie Yuan, Chao\n  Zhao, Tao Feng, Michael Yu Wang, Qifeng Chen, Jia Pan, Wei Zhang, Bo Yang,\n  Hua Chen","title":"Generative Artificial Intelligence in Robotic Manipulation: A Survey","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This survey provides a comprehensive review on recent advancements of\ngenerative learning models in robotic manipulation, addressing key challenges\nin the field. Robotic manipulation faces critical bottlenecks, including\nsignificant challenges in insufficient data and inefficient data acquisition,\nlong-horizon and complex task planning, and the multi-modality reasoning\nability for robust policy learning performance across diverse environments. To\ntackle these challenges, this survey introduces several generative model\nparadigms, including Generative Adversarial Networks (GANs), Variational\nAutoencoders (VAEs), diffusion models, probabilistic flow models, and\nautoregressive models, highlighting their strengths and limitations. The\napplications of these models are categorized into three hierarchical layers:\nthe Foundation Layer, focusing on data generation and reward generation; the\nIntermediate Layer, covering language, code, visual, and state generation; and\nthe Policy Layer, emphasizing grasp generation and trajectory generation. Each\nlayer is explored in detail, along with notable works that have advanced the\nstate of the art. Finally, the survey outlines future research directions and\nchallenges, emphasizing the need for improved efficiency in data utilization,\nbetter handling of long-horizon tasks, and enhanced generalization across\ndiverse robotic scenarios. All the related resources, including research\npapers, open-source data, and projects, are collected for the community in\nhttps:\/\/github.com\/GAI4Manipulation\/AwesomeGAIManipulation\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 12:54:54 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 02:50:27 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Zhang', 'Kun', ''], ['Yun', 'Peng', ''], ['Cen', 'Jun', ''], ['Cai', 'Junhao', ''], ['Zhu', 'Didi', ''], ['Yuan', 'Hangjie', ''], ['Zhao', 'Chao', ''], ['Feng', 'Tao', ''], ['Wang', 'Michael Yu', ''], ['Chen', 'Qifeng', ''], ['Pan', 'Jia', ''], ['Zhang', 'Wei', ''], ['Yang', 'Bo', ''], ['Chen', 'Hua', '']]","extracted_entities":"[{'text': 'diffusion models', 'label': 'AI model'}, {'text': 'probabilistic flow models', 'label': 'AI model'}, {'text': 'autoregressive models', 'label': 'AI model'}, {'text': 'Foundation Layer', 'label': 'Foundation Model'}, {'text': 'Policy Layer', 'label': 'Foundation Model'}, {'text': 'open-source data', 'label': 'Open-source LLMs'}]","assigned_concept":"Foundation Model","matched_keyword":"Foundation Layer","similarity_score":0.793219924}
{"id":2503.05245,"submitter":"Johanna Paula M\\\"uller","authors":"Johanna P. M\\\"uller, Robert Wright, Thomas G. Day, Lorenzo Venturini,\n  Samuel F. Budd, Hadrien Reynaud, Joseph V. Hajnal, Reza Razavi, Bernhard\n  Kainz","title":"L-FUSION: Laplacian Fetal Ultrasound Segmentation & Uncertainty\n  Estimation","comments":"Under Review","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Accurate analysis of prenatal ultrasound (US) is essential for early\ndetection of developmental anomalies. However, operator dependency and\ntechnical limitations (e.g. intrinsic artefacts and effects, setting errors)\ncan complicate image interpretation and the assessment of diagnostic\nuncertainty. We present L-FUSION (Laplacian Fetal US Segmentation with\nIntegrated FoundatiON models), a framework that integrates uncertainty\nquantification through unsupervised, normative learning and large-scale\nfoundation models for robust segmentation of fetal structures in normal and\npathological scans. We propose to utilise the aleatoric logit distributions of\nStochastic Segmentation Networks and Laplace approximations with fast Hessian\nestimations to estimate epistemic uncertainty only from the segmentation head.\nThis enables us to achieve reliable abnormality quantification for instant\ndiagnostic feedback. Combined with an integrated Dropout component, L-FUSION\nenables reliable differentiation of lesions from normal fetal anatomy with\nenhanced uncertainty maps and segmentation counterfactuals in US imaging. It\nimproves epistemic and aleatoric uncertainty interpretation and removes the\nneed for manual disease-labelling. Evaluations across multiple datasets show\nthat L-FUSION achieves superior segmentation accuracy and consistent\nuncertainty quantification, supporting on-site decision-making and offering a\nscalable solution for advancing fetal ultrasound analysis in clinical settings.\n","versions":"[{'version': 'v1', 'created': 'Fri, 7 Mar 2025 08:57:38 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 10:11:17 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['M\u00fcller', 'Johanna P.', ''], ['Wright', 'Robert', ''], ['Day', 'Thomas G.', ''], ['Venturini', 'Lorenzo', ''], ['Budd', 'Samuel F.', ''], ['Reynaud', 'Hadrien', ''], ['Hajnal', 'Joseph V.', ''], ['Razavi', 'Reza', ''], ['Kainz', 'Bernhard', '']]","extracted_entities":"[{'text': 'uncertainty\\nquantification', 'label': 'quantisation'}, {'text': 'normative learning', 'label': 'Few-shot Learning'}, {'text': 'large-scale\\nfoundation models', 'label': 'Foundation Model'}, {'text': 'uncertainty quantification', 'label': 'quantisation'}]","assigned_concept":"Foundation Model","matched_keyword":"large-scale\nfoundation models","similarity_score":0.8255428076}
{"id":2503.06482,"submitter":"Honglin Li","authors":"Honglin Li, Zhongyi Shui, Yunlong Zhang, Chenglu Zhu, Lin Yang","title":"PathVQ: Reforming Computational Pathology Foundation Model for Whole\n  Slide Image Analysis via Vector Quantization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Computational pathology and whole-slide image (WSI) analysis are pivotal in\ncancer diagnosis and prognosis. However, the ultra-high resolution of WSIs\npresents significant modeling challenges. Recent advancements in pathology\nfoundation models have improved performance, yet most approaches rely on [CLS]\ntoken representation of tile ViT as slide-level inputs (16x16 pixels is\nrefereed as patch and 224x224 pixels as tile). This discards critical spatial\ndetails from patch tokens, limiting downstream WSI analysis tasks. We find that\nleveraging all spatial patch tokens benefits WSI analysis but incurs nearly\n200x higher storage and training costs (e.g., 196 tokens in ViT$_{224}$). To\naddress this, we introduce vector quantized (VQ) distillation on patch feature,\nwhich efficiently compresses spatial patch tokens using discrete indices and a\ndecoder. Our method reduces token dimensionality from 1024 to 16, achieving a\n64x compression rate while preserving reconstruction fidelity. Furthermore, we\nemploy a multi-scale VQ (MSVQ) strategy, which not only enhances VQ\nreconstruction performance but also serves as a Self-supervised Learning (SSL)\nsupervision for a seamless slide-level pretraining objective. Built upon the\nquantized patch features and supervision targets of tile via MSVQ, we develop a\nprogressive convolutional module and slide-level SSL to extract representations\nwith rich spatial-information for downstream WSI tasks. Extensive evaluations\non multiple datasets demonstrate the effectiveness of our approach, achieving\nstate-of-the-art performance in WSI analysis. Code will be available soon.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 06:51:08 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Li', 'Honglin', ''], ['Shui', 'Zhongyi', ''], ['Zhang', 'Yunlong', ''], ['Zhu', 'Chenglu', ''], ['Yang', 'Lin', '']]","extracted_entities":"[{'text': 'pathology\\nfoundation models', 'label': 'Foundation Model'}, {'text': 'vector quantized (VQ) distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Foundation Model","matched_keyword":"pathology\nfoundation models","similarity_score":0.5730407238}
{"id":2503.06629,"submitter":"Tomasz Kryjak","authors":"Hiroshi Nakano and Krzysztof Blachut and Kamil Jeziorek and Piotr\n  Wzorek and Manon Dampfhoffer and Thomas Mesquida and Hiroaki Nishi and Tomasz\n  Kryjak and Thomas Dalgaty","title":"Hardware-Accelerated Event-Graph Neural Networks for Low-Latency\n  Time-Series Classification on SoC FPGA","comments":"Paper accepted for the 21st International Symposium on Applied\n  Reconfigurable Computing ARC 2025, Sevilla, Spain, April 9-11, 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI eess.SP","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  As the quantities of data recorded by embedded edge sensors grow, so too does\nthe need for intelligent local processing. Such data often comes in the form of\ntime-series signals, based on which real-time predictions can be made locally\nusing an AI model. However, a hardware-software approach capable of making\nlow-latency predictions with low power consumption is required. In this paper,\nwe present a hardware implementation of an event-graph neural network for\ntime-series classification. We leverage an artificial cochlea model to convert\nthe input time-series signals into a sparse event-data format that allows the\nevent-graph to drastically reduce the number of calculations relative to other\nAI methods. We implemented the design on a SoC FPGA and applied it to the\nreal-time processing of the Spiking Heidelberg Digits (SHD) dataset to\nbenchmark our approach against competitive solutions. Our method achieves a\nfloating-point accuracy of 92.7% on the SHD dataset for the base model, which\nis only 2.4% and 2% less than the state-of-the-art models with over 10% and 67%\nfewer model parameters, respectively. It also outperforms FPGA-based spiking\nneural network implementations by 19.3% and 4.5%, achieving 92.3% accuracy for\nthe quantised model while using fewer computational resources and reducing\nlatency.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 14:08:46 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Nakano', 'Hiroshi', ''], ['Blachut', 'Krzysztof', ''], ['Jeziorek', 'Kamil', ''], ['Wzorek', 'Piotr', ''], ['Dampfhoffer', 'Manon', ''], ['Mesquida', 'Thomas', ''], ['Nishi', 'Hiroaki', ''], ['Kryjak', 'Tomasz', ''], ['Dalgaty', 'Thomas', '']]","extracted_entities":"[{'text': 'base model', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"base model","similarity_score":0.5351421833}
{"id":2309.12862,"submitter":"Yuwei Sun","authors":"Yuwei Sun, Hideya Ochiai, Zhirong Wu, Stephen Lin, Ryota Kanai","title":"Associative Transformer","comments":"Accepted for CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV cs.NE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Emerging from the pairwise attention in conventional Transformers, there is a\ngrowing interest in sparse attention mechanisms that align more closely with\nlocalized, contextual learning in the biological brain. Existing studies such\nas the Coordination method employ iterative cross-attention mechanisms with a\nbottleneck to enable the sparse association of inputs. However, these methods\nare parameter inefficient and fail in more complex relational reasoning tasks.\nTo this end, we propose Associative Transformer (AiT) to enhance the\nassociation among sparsely attended input tokens, improving parameter\nefficiency and performance in various vision tasks such as classification and\nrelational reasoning. AiT leverages a learnable explicit memory comprising\nspecialized priors that guide bottleneck attentions to facilitate the\nextraction of diverse localized tokens. Moreover, AiT employs an associative\nmemory-based token reconstruction using a Hopfield energy function. The\nextensive empirical experiments demonstrate that AiT requires significantly\nfewer parameters and attention layers outperforming a broad range of sparse\nTransformer models. Additionally, AiT outperforms the SOTA sparse Transformer\nmodels including the Coordination method on the Sort-of-CLEVR dataset.\n","versions":"[{'version': 'v1', 'created': 'Fri, 22 Sep 2023 13:37:10 GMT'}, {'version': 'v2', 'created': 'Thu, 23 Nov 2023 07:26:55 GMT'}, {'version': 'v3', 'created': 'Wed, 31 Jan 2024 01:05:14 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Mar 2025 09:04:22 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Sun', 'Yuwei', ''], ['Ochiai', 'Hideya', ''], ['Wu', 'Zhirong', ''], ['Lin', 'Stephen', ''], ['Kanai', 'Ryota', '']]","extracted_entities":"[{'text': 'conventional Transformers', 'label': 'Transformers'}, {'text': 'sparse attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'Coordination method', 'label': 'Transformers'}, {'text': 'Coordination method', 'label': 'Transformers'}]","assigned_concept":"Attention mechanism","matched_keyword":"sparse attention mechanisms","similarity_score":0.8329435587}
{"id":2309.13669,"submitter":"Harry Freeman","authors":"Harry Freeman and George Kantor","title":"Autonomous Apple Fruitlet Sizing with Next Best View Planning","comments":null,"journal-ref":"2024 IEEE International Conference on Robotics and Automation\n  (ICRA), Yokohama, Japan, 2024, pp. 15847-15853","doi":"10.1109\/ICRA57147.2024.10610226","report-no":null,"categories":"cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper, we present a next-best-view planning approach to autonomously\nsize apple fruitlets. State-of-the-art viewpoint planners in agriculture are\ndesigned to size large and more sparsely populated fruit. They rely on lower\nresolution maps and sizing methods that do not generalize to smaller fruit\nsizes. To overcome these limitations, our method combines viewpoint sampling\naround semantically labeled regions of interest, along with an attention-guided\ninformation gain mechanism to more strategically select viewpoints that target\nthe small fruits' volume. Additionally, we integrate a dual-map representation\nof the environment that is able to both speed up expensive ray casting\noperations and maintain the high occupancy resolution required to informatively\nplan around the fruit. When sizing, a robust estimation and graph clustering\napproach is introduced to associate fruit detections across images. Through\nsimulated experiments, we demonstrate that our viewpoint planner improves\nsizing accuracy compared to state of the art and ablations. We also provide\nquantitative results on data collected by a real robotic system in the field.\n","versions":"[{'version': 'v1', 'created': 'Sun, 24 Sep 2023 15:34:52 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 21:33:21 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Freeman', 'Harry', ''], ['Kantor', 'George', '']]","extracted_entities":"[{'text': 'attention-guided\\ninformation gain mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention-guided\ninformation gain mechanism","similarity_score":0.7561488152}
{"id":2311.11091,"submitter":"Xuantao Li","authors":"Xuantao Li","title":"Deep Tensor Network","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV quant-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We introduce the Deep Tensor Network, a novel framework that integrates\ntensor-based operations into the attention mechanism, thereby enhancing both\nthe expressivity and computational efficiency of deep neural networks. Our\napproach leverages the algebraic structure of tensor products to generalize the\nconventional dot-product attention and to formulate new operators, namely,\nTensor Attention and Tensor Interaction, which capture higher-order token\ndependencies. Through rigorous theoretical analysis based on the universal\nproperties of tensor products, we demonstrate that our framework not only\nimproves efficiency by reducing computational complexity but also offers a\nprincipled method for modeling complex interactions in sequential data.\nEmpirical evaluations further substantiate that the proposed deep tensor\nnetwork can serve as a robust building block for advancing state-of-the-art\nperformance in various deep learning tasks.\n","versions":"[{'version': 'v1', 'created': 'Sat, 18 Nov 2023 14:41:33 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 04:55:59 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Li', 'Xuantao', '']]","extracted_entities":"[{'text': 'Deep Tensor Network', 'label': 'Foundation Model'}, {'text': 'attention mechanism', 'label': 'Attention mechanism'}, {'text': 'Tensor Attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanism","similarity_score":1.0}
{"id":2312.10052,"submitter":"Zhongliang Zeng","authors":"Dongdong Li, Zhongliang Zeng, Zhe Wang, Hai Yang","title":"ESTformer: Transformer Utilizing Spatiotemporal Dependencies for\n  Electroencaphalogram Super-resolution","comments":"Accepted by Knowledge-Based Systems","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Towards practical applications of Electroencephalography (EEG), lightweight\nacquisition devices garner significant attention. However, EEG channel\nselection methods are commonly data-sensitive and cannot establish a unified\nsound paradigm for EEG acquisition devices. Through reverse conceptualisation,\nwe formulated EEG applications in an EEG super-resolution (SR) manner, but\nsuffered from high computation costs, extra interpolation bias, and few\ninsights into spatiotemporal dependency modelling. To this end, we propose\nESTformer, an EEG SR framework that utilises spatiotemporal dependencies based\non the transformer. ESTformer applies positional encoding methods and a\nmultihead self-attention mechanism to the space and time dimensions, which can\nlearn spatial structural correlations and temporal functional variations.\nESTformer, with the fixed mask strategy, adopts a mask token to upsample\nlow-resolution (LR) EEG data in the case of disturbance from mathematical\ninterpolation methods. On this basis, we designed various transformer blocks to\nconstruct a spatial interpolation module (SIM) and a temporal reconstruction\nmodule (TRM). Finally, ESTformer cascades the SIM and TRM to capture and model\nthe spatiotemporal dependencies for EEG SR with fidelity. Extensive\nexperimental results on two EEG datasets show the effectiveness of ESTformer\nagainst previous state-of-the-art methods, demonstrating the versatility of the\nTransformer for EEG SR tasks. The superiority of the SR data was verified in an\nEEG-based person identification and emotion recognition task, achieving a 2% to\n38% improvement compared with the LR data at different sampling scales.\n","versions":"[{'version': 'v1', 'created': 'Sun, 3 Dec 2023 12:26:32 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 07:17:58 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Dongdong', ''], ['Zeng', 'Zhongliang', ''], ['Wang', 'Zhe', ''], ['Yang', 'Hai', '']]","extracted_entities":"[{'text': 'multihead self-attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"multihead self-attention mechanism","similarity_score":0.7830716372}
{"id":2403.1296,"submitter":"Kartik Narayan","authors":"Kartik Narayan, Vibashan VS, Rama Chellappa, Vishal M. Patel","title":"FaceXFormer: A Unified Transformer for Facial Analysis","comments":"Project page: https:\/\/kartik-3004.github.io\/facexformer\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this work, we introduce FaceXFormer, an end-to-end unified transformer\nmodel capable of performing ten facial analysis tasks within a single\nframework. These tasks include face parsing, landmark detection, head pose\nestimation, attribute prediction, age, gender, and race estimation, facial\nexpression recognition, face recognition, and face visibility. Traditional face\nanalysis approaches rely on task-specific architectures and pre-processing\ntechniques, limiting scalability and integration. In contrast, FaceXFormer\nemploys a transformer-based encoder-decoder architecture, where each task is\nrepresented as a learnable token, enabling seamless multi-task processing\nwithin a unified model. To enhance efficiency, we introduce FaceX, a\nlightweight decoder with a novel bi-directional cross-attention mechanism,\nwhich jointly processes face and task tokens to learn robust and generalized\nfacial representations. We train FaceXFormer on ten diverse face perception\ndatasets and evaluate it against both specialized and multi-task models across\nmultiple benchmarks, demonstrating state-of-the-art or competitive performance.\nAdditionally, we analyze the impact of various components of FaceXFormer on\nperformance, assess real-world robustness in \"in-the-wild\" settings, and\nconduct a computational performance evaluation. To the best of our knowledge,\nFaceXFormer is the first model capable of handling ten facial analysis tasks\nwhile maintaining real-time performance at 33.21 FPS. Code:\nhttps:\/\/github.com\/Kartik-3004\/facexformer\n","versions":"[{'version': 'v1', 'created': 'Tue, 19 Mar 2024 17:58:04 GMT'}, {'version': 'v2', 'created': 'Thu, 19 Dec 2024 22:48:46 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 17:08:19 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Narayan', 'Kartik', ''], ['VS', 'Vibashan', ''], ['Chellappa', 'Rama', ''], ['Patel', 'Vishal M.', '']]","extracted_entities":"[{'text': 'bi-directional cross-attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"bi-directional cross-attention mechanism","similarity_score":0.7649866343}
{"id":2405.0462,"submitter":"Won-Gi Paeng","authors":"Won-Gi Paeng, Daesuk Kwon, Kyungwon Jeong and Honggyo Suh","title":"Folded Context Condensation in Path Integral Formalism for Infinite\n  Context Transformers","comments":"10 pages, 12 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph cs.AI cs.CL cs.LG cs.NE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this work, we present a generalized formulation of the Transformer\nalgorithm by reinterpreting its core mechanisms within the framework of Path\nIntegral formalism. In this perspective, the attention mechanism is recast as a\nprocess that integrates all possible transition paths leading to future token\nstates, with temporal evolution governed by the Feed-Forward Network. By\nsystematically mapping each component of the Transformer to its counterpart in\nthe Path Integral formulation, we obtain a more compact and efficient\nrepresentation, in which the contextual information of a sequence is condensed\ninto memory-like segments. These segments are recurrently processed across\nTransformer layers, enabling more effective long-term information retention. We\nvalidate the effectiveness of this approach through the Passkey retrieval task\nand a summarization task, demonstrating that the proposed method preserves\nhistorical information while exhibiting memory usage that scales linearly with\nsequence length. This contrasts with the non-linear memory growth typically\nobserved in standard attention mechanisms. We expect that this quantum-inspired\ngeneralization of the Transformer architecture will open new avenues for\nenhancing both the efficiency and expressiveness of future Transformer models.\n","versions":"[{'version': 'v1', 'created': 'Tue, 7 May 2024 19:05:26 GMT'}, {'version': 'v2', 'created': 'Fri, 10 May 2024 02:18:27 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 13:24:46 GMT'}, {'version': 'v4', 'created': 'Wed, 12 Mar 2025 09:13:15 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Paeng', 'Won-Gi', ''], ['Kwon', 'Daesuk', ''], ['Jeong', 'Kyungwon', ''], ['Suh', 'Honggyo', '']]","extracted_entities":"[{'text': 'attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanism","similarity_score":1.0}
{"id":2406.02021,"submitter":"Seokju Yun","authors":"Seokju Yun, Dongheon Lee, Youngmin Ro","title":"FFNet: MetaMixer-based Efficient Convolutional Mixer Design","comments":"Code: https:\/\/github.com\/ysj9909\/FFNet","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Transformer, composed of self-attention and Feed-Forward Network, has\nrevolutionized the landscape of network design across various vision tasks.\nWhile self-attention is extensively explored as a key factor in performance,\nFFN has received little attention. FFN is a versatile operator seamlessly\nintegrated into nearly all AI models to effectively harness rich\nrepresentations. Recent works also show that FFN functions like key-value\nmemories. Thus, akin to the query-key-value mechanism within self-attention,\nFFN can be viewed as a memory network, where the input serves as query and the\ntwo projection weights operate as keys and values, respectively. Based on these\nobservations, we hypothesize that the importance lies in query-key-value\nframework itself for competitive performance. To verify this, we propose\nconverting self-attention into a more FFN-like efficient token mixer with only\nconvolutions while retaining query-key-value framework, namely FFNification.\nSpecifically, FFNification replaces query-key-value interactions with large\nkernel convolutions and adopts GELU activation function instead of softmax. The\nderived token mixer, FFNified attention, serves as key-value memories for\ndetecting locally distributed spatial patterns, and operates in the opposite\ndimension to the ConvNeXt block within each corresponding sub-operation of the\nquery-key-value framework. Building upon the above two modules, we present a\nfamily of Fast-Forward Networks (FFNet). Despite being composed of only simple\noperators, FFNet outperforms sophisticated and highly specialized methods in\neach domain, with notable efficiency gains. These results validate our\nhypothesis, leading us to propose MetaMixer, a general mixer architecture that\ndoes not specify sub-operations within the query-key-value framework.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Jun 2024 07:00:14 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 05:09:16 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Yun', 'Seokju', ''], ['Lee', 'Dongheon', ''], ['Ro', 'Youngmin', '']]","extracted_entities":"[{'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'query-key-value mechanism', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"self-attention","similarity_score":0.731767118}
{"id":2407.01469,"submitter":"Fei Chen","authors":"Jianghe Cai, Gene Cheung, Fei Chen","title":"Unrolling Plug-and-Play Gradient Graph Laplacian Regularizer for Image\n  Restoration","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Generic deep learning (DL) networks for image restoration like denoising and\ninterpolation lack mathematical interpretability, require voluminous training\ndata to tune a large parameter set, and are fragile in the face of covariate\nshift. To address these shortcomings, we build interpretable networks by\nunrolling variants of a graph-based optimization algorithm of different\ncomplexities. Specifically, for a general linear image formation model, we\nfirst formulate a convex quadratic programming (QP) problem with a new\n$\\ell_2$-norm graph smoothness prior called gradient graph Laplacian\nregularizer (GGLR) that promotes piecewise planar (PWP) signal reconstruction.\nTo solve the posed unconstrained QP problem, instead of computing a linear\nsystem solution straightforwardly, we introduce a variable number of auxiliary\nvariables and correspondingly design a family of ADMM algorithms. We then\nunroll them into variable-complexity feed-forward networks, amenable to\nparameter tuning via back-propagation. More complex unrolled networks require\nmore labeled data to train more parameters, but have better overall\nperformance. The unrolled networks have periodic insertions of a graph learning\nmodule, akin to a self-attention mechanism in a transformer architecture, to\nlearn pairwise similarity structure inherent in data. Experimental results show\nthat our unrolled networks perform competitively to generic DL networks in\nimage restoration quality while using only a fraction of parameters, and\ndemonstrate improved robustness to covariate shift.\n","versions":"[{'version': 'v1', 'created': 'Mon, 1 Jul 2024 17:01:30 GMT'}, {'version': 'v2', 'created': 'Thu, 25 Jul 2024 03:12:59 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 12:10:41 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Cai', 'Jianghe', ''], ['Cheung', 'Gene', ''], ['Chen', 'Fei', '']]","extracted_entities":"[{'text': 'parameter tuning', 'label': 'Fine-tuning'}, {'text': 'self-attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"self-attention mechanism","similarity_score":0.8757837415}
{"id":2409.03332,"submitter":"Dikai Liu","authors":"Dikai Liu, Tianwei Zhang, Jianxiong Yin, Simon See","title":"Masked Sensory-Temporal Attention for Sensor Generalization in Quadruped\n  Locomotion","comments":"Accepted for ICRA 2025. Project website for video:\n  https:\/\/johnliudk.github.io\/msta\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  With the rising focus on quadrupeds, a generalized policy capable of handling\ndifferent robot models and sensor inputs becomes highly beneficial. Although\nseveral methods have been proposed to address different morphologies, it\nremains a challenge for learning-based policies to manage various combinations\nof proprioceptive information. This paper presents Masked Sensory-Temporal\nAttention (MSTA), a novel transformer-based mechanism with masking for\nquadruped locomotion. It employs direct sensor-level attention to enhance the\nsensory-temporal understanding and handle different combinations of sensor\ndata, serving as a foundation for incorporating unseen information. MSTA can\neffectively understand its states even with a large portion of missing\ninformation, and is flexible enough to be deployed on physical systems despite\nthe long input sequence.\n","versions":"[{'version': 'v1', 'created': 'Thu, 5 Sep 2024 08:11:42 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 02:28:21 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Liu', 'Dikai', ''], ['Zhang', 'Tianwei', ''], ['Yin', 'Jianxiong', ''], ['See', 'Simon', '']]","extracted_entities":"[{'text': 'Masked Sensory-Temporal\\nAttention', 'label': 'Attention mechanism'}, {'text': 'MSTA', 'label': 'Attention mechanism'}, {'text': 'direct sensor-level attention', 'label': 'Attention mechanism'}, {'text': 'MSTA', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"direct sensor-level attention","similarity_score":0.7080028057}
{"id":2409.10283,"submitter":"Sourav Sanyal","authors":"Sourav Sanyal and Kaushik Roy","title":"ASMA: An Adaptive Safety Margin Algorithm for Vision-Language Drone\n  Navigation via Scene-Aware Control Barrier Functions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI cs.SY eess.IV eess.SY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In the rapidly evolving field of vision-language navigation (VLN), ensuring\nsafety for physical agents remains an open challenge. For a human-in-the-loop\nlanguage-operated drone to navigate safely, it must understand natural language\ncommands, perceive the environment, and simultaneously avoid hazards in real\ntime. Control Barrier Functions (CBFs) are formal methods that enforce safe\noperating conditions. Model Predictive Control (MPC) is an optimization\nframework that plans a sequence of future actions over a prediction horizon,\nensuring smooth trajectory tracking while obeying constraints. In this work, we\nconsider a VLN-operated drone platform and enhance its safety by formulating a\nnovel scene-aware CBF that leverages ego-centric observations from a camera\nwhich has both Red-Green-Blue as well as Depth (RGB-D) channels. A CBF-less\nbaseline system uses a Vision-Language Encoder with cross-modal attention to\nconvert commands into an ordered sequence of landmarks. An object detection\nmodel identifies and verifies these landmarks in the captured images to\ngenerate a planned path. To further enhance safety, an Adaptive Safety Margin\nAlgorithm (ASMA) is proposed. ASMA tracks moving objects and performs\nscene-aware CBF evaluation on-the-fly, which serves as an additional constraint\nwithin the MPC framework. By continuously identifying potentially risky\nobservations, the system performs prediction in real time about unsafe\nconditions and proactively adjusts its control actions to maintain safe\nnavigation throughout the trajectory. Deployed on a Parrot Bebop2 quadrotor in\nthe Gazebo environment using the Robot Operating System (ROS), ASMA achieves\n64%-67% increase in success rates with only a slight increase (1.4%-5.8%) in\ntrajectory lengths compared to the baseline CBF-less VLN.\n","versions":"[{'version': 'v1', 'created': 'Mon, 16 Sep 2024 13:44:50 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 21:51:26 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Sanyal', 'Sourav', ''], ['Roy', 'Kaushik', '']]","extracted_entities":"[{'text': 'cross-modal attention', 'label': 'Attention mechanism'}, {'text': 'object detection\\nmodel', 'label': 'AI model'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross-modal attention","similarity_score":0.6109256744}
{"id":2409.19853,"submitter":"Benjamin Balzer","authors":"Benjamin Balzer, Benjamin Young","title":"Mechanism Design with Endogenous Perception","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"econ.TH","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We model endogenous perception of private information in single-agent\nscreening problems, with potential evaluation errors. The agent's evaluation of\ntheir type depends on their cognitive state: either attentive (i.e., they\ncorrectly perceive their type) or inattentive (i.e., they might misperceive\ntheir type). The mechanism's incentives structure determines the agent's\ncognitive state via costly investment in cognition. We derive a general\nrepresentation of attention incentives, show how they vary with the mechanism's\nallocation rule, and define a notion of accuracy of perception. In applications\nwe showcase how perception both shapes and is shaped by the design of\nmechanisms.\n","versions":"[{'version': 'v1', 'created': 'Mon, 30 Sep 2024 01:23:17 GMT'}, {'version': 'v2', 'created': 'Tue, 8 Oct 2024 05:07:14 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 05:35:29 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Balzer', 'Benjamin', ''], ['Young', 'Benjamin', '']]","extracted_entities":"[{'text': 'mechanism', 'label': 'Attention mechanism'}, {'text': 'mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"mechanism","similarity_score":0.5001471639}
{"id":2410.16032,"submitter":"Shiyu Wang","authors":"Shiyu Wang, Jiawei Li, Xiaoming Shi, Zhou Ye, Baichuan Mo, Wenze Lin,\n  Shengtong Ju, Zhixuan Chu, Ming Jin","title":"TimeMixer++: A General Time Series Pattern Machine for Universal\n  Predictive Analysis","comments":"Accepted by the 13th International Conference on Learning\n  Representations (ICLR 2025)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Time series analysis plays a critical role in numerous applications,\nsupporting tasks such as forecasting, classification, anomaly detection, and\nimputation. In this work, we present the time series pattern machine (TSPM), a\nmodel designed to excel in a broad range of time series tasks through powerful\nrepresentation and pattern extraction capabilities. Traditional time series\nmodels often struggle to capture universal patterns, limiting their\neffectiveness across diverse tasks. To address this, we define multiple scales\nin the time domain and various resolutions in the frequency domain, employing\nvarious mixing strategies to extract intricate, task-adaptive time series\npatterns. Specifically, we introduce a general-purpose TSPM that processes\nmulti-scale time series using (1) multi-resolution time imaging (MRTI), (2)\ntime image decomposition (TID), (3) multi-scale mixing (MCM), and (4)\nmulti-resolution mixing (MRM) to extract comprehensive temporal patterns. MRTI\ntransforms multi-scale time series into multi-resolution time images, capturing\npatterns across both temporal and frequency domains. TID leverages dual-axis\nattention to extract seasonal and trend patterns, while MCM hierarchically\naggregates these patterns across scales. MRM adaptively integrates all\nrepresentations across resolutions. This method achieves state-of-the-art\nperformance across 8 time series analytical tasks, consistently surpassing both\ngeneral-purpose and task-specific models. Our work marks a promising step\ntoward the next generation of TSPMs, paving the way for further advancements in\ntime series analysis.\n","versions":"[{'version': 'v1', 'created': 'Mon, 21 Oct 2024 14:06:53 GMT'}, {'version': 'v2', 'created': 'Sat, 1 Mar 2025 15:45:57 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 11:37:38 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Wang', 'Shiyu', ''], ['Li', 'Jiawei', ''], ['Shi', 'Xiaoming', ''], ['Ye', 'Zhou', ''], ['Mo', 'Baichuan', ''], ['Lin', 'Wenze', ''], ['Ju', 'Shengtong', ''], ['Chu', 'Zhixuan', ''], ['Jin', 'Ming', '']]","extracted_entities":"[{'text': 'MCM', 'label': 'LLM'}, {'text': 'MRM', 'label': 'LLM'}, {'text': 'dual-axis\\nattention', 'label': 'Attention mechanism'}, {'text': 'MCM', 'label': 'LLM'}, {'text': 'MRM', 'label': 'LLM'}]","assigned_concept":"Attention mechanism","matched_keyword":"dual-axis\nattention","similarity_score":0.6576496363}
{"id":2410.22179,"submitter":"Eric Battenberg","authors":"Eric Battenberg, RJ Skerry-Ryan, Daisy Stanton, Soroosh Mariooryad,\n  Matt Shannon, Julian Salazar, David Kao","title":"Robust and Unbounded Length Generalization in Autoregressive\n  Transformer-Based Text-to-Speech","comments":"Accepted to NAACL 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG cs.SD eess.AS","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Autoregressive (AR) Transformer-based sequence models are known to have\ndifficulty generalizing to sequences longer than those seen during training.\nWhen applied to text-to-speech (TTS), these models tend to drop or repeat words\nor produce erratic output, especially for longer utterances. In this paper, we\nintroduce enhancements aimed at AR Transformer-based encoder-decoder TTS\nsystems that address these robustness and length generalization issues. Our\napproach uses an alignment mechanism to provide cross-attention operations with\nrelative location information. The associated alignment position is learned as\na latent property of the model via backpropagation and requires no external\nalignment information during training. While the approach is tailored to the\nmonotonic nature of TTS input-output alignment, it is still able to benefit\nfrom the flexible modeling power of interleaved multi-head self- and\ncross-attention operations. A system incorporating these improvements, which we\ncall Very Attentive Tacotron, matches the naturalness and expressiveness of a\nbaseline T5-based TTS system, while eliminating problems with repeated or\ndropped words and enabling generalization to any practical utterance length.\n","versions":"[{'version': 'v1', 'created': 'Tue, 29 Oct 2024 16:17:01 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 19:21:57 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Battenberg', 'Eric', ''], ['Skerry-Ryan', 'RJ', ''], ['Stanton', 'Daisy', ''], ['Mariooryad', 'Soroosh', ''], ['Shannon', 'Matt', ''], ['Salazar', 'Julian', ''], ['Kao', 'David', '']]","extracted_entities":"[{'text': 'alignment mechanism', 'label': 'Attention mechanism'}, {'text': 'cross-attention operations', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross-attention operations","similarity_score":0.6001650095}
{"id":2411.0332,"submitter":"Xiao Hu","authors":"Xiao Hu, Ziqi Chen, Bo Peng, Daniel Adu-Ampratwum, and Xia Ning","title":"log-RRIM: Yield Prediction via Local-to-global Reaction Representation\n  Learning and Interaction Modeling","comments":"45 pages, 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"q-bio.BM cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Accurate prediction of chemical reaction yields is crucial for optimizing\norganic synthesis, potentially reducing time and resources spent on\nexperimentation. With the rise of artificial intelligence (AI), there is\ngrowing interest in leveraging AI-based methods to accelerate yield predictions\nwithout conducting in vitro experiments. We present log-RRIM, an innovative\ngraph transformer-based framework designed for predicting chemical reaction\nyields. A key feature of log-RRIM is its integration of a cross-attention\nmechanism that focuses on the interplay between reagents and reaction centers.\nThis design reflects a fundamental principle in chemical reactions: the crucial\nrole of reagents in influencing bond-breaking and formation processes, which\nultimately affect reaction yields. log-RRIM also implements a local-to-global\nreaction representation learning strategy. This approach initially captures\ndetailed molecule-level information and then models and aggregates\nintermolecular interactions. Through this hierarchical process, log-RRIM\neffectively captures how different molecular fragments contribute to and\ninfluence the overall reaction yield, regardless of their size variations.\nlog-RRIM shows superior performance in our experiments, especially for medium\nto high-yielding reactions, proving its reliability as a predictor. The\nframework's sophisticated modeling of reactant-reagent interactions and precise\ncapture of molecular fragment contributions make it a valuable tool for\nreaction planning and optimization in chemical synthesis. The data and codes of\nlog-RRIM are accessible through https:\/\/github.com\/ninglab\/Yield_log_RRIM.\n","versions":"[{'version': 'v1', 'created': 'Sun, 20 Oct 2024 18:35:56 GMT'}, {'version': 'v2', 'created': 'Fri, 8 Nov 2024 17:50:33 GMT'}, {'version': 'v3', 'created': 'Tue, 19 Nov 2024 16:49:12 GMT'}, {'version': 'v4', 'created': 'Sun, 9 Mar 2025 03:43:34 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Hu', 'Xiao', ''], ['Chen', 'Ziqi', ''], ['Peng', 'Bo', ''], ['Adu-Ampratwum', 'Daniel', ''], ['Ning', 'Xia', '']]","extracted_entities":"[{'text': 'cross-attention\\nmechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross-attention\nmechanism","similarity_score":0.8302809}
{"id":2411.07635,"submitter":"Qihang Fan","authors":"Qihang Fan, Huaibo Huang, Ran He","title":"Breaking the Low-Rank Dilemma of Linear Attention","comments":"The paper is accepted by CVPR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps:\/\/github.com\/qhfan\/RALA.\n","versions":"[{'version': 'v1', 'created': 'Tue, 12 Nov 2024 08:30:59 GMT'}, {'version': 'v2', 'created': 'Thu, 14 Nov 2024 15:40:59 GMT'}, {'version': 'v3', 'created': 'Sun, 17 Nov 2024 12:56:16 GMT'}, {'version': 'v4', 'created': 'Thu, 27 Feb 2025 03:22:41 GMT'}, {'version': 'v5', 'created': 'Tue, 11 Mar 2025 09:17:02 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Fan', 'Qihang', ''], ['Huang', 'Huaibo', ''], ['He', 'Ran', '']]","extracted_entities":"[{'text': 'Softmax attention mechanism', 'label': 'Attention mechanism'}, {'text': 'linear attention', 'label': 'Attention mechanism'}, {'text': 'Softmax attention', 'label': 'Attention mechanism'}, {'text': 'linear attention', 'label': 'Attention mechanism'}, {'text': 'linear attention', 'label': 'Attention mechanism'}, {'text': 'linear attention', 'label': 'Attention mechanism'}, {'text': 'Linear Attention', 'label': 'Attention mechanism'}, {'text': 'Softmax attention', 'label': 'Attention mechanism'}, {'text': 'linear attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Softmax attention mechanism","similarity_score":0.7745270729}
{"id":2411.10115,"submitter":"L\\'eo Dana","authors":"L\\'eo Dana, Muni Sreenivas Pydi, Yann Chevaleyre","title":"Memorization in Attention-only Transformers","comments":"16 pages, 6 figures, submitted to AISTATS 2025,","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Recent research has explored the memorization capacity of multi-head\nattention, but these findings are constrained by unrealistic limitations on the\ncontext size. We present a novel proof for language-based Transformers that\nextends the current hypothesis to any context size. Our approach improves upon\nthe state-of-the-art by achieving more effective exact memorization with an\nattention layer, while also introducing the concept of approximate memorization\nof distributions. Through experimental validation, we demonstrate that our\nproposed bounds more accurately reflect the true memorization capacity of\nlanguage models, and provide a precise comparison with prior work.\n","versions":"[{'version': 'v1', 'created': 'Fri, 15 Nov 2024 11:29:31 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 08:40:41 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Dana', 'L\u00e9o', ''], ['Pydi', 'Muni Sreenivas', ''], ['Chevaleyre', 'Yann', '']]","extracted_entities":"[{'text': 'multi-head\\nattention', 'label': 'Attention mechanism'}, {'text': 'language-based Transformers', 'label': 'Transformers'}, {'text': 'attention layer', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention layer","similarity_score":0.7063080072}
{"id":2411.10679,"submitter":"Huan Kang","authors":"Huan Kang, Hui Li, Tianyang Xu, Rui Wang, Xiao-Jun Wu, Josef Kittler","title":"SPDFusion: An Infrared and Visible Image Fusion Network Based on a\n  Non-Euclidean Representation of Riemannian Manifolds","comments":"14 pages, 12 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Euclidean representation learning methods have achieved commendable results\nin image fusion tasks, which can be attributed to their clear advantages in\nhandling with linear space. However, data collected from a realistic scene\nusually have a non-Euclidean structure, where Euclidean metric might be limited\nin representing the true data relationships, degrading fusion performance. To\naddress this issue, a novel SPD (symmetric positive definite) manifold learning\nframework is proposed for multi-modal image fusion, named SPDFusion, which\nextends the image fusion approach from the Euclidean space to the SPD\nmanifolds. Specifically, we encode images according to the Riemannian geometry\nto exploit their intrinsic statistical correlations, thereby aligning with\nhuman visual perception. Actually, the SPD matrix underpins our network\nlearning, with a cross-modal fusion strategy employed to harness\nmodality-specific dependencies and augment complementary information.\nSubsequently, an attention module is designed to process the learned weight\nmatrix, facilitating the weighting of spatial global correlation semantics via\nSPD matrix multiplication. Based on this, we design an end-to-end fusion\nnetwork based on cross-modal manifold learning. Extensive experiments on public\ndatasets demonstrate that our framework exhibits superior performance compared\nto the current state-of-the-art methods.\n","versions":"[{'version': 'v1', 'created': 'Sat, 16 Nov 2024 03:09:49 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 15:12:15 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Kang', 'Huan', ''], ['Li', 'Hui', ''], ['Xu', 'Tianyang', ''], ['Wang', 'Rui', ''], ['Wu', 'Xiao-Jun', ''], ['Kittler', 'Josef', '']]","extracted_entities":"[{'text': 'attention module', 'label': 'Attention mechanism'}, {'text': 'cross-modal manifold learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention module","similarity_score":0.6878248453}
{"id":2411.19261,"submitter":"Huiguo He","authors":"Huiguo He, Qiuyue Wang, Yuan Zhou, Yuxuan Cai, Hongyang Chao, Jian\n  Yin, Huan Yang","title":"Improving Multi-Subject Consistency in Open-Domain Image Generation with\n  Isolation and Reposition Attention","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Training-free diffusion models have achieved remarkable progress in\ngenerating multi-subject consistent images within open-domain scenarios. The\nkey idea of these methods is to incorporate reference subject information\nwithin the attention layer. However, existing methods still obtain suboptimal\nperformance when handling numerous subjects. This paper reveals two primary\nissues contributing to this deficiency. Firstly, the undesired internal\nattraction between different subjects within the target image can lead to the\nconvergence of multiple subjects into a single entity. Secondly, tokens tend to\nreference nearby tokens, which reduces the effectiveness of the attention\nmechanism when there is a significant positional difference between subjects in\nreference and target images. To address these issues, we propose a\ntraining-free diffusion model with Isolation and Reposition Attention, named\nIR-Diffusion. Specifically, Isolation Attention ensures that multiple subjects\nin the target image do not reference each other, effectively eliminating the\nsubject convergence. On the other hand, Reposition Attention involves scaling\nand repositioning subjects in both reference and target images to the same\nposition within the images. This ensures that subjects in the target image can\nbetter reference those in the reference image, thereby maintaining better\nconsistency. Extensive experiments demonstrate that IR-Diffusion significantly\nenhances multi-subject consistency, outperforming all existing methods in\nopen-domain scenarios.\n","versions":"[{'version': 'v1', 'created': 'Thu, 28 Nov 2024 16:50:30 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 13:39:55 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['He', 'Huiguo', ''], ['Wang', 'Qiuyue', ''], ['Zhou', 'Yuan', ''], ['Cai', 'Yuxuan', ''], ['Chao', 'Hongyang', ''], ['Yin', 'Jian', ''], ['Yang', 'Huan', '']]","extracted_entities":"[{'text': 'attention\\nmechanism', 'label': 'Attention mechanism'}, {'text': 'Isolation Attention', 'label': 'Attention mechanism'}, {'text': 'Reposition Attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention\nmechanism","similarity_score":1.0}
{"id":2412.00857,"submitter":"Bohai Gu","authors":"Bohai Gu, Hao Luo, Song Guo, Peiran Dong, Qihua Zhou","title":"Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion","comments":"Project page: https:\/\/nevsnev.github.io\/FloED\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available.\n","versions":"[{'version': 'v1', 'created': 'Sun, 1 Dec 2024 15:45:26 GMT'}, {'version': 'v2', 'created': 'Sun, 12 Jan 2025 05:25:06 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 13:13:11 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Gu', 'Bohai', ''], ['Luo', 'Hao', ''], ['Guo', 'Song', ''], ['Dong', 'Peiran', ''], ['Zhou', 'Qihua', '']]","extracted_entities":"[{'text': 'flow attention cache mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"flow attention cache mechanism","similarity_score":0.6020810008}
{"id":2412.02171,"submitter":"Tianyi Wang","authors":"Tianyi Wang, Zichen Wang, Cong Wang, Yuanchao Shu, Ruilong Deng, Peng\n  Cheng, Jiming Chen (Zhejiang University, Hangzhou, China)","title":"Can't Slow me Down: Learning Robust and Hardware-Adaptive Object\n  Detectors against Latency Attacks for Edge Devices","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Object detection is a fundamental enabler for many real-time downstream\napplications such as autonomous driving, augmented reality and supply chain\nmanagement. However, the algorithmic backbone of neural networks is brittle to\nimperceptible perturbations in the system inputs, which were generally known as\nmisclassifying attacks. By targeting the real-time processing capability, a new\nclass of latency attacks are reported recently. They exploit new attack\nsurfaces in object detectors by creating a computational bottleneck in the\npost-processing module, that leads to cascading failure and puts the real-time\ndownstream tasks at risks. In this work, we take an initial attempt to defend\nagainst this attack via background-attentive adversarial training that is also\ncognizant of the underlying hardware capabilities. We first draw system-level\nconnections between latency attack and hardware capacity across heterogeneous\nGPU devices. Based on the particular adversarial behaviors, we utilize\nobjectness loss as a proxy and build background attention into the adversarial\ntraining pipeline, and achieve a reasonable balance between clean and robust\naccuracy. The extensive experiments demonstrate the defense effectiveness of\nrestoring real-time processing capability from $13$ FPS to $43$ FPS on Jetson\nOrin NX, with a better trade-off between the clean and robust accuracy.\n","versions":"[{'version': 'v1', 'created': 'Tue, 3 Dec 2024 05:00:26 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 07:31:19 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Tianyi', '', 'Zhejiang University, Hangzhou, China'], ['Wang', 'Zichen', '', 'Zhejiang University, Hangzhou, China'], ['Wang', 'Cong', '', 'Zhejiang University, Hangzhou, China'], ['Shu', 'Yuanchao', '', 'Zhejiang University, Hangzhou, China'], ['Deng', 'Ruilong', '', 'Zhejiang University, Hangzhou, China'], ['Cheng', 'Peng', '', 'Zhejiang University, Hangzhou, China'], ['Chen', 'Jiming', '', 'Zhejiang University, Hangzhou, China']]","extracted_entities":"[{'text': 'background-attentive adversarial training', 'label': 'Few-shot Learning'}, {'text': 'background attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"background attention","similarity_score":0.623593092}
{"id":2412.03021,"submitter":"Tianyu Chang","authors":"Tianyu Chang, Xiaohao Chen, Zhichao Wei, Xuanpu Zhang, Qing-Guo Chen,\n  Weihua Luo, Peipei Song and Xun Yang","title":"PEMF-VTO: Point-Enhanced Video Virtual Try-on via Mask-free Paradigm","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Video Virtual Try-on aims to seamlessly transfer a reference garment onto a\ntarget person in a video while preserving both visual fidelity and temporal\ncoherence. Existing methods typically rely on inpainting masks to define the\ntry-on area, enabling accurate garment transfer for simple scenes (e.g.,\nin-shop videos). However, these mask-based approaches struggle with complex\nreal-world scenarios, as overly large and inconsistent masks often destroy\nspatial-temporal information, leading to distorted results. Mask-free methods\nalleviate this issue but face challenges in accurately determining the try-on\narea, especially for videos with dynamic body movements. To address these\nlimitations, we propose PEMF-VTO, a novel Point-Enhanced Mask-Free Video\nVirtual Try-On framework that leverages sparse point alignments to explicitly\nguide garment transfer. Our key innovation is the introduction of\npoint-enhanced guidance, which provides flexible and reliable control over both\nspatial-level garment transfer and temporal-level video coherence.\nSpecifically, we design a Point-Enhanced Transformer (PET) with two core\ncomponents: Point-Enhanced Spatial Attention (PSA), which uses frame-cloth\npoint alignments to precisely guide garment transfer, and Point-Enhanced\nTemporal Attention (PTA), which leverages frame-frame point correspondences to\nenhance temporal coherence and ensure smooth transitions across frames.\nExtensive experiments demonstrate that our PEMF-VTO outperforms\nstate-of-the-art methods, generating more natural, coherent, and visually\nappealing try-on videos, particularly for challenging in-the-wild scenarios.\n","versions":"[{'version': 'v1', 'created': 'Wed, 4 Dec 2024 04:24:15 GMT'}, {'version': 'v2', 'created': 'Thu, 5 Dec 2024 02:57:24 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 14:22:12 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chang', 'Tianyu', ''], ['Chen', 'Xiaohao', ''], ['Wei', 'Zhichao', ''], ['Zhang', 'Xuanpu', ''], ['Chen', 'Qing-Guo', ''], ['Luo', 'Weihua', ''], ['Song', 'Peipei', ''], ['Yang', 'Xun', '']]","extracted_entities":"[{'text': 'Point-Enhanced Spatial Attention', 'label': 'Attention mechanism'}, {'text': 'Point-Enhanced\\nTemporal Attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Point-Enhanced\nTemporal Attention","similarity_score":0.605589509}
{"id":2412.05829,"submitter":"Naizhu Jin","authors":"Naizhu Jin, Zhong Li, Yinggang Guo, Chao Su, Tian Zhang and Qingkai\n  Zeng","title":"SABER: Model-agnostic Backdoor Attack on Chain-of-Thought in Neural Code\n  Generation","comments":"UNDER REVIEW","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent studies have proposed integrating Chain-of-Thought (CoT) reasoning to\nfurther enhance the reliability of Code Language Models (CLMs) in generating\ncode, a step-by-step approach that breaks down complex programming tasks into\nmanageable sub-problems. Advances in this area have introduced CoT models,\nspecifically designed to integrate CoT reasoning effectively into language\nmodels, achieving notable improvements in code generation. Despite these\nadvancements, the security of CoT models has not been systematically studied.\nIn this study, we aim to fill this gap by investigating the vulnerability of\nCoT models to backdoor injection in code generation tasks. To address this, we\npropose a model-agnostic backdoor attack method SABER (Self-Attention-BasEd\nbackdooR) based on the self-attention mechanism. SABER begins by selecting a\nmalicious output as the backdoor using code mutation operations. It then\nidentifies the tokens most relevant to poisoned content by analyzing\nself-attention scores in the CodeBERT model. Finally, it mimicks user behavior\nto generate adaptive and natural triggers. Our experiments on HumanEval-CoT and\nOpenEval-CoT test sets demonstrate that CoT models are susceptible to backdoor\nattacks via data poisoning. Taking the HumanEval-CoT dataset as an example,\nSABER achieves an ASR of 80.95%, representing an improvement of 33.33% over\nRIPPLe and a substantial 4.76% enhancement compared to BadPre. Further\nevaluations using ONION for automated detection and human studies reveal that\nSABER is stealthier and harder to detect, bypassing 61.90% of automated\ndetection, with a human detection rate of just 3.17%. Our findings reveal that\nbackdoors can be injected into CoT models to manipulate downstream code\ngeneration tasks. This highlights the urgent need for further research to\nunderstand and mitigate the security vulnerabilities in CoT models.\n","versions":"[{'version': 'v1', 'created': 'Sun, 8 Dec 2024 06:36:00 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 16:31:10 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Jin', 'Naizhu', ''], ['Li', 'Zhong', ''], ['Guo', 'Yinggang', ''], ['Su', 'Chao', ''], ['Zhang', 'Tian', ''], ['Zeng', 'Qingkai', '']]","extracted_entities":"[{'text': 'SABER', 'label': 'ALBERT'}, {'text': 'self-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'SABER', 'label': 'ALBERT'}, {'text': 'adaptive and natural triggers', 'label': 'Prompting'}, {'text': 'SABER', 'label': 'ALBERT'}, {'text': 'SABER', 'label': 'ALBERT'}]","assigned_concept":"Attention mechanism","matched_keyword":"self-attention mechanism","similarity_score":0.8757837415}
{"id":2412.07446,"submitter":"Raanan Rohekar","authors":"Raanan Y. Rohekar, Yaniv Gurwicz, Sungduk Yu, Estelle Aflalo, Vasudev\n  Lal","title":"A Causal World Model Underlying Next Token Prediction in GPT","comments":"AAAI 2025 Workshop on Artificial Intelligence with Causal Techniques","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.LG stat.ML","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Are generative pre-trained transformer (GPT) models only trained to predict\nthe next token, or do they implicitly learn a world model from which a sequence\nis generated one token at a time? We examine this question by deriving a causal\ninterpretation of the attention mechanism in GPT, and suggesting a causal world\nmodel that arises from this interpretation. Furthermore, we propose that\nGPT-models, at inference time, can be utilized for zero-shot causal structure\nlearning for in-distribution sequences. Empirical evaluation is conducted in a\ncontrolled synthetic environment using the setup and rules of the Othello board\ngame. A GPT, pre-trained on real-world games played with the intention of\nwinning, is tested on synthetic data that only adheres to the game rules,\noblivious to the goal of winning. We find that the GPT model is likely to\ngenerate moves that adhere to the game rules for sequences for which a causal\nstructure is encoded in the attention mechanism with high confidence. In\ngeneral, in cases for which the GPT model generates moves that do not adhere to\nthe game rules, it also fails to capture any causal structure.\n","versions":"[{'version': 'v1', 'created': 'Tue, 10 Dec 2024 12:05:03 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 15:02:01 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Rohekar', 'Raanan Y.', ''], ['Gurwicz', 'Yaniv', ''], ['Yu', 'Sungduk', ''], ['Aflalo', 'Estelle', ''], ['Lal', 'Vasudev', '']]","extracted_entities":"[{'text': 'GPT', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'attention mechanism', 'label': 'Attention mechanism'}, {'text': 'GPT', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'zero-shot causal structure\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'GPT', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'GPT', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'attention mechanism', 'label': 'Attention mechanism'}, {'text': 'GPT', 'label': 'Generative Pre-trained Transformer (GPT)'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanism","similarity_score":1.0}
{"id":2412.07589,"submitter":"Jianzong Wu","authors":"Jianzong Wu, Chao Tang, Jingbo Wang, Yanhong Zeng, Xiangtai Li, Yunhai\n  Tong","title":"DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for\n  Customized Manga Generation","comments":"[CVPR 2025] The project page is\n  https:\/\/jianzongwu.github.io\/projects\/diffsensei\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Story visualization, the task of creating visual narratives from textual\ndescriptions, has seen progress with text-to-image generation models. However,\nthese models often lack effective control over character appearances and\ninteractions, particularly in multi-character scenes. To address these\nlimitations, we propose a new task: \\textbf{customized manga generation} and\nintroduce \\textbf{DiffSensei}, an innovative framework specifically designed\nfor generating manga with dynamic multi-character control. DiffSensei\nintegrates a diffusion-based image generator with a multimodal large language\nmodel (MLLM) that acts as a text-compatible identity adapter. Our approach\nemploys masked cross-attention to seamlessly incorporate character features,\nenabling precise layout control without direct pixel transfer. Additionally,\nthe MLLM-based adapter adjusts character features to align with panel-specific\ntext cues, allowing flexible adjustments in character expressions, poses, and\nactions. We also introduce \\textbf{MangaZero}, a large-scale dataset tailored\nto this task, containing 43,264 manga pages and 427,147 annotated panels,\nsupporting the visualization of varied character interactions and movements\nacross sequential frames. Extensive experiments demonstrate that DiffSensei\noutperforms existing models, marking a significant advancement in manga\ngeneration by enabling text-adaptable character customization. The project page\nis https:\/\/jianzongwu.github.io\/projects\/diffsensei\/.\n","versions":"[{'version': 'v1', 'created': 'Tue, 10 Dec 2024 15:24:12 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 06:23:03 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wu', 'Jianzong', ''], ['Tang', 'Chao', ''], ['Wang', 'Jingbo', ''], ['Zeng', 'Yanhong', ''], ['Li', 'Xiangtai', ''], ['Tong', 'Yunhai', '']]","extracted_entities":"[{'text': 'MLLM', 'label': 'Large Language Model'}, {'text': 'masked cross-attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"masked cross-attention","similarity_score":0.6562166214}
{"id":2412.08464,"submitter":"Mu Zhang","authors":"Mu Zhang, Yunfan Liu, Yue Liu, Yuzhong Zhao, Qixiang Ye","title":"CC-Diff: Enhancing Contextual Coherence in Remote Sensing Image\n  Synthesis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Existing image synthesis methods for natural scenes focus primarily on\nforeground control, often reducing the background to simplistic textures.\nConsequently, these approaches tend to overlook the intrinsic correlation\nbetween foreground and background, which may lead to incoherent and unrealistic\nsynthesis results in remote sensing (RS) scenarios. In this paper, we introduce\nCC-Diff, a $\\underline{\\textbf{Diff}}$usion Model-based approach for RS image\ngeneration with enhanced $\\underline{\\textbf{C}}$ontext\n$\\underline{\\textbf{C}}$oherence. Specifically, we propose a novel Dual\nRe-sampler for feature extraction, with a built-in `Context Bridge' to\nexplicitly capture the intricate interdependency between foreground and\nbackground. Moreover, we reinforce their connection by employing a\nforeground-aware attention mechanism during the generation of background\nfeatures, thereby enhancing the plausibility of the synthesized context.\nExtensive experiments show that CC-Diff outperforms state-of-the-art methods\nacross critical quality metrics, excelling in the RS domain and effectively\ngeneralizing to natural images. Remarkably, CC-Diff also shows high\ntrainability, boosting detection accuracy by 1.83 mAP on DOTA and 2.25 mAP on\nthe COCO benchmark.\n","versions":"[{'version': 'v1', 'created': 'Wed, 11 Dec 2024 15:30:06 GMT'}, {'version': 'v2', 'created': 'Mon, 23 Dec 2024 12:23:08 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 12:47:45 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhang', 'Mu', ''], ['Liu', 'Yunfan', ''], ['Liu', 'Yue', ''], ['Zhao', 'Yuzhong', ''], ['Ye', 'Qixiang', '']]","extracted_entities":"[{'text': 'Context Bridge', 'label': 'contextual Embedding'}, {'text': 'foreground-aware attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"foreground-aware attention mechanism","similarity_score":0.7554306388}
{"id":2412.09921,"submitter":"Jaehwan Jeong","authors":"Jaehwan Jeong, Sumin In, Sieun Kim, Hannie Shin, Jongheon Jeong, Sang\n  Ho Yoon, Jaewook Chung, Sangpil Kim","title":"FaceShield: Defending Facial Image against Deepfake Threats","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The rising use of deepfakes in criminal activities presents a significant\nissue, inciting widespread controversy. While numerous studies have tackled\nthis problem, most primarily focus on deepfake detection. These reactive\nsolutions are insufficient as a fundamental approach for crimes where\nauthenticity is disregarded. Existing proactive defenses also have limitations,\nas they are effective only for deepfake models based on specific Generative\nAdversarial Networks (GANs), making them less applicable in light of recent\nadvancements in diffusion-based models. In this paper, we propose a proactive\ndefense method named FaceShield, which introduces novel defense strategies\ntargeting deepfakes generated by Diffusion Models (DMs) and facilitates\ndefenses on various existing GAN-based deepfake models through facial feature\nextractor manipulations. Our approach consists of three main components: (i)\nmanipulating the attention mechanism of DMs to exclude protected facial\nfeatures during the denoising process, (ii) targeting prominent facial feature\nextraction models to enhance the robustness of our adversarial perturbation,\nand (iii) employing Gaussian blur and low-pass filtering techniques to improve\nimperceptibility while enhancing robustness against JPEG compression.\nExperimental results on the CelebA-HQ and VGGFace2-HQ datasets demonstrate that\nour method achieves state-of-the-art performance against the latest deepfake\nmodels based on DMs, while also exhibiting transferability to GANs and\nshowcasing greater imperceptibility of noise along with enhanced robustness.\n","versions":"[{'version': 'v1', 'created': 'Fri, 13 Dec 2024 07:20:35 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 08:36:55 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Jeong', 'Jaehwan', ''], ['In', 'Sumin', ''], ['Kim', 'Sieun', ''], ['Shin', 'Hannie', ''], ['Jeong', 'Jongheon', ''], ['Yoon', 'Sang Ho', ''], ['Chung', 'Jaewook', ''], ['Kim', 'Sangpil', '']]","extracted_entities":"[{'text': 'attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanism","similarity_score":1.0}
{"id":2412.12974,"submitter":"Wenhao Sun","authors":"Wenhao Sun, Benlei Cui, Xue-Mei Dong, Jingqun Tang, Yi Liu","title":"Attentive Eraser: Unleashing Diffusion Model's Object Removal Potential\n  via Self-Attention Redirection Guidance","comments":"Accepted by AAAI 2025(Oral)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recently, diffusion models have emerged as promising newcomers in the field\nof generative models, shining brightly in image generation. However, when\nemployed for object removal tasks, they still encounter issues such as\ngenerating random artifacts and the incapacity to repaint foreground object\nareas with appropriate content after removal. To tackle these problems, we\npropose Attentive Eraser, a tuning-free method to empower pre-trained diffusion\nmodels for stable and effective object removal. Firstly, in light of the\nobservation that the self-attention maps influence the structure and shape\ndetails of the generated images, we propose Attention Activation and\nSuppression (ASS), which re-engineers the self-attention mechanism within the\npre-trained diffusion models based on the given mask, thereby prioritizing the\nbackground over the foreground object during the reverse generation process.\nMoreover, we introduce Self-Attention Redirection Guidance (SARG), which\nutilizes the self-attention redirected by ASS to guide the generation process,\neffectively removing foreground objects within the mask while simultaneously\ngenerating content that is both plausible and coherent. Experiments demonstrate\nthe stability and effectiveness of Attentive Eraser in object removal across a\nvariety of pre-trained diffusion models, outperforming even training-based\nmethods. Furthermore, Attentive Eraser can be implemented in various diffusion\nmodel architectures and checkpoints, enabling excellent scalability. Code is\navailable at https:\/\/github.com\/Anonym0u3\/AttentiveEraser.\n","versions":"[{'version': 'v1', 'created': 'Tue, 17 Dec 2024 14:56:59 GMT'}, {'version': 'v2', 'created': 'Wed, 18 Dec 2024 07:52:14 GMT'}, {'version': 'v3', 'created': 'Thu, 19 Dec 2024 08:41:19 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Mar 2025 07:51:49 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Sun', 'Wenhao', ''], ['Cui', 'Benlei', ''], ['Dong', 'Xue-Mei', ''], ['Tang', 'Jingqun', ''], ['Liu', 'Yi', '']]","extracted_entities":"[{'text': 'self-attention maps', 'label': 'Attention mechanism'}, {'text': 'self-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'Attentive Eraser', 'label': 'Prompting'}, {'text': 'scalability', 'label': 'Scaling law'}]","assigned_concept":"Attention mechanism","matched_keyword":"self-attention mechanism","similarity_score":0.8757837415}
{"id":2412.15191,"submitter":"Moayed Haji-Ali","authors":"Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Ivan\n  Skorokhodov, Alper Canberk, Kwot Sin Lee, Vicente Ordonez, Sergey Tulyakov","title":"AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal\n  Audio-Video Generation","comments":"Project Page: snap-research.github.io\/AVLink\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG cs.SD eess.AS","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We propose AV-Link, a unified framework for Video-to-Audio (A2V) and\nAudio-to-Video (A2V) generation that leverages the activations of frozen video\nand audio diffusion models for temporally-aligned cross-modal conditioning. The\nkey to our framework is a Fusion Block that facilitates bidirectional\ninformation exchange between video and audio diffusion models through\ntemporally-aligned self attention operations. Unlike prior work that uses\ndedicated models for A2V and V2A tasks and relies on pretrained feature\nextractors, AV-Link achieves both tasks in a single self-contained framework,\ndirectly leveraging features obtained by the complementary modality (i.e. video\nfeatures to generate audio, or audio features to generate video). Extensive\nautomatic and subjective evaluations demonstrate that our method achieves a\nsubstantial improvement in audio-video synchronization, outperforming more\nexpensive baselines such as the MovieGen video-to-audio model.\n","versions":"[{'version': 'v1', 'created': 'Thu, 19 Dec 2024 18:57:21 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 18:30:39 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Haji-Ali', 'Moayed', ''], ['Menapace', 'Willi', ''], ['Siarohin', 'Aliaksandr', ''], ['Skorokhodov', 'Ivan', ''], ['Canberk', 'Alper', ''], ['Lee', 'Kwot Sin', ''], ['Ordonez', 'Vicente', ''], ['Tulyakov', 'Sergey', '']]","extracted_entities":"[{'text': 'Fusion Block', 'label': 'Embedding'}, {'text': 'temporally-aligned self attention operations', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"temporally-aligned self attention operations","similarity_score":0.6579380035}
{"id":2501.08137,"submitter":"Marcella Astrid","authors":"Marcella Astrid, Enjie Ghorbel, Djamila Aouada","title":"Audio-Visual Deepfake Detection With Local Temporal Inconsistencies","comments":"Accepted in ICASSP 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CR cs.MM cs.SD eess.AS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper proposes an audio-visual deepfake detection approach that aims to\ncapture fine-grained temporal inconsistencies between audio and visual\nmodalities. To achieve this, both architectural and data synthesis strategies\nare introduced. From an architectural perspective, a temporal distance map,\ncoupled with an attention mechanism, is designed to capture these\ninconsistencies while minimizing the impact of irrelevant temporal\nsubsequences. Moreover, we explore novel pseudo-fake generation techniques to\nsynthesize local inconsistencies. Our approach is evaluated against\nstate-of-the-art methods using the DFDC and FakeAVCeleb datasets, demonstrating\nits effectiveness in detecting audio-visual deepfakes.\n","versions":"[{'version': 'v1', 'created': 'Tue, 14 Jan 2025 14:15:10 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Jan 2025 09:14:14 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 10:22:54 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 11:02:33 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Astrid', 'Marcella', ''], ['Ghorbel', 'Enjie', ''], ['Aouada', 'Djamila', '']]","extracted_entities":"[{'text': 'attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanism","similarity_score":1.0}
{"id":2501.08682,"submitter":"Siqi Li","authors":"Siqi Li, Zhengkai Jiang, Jiawei Zhou, Zhihong Liu, Xiaowei Chi,\n  Haoqian Wang","title":"RealVVT: Towards Photorealistic Video Virtual Try-on via Spatio-Temporal\n  Consistency","comments":"10 pages (8 pages main text, 2 pages references), 5 figures in the\n  main text, and 4 pages supplementary materials with 3 additional figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.GR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Virtual try-on has emerged as a pivotal task at the intersection of computer\nvision and fashion, aimed at digitally simulating how clothing items fit on the\nhuman body. Despite notable progress in single-image virtual try-on (VTO),\ncurrent methodologies often struggle to preserve a consistent and authentic\nappearance of clothing across extended video sequences. This challenge arises\nfrom the complexities of capturing dynamic human pose and maintaining target\nclothing characteristics. We leverage pre-existing video foundation models to\nintroduce RealVVT, a photoRealistic Video Virtual Try-on framework tailored to\nbolster stability and realism within dynamic video contexts. Our methodology\nencompasses a Clothing & Temporal Consistency strategy, an Agnostic-guided\nAttention Focus Loss mechanism to ensure spatial consistency, and a Pose-guided\nLong Video VTO technique adept at handling extended video sequences.Extensive\nexperiments across various datasets confirms that our approach outperforms\nexisting state-of-the-art models in both single-image and video VTO tasks,\noffering a viable solution for practical applications within the realms of\nfashion e-commerce and virtual fitting environments.\n","versions":"[{'version': 'v1', 'created': 'Wed, 15 Jan 2025 09:22:38 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 10:06:51 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Li', 'Siqi', ''], ['Jiang', 'Zhengkai', ''], ['Zhou', 'Jiawei', ''], ['Liu', 'Zhihong', ''], ['Chi', 'Xiaowei', ''], ['Wang', 'Haoqian', '']]","extracted_entities":"[{'text': 'Agnostic-guided\\nAttention Focus Loss mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Agnostic-guided\nAttention Focus Loss mechanism","similarity_score":0.6943200827}
{"id":2501.10736,"submitter":"Shanwen Wang","authors":"Shanwen Wang, Xin Sun, Changrui Chen, Danfeng Hong, Jungong Han","title":"Semi-supervised Semantic Segmentation for Remote Sensing Images via\n  Multi-scale Uncertainty Consistency and Cross-Teacher-Student Attention","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Semi-supervised learning offers an appealing solution for remote sensing (RS)\nimage segmentation to relieve the burden of labor-intensive pixel-level\nlabeling. However, RS images pose unique challenges, including rich multi-scale\nfeatures and high inter-class similarity. To address these problems, this paper\nproposes a novel semi-supervised Multi-Scale Uncertainty and\nCross-Teacher-Student Attention (MUCA) model for RS image semantic segmentation\ntasks. Specifically, MUCA constrains the consistency among feature maps at\ndifferent layers of the network by introducing a multi-scale uncertainty\nconsistency regularization. It improves the multi-scale learning capability of\nsemi-supervised algorithms on unlabeled data. Additionally, MUCA utilizes a\nCross-Teacher-Student attention mechanism to guide the student network, guiding\nthe student network to construct more discriminative feature representations\nthrough complementary features from the teacher network. This design\neffectively integrates weak and strong augmentations (WA and SA) to further\nboost segmentation performance. To verify the effectiveness of our model, we\nconduct extensive experiments on ISPRS-Potsdam and LoveDA datasets. The\nexperimental results show the superiority of our method over state-of-the-art\nsemi-supervised methods. Notably, our model excels in distinguishing highly\nsimilar objects, showcasing its potential for advancing semi-supervised RS\nimage segmentation tasks.\n","versions":"[{'version': 'v1', 'created': 'Sat, 18 Jan 2025 11:57:20 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 14:18:36 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Shanwen', ''], ['Sun', 'Xin', ''], ['Chen', 'Changrui', ''], ['Hong', 'Danfeng', ''], ['Han', 'Jungong', '']]","extracted_entities":"[{'text': 'Semi-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'Cross-Teacher-Student attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Cross-Teacher-Student attention mechanism","similarity_score":0.6716021299}
{"id":2501.12235,"submitter":"Junyu Xia","authors":"Junyu Xia and Jiesong Bai and Yihang Dong","title":"DLEN: Dual Branch of Transformer for Low-Light Image Enhancement in Dual\n  Domains","comments":"some technical problems are found and need some improvement","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV eess.IV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Low-light image enhancement (LLE) aims to improve the visual quality of\nimages captured in poorly lit conditions, which often suffer from low\nbrightness, low contrast, noise, and color distortions. These issues hinder the\nperformance of computer vision tasks such as object detection, facial\nrecognition, and autonomous driving.Traditional enhancement techniques, such as\nmulti-scale fusion and histogram equalization, fail to preserve fine details\nand often struggle with maintaining the natural appearance of enhanced images\nunder complex lighting conditions. Although the Retinex theory provides a\nfoundation for image decomposition, it often amplifies noise, leading to\nsuboptimal image quality. In this paper, we propose the Dual Light Enhance\nNetwork (DLEN), a novel architecture that incorporates two distinct attention\nmechanisms, considering both spatial and frequency domains. Our model\nintroduces a learnable wavelet transform module in the illumination estimation\nphase, preserving high- and low-frequency components to enhance edge and\ntexture details. Additionally, we design a dual-branch structure that leverages\nthe power of the Transformer architecture to enhance both the illumination and\nstructural components of the image.Through extensive experiments, our model\noutperforms state-of-the-art methods on standard benchmarks.Code is available\nhere: https:\/\/github.com\/LaLaLoXX\/DLEN\n","versions":"[{'version': 'v1', 'created': 'Tue, 21 Jan 2025 15:58:16 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 02:13:57 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Xia', 'Junyu', ''], ['Bai', 'Jiesong', ''], ['Dong', 'Yihang', '']]","extracted_entities":"[{'text': 'two distinct attention\\nmechanisms', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"two distinct attention\nmechanisms","similarity_score":0.8797505498}
{"id":2501.14729,"submitter":"Xin Zhou","authors":"Xin Zhou, Dingkang Liang, Sifan Tu, Xiwu Chen, Yikang Ding, Dingyuan\n  Zhang, Feiyang Tan, Hengshuang Zhao, Xiang Bai","title":"HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene\n  Understanding and Generation","comments":"The code will be available at https:\/\/github.com\/LMD0311\/HERMES","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Driving World Models (DWMs) have become essential for autonomous driving by\nenabling future scene prediction. However, existing DWMs are limited to scene\ngeneration and fail to incorporate scene understanding, which involves\ninterpreting and reasoning about the driving environment. In this paper, we\npresent a unified Driving World Model named HERMES. We seamlessly integrate 3D\nscene understanding and future scene evolution (generation) through a unified\nframework in driving scenarios. Specifically, HERMES leverages a Bird's-Eye\nView (BEV) representation to consolidate multi-view spatial information while\npreserving geometric relationships and interactions. We also introduce world\nqueries, which incorporate world knowledge into BEV features via causal\nattention in the Large Language Model, enabling contextual enrichment for\nunderstanding and generation tasks. We conduct comprehensive studies on\nnuScenes and OmniDrive-nuScenes datasets to validate the effectiveness of our\nmethod. HERMES achieves state-of-the-art performance, reducing generation error\nby 32.4% and improving understanding metrics such as CIDEr by 8.0%. The model\nand code will be publicly released at https:\/\/github.com\/LMD0311\/HERMES.\n","versions":"[{'version': 'v1', 'created': 'Fri, 24 Jan 2025 18:59:51 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 17:58:02 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Zhou', 'Xin', ''], ['Liang', 'Dingkang', ''], ['Tu', 'Sifan', ''], ['Chen', 'Xiwu', ''], ['Ding', 'Yikang', ''], ['Zhang', 'Dingyuan', ''], ['Tan', 'Feiyang', ''], ['Zhao', 'Hengshuang', ''], ['Bai', 'Xiang', '']]","extracted_entities":"[{'text': 'causal\\nattention', 'label': 'Attention mechanism'}, {'text': 'contextual enrichment', 'label': 'contextual Embedding'}]","assigned_concept":"Attention mechanism","matched_keyword":"causal\nattention","similarity_score":0.6813284755}
{"id":2502.05383,"submitter":"Max Geier","authors":"Max Geier, Khachatur Nazaryan, Timothy Zaklama, Liang Fu","title":"Is attention all you need to solve the correlated electron problem?","comments":"10+5 pages, comments welcome; v2: update refs, extend ED results","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.str-el cond-mat.mes-hall cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  The attention mechanism has transformed artificial intelligence research by\nits ability to learn relations between objects. In this work, we explore how a\nmany-body wavefunction ansatz constructed from a large-parameter self-attention\nneural network can be used to solve the interacting electron problem in solids.\nBy a systematic neural-network variational Monte Carlo study on a moir\\'e\nquantum material, we demonstrate that the self-attention ansatz provides an\naccurate, efficient, and unbiased solution. Moreover, our numerical study finds\nthat the required number of variational parameters scales roughly as $N^2$ with\nthe number of electrons, which opens a path towards efficient large-scale\nsimulations.\n","versions":"[{'version': 'v1', 'created': 'Fri, 7 Feb 2025 23:41:41 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 02:05:44 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Geier', 'Max', ''], ['Nazaryan', 'Khachatur', ''], ['Zaklama', 'Timothy', ''], ['Fu', 'Liang', '']]","extracted_entities":"[{'text': 'The attention mechanism', 'label': 'Attention mechanism'}, {'text': \"moir\\\\'e\", 'label': 'Mistral'}]","assigned_concept":"Attention mechanism","matched_keyword":"The attention mechanism","similarity_score":0.976074338}
{"id":2502.06268,"submitter":"Wu Lin","authors":"Wu Lin, Felix Dangel, Runa Eschenhagen, Juhan Bae, Richard E. Turner,\n  Roger B. Grosse","title":"Spectral-factorized Positive-definite Curvature Learning for NN Training","comments":"technical report","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Many training methods, such as Adam(W) and Shampoo, learn a positive-definite\ncurvature matrix and apply an inverse root before preconditioning. Recently,\nnon-diagonal training methods, such as Shampoo, have gained significant\nattention; however, they remain computationally inefficient and are limited to\nspecific types of curvature information due to the costly matrix root\ncomputation via matrix decomposition. To address this, we propose a Riemannian\noptimization approach that dynamically adapts spectral-factorized\npositive-definite curvature estimates, enabling the efficient application of\narbitrary matrix roots and generic curvature learning. We demonstrate the\nefficacy and versatility of our approach in positive-definite matrix\noptimization and covariance adaptation for gradient-free optimization, as well\nas its efficiency in curvature learning for neural net training.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Feb 2025 09:07:04 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 16:22:52 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Lin', 'Wu', ''], ['Dangel', 'Felix', ''], ['Eschenhagen', 'Runa', ''], ['Bae', 'Juhan', ''], ['Turner', 'Richard E.', ''], ['Grosse', 'Roger B.', '']]","extracted_entities":"[{'text': 'Shampoo', 'label': 'ALBERT'}, {'text': 'significant\\nattention', 'label': 'Attention mechanism'}, {'text': 'generic curvature learning', 'label': 'Few-shot Learning'}, {'text': 'curvature learning', 'label': 'Zero-shot Learning'}]","assigned_concept":"Attention mechanism","matched_keyword":"significant\nattention","similarity_score":0.6931269765}
{"id":2502.10392,"submitter":"Wenxuan Guo","authors":"Wenxuan Guo, Xiuwei Xu, Ziwei Wang, Jianjiang Feng, Jie Zhou, Jiwen Lu","title":"TSP3D: Text-guided Sparse Voxel Pruning for Efficient 3D Visual\n  Grounding","comments":"Accepted at CVPR2025 with a top score","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  In this paper, we propose an efficient multi-level convolution architecture\nfor 3D visual grounding. Conventional methods are difficult to meet the\nrequirements of real-time inference due to the two-stage or point-based\narchitecture. Inspired by the success of multi-level fully sparse convolutional\narchitecture in 3D object detection, we aim to build a new 3D visual grounding\nframework following this technical route. However, as in 3D visual grounding\ntask the 3D scene representation should be deeply interacted with text\nfeatures, sparse convolution-based architecture is inefficient for this\ninteraction due to the large amount of voxel features. To this end, we propose\ntext-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D\nscene representation and text features in an efficient way by gradual region\npruning and target completion. Specifically, TGP iteratively sparsifies the 3D\nscene representation and thus efficiently interacts the voxel features with\ntext features by cross-attention. To mitigate the affect of pruning on delicate\ngeometric information, CBA adaptively fixes the over-pruned region by voxel\ncompletion with negligible computational overhead. Compared with previous\nsingle-stage methods, our method achieves top inference speed and surpasses\nprevious fastest method by 100\\% FPS. Our method also achieves state-of-the-art\naccuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on\nScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively. The code\nis available at\n\\href{https:\/\/github.com\/GWxuan\/TSP3D}{https:\/\/github.com\/GWxuan\/TSP3D}.\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Feb 2025 18:59:59 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 14:42:27 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Guo', 'Wenxuan', ''], ['Xu', 'Xiuwei', ''], ['Wang', 'Ziwei', ''], ['Feng', 'Jianjiang', ''], ['Zhou', 'Jie', ''], ['Lu', 'Jiwen', '']]","extracted_entities":"[{'text': 'cross-attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross-attention","similarity_score":0.6773566008}
{"id":2502.15488,"submitter":"Changyong Shu","authors":"Jiangyong Yu, Changyong Shu, Dawei Yang, Sifan Zhou, Zichen Yu, Xing\n  Hu, Yan Chen","title":"Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D\n  Object Detection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Camera-based multi-view 3D detection has emerged as an attractive solution\nfor autonomous driving due to its low cost and broad applicability. However,\ndespite the strong performance of PETR-based methods in 3D perception\nbenchmarks, their direct INT8 quantization for onboard deployment leads to\ndrastic accuracy drops-up to 58.2% in mAP and 36.9% in NDS on the NuScenes\ndataset. In this work, we propose Q-PETR, a quantization-aware position\nembedding transformation that re-engineers key components of the PETR framework\nto reconcile the discrepancy between the dynamic ranges of positional encodings\nand image features, and to adapt the cross-attention mechanism for low-bit\ninference. By redesigning the positional encoding module and introducing an\nadaptive quantization strategy, Q-PETR maintains floating-point performance\nwith a performance degradation of less than 1% under standard 8-bit per-tensor\npost-training quantization. Moreover, compared to its FP32 counterpart, Q-PETR\nachieves a two-fold speedup and reduces memory usage by three times, thereby\noffering a deployment-friendly solution for resource-constrained onboard\ndevices. Extensive experiments across various PETR-series models validate the\nstrong generalization and practical benefits of our approach.\n","versions":"[{'version': 'v1', 'created': 'Fri, 21 Feb 2025 14:26:23 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 15:05:41 GMT'}]","update_date":"2025-03-12","authors_parsed":"[['Yu', 'Jiangyong', ''], ['Shu', 'Changyong', ''], ['Yang', 'Dawei', ''], ['Zhou', 'Sifan', ''], ['Yu', 'Zichen', ''], ['Hu', 'Xing', ''], ['Chen', 'Yan', '']]","extracted_entities":"[{'text': 'INT8 quantization', 'label': 'quantisation'}, {'text': 'cross-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'standard 8-bit per-tensor\\npost-training quantization', 'label': 'quantisation'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross-attention mechanism","similarity_score":0.8302809}
{"id":2502.18786,"submitter":"Jun-En Ding","authors":"Jun-En Ding, Dongsheng Luo, Anna Zilverstand, Feng Liu","title":"NeuroTree: Hierarchical Functional Brain Pathway Decoding for Mental\n  Health Disorders","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.NE cs.AI q-bio.NC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Analyzing functional brain networks using functional magnetic resonance\nimaging (fMRI) is crucial for understanding psychiatric disorders and addictive\nbehaviors. While existing fMRI-based graph convolutional networks (GCNs) show\nconsiderable promise for feature extraction, they often fall short in\ncharacterizing complex relationships between brain regions and demographic\nfactors and accounting for interpretable variables linked to psychiatric\nconditions. We propose NeuroTree to overcome these limitations, integrating a\nk-hop AGE-GCN with neural ordinary differential equations (ODEs). This\nframework leverages an attention mechanism to optimize functional connectivity\n(FC), thereby enhancing dynamic FC feature learning for brain disease\nclassification. Furthermore, NeuroTree effectively decodes fMRI network\nfeatures into tree structures, which improves the capture of high-order brain\nregional pathway features and enables the identification of hierarchical neural\nbehavioral patterns essential for understanding disease-related brain\nsubnetworks. Our empirical evaluations demonstrate that NeuroTree achieves\nstate-of-the-art performance across two distinct mental disorder datasets and\nprovides valuable insights into age-related deterioration patterns. These\nfindings underscore the model's efficacy in predicting psychiatric disorders\nand elucidating their underlying neural mechanisms.\n","versions":"[{'version': 'v1', 'created': 'Wed, 26 Feb 2025 03:42:58 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 03:03:09 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Ding', 'Jun-En', ''], ['Luo', 'Dongsheng', ''], ['Zilverstand', 'Anna', ''], ['Liu', 'Feng', '']]","extracted_entities":"[{'text': 'attention mechanism', 'label': 'Attention mechanism'}, {'text': 'dynamic FC feature learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanism","similarity_score":1.0}
{"id":2503.02459,"submitter":"Dengke Zhang","authors":"Dengke Zhang, Quan Tang, Fagui Liu, Haiqing Mei, C. L. Philip Chen","title":"Exploring Token-Level Augmentation in Vision Transformer for\n  Semi-Supervised Semantic Segmentation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Semi-supervised semantic segmentation has witnessed remarkable advancements\nin recent years. However, existing algorithms are based on convolutional neural\nnetworks and directly applying them to Vision Transformers poses certain\nlimitations due to conceptual disparities. To this end, we propose TokenMix, a\ndata augmentation technique specifically designed for semi-supervised semantic\nsegmentation with Vision Transformers. TokenMix aligns well with the global\nattention mechanism by mixing images at the token level, enhancing learning\ncapability for contextual information among image patches. We further\nincorporate image augmentation and feature augmentation to promote the\ndiversity of augmentation. Moreover, to enhance consistency regularization, we\npropose a dual-branch framework where each branch applies image and feature\naugmentation to the input image. We conduct extensive experiments across\nmultiple benchmark datasets, including Pascal VOC 2012, Cityscapes, and COCO.\nResults suggest that the proposed method outperforms state-of-the-art\nalgorithms with notably observed accuracy improvement, especially under limited\nfine annotations.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 10:09:46 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 12:48:54 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhang', 'Dengke', ''], ['Tang', 'Quan', ''], ['Liu', 'Fagui', ''], ['Mei', 'Haiqing', ''], ['Chen', 'C. L. Philip', '']]","extracted_entities":"[{'text': 'Vision Transformers', 'label': 'Transformers'}, {'text': 'TokenMix', 'label': 'contextual Embedding'}, {'text': 'Vision Transformers', 'label': 'Transformers'}, {'text': 'TokenMix', 'label': 'contextual Embedding'}, {'text': 'global\\nattention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"global\nattention mechanism","similarity_score":0.8699287176}
{"id":2503.04823,"submitter":"Yuheng Kuang","authors":"Yuheng Kuang, Zhengning Wang, Jianping Zhang, Zhenyu Shi, Yuding Zhang","title":"DA-STGCN: 4D Trajectory Prediction Based on Spatiotemporal Feature\n  Extraction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The importance of four-dimensional (4D) trajectory prediction within air\ntraffic management systems is on the rise. Key operations such as conflict\ndetection and resolution, aircraft anomaly monitoring, and the management of\ncongested flight paths are increasingly reliant on this foundational\ntechnology, underscoring the urgent demand for intelligent solutions. The\ndynamics in airport terminal zones and crowded airspaces are intricate and\never-changing; however, current methodologies do not sufficiently account for\nthe interactions among aircraft. To tackle these challenges, we propose\nDA-STGCN, an innovative spatiotemporal graph convolutional network that\nintegrates a dual attention mechanism. Our model reconstructs the adjacency\nmatrix through a self-attention approach, enhancing the capture of node\ncorrelations, and employs graph attention to distill spatiotemporal\ncharacteristics, thereby generating a probabilistic distribution of predicted\ntrajectories. This novel adjacency matrix, reconstructed with the\nself-attention mechanism, is dynamically optimized throughout the network's\ntraining process, offering a more nuanced reflection of the inter-node\nrelationships compared to traditional algorithms. The performance of the model\nis validated on two ADS-B datasets, one near the airport terminal area and the\nother in dense airspace. Experimental results demonstrate a notable improvement\nover current 4D trajectory prediction methods, achieving a 20% and 30%\nreduction in the Average Displacement Error (ADE) and Final Displacement Error\n(FDE), respectively. The incorporation of a Dual-Attention module has been\nshown to significantly enhance the extraction of node correlations, as verified\nby ablation experiments.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 03:42:49 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 03:39:44 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Kuang', 'Yuheng', ''], ['Wang', 'Zhengning', ''], ['Zhang', 'Jianping', ''], ['Shi', 'Zhenyu', ''], ['Zhang', 'Yuding', '']]","extracted_entities":"[{'text': 'dual attention mechanism', 'label': 'Attention mechanism'}, {'text': 'self-attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"self-attention mechanism","similarity_score":0.8757837415}
{"id":2503.05858,"submitter":"Jiachen Luo","authors":"Jiachen Luo, Huy Phan, Lin Wang, Joshua D. Reiss","title":"Bimodal Connection Attention Fusion for Speech Emotion Recognition","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.AI cs.CL cs.MM eess.AS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multi-modal emotion recognition is challenging due to the difficulty of\nextracting features that capture subtle emotional differences. Understanding\nmulti-modal interactions and connections is key to building effective bimodal\nspeech emotion recognition systems. In this work, we propose Bimodal Connection\nAttention Fusion (BCAF) method, which includes three main modules: the\ninteractive connection network, the bimodal attention network, and the\ncorrelative attention network. The interactive connection network uses an\nencoder-decoder architecture to model modality connections between audio and\ntext while leveraging modality-specific features. The bimodal attention network\nenhances semantic complementation and exploits intra- and inter-modal\ninteractions. The correlative attention network reduces cross-modal noise and\ncaptures correlations between audio and text. Experiments on the MELD and\nIEMOCAP datasets demonstrate that the proposed BCAF method outperforms existing\nstate-of-the-art baselines.\n","versions":"[{'version': 'v1', 'created': 'Sat, 8 Mar 2025 10:20:57 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 19:50:21 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Luo', 'Jiachen', ''], ['Phan', 'Huy', ''], ['Wang', 'Lin', ''], ['Reiss', 'Joshua D.', '']]","extracted_entities":"[{'text': 'interactive connection network', 'label': 'Attention mechanism'}, {'text': 'bimodal attention network', 'label': 'Attention mechanism'}, {'text': 'correlative attention network', 'label': 'Attention mechanism'}, {'text': 'interactive connection network', 'label': 'Attention mechanism'}, {'text': 'bimodal attention network', 'label': 'Attention mechanism'}, {'text': 'correlative attention network', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"bimodal attention network","similarity_score":0.6283652782}
{"id":2503.0617,"submitter":"Junha Chun","authors":"Youngjoon Jeong, Junha Chun, Soonwoo Cha, Taesup Kim","title":"Object-Centric World Model for Language-Guided Manipulation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CV cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  A world model is essential for an agent to predict the future and plan in\ndomains such as autonomous driving and robotics. To achieve this, recent\nadvancements have focused on video generation, which has gained significant\nattention due to the impressive success of diffusion models. However, these\nmodels require substantial computational resources. To address these\nchallenges, we propose a world model leveraging object-centric representation\nspace using slot attention, guided by language instructions. Our model\nperceives the current state as an object-centric representation and predicts\nfuture states in this representation space conditioned on natural language\ninstructions. This approach results in a more compact and computationally\nefficient model compared to diffusion-based generative alternatives.\nFurthermore, it flexibly predicts future states based on language instructions,\nand offers a significant advantage in manipulation tasks where object\nrecognition is crucial. In this paper, we demonstrate that our latent\npredictive world model surpasses generative world models in visuo-linguo-motor\ncontrol tasks, achieving superior sample and computation efficiency. We also\ninvestigate the generalization performance of the proposed method and explore\nvarious strategies for predicting actions using object-centric representations.\n","versions":"[{'version': 'v1', 'created': 'Sat, 8 Mar 2025 11:17:37 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 13:52:50 GMT'}]","update_date":"2025-03-13","authors_parsed":"[['Jeong', 'Youngjoon', ''], ['Chun', 'Junha', ''], ['Cha', 'Soonwoo', ''], ['Kim', 'Taesup', '']]","extracted_entities":"[{'text': 'slot attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"slot attention","similarity_score":0.6686759591}
{"id":2503.06397,"submitter":"Yanyu Zhu","authors":"Yanyu Zhu, Licheng Bai, Jintao Xu, Jiwei Tang, Hai-tao Zheng","title":"Removing Averaging: Personalized Lip-Sync Driven Characters Based on\n  Identity Adapter","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent advances in diffusion-based lip-syncing generative models have\ndemonstrated their ability to produce highly synchronized talking face videos\nfor visual dubbing. Although these models excel at lip synchronization, they\noften struggle to maintain fine-grained control over facial details in\ngenerated images. In this work, we identify \"lip averaging\" phenomenon where\nthe model fails to preserve subtle facial details when dubbing unseen\nin-the-wild videos. This issue arises because the commonly used UNet backbone\nprimarily integrates audio features into visual representations in the latent\nspace via cross-attention mechanisms and multi-scale fusion, but it struggles\nto retain fine-grained lip details in the generated faces. To address this\nissue, we propose UnAvgLip, which extracts identity embeddings from reference\nvideos to generate highly faithful facial sequences while maintaining accurate\nlip synchronization. Specifically, our method comprises two primary components:\n(1) an Identity Perceiver module that encodes facial embeddings to align with\nconditioned audio features; and (2) an ID-CrossAttn module that injects facial\nembeddings into the generation process, enhancing model's capability of\nidentity retention. Extensive experiments demonstrate that, at a modest\ntraining and inference cost, UnAvgLip effectively mitigates the \"averaging\"\nphenomenon in lip inpainting, significantly preserving unique facial\ncharacteristics while maintaining precise lip synchronization. Compared with\nthe original approach, our method demonstrates significant improvements of 5%\non the identity consistency metric and 2% on the SSIM metric across two\nbenchmark datasets (HDTF and LRW).\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 02:36:31 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Zhu', 'Yanyu', ''], ['Bai', 'Licheng', ''], ['Xu', 'Jintao', ''], ['Tang', 'Jiwei', ''], ['Zheng', 'Hai-tao', '']]","extracted_entities":"[{'text': 'cross-attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'identity embeddings', 'label': 'Embedding'}, {'text': 'facial embeddings', 'label': 'Embedding'}, {'text': 'facial\\nembeddings', 'label': 'Embedding'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross-attention mechanisms","similarity_score":0.8177332282}
{"id":2503.06405,"submitter":"Jiachen Luo","authors":"Jiachen Luo, Huy Phan, Lin Wang, Joshua Reiss","title":"Heterogeneous bimodal attention fusion for speech emotion recognition","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.AI eess.AS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multi-modal emotion recognition in conversations is a challenging problem due\nto the complex and complementary interactions between different modalities.\nAudio and textual cues are particularly important for understanding emotions\nfrom a human perspective. Most existing studies focus on exploring interactions\nbetween audio and text modalities at the same representation level. However, a\ncritical issue is often overlooked: the heterogeneous modality gap between\nlow-level audio representations and high-level text representations. To address\nthis problem, we propose a novel framework called Heterogeneous Bimodal\nAttention Fusion (HBAF) for multi-level multi-modal interaction in\nconversational emotion recognition. The proposed method comprises three key\nmodules: the uni-modal representation module, the multi-modal fusion module,\nand the inter-modal contrastive learning module. The uni-modal representation\nmodule incorporates contextual content into low-level audio representations to\nbridge the heterogeneous multi-modal gap, enabling more effective fusion. The\nmulti-modal fusion module uses dynamic bimodal attention and a dynamic gating\nmechanism to filter incorrect cross-modal relationships and fully exploit both\nintra-modal and inter-modal interactions. Finally, the inter-modal contrastive\nlearning module captures complex absolute and relative interactions between\naudio and text modalities. Experiments on the MELD and IEMOCAP datasets\ndemonstrate that the proposed HBAF method outperforms existing state-of-the-art\nbaselines.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 02:50:49 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Luo', 'Jiachen', ''], ['Phan', 'Huy', ''], ['Wang', 'Lin', ''], ['Reiss', 'Joshua', '']]","extracted_entities":"[{'text': 'uni-modal representation module', 'label': 'contextual Embedding'}, {'text': 'multi-modal fusion module', 'label': 'contextual Embedding'}, {'text': 'inter-modal contrastive learning module', 'label': 'contextual Embedding'}, {'text': 'uni-modal representation\\nmodule', 'label': 'contextual Embedding'}, {'text': 'multi-modal fusion module', 'label': 'contextual Embedding'}, {'text': 'dynamic bimodal attention', 'label': 'Attention mechanism'}, {'text': 'dynamic gating\\nmechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"dynamic bimodal attention","similarity_score":0.6811416149}
{"id":2503.06427,"submitter":"Yao-Xiang Ding","authors":"Yu Jin, Jingming Liu, Zhexu Luo, Yifei Peng, Ziang Qin, Wang-Zhou Dai,\n  Yao-Xiang Ding, Kun Zhou","title":"Pre-Training Meta-Rule Selection Policy for Visual Generative Abductive\n  Learning","comments":"Published as a conference paper at IJCLR'24","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Visual generative abductive learning studies jointly training symbol-grounded\nneural visual generator and inducing logic rules from data, such that after\nlearning, the visual generation process is guided by the induced logic rules. A\nmajor challenge for this task is to reduce the time cost of logic abduction\nduring learning, an essential step when the logic symbol set is large and the\nlogic rule to induce is complicated. To address this challenge, we propose a\npre-training method for obtaining meta-rule selection policy for the recently\nproposed visual generative learning approach AbdGen [Peng et al., 2023], aiming\nat significantly reducing the candidate meta-rule set and pruning the search\nspace. The selection model is built based on the embedding representation of\nboth symbol grounding of cases and meta-rules, which can be effectively\nintegrated with both neural model and logic reasoning system. The pre-training\nprocess is done on pure symbol data, not involving symbol grounding learning of\nraw visual inputs, making the entire learning process low-cost. An additional\ninteresting observation is that the selection policy can rectify symbol\ngrounding errors unseen during pre-training, which is resulted from the\nmemorization ability of attention mechanism and the relative stability of\nsymbolic patterns. Experimental results show that our method is able to\neffectively address the meta-rule selection problem for visual abduction,\nboosting the efficiency of visual generative abductive learning. Code is\navailable at https:\/\/github.com\/future-item\/metarule-select.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 03:41:11 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Jin', 'Yu', ''], ['Liu', 'Jingming', ''], ['Luo', 'Zhexu', ''], ['Peng', 'Yifei', ''], ['Qin', 'Ziang', ''], ['Dai', 'Wang-Zhou', ''], ['Ding', 'Yao-Xiang', ''], ['Zhou', 'Kun', '']]","extracted_entities":"[{'text': 'embedding representation', 'label': 'Embedding'}, {'text': 'attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanism","similarity_score":1.0}
{"id":2503.06451,"submitter":"Basudha Pal","authors":"Basudha Pal, Siyuan (Cyan) Huang, Rama Chellappa","title":"A Quantitative Evaluation of the Expressivity of BMI, Pose and Gender in\n  Body Embeddings for Recognition and Identification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Person Re-identification (ReID) systems identify individuals across images or\nvideo frames and play a critical role in various real-world applications.\nHowever, many ReID methods are influenced by sensitive attributes such as\ngender, pose, and body mass index (BMI), which vary in uncontrolled\nenvironments, leading to biases and reduced generalization. To address this, we\nextend the concept of expressivity to the body recognition domain to better\nunderstand how ReID models encode these attributes. Expressivity, defined as\nthe mutual information between feature vector representations and specific\nattributes, is computed using a secondary neural network that takes feature and\nattribute vectors as inputs. This provides a quantitative framework for\nanalyzing the extent to which sensitive attributes are embedded in the model's\nrepresentations. We apply expressivity analysis to SemReID, a state-of-the-art\nself-supervised ReID model, and find that BMI consistently exhibits the highest\nexpressivity scores in the model's final layers, underscoring its dominant role\nin feature encoding. In the final attention layer of the trained network, the\nexpressivity order for body attributes is BMI > Pitch > Yaw > Gender,\nhighlighting their relative importance in learned representations.\nAdditionally, expressivity values evolve progressively across network layers\nand training epochs, reflecting a dynamic encoding of attributes during feature\nextraction. These insights emphasize the influence of body-related attributes\non ReID models and provide a systematic methodology for identifying and\nmitigating attribute-driven biases. By leveraging expressivity analysis, we\noffer valuable tools to enhance the fairness, robustness, and generalization of\nReID systems in diverse real-world settings.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 05:15:54 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Pal', 'Basudha', '', 'Cyan'], ['Siyuan', '', '', 'Cyan'], ['Huang', '', ''], ['Chellappa', 'Rama', '']]","extracted_entities":"[{'text': 'final attention layer', 'label': 'Attention mechanism'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Attention mechanism","matched_keyword":"final attention layer","similarity_score":0.6932016611}
{"id":2503.06473,"submitter":"Hanze Li","authors":"Hanze Li, Xiande Huang","title":"Enhancing Layer Attention Efficiency through Pruning Redundant\n  Retrievals","comments":"11 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Growing evidence suggests that layer attention mechanisms, which enhance\ninteraction among layers in deep neural networks, have significantly advanced\nnetwork architectures. However, existing layer attention methods suffer from\nredundancy, as attention weights learned by adjacent layers often become highly\nsimilar. This redundancy causes multiple layers to extract nearly identical\nfeatures, reducing the model's representational capacity and increasing\ntraining time. To address this issue, we propose a novel approach to quantify\nredundancy by leveraging the Kullback-Leibler (KL) divergence between adjacent\nlayers. Additionally, we introduce an Enhanced Beta Quantile Mapping (EBQM)\nmethod that accurately identifies and skips redundant layers, thereby\nmaintaining model stability. Our proposed Efficient Layer Attention (ELA)\narchitecture, improves both training efficiency and overall performance,\nachieving a 30\\% reduction in training time while enhancing performance in\ntasks such as image classification and object detection.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 06:20:11 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Li', 'Hanze', ''], ['Huang', 'Xiande', '']]","extracted_entities":"[{'text': 'layer attention mechanisms', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"layer attention mechanisms","similarity_score":0.8598178029}
{"id":2503.06505,"submitter":"Xirui Hu","authors":"Xirui Hu, Jiahao Wang, Hao Chen, Weizhan Zhang, Benqi Wang, Yikun Li,\n  Haishun Nan","title":"DynamicID: Zero-Shot Multi-ID Image Personalization with Flexible Facial\n  Editability","comments":"17 pages, 16 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advancements in text-to-image generation have spurred interest in\npersonalized human image generation, which aims to create novel images\nfeaturing specific human identities as reference images indicate. Although\nexisting methods achieve high-fidelity identity preservation, they often\nstruggle with limited multi-ID usability and inadequate facial editability. We\npresent DynamicID, a tuning-free framework supported by a dual-stage training\nparadigm that inherently facilitates both single-ID and multi-ID personalized\ngeneration with high fidelity and flexible facial editability. Our key\ninnovations include: 1) Semantic-Activated Attention (SAA), which employs\nquery-level activation gating to minimize disruption to the original model when\ninjecting ID features and achieve multi-ID personalization without requiring\nmulti-ID samples during training. 2) Identity-Motion Reconfigurator (IMR),\nwhich leverages contrastive learning to effectively disentangle and re-entangle\nfacial motion and identity features, thereby enabling flexible facial editing.\nAdditionally, we have developed a curated VariFace-10k facial dataset,\ncomprising 10k unique individuals, each represented by 35 distinct facial\nimages. Experimental results demonstrate that DynamicID outperforms\nstate-of-the-art methods in identity fidelity, facial editability, and multi-ID\npersonalization capability.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 08:16:19 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['Hu', 'Xirui', ''], ['Wang', 'Jiahao', ''], ['Chen', 'Hao', ''], ['Zhang', 'Weizhan', ''], ['Wang', 'Benqi', ''], ['Li', 'Yikun', ''], ['Nan', 'Haishun', '']]","extracted_entities":"[{'text': 'Semantic-Activated Attention', 'label': 'Attention mechanism'}, {'text': 'contrastive learning', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Semantic-Activated Attention","similarity_score":0.645902276}
{"id":2503.06568,"submitter":"Qiyuan He","authors":"Qiyuan He and Angela Yao","title":"Conceptrol: Concept Control of Zero-shot Personalized Image Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Personalized image generation with text-to-image diffusion models generates\nunseen images based on reference image content. Zero-shot adapter methods such\nas IP-Adapter and OminiControl are especially interesting because they do not\nrequire test-time fine-tuning. However, they struggle to balance preserving\npersonalized content and adherence to the text prompt. We identify a critical\ndesign flaw resulting in this performance gap: current adapters inadequately\nintegrate personalization images with the textual descriptions. The generated\nimages, therefore, replicate the personalized content rather than adhere to the\ntext prompt instructions. Yet the base text-to-image has strong conceptual\nunderstanding capabilities that can be leveraged.\n  We propose Conceptrol, a simple yet effective framework that enhances\nzero-shot adapters without adding computational overhead. Conceptrol constrains\nthe attention of visual specification with a textual concept mask that improves\nsubject-driven generation capabilities. It achieves as much as 89% improvement\non personalization benchmarks over the vanilla IP-Adapter and can even\noutperform fine-tuning approaches such as Dreambooth LoRA. The source code is\navailable at https:\/\/github.com\/QY-H00\/Conceptrol.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 11:54:08 GMT'}]","update_date":"2025-03-11","authors_parsed":"[['He', 'Qiyuan', ''], ['Yao', 'Angela', '']]","extracted_entities":"[{'text': 'test-time fine-tuning', 'label': 'Fine-tuning'}, {'text': 'text prompt', 'label': 'Prompting'}, {'text': 'text prompt', 'label': 'Prompting'}, {'text': 'attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention","similarity_score":0.7383304834}
