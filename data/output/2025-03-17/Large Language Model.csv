id,submitter,authors,title,comments,journal-ref,doi,report-no,categories,license,abstract,versions,update_date,authors_parsed,extracted_entities,assigned_concept,matched_keyword,similarity_score
2305.18226,Christoforos Vasilatos,"Christoforos Vasilatos, Manaar Alam, Talal Rahwan, Yasir Zaki and
  Michail Maniatakos","HowkGPT: Investigating the Detection of ChatGPT-generated University
  Student Homework through Context-Aware Perplexity Analysis",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As the use of Large Language Models (LLMs) in text generation tasks
proliferates, concerns arise over their potential to compromise academic
integrity. The education sector currently tussles with distinguishing
student-authored homework assignments from AI-generated ones. This paper
addresses the challenge by introducing HowkGPT, designed to identify homework
assignments generated by AI. HowkGPT is built upon a dataset of academic
assignments and accompanying metadata [17] and employs a pretrained LLM to
compute perplexity scores for student-authored and ChatGPT-generated responses.
These scores then assist in establishing a threshold for discerning the origin
of a submitted assignment. Given the specificity and contextual nature of
academic work, HowkGPT further refines its analysis by defining
category-specific thresholds derived from the metadata, enhancing the precision
of the detection. This study emphasizes the critical need for effective
strategies to uphold academic integrity amidst the growing influence of LLMs
and provides an approach to ensuring fair and accurate grading in educational
institutions.
","[{'version': 'v1', 'created': 'Fri, 26 May 2023 11:07:25 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Jun 2023 11:43:44 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 08:08:05 GMT'}]",2025-03-12,"[['Vasilatos', 'Christoforos', ''], ['Alam', 'Manaar', ''], ['Rahwan', 'Talal', ''], ['Zaki', 'Yasir', ''], ['Maniatakos', 'Michail', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'academic\nintegrity', 'label': 'AI Ethics'}, {'text': 'academic integrity', 'label': 'AI Ethics'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2310.11829,Jiawei Liu,"Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei
  Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S. Yu, Chuan Shi","Graph Foundation Models: Concepts, Opportunities and Challenges","This is the author's version of the accepted paper (not the
  IEEE-published version). Citation information: DOI
  10.1109/TPAMI.2025.3548729. For access to the final edited and published
  article, please follow the link provided:
  https://ieeexplore.ieee.org/document/10915556",,10.1109/TPAMI.2025.3548729,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Foundation models have emerged as critical components in a variety of
artificial intelligence applications, and showcase significant success in
natural language processing and several other domains. Meanwhile, the field of
graph machine learning is witnessing a paradigm transition from shallow methods
to more sophisticated deep learning approaches. The capabilities of foundation
models in generalization and adaptation motivate graph machine learning
researchers to discuss the potential of developing a new graph learning
paradigm. This paradigm envisions models that are pre-trained on extensive
graph data and can be adapted for various graph tasks. Despite this burgeoning
interest, there is a noticeable lack of clear definitions and systematic
analyses pertaining to this new domain. To this end, this article introduces
the concept of Graph Foundation Models (GFMs), and offers an exhaustive
explanation of their key characteristics and underlying technologies. We
proceed to classify the existing work related to GFMs into three distinct
categories, based on their dependence on graph neural networks and large
language models. In addition to providing a thorough review of the current
state of GFMs, this article also outlooks potential avenues for future research
in this rapidly evolving domain.
","[{'version': 'v1', 'created': 'Wed, 18 Oct 2023 09:31:21 GMT'}, {'version': 'v2', 'created': 'Sat, 2 Dec 2023 08:36:17 GMT'}, {'version': 'v3', 'created': 'Mon, 1 Jul 2024 02:06:42 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Mar 2025 16:14:30 GMT'}]",2025-03-11,"[['Liu', 'Jiawei', ''], ['Yang', 'Cheng', ''], ['Lu', 'Zhiyuan', ''], ['Chen', 'Junze', ''], ['Li', 'Yibo', ''], ['Zhang', 'Mengmei', ''], ['Bai', 'Ting', ''], ['Fang', 'Yuan', ''], ['Sun', 'Lichao', ''], ['Yu', 'Philip S.', ''], ['Shi', 'Chuan', '']]","[{'text': 'Foundation models', 'label': 'Foundation Model'}, {'text': 'foundation\nmodels', 'label': 'Foundation Model'}, {'text': 'Graph Foundation Models', 'label': 'Foundation Model'}, {'text': 'GFMs', 'label': 'Foundation Model'}, {'text': 'large\nlanguage models', 'label': 'Large Language Model'}, {'text': 'GFMs', 'label': 'Foundation Model'}]",Large Language Model,"large
language models",0.9664971828460693
2312.10321,Fuheng Zhao,"Fuheng Zhao, Jiayue Chen, Lawrence Lim, Ishtiyaque Ahmad, Divyakant
  Agrawal, Amr El Abbadi",LLM-SQL-Solver: Can LLMs Determine SQL Equivalence?,,,,,cs.DB cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Judging the equivalence between two SQL queries is a fundamental problem with
many practical applications in data management and SQL generation (i.e.,
evaluating the quality of generated SQL queries in text-to-SQL task). While the
research community has reasoned about SQL equivalence for decades, it poses
considerable difficulties and no complete solutions exist. Recently, Large
Language Models (LLMs) have shown strong reasoning capability in conversation,
question answering and solving mathematics challenges. In this paper, we study
if LLMs can be used to determine the equivalence between SQL queries under two
notions of SQL equivalence (semantic equivalence and relaxed equivalence). To
assist LLMs in generating high quality responses, we present two prompting
techniques: Miniature & Mull and Explain & Compare. The former technique is
used to evaluate the semantic equivalence in which it asks LLMs to execute a
query on a simple database instance and then explore if a counterexample exists
by modifying the database. The latter technique is used to evaluate the relaxed
equivalence in which it asks LLMs to explain the queries and then compare if
they contain significant logical differences. Our experiments demonstrate using
our techniques, LLMs is a promising tool to help data engineers in writing
semantically equivalent SQL queries, however challenges still persist, and is a
better metric for evaluating SQL generation than the popular execution
accuracy.
","[{'version': 'v1', 'created': 'Sat, 16 Dec 2023 05:01:23 GMT'}, {'version': 'v2', 'created': 'Wed, 17 Jan 2024 20:11:38 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Jun 2024 20:19:00 GMT'}, {'version': 'v4', 'created': 'Wed, 12 Mar 2025 03:16:27 GMT'}]",2025-03-13,"[['Zhao', 'Fuheng', ''], ['Chen', 'Jiayue', ''], ['Lim', 'Lawrence', ''], ['Ahmad', 'Ishtiyaque', ''], ['Agrawal', 'Divyakant', ''], ['Abbadi', 'Amr El', '']]","[{'text': 'Large\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Miniature & Mull', 'label': 'Prompting'}, {'text': 'Explain & Compare', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,"Large
Language Models",0.9664971828460693
2312.15960,Jingyao Li,"Jingyao Li, Pengguang Chen, Bin Xia, Hong Xu, Jiaya Jia","MoTCoder: Elevating Large Language Models with Modular of Thought for
  Challenging Programming Tasks","Model: https://huggingface.co/JingyaoLi/MoTCoder-15B-v1.0. Code:
  https://github.com/dvlab-research/MoTCoder",,,,cs.LG cs.PL cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have showcased impressive capabilities in
handling straightforward programming tasks. However, their performance tends to
falter when confronted with more challenging programming problems. We observe
that conventional models often generate solutions as monolithic code blocks,
restricting their effectiveness in tackling intricate questions. To overcome
this limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a
pioneering framework for MoT instruction tuning, designed to promote the
decomposition of tasks into logical sub-tasks and sub-modules. Our
investigations reveal that, through the cultivation and utilization of
sub-modules, MoTCoder significantly improves both the modularity and
correctness of the generated solutions, leading to substantial relative pass@1
improvements of 12.9% on APPS and 9.43% on CodeContests. Our codes are
available at https://github.com/dvlab-research/MoTCoder.
","[{'version': 'v1', 'created': 'Tue, 26 Dec 2023 08:49:57 GMT'}, {'version': 'v2', 'created': 'Fri, 5 Jan 2024 10:33:32 GMT'}, {'version': 'v3', 'created': 'Thu, 22 Aug 2024 06:24:12 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 05:36:12 GMT'}]",2025-03-14,"[['Li', 'Jingyao', ''], ['Chen', 'Pengguang', ''], ['Xia', 'Bin', ''], ['Xu', 'Hong', ''], ['Jia', 'Jiaya', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2312.16893,Zhecheng Sheng,"Zhecheng Sheng, Tianhao Zhang, Chen Jiang, Dongyeop Kang",BBScore: A Brownian Bridge Based Metric for Assessing Text Coherence,"Accepted to the 38th Annual AAAI Conference on Artificial
  Intelligence (AAAI-24)",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Measuring the coherence of text is a vital aspect of evaluating the quality
of written content. Recent advancements in neural coherence modeling have
demonstrated their efficacy in capturing entity coreference and discourse
relations, thereby enhancing coherence evaluation. However, many existing
methods heavily depend on static embeddings or focus narrowly on nearby
context, constraining their capacity to measure the overarching coherence of
long texts. In this paper, we posit that coherent texts inherently manifest a
sequential and cohesive interplay among sentences, effectively conveying the
central theme, purpose, or standpoint. To explore this abstract relationship,
we introduce the ""BBScore,"" a novel reference-free metric grounded in Brownian
bridge theory for assessing text coherence. Our findings showcase that when
synergized with a simple additional classification component, this metric
attains a performance level comparable to state-of-the-art techniques on
standard artificial discrimination tasks. We also establish in downstream tasks
that this metric effectively differentiates between human-written documents and
text generated by large language models under a specific domain. Furthermore,
we illustrate the efficacy of this approach in detecting written styles
attributed to diverse large language models, underscoring its potential for
generalizability. In summary, we present a novel Brownian bridge coherence
metric capable of measuring both local and global text coherence, while
circumventing the need for end-to-end model training. This flexibility allows
for its application in various downstream tasks.
","[{'version': 'v1', 'created': 'Thu, 28 Dec 2023 08:34:17 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 19:00:39 GMT'}]",2025-03-13,"[['Sheng', 'Zhecheng', ''], ['Zhang', 'Tianhao', ''], ['Jiang', 'Chen', ''], ['Kang', 'Dongyeop', '']]","[{'text': 'static embeddings', 'label': 'contextual Embedding'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2402.03848,David Peer,"David Peer, Philemon Sch\""opf, Volckmar Nebendahl, Alexander Rietzler,
  Sebastian Stabinger","ANLS* -- A Universal Document Processing Metric for Generative Large
  Language Models",,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Traditionally, discriminative models have been the predominant choice for
tasks like document classification and information extraction. These models
make predictions that fall into a limited number of predefined classes,
facilitating a binary true or false evaluation and enabling the direct
calculation of metrics such as the F1 score. However, recent advancements in
generative large language models (GLLMs) have prompted a shift in the field due
to their enhanced zero-shot capabilities, which eliminate the need for a
downstream dataset and computationally expensive fine-tuning. However,
evaluating GLLMs presents a challenge as the binary true or false evaluation
used for discriminative models is not applicable to the predictions made by
GLLMs.
  This paper introduces a new metric for generative models called ANLS* for
evaluating a wide variety of tasks, including information extraction and
classification tasks. The ANLS* metric extends existing ANLS metrics as a
drop-in-replacement and is still compatible with previously reported ANLS
scores. An evaluation of 7 different datasets, and more than 20 different GLLMs
together with 3 different prompting methods using the ANLS* metric is also
provided, demonstrating the importance of the proposed metric.
  We also benchmark a novel approach to generate prompts for documents, called
SFT, against other prompting techniques such as LATIN. In almost all cases, SFT
outperforms other techniques and improves the state-of-the-art, sometimes by as
much as $10$ percentage points.
  Sources are available at https://github.com/deepopinion/anls_star_metric
","[{'version': 'v1', 'created': 'Tue, 6 Feb 2024 09:50:08 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Feb 2024 13:14:28 GMT'}, {'version': 'v3', 'created': 'Thu, 21 Mar 2024 05:58:10 GMT'}, {'version': 'v4', 'created': 'Tue, 16 Apr 2024 09:14:46 GMT'}, {'version': 'v5', 'created': 'Sat, 25 May 2024 06:31:45 GMT'}, {'version': 'v6', 'created': 'Fri, 28 Jun 2024 06:49:39 GMT'}, {'version': 'v7', 'created': 'Tue, 27 Aug 2024 08:33:29 GMT'}, {'version': 'v8', 'created': 'Mon, 3 Mar 2025 12:50:31 GMT'}, {'version': 'v9', 'created': 'Wed, 12 Mar 2025 08:02:54 GMT'}]",2025-03-13,"[['Peer', 'David', ''], ['Schöpf', 'Philemon', ''], ['Nebendahl', 'Volckmar', ''], ['Rietzler', 'Alexander', ''], ['Stabinger', 'Sebastian', '']]","[{'text': 'generative large language models', 'label': 'Large Language Model'}, {'text': 'GLLMs', 'label': 'Large Language Model'}, {'text': 'computationally expensive fine-tuning', 'label': 'Fine-tuning'}, {'text': 'GLLMs', 'label': 'Large Language Model'}, {'text': 'GLLMs', 'label': 'Large Language Model'}, {'text': 'GLLMs', 'label': 'Large Language Model'}, {'text': 'SFT', 'label': 'BERT'}, {'text': 'LATIN', 'label': 'Prompting'}, {'text': 'SFT', 'label': 'BERT'}]",Large Language Model,generative large language models,0.8079476952552795
2402.04863,Yingjie Mao,"Xiaoqi Li, Yingjie Mao, Zexin Lu, Wenkai Li, Zongwei Li","SCLA: Automated Smart Contract Summarization via LLMs and Control Flow
  Prompt",,,,,cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Smart contract code summarization is crucial for efficient maintenance and
vulnerability mitigation. While many studies use Large Language Models (LLMs)
for summarization, their performance still falls short compared to fine-tuned
models like CodeT5+ and CodeBERT. Some approaches combine LLMs with data flow
analysis but fail to fully capture the hierarchy and control structures of the
code, leading to information loss and degraded summarization quality. We
propose SCLA, an LLM-based method that enhances summarization by integrating a
Control Flow Graph (CFG) and semantic facts from the code's control flow into a
semantically enriched prompt. SCLA uses a control flow extraction algorithm to
derive control flows from semantic nodes in the Abstract Syntax Tree (AST) and
constructs the corresponding CFG. Code semantic facts refer to both explicit
and implicit information within the AST that is relevant to smart contracts.
This method enables LLMs to better capture the structural and contextual
dependencies of the code. We validate the effectiveness of SCLA through
comprehensive experiments on a dataset of 40,000 real-world smart contracts.
The experiment shows that SCLA significantly improves summarization quality,
outperforming the SOTA baselines with improvements of 26.7%, 23.2%, 16.7%, and
14.7% in BLEU-4, METEOR, ROUGE-L, and BLEURT scores, respectively.
","[{'version': 'v1', 'created': 'Wed, 7 Feb 2024 13:58:26 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Feb 2024 06:09:16 GMT'}, {'version': 'v3', 'created': 'Wed, 21 Feb 2024 14:18:32 GMT'}, {'version': 'v4', 'created': 'Sat, 17 Aug 2024 03:41:42 GMT'}, {'version': 'v5', 'created': 'Tue, 20 Aug 2024 02:34:56 GMT'}, {'version': 'v6', 'created': 'Thu, 13 Mar 2025 07:05:15 GMT'}]",2025-03-14,"[['Li', 'Xiaoqi', ''], ['Mao', 'Yingjie', ''], ['Lu', 'Zexin', ''], ['Li', 'Wenkai', ''], ['Li', 'Zongwei', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'CodeT5+', 'label': 'Transformer-based model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'SCLA', 'label': 'LLM-based'}, {'text': 'semantically enriched prompt', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'SCLA', 'label': 'LLM-based'}, {'text': 'SCLA', 'label': 'LLM-based'}]",Large Language Model,Large Language Models,0.9664971828460693
2402.15131,Guanming Xiong,"Guanming Xiong, Junwei Bao, Wen Zhao","Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question
  Answering with Large Language Models","This work has been accepted by the ACL 2024 main conference. Code and
  data are available at: https://github.com/JimXiongGM/Interactive-KBQA",,10.18653/v1/2024.acl-long.569,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  This study explores the realm of knowledge base question answering (KBQA).
KBQA is considered a challenging task, particularly in parsing intricate
questions into executable logical forms. Traditional semantic parsing
(SP)-based methods require extensive data annotations, which result in
significant costs. Recently, the advent of few-shot in-context learning,
powered by large language models (LLMs), has showcased promising capabilities.
However, fully leveraging LLMs to parse questions into logical forms in
low-resource scenarios poses a substantial challenge. To tackle these hurdles,
we introduce Interactive-KBQA, a framework designed to generate logical forms
through direct interaction with knowledge bases (KBs). Within this framework,
we have developed three generic APIs for KB interaction. For each category of
complex question, we devised exemplars to guide LLMs through the reasoning
processes. Our method achieves competitive results on the WebQuestionsSP,
ComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of
examples (shots). Importantly, our approach supports manual intervention,
allowing for the iterative refinement of LLM outputs. By annotating a dataset
with step-wise reasoning processes, we showcase our model's adaptability and
highlight its potential for contributing significant enhancements to the field.
","[{'version': 'v1', 'created': 'Fri, 23 Feb 2024 06:32:18 GMT'}, {'version': 'v2', 'created': 'Fri, 19 Jul 2024 06:14:20 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 06:15:34 GMT'}]",2025-03-13,"[['Xiong', 'Guanming', ''], ['Bao', 'Junwei', ''], ['Zhao', 'Wen', '']]","[{'text': 'few-shot in-context learning', 'label': 'Few-shot Learning'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2402.15183,Zirui Guo,"Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Kangkang Lu, Zhiyong
  Huang, Chao Huang",GraphEdit: Large Language Models for Graph Structure Learning,,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies
and interactions among nodes in graph-structured data by generating novel graph
structures. Graph Neural Networks (GNNs) have emerged as promising GSL
solutions, utilizing recursive message passing to encode node-wise
inter-dependencies. However, many existing GSL methods heavily depend on
explicit graph structural information as supervision signals, leaving them
susceptible to challenges such as data noise and sparsity. In this work, we
propose GraphEdit, an approach that leverages large language models (LLMs) to
learn complex node relationships in graph-structured data. By enhancing the
reasoning capabilities of LLMs through instruction-tuning over graph
structures, we aim to overcome the limitations associated with explicit graph
structural information and enhance the reliability of graph structure learning.
Our approach not only effectively denoises noisy connections but also
identifies node-wise dependencies from a global perspective, providing a
comprehensive understanding of the graph structure. We conduct extensive
experiments on multiple benchmark datasets to demonstrate the effectiveness and
robustness of GraphEdit across various settings. We have made our model
implementation available at: https://github.com/HKUDS/GraphEdit.
","[{'version': 'v1', 'created': 'Fri, 23 Feb 2024 08:29:42 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Feb 2024 08:22:11 GMT'}, {'version': 'v3', 'created': 'Thu, 29 Feb 2024 04:15:44 GMT'}, {'version': 'v4', 'created': 'Tue, 5 Mar 2024 05:22:00 GMT'}, {'version': 'v5', 'created': 'Mon, 10 Mar 2025 14:04:39 GMT'}]",2025-03-11,"[['Guo', 'Zirui', ''], ['Xia', 'Lianghao', ''], ['Yu', 'Yanhua', ''], ['Wang', 'Yuling', ''], ['Lu', 'Kangkang', ''], ['Huang', 'Zhiyong', ''], ['Huang', 'Chao', '']]","[{'text': 'Graph Structure Learning', 'label': 'Few-shot Learning'}, {'text': 'GraphEdit', 'label': 'LLM'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'graph structure learning', 'label': 'Few-shot Learning'}, {'text': 'GraphEdit', 'label': 'LLM'}, {'text': 'GraphEdit', 'label': 'LLM'}]",Large Language Model,large language models,0.9664971828460693
2403.09905,Vishnu Sashank Dorbala,"Vishnu Sashank Dorbala, Bhrij Patel, Amrit Singh Bedi, Dinesh Manocha","Right Place, Right Time! Dynamizing Topological Graphs for Embodied
  Navigation",18,,,,cs.RO cs.CV,http://creativecommons.org/publicdomain/zero/1.0/,"  Embodied Navigation tasks often involve constructing topological graphs of a
scene during exploration to facilitate high-level planning and decision-making
for execution in continuous environments. Prior literature makes the assumption
of static graphs with stationary targets, which does not hold in many
real-world environments with moving objects. To address this, we present a
novel formulation generalizing navigation to dynamic environments by
introducing structured object transitions to dynamize static topological graphs
called Object Transition Graphs (OTGs). OTGs simulate portable targets
following structured routes inspired by human habits. We apply this technique
to Matterport3D (MP3D), a popular simulator for evaluating embodied tasks. On
these dynamized OTGs, we establish a navigation benchmark by evaluating
Oracle-based, Reinforcement Learning, and Large Language Model (LLM)-based
approaches on a multi-object finding task. Further, we quantify agent
adaptability, and make key inferences such as agents employing learned
decision-making strategies generalize better than those relying on privileged
oracle knowledge. To the best of our knowledge, ours is the first work to
introduce structured temporal dynamism on topological graphs for studying
generalist embodied navigation policies. The code and dataset for our OTGs will
be made publicly available to foster research on embodied navigation in dynamic
scenes.
","[{'version': 'v1', 'created': 'Thu, 14 Mar 2024 22:33:22 GMT'}, {'version': 'v2', 'created': 'Sun, 1 Dec 2024 21:42:37 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 22:26:37 GMT'}]",2025-03-12,"[['Dorbala', 'Vishnu Sashank', ''], ['Patel', 'Bhrij', ''], ['Bedi', 'Amrit Singh', ''], ['Manocha', 'Dinesh', '']]","[{'text': 'OTGs', 'label': 'LLMs'}, {'text': 'OTGs', 'label': 'LLMs'}, {'text': 'Oracle-based', 'label': 'LLM-based'}, {'text': 'Large Language Model', 'label': 'Large Language Model'}, {'text': 'OTGs', 'label': 'LLMs'}]",Large Language Model,Large Language Model,1.0
2403.14362,Jiaqi Yue,"Jiaqi Yue, Chunhui Zhao, Jiancheng Zhao, Biao Huang","Enabling Generalized Zero-shot Learning Towards Unseen Domains by
  Intrinsic Learning from Redundant LLM Semantics",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen
classes against domain shift problem where data of unseen classes may be
misclassified as seen classes. However, existing GZSL is still limited to seen
domains. In the current work, we study cross-domain GZSL (CDGZSL) which
addresses GZSL towards unseen domains. Different from existing GZSL methods,
CDGZSL constructs a common feature space across domains and acquires the
corresponding intrinsic semantics shared among domains to transfer from seen to
unseen domains. Considering the information asymmetry problem caused by
redundant class semantics annotated with large language models (LLMs), we
present Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR
consists of two parts: Inter-class similarity alignment, which eliminates the
non-intrinsic semantics not shared across all domains under the guidance of
inter-class feature relationships, and unseen-class meta generation, which
preserves intrinsic semantics to maintain connectivity between seen and unseen
classes by simulating feature generation. MDASR effectively aligns the
redundant semantic space with the common feature space, mitigating the
information asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on
two datasets, Office-Home and Mini-DomainNet, and we have shared the LLM-based
semantics for these datasets as a benchmark.
","[{'version': 'v1', 'created': 'Thu, 21 Mar 2024 12:45:01 GMT'}, {'version': 'v2', 'created': 'Thu, 23 May 2024 07:50:31 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Aug 2024 07:32:46 GMT'}, {'version': 'v4', 'created': 'Mon, 19 Aug 2024 12:28:55 GMT'}, {'version': 'v5', 'created': 'Mon, 10 Mar 2025 09:35:20 GMT'}]",2025-03-11,"[['Yue', 'Jiaqi', ''], ['Zhao', 'Chunhui', ''], ['Zhao', 'Jiancheng', ''], ['Huang', 'Biao', '']]","[{'text': 'Generalized zero-shot learning', 'label': 'Zero-shot Learning'}, {'text': 'GZSL', 'label': 'Zero-shot Learning'}, {'text': 'GZSL', 'label': 'Zero-shot Learning'}, {'text': 'GZSL', 'label': 'Zero-shot Learning'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2403.14743,Ahmad Mahmood,"Ahmad Mahmood, Ashmal Vayani, Muzammal Naseer, Salman Khan, Fahad
  Shahbaz Khan","VURF: A General-purpose Reasoning and Self-refinement Framework for
  Video Understanding",,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Recent studies have demonstrated the effectiveness of Large Language Models
(LLMs) as reasoning modules that can deconstruct complex tasks into more
manageable sub-tasks, particularly when applied to visual reasoning tasks for
images. In contrast, this paper introduces a Video Understanding and Reasoning
Framework (VURF) based on the reasoning power of LLMs. Ours is a novel approach
to extend the utility of LLMs in the context of video tasks, leveraging their
capacity to generalize from minimal input and output demonstrations within a
contextual framework. We harness their contextual learning capabilities by
presenting LLMs with pairs of instructions and their corresponding high-level
programs to generate executable visual programs for video understanding. To
enhance the program's accuracy and robustness, we implement two important
strategies. \emph{Firstly,} we employ a feedback-generation approach, powered
by GPT-3.5, to rectify errors in programs utilizing unsupported functions.
\emph{Secondly}, taking motivation from recent works on self-refinement of LLM
outputs, we introduce an iterative procedure for improving the quality of the
in-context examples by aligning the initial outputs to the outputs that would
have been generated had the LLM not been bound by the structure of the
in-context examples. Our results on several video-specific tasks, including
visual QA, video anticipation, pose estimation, and multi-video QA, illustrate
these enhancements' efficacy in improving the performance of visual programming
approaches for video tasks.
","[{'version': 'v1', 'created': 'Thu, 21 Mar 2024 18:00:00 GMT'}, {'version': 'v2', 'created': 'Mon, 25 Mar 2024 01:18:37 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 03:53:38 GMT'}]",2025-03-11,"[['Mahmood', 'Ahmad', ''], ['Vayani', 'Ashmal', ''], ['Naseer', 'Muzammal', ''], ['Khan', 'Salman', ''], ['Khan', 'Fahad Shahbaz', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'contextual framework', 'label': 'contextual Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GPT-3.5', 'label': 'GPT'}]",Large Language Model,Large Language Models,0.9664971828460693
2403.16812,Shuai Ma,"Shuai Ma, Qiaoyi Chen, Xinru Wang, Chengbo Zheng, Zhenhui Peng, Ming
  Yin, Xiaojuan Ma","Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered
  Deliberative AI for AI-Assisted Decision-Making","23 pages, ACM CHI 2025",,,,cs.HC cs.AI,http://creativecommons.org/licenses/by/4.0/,"  In AI-assisted decision-making, humans often passively review AI's suggestion
and decide whether to accept or reject it as a whole. In such a paradigm,
humans are found to rarely trigger analytical thinking and face difficulties in
communicating the nuances of conflicting opinions to the AI when disagreements
occur. To tackle this challenge, we propose Human-AI Deliberation, a novel
framework to promote human reflection and discussion on conflicting human-AI
opinions in decision-making. Based on theories in human deliberation, this
framework engages humans and AI in dimension-level opinion elicitation,
deliberative discussion, and decision updates. To empower AI with deliberative
capabilities, we designed Deliberative AI, which leverages large language
models (LLMs) as a bridge between humans and domain-specific models to enable
flexible conversational interactions and faithful information provision. An
exploratory evaluation on a graduate admissions task shows that Deliberative AI
outperforms conventional explainable AI (XAI) assistants in improving humans'
appropriate reliance and task performance. Based on a mixed-methods analysis of
participant behavior, perception, user experience, and open-ended feedback, we
draw implications for future AI-assisted decision tool design.
","[{'version': 'v1', 'created': 'Mon, 25 Mar 2024 14:34:06 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 19:23:23 GMT'}]",2025-03-13,"[['Ma', 'Shuai', ''], ['Chen', 'Qiaoyi', ''], ['Wang', 'Xinru', ''], ['Zheng', 'Chengbo', ''], ['Peng', 'Zhenhui', ''], ['Yin', 'Ming', ''], ['Ma', 'Xiaojuan', '']]","[{'text': 'large language\nmodels', 'label': 'Large Language Model'}]",Large Language Model,"large language
models",0.9664971828460693
2405.05966,Juri Opitz,Juri Opitz and Shira Wein and Nathan Schneider,Natural Language Processing RELIES on Linguistics,"To appear in Computational Linguistics. This is a pre-MIT Press
  publication version",,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have become capable of generating highly fluent
text in certain languages, without modules specially designed to capture
grammar or semantic coherence. What does this mean for the future of linguistic
expertise in NLP? We highlight several aspects in which NLP (still) relies on
linguistics, or where linguistic thinking can illuminate new directions. We
argue our case around the acronym RELIES that encapsulates six major facets
where linguistics contributes to NLP: Resources, Evaluation, Low-resource
settings, Interpretability, Explanation, and the Study of language. This list
is not exhaustive, nor is linguistics the main point of reference for every
effort under these themes; but at a macro level, these facets highlight the
enduring importance of studying machine systems vis-\`a-vis systems of human
language.
","[{'version': 'v1', 'created': 'Thu, 9 May 2024 17:59:32 GMT'}, {'version': 'v2', 'created': 'Mon, 9 Sep 2024 08:21:13 GMT'}, {'version': 'v3', 'created': 'Fri, 22 Nov 2024 15:36:32 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Mar 2025 15:07:49 GMT'}]",2025-03-11,"[['Opitz', 'Juri', ''], ['Wein', 'Shira', ''], ['Schneider', 'Nathan', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2405.16918,Nils Philipp Walter,"Nils Philipp Walter, Linara Adilova, Jilles Vreeken, Michael Kamp","The Uncanny Valley: Exploring Adversarial Robustness from a Flatness
  Perspective",,,,,cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Flatness of the loss surface not only correlates positively with
generalization, but is also related to adversarial robustness since
perturbations of inputs relate non-linearly to perturbations of weights. In
this paper, we empirically analyze the relation between adversarial examples
and relative flatness with respect to the parameters of one layer. We observe a
peculiar property of adversarial examples in the context of relative flatness:
during an iterative first-order white-box attack, the flatness of the loss
surface measured around the adversarial example first becomes sharper until the
label is flipped, but if we keep the attack running, it runs into a flat
uncanny valley where the label remains flipped. In extensive experiments, we
observe this phenomenon across various model architectures and datasets, even
for adversarially trained models. Our results also extend to large language
models (LLMs), but due to the discrete nature of the input space and
comparatively weak attacks, adversarial examples rarely reach truly flat
regions. Most importantly, this phenomenon shows that flatness alone cannot
explain adversarial robustness unless we can also guarantee the behavior of the
function around the examples. We, therefore theoretically connect relative
flatness to adversarial robustness by bounding the third derivative of the loss
surface, underlining the need for flatness in combination with a low global
Lipschitz constant for a robust model.
","[{'version': 'v1', 'created': 'Mon, 27 May 2024 08:10:46 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 14:47:37 GMT'}]",2025-03-11,"[['Walter', 'Nils Philipp', ''], ['Adilova', 'Linara', ''], ['Vreeken', 'Jilles', ''], ['Kamp', 'Michael', '']]","[{'text': 'large language\nmodels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,"large language
models",0.9664971828460693
2406.04443,Eduard Gorbunov,"Savelii Chezhegov, Yaroslav Klyukin, Andrei Semenov, Aleksandr
  Beznosikov, Alexander Gasnikov, Samuel Horv\'ath, Martin Tak\'a\v{c}, Eduard
  Gorbunov","Clipping Improves Adam-Norm and AdaGrad-Norm when the Noise Is
  Heavy-Tailed","63 pages, 8 figures",,,,cs.LG math.OC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Methods with adaptive stepsizes, such as AdaGrad and Adam, are essential for
training modern Deep Learning models, especially Large Language Models.
Typically, the noise in the stochastic gradients is heavy-tailed for the later
ones. Gradient clipping provably helps to achieve good high-probability
convergence for such noises. However, despite the similarity between
AdaGrad/Adam and Clip-SGD, the current understanding of the high-probability
convergence of AdaGrad/Adam-type methods is limited in this case. In this work,
we prove that AdaGrad/Adam (and their delayed version) can have provably bad
high-probability convergence if the noise is heavy-tailed. We also show that
gradient clipping fixes this issue, i.e., we derive new high-probability
convergence bounds with polylogarithmic dependence on the confidence level for
AdaGrad-Norm and Adam-Norm with clipping and with/without delay for smooth
convex/non-convex stochastic optimization with heavy-tailed noise. Our
empirical evaluations highlight the superiority of clipped versions of
AdaGrad/Adam-Norm in handling the heavy-tailed noise.
","[{'version': 'v1', 'created': 'Thu, 6 Jun 2024 18:49:10 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 10:26:57 GMT'}]",2025-03-14,"[['Chezhegov', 'Savelii', ''], ['Klyukin', 'Yaroslav', ''], ['Semenov', 'Andrei', ''], ['Beznosikov', 'Aleksandr', ''], ['Gasnikov', 'Alexander', ''], ['Horváth', 'Samuel', ''], ['Takáč', 'Martin', ''], ['Gorbunov', 'Eduard', '']]","[{'text': 'Adam', 'label': 'ALBERT'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'Adam', 'label': 'ALBERT'}, {'text': 'Adam', 'label': 'ALBERT'}, {'text': 'Adam-Norm', 'label': 'ALBERT'}]",Large Language Model,Large Language Models,0.9664971828460693
2406.08426,Zijin Hong,"Zijin Hong, Zheng Yuan, Qinggang Zhang, Hao Chen, Junnan Dong, Feiran
  Huang, Xiao Huang",Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL,,,,,cs.CL cs.AI cs.DB,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generating accurate SQL from users' natural language questions (text-to-SQL)
remains a long-standing challenge due to the complexities involved in user
question understanding, database schema comprehension, and SQL generation.
Traditional text-to-SQL systems, which combine human engineering and deep
neural networks, have made significant progress. Subsequently, pre-trained
language models (PLMs) have been developed for text-to-SQL tasks, achieving
promising results. However, as modern databases and user questions grow more
complex, PLMs with a limited parameter size often produce incorrect SQL. This
necessitates more sophisticated and tailored optimization methods, which
restricts the application of PLM-based systems. Recently, large language models
(LLMs) have shown significant capabilities in natural language understanding as
model scale increases. Thus, integrating LLM-based solutions can bring unique
opportunities, improvements, and solutions to text-to-SQL research. In this
survey, we provide a comprehensive review of existing LLM-based text-to-SQL
studies. Specifically, we offer a brief overview of the technical challenges
and evolutionary process of text-to-SQL. Next, we introduce the datasets and
metrics designed to evaluate text-to-SQL systems. Subsequently, we present a
systematic analysis of recent advances in LLM-based text-to-SQL. Finally, we
make a summarization and discuss the remaining challenges in this field and
suggest expectations for future research directions.
","[{'version': 'v1', 'created': 'Wed, 12 Jun 2024 17:13:17 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Jun 2024 13:51:30 GMT'}, {'version': 'v3', 'created': 'Tue, 16 Jul 2024 08:06:57 GMT'}, {'version': 'v4', 'created': 'Sun, 23 Feb 2025 22:22:20 GMT'}, {'version': 'v5', 'created': 'Thu, 13 Mar 2025 08:45:35 GMT'}]",2025-03-14,"[['Hong', 'Zijin', ''], ['Yuan', 'Zheng', ''], ['Zhang', 'Qinggang', ''], ['Chen', 'Hao', ''], ['Dong', 'Junnan', ''], ['Huang', 'Feiran', ''], ['Huang', 'Xiao', '']]","[{'text': 'PLMs', 'label': 'Large Language Model'}, {'text': 'PLMs', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2406.16810,William F. Shen,"Xinchi Qiu, William F. Shen, Yihong Chen, Meghdad Kurmanji, Nicola
  Cancedda, Pontus Stenetorp, Nicholas D. Lane","How Data Inter-connectivity Shapes LLMs Unlearning: A Structural
  Unlearning Perspective",,,,,cs.LG cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  While unlearning knowledge from large language models (LLMs) is receiving
increasing attention, one important aspect remains unexplored. Existing
approaches and benchmarks assume data points to-be-forgotten are independent,
ignoring their inter-connectivity - a fundamental characteristic of real-world
data structures. In this paper, we propose PISTOL, a method for compiling
structural datasets. PISTOL leverages the inherently structured nature of
contractual relationships, offering several key benefits. First, it enables
insights into the impact of structural data on unlearning effectiveness.
Second, it provides precise and concise ground truths for clearer evaluation.
Third, its attribute generation does not require input from pre-trained LLMs,
mitigating confounding risks. Leveraging datasets synthesized using PISTOL, we
demonstrate how data inter-connectivity impacts LLM unlearning. Specifically,
(a) in both the pre-trained and fine-tuned models, unlearning difficulty
increases as data inter-connectivity grows, (b) there is a positive correlation
between the density of the knowledge graph and unlearning difficulty, and (c)
when the to-be-forgotten data is skewed towards one domain, balancing retaining
performance across all domains is challenging.
","[{'version': 'v1', 'created': 'Mon, 24 Jun 2024 17:22:36 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 21:33:53 GMT'}]",2025-03-12,"[['Qiu', 'Xinchi', ''], ['Shen', 'William F.', ''], ['Chen', 'Yihong', ''], ['Kurmanji', 'Meghdad', ''], ['Cancedda', 'Nicola', ''], ['Stenetorp', 'Pontus', ''], ['Lane', 'Nicholas D.', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'PISTOL', 'label': 'Generative Pre-trained Transformer (GPT)'}]",Large Language Model,large language models,0.9664971828460693
2406.17055,Ryan Liu,"Ryan Liu, Jiayi Geng, Joshua C. Peterson, Ilia Sucholutsky, Thomas L.
  Griffiths",Large Language Models Assume People are More Rational than We Really are,,,,,cs.CL cs.AI cs.CY cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  In order for AI systems to communicate effectively with people, they must
understand how we make decisions. However, people's decisions are not always
rational, so the implicit internal models of human decision-making in Large
Language Models (LLMs) must account for this. Previous empirical evidence seems
to suggest that these implicit models are accurate -- LLMs offer believable
proxies of human behavior, acting how we expect humans would in everyday
interactions. However, by comparing LLM behavior and predictions to a large
dataset of human decisions, we find that this is actually not the case: when
both simulating and predicting people's choices, a suite of cutting-edge LLMs
(GPT-4o & 4-Turbo, Llama-3-8B & 70B, Claude 3 Opus) assume that people are more
rational than we really are. Specifically, these models deviate from human
behavior and align more closely with a classic model of rational choice --
expected value theory. Interestingly, people also tend to assume that other
people are rational when interpreting their behavior. As a consequence, when we
compare the inferences that LLMs and people draw from the decisions of others
using another psychological dataset, we find that these inferences are highly
correlated. Thus, the implicit decision-making models of LLMs appear to be
aligned with the human expectation that other people will act rationally,
rather than with how people actually act.
","[{'version': 'v1', 'created': 'Mon, 24 Jun 2024 18:15:27 GMT'}, {'version': 'v2', 'created': 'Mon, 1 Jul 2024 17:29:54 GMT'}, {'version': 'v3', 'created': 'Tue, 30 Jul 2024 14:22:26 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Mar 2025 17:42:37 GMT'}]",2025-03-11,"[['Liu', 'Ryan', ''], ['Geng', 'Jiayi', ''], ['Peterson', 'Joshua C.', ''], ['Sucholutsky', 'Ilia', ''], ['Griffiths', 'Thomas L.', '']]","[{'text': 'Large\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,"Large
Language Models",0.9664971828460693
2406.18113,Boris Meinardus,"Boris Meinardus, Hector Rodriguez, Anil Batra, Anna Rohrbach, Marcus
  Rohrbach",Chrono: A Simple Blueprint for Representing Time in MLLMs,Code: https://github.com/sudo-Boris/mr-Blip,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The recent success of Large Language Models (LLMs) has prompted the extension
to the multimodal domain developing image-text Multimodal LLMs (MLLMs) and then
video-text models. In this work, we investigate the challenge of contextual and
temporal comprehension in video-language models by exploring the task of
temporal localization in videos. To address this problem, prior works have
developed complex task-specific architectures, novel modules to embed time into
MLLMs, or leveraged additional input signals such as video transcripts to best
encode contextual and temporal information. Interestingly, we find that most of
these efforts are surpassed by a much simpler design. We introduce Chrono, a
universal sequence blueprint that can be applied to an image-text pretrained
MLLM. Through extensive ablations across different MLLM architectures,
finetuning and zero-shot settings, and different datasets, we achieve a new
SOTA in moment retrieval on the most widely used benchmarks Charades-STA,
QVHighlights, ActivityNet Captions, and grounded video question answering on
NeXT-GQA.
","[{'version': 'v1', 'created': 'Wed, 26 Jun 2024 06:59:09 GMT'}, {'version': 'v2', 'created': 'Wed, 24 Jul 2024 06:43:07 GMT'}, {'version': 'v3', 'created': 'Mon, 14 Oct 2024 06:50:19 GMT'}, {'version': 'v4', 'created': 'Fri, 21 Feb 2025 00:49:07 GMT'}, {'version': 'v5', 'created': 'Tue, 11 Mar 2025 10:03:46 GMT'}]",2025-03-12,"[['Meinardus', 'Boris', ''], ['Rodriguez', 'Hector', ''], ['Batra', 'Anil', ''], ['Rohrbach', 'Anna', ''], ['Rohrbach', 'Marcus', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'finetuning', 'label': 'Fine-tuning'}, {'text': 'zero-shot settings', 'label': 'Zero-shot Learning'}]",Large Language Model,Large Language Models,0.9664971828460693
2407.00936,Zirui Chen,"Xin Wang, Zirui Chen, Haofen Wang, Leong Hou U, Zhao Li, Wenbin Guo","Large Language Model Enhanced Knowledge Representation Learning: A
  Survey",,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Knowledge Representation Learning (KRL) is crucial for enabling applications
of symbolic knowledge from Knowledge Graphs (KGs) to downstream tasks by
projecting knowledge facts into vector spaces. Despite their effectiveness in
modeling KG structural information, KRL methods are suffering from the
sparseness of KGs. The rise of Large Language Models (LLMs) built on the
Transformer architecture presents promising opportunities for enhancing KRL by
incorporating textual information to address information sparsity in KGs.
LLM-enhanced KRL methods, including three key approaches, encoder-based methods
that leverage detailed contextual information, encoder-decoder-based methods
that utilize a unified Seq2Seq model for comprehensive encoding and decoding,
and decoder-based methods that utilize extensive knowledge from large corpora,
have significantly advanced the effectiveness and generalization of KRL in
addressing a wide range of downstream tasks. This work provides a broad
overview of downstream tasks while simultaneously identifying emerging research
directions in these evolving domains.
","[{'version': 'v1', 'created': 'Mon, 1 Jul 2024 03:37:35 GMT'}, {'version': 'v2', 'created': 'Thu, 18 Jul 2024 02:19:34 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Feb 2025 02:38:25 GMT'}, {'version': 'v4', 'created': 'Wed, 12 Mar 2025 05:48:32 GMT'}]",2025-03-13,"[['Wang', 'Xin', ''], ['Chen', 'Zirui', ''], ['Wang', 'Haofen', ''], ['U', 'Leong Hou', ''], ['Li', 'Zhao', ''], ['Guo', 'Wenbin', '']]","[{'text': 'Knowledge Representation Learning', 'label': 'Few-shot Learning'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'KRL', 'label': 'Few-shot Learning'}, {'text': 'decoder-based methods', 'label': 'LLM-powered'}]",Large Language Model,Large Language Models,0.9664971828460693
2407.08952,Ye Liu,"Ye Liu, Jiajun Zhu, Xukai Liu, Haoyu Tang, Yanghai Zhang, Kai Zhang,
  Xiaofang Zhou, Enhong Chen","Detect, Investigate, Judge and Determine: A Knowledge-guided Framework
  for Few-shot Fake News Detection",,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news
from real ones in extremely low-resource scenarios. This task has garnered
increased attention due to the widespread dissemination and harmful impact of
fake news on social media. Large Language Models (LLMs) have demonstrated
competitive performance with the help of their rich prior knowledge and
excellent in-context learning abilities. However, existing methods face
significant limitations, such as the Understanding Ambiguity and Information
Scarcity, which significantly undermine the potential of LLMs. To address these
shortcomings, we propose a Dual-perspective Knowledge-guided Fake News
Detection (DKFND) model, designed to enhance LLMs from both inside and outside
perspectives. Specifically, DKFND first identifies the knowledge concepts of
each news article through a Detection Module. Subsequently, DKFND creatively
designs an Investigation Module to retrieve inside and outside valuable
information concerning to the current news, followed by another Judge Module to
evaluate the relevance and confidence of them. Finally, a Determination Module
further derives two respective predictions and obtain the final result.
Extensive experiments on two public datasets show the efficacy of our proposed
method, particularly in low-resource settings.
","[{'version': 'v1', 'created': 'Fri, 12 Jul 2024 03:15:01 GMT'}, {'version': 'v2', 'created': 'Fri, 14 Feb 2025 04:56:16 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Feb 2025 05:25:32 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Mar 2025 13:06:04 GMT'}, {'version': 'v5', 'created': 'Wed, 12 Mar 2025 04:46:47 GMT'}]",2025-03-13,"[['Liu', 'Ye', ''], ['Zhu', 'Jiajun', ''], ['Liu', 'Xukai', ''], ['Tang', 'Haoyu', ''], ['Zhang', 'Yanghai', ''], ['Zhang', 'Kai', ''], ['Zhou', 'Xiaofang', ''], ['Chen', 'Enhong', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Detection Module', 'label': 'Embedding'}, {'text': 'Investigation Module', 'label': 'Embedding'}, {'text': 'Determination Module', 'label': 'Embedding'}, {'text': 'public datasets', 'label': 'Open-source LLMs'}]",Large Language Model,Large Language Models,0.9664971828460693
2407.12358,Chuwei Luo,"Yufan Shen, Chuwei Luo, Zhaoqing Zhu, Yang Chen, Qi Zheng, Zhi Yu,
  Jiajun Bu, Cong Yao","ProcTag: Process Tagging for Assessing the Efficacy of Document
  Instruction Data",AAAI 2025,,,,cs.CV cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recently, large language models (LLMs) and multimodal large language models
(MLLMs) have demonstrated promising results on document visual question
answering (VQA) task, particularly after training on document instruction
datasets. An effective evaluation method for document instruction data is
crucial in constructing instruction data with high efficacy, which, in turn,
facilitates the training of LLMs and MLLMs for document VQA. However, most
existing evaluation methods for instruction data are limited to the textual
content of the instructions themselves, thereby hindering the effective
assessment of document instruction datasets and constraining their
construction. In this paper, we propose ProcTag, a data-oriented method that
assesses the efficacy of document instruction data. ProcTag innovatively
performs tagging on the execution process of instructions rather than the
instruction text itself. By leveraging the diversity and complexity of these
tags to assess the efficacy of the given dataset, ProcTag enables selective
sampling or filtering of document instructions. Furthermore, DocLayPrompt, a
novel semi-structured layout-aware document prompting strategy, is proposed for
effectively representing documents. Experiments demonstrate that sampling
existing open-sourced and generated document VQA/instruction datasets with
ProcTag significantly outperforms current methods for evaluating instruction
data. Impressively, with ProcTag-based sampling in the generated document
datasets, only 30.5\% of the document instructions are required to achieve
100\% efficacy compared to the complete dataset. The code is publicly available
at
https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/ProcTag.
","[{'version': 'v1', 'created': 'Wed, 17 Jul 2024 07:29:59 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 02:20:28 GMT'}]",2025-03-13,"[['Shen', 'Yufan', ''], ['Luo', 'Chuwei', ''], ['Zhu', 'Zhaoqing', ''], ['Chen', 'Yang', ''], ['Zheng', 'Qi', ''], ['Yu', 'Zhi', ''], ['Bu', 'Jiajun', ''], ['Yao', 'Cong', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'ProcTag', 'label': 'Prompting'}, {'text': 'ProcTag', 'label': 'Prompting'}, {'text': 'ProcTag', 'label': 'Prompting'}, {'text': 'DocLayPrompt', 'label': 'Prompting'}, {'text': 'ProcTag', 'label': 'Prompting'}, {'text': 'ProcTag', 'label': 'Prompting'}]",Large Language Model,large language models,0.9664971828460693
2407.17417,Michael-Andrei Panaitescu-Liess,"Michael-Andrei Panaitescu-Liess, Zora Che, Bang An, Yuancheng Xu,
  Pankayaraj Pathmanathan, Souradip Chakraborty, Sicheng Zhu, Tom Goldstein,
  Furong Huang","Can Watermarking Large Language Models Prevent Copyrighted Text
  Generation and Hide Training Data?","19 pages, 7 figures. Published at AAAI 2025. Code will be available
  at https://github.com/michael-panaitescu/watermark_copyright_aaai25",,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have demonstrated impressive capabilities in
generating diverse and contextually rich text. However, concerns regarding
copyright infringement arise as LLMs may inadvertently produce copyrighted
material. In this paper, we first investigate the effectiveness of watermarking
LLMs as a deterrent against the generation of copyrighted texts. Through
theoretical analysis and empirical evaluation, we demonstrate that
incorporating watermarks into LLMs significantly reduces the likelihood of
generating copyrighted content, thereby addressing a critical concern in the
deployment of LLMs. However, we also find that watermarking can have unintended
consequences on Membership Inference Attacks (MIAs), which aim to discern
whether a sample was part of the pretraining dataset and may be used to detect
copyright violations. Surprisingly, we find that watermarking adversely affects
the success rate of MIAs, complicating the task of detecting copyrighted text
in the pretraining dataset. These results reveal the complex interplay between
different regulatory measures, which may impact each other in unforeseen ways.
Finally, we propose an adaptive technique to improve the success rate of a
recent MIA under watermarking. Our findings underscore the importance of
developing adaptive methods to study critical problems in LLMs with potential
legal implications.
","[{'version': 'v1', 'created': 'Wed, 24 Jul 2024 16:53:09 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 06:18:24 GMT'}]",2025-03-11,"[['Panaitescu-Liess', 'Michael-Andrei', ''], ['Che', 'Zora', ''], ['An', 'Bang', ''], ['Xu', 'Yuancheng', ''], ['Pathmanathan', 'Pankayaraj', ''], ['Chakraborty', 'Souradip', ''], ['Zhu', 'Sicheng', ''], ['Goldstein', 'Tom', ''], ['Huang', 'Furong', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2407.21227,Florian Tambon,"Florian Tambon, Amin Nikanjam, Cyrine Zid, Foutse Khomh, Giuliano
  Antoniol","TaskEval: Assessing Difficulty of Code Generation Tasks for Large
  Language Models",,,,,cs.SE cs.AI,http://creativecommons.org/publicdomain/zero/1.0/,"  Large Language Models (LLMs) excel in code-related tasks like code
generation, but benchmark evaluations often overlook task characteristics, such
as difficulty. Moreover, benchmarks are usually built using tasks described
with one single prompt, despite the formulation of prompts having a profound
impact on the outcome. This paper introduces a generalist approach, TaskEval, a
framework using diverse prompts and Item Response Theory (IRT) to efficiently
assess LLMs' capabilities and benchmark task characteristics, improving the
understanding of their performance.
  Using two code generation benchmarks, HumanEval+ and ClassEval, as well as 5
code generation LLMs, we show that TaskEval is capable of characterizing the
properties of tasks. Using topic analysis, we identify and analyze the tasks of
respectively 17 and 21 topics within the benchmarks. We also cross-analyze
tasks' characteristics with programming constructs (e.g., variable assignment,
conditions, etc.) used by LLMs, emphasizing some patterns with tasks'
difficulty. Finally, we conduct a comparison between the difficulty assessment
of tasks by human-annotators and LLMs. Orthogonal to current benchmarking
evaluation efforts, TaskEval can assist researchers and practitioners in
fostering better assessments of LLMs. The tasks' characteristics can be used to
identify shortcomings within existing benchmarks. This could be used to
generate additional related tasks for the evaluation or improvement of LLM.
","[{'version': 'v1', 'created': 'Tue, 30 Jul 2024 22:31:19 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 17:41:17 GMT'}]",2025-03-11,"[['Tambon', 'Florian', ''], ['Nikanjam', 'Amin', ''], ['Zid', 'Cyrine', ''], ['Khomh', 'Foutse', ''], ['Antoniol', 'Giuliano', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2408.08158,Mar Gonzalez-Franco,"Riccardo Bovo, Steven Abreu, Karan Ahuja, Eric J Gonzalez, Li-Te
  Cheng, Mar Gonzalez-Franco",EmBARDiment: an Embodied AI Agent for Productivity in XR,,IEEE Virtual Reality Conference 2025,,,cs.HC cs.MA,http://creativecommons.org/licenses/by/4.0/,"  XR devices running chat-bots powered by Large Language Models (LLMs) have the
to become always-on agents that enable much better productivity scenarios.
Current screen based chat-bots do not take advantage of the the full-suite of
natural inputs available in XR, including inward facing sensor data, instead
they over-rely on explicit voice or text prompts, sometimes paired with
multi-modal data dropped as part of the query. We propose a solution that
leverages an attention framework that derives context implicitly from user
actions, eye-gaze, and contextual memory within the XR environment. Our work
minimizes the need for engineered explicit prompts, fostering grounded and
intuitive interactions that glean user insights for the chat-bot.
","[{'version': 'v1', 'created': 'Thu, 15 Aug 2024 13:48:44 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 21:54:08 GMT'}]",2025-03-13,"[['Bovo', 'Riccardo', ''], ['Abreu', 'Steven', ''], ['Ahuja', 'Karan', ''], ['Gonzalez', 'Eric J', ''], ['Cheng', 'Li-Te', ''], ['Gonzalez-Franco', 'Mar', '']]","[{'text': 'chat-bots', 'label': 'ChatGPT'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'chat-bots', 'label': 'ChatGPT'}, {'text': 'explicit voice or text prompts', 'label': 'Prompting'}, {'text': 'attention framework', 'label': 'Attention mechanism'}]",Large Language Model,Large Language Models,0.9664971828460693
2408.10883,Xinqi Su,"Xinqi Su, Zitong Yu, Yawen Cui, Ajian Liu, Xun Lin, Yuhao Wang,
  Haochen Liang, Wenhui Li, Li Shen, Xiaochun Cao",Dynamic Analysis and Adaptive Discriminator for Fake News Detection,,,,,cs.AI cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In current web environment, fake news spreads rapidly across online social
networks, posing serious threats to society. Existing multimodal fake news
detection methods can generally be classified into knowledge-based and
semantic-based approaches. However, these methods are heavily rely on human
expertise and feedback, lacking flexibility. To address this challenge, we
propose a Dynamic Analysis and Adaptive Discriminator (DAAD) approach for fake
news detection. For knowledge-based methods, we introduce the Monte Carlo Tree
Search algorithm to leverage the self-reflective capabilities of large language
models (LLMs) for prompt optimization, providing richer, domain-specific
details and guidance to the LLMs, while enabling more flexible integration of
LLM comment on news content. For semantic-based methods, we define four typical
deceit patterns: emotional exaggeration, logical inconsistency, image
manipulation, and semantic inconsistency, to reveal the mechanisms behind fake
news creation. To detect these patterns, we carefully design four
discriminators and expand them in depth and breadth, using the soft-routing
mechanism to explore optimal detection models. Experimental results on three
real-world datasets demonstrate the superiority of our approach. The code will
be available at: https://github.com/SuXinqi/DAAD.
","[{'version': 'v1', 'created': 'Tue, 20 Aug 2024 14:13:54 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 03:05:45 GMT'}]",2025-03-12,"[['Su', 'Xinqi', ''], ['Yu', 'Zitong', ''], ['Cui', 'Yawen', ''], ['Liu', 'Ajian', ''], ['Lin', 'Xun', ''], ['Wang', 'Yuhao', ''], ['Liang', 'Haochen', ''], ['Li', 'Wenhui', ''], ['Shen', 'Li', ''], ['Cao', 'Xiaochun', '']]","[{'text': 'large language\nmodels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompt optimization', 'label': 'Prompting'}]",Large Language Model,"large language
models",0.9664971828460693
2409.11863,Kejia Chen,"Kejia Chen, Zheng Shen, Yue Zhang, Lingyun Chen, Fan Wu, Zhenshan
  Bing, Sami Haddadin, Alois Knoll","LEMMo-Plan: LLM-Enhanced Learning from Multi-Modal Demonstration for
  Planning Sequential Contact-Rich Manipulation Tasks",,,,,cs.RO cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Large Language Models (LLMs) have gained popularity in task planning for
long-horizon manipulation tasks. To enhance the validity of LLM-generated
plans, visual demonstrations and online videos have been widely employed to
guide the planning process. However, for manipulation tasks involving subtle
movements but rich contact interactions, visual perception alone may be
insufficient for the LLM to fully interpret the demonstration. Additionally,
visual data provides limited information on force-related parameters and
conditions, which are crucial for effective execution on real robots. In this
paper, we introduce an in-context learning framework that incorporates tactile
and force-torque information from human demonstrations to enhance LLMs' ability
to generate plans for new task scenarios. We propose a bootstrapped reasoning
pipeline that sequentially integrates each modality into a comprehensive task
plan. This task plan is then used as a reference for planning in new task
configurations. Real-world experiments on two different sequential manipulation
tasks demonstrate the effectiveness of our framework in improving LLMs'
understanding of multi-modal demonstrations and enhancing the overall planning
performance.
","[{'version': 'v1', 'created': 'Wed, 18 Sep 2024 10:36:47 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 18:24:51 GMT'}]",2025-03-12,"[['Chen', 'Kejia', ''], ['Shen', 'Zheng', ''], ['Zhang', 'Yue', ''], ['Chen', 'Lingyun', ''], ['Wu', 'Fan', ''], ['Bing', 'Zhenshan', ''], ['Haddadin', 'Sami', ''], ['Knoll', 'Alois', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2409.14572,Hongchen Wang,"Hongchen Wang, Kangming Li, Scott Ramsay, Yao Fehlis, Edward Kim, and
  Jason Hattrick-Simpers","Evaluating the Performance and Robustness of LLMs in Materials Science
  Q&A and Property Predictions",,,,,cs.CL cond-mat.mtrl-sci cs.AI cs.LG,http://creativecommons.org/publicdomain/zero/1.0/,"  Large Language Models (LLMs) have the potential to revolutionize scientific
research, yet their robustness and reliability in domain-specific applications
remain insufficiently explored. In this study, we evaluate the performance and
robustness of LLMs for materials science, focusing on domain-specific question
answering and materials property prediction across diverse real-world and
adversarial conditions. Three distinct datasets are used in this study: 1) a
set of multiple-choice questions from undergraduate-level materials science
courses, 2) a dataset including various steel compositions and yield strengths,
and 3) a band gap dataset, containing textual descriptions of material crystal
structures and band gap values. The performance of LLMs is assessed using
various prompting strategies, including zero-shot chain-of-thought, expert
prompting, and few-shot in-context learning. The robustness of these models is
tested against various forms of 'noise', ranging from realistic disturbances to
intentionally adversarial manipulations, to evaluate their resilience and
reliability under real-world conditions. Additionally, the study showcases
unique phenomena of LLMs during predictive tasks, such as mode collapse
behavior when the proximity of prompt examples is altered and performance
recovery from train/test mismatch. The findings aim to provide informed
skepticism for the broad use of LLMs in materials science and to inspire
advancements that enhance their robustness and reliability for practical
applications.
","[{'version': 'v1', 'created': 'Sun, 22 Sep 2024 19:31:16 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 22:03:26 GMT'}]",2025-03-13,"[['Wang', 'Hongchen', ''], ['Li', 'Kangming', ''], ['Ramsay', 'Scott', ''], ['Fehlis', 'Yao', ''], ['Kim', 'Edward', ''], ['Hattrick-Simpers', 'Jason', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'zero-shot chain-of-thought', 'label': 'Chain of thought'}, {'text': 'expert\nprompting', 'label': 'Prompting'}, {'text': 'few-shot in-context learning', 'label': 'Few-shot Learning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2409.17954,Jian Gao,"Jian Gao, Xiao Zhang, Ji Wu, Miao Li","Enhancing elusive clues in knowledge learning by contrasting attention
  of language models",Oral presentation in AAAI 2025,,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Causal language models acquire vast amount of knowledge from general text
corpus during pretraining, but the efficiency of knowledge learning is known to
be unsatisfactory, especially when learning from knowledge-dense and
small-sized corpora. The deficiency can come from long-distance dependencies
which are hard to capture by language models, and overfitting to co-occurrence
patterns and distracting clues in the training text. To address these issues,
the paper proposes a method to enhance knowledge learning during language model
pretraining, by enhancing elusive but important clues in text discovered by the
language model themselves. We found that larger language models pay more
attention to non-obvious but important clues, which are often overlooked by
smaller language models. Therefore, we can identify these clues by contrasting
the attention weights of large and small language models. We use the identified
clues as a guide to perform token-dropout data augmentation on the training
text, and observed a significant boost in both small and large models'
performance in fact memorization. This shows that the behavior contrast between
more and less-performant language models contains important clues for knowledge
learning, and it can be ``amplified"" for a straight-forward improvement in
knowledge learning efficiency.
","[{'version': 'v1', 'created': 'Thu, 26 Sep 2024 15:30:54 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 09:42:19 GMT'}]",2025-03-13,"[['Gao', 'Jian', ''], ['Zhang', 'Xiao', ''], ['Wu', 'Ji', ''], ['Li', 'Miao', '']]","[{'text': 'knowledge learning', 'label': 'Few-shot Learning'}, {'text': 'knowledge learning', 'label': 'Few-shot Learning'}, {'text': 'language models', 'label': 'Large Language Model'}, {'text': 'language models', 'label': 'Large Language Model'}, {'text': 'knowledge\nlearning', 'label': 'Few-shot Learning'}]",Large Language Model,language models,0.8378260731697083
2409.18042,Kai Chen,"Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu,
  Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, Dingdong Wang, Kun Xiang, Haoyuan
  Li, Haoli Bai, Jianhua Han, Xiaohui Li, Weike Jin, Nian Xie, Yu Zhang, James
  T. Kwok, Hengshuang Zhao, Xiaodan Liang, Dit-Yan Yeung, Xiao Chen, Zhenguo
  Li, Wei Zhang, Qun Liu, Jun Yao, Lanqing Hong, Lu Hou, Hang Xu","EMOVA: Empowering Language Models to See, Hear and Speak with Vivid
  Emotions",Accepted by CVPR 2025. Project Page: https://emova-ollm.github.io/,,,,cs.CV cs.CL,http://creativecommons.org/licenses/by/4.0/,"  GPT-4o, an omni-modal model that enables vocal conversations with diverse
emotions and tones, marks a milestone for omni-modal foundation models.
However, empowering Large Language Models to perceive and generate images,
texts, and speeches end-to-end with publicly available data remains challenging
for the open-source community. Existing vision-language models rely on external
tools for speech processing, while speech-language models still suffer from
limited or totally without vision-understanding capabilities. To address this
gap, we propose the EMOVA (EMotionally Omni-present Voice Assistant), to enable
Large Language Models with end-to-end speech abilities while maintaining the
leading vision-language performance. With a semantic-acoustic disentangled
speech tokenizer, we surprisingly notice that omni-modal alignment can further
enhance vision-language and speech abilities compared with the bi-modal aligned
counterparts. Moreover, a lightweight style module is introduced for the
flexible speech style controls including emotions and pitches. For the first
time, EMOVA achieves state-of-the-art performance on both the vision-language
and speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue
with vivid emotions.
","[{'version': 'v1', 'created': 'Thu, 26 Sep 2024 16:44:02 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Oct 2024 06:25:52 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 14:51:04 GMT'}]",2025-03-14,"[['Chen', 'Kai', ''], ['Gou', 'Yunhao', ''], ['Huang', 'Runhui', ''], ['Liu', 'Zhili', ''], ['Tan', 'Daxin', ''], ['Xu', 'Jing', ''], ['Wang', 'Chunwei', ''], ['Zhu', 'Yi', ''], ['Zeng', 'Yihan', ''], ['Yang', 'Kuo', ''], ['Wang', 'Dingdong', ''], ['Xiang', 'Kun', ''], ['Li', 'Haoyuan', ''], ['Bai', 'Haoli', ''], ['Han', 'Jianhua', ''], ['Li', 'Xiaohui', ''], ['Jin', 'Weike', ''], ['Xie', 'Nian', ''], ['Zhang', 'Yu', ''], ['Kwok', 'James T.', ''], ['Zhao', 'Hengshuang', ''], ['Liang', 'Xiaodan', ''], ['Yeung', 'Dit-Yan', ''], ['Chen', 'Xiao', ''], ['Li', 'Zhenguo', ''], ['Zhang', 'Wei', ''], ['Liu', 'Qun', ''], ['Yao', 'Jun', ''], ['Hong', 'Lanqing', ''], ['Hou', 'Lu', ''], ['Xu', 'Hang', '']]","[{'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'open-source community', 'label': 'Open-source LLMs'}, {'text': 'vision-language models', 'label': 'Large Language Model'}, {'text': 'speech-language models', 'label': 'Large Language Model'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2409.20548,Anxing Xiao,"Anxing Xiao, Nuwan Janaka, Tianrun Hu, Anshul Gupta, Kaixin Li, Cunjun
  Yu, David Hsu","Robi Butler: Multimodal Remote Interaction with a Household Robot
  Assistant",Accepted to ICRA 2025,,,,cs.RO cs.AI cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Imagine a future when we can Zoom-call a robot to manage household chores
remotely. This work takes one step in this direction. Robi Butler is a new
household robot assistant that enables seamless multimodal remote interaction.
It allows the human user to monitor its environment from a first-person view,
issue voice or text commands, and specify target objects through hand-pointing
gestures. At its core, a high-level behavior module, powered by Large Language
Models (LLMs), interprets multimodal instructions to generate multistep action
plans. Each plan consists of open-vocabulary primitives supported by
vision-language models, enabling the robot to process both textual and gestural
inputs. Zoom provides a convenient interface to implement remote interactions
between the human and the robot. The integration of these components allows
Robi Butler to ground remote multimodal instructions in real-world home
environments in a zero-shot manner. We evaluated the system on various
household tasks, demonstrating its ability to execute complex user commands
with multimodal inputs. We also conducted a user study to examine how
multimodal interaction influences user experiences in remote human-robot
interaction. These results suggest that with the advances in robot foundation
models, we are moving closer to the reality of remote household robot
assistants.
","[{'version': 'v1', 'created': 'Mon, 30 Sep 2024 17:49:09 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 06:00:08 GMT'}]",2025-03-11,"[['Xiao', 'Anxing', ''], ['Janaka', 'Nuwan', ''], ['Hu', 'Tianrun', ''], ['Gupta', 'Anshul', ''], ['Li', 'Kaixin', ''], ['Yu', 'Cunjun', ''], ['Hsu', 'David', '']]","[{'text': 'Large Language\nModels', 'label': 'Large Language Model'}, {'text': 'robot foundation\nmodels', 'label': 'Foundation Model'}]",Large Language Model,"Large Language
Models",0.9664971828460693
2410.00263,Kun Yuan,"Kun Yuan, Vinkle Srivastav, Nassir Navab, Nicolas Padoy","Procedure-Aware Surgical Video-language Pretraining with Hierarchical
  Knowledge Augmentation","Accepted at the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024 Spolight)",,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Surgical video-language pretraining (VLP) faces unique challenges due to the
knowledge domain gap and the scarcity of multi-modal data. This study aims to
bridge the gap by addressing issues regarding textual information loss in
surgical lecture videos and the spatial-temporal challenges of surgical VLP. We
propose a hierarchical knowledge augmentation approach and a novel
Procedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining
(PeskaVLP) framework to tackle these issues. The knowledge augmentation uses
large language models (LLM) for refining and enriching surgical concepts, thus
providing comprehensive language supervision and reducing the risk of
overfitting. PeskaVLP combines language supervision with visual
self-supervision, constructing hard negative samples and employing a Dynamic
Time Warping (DTW) based loss function to effectively comprehend the
cross-modal procedural alignment. Extensive experiments on multiple public
surgical scene understanding and cross-modal retrieval datasets show that our
proposed method significantly improves zero-shot transferring performance and
offers a generalist visual representation for further advancements in surgical
scene understanding.The code is available at
https://github.com/CAMMA-public/SurgVLP
","[{'version': 'v1', 'created': 'Mon, 30 Sep 2024 22:21:05 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 15:21:36 GMT'}]",2025-03-14,"[['Yuan', 'Kun', ''], ['Srivastav', 'Vinkle', ''], ['Navab', 'Nassir', ''], ['Padoy', 'Nicolas', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2410.01727,Yilmazcan Ozyurt,"Yilmazcan Ozyurt, Stefan Feuerriegel, Mrinmaya Sachan","Automated Knowledge Concept Annotation and Question Representation
  Learning for Knowledge Tracing",,,,,cs.LG cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge tracing (KT) is a popular approach for modeling students' learning
progress over time, which can enable more personalized and adaptive learning.
However, existing KT approaches face two major limitations: (1) they rely
heavily on expert-defined knowledge concepts (KCs) in questions, which is
time-consuming and prone to errors; and (2) KT methods tend to overlook the
semantics of both questions and the given KCs. In this work, we address these
challenges and present KCQRL, a framework for automated knowledge concept
annotation and question representation learning that can improve the
effectiveness of any existing KT model. First, we propose an automated KC
annotation process using large language models (LLMs), which generates question
solutions and then annotates KCs in each solution step of the questions.
Second, we introduce a contrastive learning approach to generate semantically
rich embeddings for questions and solution steps, aligning them with their
associated KCs via a tailored false negative elimination approach. These
embeddings can be readily integrated into existing KT models, replacing their
randomly initialized embeddings. We demonstrate the effectiveness of KCQRL
across 15 KT algorithms on two large real-world Math learning datasets, where
we achieve consistent performance improvements.
","[{'version': 'v1', 'created': 'Wed, 2 Oct 2024 16:37:19 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 13:09:14 GMT'}]",2025-03-14,"[['Ozyurt', 'Yilmazcan', ''], ['Feuerriegel', 'Stefan', ''], ['Sachan', 'Mrinmaya', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}]",Large Language Model,large language models,0.9664971828460693
2410.01824,Alexander Wuttke,"Alexander Wuttke, Matthias A{\ss}enmacher, Christopher Klamm, Max M.
  Lang, Quirin W\""urschinger, Frauke Kreuter","AI Conversational Interviewing: Transforming Surveys with LLMs as
  Adaptive Interviewers",,LaTeCH-CLfL2025,,,cs.HC cs.AI cs.CL,http://creativecommons.org/licenses/by-sa/4.0/,"  Traditional methods for eliciting people's opinions face a trade-off between
depth and scale: structured surveys enable large-scale data collection but
limit respondents' ability to voice their opinions in their own words, while
conversational interviews provide deeper insights but are resource-intensive.
This study explores the potential of replacing human interviewers with large
language models (LLMs) to conduct scalable conversational interviews. Our goal
is to assess the performance of AI Conversational Interviewing and to identify
opportunities for improvement in a controlled environment. We conducted a
small-scale, in-depth study with university students who were randomly assigned
to a conversational interview by either AI or human interviewers, both
employing identical questionnaires on political topics. Various quantitative
and qualitative measures assessed interviewer adherence to guidelines, response
quality, participant engagement, and overall interview efficacy. The findings
indicate the viability of AI Conversational Interviewing in producing quality
data comparable to traditional methods, with the added benefit of scalability.
We publish our data and materials for re-use and present specific
recommendations for effective implementation.
","[{'version': 'v1', 'created': 'Mon, 16 Sep 2024 16:03:08 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 09:55:22 GMT'}]",2025-03-13,"[['Wuttke', 'Alexander', ''], ['Aßenmacher', 'Matthias', ''], ['Klamm', 'Christopher', ''], ['Lang', 'Max M.', ''], ['Würschinger', 'Quirin', ''], ['Kreuter', 'Frauke', '']]","[{'text': 'large\nlanguage models', 'label': 'Large Language Model'}]",Large Language Model,"large
language models",0.9664971828460693
2410.02056,Sreyan Ghosh,"Sreyan Ghosh and Sonal Kumar and Zhifeng Kong and Rafael Valle and
  Bryan Catanzaro and Dinesh Manocha","Synthio: Augmenting Small-Scale Audio Classification Datasets with
  Synthetic Data","Accepted at ICLR 2025. Code and Checkpoints available here:
  https://github.com/Sreyan88/Synthio",,,,eess.AS cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  We present Synthio, a novel approach for augmenting small-scale audio
classification datasets with synthetic data. Our goal is to improve audio
classification accuracy with limited labeled data. Traditional data
augmentation techniques, which apply artificial transformations (e.g., adding
random noise or masking segments), struggle to create data that captures the
true diversity present in real-world audios. To address this shortcoming, we
propose to augment the dataset with synthetic audio generated from
text-to-audio (T2A) diffusion models. However, synthesizing effective
augmentations is challenging because not only should the generated data be
acoustically consistent with the underlying small-scale dataset, but they
should also have sufficient compositional diversity. To overcome the first
challenge, we align the generations of the T2A model with the small-scale
dataset using preference optimization. This ensures that the acoustic
characteristics of the generated data remain consistent with the small-scale
dataset. To address the second challenge, we propose a novel caption generation
technique that leverages the reasoning capabilities of Large Language Models to
(1) generate diverse and meaningful audio captions and (2) iteratively refine
their quality. The generated captions are then used to prompt the aligned T2A
model. We extensively evaluate Synthio on ten datasets and four simulated
limited-data settings. Results indicate our method consistently outperforms all
baselines by 0.1%-39% using a T2A model trained only on weakly-captioned
AudioSet.
","[{'version': 'v1', 'created': 'Wed, 2 Oct 2024 22:05:36 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 00:25:08 GMT'}]",2025-03-13,"[['Ghosh', 'Sreyan', ''], ['Kumar', 'Sonal', ''], ['Kong', 'Zhifeng', ''], ['Valle', 'Rafael', ''], ['Catanzaro', 'Bryan', ''], ['Manocha', 'Dinesh', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'prompt', 'label': 'Prompting'}]",Large Language Model,Large Language Models,0.9664971828460693
2410.02191,Qianru Zhang,"Qianru Zhang, Peng Yang, Junliang Yu, Haixin Wang, Xingwei He,
  Siu-Ming Yiu, Hongzhi Yin","A Survey on Point-of-Interest Recommendation: Models, Architectures, and
  Security",20 pages,TKDE 2025,,20 pages,cs.IR cs.AI cs.CE cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The widespread adoption of smartphones and Location-Based Social Networks has
led to a massive influx of spatio-temporal data, creating unparalleled
opportunities for enhancing Point-of-Interest (POI) recommendation systems.
These advanced POI systems are crucial for enriching user experiences, enabling
personalized interactions, and optimizing decision-making processes in the
digital landscape. However, existing surveys tend to focus on traditional
approaches and few of them delve into cutting-edge developments, emerging
architectures, as well as security considerations in POI recommendations. To
address this gap, our survey stands out by offering a comprehensive, up-to-date
review of POI recommendation systems, covering advancements in models,
architectures, and security aspects. We systematically examine the transition
from traditional models to advanced techniques such as large language models.
Additionally, we explore the architectural evolution from centralized to
decentralized and federated learning systems, highlighting the improvements in
scalability and privacy. Furthermore, we address the increasing importance of
security, examining potential vulnerabilities and privacy-preserving
approaches. Our taxonomy provides a structured overview of the current state of
POI recommendation, while we also identify promising directions for future
research in this rapidly advancing field.
","[{'version': 'v1', 'created': 'Thu, 3 Oct 2024 04:11:42 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 02:57:32 GMT'}]",2025-03-11,"[['Zhang', 'Qianru', ''], ['Yang', 'Peng', ''], ['Yu', 'Junliang', ''], ['Wang', 'Haixin', ''], ['He', 'Xingwei', ''], ['Yiu', 'Siu-Ming', ''], ['Yin', 'Hongzhi', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'scalability', 'label': 'AI Ethics'}]",Large Language Model,large language models,0.9664971828460693
2410.02705,Tianheng Cheng,"Zongming Li, Tianheng Cheng, Shoufa Chen, Peize Sun, Haocheng Shen,
  Longjin Ran, Xiaoxin Chen, Wenyu Liu, Xinggang Wang",ControlAR: Controllable Image Generation with Autoregressive Models,To appear in ICLR 2025,,,,cs.CV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Autoregressive (AR) models have reformulated image generation as next-token
prediction, demonstrating remarkable potential and emerging as strong
competitors to diffusion models. However, control-to-image generation, akin to
ControlNet, remains largely unexplored within AR models. Although a natural
approach, inspired by advancements in Large Language Models, is to tokenize
control images into tokens and prefill them into the autoregressive model
before decoding image tokens, it still falls short in generation quality
compared to ControlNet and suffers from inefficiency. To this end, we introduce
ControlAR, an efficient and effective framework for integrating spatial
controls into autoregressive image generation models. Firstly, we explore
control encoding for AR models and propose a lightweight control encoder to
transform spatial inputs (e.g., canny edges or depth maps) into control tokens.
Then ControlAR exploits the conditional decoding method to generate the next
image token conditioned on the per-token fusion between control and image
tokens, similar to positional encodings. Compared to prefilling tokens, using
conditional decoding significantly strengthens the control capability of AR
models but also maintains the model's efficiency. Furthermore, the proposed
ControlAR surprisingly empowers AR models with arbitrary-resolution image
generation via conditional decoding and specific controls. Extensive
experiments can demonstrate the controllability of the proposed ControlAR for
the autoregressive control-to-image generation across diverse inputs, including
edges, depths, and segmentation masks. Furthermore, both quantitative and
qualitative results indicate that ControlAR surpasses previous state-of-the-art
controllable diffusion models, e.g., ControlNet++. Code, models, and demo will
soon be available at https://github.com/hustvl/ControlAR.
","[{'version': 'v1', 'created': 'Thu, 3 Oct 2024 17:28:07 GMT'}, {'version': 'v2', 'created': 'Fri, 24 Jan 2025 05:25:24 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 06:33:37 GMT'}]",2025-03-11,"[['Li', 'Zongming', ''], ['Cheng', 'Tianheng', ''], ['Chen', 'Shoufa', ''], ['Sun', 'Peize', ''], ['Shen', 'Haocheng', ''], ['Ran', 'Longjin', ''], ['Chen', 'Xiaoxin', ''], ['Liu', 'Wenyu', ''], ['Wang', 'Xinggang', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2410.03735,David Grangier,"David Grangier, Simin Fan, Skyler Seto, Pierre Ablin","Task-Adaptive Pretrained Language Models via Clustered-Importance
  Sampling","23 pages, presented at the International Conference on Learning
  Representation (ICLR), 2025",,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Specialist language models (LMs) focus on a specific task or domain on which
they often outperform generalist LMs of the same size. However, the specialist
data needed to pretrain these models is only available in limited amount for
most tasks. In this work, we build specialist models from large generalist
training sets instead. We propose a novel method, ClusteRed Importance SamPling
(CRISP). CRISP clusters the generalist dataset and samples from these clusters
based on their frequencies in the smaller specialist dataset. It is scalable,
suitable for both pretraining and continued pretraining, and works well in
multi-task settings. CRISP performs favorably compared to other methods that
adjust the training distribution of the generalist data with guidance from the
limited domain-specific data. Our findings demonstrate improvements across
different domains in terms of language modeling perplexity and accuracy on
multiple-choice question tasks. We also present ablation studies that examine
the impact of dataset sizes, clustering configurations, and model sizes.
","[{'version': 'v1', 'created': 'Mon, 30 Sep 2024 20:49:54 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 00:20:30 GMT'}]",2025-03-12,"[['Grangier', 'David', ''], ['Fan', 'Simin', ''], ['Seto', 'Skyler', ''], ['Ablin', 'Pierre', '']]","[{'text': 'Specialist language models', 'label': 'Large Language Model'}]",Large Language Model,Specialist language models,0.7475558519363403
2410.04759,Yifan Liu,"Tianhui Cai, Yifan Liu, Zewei Zhou, Haoxuan Ma, Seth Z. Zhao, Zhiwen
  Wu and Jiaqi Ma","Driving with Regulation: Interpretable Decision-Making for Autonomous
  Vehicles with Retrieval-Augmented Reasoning via LLM",,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This work presents an interpretable decision-making framework for autonomous
vehicles that integrates traffic regulations, norms, and safety guidelines
comprehensively and enables seamless adaptation to different regions. While
traditional rule-based methods struggle to incorporate the full scope of
traffic rules, we develop a Traffic Regulation Retrieval (TRR) Agent based on
Retrieval-Augmented Generation (RAG) to automatically retrieve relevant traffic
rules and guidelines from extensive regulation documents and relevant records
based on the ego vehicle's situation. Given the semantic complexity of the
retrieved rules, we also design a reasoning module powered by a Large Language
Model (LLM) to interpret these rules, differentiate between mandatory rules and
safety guidelines, and assess actions on legal compliance and safety.
Additionally, the reasoning is designed to be interpretable, enhancing both
transparency and reliability. The framework demonstrates robust performance on
both hypothesized and real-world cases across diverse scenarios, along with the
ability to adapt to different regions with ease.
","[{'version': 'v1', 'created': 'Mon, 7 Oct 2024 05:27:22 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 04:00:16 GMT'}]",2025-03-14,"[['Cai', 'Tianhui', ''], ['Liu', 'Yifan', ''], ['Zhou', 'Zewei', ''], ['Ma', 'Haoxuan', ''], ['Zhao', 'Seth Z.', ''], ['Wu', 'Zhiwen', ''], ['Ma', 'Jiaqi', '']]","[{'text': 'Large Language\nModel', 'label': 'Large Language Model'}]",Large Language Model,"Large Language
Model",1.0
2410.04949,Yongming Chen,"Yongming Chen, Miner Chen, Ye Zhu, Juan Pei, Siyu Chen, Yu Zhou, Yi
  Wang, Yifan Zhou, Hao Li, Songan Zhang","Leverage Knowledge Graph and Large Language Model for Law Article
  Recommendation: A Case Study of Chinese Criminal Law",,,,,cs.IR cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Court efficiency is vital for social stability. However, in most countries
around the world, the grassroots courts face case backlogs, with decisions
relying heavily on judicial personnel's cognitive labor, lacking intelligent
tools to improve efficiency. To address this issue, we propose an efficient law
article recommendation approach utilizing a Knowledge Graph (KG) and a Large
Language Model (LLM). Firstly, we propose a Case-Enhanced Law Article Knowledge
Graph (CLAKG) as a database to store current law statutes, historical case
information, and correspondence between law articles and historical cases.
Additionally, we introduce an automated CLAKG construction method based on LLM.
On this basis, we propose a closed-loop law article recommendation method.
Finally, through a series of experiments using judgment documents from the
website ""China Judgements Online"", we have improved the accuracy of law article
recommendation in cases from 0.549 to 0.694, demonstrating that our proposed
method significantly outperforms baseline approaches.
","[{'version': 'v1', 'created': 'Mon, 7 Oct 2024 11:45:04 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 05:10:23 GMT'}]",2025-03-11,"[['Chen', 'Yongming', ''], ['Chen', 'Miner', ''], ['Zhu', 'Ye', ''], ['Pei', 'Juan', ''], ['Chen', 'Siyu', ''], ['Zhou', 'Yu', ''], ['Wang', 'Yi', ''], ['Zhou', 'Yifan', ''], ['Li', 'Hao', ''], ['Zhang', 'Songan', '']]","[{'text': 'Large\nLanguage Model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}]",Large Language Model,"Large
Language Model",1.0
2410.05440,Zihao Zhou,"Zihao Zhou, Rose Yu",Can LLMs Understand Time Series Anomalies?,,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have gained popularity in time series
forecasting, but their potential for anomaly detection remains largely
unexplored. Our study investigates whether LLMs can understand and detect
anomalies in time series data, focusing on zero-shot and few-shot scenarios.
Inspired by conjectures about LLMs' behavior from time series forecasting
research, we formulate key hypotheses about LLMs' capabilities in time series
anomaly detection. We design and conduct principled experiments to test each of
these hypotheses. Our investigation reveals several surprising findings about
LLMs for time series: (1) LLMs understand time series better as images rather
than as text, (2) LLMs do not demonstrate enhanced performance when prompted to
engage in explicit reasoning about time series analysis. (3) Contrary to common
beliefs, LLMs' understanding of time series does not stem from their repetition
biases or arithmetic abilities. (4) LLMs' behaviors and performance in time
series analysis vary significantly across different models. This study provides
the first comprehensive analysis of contemporary LLM capabilities in time
series anomaly detection. Our results suggest that while LLMs can understand
trivial time series anomalies, we have no evidence that they can understand
more subtle real-world anomalies. Many common conjectures based on their
reasoning capabilities do not hold. All synthetic dataset generators, final
prompts, and evaluation scripts have been made available in
https://github.com/rose-stl-lab/anomllm.
","[{'version': 'v1', 'created': 'Mon, 7 Oct 2024 19:16:02 GMT'}, {'version': 'v2', 'created': 'Mon, 14 Oct 2024 23:32:50 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 18:04:52 GMT'}]",2025-03-13,"[['Zhou', 'Zihao', ''], ['Yu', 'Rose', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'zero-shot and few-shot scenarios', 'label': 'Zero-shot Learning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'final\nprompts', 'label': 'Prompting'}]",Large Language Model,Large Language Models,0.9664971828460693
2410.10855,Hokin Deng,"Yijiang Li, Qingying Gao, Tianwei Zhao, Bingyang Wang, Haoran Sun,
  Haiyun Lyu, Dezhi Luo, Hokin Deng",Core Knowledge Deficits in Multi-Modal Language Models,"Website with this
  $\href{https://growing-ai-like-a-child.github.io/}{link}$",,,,cs.CL cs.AI cs.CV,http://creativecommons.org/licenses/by/4.0/,"  While Multimodal Large Language Models (MLLMs) demonstrate impressive
abilities over high level perception and reasoning, their robustness in the
wild still lags behind humans and exhibits diminished efficacy on simple tasks
that are intuitive for humans. We examine the hypothesis that these
deficiencies stem from the absence of core knowledge, rudimentary cognitive
abilities innate to humans from early childhood. To probe core knowledge
representation in MLLMs, we draw from developmental cognitive sciences and
develop a large-scale benchmark, CoreCognition dataset, encompassing 12 core
cognitive concepts. We evaluate 219 models with 10 different prompts, leading
to a total of 2409 data points for analysis. Our findings reveal core knowledge
deficits in early developed core abilities while models demonstrate human
comparable performance in high level cognition. Moreover, we find that low
level abilities show little to no scaling, in stark contrast to high level
abilities. Finally, we introduce an evaluation technique, Concept Hacking,
through which we demonstrate that MLLMs do not genuinely advance toward core
knowledge but instead rely on illusory understanding and shortcut learning as
they scale. Website with this
$\href{https://growing-ai-like-a-child.github.io/}{link}$.
","[{'version': 'v1', 'created': 'Sun, 6 Oct 2024 20:13:11 GMT'}, {'version': 'v2', 'created': 'Sat, 2 Nov 2024 21:07:54 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 04:39:42 GMT'}]",2025-03-11,"[['Li', 'Yijiang', ''], ['Gao', 'Qingying', ''], ['Zhao', 'Tianwei', ''], ['Wang', 'Bingyang', ''], ['Sun', 'Haoran', ''], ['Lyu', 'Haiyun', ''], ['Luo', 'Dezhi', ''], ['Deng', 'Hokin', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': '10 different prompts', 'label': 'Prompting'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2410.11996,Seiji Maekawa,"Seiji Maekawa, Hayate Iso, Nikita Bhutani","Holistic Reasoning with Long-Context LMs: A Benchmark for Database
  Operations on Massive Textual Data",ICLR2025,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The rapid increase in textual information means we need more efficient
methods to sift through, organize, and understand it all. While
retrieval-augmented generation (RAG) models excel in accessing information from
large document collections, they struggle with complex tasks that require
aggregation and reasoning over information spanning across multiple
documents--what we call holistic reasoning. Long-context language models
(LCLMs) have great potential for managing large-scale documents, but their
holistic reasoning capabilities remain unclear. In this work, we introduce
HoloBench, a novel framework that brings database reasoning operations into
text-based contexts, making it easier to systematically evaluate how LCLMs
handle holistic reasoning across large documents. Our approach adjusts key
factors such as context length, information density, distribution of
information, and query complexity to evaluate LCLMs comprehensively. Our
experiments show that the amount of information in the context has a bigger
influence on LCLM performance than the actual context length. Furthermore, the
complexity of queries affects performance more than the amount of information,
particularly for different types of queries. Interestingly, queries that
involve finding maximum or minimum values are easier for LCLMs and are less
affected by context length, even though they pose challenges for RAG systems.
However, tasks requiring the aggregation of multiple pieces of information show
a noticeable drop in accuracy as context length increases. Additionally, we
find that while grouping relevant information generally improves performance,
the optimal positioning varies across models. Our findings surface both the
advancements and the ongoing challenges in achieving a holistic understanding
of long contexts.
","[{'version': 'v1', 'created': 'Tue, 15 Oct 2024 19:04:13 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Feb 2025 01:36:03 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 18:21:04 GMT'}]",2025-03-14,"[['Maekawa', 'Seiji', ''], ['Iso', 'Hayate', ''], ['Bhutani', 'Nikita', '']]","[{'text': 'Long-context language models', 'label': 'Large Language Model'}, {'text': 'LCLMs', 'label': 'Large Language Model'}, {'text': 'text-based contexts', 'label': 'contextual Embedding'}, {'text': 'LCLMs', 'label': 'Large Language Model'}, {'text': 'LCLMs', 'label': 'Large Language Model'}, {'text': 'LCLMs', 'label': 'Large Language Model'}]",Large Language Model,Long-context language models,0.7303164601325989
2410.14211,Xingyu Tan,"Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Wenjie Zhang","Paths-over-Graph: Knowledge Graph Empowered Large Language Model
  Reasoning","Accepted by The Web Conference 2025 (WWW, 2025)",,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) have achieved impressive results in various
tasks but struggle with hallucination problems and lack of relevant knowledge,
especially in deep complex reasoning and knowledge-intensive tasks. Knowledge
Graphs (KGs), which capture vast amounts of facts in a structured format, offer
a reliable source of knowledge for reasoning. However, existing KG-based LLM
reasoning methods face challenges like handling multi-hop reasoning,
multi-entity questions, and effectively utilizing graph structures. To address
these issues, we propose Paths-over-Graph (PoG), a novel method that enhances
LLM reasoning by integrating knowledge reasoning paths from KGs, improving the
interpretability and faithfulness of LLM outputs. PoG tackles multi-hop and
multi-entity questions through a three-phase dynamic multi-hop path
exploration, which combines the inherent knowledge of LLMs with factual
knowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant
information from the graph exploration first and introduces efficient
three-step pruning techniques that incorporate graph structures, LLM prompting,
and a pre-trained language model (e.g., SBERT) to effectively narrow down the
explored candidate paths. This ensures all reasoning paths contain highly
relevant information captured from KGs, making the reasoning faithful and
interpretable in problem-solving. PoG innovatively utilizes graph structure to
prune the irrelevant noise and represents the first method to implement
multi-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive
experiments on five benchmark KGQA datasets demonstrate PoG outperforms the
state-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an
average accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo
surpasses ToG with GPT-4 by up to 23.9%.
","[{'version': 'v1', 'created': 'Fri, 18 Oct 2024 06:57:19 GMT'}, {'version': 'v2', 'created': 'Mon, 21 Oct 2024 01:22:16 GMT'}, {'version': 'v3', 'created': 'Tue, 28 Jan 2025 04:31:11 GMT'}, {'version': 'v4', 'created': 'Wed, 12 Mar 2025 23:45:13 GMT'}]",2025-03-14,"[['Tan', 'Xingyu', ''], ['Wang', 'Xiaoyang', ''], ['Liu', 'Qing', ''], ['Xu', 'Xiwei', ''], ['Yuan', 'Xin', ''], ['Zhang', 'Wenjie', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'PoG', 'label': 'LLM-based'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'SBERT', 'label': 'BERT'}, {'text': 'GPT-3.5-Turbo', 'label': 'GPT'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'GPT-3.5-Turbo', 'label': 'GPT'}, {'text': 'GPT-4', 'label': 'GPT'}]",Large Language Model,Large Language Models,0.9664971828460693
2410.18469,Chung En Sun,"Chung-En Sun, Xiaodong Liu, Weiwei Yang, Tsui-Wei Weng, Hao Cheng,
  Aidan San, Michel Galley, Jianfeng Gao",Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities,Accepted to NAACL 2025 Main (oral),,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent research has shown that Large Language Models (LLMs) are vulnerable to
automated jailbreak attacks, where adversarial suffixes crafted by algorithms
appended to harmful queries bypass safety alignment and trigger unintended
responses. Current methods for generating these suffixes are computationally
expensive and have low Attack Success Rates (ASR), especially against
well-aligned models like Llama2 and Llama3. To overcome these limitations, we
introduce ADV-LLM, an iterative self-tuning process that crafts adversarial
LLMs with enhanced jailbreak ability. Our framework significantly reduces the
computational cost of generating adversarial suffixes while achieving nearly
100\% ASR on various open-source LLMs. Moreover, it exhibits strong attack
transferability to closed-source models, achieving 99\% ASR on GPT-3.5 and 49\%
ASR on GPT-4, despite being optimized solely on Llama3. Beyond improving
jailbreak ability, ADV-LLM provides valuable insights for future safety
alignment research through its ability to generate large datasets for studying
LLM safety.
","[{'version': 'v1', 'created': 'Thu, 24 Oct 2024 06:36:12 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Oct 2024 23:05:59 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 23:26:25 GMT'}]",2025-03-13,"[['Sun', 'Chung-En', ''], ['Liu', 'Xiaodong', ''], ['Yang', 'Weiwei', ''], ['Weng', 'Tsui-Wei', ''], ['Cheng', 'Hao', ''], ['San', 'Aidan', ''], ['Galley', 'Michel', ''], ['Gao', 'Jianfeng', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Llama2', 'label': 'Llama'}, {'text': 'Llama3', 'label': 'Llama'}, {'text': 'ADV-LLM', 'label': 'Open-source LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GPT-3', 'label': 'GPT'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'Llama3', 'label': 'Llama'}]",Large Language Model,Large Language Models,0.9664971828460693
2410.19482,Jamie Hayes,"Jamie Hayes, Marika Swanberg, Harsh Chaudhari, Itay Yona, Ilia
  Shumailov, Milad Nasr, Christopher A. Choquette-Choo, Katherine Lee, A. Feder
  Cooper",Measuring memorization in language models via probabilistic extraction,NAACL 25,,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large language models (LLMs) are susceptible to memorizing training data,
raising concerns about the potential extraction of sensitive information at
generation time. Discoverable extraction is the most common method for
measuring this issue: split a training example into a prefix and suffix, then
prompt the LLM with the prefix, and deem the example extractable if the LLM
generates the matching suffix using greedy sampling. This definition yields a
yes-or-no determination of whether extraction was successful with respect to a
single query. Though efficient to compute, we show that this definition is
unreliable because it does not account for non-determinism present in more
realistic (non-greedy) sampling schemes, for which LLMs produce a range of
outputs for the same prompt. We introduce probabilistic discoverable
extraction, which, without additional cost, relaxes discoverable extraction by
considering multiple queries to quantify the probability of extracting a target
sequence. We evaluate our probabilistic measure across different models,
sampling schemes, and training-data repetitions, and find that this measure
provides more nuanced information about extraction risk compared to traditional
discoverable extraction.
","[{'version': 'v1', 'created': 'Fri, 25 Oct 2024 11:37:04 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 14:25:10 GMT'}]",2025-03-13,"[['Hayes', 'Jamie', ''], ['Swanberg', 'Marika', ''], ['Chaudhari', 'Harsh', ''], ['Yona', 'Itay', ''], ['Shumailov', 'Ilia', ''], ['Nasr', 'Milad', ''], ['Choquette-Choo', 'Christopher A.', ''], ['Lee', 'Katherine', ''], ['Cooper', 'A. Feder', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2410.20215,Xinyu Tang,"Xinyu Tang, Xiaolei Wang, Wayne Xin Zhao, Ji-Rong Wen","DAWN-ICL: Strategic Planning of Problem-solving Trajectories for
  Zero-Shot In-Context Learning",NAACL 2025 Main Conference,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Zero-shot in-context learning (ZS-ICL) aims to conduct in-context learning
(ICL) without using human-annotated demonstrations. Most ZS-ICL methods use
large language models (LLMs) to generate (input, label) pairs as
pseudo-demonstrations and leverage historical pseudo-demonstrations to help
solve the current problem. They assume that problems are from the same task and
traverse them in a random order. However, in real-world scenarios, problems
usually come from diverse tasks, and only a few belong to the same task. The
random traversing order may generate unreliable pseudo-demonstrations and lead
to error accumulation. To address this problem, we reformulate ZS-ICL as a
planning problem and propose a Demonstration-aware Monte Carlo Tree Search
(MCTS) approach (DAWN-ICL), which leverages MCTS to strategically plan the
problem-solving trajectories for ZS-ICL. In addition, to achieve effective and
efficient Q value estimation, we propose a novel demonstration-aware Q-value
function and use it to enhance the selection phase and accelerate the expansion
and simulation phases in MCTS. Extensive experiments demonstrate the
effectiveness and efficiency of DAWN-ICL on in-domain and cross-domain
scenarios, and it even outperforms ICL using human-annotated labels. The code
is available at https://github.com/RUCAIBox/MCTS4ZSICL.
","[{'version': 'v1', 'created': 'Sat, 26 Oct 2024 16:17:02 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 04:20:07 GMT'}]",2025-03-11,"[['Tang', 'Xinyu', ''], ['Wang', 'Xiaolei', ''], ['Zhao', 'Wayne Xin', ''], ['Wen', 'Ji-Rong', '']]","[{'text': 'Zero-shot in-context learning', 'label': 'Few-shot Learning'}, {'text': 'ZS-ICL', 'label': 'Zero-shot Learning'}, {'text': 'in-context learning', 'label': 'Few-shot Learning'}, {'text': 'ICL', 'label': 'Zero-shot Learning'}, {'text': 'ZS-ICL', 'label': 'Few-shot Learning'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ZS-ICL', 'label': 'Few-shot Learning'}, {'text': 'ZS-ICL', 'label': 'Few-shot Learning'}, {'text': 'ICL', 'label': 'Few-shot Learning'}]",Large Language Model,large language models,0.9664971828460693
2410.20666,Sangmim Song Mr,"Sangmim Song, Sarath Kodagoda, Amal Gunatilake, Marc G. Carmichael,
  Karthick Thiyagarajan, Jodi Martin","Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for
  Robotic Guidance of People with Visual Impairments",,,,,cs.RO cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Navigation presents a significant challenge for persons with visual
impairments (PVI). While traditional aids such as white canes and guide dogs
are invaluable, they fall short in delivering detailed spatial information and
precise guidance to desired locations. Recent developments in large language
models (LLMs) and vision-language models (VLMs) offer new avenues for enhancing
assistive navigation. In this paper, we introduce Guide-LLM, an embodied
LLM-based agent designed to assist PVI in navigating large indoor environments.
Our approach features a novel text-based topological map that enables the LLM
to plan global paths using a simplified environmental representation, focusing
on straight paths and right-angle turns to facilitate navigation. Additionally,
we utilize the LLM's commonsense reasoning for hazard detection and
personalized path planning based on user preferences. Simulated experiments
demonstrate the system's efficacy in guiding PVI, underscoring its potential as
a significant advancement in assistive technology. The results highlight
Guide-LLM's ability to offer efficient, adaptive, and personalized navigation
assistance, pointing to promising advancements in this field.
","[{'version': 'v1', 'created': 'Mon, 28 Oct 2024 01:58:21 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 23:45:58 GMT'}]",2025-03-13,"[['Song', 'Sangmim', ''], ['Kodagoda', 'Sarath', ''], ['Gunatilake', 'Amal', ''], ['Carmichael', 'Marc G.', ''], ['Thiyagarajan', 'Karthick', ''], ['Martin', 'Jodi', '']]","[{'text': 'large language\nmodels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'Guide-LLM', 'label': 'LLM'}, {'text': 'Guide-LLM', 'label': 'LLM'}]",Large Language Model,"large language
models",0.9664971828460693
2410.22592,Royi Rassin,"Royi Rassin, Aviv Slobodkin, Shauli Ravfogel, Yanai Elazar, Yoav
  Goldberg",GRADE: Quantifying Sample Diversity in Text-to-Image Models,For project page and code see https://royira.github.io/GRADE,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  We introduce GRADE, an automatic method for quantifying sample diversity in
text-to-image models. Our method leverages the world knowledge embedded in
large language models and visual question-answering systems to identify
relevant concept-specific axes of diversity (e.g., ``shape'' for the concept
``cookie''). It then estimates frequency distributions of concepts and their
attributes and quantifies diversity using entropy. We use GRADE to measure the
diversity of 12 models over a total of 720K images, revealing that all models
display limited variation, with clear deterioration in stronger models.
Further, we find that models often exhibit default behaviors, a phenomenon
where a model consistently generates concepts with the same attributes (e.g.,
98% of the cookies are round). Lastly, we show that a key reason for low
diversity is underspecified captions in training data. Our work proposes an
automatic, semantically-driven approach to measure sample diversity and
highlights the stunning homogeneity in text-to-image models.
","[{'version': 'v1', 'created': 'Tue, 29 Oct 2024 23:10:28 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 07:44:10 GMT'}]",2025-03-12,"[['Rassin', 'Royi', ''], ['Slobodkin', 'Aviv', ''], ['Ravfogel', 'Shauli', ''], ['Elazar', 'Yanai', ''], ['Goldberg', 'Yoav', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2411.00915,Liang Mi,"Liang Mi, Weijun Wang, Wenming Tu, Qingfeng He, Rui Kong, Xinyu Fang,
  Yazhu Dong, Yikang Zhang, Yunchun Li, Meng Li, Haipeng Dai, Guihai Chen,
  Yunxin Liu","V-LoRA: An Efficient and Flexible System Boosts Vision Applications with
  LoRA LMM",EuroSys'2025,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Multimodal Models (LMMs) have shown significant progress in various
complex vision tasks with the solid linguistic and reasoning capacity inherited
from large language models (LMMs). Low-rank adaptation (LoRA) offers a
promising method to integrate external knowledge into LMMs, compensating for
their limitations on domain-specific tasks. However, the existing LoRA model
serving is excessively computationally expensive and causes extremely high
latency. In this paper, we present an end-to-end solution that empowers diverse
vision tasks and enriches vision applications with LoRA LMMs. Our system,
VaLoRA, enables accurate and efficient vision tasks by 1) an accuracy-aware
LoRA adapter generation approach that generates LoRA adapters rich in
domain-specific knowledge to meet application-specific accuracy requirements,
2) an adaptive-tiling LoRA adapters batching operator that efficiently computes
concurrent heterogeneous LoRA adapters, and 3) a flexible LoRA adapter
orchestration mechanism that manages application requests and LoRA adapters to
achieve the lowest average response latency. We prototype VaLoRA on five
popular vision tasks on three LMMs. Experiment results reveal that VaLoRA
improves 24-62% of the accuracy compared to the original LMMs and reduces
20-89% of the latency compared to the state-of-the-art LoRA model serving
systems.
","[{'version': 'v1', 'created': 'Fri, 1 Nov 2024 13:43:33 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Feb 2025 05:57:42 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 13:26:38 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 08:38:15 GMT'}]",2025-03-14,"[['Mi', 'Liang', ''], ['Wang', 'Weijun', ''], ['Tu', 'Wenming', ''], ['He', 'Qingfeng', ''], ['Kong', 'Rui', ''], ['Fang', 'Xinyu', ''], ['Dong', 'Yazhu', ''], ['Zhang', 'Yikang', ''], ['Li', 'Yunchun', ''], ['Li', 'Meng', ''], ['Dai', 'Haipeng', ''], ['Chen', 'Guihai', ''], ['Liu', 'Yunxin', '']]","[{'text': 'Large Multimodal Models', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}]",Large Language Model,Large Multimodal Models,0.573912501335144
2411.02348,Claire Stevenson,"Claire E. Stevenson, Alexandra Pafford, Han L. J. van der Maas,
  Melanie Mitchell",Can Large Language Models generalize analogy solving like people can?,,,,,cs.AI cs.CL cs.HC,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  When we solve an analogy we transfer information from a known context to a
new one through abstract rules and relational similarity. In people, the
ability to solve analogies such as ""body : feet :: table : ?"" emerges in
childhood, and appears to transfer easily to other domains, such as the visual
domain ""( : ) :: < : ?"". Recent research shows that large language models
(LLMs) can solve various forms of analogies. However, can LLMs generalize
analogy solving to new domains like people can? To investigate this, we had
children, adults, and LLMs solve a series of letter-string analogies (e.g., a b
: a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek
alphabet), and a far transfer domain (list of symbols). As expected, children
and adults easily generalized their knowledge to unfamiliar domains, whereas
LLMs did not. This key difference between human and AI performance is evidence
that these LLMs still struggle with robust human-like analogical transfer.
","[{'version': 'v1', 'created': 'Mon, 4 Nov 2024 18:18:38 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 19:51:32 GMT'}]",2025-03-13,"[['Stevenson', 'Claire E.', ''], ['Pafford', 'Alexandra', ''], ['van der Maas', 'Han L. J.', ''], ['Mitchell', 'Melanie', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2411.08127,SangHyun Park,"Shih-Ying Yeh, Sang-Hyun Park, Yi Li, Giyeong Oh, Xuehai Wang, Min
  Song, Youngjae Yu",TIPO: Text to Image with Text Presampling for Prompt Optimization,"41 pages, 32 figures",,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  TIPO (Text-to-Image Prompt Optimization) introduces an efficient approach for
automatic prompt refinement in text-to-image (T2I) generation. Starting from
simple user prompts, TIPO leverages a lightweight pre-trained model to expand
these prompts into richer, detailed versions. Conceptually, TIPO samples
refined prompts from a targeted sub-distribution within the broader semantic
space, preserving the original intent while significantly improving visual
quality, coherence, and detail. Unlike resource-intensive methods based on
large language models (LLMs) or reinforcement learning (RL), TIPO provides
computational efficiency and scalability, opening new possibilities for
effective, automated prompt engineering in T2I tasks.
  We provide visual results, human preference report to investigate TIPO's
effectiveness. Experimental evaluations on benchmark datasets demonstrate
substantial improvements in aesthetic quality, significant reduction of visual
artifacts, and enhanced alignment with target distributions along with
significant human preference proficiency. These results highlight the
importance of targeted prompt engineering in text-to-image tasks and indicate
broader opportunities for automated prompt refinement.
","[{'version': 'v1', 'created': 'Tue, 12 Nov 2024 19:09:45 GMT'}, {'version': 'v2', 'created': 'Fri, 22 Nov 2024 14:58:31 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 18:21:57 GMT'}]",2025-03-13,"[['Yeh', 'Shih-Ying', ''], ['Park', 'Sang-Hyun', ''], ['Li', 'Yi', ''], ['Oh', 'Giyeong', ''], ['Wang', 'Xuehai', ''], ['Song', 'Min', ''], ['Yu', 'Youngjae', '']]","[{'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'scalability', 'label': 'Scaling law'}]",Large Language Model,large language models,0.9664971828460693
2411.12279,Ziyang Zong,"Ziyang Zong, Guanying Chen, Zhaohuan Zhan, Fengcheng Yu, Guang Tan",HouseTune: Two-Stage Floorplan Generation with LLM Assistance,,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  This paper proposes a two-stage text-to-floorplan generation framework that
combines the reasoning capability of Large Language Models (LLMs) with the
generative power of diffusion models. In the first stage, we leverage a
Chain-of-Thought (CoT) prompting strategy to guide an LLM in generating an
initial layout (Layout-Init) from natural language descriptions, which ensures
a user-friendly and intuitive design process. However, Layout-Init may lack
precise geometric alignment and fine-grained structural details. To address
this, the second stage employs a conditional diffusion model to refine
Layout-Init into a final floorplan (Layout-Final) that better adheres to
physical constraints and user requirements. Unlike prior methods, our approach
effectively reduces the difficulty of floorplan generation learning without the
need for extensive domain-specific training data. Experimental results
demonstrate that our approach achieves state-of-the-art performance across all
metrics, which validates its effectiveness in practical home design
applications.
","[{'version': 'v1', 'created': 'Tue, 19 Nov 2024 06:57:45 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Nov 2024 05:05:48 GMT'}, {'version': 'v3', 'created': 'Sun, 1 Dec 2024 02:12:08 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Mar 2025 11:08:17 GMT'}]",2025-03-11,"[['Zong', 'Ziyang', ''], ['Chen', 'Guanying', ''], ['Zhan', 'Zhaohuan', ''], ['Yu', 'Fengcheng', ''], ['Tan', 'Guang', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2411.12789,Haoyu Zhao,"Haoyu Zhao, Hao Wang, Xingyue Zhao, Hao Fei, Hongqiu Wang, Chengjiang
  Long, Hua Zou","Efficient Physics Simulation for 3D Scenes via MLLM-Guided Gaussian
  Splatting",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advancements in 3D generation models have opened new possibilities for
simulating dynamic 3D object movements and customizing behaviors, yet creating
this content remains challenging. Current methods often require manual
assignment of precise physical properties for simulations or rely on video
generation models to predict them, which is computationally intensive. In this
paper, we rethink the usage of multi-modal large language model (MLLM) in
physics-based simulation, and present Sim Anything, a physics-based approach
that endows static 3D objects with interactive dynamics. We begin with detailed
scene reconstruction and object-level 3D open-vocabulary segmentation,
progressing to multi-view image in-painting. Inspired by human visual
reasoning, we propose MLLM-based Physical Property Perception (MLLM-P3) to
predict mean physical properties of objects in a zero-shot manner. Based on the
mean values and the object's geometry, the Material Property Distribution
Prediction model (MPDP) model then estimates the full distribution,
reformulating the problem as probability distribution estimation to reduce
computational costs. Finally, we simulate objects in an open-world scene with
particles sampled via the Physical-Geometric Adaptive Sampling (PGAS) strategy,
efficiently capturing complex deformations and significantly reducing
computational costs. Extensive experiments and user studies demonstrate our Sim
Anything achieves more realistic motion than state-of-the-art methods within 2
minutes on a single GPU.
","[{'version': 'v1', 'created': 'Tue, 19 Nov 2024 12:52:21 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 04:46:09 GMT'}]",2025-03-12,"[['Zhao', 'Haoyu', ''], ['Wang', 'Hao', ''], ['Zhao', 'Xingyue', ''], ['Fei', 'Hao', ''], ['Wang', 'Hongqiu', ''], ['Long', 'Chengjiang', ''], ['Zou', 'Hua', '']]","[{'text': 'multi-modal large language model', 'label': 'Large Language Model'}, {'text': 'MLLM', 'label': 'Large Language Model'}]",Large Language Model,multi-modal large language model,0.8085874915122986
2411.15594,Xuhui Jiang,"Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai,
  Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang,
  Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni, Jian Guo",A Survey on LLM-as-a-Judge,Project Page: https://awesome-llm-as-a-judge.github.io/,,,,cs.CL cs.AI,http://creativecommons.org/publicdomain/zero/1.0/,"  Accurate and consistent evaluation is crucial for decision-making across
numerous fields, yet it remains a challenging task due to inherent
subjectivity, variability, and scale. Large Language Models (LLMs) have
achieved remarkable success across diverse domains, leading to the emergence of
""LLM-as-a-Judge,"" where LLMs are employed as evaluators for complex tasks. With
their ability to process diverse data types and provide scalable,
cost-effective, and consistent assessments, LLMs present a compelling
alternative to traditional expert-driven evaluations. However, ensuring the
reliability of LLM-as-a-Judge systems remains a significant challenge that
requires careful design and standardization. This paper provides a
comprehensive survey of LLM-as-a-Judge, addressing the core question: How can
reliable LLM-as-a-Judge systems be built? We explore strategies to enhance
reliability, including improving consistency, mitigating biases, and adapting
to diverse assessment scenarios. Additionally, we propose methodologies for
evaluating the reliability of LLM-as-a-Judge systems, supported by a novel
benchmark designed for this purpose. To advance the development and real-world
deployment of LLM-as-a-Judge systems, we also discussed practical applications,
challenges, and future directions. This survey serves as a foundational
reference for researchers and practitioners in this rapidly evolving field.
","[{'version': 'v1', 'created': 'Sat, 23 Nov 2024 16:03:35 GMT'}, {'version': 'v2', 'created': 'Mon, 16 Dec 2024 15:00:53 GMT'}, {'version': 'v3', 'created': 'Thu, 9 Jan 2025 03:08:17 GMT'}, {'version': 'v4', 'created': 'Sat, 1 Feb 2025 08:55:51 GMT'}, {'version': 'v5', 'created': 'Sun, 9 Mar 2025 05:21:22 GMT'}]",2025-03-11,"[['Gu', 'Jiawei', ''], ['Jiang', 'Xuhui', ''], ['Shi', 'Zhichao', ''], ['Tan', 'Hexiang', ''], ['Zhai', 'Xuehao', ''], ['Xu', 'Chengjin', ''], ['Li', 'Wei', ''], ['Shen', 'Yinghan', ''], ['Ma', 'Shengjie', ''], ['Liu', 'Honghao', ''], ['Wang', 'Saizhuo', ''], ['Zhang', 'Kun', ''], ['Wang', 'Yuanzhuo', ''], ['Gao', 'Wen', ''], ['Ni', 'Lionel', ''], ['Guo', 'Jian', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM-as-a-Judge', 'label': 'Large Language Model'}, {'text': 'LLM-as-a-Judge', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2411.17017,Zhenchen Wan,"Zhenchen Wan, Yanwu Xu, Zhaoqing Wang, Feng Liu, Tongliang Liu,
  Mingming Gong",TED-VITON: Transformer-Empowered Diffusion Models for Virtual Try-On,Project page: https://github.com/ZhenchenWan/TED-VITON,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advancements in Virtual Try-On (VTO) have demonstrated exceptional
efficacy in generating realistic images and preserving garment details, largely
attributed to the robust generative capabilities of text-to-image (T2I)
diffusion backbones. However, the T2I models that underpin these methods have
become outdated, thereby limiting the potential for further improvement in VTO.
Additionally, current methods face notable challenges in accurately rendering
text on garments without distortion and preserving fine-grained details, such
as textures and material fidelity. The emergence of Diffusion Transformer (DiT)
based T2I models has showcased impressive performance and offers a promising
opportunity for advancing VTO. Directly applying existing VTO techniques to
transformer-based T2I models is ineffective due to substantial architectural
differences, which hinder their ability to fully leverage the models' advanced
capabilities for improved text generation. To address these challenges and
unlock the full potential of DiT-based T2I models for VTO, we propose
TED-VITON, a novel framework that integrates a Garment Semantic (GS) Adapter
for enhancing garment-specific features, a Text Preservation Loss to ensure
accurate and distortion-free text rendering, and a constraint mechanism to
generate prompts by optimizing Large Language Model (LLM). These innovations
enable state-of-the-art (SOTA) performance in visual quality and text fidelity,
establishing a new benchmark for VTO task. Project page:
https://zhenchenwan.github.io/TED-VITON/
","[{'version': 'v1', 'created': 'Tue, 26 Nov 2024 01:00:09 GMT'}, {'version': 'v2', 'created': 'Sun, 1 Dec 2024 14:37:22 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 17:42:55 GMT'}]",2025-03-12,"[['Wan', 'Zhenchen', ''], ['Xu', 'Yanwu', ''], ['Wang', 'Zhaoqing', ''], ['Liu', 'Feng', ''], ['Liu', 'Tongliang', ''], ['Gong', 'Mingming', '']]","[{'text': 'prompts', 'label': 'Prompting'}, {'text': 'Large Language Model', 'label': 'Large Language Model'}]",Large Language Model,Large Language Model,1.0
2411.17237,Zheng Chen,"Zheng Chen, Xun Zhang, Wenbo Li, Renjing Pei, Fenglong Song, Xiongkuo
  Min, Xiaohong Liu, Xin Yuan, Yong Guo, Yulun Zhang","Grounding-IQA: Multimodal Language Grounding Model for Image Quality
  Assessment",Code is available at: https://github.com/zhengchen1999/Grounding-IQA,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The development of multimodal large language models (MLLMs) enables the
evaluation of image quality through natural language descriptions. This
advancement allows for more detailed assessments. However, these MLLM-based IQA
methods primarily rely on general contextual descriptions, sometimes limiting
fine-grained quality assessment. To address this limitation, we introduce a new
image quality assessment (IQA) task paradigm, grounding-IQA. This paradigm
integrates multimodal referring and grounding with IQA to realize more
fine-grained quality perception. Specifically, grounding-IQA comprises two
subtasks: grounding-IQA-description (GIQA-DES) and visual question answering
(GIQA-VQA). GIQA-DES involves detailed descriptions with precise locations
(e.g., bounding boxes), while GIQA-VQA focuses on quality QA for local regions.
To realize grounding-IQA, we construct a corresponding dataset, GIQA-160K,
through our proposed automated annotation pipeline. Furthermore, we develop a
well-designed benchmark, GIQA-Bench. The benchmark comprehensively evaluates
the model grounding-IQA performance from three perspectives: description
quality, VQA accuracy, and grounding precision. Experiments demonstrate that
our proposed task paradigm, dataset, and benchmark facilitate the more
fine-grained IQA application. Code:
https://github.com/zhengchen1999/Grounding-IQA.
","[{'version': 'v1', 'created': 'Tue, 26 Nov 2024 09:03:16 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 02:18:29 GMT'}]",2025-03-12,"[['Chen', 'Zheng', ''], ['Zhang', 'Xun', ''], ['Li', 'Wenbo', ''], ['Pei', 'Renjing', ''], ['Song', 'Fenglong', ''], ['Min', 'Xiongkuo', ''], ['Liu', 'Xiaohong', ''], ['Yuan', 'Xin', ''], ['Guo', 'Yong', ''], ['Zhang', 'Yulun', '']]","[{'text': 'multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,multimodal large language models,0.7649828195571899
2411.19038,Alon Zolfi,"Ben Ganon, Alon Zolfi, Omer Hofman, Inderjeet Singh, Hisashi Kojima,
  Yuval Elovici, Asaf Shabtai","DIESEL -- Dynamic Inference-Guidance via Evasion of Semantic Embeddings
  in LLMs",,,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  In recent years, large language models (LLMs) have had great success in tasks
such as casual conversation, contributing to significant advancements in
domains like virtual assistance. However, they often generate responses that
are not aligned with human values (e.g., ethical standards, safety), leading to
potentially unsafe or inappropriate outputs. While several techniques have been
proposed to address this problem, they come with a cost, requiring
computationally expensive training or dramatically increasing the inference
time. In this paper, we present DIESEL, a lightweight inference-guidance
technique that can be seamlessly integrated into any autoregressive LLM to
semantically filter undesired concepts from the response. DIESEL can function
either as a standalone safeguard or as an additional layer of defense,
enhancing response safety by reranking the LLM's proposed tokens based on their
similarity to predefined negative concepts in the latent space. Our evaluation
demonstrates DIESEL's effectiveness on state-of-the-art conversational models,
even in adversarial jailbreaking scenarios that challenge response safety. We
also highlight DIESEL's generalization capabilities, showing that it can be
used in use cases other than safety, providing general-purpose response
filtering.
","[{'version': 'v1', 'created': 'Thu, 28 Nov 2024 10:33:11 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 09:54:02 GMT'}]",2025-03-11,"[['Ganon', 'Ben', ''], ['Zolfi', 'Alon', ''], ['Hofman', 'Omer', ''], ['Singh', 'Inderjeet', ''], ['Kojima', 'Hisashi', ''], ['Elovici', 'Yuval', ''], ['Shabtai', 'Asaf', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'ethical standards', 'label': 'AI Ethics'}, {'text': 'safety', 'label': 'AI Ethics'}, {'text': 'DIESEL', 'label': 'LLM'}, {'text': 'DIESEL', 'label': 'LLM'}, {'text': 'safety', 'label': 'AI Ethics'}, {'text': 'DIESEL', 'label': 'LLM'}, {'text': 'safety', 'label': 'AI Ethics'}, {'text': 'DIESEL', 'label': 'LLM'}, {'text': 'safety', 'label': 'AI Ethics'}]",Large Language Model,large language models,0.9664971828460693
2411.19275,Merlijn Sevenhuijsen,"Merlijn Sevenhuijsen, Khashayar Etemadi, Mattias Nyberg","VeCoGen: Automating Generation of Formally Verified C Code with Large
  Language Models",,,,,cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Large language models have demonstrated impressive capabilities in generating
code, yet they often produce programs with flaws or deviations from intended
behavior, limiting their suitability for safety-critical applications. To
address this limitation, this paper introduces VECOGEN, a novel tool that
combines large language models with formal verification to automate the
generation of formally verified C programs. VECOGEN takes a formal
specification in ANSI/ISO C Specification Language, a natural language
specification, and a set of test cases to attempt to generate a verified
program. This program-generation process consists of two steps. First, VECOGEN
generates an initial set of candidate programs. Secondly, the tool iteratively
improves on previously generated candidates. If a candidate program meets the
formal specification, then we are sure the program.is correct. We evaluate
VECOGEN on 15 problems presented in Codeforces competitions. On these problems,
VECOGEN solves 13 problems. This work shows the potential of combining large
language models with formal verification to automate program generation.
","[{'version': 'v1', 'created': 'Thu, 28 Nov 2024 17:12:21 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 09:40:22 GMT'}]",2025-03-13,"[['Sevenhuijsen', 'Merlijn', ''], ['Etemadi', 'Khashayar', ''], ['Nyberg', 'Mattias', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'large\nlanguage models', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2412.11934,Jingyu Peng,"Jingyu Peng, Maolin Wang, Xiangyu Zhao, Kai Zhang, Wanyu Wang, Pengyue
  Jia, Qidong Liu, Ruocheng Guo, Qi Liu",Stepwise Reasoning Error Disruption Attack of LLMs,,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have made remarkable strides in complex
reasoning tasks, but their safety and robustness in reasoning processes remain
underexplored. Existing attacks on LLM reasoning are constrained by specific
settings or lack of imperceptibility, limiting their feasibility and
generalizability. To address these challenges, we propose the Stepwise
rEasoning Error Disruption (SEED) attack, which subtly injects errors into
prior reasoning steps to mislead the model into producing incorrect subsequent
reasoning and final answers. Unlike previous methods, SEED is compatible with
zero-shot and few-shot settings, maintains the natural reasoning flow, and
ensures covert execution without modifying the instruction. Extensive
experiments on four datasets across four different models demonstrate SEED's
effectiveness, revealing the vulnerabilities of LLMs to disruptions in
reasoning processes. These findings underscore the need for greater attention
to the robustness of LLM reasoning to ensure safety in practical applications.
","[{'version': 'v1', 'created': 'Mon, 16 Dec 2024 16:20:41 GMT'}, {'version': 'v2', 'created': 'Tue, 24 Dec 2024 03:55:40 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 06:22:15 GMT'}]",2025-03-11,"[['Peng', 'Jingyu', ''], ['Wang', 'Maolin', ''], ['Zhao', 'Xiangyu', ''], ['Zhang', 'Kai', ''], ['Wang', 'Wanyu', ''], ['Jia', 'Pengyue', ''], ['Liu', 'Qidong', ''], ['Guo', 'Ruocheng', ''], ['Liu', 'Qi', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'zero-shot', 'label': 'Zero-shot Learning'}, {'text': 'natural reasoning flow', 'label': 'Chain of thought'}, {'text': 'SEED', 'label': 'BERT'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2412.12932,Zihui Cheng,"Zihui Cheng, Qiguang Chen, Jin Zhang, Hao Fei, Xiaocheng Feng,
  Wanxiang Che, Min Li, Libo Qin","CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large
  Vision-Language Models",Accepted at AAAI 2025; Project Page: https://github.com/czhhzc/CoMT,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Large Vision-Language Models (LVLMs) have recently demonstrated amazing
success in multi-modal tasks, including advancements in Multi-modal
Chain-of-Thought (MCoT) reasoning. Despite these successes, current benchmarks
still follow a traditional paradigm with multi-modal input and text-modal
output, which leads to significant drawbacks such as missing visual operations
and vague expressions. Motivated by this, we introduce a novel Chain of
Multi-modal Thought (CoMT) benchmark to address these limitations. Different
from the traditional MCoT benchmark, CoMT requires both multi-modal input and
multi-modal reasoning output, aiming to mimic human-like reasoning that
inherently integrates visual operation. Specifically, CoMT consists of four
categories: (1) Visual Creation, (2) Visual Deletion, (3) Visual Update, and
(4) Visual Selection to comprehensively explore complex visual operations and
concise expression in real scenarios. We evaluate various LVLMs and strategies
on CoMT, revealing some key insights into the capabilities and limitations of
the current approaches. We hope that CoMT can inspire more research on
introducing multi-modal generation into the reasoning process.
","[{'version': 'v1', 'created': 'Tue, 17 Dec 2024 14:10:16 GMT'}, {'version': 'v2', 'created': 'Sat, 15 Feb 2025 06:32:55 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 08:47:34 GMT'}]",2025-03-11,"[['Cheng', 'Zihui', ''], ['Chen', 'Qiguang', ''], ['Zhang', 'Jin', ''], ['Fei', 'Hao', ''], ['Feng', 'Xiaocheng', ''], ['Che', 'Wanxiang', ''], ['Li', 'Min', ''], ['Qin', 'Libo', '']]","[{'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'Multi-modal\nChain-of-Thought', 'label': 'Chain of thought'}, {'text': 'Visual Creation', 'label': 'Chain of thought'}]",Large Language Model,Large Vision-Language Models,0.7742220759391785
2412.16833,Kaiwen Zuo,"Kaiwen Zuo, Yirui Jiang, Fan Mo, Pietro Lio","KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge
  Graph Enhancement for Medical Diagnosis","10 pages,5 figures,published to AAAI-25 Bridge Program",,,,cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Integrating Large Language Models (LLMs) in healthcare diagnosis demands
systematic frameworks that can handle complex medical scenarios while
maintaining specialized expertise. We present KG4Diagnosis, a novel
hierarchical multi-agent framework that combines LLMs with automated knowledge
graph construction, encompassing 362 common diseases across medical
specialties. Our framework mirrors real-world medical systems through a
two-tier architecture: a general practitioner (GP) agent for initial assessment
and triage, coordinating with specialized agents for in-depth diagnosis in
specific domains. The core innovation lies in our end-to-end knowledge graph
generation methodology, incorporating: (1) semantic-driven entity and relation
extraction optimized for medical terminology, (2) multi-dimensional decision
relationship reconstruction from unstructured medical texts, and (3)
human-guided reasoning for knowledge expansion. KG4Diagnosis serves as an
extensible foundation for specialized medical diagnosis systems, with
capabilities to incorporate new diseases and medical knowledge. The framework's
modular design enables seamless integration of domain-specific enhancements,
making it valuable for developing targeted medical diagnosis systems. We
provide architectural guidelines and protocols to facilitate adoption across
medical contexts.
","[{'version': 'v1', 'created': 'Sun, 22 Dec 2024 02:40:59 GMT'}, {'version': 'v2', 'created': 'Fri, 3 Jan 2025 00:07:09 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 03:05:30 GMT'}]",2025-03-14,"[['Zuo', 'Kaiwen', ''], ['Jiang', 'Yirui', ''], ['Mo', 'Fan', ''], ['Lio', 'Pietro', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'KG4Diagnosis', 'label': 'Foundation Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'KG4Diagnosis', 'label': 'Foundation Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2412.17574,Daoyuan Chen,"Ting Zhou, Daoyuan Chen, Qirui Jiao, Bolin Ding, Yaliang Li, Ying Shen","HumanVBench: Exploring Human-Centric Video Understanding Capabilities of
  MLLMs with Synthetic Benchmark Data","22 pages, 23 figures, 7 tables",,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the domain of Multimodal Large Language Models (MLLMs), achieving
human-centric video understanding remains a formidable challenge. Existing
benchmarks primarily emphasize object and action recognition, often neglecting
the intricate nuances of human emotions, behaviors, and speech-visual alignment
within video content. We present HumanVBench, an innovative benchmark
meticulously crafted to bridge these gaps in the evaluation of video MLLMs.
HumanVBench comprises 16 carefully designed tasks that explore two primary
dimensions: inner emotion and outer manifestations, spanning static and
dynamic, basic and complex, as well as single-modal and cross-modal aspects.
With two advanced automated pipelines for video annotation and
distractor-included QA generation, HumanVBench utilizes diverse
state-of-the-art (SOTA) techniques to streamline benchmark data synthesis and
quality assessment, minimizing human annotation dependency tailored to
human-centric multimodal attributes. A comprehensive evaluation across 22 SOTA
video MLLMs reveals notable limitations in current performance, especially in
cross-modal and emotion perception, underscoring the necessity for further
refinement toward achieving more human-like understanding. HumanVBench is
open-sourced to facilitate future advancements and real-world applications in
video MLLMs.
","[{'version': 'v1', 'created': 'Mon, 23 Dec 2024 13:45:56 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 03:42:48 GMT'}]",2025-03-13,"[['Zhou', 'Ting', ''], ['Chen', 'Daoyuan', ''], ['Jiao', 'Qirui', ''], ['Ding', 'Bolin', ''], ['Li', 'Yaliang', ''], ['Shen', 'Ying', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'HumanVBench', 'label': 'Open-source LLMs'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2412.18084,Long Chen,"Xuan Lin, Long Chen, Yile Wang, Xiangxiang Zeng, Philip S. Yu","Property Enhanced Instruction Tuning for Multi-task Molecule Generation
  with Large Language Models",9,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) are widely applied in various natural language
processing tasks such as question answering and machine translation. However,
due to the lack of labeled data and the difficulty of manual annotation for
biochemical properties, the performance for molecule generation tasks is still
limited, especially for tasks involving multi-properties constraints. In this
work, we present a two-step framework PEIT (Property Enhanced Instruction
Tuning) to improve LLMs for molecular-related tasks. In the first step, we use
textual descriptions, SMILES, and biochemical properties as multimodal inputs
to pre-train a model called PEIT-GEN, by aligning multi-modal representations
to synthesize instruction data. In the second step, we fine-tune existing
open-source LLMs with the synthesized data, the resulting PEIT-LLM can handle
molecule captioning, text-based molecule generation, molecular property
prediction, and our newly proposed multi-constraint molecule generation tasks.
Experimental results show that our pre-trained PEIT-GEN outperforms MolT5 and
BioT5 in molecule captioning, demonstrating modalities align well between
textual descriptions, structures, and biochemical properties. Furthermore,
PEIT-LLM shows promising improvements in multi-task molecule generation,
proving the scalability of the PEIT framework for various molecular tasks. We
release the code, constructed instruction data, and model checkpoints in
https://github.com/chenlong164/PEIT.
","[{'version': 'v1', 'created': 'Tue, 24 Dec 2024 01:48:07 GMT'}, {'version': 'v2', 'created': 'Wed, 5 Mar 2025 02:08:32 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 04:25:11 GMT'}]",2025-03-11,"[['Lin', 'Xuan', ''], ['Chen', 'Long', ''], ['Wang', 'Yile', ''], ['Zeng', 'Xiangxiang', ''], ['Yu', 'Philip S.', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'PEIT-GEN', 'label': 'Open-source LLMs'}, {'text': 'PEIT-LLM', 'label': 'Open-source LLMs'}, {'text': 'PEIT-GEN', 'label': 'Open-source LLMs'}, {'text': 'PEIT-LLM', 'label': 'Open-source LLMs'}]",Large Language Model,Large language models,0.9664971828460693
2412.18947,Kaiwen Zuo,"Kaiwen Zuo, Yirui Jiang","MedHallBench: A New Benchmark for Assessing Hallucination in Medical
  Large Language Models",Published to AAAI-25 Bridge Program,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Medical Large Language Models (MLLMs) have demonstrated potential in
healthcare applications, yet their propensity for hallucinations -- generating
medically implausible or inaccurate information -- presents substantial risks
to patient care. This paper introduces MedHallBench, a comprehensive benchmark
framework for evaluating and mitigating hallucinations in MLLMs. Our
methodology integrates expert-validated medical case scenarios with established
medical databases to create a robust evaluation dataset. The framework employs
a sophisticated measurement system that combines automated ACHMI (Automatic
Caption Hallucination Measurement in Medical Imaging) scoring with rigorous
clinical expert evaluations and utilizes reinforcement learning methods to
achieve automatic annotation. Through an optimized reinforcement learning from
human feedback (RLHF) training pipeline specifically designed for medical
applications, MedHallBench enables thorough evaluation of MLLMs across diverse
clinical contexts while maintaining stringent accuracy standards. We conducted
comparative experiments involving various models, utilizing the benchmark to
establish a baseline for widely adopted large language models (LLMs). Our
findings indicate that ACHMI provides a more nuanced understanding of the
effects of hallucinations compared to traditional metrics, thereby highlighting
its advantages in hallucination assessment. This research establishes a
foundational framework for enhancing MLLMs' reliability in healthcare settings
and presents actionable strategies for addressing the critical challenge of AI
hallucinations in medical applications.
","[{'version': 'v1', 'created': 'Wed, 25 Dec 2024 16:51:29 GMT'}, {'version': 'v2', 'created': 'Fri, 3 Jan 2025 00:16:52 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 02:29:47 GMT'}]",2025-03-14,"[['Zuo', 'Kaiwen', ''], ['Jiang', 'Yirui', '']]","[{'text': 'Medical Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MedHallBench', 'label': 'Foundation Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MedHallBench', 'label': 'Foundation Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,Medical Large Language Models,0.7974728345870972
2412.19496,Xiangkui Cao,"Jie Zhang, Xiangkui Cao, Zhouyu Han, Shiguang Shan, Xilin Chen","Multi-P$^2$A: A Multi-perspective Benchmark on Privacy Assessment for
  Large Vision-Language Models",,,,,cs.CR cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Vision-Language Models (LVLMs) exhibit impressive potential across
various tasks but also face significant privacy risks, limiting their practical
applications. Current researches on privacy assessment for LVLMs is limited in
scope, with gaps in both assessment dimensions and privacy categories. To
bridge this gap, we propose Multi-P$^2$A, a comprehensive benchmark for
evaluating the privacy preservation capabilities of LVLMs in terms of privacy
awareness and leakage. Privacy awareness measures the model's ability to
recognize the privacy sensitivity of input data, while privacy leakage assesses
the risk of the model unintentionally disclosing privacy information in its
output. We design a range of sub-tasks to thoroughly evaluate the model's
privacy protection offered by LVLMs. Multi-P$^2$A covers 26 categories of
personal privacy, 15 categories of trade secrets, and 18 categories of state
secrets, totaling 31,962 samples. Based on Multi-P$^2$A, we evaluate the
privacy preservation capabilities of 21 open-source and 2 closed-source LVLMs.
Our results reveal that current LVLMs generally pose a high risk of
facilitating privacy breaches, with vulnerabilities varying across personal
privacy, trade secret, and state secret.
","[{'version': 'v1', 'created': 'Fri, 27 Dec 2024 07:33:39 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 04:32:32 GMT'}]",2025-03-12,"[['Zhang', 'Jie', ''], ['Cao', 'Xiangkui', ''], ['Han', 'Zhouyu', ''], ['Shan', 'Shiguang', ''], ['Chen', 'Xilin', '']]","[{'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Vision-Language Models,0.7742220759391785
2412.20504,Xiao Wang,"Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, and Liqiang Nie","ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video
  Understanding","Rewrite the methods section. Add more ablation studies and results in
  LongVideoBench",,,,cs.CV cs.CL cs.MM,http://creativecommons.org/licenses/by/4.0/,"  Video Large Language Models (VideoLLMs) have achieved remarkable progress in
video understanding. However, existing VideoLLMs often inherit the limitations
of their backbone LLMs in handling long sequences, leading to challenges for
long video understanding. Common solutions either simply uniformly sample
videos' frames or compress visual tokens, which focus primarily on low-level
temporal visual redundancy, overlooking high-level knowledge redundancy. This
limits the achievable compression rate with minimal loss. To this end. we
introduce a training-free method, $\textbf{ReTaKe}$, containing two novel
modules DPSelect and PivotKV, to jointly model and reduce both temporal visual
redundancy and knowledge redundancy for long video understanding. Specifically,
DPSelect identifies keyframes with local maximum peak distance based on their
visual features, which are closely aligned with human video perception. PivotKV
employs the obtained keyframes as pivots and conducts KV-Cache compression for
the non-pivot tokens with low attention scores, which are derived from the
learned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and
LVBench, show that ReTaKe can support 4x longer video sequences with minimal
performance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,
even surpassing or on par with much larger ones. Our code is available at
https://github.com/SCZwangxiao/video-ReTaKe
","[{'version': 'v1', 'created': 'Sun, 29 Dec 2024 15:42:24 GMT'}, {'version': 'v2', 'created': 'Sun, 5 Jan 2025 14:11:48 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 16:35:59 GMT'}]",2025-03-12,"[['Wang', 'Xiao', ''], ['Si', 'Qingyi', ''], ['Wu', 'Jianlong', ''], ['Zhu', 'Shiyu', ''], ['Cao', 'Li', ''], ['Nie', 'Liqiang', '']]","[{'text': 'Video Large Language Models', 'label': 'Large Language Model'}, {'text': 'VideoLLMs', 'label': 'Large Language Model'}, {'text': 'VideoLLMs', 'label': 'Large Language Model'}, {'text': 'low attention scores', 'label': 'Attention mechanism'}, {'text': 'VideoLLMs', 'label': 'Large Language Model'}]",Large Language Model,Video Large Language Models,0.7616418600082397
2501.01926,Jiaming Li,"Jiaming Li, Jiacheng Zhang, Zequn Jie, Lin Ma, Guanbin Li","Mitigating Hallucination for Large Vision Language Model by
  Inter-Modality Correlation Calibration Decoding",,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large vision-language models (LVLMs) have shown remarkable capabilities in
visual-language understanding for downstream multi-modal tasks. Despite their
success, LVLMs still suffer from generating hallucinations in complex
generation tasks, leading to inconsistencies between visual inputs and
generated content. To address this issue, some approaches have introduced
inference-time interventions, such as contrastive decoding and attention
rectification, to reduce overreliance on language priors. However, these
approaches overlook hallucinations stemming from spurious inter-modality
correlations. In this paper, we propose an Inter-Modality Correlation
Calibration Decoding (IMCCD) method to mitigate hallucinations in LVLMs in a
training-free manner. In this method, we design a Cross-Modal Value-Enhanced
Decoding(CMVED) module to alleviate hallucination by a novel contrastive
decoding mechanism. During the estimation of distorted distribution, CMVED
masks the value vectors associated with significant cross-modal attention
weights, which address both uni-modality overreliance and misleading
inter-modality correlations. Additionally, a Content-Driven Attention
Refinement(CDAR) module refines cross-modal attention weights, guiding LVLMs to
focus on important visual content. Experimental results on diverse
hallucination benchmarks validate the superiority of our method over existing
state-of-the-art techniques in reducing hallucinations in LVLM text generation.
Our code will be available at https://github.com/lijm48/IMCCD.
","[{'version': 'v1', 'created': 'Fri, 3 Jan 2025 17:56:28 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 18:21:46 GMT'}]",2025-03-13,"[['Li', 'Jiaming', ''], ['Zhang', 'Jiacheng', ''], ['Jie', 'Zequn', ''], ['Ma', 'Lin', ''], ['Li', 'Guanbin', '']]","[{'text': 'Large vision-language models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'contrastive decoding', 'label': 'Attention mechanism'}, {'text': 'attention\nrectification', 'label': 'Attention mechanism'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]",Large Language Model,Large vision-language models,0.7742220759391785
2501.05031,Ronghao Dang,"Ronghao Dang, Yuqian Yuan, Wenqi Zhang, Yifei Xin, Boqiang Zhang, Long
  Li, Liuyi Wang, Qinyang Zeng, Xin Li, Lidong Bing","ECBench: Can Multi-modal Foundation Models Understand the Egocentric
  World? A Holistic Embodied Cognition Benchmark",,,,,cs.CV cs.LG cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The enhancement of generalization in robots by large vision-language models
(LVLMs) is increasingly evident. Therefore, the embodied cognitive abilities of
LVLMs based on egocentric videos are of great interest. However, current
datasets for embodied video question answering lack comprehensive and
systematic evaluation frameworks. Critical embodied cognitive issues, such as
robotic self-cognition, dynamic scene perception, and hallucination, are rarely
addressed. To tackle these challenges, we propose ECBench, a high-quality
benchmark designed to systematically evaluate the embodied cognitive abilities
of LVLMs. ECBench features a diverse range of scene video sources, open and
varied question formats, and 30 dimensions of embodied cognition. To ensure
quality, balance, and high visual dependence, ECBench uses class-independent
meticulous human annotation and multi-round question screening strategies.
Additionally, we introduce ECEval, a comprehensive evaluation system that
ensures the fairness and rationality of the indicators. Utilizing ECBench, we
conduct extensive evaluations of proprietary, open-source, and task-specific
LVLMs. ECBench is pivotal in advancing the embodied cognitive capabilities of
LVLMs, laying a solid foundation for developing reliable core models for
embodied agents. All data and code are available at
https://github.com/Rh-Dang/ECBench.
","[{'version': 'v1', 'created': 'Thu, 9 Jan 2025 07:43:49 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 07:45:55 GMT'}]",2025-03-14,"[['Dang', 'Ronghao', ''], ['Yuan', 'Yuqian', ''], ['Zhang', 'Wenqi', ''], ['Xin', 'Yifei', ''], ['Zhang', 'Boqiang', ''], ['Li', 'Long', ''], ['Wang', 'Liuyi', ''], ['Zeng', 'Qinyang', ''], ['Li', 'Xin', ''], ['Bing', 'Lidong', '']]","[{'text': 'large vision-language models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]",Large Language Model,large vision-language models,0.7742220759391785
2501.05179,Xuyang Liu,"Xuyang Liu, Ziming Wang, Yuhang Han, Yingyao Wang, Jiale Yuan, Jun
  Song, Bo Zheng, Linfeng Zhang, Siteng Huang, Honggang Chen","Global Compression Commander: Plug-and-Play Inference Acceleration for
  High-Resolution Large Vision-Language Models",,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Large vision-language models (LVLMs) excel at visual understanding and
reasoning, but face efficiency challenges due to quadratic complexity in
processing long multimodal contexts. While token compression techniques can
reduce computational costs, existing approaches are designed for single-view
LVLMs and fail to consider the unique multi-view characteristics of recent
high-resolution LVLMs with dynamic tiling. While existing methods treat all
tokens uniformly, our analysis reveals that global thumbnails can naturally
guide the compression of local crops by providing holistic context for
informativeness evaluation. In this paper, we first analyze dynamic tiling
strategy comprehensively, revealing both the complementary nature between
thumbnails and crops, and the distinctive characteristics across different
crops. Based on our observations, we propose ""Global Compression Commander""
(i.e., GlobalCom$^2$), a novel plug-and-play token compression framework for
HR-LVLMs. GlobalCom$^2$ leverages thumbnail as the ""commander"" to guide the
compression process of local crops, adaptively preserving informative details
while eliminating redundancy. Extensive experiments show that GlobalCom$^2$
maintains over 90\% performance while compressing 90\% visual tokens, reducing
FLOPs and peak memory to 9.1\% and 60\% respectively across multiple
benchmarks. Our code is available at
https://github.com/xuyang-liu16/GlobalCom2.
","[{'version': 'v1', 'created': 'Thu, 9 Jan 2025 11:57:58 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Jan 2025 17:34:26 GMT'}, {'version': 'v3', 'created': 'Sun, 16 Feb 2025 18:33:57 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Mar 2025 05:18:12 GMT'}]",2025-03-12,"[['Liu', 'Xuyang', ''], ['Wang', 'Ziming', ''], ['Han', 'Yuhang', ''], ['Wang', 'Yingyao', ''], ['Yuan', 'Jiale', ''], ['Song', 'Jun', ''], ['Zheng', 'Bo', ''], ['Zhang', 'Linfeng', ''], ['Huang', 'Siteng', ''], ['Chen', 'Honggang', '']]","[{'text': 'Large vision-language models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]",Large Language Model,Large vision-language models,0.7742220759391785
2501.06828,Ruizhe Ou,"Ruizhe Ou, Yuan Hu, Fan Zhang, Jiaxin Chen, Yu Liu","GeoPix: Multi-Modal Large Language Model for Pixel-level Image
  Understanding in Remote Sensing",,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Multi-modal large language models (MLLMs) have achieved remarkable success in
image- and region-level remote sensing (RS) image understanding tasks, such as
image captioning, visual question answering, and visual grounding. However,
existing RS MLLMs lack the pixel-level dialogue capability, which involves
responding to user instructions with segmentation masks for specific instances.
In this paper, we propose GeoPix, a RS MLLM that extends image understanding
capabilities to the pixel level. This is achieved by equipping the MLLM with a
mask predictor, which transforms visual features from the vision encoder into
masks conditioned on the LLM's segmentation token embeddings. To facilitate the
segmentation of multi-scale objects in RS imagery, a class-wise learnable
memory module is integrated into the mask predictor to capture and store
class-wise geo-context at the instance level across the entire dataset. In
addition, to address the absence of large-scale datasets for training
pixel-level RS MLLMs, we construct the GeoPixInstruct dataset, comprising
65,463 images and 140,412 instances, with each instance annotated with text
descriptions, bounding boxes, and masks. Furthermore, we develop a two-stage
training strategy to balance the distinct requirements of text generation and
masks prediction in multi-modal multi-task optimization. Extensive experiments
verify the effectiveness and superiority of GeoPix in pixel-level segmentation
tasks, while also maintaining competitive performance in image- and
region-level benchmarks.
","[{'version': 'v1', 'created': 'Sun, 12 Jan 2025 14:45:27 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 08:16:01 GMT'}]",2025-03-14,"[['Ou', 'Ruizhe', ''], ['Hu', 'Yuan', ''], ['Zhang', 'Fan', ''], ['Chen', 'Jiaxin', ''], ['Liu', 'Yu', '']]","[{'text': 'Multi-modal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'segmentation token embeddings', 'label': 'Embedding'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,Multi-modal large language models,0.7925285696983337
2501.13652,Yizheng Sun,"Yizheng Sun, Yanze Xin, Hao Li, Jingyuan Sun, Chenghua Lin, Riza
  Batista-Navarro","LVPruning: An Effective yet Simple Language-Guided Vision Token Pruning
  Approach for Multi-modal Large Language Models",Accepted to NAACL 2025 Findings,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multi-modal Large Language Models (MLLMs) have achieved remarkable success by
integrating visual and textual modalities. However, they incur significant
computational overhead due to the large number of vision tokens processed,
limiting their practicality in resource-constrained environments. We introduce
Language-Guided Vision Token Pruning (LVPruning) for MLLMs, an effective yet
simple method that significantly reduces the computational burden while
preserving model performance. LVPruning employs cross-attention modules to
compute the importance of vision tokens based on their interaction with
language tokens, determining which to prune. Importantly, LVPruning can be
integrated without modifying the original MLLM parameters, which makes
LVPruning simple to apply or remove. Our experiments show that LVPruning can
effectively reduce up to 90% of vision tokens by the middle layer of LLaVA-1.5,
resulting in a 62.1% decrease in inference Tera Floating-Point Operations Per
Second (TFLOPs), with an average performance loss of just 0.45% across nine
multi-modal benchmarks.
","[{'version': 'v1', 'created': 'Thu, 23 Jan 2025 13:31:51 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 21:32:52 GMT'}]",2025-03-11,"[['Sun', 'Yizheng', ''], ['Xin', 'Yanze', ''], ['Li', 'Hao', ''], ['Sun', 'Jingyuan', ''], ['Lin', 'Chenghua', ''], ['Batista-Navarro', 'Riza', '']]","[{'text': 'Multi-modal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'cross-attention modules', 'label': 'Attention mechanism'}]",Large Language Model,Multi-modal Large Language Models,0.7925285696983337
2501.13778,Yoonsang Kim,"Yoonsang Kim, Zainab Aamir, Mithilesh Singh, Saeed Boorboor, Klaus
  Mueller, Arie E. Kaufman","Explainable XR: Understanding User Behaviors of XR Environments using
  LLM-assisted Analytics Framework","11 pages, 8 figures. This is the author's version of the article that
  has been accepted for publication in IEEE Transactions on Visualization and
  Computer Graphics",,10.1109/TVCG.2025.3549537,,cs.HC cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present Explainable XR, an end-to-end framework for analyzing user
behavior in diverse eXtended Reality (XR) environments by leveraging Large
Language Models (LLMs) for data interpretation assistance. Existing XR user
analytics frameworks face challenges in handling cross-virtuality - AR, VR, MR
- transitions, multi-user collaborative application scenarios, and the
complexity of multimodal data. Explainable XR addresses these challenges by
providing a virtuality-agnostic solution for the collection, analysis, and
visualization of immersive sessions. We propose three main components in our
framework: (1) A novel user data recording schema, called User Action
Descriptor (UAD), that can capture the users' multimodal actions, along with
their intents and the contexts; (2) a platform-agnostic XR session recorder,
and (3) a visual analytics interface that offers LLM-assisted insights tailored
to the analysts' perspectives, facilitating the exploration and analysis of the
recorded XR session data. We demonstrate the versatility of Explainable XR by
demonstrating five use-case scenarios, in both individual and collaborative XR
applications across virtualities. Our technical evaluation and user studies
show that Explainable XR provides a highly usable analytics solution for
understanding user actions and delivering multifaceted, actionable insights
into user behaviors in immersive environments.
","[{'version': 'v1', 'created': 'Thu, 23 Jan 2025 15:55:07 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 22:15:10 GMT'}]",2025-03-12,"[['Kim', 'Yoonsang', ''], ['Aamir', 'Zainab', ''], ['Singh', 'Mithilesh', ''], ['Boorboor', 'Saeed', ''], ['Mueller', 'Klaus', ''], ['Kaufman', 'Arie E.', '']]","[{'text': 'Large\nLanguage Models', 'label': 'Large Language Model'}]",Large Language Model,"Large
Language Models",0.9664971828460693
2501.17202,Chen Chen,"Chen Chen, Yuchen Hu, Siyin Wang, Helin Wang, Zhehuai Chen, Chao
  Zhang, Chao-Han Huck Yang, and Eng Siong Chng",Audio Large Language Models Can Be Descriptive Speech Quality Evaluators,ICLR 2025,,,,cs.SD cs.CL eess.AS,http://creativecommons.org/licenses/by/4.0/,"  An ideal multimodal agent should be aware of the quality of its input
modalities. Recent advances have enabled large language models (LLMs) to
incorporate auditory systems for handling various speech-related tasks.
However, most audio LLMs remain unaware of the quality of the speech they
process. This limitation arises because speech quality evaluation is typically
excluded from multi-task training due to the lack of suitable datasets. To
address this, we introduce the first natural language-based speech evaluation
corpus, generated from authentic human ratings. In addition to the overall Mean
Opinion Score (MOS), this corpus offers detailed analysis across multiple
dimensions and identifies causes of quality degradation. It also enables
descriptive comparisons between two speech samples (A/B tests) with human-like
judgment. Leveraging this corpus, we propose an alignment approach with LLM
distillation (ALLD) to guide the audio LLM in extracting relevant information
from raw speech and generating meaningful responses. Experimental results
demonstrate that ALLD outperforms the previous state-of-the-art regression
model in MOS prediction, with a mean square error of 0.17 and an A/B test
accuracy of 98.6%. Additionally, the generated responses achieve BLEU scores of
25.8 and 30.2 on two tasks, surpassing the capabilities of task-specific
models. This work advances the comprehensive perception of speech signals by
audio LLMs, contributing to the development of real-world auditory and sensory
intelligent agents.
","[{'version': 'v1', 'created': 'Mon, 27 Jan 2025 22:47:51 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 02:01:46 GMT'}]",2025-03-13,"[['Chen', 'Chen', ''], ['Hu', 'Yuchen', ''], ['Wang', 'Siyin', ''], ['Wang', 'Helin', ''], ['Chen', 'Zhehuai', ''], ['Zhang', 'Chao', ''], ['Yang', 'Chao-Han Huck', ''], ['Chng', 'Eng Siong', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM\ndistillation', 'label': 'Knowledge distillation'}, {'text': 'ALLD', 'label': 'Knowledge distillation'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2502.01403,Zhiteng Li,"Zhiteng Li, Mingyuan Xia, Jingyuan Zhang, Zheng Hui, Linghe Kong,
  Yulun Zhang, Xiaokang Yang",AdaSVD: Adaptive Singular Value Decomposition for Large Language Models,"The code and models will be available at
  https://github.com/ZHITENGLI/AdaSVD",,,,cs.CV cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have achieved remarkable success in natural
language processing (NLP) tasks, yet their substantial memory requirements
present significant challenges for deployment on resource-constrained devices.
Singular Value Decomposition (SVD) has emerged as a promising compression
technique for LLMs, offering considerable reductions in memory overhead.
However, existing SVD-based methods often struggle to effectively mitigate the
errors introduced by SVD truncation, leading to a noticeable performance gap
when compared to the original models. Furthermore, applying a uniform
compression ratio across all transformer layers fails to account for the
varying importance of different layers. To address these challenges, we propose
AdaSVD, an adaptive SVD-based LLM compression approach. Specifically, AdaSVD
introduces adaComp, which adaptively compensates for SVD truncation errors by
alternately updating the singular matrices $\mathcal{U}$ and
$\mathcal{V}^\top$. Additionally, AdaSVD introduces adaCR, which adaptively
assigns layer-specific compression ratios based on the relative importance of
each layer. Extensive experiments across multiple LLM/VLM families and
evaluation metrics demonstrate that AdaSVD consistently outperforms
state-of-the-art (SOTA) SVD-based methods, achieving superior performance with
significantly reduced memory requirements. Code and models of AdaSVD will be
available at https://github.com/ZHITENGLI/AdaSVD.
","[{'version': 'v1', 'created': 'Mon, 3 Feb 2025 14:34:37 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Feb 2025 03:51:28 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 09:04:18 GMT'}]",2025-03-11,"[['Li', 'Zhiteng', ''], ['Xia', 'Mingyuan', ''], ['Zhang', 'Jingyuan', ''], ['Hui', 'Zheng', ''], ['Kong', 'Linghe', ''], ['Zhang', 'Yulun', ''], ['Yang', 'Xiaokang', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'AdaSVD', 'label': 'LLM-based'}, {'text': 'AdaSVD', 'label': 'LLM-based'}, {'text': 'AdaSVD', 'label': 'LLM-based'}, {'text': 'AdaSVD', 'label': 'LLM-based'}]",Large Language Model,Large language models,0.9664971828460693
2502.01821,Runxiang Cheng,"Runxiang Cheng, Michele Tufano, J\""urgen Cito, Jos\'e Cambronero, Pat
  Rondon, Renyao Wei, Aaron Sun, Satish Chandra","Agentic Bug Reproduction for Effective Automated Program Repair at
  Google",,,,,cs.SE cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Bug reports often lack sufficient detail for developers to reproduce and fix
the underlying defects. Bug Reproduction Tests (BRTs), tests that fail when the
bug is present and pass when it has been resolved, are crucial for debugging,
but they are rarely included in bug reports, both in open-source and in
industrial settings. Thus, automatically generating BRTs from bug reports has
the potential to accelerate the debugging process and lower time to repair.
This paper investigates automated BRT generation within an industry setting,
specifically at Google, focusing on the challenges of a large-scale,
proprietary codebase and considering real-world industry bugs extracted from
Google's internal issue tracker. We adapt and evaluate a state-of-the-art BRT
generation technique, LIBRO, and present our agent-based approach, BRT Agent,
which makes use of a fine-tuned Large Language Model (LLM) for code editing.
Our BRT Agent significantly outperforms LIBRO, achieving a 28% plausible BRT
generation rate, compared to 10% by LIBRO, on 80 human-reported bugs from
Google's internal issue tracker. We further investigate the practical value of
generated BRTs by integrating them with an Automated Program Repair (APR)
system at Google. Our results show that providing BRTs to the APR system
results in 30% more bugs with plausible fixes. Additionally, we introduce
Ensemble Pass Rate (EPR), a metric which leverages the generated BRTs to select
the most promising fixes from all fixes generated by APR system. Our evaluation
on EPR for Top-K and threshold-based fix selections demonstrates promising
results and trade-offs. For example, EPR correctly selects a plausible fix from
a pool of 20 candidates in 70% of cases, based on its top-1 ranking.
","[{'version': 'v1', 'created': 'Mon, 3 Feb 2025 20:57:17 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 02:30:46 GMT'}]",2025-03-12,"[['Cheng', 'Runxiang', ''], ['Tufano', 'Michele', ''], ['Cito', 'Jürgen', ''], ['Cambronero', 'José', ''], ['Rondon', 'Pat', ''], ['Wei', 'Renyao', ''], ['Sun', 'Aaron', ''], ['Chandra', 'Satish', '']]","[{'text': 'Large Language Model', 'label': 'Large Language Model'}]",Large Language Model,Large Language Model,1.0
2502.02088,Xiaomeng Yang,"Xiaomeng Yang, Zhiyu Tan, and Hao Li",IPO: Iterative Preference Optimization for Text-to-Video Generation,,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by-sa/4.0/,"  Video foundation models have achieved significant advancement with the help
of network upgrade as well as model scale-up. However, they are still hard to
meet requirements of applications due to unsatisfied generation quality. To
solve this problem, we propose to align video foundation models with human
preferences from the perspective of post-training in this paper. Consequently,
we introduce an Iterative Preference Optimization strategy to enhance generated
video quality by incorporating human feedback. Specifically, IPO exploits a
critic model to justify video generations for pairwise ranking as in Direct
Preference Optimization or point-wise scoring as in Kahneman-Tversky
Optimization. Given this, IPO optimizes video foundation models with guidance
of signals from preference feedback, which helps improve generated video
quality in subject consistency, motion smoothness and aesthetic quality, etc.
In addition, IPO incorporates the critic model with the multi-modality large
language model, which enables it to automatically assign preference labels
without need of retraining or relabeling. In this way, IPO can efficiently
perform multi-round preference optimization in an iterative manner, without the
need of tediously manual labeling. Comprehensive experiments demonstrate that
the proposed IPO can effectively improve the video generation quality of a
pretrained model and help a model with only 2B parameters surpass the one with
5B parameters. Besides, IPO achieves new state-of-the-art performance on VBench
benchmark.
","[{'version': 'v1', 'created': 'Tue, 4 Feb 2025 08:14:34 GMT'}, {'version': 'v2', 'created': 'Wed, 5 Feb 2025 06:18:12 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 12:30:00 GMT'}]",2025-03-11,"[['Yang', 'Xiaomeng', ''], ['Tan', 'Zhiyu', ''], ['Li', 'Hao', '']]","[{'text': 'critic model', 'label': 'Foundation Model'}, {'text': 'IPO', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'critic model', 'label': 'Foundation Model'}, {'text': 'multi-modality large\nlanguage model', 'label': 'Large Language Model'}, {'text': 'IPO', 'label': 'Generative Pre-trained Transformer (GPT)'}]",Large Language Model,"multi-modality large
language model",0.788622260093689
2502.03678,Zeyu Tang,"Zeyu Tang, Zhenhao Chen, Loka Li, Xiangchen Song, Yunlong Deng, Yifan
  Shen, Guangyi Chen, Peter Spirtes, Kun Zhang",Reflection-Window Decoding: Text Generation with Selective Refinement,,,,,cs.CL cs.AI cs.LG,http://creativecommons.org/licenses/by/4.0/,"  The autoregressive decoding for text generation in large language models
(LLMs), while widely used, is inherently suboptimal due to the lack of a
built-in mechanism to perform refinement and/or correction of the generated
content. In this paper, we consider optimality in terms of the joint
probability over the generated response, when jointly considering all tokens at
the same time. We theoretically characterize the potential deviation of the
autoregressively generated response from its globally optimal counterpart that
is of the same length. Our analysis suggests that we need to be cautious when
noticeable uncertainty arises during text generation, which may signal the
sub-optimality of the generation history. To address the pitfall of
autoregressive decoding for text generation, we propose an approach that
incorporates a sliding reflection window and a pausing criterion, such that
refinement and generation can be carried out interchangeably as the decoding
proceeds. Our selective refinement framework strikes a balance between
efficiency and optimality, and our extensive experimental results demonstrate
the effectiveness of our approach.
","[{'version': 'v1', 'created': 'Wed, 5 Feb 2025 23:53:08 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 19:34:32 GMT'}]",2025-03-12,"[['Tang', 'Zeyu', ''], ['Chen', 'Zhenhao', ''], ['Li', 'Loka', ''], ['Song', 'Xiangchen', ''], ['Deng', 'Yunlong', ''], ['Shen', 'Yifan', ''], ['Chen', 'Guangyi', ''], ['Spirtes', 'Peter', ''], ['Zhang', 'Kun', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2502.06759,Gaetano Rossiello,"Gaetano Rossiello, Nhan Pham, Michael Glass, Junkyu Lee, Dharmashankar
  Subramanian",Rationalization Models for Text-to-SQL,Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs,,,,cs.CL cs.AI cs.DB,http://creativecommons.org/licenses/by/4.0/,"  We introduce a framework for generating Chain-of-Thought (CoT) rationales to
enhance text-to-SQL model fine-tuning. These rationales consist of intermediate
SQL statements and explanations, serving as incremental steps toward
constructing the final SQL query. The process begins with manually annotating a
small set of examples, which are then used to prompt a large language model in
an iterative, dynamic few-shot knowledge distillation procedure from a teacher
model. A rationalization model is subsequently trained on the validated
decomposed queries, enabling extensive synthetic CoT annotations for
text-to-SQL datasets. To evaluate the approach, we fine-tune small language
models with and without these rationales on the BIRD dataset. Results indicate
that step-by-step query generation improves execution accuracy, especially for
moderately and highly complex queries, while also enhancing explainability.
","[{'version': 'v1', 'created': 'Mon, 10 Feb 2025 18:38:57 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Feb 2025 17:12:34 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 17:37:30 GMT'}]",2025-03-12,"[['Rossiello', 'Gaetano', ''], ['Pham', 'Nhan', ''], ['Glass', 'Michael', ''], ['Lee', 'Junkyu', ''], ['Subramanian', 'Dharmashankar', '']]","[{'text': 'prompt', 'label': 'Prompting'}, {'text': 'large language model', 'label': 'Large Language Model'}, {'text': 'few-shot knowledge distillation', 'label': 'Knowledge distillation'}]",Large Language Model,large language model,1.0
2502.07972,Zach Nussbaum,"Zach Nussbaum, Brandon Duderstadt",Training Sparse Mixture Of Experts Text Embedding Models,,,,,cs.CL cs.AI cs.IR,http://creativecommons.org/licenses/by/4.0/,"  Transformer-based text embedding models have improved their performance on
benchmarks like MIRACL and BEIR by increasing their parameter counts. However,
this scaling approach introduces significant deployment challenges, including
increased inference latency and memory usage. These challenges are particularly
severe in retrieval-augmented generation (RAG) applications, where large
models' increased memory requirements constrain dataset ingestion capacity, and
their higher latency directly impacts query-time performance. While causal
language models have addressed similar efficiency challenges using Mixture of
Experts (MoE) architectures, this approach hasn't been successfully adapted to
the general text embedding setting. In this paper, we introduce Nomic Embed v2,
the first general purpose MoE text embedding model. Our model outperforms
models in the same parameter class on both monolingual and multilingual
benchmarks while also maintaining competitive performance with models twice its
size. We open-source all code, models, and evaluation data to ensure full
reproducibility of our training pipeline at
\href{https://github.com/nomic-ai/contrastors}{https://github.com/nomic-ai/contrastors}.
","[{'version': 'v1', 'created': 'Tue, 11 Feb 2025 21:36:31 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Feb 2025 01:23:29 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 19:39:00 GMT'}]",2025-03-11,"[['Nussbaum', 'Zach', ''], ['Duderstadt', 'Brandon', '']]","[{'text': 'retrieval-augmented generation (RAG)', 'label': 'RAG'}, {'text': 'large\nmodels', 'label': 'Large Language Model'}]",Large Language Model,"large
models",0.6642580628395081
2502.11418,Geon Lee,"Geon Lee, Wenchao Yu, Kijung Shin, Wei Cheng, Haifeng Chen","TimeCAP: Learning to Contextualize, Augment, and Predict Time Series
  Events with Large Language Model Agents",AAAI 2025,,,,cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Time series data is essential in various applications, including climate
modeling, healthcare monitoring, and financial analytics. Understanding the
contextual information associated with real-world time series data is often
essential for accurate and reliable event predictions. In this paper, we
introduce TimeCAP, a time-series processing framework that creatively employs
Large Language Models (LLMs) as contextualizers of time series data, extending
their typical usage as predictors. TimeCAP incorporates two independent LLM
agents: one generates a textual summary capturing the context of the time
series, while the other uses this enriched summary to make more informed
predictions. In addition, TimeCAP employs a multi-modal encoder that synergizes
with the LLM agents, enhancing predictive performance through mutual
augmentation of inputs with in-context examples. Experimental results on
real-world datasets demonstrate that TimeCAP outperforms state-of-the-art
methods for time series event prediction, including those utilizing LLMs as
predictors, achieving an average improvement of 28.75% in F1 score.
","[{'version': 'v1', 'created': 'Mon, 17 Feb 2025 04:17:27 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 04:15:20 GMT'}]",2025-03-11,"[['Lee', 'Geon', ''], ['Yu', 'Wenchao', ''], ['Shin', 'Kijung', ''], ['Cheng', 'Wei', ''], ['Chen', 'Haifeng', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2502.11649,Mehwish Nasim,Amin Qasmi and Usman Naseem and Mehwish Nasim,Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation,,,,,cs.AI cs.SI,http://creativecommons.org/licenses/by/4.0/,"  We introduce a novel non-cooperative game to analyse opinion formation and
resistance, incorporating principles from social psychology such as
confirmation bias, resource constraints, and influence penalties. Our
simulation features Large Language Model (LLM) agents competing to influence a
population, with penalties imposed for generating messages that propagate or
counter misinformation. This framework integrates resource optimisation into
the agents' decision-making process. Our findings demonstrate that while higher
confirmation bias strengthens opinion alignment within groups, it also
exacerbates overall polarisation. Conversely, lower confirmation bias leads to
fragmented opinions and limited shifts in individual beliefs. Investing heavily
in a high-resource debunking strategy can initially align the population with
the debunking agent, but risks rapid resource depletion and diminished
long-term influence.
","[{'version': 'v1', 'created': 'Mon, 17 Feb 2025 10:41:55 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 02:14:41 GMT'}]",2025-03-11,"[['Qasmi', 'Amin', ''], ['Naseem', 'Usman', ''], ['Nasim', 'Mehwish', '']]","[{'text': 'confirmation bias', 'label': 'Model Bias and Fairness'}, {'text': 'Large Language Model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'confirmation bias', 'label': 'Model Bias and Fairness'}, {'text': 'confirmation bias', 'label': 'Model Bias and Fairness'}]",Large Language Model,Large Language Model,1.0
2502.12029,Qi Zhao,"Qi Zhao, Hongyu Yang, Qi Song, Xinwei Yao, Xiangyang Li","KnowPath: Knowledge-enhanced Reasoning via LLM-generated Inference Paths
  over Knowledge Graphs",,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) have demonstrated remarkable capabilities in
various complex tasks, yet they still suffer from hallucinations. Introducing
external knowledge, such as knowledge graph, can enhance the LLMs' ability to
provide factual answers. LLMs have the ability to interactively explore
knowledge graphs. However, most approaches have been affected by insufficient
internal knowledge excavation in LLMs, limited generation of trustworthy
knowledge reasoning paths, and a vague integration between internal and
external knowledge. Therefore, we propose KnowPath, a knowledge-enhanced large
model framework driven by the collaboration of internal and external knowledge.
It relies on the internal knowledge of the LLM to guide the exploration of
interpretable directed subgraphs in external knowledge graphs, better
integrating the two knowledge sources for more accurate reasoning. Extensive
experiments on multiple real-world datasets confirm the superiority of
KnowPath.
","[{'version': 'v1', 'created': 'Mon, 17 Feb 2025 17:02:01 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 13:22:46 GMT'}]",2025-03-14,"[['Zhao', 'Qi', ''], ['Yang', 'Hongyu', ''], ['Song', 'Qi', ''], ['Yao', 'Xinwei', ''], ['Li', 'Xiangyang', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'KnowPath', 'label': 'LLM-based'}]",Large Language Model,Large language models,0.9664971828460693
2502.12455,Minxuan Lv,"Minxuan Lv, Zhenpeng Su, Leiyu Pan, Yizhe Xiong, Zijia Lin, Hui Chen,
  Wei Zhou, Jungong Han, Guiguang Ding, Cheng Luo, Di Zhang, Kun Gai, Songlin
  Hu","DSMoE: Matrix-Partitioned Experts with Dynamic Routing for
  Computation-Efficient Dense LLMs",,,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  As large language models continue to scale, computational costs and resource
consumption have emerged as significant challenges. While existing
sparsification methods like pruning reduce computational overhead, they risk
losing model knowledge through parameter removal. This paper proposes DSMoE
(Dynamic Sparse Mixture-of-Experts), a novel approach that achieves
sparsification by partitioning pre-trained FFN layers into computational
blocks. We implement adaptive expert routing using sigmoid activation and
straight-through estimators, enabling tokens to flexibly access different
aspects of model knowledge based on input complexity. Additionally, we
introduce a sparsity loss term to balance performance and computational
efficiency. Extensive experiments on LLaMA models demonstrate that under
equivalent computational constraints, DSMoE achieves superior performance
compared to existing pruning and MoE approaches across language modeling and
downstream tasks, particularly excelling in generation tasks. Analysis reveals
that DSMoE learns distinctive layerwise activation patterns, providing new
insights for future MoE architecture design.
","[{'version': 'v1', 'created': 'Tue, 18 Feb 2025 02:37:26 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 10:40:09 GMT'}]",2025-03-14,"[['Lv', 'Minxuan', ''], ['Su', 'Zhenpeng', ''], ['Pan', 'Leiyu', ''], ['Xiong', 'Yizhe', ''], ['Lin', 'Zijia', ''], ['Chen', 'Hui', ''], ['Zhou', 'Wei', ''], ['Han', 'Jungong', ''], ['Ding', 'Guiguang', ''], ['Luo', 'Cheng', ''], ['Zhang', 'Di', ''], ['Gai', 'Kun', ''], ['Hu', 'Songlin', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2502.12509,Kangda Wei,"Kangda Wei, Xi Shi, Jonathan Tong, Sai Ramana Reddy, Anandhavelu
  Natarajan, Rajiv Jain, Aparna Garimella, Ruihong Huang",LegalCore: A Dataset for Event Coreference Resolution in Legal Documents,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Recognizing events and their coreferential mentions in a document is
essential for understanding semantic meanings of text. The existing research on
event coreference resolution is mostly limited to news articles. In this paper,
we present the first dataset for the legal domain, LegalCore, which has been
annotated with comprehensive event and event coreference information. The legal
contract documents we annotated in this dataset are several times longer than
news articles, with an average length of around 25k tokens per document. The
annotations show that legal documents have dense event mentions and feature
both short-distance and super long-distance coreference links between event
mentions. We further benchmark mainstream Large Language Models (LLMs) on this
dataset for both event detection and event coreference resolution tasks, and
find that this dataset poses significant challenges for state-of-the-art
open-source and proprietary LLMs, which perform significantly worse than a
supervised baseline. We will publish the dataset as well as the code.
","[{'version': 'v1', 'created': 'Tue, 18 Feb 2025 03:47:53 GMT'}, {'version': 'v2', 'created': 'Mon, 3 Mar 2025 19:36:00 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 16:53:11 GMT'}]",2025-03-11,"[['Wei', 'Kangda', ''], ['Shi', 'Xi', ''], ['Tong', 'Jonathan', ''], ['Reddy', 'Sai Ramana', ''], ['Natarajan', 'Anandhavelu', ''], ['Jain', 'Rajiv', ''], ['Garimella', 'Aparna', ''], ['Huang', 'Ruihong', '']]","[{'text': 'mainstream Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,mainstream Large Language Models,0.848070502281189
2502.12558,Huaying Yuan,"Huaying Yuan, Jian Ni, Yueze Wang, Junjie Zhou, Zhengyang Liang, Zheng
  Liu, Zhao Cao, Zhicheng Dou, and Ji-Rong Wen","MomentSeeker: A Comprehensive Benchmark and A Strong Baseline For Moment
  Retrieval Within Long Videos",,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Retrieval augmented generation (RAG) holds great promise in addressing
challenges associated with long video understanding. These methods retrieve
useful moments from long videos for their presented tasks, thereby enabling
multimodal large language models (MLLMs) to generate high-quality answers in a
cost-effective way. In this work, we present MomentSeeker, a comprehensive
benchmark to evaluate retrieval models' performance in handling general
long-video moment retrieval (LVMR) tasks. MomentSeeker offers three key
advantages. First, it incorporates long videos of over 500 seconds on average,
making it the first benchmark specialized for long-video moment retrieval.
Second, it covers a wide range of task categories (including Moment Search,
Caption Alignment, Image-conditioned Moment Search, and Video-conditioned
Moment Search) and diverse application scenarios (e.g., sports, movies,
cartoons, and ego), making it a comprehensive tool for assessing retrieval
models' general LVMR performance. Additionally, the evaluation tasks are
carefully curated through human annotation, ensuring the reliability of
assessment. We further fine-tune an MLLM-based LVMR retriever on synthetic
data, which demonstrates strong performance on our benchmark. We perform
extensive experiments with various popular multimodal retrievers based on our
benchmark, whose results highlight the challenges of LVMR and limitations for
existing methods. Our created resources will be shared with community to
advance future research in this field.
","[{'version': 'v1', 'created': 'Tue, 18 Feb 2025 05:50:23 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 05:34:20 GMT'}]",2025-03-11,"[['Yuan', 'Huaying', ''], ['Ni', 'Jian', ''], ['Wang', 'Yueze', ''], ['Zhou', 'Junjie', ''], ['Liang', 'Zhengyang', ''], ['Liu', 'Zheng', ''], ['Cao', 'Zhao', ''], ['Dou', 'Zhicheng', ''], ['Wen', 'Ji-Rong', '']]","[{'text': 'multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MomentSeeker', 'label': 'RAG'}, {'text': 'MomentSeeker', 'label': 'RAG'}]",Large Language Model,multimodal large language models,0.7649828195571899
2502.14856,Weilin Zhao,"Weilin Zhao, Tengyu Pan, Xu Han, Yudi Zhang, Ao Sun, Yuxiang Huang,
  Kaihuo Zhang, Weilun Zhao, Yuxuan Li, Jianyong Wang, Zhiyuan Liu, Maosong Sun","FR-Spec: Accelerating Large-Vocabulary Language Models via
  Frequency-Ranked Speculative Sampling",,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Speculative sampling has emerged as an important technique for accelerating
the auto-regressive generation process of large language models (LLMs) by
utilizing a draft-then-verify mechanism to produce multiple tokens per forward
pass. While state-of-the-art speculative sampling methods use only a single
layer and a language modeling (LM) head as the draft model to achieve
impressive layer compression, their efficiency gains are substantially reduced
for large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens.
To address this, we present FR-Spec, a frequency-ranked speculative sampling
framework that optimizes draft candidate selection through vocabulary space
compression. By constraining the draft search to a frequency-prioritized token
subset, our method reduces LM Head computation overhead by 75% while ensuring
the equivalence of the final output distribution. Experiments across multiple
datasets demonstrate an average of 1.12$\times$ speedup over the
state-of-the-art speculative sampling method EAGLE-2. Code available at
https://github.com/thunlp/FR-Spec.
","[{'version': 'v1', 'created': 'Thu, 20 Feb 2025 18:58:10 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 08:54:55 GMT'}]",2025-03-12,"[['Zhao', 'Weilin', ''], ['Pan', 'Tengyu', ''], ['Han', 'Xu', ''], ['Zhang', 'Yudi', ''], ['Sun', 'Ao', ''], ['Huang', 'Yuxiang', ''], ['Zhang', 'Kaihuo', ''], ['Zhao', 'Weilun', ''], ['Li', 'Yuxuan', ''], ['Wang', 'Jianyong', ''], ['Liu', 'Zhiyuan', ''], ['Sun', 'Maosong', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Llama-3-8B', 'label': 'Llama'}]",Large Language Model,large language models,0.9664971828460693
2502.15844,Borui Yang,"Borui Yang, Md Afif Al Mamun, Jie M. Zhang, Gias Uddin","Hallucination Detection in Large Language Models with Metamorphic
  Relations","Accepted to the ACM Joint European Software Engineering Conference
  and Symposium on the Foundations of Software Engineering (ESEC/FSE 2025)",,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) are prone to hallucinations, e.g., factually
incorrect information, in their responses. These hallucinations present
challenges for LLM-based applications that demand high factual accuracy.
Existing hallucination detection methods primarily depend on external
resources, which can suffer from issues such as low availability, incomplete
coverage, privacy concerns, high latency, low reliability, and poor
scalability. There are also methods depending on output probabilities, which
are often inaccessible for closed-source LLMs like GPT models. This paper
presents MetaQA, a self-contained hallucination detection approach that
leverages metamorphic relation and prompt mutation. Unlike existing methods,
MetaQA operates without any external resources and is compatible with both
open-source and closed-source LLMs. MetaQA is based on the hypothesis that if
an LLM's response is a hallucination, the designed metamorphic relations will
be violated. We compare MetaQA with the state-of-the-art zero-resource
hallucination detection method, SelfCheckGPT, across multiple datasets, and on
two open-source and two closed-source LLMs. Our results reveal that MetaQA
outperforms SelfCheckGPT in terms of precision, recall, and f1 score. For the
four LLMs we study, MetaQA outperforms SelfCheckGPT with a superiority margin
ranging from 0.041 - 0.113 (for precision), 0.143 - 0.430 (for recall), and
0.154 - 0.368 (for F1-score). For instance, with Mistral-7B, MetaQA achieves an
average F1-score of 0.435, compared to SelfCheckGPT's F1-score of 0.205,
representing an improvement rate of 112.2%. MetaQA also demonstrates
superiority across all different categories of questions.
","[{'version': 'v1', 'created': 'Thu, 20 Feb 2025 19:44:33 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 18:28:18 GMT'}]",2025-03-13,"[['Yang', 'Borui', ''], ['Mamun', 'Md Afif Al', ''], ['Zhang', 'Jie M.', ''], ['Uddin', 'Gias', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2502.17591,Martin Kuo,"Martin Kuo, Jingyang Zhang, Jianyi Zhang, Minxue Tang, Louis
  DiValentin, Aolin Ding, Jingwei Sun, William Chen, Amin Hass, Tianlong Chen,
  Yiran Chen, Hai Li","Proactive Privacy Amnesia for Large Language Models: Safeguarding PII
  with Negligible Impact on Model Utility","ICLR'25 Poster. Project page and code is available at
  https://ppa-iclr2025.my.canva.site/",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the rise of large language models (LLMs), increasing research has
recognized their risk of leaking personally identifiable information (PII)
under malicious attacks. Although efforts have been made to protect PII in
LLMs, existing methods struggle to balance privacy protection with maintaining
model utility. In this paper, inspired by studies of amnesia in cognitive
science, we propose a novel approach, Proactive Privacy Amnesia (PPA), to
safeguard PII in LLMs while preserving their utility. This mechanism works by
actively identifying and forgetting key memories most closely associated with
PII in sequences, followed by a memory implanting using suitable substitute
memories to maintain the LLM's functionality. We conduct evaluations across
multiple models to protect common PII, such as phone numbers and physical
addresses, against prevalent PII-targeted attacks, demonstrating the
superiority of our method compared with other existing defensive techniques.
The results show that our PPA method completely eliminates the risk of phone
number exposure by 100% and significantly reduces the risk of physical address
exposure by 9.8% - 87.6%, all while maintaining comparable model utility
performance.
","[{'version': 'v1', 'created': 'Mon, 24 Feb 2025 19:16:39 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 17:32:22 GMT'}]",2025-03-12,"[['Kuo', 'Martin', ''], ['Zhang', 'Jingyang', ''], ['Zhang', 'Jianyi', ''], ['Tang', 'Minxue', ''], ['DiValentin', 'Louis', ''], ['Ding', 'Aolin', ''], ['Sun', 'Jingwei', ''], ['Chen', 'William', ''], ['Hass', 'Amin', ''], ['Chen', 'Tianlong', ''], ['Chen', 'Yiran', ''], ['Li', 'Hai', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2502.17599,Zhongwei Wan,"Zhongwei Wan, Hui Shen, Xin Wang, Che Liu, Zheda Mai, Mi Zhang","MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context
  Inference",NAACL 2025 Main,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Long-context Multimodal Large Language Models (MLLMs) that incorporate long
text-image and text-video modalities, demand substantial resources as their
multimodal Key-Value (KV) caches grow with increasing input lengths,
challenging inference efficiency. Existing methods for KV cache compression, in
both text-only and multimodal LLMs, have neglected attention density variations
across layers, thus often adopting uniform or progressive reduction strategies
for layer-wise cache allocation. In this work, we propose MEDA, a dynamic
layer-wise KV cache allocation method for efficient multimodal long-context
inference. As its core, MEDA utilizes cross-modal attention entropy to
determine the KV cache size at each MLLMs layer. Given the dynamically
allocated KV cache size at each layer, MEDA also employs a KV pair selection
scheme to identify which KV pairs to select and a KV pair merging strategy that
merges the selected and non-selected ones to preserve information from the
entire context. MEDA achieves up to 72% KV cache memory reduction and 2.82
times faster decoding speed, while maintaining or enhancing performance on
various multimodal tasks in long-context settings, including multi-images and
long-video scenarios. Our code is released at
https://github.com/AIoT-MLSys-Lab/MEDA.
","[{'version': 'v1', 'created': 'Mon, 24 Feb 2025 19:34:52 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 04:04:08 GMT'}]",2025-03-14,"[['Wan', 'Zhongwei', ''], ['Shen', 'Hui', ''], ['Wang', 'Xin', ''], ['Liu', 'Che', ''], ['Mai', 'Zheda', ''], ['Zhang', 'Mi', '']]","[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MEDA', 'label': 'LLM'}, {'text': 'cross-modal attention entropy', 'label': 'Attention mechanism'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]",Large Language Model,Multimodal Large Language Models,0.7649828195571899
2502.19263,Arnavi Chheda-Kothary,"Arnavi Chheda-Kothary, Ritesh Kanchi, Chris Sanders, Kevin Xiao,
  Aditya Sengupta, Melanie Kneitmix, Jacob O. Wobbrock, Jon E. Froehlich","ArtInsight: Enabling AI-Powered Artwork Engagement for Mixed
  Visual-Ability Families","21 pages, 30th International Conference on Intelligent User
  Interfaces (IUI 2025)",30th International Conference on Intelligent User Interfaces 2025,10.1145/3708359.3712082,,cs.HC,http://creativecommons.org/licenses/by/4.0/,"  We introduce ArtInsight, a novel AI-powered system to facilitate deeper
engagement with child-created artwork in mixed visual-ability families.
ArtInsight leverages large language models (LLMs) to craft a respectful and
thorough initial description of a child's artwork, and provides: creative
AI-generated descriptions for a vivid overview, audio recording to capture the
child's own description of their artwork, and a set of AI-generated questions
to facilitate discussion between blind or low-vision (BLV) family members and
their children. Alongside ArtInsight, we also contribute a new rubric to score
AI-generated descriptions of child-created artwork and an assessment of
state-of-the-art LLMs. We evaluated ArtInsight with five groups of BLV family
members and their children, and as a case study with one BLV child therapist.
Our findings highlight a preference for ArtInsight's longer,
artistically-tailored descriptions over those generated by existing BLV AI
tools. Participants highlighted the creative description and audio recording
components as most beneficial, with the former helping ``bring a picture to
life'' and the latter centering the child's narrative to generate context-aware
AI responses. Our findings reveal different ways that AI can be used to support
art engagement, including before, during, and after interaction with the child
artist, as well as expectations that BLV adults and their sighted children have
about AI-powered tools.
","[{'version': 'v1', 'created': 'Wed, 26 Feb 2025 16:17:15 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 16:33:23 GMT'}]",2025-03-11,"[['Chheda-Kothary', 'Arnavi', ''], ['Kanchi', 'Ritesh', ''], ['Sanders', 'Chris', ''], ['Xiao', 'Kevin', ''], ['Sengupta', 'Aditya', ''], ['Kneitmix', 'Melanie', ''], ['Wobbrock', 'Jacob O.', ''], ['Froehlich', 'Jon E.', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}]",Large Language Model,large language models,0.9664971828460693
2502.19679,Linzhuo Li,Linzhuo li,"Old Experience Helps: Leveraging Survey Methodology to Improve AI Text
  Annotation Reliability in Social Sciences",8 figures,,,,cs.DL cs.HC,http://creativecommons.org/licenses/by/4.0/,"  This paper introduces a framework for assessing the reliability of Large
Language Model (LLM) text annotations in social science research by adapting
established survey methodology principles. Drawing parallels between survey
respondent behavior and LLM outputs, the study implements three key
interventions: option randomization, position randomization, and reverse
validation. While traditional accuracy metrics may mask model instabilities,
particularly in edge cases, the framework provides a more comprehensive
reliability assessment. Using the F1000 dataset in biomedical science and three
sizes of Llama models (8B, 70B, and 405B parameters), the paper demonstrates
that these survey-inspired interventions can effectively identify unreliable
annotations that might otherwise go undetected through accuracy metrics alone.
The results show that 5-25% of LLM annotations change under these
interventions, with larger models exhibiting greater stability. Notably, for
rare categories approximately 50% of ""correct"" annotations demonstrate low
reliability when subjected to this framework. The paper then introduce an
information-theoretic reliability score (R-score) based on Kullback-Leibler
divergence that quantifies annotation confidence and distinguishes between
random guessing and meaningful annotations at the case level. This approach
complements existing expert validation methods by providing a scalable way to
assess internal annotation reliability and offers practical guidance for prompt
design and downstream analysis.
","[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 01:42:10 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 03:06:47 GMT'}]",2025-03-14,"[['li', 'Linzhuo', '']]","[{'text': 'Large\nLanguage Model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'prompt\ndesign', 'label': 'Prompting'}]",Large Language Model,"Large
Language Model",1.0
2502.19834,Guanzhou Ke,"Guanzhou Ke, Shengfeng He, Xiao Li Wang, Bo Wang, Guoqing Chao,
  Yuanyang Zhang, Yi Xie, and HeXing Su","Knowledge Bridger: Towards Training-free Missing Multi-modality
  Completion",Accepted to CVPR 2025,,,,cs.LG cs.CV cs.MM,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Previous successful approaches to missing modality completion rely on
carefully designed fusion techniques and extensive pre-training on complete
data, which can limit their generalizability in out-of-domain (OOD) scenarios.
In this study, we pose a new challenge: can we develop a missing modality
completion model that is both resource-efficient and robust to OOD
generalization? To address this, we present a training-free framework for
missing modality completion that leverages large multimodal models (LMMs). Our
approach, termed the ""Knowledge Bridger"", is modality-agnostic and integrates
generation and ranking of missing modalities. By defining domain-specific
priors, our method automatically extracts structured information from available
modalities to construct knowledge graphs. These extracted graphs connect the
missing modality generation and ranking modules through the LMM, resulting in
high-quality imputations of missing modalities. Experimental results across
both general and medical domains show that our approach consistently
outperforms competing methods, including in OOD generalization. Additionally,
our knowledge-driven generation and ranking techniques demonstrate superiority
over variants that directly employ LMMs for generation and ranking, offering
insights that may be valuable for applications in other domains.
","[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 07:14:11 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 04:45:52 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 01:45:10 GMT'}]",2025-03-12,"[['Ke', 'Guanzhou', ''], ['He', 'Shengfeng', ''], ['Wang', 'Xiao Li', ''], ['Wang', 'Bo', ''], ['Chao', 'Guoqing', ''], ['Zhang', 'Yuanyang', ''], ['Xie', 'Yi', ''], ['Su', 'HeXing', '']]","[{'text': 'large multimodal models', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}]",Large Language Model,large multimodal models,0.573912501335144
2502.19844,Xiangyan Qu,"Xiangyan Qu, Gaopeng Gou, Jiamin Zhuang, Jing Yu, Kun Song, Qihao
  Wang, Yili Li, Gang Xiong","ProAPO: Progressively Automatic Prompt Optimization for Visual
  Classification","Accepted to the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2025",,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Vision-language models (VLMs) have made significant progress in image
classification by training with large-scale paired image-text data. Their
performances largely depend on the prompt quality. While recent methods show
that visual descriptions generated by large language models (LLMs) enhance the
generalization of VLMs, class-specific prompts may be inaccurate or lack
discrimination due to the hallucination in LLMs. In this paper, we aim to find
visually discriminative prompts for fine-grained categories with minimal
supervision and no human-in-the-loop. An evolution-based algorithm is proposed
to progressively optimize language prompts from task-specific templates to
class-specific descriptions. Unlike optimizing templates, the search space
shows an explosion in class-specific candidate prompts. This increases prompt
generation costs, iterative times, and the overfitting problem. To this end, we
first introduce several simple yet effective edit-based and evolution-based
operations to generate diverse candidate prompts by one-time query of LLMs.
Then, two sampling strategies are proposed to find a better initial search
point and reduce traversed categories, saving iteration costs. Moreover, we
apply a novel fitness score with entropy constraints to mitigate overfitting.
In a challenging one-shot image classification setting, our method outperforms
existing textual prompt-based methods and improves LLM-generated description
methods across 13 datasets. Meanwhile, we demonstrate that our optimal prompts
improve adapter-based methods and transfer effectively across different
backbones.
","[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 07:39:23 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Mar 2025 01:18:01 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 08:56:58 GMT'}]",2025-03-13,"[['Qu', 'Xiangyan', ''], ['Gou', 'Gaopeng', ''], ['Zhuang', 'Jiamin', ''], ['Yu', 'Jing', ''], ['Song', 'Kun', ''], ['Wang', 'Qihao', ''], ['Li', 'Yili', ''], ['Xiong', 'Gang', '']]","[{'text': 'Vision-language models', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'class-specific prompts', 'label': 'Prompting'}, {'text': 'visually discriminative prompts', 'label': 'Prompting'}, {'text': 'evolution-based algorithm', 'label': 'LLM-based'}, {'text': 'language prompts', 'label': 'Prompting'}, {'text': 'task-specific templates', 'label': 'Embedding'}, {'text': 'class-specific candidate prompts', 'label': 'Prompting'}]",Large Language Model,large language models,0.9664971828460693
2502.19902,Zaijing Li,"Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, Liqiang
  Nie","Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action
  Conditioned Policy","Accept to CVPR 2025, Project page:
  https://cybertronagent.github.io/Optimus-2.github.io/",,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Building an agent that can mimic human behavior patterns to accomplish
various open-world tasks is a long-term goal. To enable agents to effectively
learn behavioral patterns across diverse tasks, a key challenge lies in
modeling the intricate relationships among observations, actions, and language.
To this end, we propose Optimus-2, a novel Minecraft agent that incorporates a
Multimodal Large Language Model (MLLM) for high-level planning, alongside a
Goal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP
contains (1) an Action-guided Behavior Encoder that models causal relationships
between observations and actions at each timestep, then dynamically interacts
with the historical observation-action sequence, consolidating it into
fixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with
open-ended language instructions to predict actions auto-regressively.
Moreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)}
dataset, which contains 25,000 videos across 8 atomic tasks, providing about
30M goal-observation-action pairs. The automated construction method, along
with the MGOA dataset, can contribute to the community's efforts to train
Minecraft agents. Extensive experimental results demonstrate that Optimus-2
exhibits superior performance across atomic tasks, long-horizon tasks, and
open-ended instruction tasks in Minecraft. Please see the project page at
https://cybertronagent.github.io/Optimus-2.github.io/.
","[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 09:18:04 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 07:51:05 GMT'}]",2025-03-12,"[['Li', 'Zaijing', ''], ['Xie', 'Yuquan', ''], ['Shao', 'Rui', ''], ['Chen', 'Gongwei', ''], ['Jiang', 'Dongmei', ''], ['Nie', 'Liqiang', '']]","[{'text': 'Multimodal Large Language Model', 'label': 'Large Language Model'}]",Large Language Model,Multimodal Large Language Model,0.7924776673316956
2502.19920,Marco Pleines,"Marco Pleines, Daniel Addis, David Rubinstein, Frank Zimmer, Mike
  Preuss, Peter Whidden",Pokemon Red via Reinforcement Learning,"8 pages, 3 figures, 3 tables, under review",,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Pok\'emon Red, a classic Game Boy JRPG, presents significant challenges as a
testbed for agents, including multi-tasking, long horizons of tens of thousands
of steps, hard exploration, and a vast array of potential policies. We
introduce a simplistic environment and a Deep Reinforcement Learning (DRL)
training methodology, demonstrating a baseline agent that completes an initial
segment of the game up to completing Cerulean City. Our experiments include
various ablations that reveal vulnerabilities in reward shaping, where agents
exploit specific reward signals. We also discuss limitations and argue that
games like Pok\'emon hold strong potential for future research on Large
Language Model agents, hierarchical training algorithms, and advanced
exploration methods. Source Code:
https://github.com/MarcoMeter/neroRL/tree/poke_red
","[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 09:42:23 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 05:44:11 GMT'}]",2025-03-12,"[['Pleines', 'Marco', ''], ['Addis', 'Daniel', ''], ['Rubinstein', 'David', ''], ['Zimmer', 'Frank', ''], ['Preuss', 'Mike', ''], ['Whidden', 'Peter', '']]","[{'text': 'Large\nLanguage Model', 'label': 'Large Language Model'}]",Large Language Model,"Large
Language Model",1.0
2502.19973,Luning Zhang,"Chao Wang, Luning Zhang, Zheng Wang, Yang Zhou","Can Large Language Models Unveil the Mysteries? An Exploration of Their
  Ability to Unlock Information in Complex Scenarios",11pages,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Combining multiple perceptual inputs and performing combinatorial reasoning
in complex scenarios is a sophisticated cognitive function in humans. With
advancements in multi-modal large language models, recent benchmarks tend to
evaluate visual understanding across multiple images. However, they often
overlook the necessity of combinatorial reasoning across multiple perceptual
information. To explore the ability of advanced models to integrate multiple
perceptual inputs for combinatorial reasoning in complex scenarios, we
introduce two benchmarks: Clue-Visual Question Answering (CVQA), with three
task types to assess visual comprehension and synthesis, and Clue of
Password-Visual Question Answering (CPVQA), with two task types focused on
accurate interpretation and application of visual data. For our benchmarks, we
present three plug-and-play approaches: utilizing model input for reasoning,
enhancing reasoning through minimum margin decoding with randomness generation,
and retrieving semantically relevant visual information for effective data
integration. The combined results reveal current models' poor performance on
combinatorial reasoning benchmarks, even the state-of-the-art (SOTA)
closed-source model achieves only 33.04% accuracy on CVQA, and drops to 7.38%
on CPVQA. Notably, our approach improves the performance of models on
combinatorial reasoning, with a 22.17% boost on CVQA and 9.40% on CPVQA over
the SOTA closed-source model, demonstrating its effectiveness in enhancing
combinatorial reasoning with multiple perceptual inputs in complex scenarios.
The code will be publicly available.
","[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 10:58:27 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 05:35:07 GMT'}]",2025-03-11,"[['Wang', 'Chao', ''], ['Zhang', 'Luning', ''], ['Wang', 'Zheng', ''], ['Zhou', 'Yang', '']]","[{'text': 'multi-modal large language models', 'label': 'Large Language Model'}]",Large Language Model,multi-modal large language models,0.7925285696983337
2502.20489,Linying Lv,Linying Lv,Do Sell-side Analyst Reports Have Investment Value?,,,,,q-fin.PR,http://creativecommons.org/licenses/by/4.0/,"  This paper documents new investment value in analyst reports. Analyst
narratives embedded with large language models strongly forecast future stock
returns, generating significant alpha beyond established analyst-based and
fundamental-based factors. The return predictability arises primarily from
reports that convey negative sentiment but forecast favorable long-term
prospects, suggesting systematic market overreaction to near-term negative
news. The effect is more pronounced for large, mature firms and for reports
authored by skilled, experienced analysts. A Shapley value decomposition
reveals that analysts' strategic outlook contributes the most to portfolio
performance, especially forward-looking discussions on fundamentals. Beyond
demonstrating untapped value in qualitative information, this paper illustrates
the broader potential of artificial intelligence to augment, rather than
replace, expert human judgment in financial markets.
","[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 19:53:59 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 17:02:27 GMT'}]",2025-03-12,"[['Lv', 'Linying', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2502.21037,Mingmin Feng,"Eric Hitz, Mingmin Feng, Radu Tanase, Ren\'e Algesheimer, Manuel S.
  Mariani",The amplifier effect of artificial agents in social contagion,Main text pp. 1-4; Supplementary Material pp. 5-10,,,,cs.SI econ.GN physics.soc-ph q-fin.EC,http://creativecommons.org/licenses/by/4.0/,"  Recent advances in artificial intelligence have led to the proliferation of
artificial agents in social contexts, ranging from education to online social
media and financial markets, among many others. The increasing rate at which
artificial and human agents interact makes it urgent to understand the
consequences of human-machine interactions for the propagation of new ideas,
products, and behaviors in society. Across two distinct empirical contexts, we
find here that artificial agents lead to significantly faster and wider social
contagion. To this end, we replicate a choice experiment previously conducted
with human subjects by using artificial agents powered by large language models
(LLMs). We use the experiment's results to measure the adoption thresholds of
artificial agents and their impact on the spread of social contagion. We find
that artificial agents tend to exhibit lower adoption thresholds than humans,
which leads to wider network-based social contagions. Our findings suggest that
the increased presence of artificial agents in real-world networks may
accelerate behavioral shifts, potentially in unforeseen ways.
","[{'version': 'v1', 'created': 'Fri, 28 Feb 2025 13:29:52 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 13:02:48 GMT'}]",2025-03-11,"[['Hitz', 'Eric', ''], ['Feng', 'Mingmin', ''], ['Tanase', 'Radu', ''], ['Algesheimer', 'René', ''], ['Mariani', 'Manuel S.', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2503.00399,Lijie Yang,"Juan Song, Lijie Yang, Mingtao Feng","Taming Large Multimodal Agents for Ultra-low Bitrate Semantically
  Disentangled Image Compression",,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  It remains a significant challenge to compress images at ultra-low bitrate
while achieving both semantic consistency and high perceptual quality. We
propose a novel image compression framework, Semantically Disentangled Image
Compression (SEDIC) in this paper. Our proposed SEDIC leverages large
multimodal models (LMMs) to disentangle the image into several essential
semantic information, including an extremely compressed reference image,
overall and object-level text descriptions, and the semantic masks. A
multi-stage semantic decoder is designed to progressively restore the
transmitted reference image object-by-object, ultimately producing high-quality
and perceptually consistent reconstructions. In each decoding stage, a
pre-trained controllable diffusion model is utilized to restore the object
details on the reference image conditioned by the text descriptions and
semantic masks. Experimental results demonstrate that SEDIC significantly
outperforms state-of-the-art approaches, achieving superior perceptual quality
and semantic consistency at ultra-low bitrates ($\le$ 0.05 bpp).
","[{'version': 'v1', 'created': 'Sat, 1 Mar 2025 08:27:11 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 02:03:22 GMT'}]",2025-03-13,"[['Song', 'Juan', ''], ['Yang', 'Lijie', ''], ['Feng', 'Mingtao', '']]","[{'text': 'large\nmultimodal models', 'label': 'Large Language Model'}]",Large Language Model,"large
multimodal models",0.573912501335144
2503.01001,Allen Lin,"Allen Lin, Renqin Cai, Yun He, Hanchao Yu, Jing Qian, Rui Li, Qifan
  Wang, James Caverlee",Towards An Efficient LLM Training Paradigm for CTR Prediction,,,,,cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) have demonstrated tremendous potential as the
next-generation ranking-based recommendation system. Many recent works have
shown that LLMs can significantly outperform conventional click-through-rate
(CTR) prediction approaches. Despite such promising results, the computational
inefficiency inherent in the current training paradigm makes it particularly
challenging to train LLMs for ranking-based recommendation tasks on large
datasets. To train LLMs for CTR prediction, most existing studies adopt the
prevalent ''sliding-window'' paradigm. Given a sequence of $m$ user
interactions, a unique training prompt is constructed for each interaction by
designating it as the prediction target along with its preceding $n$
interactions serving as context. In turn, the sliding-window paradigm results
in an overall complexity of $O(mn^2)$ that scales linearly with the length of
user interactions. Consequently, a direct adoption to train LLMs with such
strategy can result in prohibitively high training costs as the length of
interactions grows. To alleviate the computational inefficiency, we propose a
novel training paradigm, namely Dynamic Target Isolation (DTI), that
structurally parallelizes the training of $k$ (where $k >> 1$) target
interactions. Furthermore, we identify two major bottlenecks - hidden-state
leakage and positional bias overfitting - that limit DTI to only scale up to a
small value of $k$ (e.g., 5) then propose a computationally light solution to
effectively tackle each. Through extensive experiments on three widely adopted
public CTR datasets, we empirically show that DTI reduces training time by an
average of $\textbf{92%}$ (e.g., from $70.5$ hrs to $5.31$ hrs), without
compromising CTR prediction performance.
","[{'version': 'v1', 'created': 'Sun, 2 Mar 2025 19:43:35 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 21:50:37 GMT'}]",2025-03-11,"[['Lin', 'Allen', ''], ['Cai', 'Renqin', ''], ['He', 'Yun', ''], ['Yu', 'Hanchao', ''], ['Qian', 'Jing', ''], ['Li', 'Rui', ''], ['Wang', 'Qifan', ''], ['Caverlee', 'James', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'unique training prompt', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.01564,Darya Frolova,"Eli Sason, Darya Frolova, Boris Nazarov, Felix Goldberd",Attention Condensation via Sparsity Induced Regularized Training,"The loss described in the section 3 (pg 4, expression (2)) has an
  error and needs to be corrected. The experiments should be re-run according
  to the modified loss. This loss correction doesn't affect the general idea of
  the paper, and the paper will be resubmitted after the new corrected
  experimental results are obtained",,,,cs.CL,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  As the context window expands, self-attention increasingly dominates the
transformer's inference time. Therefore, accelerating attention computation
while minimizing performance degradation is essential for the efficient
deployment of Large Language Models (LLMs). In this study we extend a
theoretical framework of attention sparsity in LLMs. A customized loss function
is designed to enforce the sparsity by restricting the number of top elements
in the attention matrix. We perform an initial set of evaluations with GPT-2 to
show the effectiveness of our sparsification approach. The attention matrices
of the models trained with the proposed loss are both sparse and effective in
capturing relevant input dependencies. We now continue working to demonstrate
the value of our approach on larger models and different architectures.
","[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 14:09:13 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 18:12:59 GMT'}]",2025-03-14,"[['Sason', 'Eli', ''], ['Frolova', 'Darya', ''], ['Nazarov', 'Boris', ''], ['Goldberd', 'Felix', '']]","[{'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'GPT-2', 'label': 'GPT'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.02233,Hang Zheng,"Hang Zheng, Hongshen Xu, Yuncong Liu, Lu Chen, Pascale Fung, Kai Yu",Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models (LLMs) frequently hallucinate due to misaligned
self-awareness, generating erroneous outputs when addressing queries beyond
their knowledge boundaries. While existing approaches mitigate hallucinations
via uncertainty estimation or query rejection, they suffer from computational
inefficiency or sacrificed helpfulness. To address these issues, we propose the
Explicit Knowledge Boundary Modeling (EKBM) framework, integrating fast and
slow reasoning systems to harmonize reliability and usability. The framework
first employs a fast-thinking model to generate confidence-labeled responses,
enabling immediate use of high-confidence outputs. For uncertain predictions, a
slow refinement model conducts targeted reasoning to improve accuracy. To align
model behavior with our proposed object, we propose a hybrid training pipeline,
enhancing self-awareness without degrading task performance. Evaluations on
dialogue state tracking tasks demonstrate that EKBM achieves superior model
reliability over uncertainty-based baselines. Further analysis reveals that
refinement substantially boosts accuracy while maintaining low computational
overhead. Our work establishes a scalable paradigm for advancing LLM
reliability and balancing accuracy and practical utility in error-sensitive
applications.
","[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 03:16:02 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 07:42:04 GMT'}]",2025-03-13,"[['Zheng', 'Hang', ''], ['Xu', 'Hongshen', ''], ['Liu', 'Yuncong', ''], ['Chen', 'Lu', ''], ['Fung', 'Pascale', ''], ['Yu', 'Kai', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'dialogue state tracking tasks', 'label': 'ChatGPT'}]",Large Language Model,Large language models,0.9664971828460693
2503.03122,Zichao Li,"Zichao Li, Xueru Wen, Jie Lou, Yuqiu Ji, Yaojie Lu, Xianpei Han,
  Debing Zhang, Le Sun","The Devil Is in the Details: Tackling Unimodal Spurious Correlations for
  Generalizable Multimodal Reward Models",,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language
Models (LLMs) with human preferences, particularly as LLMs increasingly
interact with multimodal data. However, we find that MM-RMs trained on existing
datasets often struggle to generalize to out-of-distribution data due to their
reliance on unimodal spurious correlations, primarily text-only shortcuts
within the training distribution, which prevents them from leveraging true
multimodal reward functions. To address this, we introduce a Shortcut-aware
MM-RM learning algorithm that mitigates this issue by dynamically reweighting
training samples, shifting the distribution toward better multimodal
understanding, and reducing dependence on unimodal spurious correlations. Our
experiments demonstrate significant improvements in generalization, downstream
task performance, and scalability, establishing a more robust framework for
multimodal reward modeling.
","[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 02:37:41 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 02:34:53 GMT'}]",2025-03-11,"[['Li', 'Zichao', ''], ['Wen', 'Xueru', ''], ['Lou', 'Jie', ''], ['Ji', 'Yuqiu', ''], ['Lu', 'Yaojie', ''], ['Han', 'Xianpei', ''], ['Zhang', 'Debing', ''], ['Sun', 'Le', '']]","[{'text': 'Multimodal Reward Models', 'label': 'Large Language Model'}, {'text': 'Large Language\nModels', 'label': 'Large Language Model'}, {'text': 'scalability', 'label': 'Scaling law'}]",Large Language Model,"Large Language
Models",0.9664971828460693
2503.04299,Henry Papadatos,"Malcolm Murray, Henry Papadatos, Otter Quarks, Pierre-Fran\c{c}ois
  Gimenez, Simeon Campos","Mapping AI Benchmark Data to Quantitative Risk Estimates Through Expert
  Elicitation","23 pages, 4 figures",,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The literature and multiple experts point to many potential risks from large
language models (LLMs), but there are still very few direct measurements of the
actual harms posed. AI risk assessment has so far focused on measuring the
models' capabilities, but the capabilities of models are only indicators of
risk, not measures of risk. Better modeling and quantification of AI risk
scenarios can help bridge this disconnect and link the capabilities of LLMs to
tangible real-world harm. This paper makes an early contribution to this field
by demonstrating how existing AI benchmarks can be used to facilitate the
creation of risk estimates. We describe the results of a pilot study in which
experts use information from Cybench, an AI benchmark, to generate probability
estimates. We show that the methodology seems promising for this purpose, while
noting improvements that can be made to further strengthen its application in
quantitative AI risk assessment.
","[{'version': 'v1', 'created': 'Thu, 6 Mar 2025 10:39:47 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 13:00:00 GMT'}]",2025-03-11,"[['Murray', 'Malcolm', ''], ['Papadatos', 'Henry', ''], ['Quarks', 'Otter', ''], ['Gimenez', 'Pierre-François', ''], ['Campos', 'Simeon', '']]","[{'text': 'large\nlanguage models', 'label': 'Large Language Model'}, {'text': 'AI risk assessment', 'label': 'AI Ethics'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Cybench', 'label': 'Open-source LLMs'}, {'text': 'AI risk assessment', 'label': 'AI Ethics'}]",Large Language Model,"large
language models",0.9664971828460693
2503.04773,Bingbing Fan,"Bingbing Fan, Lin Chen, Songwei Li, Jian Yuan, Fengli Xu, Pan Hui,
  Yong Li","Invisible Walls in Cities: Leveraging Large Language Models to Predict
  Urban Segregation Experience with Social Media Content","11 pages, 6 figures",,,,cs.CL cs.CY cs.SI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Understanding experienced segregation in urban daily life is crucial for
addressing societal inequalities and fostering inclusivity. The abundance of
user-generated reviews on social media encapsulates nuanced perceptions and
feelings associated with different places, offering rich insights into
segregation. However, leveraging this data poses significant challenges due to
its vast volume, ambiguity, and confluence of diverse perspectives. To tackle
these challenges, we propose using Large Language Models (LLMs) to automate
online review mining for segregation prediction. We design a Reflective LLM
Coder to digest social media content into insights consistent with real-world
feedback, and eventually produce a codebook capturing key dimensions that
signal segregation experience, such as cultural resonance and appeal,
accessibility and convenience, and community engagement and local involvement.
Guided by the codebook, LLMs can generate both informative review summaries and
ratings for segregation prediction. Moreover, we design a
REasoning-and-EMbedding (RE'EM) framework, which combines the reasoning and
embedding capabilities of language models to integrate multi-channel features
for segregation prediction. Experiments on real-world data demonstrate that our
framework greatly improves prediction accuracy, with a 22.79% elevation in R2
and a 9.33% reduction in MSE. The derived codebook is generalizable across
three different cities, consistently improving prediction accuracy. Moreover,
our user study confirms that the codebook-guided summaries provide cognitive
gains for human participants in perceiving POIs' social inclusiveness. Our
study marks an important step toward understanding implicit social barriers and
inequalities, demonstrating the great potential of promoting social
inclusiveness with AI.
","[{'version': 'v1', 'created': 'Mon, 17 Feb 2025 09:52:17 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 06:15:16 GMT'}]",2025-03-11,"[['Fan', 'Bingbing', ''], ['Chen', 'Lin', ''], ['Li', 'Songwei', ''], ['Yuan', 'Jian', ''], ['Xu', 'Fengli', ''], ['Hui', 'Pan', ''], ['Li', 'Yong', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.04779,Thanh Le-Cong Le-Cong Thanh,"Thanh Le-Cong, Bach Le, Toby Murray","Can LLMs Reason About Program Semantics? A Comprehensive Evaluation of
  LLMs on Formal Specification Inference",,,,,cs.PL cs.AI cs.SE,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) are increasingly being used to automate
programming tasks. Yet, LLMs' capabilities in reasoning about program semantics
are still inadequately studied, leaving significant potential for further
exploration. This paper introduces FormalBench, a comprehensive benchmark
designed to evaluate LLMs' reasoning abilities on program semantics,
particularly via the task of synthesizing formal program specifications to
assist verifying program correctness. This task requires both comprehensive
reasoning over all possible program executions and the generation of precise,
syntactically correct expressions that adhere to formal syntax and semantics.
Using this benchmark, we evaluated the ability of LLMs in synthesizing
consistent and complete specifications. Our findings show that LLMs perform
well with simple control flows but struggle with more complex structures,
especially loops, even with advanced prompting. Additionally, LLMs exhibit
limited robustness against semantic-preserving transformations. We also
highlight common failure patterns and design self-repair prompts, improving
success rates by 25%.
","[{'version': 'v1', 'created': 'Sat, 22 Feb 2025 13:27:31 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 07:41:37 GMT'}]",2025-03-14,"[['Le-Cong', 'Thanh', ''], ['Le', 'Bach', ''], ['Murray', 'Toby', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'advanced prompting', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'design self-repair prompts', 'label': 'Prompting'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.04784,Jiexiong Liu,"Cheng Li, Jiexiong Liu, Yixuan Chen, Yanqin Jia, Zhepeng Li","KunlunBaize: LLM with Multi-Scale Convolution and Multi-Token Prediction
  Under TransformerX Framework",21 pages,,,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large language models have demonstrated remarkable performance across various
tasks, yet they face challenges such as low computational efficiency, gradient
vanishing, and difficulties in capturing complex feature interactions. To
address these limitations, a novel framework has been proposed. This framework
incorporates a learnable dense residual skip connection mechanism, a
TransformerX module a transformer based component integrating multiscale
convolution and adaptive activation functions and a multitoken prediction
interaction module. The learnable dense residual connections enhance
information flow and feature capture across layers. Within the TransformerX
module, large convolutional kernels aggregate semantic information from
extensive text segments, while smaller convolutions focus on local word order
and syntactic structures. The adaptive activation function dynamically adjusts
its parameters based on the semantic features of the input text, improving the
model's ability to handle diverse semantic expressions and complex
relationships. The multitoken prediction module boosts data utilization and
accelerates inference by predicting multiple future tokens. These components
significantly enhance the performance and efficiency of large language models.
","[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 01:56:09 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 01:59:26 GMT'}]",2025-03-12,"[['Li', 'Cheng', ''], ['Liu', 'Jiexiong', ''], ['Chen', 'Yixuan', ''], ['Jia', 'Yanqin', ''], ['Li', 'Zhepeng', '']]","[{'text': 'Large language models', 'label': 'Large Language Model'}]",Large Language Model,Large language models,0.9664971828460693
2503.04809,Lang Mei,"Lang Mei, Chong Chen, Jiaxin Mao",PanguIR Technical Report for NTCIR-18 AEOLLM Task,,,,,cs.CL cs.AI,http://creativecommons.org/licenses/by/4.0/,"  As large language models (LLMs) gain widespread attention in both academia
and industry, it becomes increasingly critical and challenging to effectively
evaluate their capabilities. Existing evaluation methods can be broadly
categorized into two types: manual evaluation and automatic evaluation. Manual
evaluation, while comprehensive, is often costly and resource-intensive.
Conversely, automatic evaluation offers greater scalability but is constrained
by the limitations of its evaluation criteria (dominated by reference-based
answers). To address these challenges, NTCIR-18 introduced the AEOLLM
(Automatic Evaluation of LLMs) task, aiming to encourage reference-free
evaluation methods that can overcome the limitations of existing approaches. In
this paper, to enhance the evaluation performance of the AEOLLM task, we
propose three key methods to improve the reference-free evaluation: 1)
Multi-model Collaboration: Leveraging multiple LLMs to approximate human
ratings across various subtasks; 2) Prompt Auto-optimization: Utilizing LLMs to
iteratively refine the initial task prompts based on evaluation feedback from
training samples; and 3) In-context Learning (ICL) Optimization: Based on the
multi-task evaluation feedback, we train a specialized in-context example
retrieval model, combined with a semantic relevance retrieval model, to jointly
identify the most effective in-context learning examples. Experiments conducted
on the final dataset demonstrate that our approach achieves superior
performance on the AEOLLM task.
","[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 07:40:02 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 06:49:01 GMT'}]",2025-03-11,"[['Mei', 'Lang', ''], ['Chen', 'Chong', ''], ['Mao', 'Jiaxin', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Prompt Auto-optimization', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'initial task prompts', 'label': 'Prompting'}]",Large Language Model,large language models,0.9664971828460693
2503.06362,Umberto Cappellazzo,"Umberto Cappellazzo, Minsu Kim, Stavros Petridis","Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal
  LLMs",,,,,cs.CV cs.MM cs.SD eess.AS,http://creativecommons.org/licenses/by/4.0/,"  Audio-Visual Speech Recognition (AVSR) leverages both audio and visual
modalities to enhance speech recognition robustness, particularly in noisy
environments. Recent advancements in Large Language Models (LLMs) have
demonstrated their effectiveness in speech recognition, including AVSR.
However, due to the significant length of speech representations, direct
integration with LLMs imposes substantial computational costs. Prior approaches
address this by compressing speech representations before feeding them into
LLMs. However, higher compression ratios often lead to performance degradation,
necessitating a trade-off between computational efficiency and recognition
accuracy. To address this challenge, we propose Llama-MTSK, the first
Matryoshka-based Multimodal LLM for AVSR, which enables flexible adaptation of
the audio-visual token allocation based on specific computational constraints
while preserving high performance. Our approach, inspired by Matryoshka
Representation Learning, encodes audio-visual representations at multiple
granularities within a single model, eliminating the need to train separate
models for different compression levels. Moreover, to efficiently fine-tune the
LLM, we introduce three LoRA-based Matryoshka strategies using global and
scale-specific LoRA modules. Extensive evaluations on the two largest AVSR
datasets demonstrate that Llama-MTSK achieves state-of-the-art results,
matching or surpassing models trained independently at fixed compression
levels.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 00:02:10 GMT'}]",2025-03-11,"[['Cappellazzo', 'Umberto', ''], ['Kim', 'Minsu', ''], ['Petridis', 'Stavros', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Llama-MTSK', 'label': 'Llama'}, {'text': 'Matryoshka\nRepresentation Learning', 'label': 'Few-shot Learning'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.06387,Joseph Wilson,Anurag Swarnim Yadav and Joseph N. Wilson,R+R: Security Vulnerability Dataset Quality Is Critical,"15 pages, 1 figure, 35 tables. To be published in Proceedings of the
  2024 Annual Computer Security Applications Conference (ACSAC)",,,,cs.SE cs.CR,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) are of great interest in vulnerability detection
and repair. The effectiveness of these models hinges on the quality of the
datasets used for both training and evaluation. Our investigation reveals that
a number of studies featured in prominent software engineering conferences have
employed datasets that are plagued by high duplication rates, questionable
label accuracy, and incomplete samples. Using these datasets for
experimentation will yield incorrect results that are significantly different
from actual expected behavior. For example, the state-of-the-art VulRepair
Model, which is reported to have 44% accuracy, on average yielded 9% accuracy
when test-set duplicates were removed from its training set and 13% accuracy
when training-set duplicates were removed from its test set. In an effort to
tackle these data quality concerns, we have retrained models from several
papers without duplicates and conducted an accuracy assessment of labels for
the top ten most hazardous Common Weakness Enumerations (CWEs). Our findings
indicate that 56% of the samples had incorrect labels and 44% comprised
incomplete samples--only 31% were both accurate and complete. Finally, we
employ transfer learning using a large deduplicated bugfix corpus to show that
these models can exhibit better performance if given larger amounts of
high-quality pre-training data, leading us to conclude that while previous
studies have over-estimated performance due to poor dataset quality, this does
not demonstrate that better performance is not possible.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 01:49:30 GMT'}]",2025-03-11,"[['Yadav', 'Anurag Swarnim', ''], ['Wilson', 'Joseph N.', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'transfer learning', 'label': 'Few-shot Learning'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.06394,Tatsuro Inaba,"Tatsuro Inaba, Kentaro Inui, Yusuke Miyao, Yohei Oseki, Benjamin
  Heinzerling, Yu Takagi","How LLMs Learn: Tracing Internal Representations with Sparse
  Autoencoders","Our code, demo, SAE weights are available at:
  https://github.com/llm-jp/llm-jp-sae",,,,cs.CL cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) demonstrate remarkable multilingual capabilities
and broad knowledge. However, the internal mechanisms underlying the
development of these capabilities remain poorly understood. To investigate
this, we analyze how the information encoded in LLMs' internal representations
evolves during the training process. Specifically, we train sparse autoencoders
at multiple checkpoints of the model and systematically compare the
interpretative results across these stages. Our findings suggest that LLMs
initially acquire language-specific knowledge independently, followed by
cross-linguistic correspondences. Moreover, we observe that after mastering
token-level knowledge, the model transitions to learning higher-level, abstract
concepts, indicating the development of more conceptual understanding.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 02:13:44 GMT'}]",2025-03-11,"[['Inaba', 'Tatsuro', ''], ['Inui', 'Kentaro', ''], ['Miyao', 'Yusuke', ''], ['Oseki', 'Yohei', ''], ['Heinzerling', 'Benjamin', ''], ['Takagi', 'Yu', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.06410,Haohan Wang,"Alex Casella, Wayne Wang",Performant LLM Agentic Framework for Conversational AI,"6 pages, 3 figures",,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The rise of Agentic applications and automation in the Voice AI industry has
led to an increased reliance on Large Language Models (LLMs) to navigate
graph-based logic workflows composed of nodes and edges. However, existing
methods face challenges such as alignment errors in complex workflows and
hallucinations caused by excessive context size. To address these limitations,
we introduce the Performant Agentic Framework (PAF), a novel system that
assists LLMs in selecting appropriate nodes and executing actions in order when
traversing complex graphs. PAF combines LLM-based reasoning with a
mathematically grounded vector scoring mechanism, achieving both higher
accuracy and reduced latency. Our approach dynamically balances strict
adherence to predefined paths with flexible node jumps to handle various user
inputs efficiently. Experiments demonstrate that PAF significantly outperforms
baseline methods, paving the way for scalable, real-time Conversational AI
systems in complex business environments.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 02:58:34 GMT'}]",2025-03-11,"[['Casella', 'Alex', ''], ['Wang', 'Wayne', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.06421,Weihao Cui,"Weihao Cui, Ziyi Xu, Han Zhao, Quan Chen, Zijun Li, Bingsheng He,
  Minyi Guo",Efficient Function-as-a-Service for Large Language Models with TIDAL,,,,,cs.OS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Model (LLM) applications have emerged as a prominent use case
for Function-as-a-Service (FaaS) due to their high computational demands and
sporadic invocation patterns. However, serving LLM functions within FaaS
frameworks faces significant GPU-side cold start. A fundamental approach
involves leveraging a template with function state saved on GPUs to bypass the
cold start for new invocations. Yet, this approach struggles with the high GPU
footprint, dynamic initialization behaviors, and lazy GPU kernel loading
inherent in LLM functions, primarily due to a lack of insight into the
underlying execution details. In this paper, we introduce TIDAL, an optimized
FaaS framework for LLM applications that achieves fast startups by tracing
fine-grained execution paths. By utilizing the traced execution details, TIDAL
generates adaptive function templates, effectively breaking startup barriers
for LLM functions. Extensive evaluations demonstrate that TIDAL reduces cold
start latency by $1.79\times\text{\textasciitilde}2.11\times$ and improves the
$95\%$-ile time-to-first-token by $76.0\%$, surpassing state-of-the-art
methods.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 03:33:15 GMT'}]",2025-03-11,"[['Cui', 'Weihao', ''], ['Xu', 'Ziyi', ''], ['Zhao', 'Han', ''], ['Chen', 'Quan', ''], ['Li', 'Zijun', ''], ['He', 'Bingsheng', ''], ['Guo', 'Minyi', '']]","[{'text': 'Large Language Model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]",Large Language Model,Large Language Model,1.0
2503.06430,Zhangchi Qiu,"Zhangchi Qiu, Linhao Luo, Zicheng Zhao, Shirui Pan and Alan Wee-Chung
  Liew",Graph Retrieval-Augmented LLM for Conversational Recommendation Systems,Accepted by PAKDD 2025,,,,cs.CL cs.AI cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Conversational Recommender Systems (CRSs) have emerged as a transformative
paradigm for offering personalized recommendations through natural language
dialogue. However, they face challenges with knowledge sparsity, as users often
provide brief, incomplete preference statements. While recent methods have
integrated external knowledge sources to mitigate this, they still struggle
with semantic understanding and complex preference reasoning. Recent Large
Language Models (LLMs) demonstrate promising capabilities in natural language
understanding and reasoning, showing significant potential for CRSs.
Nevertheless, due to the lack of domain knowledge, existing LLM-based CRSs
either produce hallucinated recommendations or demand expensive domain-specific
training, which largely limits their applicability. In this work, we present
G-CRS (Graph Retrieval-Augmented Large Language Model for Conversational
Recommender Systems), a novel training-free framework that combines graph
retrieval-augmented generation and in-context learning to enhance LLMs'
recommendation capabilities. Specifically, G-CRS employs a two-stage
retrieve-and-recommend architecture, where a GNN-based graph reasoner first
identifies candidate items, followed by Personalized PageRank exploration to
jointly discover potential items and similar user interactions. These retrieved
contexts are then transformed into structured prompts for LLM reasoning,
enabling contextually grounded recommendations without task-specific training.
Extensive experiments on two public datasets show that G-CRS achieves superior
recommendation performance compared to existing methods without requiring
task-specific training.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 03:56:22 GMT'}]",2025-03-11,"[['Qiu', 'Zhangchi', ''], ['Luo', 'Linhao', ''], ['Zhao', 'Zicheng', ''], ['Pan', 'Shirui', ''], ['Liew', 'Alan Wee-Chung', '']]","[{'text': 'Recent Large\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'structured prompts', 'label': 'Prompting'}]",Large Language Model,"Recent Large
Language Models",0.9028562307357788
2503.06463,Tongze Zhang,"Tongze Zhang, Tammy Chung, Anind Dey, Sang Won Bae","AXAI-CDSS : An Affective Explainable AI-Driven Clinical Decision Support
  System for Cannabis Use",,,,,cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As cannabis use has increased in recent years, researchers have come to rely
on sophisticated machine learning models to predict cannabis use behavior and
its impact on health. However, many artificial intelligence (AI) models lack
transparency and interpretability due to their opaque nature, limiting their
trust and adoption in real-world medical applications, such as clinical
decision support systems (CDSS). To address this issue, this paper enhances
algorithm explainability underlying CDSS by integrating multiple Explainable
Artificial Intelligence (XAI) methods and applying causal inference techniques
to clarify the model' predictive decisions under various scenarios. By
providing deeper interpretability of the XAI outputs using Large Language
Models (LLMs), we provide users with more personalized and accessible insights
to overcome the challenges posed by AI's ""black box"" nature. Our system
dynamically adjusts feedback based on user queries and emotional states,
combining text-based sentiment analysis with real-time facial emotion
recognition to ensure responses are empathetic, context-adaptive, and
user-centered. This approach bridges the gap between the learning demands of
interpretability and the need for intuitive understanding, enabling
non-technical users such as clinicians and clinical researchers to interact
effectively with AI models.} Ultimately, this approach improves usability,
enhances perceived trustworthiness, and increases the impact of CDSS in
healthcare applications.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 05:40:44 GMT'}]",2025-03-11,"[['Zhang', 'Tongze', ''], ['Chung', 'Tammy', ''], ['Dey', 'Anind', ''], ['Bae', 'Sang Won', '']]","[{'text': 'Large Language\nModels', 'label': 'Large Language Model'}]",Large Language Model,"Large Language
Models",0.9664971828460693
2503.06474,Kong Huanjun,"Huanjun Kong, Zhefan Wang, Chenyang Wang, Zhe Ma, Nanqing Dong",HuixiangDou2: A Robustly Optimized GraphRAG Approach,11 pages,,,,cs.IR cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Large Language Models (LLMs) perform well on familiar queries but struggle
with specialized or emerging topics. Graph-based Retrieval-Augmented Generation
(GraphRAG) addresses this by structuring domain knowledge as a graph for
dynamic retrieval. However, existing pipelines involve complex engineering
workflows, making it difficult to isolate the impact of individual components.
Evaluating retrieval effectiveness is also challenging due to dataset overlap
with LLM pretraining data. In this work, we introduce HuixiangDou2, a robustly
optimized GraphRAG framework. Specifically, we leverage the effectiveness of
dual-level retrieval and optimize its performance in a 32k context for maximum
precision, and compare logic-based retrieval and dual-level retrieval to
enhance overall functionality. Our implementation includes comparative
experiments on a test set, where Qwen2.5-7B-Instruct initially underperformed.
With our approach, the score improved significantly from 60 to 74.5, as
illustrated in the Figure. Experiments on domain-specific datasets reveal that
dual-level retrieval enhances fuzzy matching, while logic-form retrieval
improves structured reasoning. Furthermore, we propose a multi-stage
verification mechanism to improve retrieval robustness without increasing
computational cost. Empirical results show significant accuracy gains over
baselines, highlighting the importance of adaptive retrieval. To support
research and adoption, we release HuixiangDou2 as an open-source resource
https://github.com/tpoisonooo/huixiangdou2.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 06:20:24 GMT'}]",2025-03-11,"[['Kong', 'Huanjun', ''], ['Wang', 'Zhefan', ''], ['Wang', 'Chenyang', ''], ['Ma', 'Zhe', ''], ['Dong', 'Nanqing', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GraphRAG', 'label': 'RAG'}, {'text': 'GraphRAG', 'label': 'RAG'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.06486,Cong Chen,"Cong Chen, Mingyu Liu, Chenchen Jing, Yizhou Zhou, Fengyun Rao, Hao
  Chen, Bo Zhang, Chunhua Shen","PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative
  Visual Training",,,,,cs.CV cs.AI,http://creativecommons.org/licenses/by/4.0/,"  This paper aims to address the challenge of hallucinations in Multimodal
Large Language Models (MLLMs) particularly for dense image captioning tasks. To
tackle the challenge, we identify the current lack of a metric that finely
measures the caption quality in concept level. We hereby introduce HalFscore, a
novel metric built upon the language graph and is designed to evaluate both the
accuracy and completeness of dense captions at a granular level. Additionally,
we identify the root cause of hallucination as the model's over-reliance on its
language prior. To address this, we propose PerturboLLaVA, which reduces the
model's reliance on the language prior by incorporating adversarially perturbed
text during training. This method enhances the model's focus on visual inputs,
effectively reducing hallucinations and producing accurate, image-grounded
descriptions without incurring additional computational overhead. PerturboLLaVA
significantly improves the fidelity of generated captions, outperforming
existing approaches in handling multimodal hallucinations and achieving
improved performance across general multimodal benchmarks.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 07:07:03 GMT'}]",2025-03-11,"[['Chen', 'Cong', ''], ['Liu', 'Mingyu', ''], ['Jing', 'Chenchen', ''], ['Zhou', 'Yizhou', ''], ['Rao', 'Fengyun', ''], ['Chen', 'Hao', ''], ['Zhang', 'Bo', ''], ['Shen', 'Chunhua', '']]","[{'text': 'Multimodal\nLarge Language Models', 'label': 'Large Language Model'}]",Large Language Model,"Multimodal
Large Language Models",0.7649828195571899
2503.06497,Enming Zhang,"Enming Zhang, Peizhe Gong, Xingyuan Dai, Yisheng Lv, Qinghai Miao","Evaluation of Safety Cognition Capability in Vision-Language Models for
  Autonomous Driving",,,,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Assessing the safety of vision-language models (VLMs) in autonomous driving
is particularly important; however, existing work mainly focuses on traditional
benchmark evaluations. As interactive components within autonomous driving
systems, VLMs must maintain strong safety cognition during interactions. From
this perspective, we propose a novel evaluation method: Safety Cognitive
Driving Benchmark (SCD-Bench) . To address the large-scale annotation challenge
for SCD-Bench, we develop the Autonomous Driving Image-Text Annotation System
(ADA) . Additionally, to ensure data quality in SCD-Bench, our dataset
undergoes manual refinement by experts with professional knowledge in
autonomous driving. We further develop an automated evaluation method based on
large language models (LLMs). To verify its effectiveness, we compare its
evaluation results with those of expert human evaluations, achieving a
consistency rate of 99.74%. Preliminary experimental results indicate that
existing open-source models still lack sufficient safety cognition, showing a
significant gap compared to GPT-4o. Notably, lightweight models (1B-4B)
demonstrate minimal safety cognition. However, since lightweight models are
crucial for autonomous driving systems, this presents a significant challenge
for integrating VLMs into the field.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 07:53:19 GMT'}]",2025-03-11,"[['Zhang', 'Enming', ''], ['Gong', 'Peizhe', ''], ['Dai', 'Xingyuan', ''], ['Lv', 'Yisheng', ''], ['Miao', 'Qinghai', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'GPT-4o', 'label': 'GPT'}]",Large Language Model,large language models,0.9664971828460693
2503.06519,Wenhui Zhang,"Wenhui Zhang, Huiyu Xu, Zhibo Wang, Zeqing He, Ziqi Zhu, Kui Ren","Can Small Language Models Reliably Resist Jailbreak Attacks? A
  Comprehensive Evaluation","19 pages, 12 figures",,,,cs.CR cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Small language models (SLMs) have emerged as promising alternatives to large
language models (LLMs) due to their low computational demands, enhanced privacy
guarantees and comparable performance in specific domains through light-weight
fine-tuning. Deploying SLMs on edge devices, such as smartphones and smart
vehicles, has become a growing trend. However, the security implications of
SLMs have received less attention than LLMs, particularly regarding jailbreak
attacks, which is recognized as one of the top threats of LLMs by the OWASP. In
this paper, we conduct the first large-scale empirical study of SLMs'
vulnerabilities to jailbreak attacks. Through systematically evaluation on 63
SLMs from 15 mainstream SLM families against 8 state-of-the-art jailbreak
methods, we demonstrate that 47.6% of evaluated SLMs show high susceptibility
to jailbreak attacks (ASR > 40%) and 38.1% of them can not even resist direct
harmful query (ASR > 50%). We further analyze the reasons behind the
vulnerabilities and identify four key factors: model size, model architecture,
training datasets and training techniques. Moreover, we assess the
effectiveness of three prompt-level defense methods and find that none of them
achieve perfect performance, with detection accuracy varying across different
SLMs and attack methods. Notably, we point out that the inherent security
awareness play a critical role in SLM security, and models with strong security
awareness could timely terminate unsafe response with little reminder. Building
upon the findings, we highlight the urgent need for security-by-design
approaches in SLM development and provide valuable insights for building more
trustworthy SLM ecosystem.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 08:47:16 GMT'}]",2025-03-11,"[['Zhang', 'Wenhui', ''], ['Xu', 'Huiyu', ''], ['Wang', 'Zhibo', ''], ['He', 'Zeqing', ''], ['Zhu', 'Ziqi', ''], ['Ren', 'Kui', '']]","[{'text': 'large\nlanguage models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'light-weight\nfine-tuning', 'label': 'Fine-tuning'}, {'text': 'SLMs', 'label': 'Large Language Model'}, {'text': 'SLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'SLMs', 'label': 'Large Language Model'}, {'text': 'SLMs', 'label': 'Large Language Model'}, {'text': 'SLMs', 'label': 'Large Language Model'}, {'text': 'prompt-level defense methods', 'label': 'Prompting'}, {'text': 'SLMs', 'label': 'Large Language Model'}]",Large Language Model,"large
language models",0.9664971828460693
2503.06525,Xian Gao,"Xian Gao, Jiacheng Ruan, Jingsheng Gao, Mingye Xie, Zongyun Zhang,
  Ting Liu and Yuzhuo Fu","From Motion Signals to Insights: A Unified Framework for Student
  Behavior Analysis and Feedback in Physical Education Classes",Work in progress,,,,cs.CY cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Analyzing student behavior in educational scenarios is crucial for enhancing
teaching quality and student engagement. Existing AI-based models often rely on
classroom video footage to identify and analyze student behavior. While these
video-based methods can partially capture and analyze student actions, they
struggle to accurately track each student's actions in physical education
classes, which take place in outdoor, open spaces with diverse activities, and
are challenging to generalize to the specialized technical movements involved
in these settings. Furthermore, current methods typically lack the ability to
integrate specialized pedagogical knowledge, limiting their ability to provide
in-depth insights into student behavior and offer feedback for optimizing
instructional design. To address these limitations, we propose a unified
end-to-end framework that leverages human activity recognition technologies
based on motion signals, combined with advanced large language models, to
conduct more detailed analyses and feedback of student behavior in physical
education classes. Our framework begins with the teacher's instructional
designs and the motion signals from students during physical education
sessions, ultimately generating automated reports with teaching insights and
suggestions for improving both learning and class instructions. This solution
provides a motion signal-based approach for analyzing student behavior and
optimizing instructional design tailored to physical education classes.
Experimental results demonstrate that our framework can accurately identify
student behaviors and produce meaningful pedagogical insights.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 09:04:36 GMT'}]",2025-03-11,"[['Gao', 'Xian', ''], ['Ruan', 'Jiacheng', ''], ['Gao', 'Jingsheng', ''], ['Xie', 'Mingye', ''], ['Zhang', 'Zongyun', ''], ['Liu', 'Ting', ''], ['Fu', 'Yuzhuo', '']]","[{'text': 'advanced large language models', 'label': 'Large Language Model'}]",Large Language Model,advanced large language models,0.920845627784729
2503.06534,Xingwei Tan,"Xingwei Tan, Chen Lyu, Hafiz Muhammad Umer, Sahrish Khan, Mahathi
  Parvatham, Lois Arthurs, Simon Cullen, Shelley Wilson, Arshad Jhumka,
  Gabriele Pergola","SafeSpeech: A Comprehensive and Interactive Tool for Analysing Sexist
  and Abusive Language in Conversations",NAACL 2025 system demonstration camera-ready,,,,cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Detecting toxic language including sexism, harassment and abusive behaviour,
remains a critical challenge, particularly in its subtle and context-dependent
forms. Existing approaches largely focus on isolated message-level
classification, overlooking toxicity that emerges across conversational
contexts. To promote and enable future research in this direction, we introduce
SafeSpeech, a comprehensive platform for toxic content detection and analysis
that bridges message-level and conversation-level insights. The platform
integrates fine-tuned classifiers and large language models (LLMs) to enable
multi-granularity detection, toxic-aware conversation summarization, and
persona profiling. SafeSpeech also incorporates explainability mechanisms, such
as perplexity gain analysis, to highlight the linguistic elements driving
predictions. Evaluations on benchmark datasets, including EDOS, OffensEval, and
HatEval, demonstrate the reproduction of state-of-the-art performance across
multiple tasks, including fine-grained sexism detection.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 09:31:17 GMT'}]",2025-03-11,"[['Tan', 'Xingwei', ''], ['Lyu', 'Chen', ''], ['Umer', 'Hafiz Muhammad', ''], ['Khan', 'Sahrish', ''], ['Parvatham', 'Mahathi', ''], ['Arthurs', 'Lois', ''], ['Cullen', 'Simon', ''], ['Wilson', 'Shelley', ''], ['Jhumka', 'Arshad', ''], ['Pergola', 'Gabriele', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'toxic-aware conversation summarization', 'label': 'Knowledge distillation'}]",Large Language Model,large language models,0.9664971828460693
2503.06550,Fan Yin,"Fan Yin and Philippe Laban and Xiangyu Peng and Yilun Zhou and Yixin
  Mao and Vaibhav Vats and Linnea Ross and Divyansh Agarwal and Caiming Xiong
  and Chien-Sheng Wu",BingoGuard: LLM Content Moderation Tools with Risk Levels,"10 pages, 4 figures, 4 tables. ICLR 2025 poster",,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Malicious content generated by large language models (LLMs) can pose varying
degrees of harm. Although existing LLM-based moderators can detect harmful
content, they struggle to assess risk levels and may miss lower-risk outputs.
Accurate risk assessment allows platforms with different safety thresholds to
tailor content filtering and rejection. In this paper, we introduce per-topic
severity rubrics for 11 harmful topics and build BingoGuard, an LLM-based
moderation system designed to predict both binary safety labels and severity
levels. To address the lack of annotations on levels of severity, we propose a
scalable generate-then-filter framework that first generates responses across
different severity levels and then filters out low-quality responses. Using
this framework, we create BingoGuardTrain, a training dataset with 54,897
examples covering a variety of topics, response severity, styles, and
BingoGuardTest, a test set with 988 examples explicitly labeled based on our
severity rubrics that enables fine-grained analysis on model behaviors on
different severity levels. Our BingoGuard-8B, trained on BingoGuardTrain,
achieves the state-of-the-art performance on several moderation benchmarks,
including WildGuardTest and HarmBench, as well as BingoGuardTest, outperforming
best public models, WildGuard, by 4.3\%. Our analysis demonstrates that
incorporating severity levels into training significantly enhances detection
performance and enables the model to effectively gauge the severity of harmful
responses.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 10:43:09 GMT'}]",2025-03-11,"[['Yin', 'Fan', ''], ['Laban', 'Philippe', ''], ['Peng', 'Xiangyu', ''], ['Zhou', 'Yilun', ''], ['Mao', 'Yixin', ''], ['Vats', 'Vaibhav', ''], ['Ross', 'Linnea', ''], ['Agarwal', 'Divyansh', ''], ['Xiong', 'Caiming', ''], ['Wu', 'Chien-Sheng', '']]","[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,large language models,0.9664971828460693
2503.06620,Xiangyu Zhang,"Xiangyu Zhang, Beena Ahmed, Julien Epps","Why Pre-trained Models Fail: Feature Entanglement in Multi-modal
  Depression Detection",,,,,eess.AS,http://creativecommons.org/licenses/by/4.0/,"  Depression remains a pressing global mental health issue, driving
considerable research into AI-driven detection approaches. While pre-trained
models, particularly speech self-supervised models (SSL Models), have been
applied to depression detection, they show unexpectedly poor performance
without extensive data augmentation. Large Language Models (LLMs), despite
their success across various domains, have not been explored in multi-modal
depression detection. In this paper, we first establish an LLM-based system to
investigate its potential in this task, uncovering fundamental limitations in
handling multi-modal information. Through systematic analysis, we discover that
the poor performance of pre-trained models stems from the conflation of
high-level information, where high-level features derived from both content and
speech are mixed within pre-trained models model representations, making it
challenging to establish effective decision boundaries. To address this, we
propose an information separation framework that disentangles these features,
significantly improving the performance of both SSL models and LLMs in
depression detection. Our experiments validate this finding and demonstrate
that the integration of separated features yields substantial improvements over
existing approaches, providing new insights for developing more effective
multi-modal depression detection systems.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 13:45:21 GMT'}]",2025-03-11,"[['Zhang', 'Xiangyu', ''], ['Ahmed', 'Beena', ''], ['Epps', 'Julien', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
2503.06626,Hasan Abed Al Kader Hammoud,"Hasan Abed Al Kader Hammoud, Bernard Ghanem",DiffCLIP: Differential Attention Meets CLIP,Under review,,,,cs.CV cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose DiffCLIP, a novel vision-language model that extends the
differential attention mechanism to CLIP architectures. Differential attention
was originally developed for large language models to amplify relevant context
while canceling out noisy information. In this work, we integrate this
mechanism into CLIP's dual encoder (image and text) framework. With minimal
additional parameters, DiffCLIP achieves superior performance on image-text
understanding tasks. Across zero-shot classification, retrieval, and robustness
benchmarks, DiffCLIP consistently outperforms baseline CLIP models. Notably,
these gains come with negligible computational overhead, demonstrating that
differential attention can significantly enhance multi-modal representations
without sacrificing efficiency. Code can be found at
https://github.com/hammoudhasan/DiffCLIP.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 14:04:09 GMT'}]",2025-03-11,"[['Hammoud', 'Hasan Abed Al Kader', ''], ['Ghanem', 'Bernard', '']]","[{'text': 'differential attention mechanism', 'label': 'Attention mechanism'}, {'text': 'Differential attention', 'label': 'Attention mechanism'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'zero-shot classification', 'label': 'Few-shot Learning'}, {'text': 'differential attention', 'label': 'Attention mechanism'}]",Large Language Model,large language models,0.9664971828460693
2503.06646,Jiaxin Liu,"Jiaxin Liu, Yi Yang, Kar Yan Tam",Evaluating and Aligning Human Economic Risk Preferences in LLMs,,,,,econ.GN cs.CL q-fin.EC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large Language Models (LLMs) are increasingly used in decision-making
scenarios that involve risk assessment, yet their alignment with human economic
rationality remains unclear. In this study, we investigate whether LLMs exhibit
risk preferences consistent with human expectations across different personas.
Specifically, we assess whether LLM-generated responses reflect appropriate
levels of risk aversion or risk-seeking behavior based on individual's persona.
Our results reveal that while LLMs make reasonable decisions in simplified,
personalized risk contexts, their performance declines in more complex economic
decision-making tasks. To address this, we propose an alignment method designed
to enhance LLM adherence to persona-specific risk preferences. Our approach
improves the economic rationality of LLMs in risk-related applications,
offering a step toward more human-aligned AI decision-making.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 14:47:31 GMT'}]",2025-03-11,"[['Liu', 'Jiaxin', ''], ['Yang', 'Yi', ''], ['Tam', 'Kar Yan', '']]","[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]",Large Language Model,Large Language Models,0.9664971828460693
