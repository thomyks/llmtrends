id,submitter,authors,title,comments,journal-ref,doi,report-no,categories,license,abstract,versions,update_date,authors_parsed,extracted_entities,assigned_concept,matched_keyword,similarity_score
2310.19380,Meng Lou,"Meng Lou, Shu Zhang, Hong-Yu Zhou, Chuan Wu, Sibei Yang, Yizhou Yu","TransXNet: Learning Both Global and Local Dynamics with a Dual Dynamic
  Token Mixer for Visual Recognition","Accepted by IEEE TNNLS. Code is available at
  https://github.com/LMMMEng/TransXNet",,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Recent studies have integrated convolutions into transformers to introduce
inductive bias and improve generalization performance. However, the static
nature of conventional convolution prevents it from dynamically adapting to
input variations, resulting in a representation discrepancy between convolution
and self-attention as self-attention calculates attention matrices dynamically.
Furthermore, when stacking token mixers that consist of convolution and
self-attention to form a deep network, the static nature of convolution hinders
the fusion of features previously generated by self-attention into convolution
kernels. These two limitations result in a sub-optimal representation capacity
of the constructed networks. To find a solution, we propose a lightweight Dual
Dynamic Token Mixer (D-Mixer) to simultaneously learn global and local
dynamics, that is, mechanisms that compute weights for aggregating global
contexts and local details in an input-dependent manner. D-Mixer works by
applying an efficient global attention module and an input-dependent depthwise
convolution separately on evenly split feature segments, endowing the network
with strong inductive bias and an enlarged effective receptive field. We use
D-Mixer as the basic building block to design TransXNet, a novel hybrid
CNN-Transformer vision backbone network that delivers compelling performance.
In the ImageNet-1K classification task, TransXNet-T surpasses Swin-T by 0.3% in
top-1 accuracy while requiring less than half of the computational cost.
Furthermore, TransXNet-S and TransXNet-B exhibit excellent model scalability,
achieving top-1 accuracy of 83.8% and 84.6% respectively, with reasonable
computational costs. Additionally, our proposed network architecture
demonstrates strong generalization capabilities in various dense prediction
tasks, outperforming other state-of-the-art networks while having lower
computational costs.
","[{'version': 'v1', 'created': 'Mon, 30 Oct 2023 09:35:56 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Nov 2023 01:48:03 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 09:09:26 GMT'}]",2025-03-12,"[['Lou', 'Meng', ''], ['Zhang', 'Shu', ''], ['Zhou', 'Hong-Yu', ''], ['Wu', 'Chuan', ''], ['Yang', 'Sibei', ''], ['Yu', 'Yizhou', '']]","[{'text': 'transformers', 'label': 'Transformers'}, {'text': 'inductive bias', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'global attention', 'label': 'Attention mechanism'}, {'text': 'inductive bias', 'label': 'Attention mechanism'}]",Transformers,transformers,1.0
2311.00226,Vishnu Teja Kunde,"Vishnu Teja Kunde, Vicram Rajagopalan, Chandra Shekhara Kaushik
  Valmeekam, Krishna Narayanan, Srinivas Shakkottai, Dileep Kalathil,
  Jean-Francois Chamberland","Transformers are Provably Optimal In-context Estimators for Wireless
  Communications",Accepted at AISTATS 2025,,,,eess.SP cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Pre-trained transformers exhibit the capability of adapting to new tasks
through in-context learning (ICL), where they efficiently utilize a limited set
of prompts without explicit model optimization. The canonical communication
problem of estimating transmitted symbols from received observations can be
modeled as an in-context learning problem: received observations are a noisy
function of transmitted symbols, and this function can be represented by an
unknown parameter whose statistics depend on an unknown latent context. This
problem, which we term in-context estimation (ICE), has significantly greater
complexity than the extensively studied linear regression problem. The optimal
solution to the ICE problem is a non-linear function of the underlying context.
In this paper, we prove that, for a subclass of such problems, a single-layer
softmax attention transformer (SAT) computes the optimal solution of the above
estimation problem in the limit of large prompt length. We also prove that the
optimal configuration of such a transformer is indeed the minimizer of the
corresponding training loss. Further, we empirically demonstrate the
proficiency of multi-layer transformers in efficiently solving broader
in-context estimation problems. Through extensive simulations, we show that
solving ICE problems using transformers significantly outperforms standard
approaches. Moreover, just with a few context examples, it achieves the same
performance as an estimator with perfect knowledge of the latent context. The
code is available
\href{https://github.com/vishnutez/in-context-estimation}{here}.
","[{'version': 'v1', 'created': 'Wed, 1 Nov 2023 02:16:24 GMT'}, {'version': 'v2', 'created': 'Sun, 3 Dec 2023 04:31:28 GMT'}, {'version': 'v3', 'created': 'Fri, 14 Jun 2024 18:05:14 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Mar 2025 16:24:05 GMT'}]",2025-03-12,"[['Kunde', 'Vishnu Teja', ''], ['Rajagopalan', 'Vicram', ''], ['Valmeekam', 'Chandra Shekhara Kaushik', ''], ['Narayanan', 'Krishna', ''], ['Shakkottai', 'Srinivas', ''], ['Kalathil', 'Dileep', ''], ['Chamberland', 'Jean-Francois', '']]","[{'text': 'Pre-trained transformers', 'label': 'Transformers'}, {'text': 'in-context learning', 'label': 'Few-shot Learning'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'in-context learning', 'label': 'Few-shot Learning'}, {'text': 'in-context estimation', 'label': 'Few-shot Learning'}, {'text': 'multi-layer transformers', 'label': 'Transformers'}, {'text': 'transformers', 'label': 'Transformers'}]",Transformers,transformers,1.0
2401.00740,Zeke Zexi Hu,"Zeke Zexi Hu, Xiaoming Chen, Vera Yuk Ying Chung, Yiran Shen","Beyond Subspace Isolation: Many-to-Many Transformer for Light Field
  Image Super-resolution",Accepted by IEEE Transactions on Multimedia,,10.1109/TMM.2024.3521795,,eess.IV cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The effective extraction of spatial-angular features plays a crucial role in
light field image super-resolution (LFSR) tasks, and the introduction of
convolution and Transformers leads to significant improvement in this area.
Nevertheless, due to the large 4D data volume of light field images, many
existing methods opted to decompose the data into a number of lower-dimensional
subspaces and perform Transformers in each sub-space individually. As a side
effect, these methods inadvertently restrict the self-attention mechanisms to a
One-to-One scheme accessing only a limited subset of LF data, explicitly
preventing comprehensive optimization on all spatial and angular cues. In this
paper, we identify this limitation as subspace isolation and introduce a novel
Many-to-Many Transformer (M2MT) to address it. M2MT aggregates angular
information in the spatial subspace before performing the self-attention
mechanism. It enables complete access to all information across all
sub-aperture images (SAIs) in a light field image. Consequently, M2MT is
enabled to comprehensively capture long-range correlation dependencies. With
M2MT as the pivotal component, we develop a simple yet effective M2MT network
for LFSR. Our experimental results demonstrate that M2MT achieves
state-of-the-art performance across various public datasets. We further conduct
in-depth analysis using local attribution maps (LAM) to obtain visual
interpretability, and the results validate that M2MT is empowered with a truly
non-local context in both spatial and angular subspaces to mitigate subspace
isolation and acquire effective spatial-angular representation.
","[{'version': 'v1', 'created': 'Mon, 1 Jan 2024 12:48:23 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 12:54:24 GMT'}]",2025-03-12,"[['Hu', 'Zeke Zexi', ''], ['Chen', 'Xiaoming', ''], ['Chung', 'Vera Yuk Ying', ''], ['Shen', 'Yiran', '']]","[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'self-attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'self-attention\nmechanism', 'label': 'Attention mechanism'}]",Transformers,Transformers,1.0
2402.09469,Zhenmei Shi,"Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Tianyi Zhou","Fourier Circuits in Neural Networks and Transformers: A Case Study of
  Modular Arithmetic with Multiple Inputs",AIStats 2025,,,,cs.LG stat.ML,http://creativecommons.org/licenses/by/4.0/,"  In the evolving landscape of machine learning, a pivotal challenge lies in
deciphering the internal representations harnessed by neural networks and
Transformers. Building on recent progress toward comprehending how networks
execute distinct target functions, our study embarks on an exploration of the
underlying reasons behind networks adopting specific computational strategies.
We direct our focus to the complex algebraic learning task of modular addition
involving $k$ inputs. Our research presents a thorough analytical
characterization of the features learned by stylized one-hidden layer neural
networks and one-layer Transformers in addressing this task. A cornerstone of
our theoretical framework is the elucidation of how the principle of margin
maximization shapes the features adopted by one-hidden layer neural networks.
Let $p$ denote the modulus, $D_p$ denote the dataset of modular arithmetic with
$k$ inputs and $m$ denote the network width. We demonstrate that a neuron count
of $ m \geq 2^{2k-2} \cdot (p-1) $, these networks attain a maximum $ L_{2,k+1}
$-margin on the dataset $ D_p $. Furthermore, we establish that each
hidden-layer neuron aligns with a specific Fourier spectrum, integral to
solving modular addition problems. By correlating our findings with the
empirical observations of similar studies, we contribute to a deeper
comprehension of the intrinsic computational mechanisms of neural networks.
Furthermore, we observe similar computational mechanisms in attention matrices
of one-layer Transformers. Our work stands as a significant stride in
unraveling their operation complexities, particularly in the realm of complex
algebraic tasks.
","[{'version': 'v1', 'created': 'Mon, 12 Feb 2024 05:52:06 GMT'}, {'version': 'v2', 'created': 'Fri, 24 May 2024 07:28:24 GMT'}, {'version': 'v3', 'created': 'Wed, 16 Oct 2024 06:48:42 GMT'}, {'version': 'v4', 'created': 'Sun, 9 Mar 2025 07:14:46 GMT'}]",2025-03-11,"[['Li', 'Chenyang', ''], ['Liang', 'Yingyu', ''], ['Shi', 'Zhenmei', ''], ['Song', 'Zhao', ''], ['Zhou', 'Tianyi', '']]","[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'one-layer Transformers', 'label': 'Transformers'}, {'text': 'attention matrices', 'label': 'Attention mechanism'}, {'text': 'one-layer Transformers', 'label': 'Transformers'}]",Transformers,Transformers,1.0
2404.02082,Dong Chen,"Chen Yang, Yangfan He, Aaron Xuxiang Tian, Dong Chen, Jianhui Wang,
  Tianyu Shi, Arsalan Heydarian, Pei Liu",WcDT: World-centric Diffusion Transformer for Traffic Scene Generation,"7 pages, 5 figures",ICRA 2025,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we introduce a novel approach for autonomous driving
trajectory generation by harnessing the complementary strengths of diffusion
probabilistic models (a.k.a., diffusion models) and transformers. Our proposed
framework, termed the ""World-Centric Diffusion Transformer""(WcDT), optimizes
the entire trajectory generation process, from feature extraction to model
inference. To enhance the scene diversity and stochasticity, the historical
trajectory data is first preprocessed into ""Agent Move Statement"" and encoded
into latent space using Denoising Diffusion Probabilistic Models (DDPM)
enhanced with Diffusion with Transformer (DiT) blocks. Then, the latent
features, historical trajectories, HD map features, and historical traffic
signal information are fused with various transformer-based encoders that are
used to enhance the interaction of agents with other elements in the traffic
scene. The encoded traffic scenes are then decoded by a trajectory decoder to
generate multimodal future trajectories. Comprehensive experimental results
show that the proposed approach exhibits superior performance in generating
both realistic and diverse trajectories, showing its potential for integration
into automatic driving simulation systems. Our code is available at
\url{https://github.com/yangchen1997/WcDT}.
","[{'version': 'v1', 'created': 'Tue, 2 Apr 2024 16:28:41 GMT'}, {'version': 'v2', 'created': 'Sat, 28 Sep 2024 20:12:51 GMT'}, {'version': 'v3', 'created': 'Fri, 4 Oct 2024 01:10:29 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Mar 2025 02:32:41 GMT'}]",2025-03-11,"[['Yang', 'Chen', ''], ['He', 'Yangfan', ''], ['Tian', 'Aaron Xuxiang', ''], ['Chen', 'Dong', ''], ['Wang', 'Jianhui', ''], ['Shi', 'Tianyu', ''], ['Heydarian', 'Arsalan', ''], ['Liu', 'Pei', '']]","[{'text': 'transformers', 'label': 'Transformers'}]",Transformers,transformers,1.0
2404.06564,Haoyang He,"Haoyang He, Yuhu Bai, Jiangning Zhang, Qingdong He, Hongxu Chen,
  Zhenye Gan, Chengjie Wang, Xiangtai Li, Guanzhong Tian, Lei Xie","MambaAD: Exploring State Space Models for Multi-class Unsupervised
  Anomaly Detection",NeurIPS'24,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advancements in anomaly detection have seen the efficacy of CNN- and
transformer-based approaches. However, CNNs struggle with long-range
dependencies, while transformers are burdened by quadratic computational
complexity. Mamba-based models, with their superior long-range modeling and
linear efficiency, have garnered substantial attention. This study pioneers the
application of Mamba to multi-class unsupervised anomaly detection, presenting
MambaAD, which consists of a pre-trained encoder and a Mamba decoder featuring
(Locality-Enhanced State Space) LSS modules at multi-scales. The proposed LSS
module, integrating parallel cascaded (Hybrid State Space) HSS blocks and
multi-kernel convolutions operations, effectively captures both long-range and
local information. The HSS block, utilizing (Hybrid Scanning) HS encoders,
encodes feature maps into five scanning methods and eight directions, thereby
strengthening global connections through the (State Space Model) SSM. The use
of Hilbert scanning and eight directions significantly improves feature
sequence modeling. Comprehensive experiments on six diverse anomaly detection
datasets and seven metrics demonstrate state-of-the-art performance,
substantiating the method's effectiveness. The code and models are available at
https://lewandofskee.github.io/projects/MambaAD.
","[{'version': 'v1', 'created': 'Tue, 9 Apr 2024 18:28:55 GMT'}, {'version': 'v2', 'created': 'Thu, 11 Apr 2024 16:06:39 GMT'}, {'version': 'v3', 'created': 'Sun, 14 Apr 2024 09:14:23 GMT'}, {'version': 'v4', 'created': 'Sun, 9 Mar 2025 15:56:38 GMT'}]",2025-03-11,"[['He', 'Haoyang', ''], ['Bai', 'Yuhu', ''], ['Zhang', 'Jiangning', ''], ['He', 'Qingdong', ''], ['Chen', 'Hongxu', ''], ['Gan', 'Zhenye', ''], ['Wang', 'Chengjie', ''], ['Li', 'Xiangtai', ''], ['Tian', 'Guanzhong', ''], ['Xie', 'Lei', '']]","[{'text': 'transformers', 'label': 'Transformers'}]",Transformers,transformers,1.0
2407.11496,Xinyi Wang,"Xinyi Wang, Angeliki Katsenou, and David Bull","ReLaX-VQA: Residual Fragment and Layer Stack Extraction for Enhancing
  Video Quality Assessment","10 pages, 3 figures",,,,eess.IV cs.CV cs.MM,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  With the rapid growth of User-Generated Content (UGC) exchanged between users
and sharing platforms, the need for video quality assessment in the wild is
increasingly evident. UGC is typically acquired using consumer devices and
undergoes multiple rounds of compression (transcoding) before reaching the end
user. Therefore, traditional quality metrics that employ the original content
as a reference are not suitable. In this paper, we propose ReLaX-VQA, a novel
No-Reference Video Quality Assessment (NR-VQA) model that aims to address the
challenges of evaluating the quality of diverse video content without reference
to the original uncompressed videos. ReLaX-VQA uses frame differences to select
spatio-temporal fragments intelligently together with different expressions of
spatial features associated with the sampled frames. These are then used to
better capture spatial and temporal variabilities in the quality of
neighbouring frames. Furthermore, the model enhances abstraction by employing
layer-stacking techniques in deep neural network features from Residual
Networks and Vision Transformers. Extensive testing across four UGC datasets
demonstrates that ReLaX-VQA consistently outperforms existing NR-VQA methods,
achieving an average SRCC of 0.8658 and PLCC of 0.8873. Open-source code and
trained models that will facilitate further research and applications of NR-VQA
can be found at https://github.com/xinyiW915/ReLaX-VQA.
","[{'version': 'v1', 'created': 'Tue, 16 Jul 2024 08:33:55 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 17:37:47 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 18:07:16 GMT'}]",2025-03-14,"[['Wang', 'Xinyi', ''], ['Katsenou', 'Angeliki', ''], ['Bull', 'David', '']]","[{'text': 'Vision Transformers', 'label': 'Transformers'}]",Transformers,Vision Transformers,0.7330732345581055
2408.07514,Andr\'as Kalapos,"Andr\'as Kalapos, B\'alint Gyires-T\'oth","CNN-JEPA: Self-Supervised Pretraining Convolutional Neural Networks
  Using Joint Embedding Predictive Architecture",Preprint,"2024 International Conference on Machine Learning and Applications
  (ICMLA), Miami, FL, USA, 2024, pp. 1111-1114",10.1109/ICMLA61862.2024.00169,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Self-supervised learning (SSL) has become an important approach in
pretraining large neural networks, enabling unprecedented scaling of model and
dataset sizes. While recent advances like I-JEPA have shown promising results
for Vision Transformers, adapting such methods to Convolutional Neural Networks
(CNNs) presents unique challenges. In this paper, we introduce CNN-JEPA, a
novel SSL method that successfully applies the joint embedding predictive
architecture approach to CNNs. Our method incorporates a sparse CNN encoder to
handle masked inputs, a fully convolutional predictor using depthwise separable
convolutions, and an improved masking strategy. We demonstrate that CNN-JEPA
outperforms I-JEPA with ViT architectures on ImageNet-100, achieving a 73.3%
linear top-1 accuracy using a standard ResNet-50 encoder. Compared to other
CNN-based SSL methods, CNN-JEPA requires 17-35% less training time for the same
number of epochs and approaches the linear and k-NN top-1 accuracies of BYOL,
SimCLR, and VICReg. Our approach offers a simpler, more efficient alternative
to existing SSL methods for CNNs, requiring minimal augmentations and no
separate projector network.
","[{'version': 'v1', 'created': 'Wed, 14 Aug 2024 12:48:37 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 09:42:28 GMT'}]",2025-03-12,"[['Kalapos', 'András', ''], ['Gyires-Tóth', 'Bálint', '']]","[{'text': 'Self-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'Vision Transformers', 'label': 'Transformers'}, {'text': 'SSL', 'label': 'Few-shot Learning'}, {'text': 'depthwise separable\nconvolutions', 'label': 'Embedding'}]",Transformers,Vision Transformers,0.7330732345581055
2408.15993,Sungduk Yu,"Sungduk Yu, Brian L. White, Anahita Bhiwandiwalla, Musashi Hinck,
  Matthew Lyle Olson, Yaniv Gurwicz, Raanan Y. Rohekar, Tung Nguyen, Vasudev
  Lal","ClimDetect: A Benchmark Dataset for Climate Change Detection and
  Attribution",,,,,cs.CV cs.LG physics.ao-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Detecting and attributing temperature increases driven by climate change is
crucial for understanding global warming and informing adaptation strategies.
However, distinguishing human-induced climate signals from natural variability
remains challenging for traditional detection and attribution (D&A) methods,
which rely on identifying specific ""fingerprints"" -- spatial patterns expected
to emerge from external forcings such as greenhouse gas emissions. Deep
learning offers promise in discerning these complex patterns within expansive
spatial datasets, yet the lack of standardized protocols has hindered
consistent comparisons across studies.
  To address this gap, we introduce ClimDetect, a standardized dataset
comprising 1.17M daily climate snapshots paired with target climate change
indicator variables. The dataset is curated from both CMIP6 climate model
simulations and real-world observation-assimilated reanalysis datasets (ERA5,
JRA-3Q, and MERRA-2), and is designed to enhance model accuracy in detecting
climate change signals. ClimDetect integrates various input and target
variables used in previous research, ensuring comparability and consistency
across studies. We also explore the application of vision transformers (ViT) to
climate data -- a novel approach that, to our knowledge, has not been attempted
before for climate change detection tasks. Our open-access data serve as a
benchmark for advancing climate science by enabling end-to-end model
development and evaluation. ClimDetect is publicly accessible via Hugging Face
dataset repository at: https://huggingface.co/datasets/ClimDetect/ClimDetect.
","[{'version': 'v1', 'created': 'Wed, 28 Aug 2024 17:58:53 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 20:45:11 GMT'}]",2025-03-12,"[['Yu', 'Sungduk', ''], ['White', 'Brian L.', ''], ['Bhiwandiwalla', 'Anahita', ''], ['Hinck', 'Musashi', ''], ['Olson', 'Matthew Lyle', ''], ['Gurwicz', 'Yaniv', ''], ['Rohekar', 'Raanan Y.', ''], ['Nguyen', 'Tung', ''], ['Lal', 'Vasudev', '']]","[{'text': 'Deep\nlearning', 'label': 'Open-source LLMs'}, {'text': 'vision transformers', 'label': 'Transformers'}]",Transformers,vision transformers,0.7330732345581055
2410.03522,Songsong Xiong,"Songsong Xiong, Hamidreza Kasaei","HMT-Grasp: A Hybrid Mamba-Transformer Approach for Robot Grasping in
  Cluttered Environments",,,,,cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Robot grasping, whether handling isolated objects, cluttered items, or
stacked objects, plays a critical role in industrial and service applications.
However, current visual grasp detection methods based on Convolutional Neural
Networks (CNNs) and Vision Transformers (ViTs) often struggle to adapt to
diverse scenarios, as they tend to emphasize either local or global features
exclusively, neglecting complementary cues. In this paper, we propose a novel
hybrid Mamba-Transformer approach to address these challenges. Our method
improves robotic visual grasping by effectively capturing both global and local
information through the integration of Vision Mamba and parallel
convolutional-transformer blocks. This hybrid architecture significantly
improves adaptability, precision, and flexibility across various robotic tasks.
To ensure a fair evaluation, we conducted extensive experiments on the Cornell,
Jacquard, and OCID-Grasp datasets, ranging from simple to complex scenarios.
Additionally, we performed both simulated and real-world robotic experiments.
The results demonstrate that our method not only surpasses state-of-the-art
techniques on standard grasping datasets but also delivers strong performance
in both simulation and real-world robot applications.
","[{'version': 'v1', 'created': 'Fri, 4 Oct 2024 15:43:01 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 17:26:26 GMT'}]",2025-03-11,"[['Xiong', 'Songsong', ''], ['Kasaei', 'Hamidreza', '']]","[{'text': 'Vision Transformers', 'label': 'Transformers'}]",Transformers,Vision Transformers,0.7330732345581055
2410.13981,Renpu Liu,"Renpu Liu, Ruida Zhou, Cong Shen, Jing Yang","On the Learn-to-Optimize Capabilities of Transformers in In-Context
  Sparse Recovery",,,,,cs.LG cs.AI,http://creativecommons.org/licenses/by/4.0/,"  An intriguing property of the Transformer is its ability to perform
in-context learning (ICL), where the Transformer can solve different inference
tasks without parameter updating based on the contextual information provided
by the corresponding input-output demonstration pairs. It has been
theoretically proved that ICL is enabled by the capability of Transformers to
perform gradient-descent algorithms (Von Oswald et al., 2023a; Bai et al.,
2024). This work takes a step further and shows that Transformers can perform
learning-to-optimize (L2O) algorithms. Specifically, for the ICL sparse
recovery (formulated as LASSO) tasks, we show that a K-layer Transformer can
perform an L2O algorithm with a provable convergence rate linear in K. This
provides a new perspective explaining the superior ICL capability of
Transformers, even with only a few layers, which cannot be achieved by the
standard gradient-descent algorithms. Moreover, unlike the conventional L2O
algorithms that require the measurement matrix involved in training to match
that in testing, the trained Transformer is able to solve sparse recovery
problems generated with different measurement matrices. Besides, Transformers
as an L2O algorithm can leverage structural information embedded in the
training tasks to accelerate its convergence during ICL, and generalize across
different lengths of demonstration pairs, where conventional L2O algorithms
typically struggle or fail. Such theoretical findings are supported by our
experimental results.
","[{'version': 'v1', 'created': 'Thu, 17 Oct 2024 19:18:28 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 05:09:21 GMT'}]",2025-03-13,"[['Liu', 'Renpu', ''], ['Zhou', 'Ruida', ''], ['Shen', 'Cong', ''], ['Yang', 'Jing', '']]","[{'text': 'in-context learning', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'ICL', 'label': 'contextual Embedding'}]",Transformers,Transformers,1.0
2411.06390,Yutong Chen,"Yutong Chen, Marko Mihajlovic, Xiyi Chen, Yiming Wang, Sergey Prokudin
  and Siyu Tang",SplatFormer: Point Transformer for Robust 3D Gaussian Splatting,ICLR 2025,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  3D Gaussian Splatting (3DGS) has recently transformed photorealistic
reconstruction, achieving high visual fidelity and real-time performance.
However, rendering quality significantly deteriorates when test views deviate
from the camera angles used during training, posing a major challenge for
applications in immersive free-viewpoint rendering and navigation. In this
work, we conduct a comprehensive evaluation of 3DGS and related novel view
synthesis methods under out-of-distribution (OOD) test camera scenarios. By
creating diverse test cases with synthetic and real-world datasets, we
demonstrate that most existing methods, including those incorporating various
regularization techniques and data-driven priors, struggle to generalize
effectively to OOD views. To address this limitation, we introduce SplatFormer,
the first point transformer model specifically designed to operate on Gaussian
splats. SplatFormer takes as input an initial 3DGS set optimized under limited
training views and refines it in a single forward pass, effectively removing
potential artifacts in OOD test views. To our knowledge, this is the first
successful application of point transformers directly on 3DGS sets, surpassing
the limitations of previous multi-scene training methods, which could handle
only a restricted number of input views during inference. Our model
significantly improves rendering quality under extreme novel views, achieving
state-of-the-art performance in these challenging scenarios and outperforming
various 3DGS regularization techniques, multi-scene models tailored for sparse
view synthesis, and diffusion-based frameworks.
","[{'version': 'v1', 'created': 'Sun, 10 Nov 2024 08:23:27 GMT'}, {'version': 'v2', 'created': 'Tue, 12 Nov 2024 06:41:21 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 08:37:42 GMT'}]",2025-03-11,"[['Chen', 'Yutong', ''], ['Mihajlovic', 'Marko', ''], ['Chen', 'Xiyi', ''], ['Wang', 'Yiming', ''], ['Prokudin', 'Sergey', ''], ['Tang', 'Siyu', '']]","[{'text': 'point transformers', 'label': 'Transformers'}]",Transformers,point transformers,0.7047149538993835
2411.15958,Enea Monzio Compagnoni Mr.,"Enea Monzio Compagnoni, Tianlin Liu, Rustem Islamov, Frank Norbert
  Proske, Antonio Orvieto, Aurelien Lucchi","Adaptive Methods through the Lens of SDEs: Theoretical Insights on the
  Role of Noise","Accepted at ICLR 2025 (Poster); An earlier version, titled 'SDEs for
  Adaptive Methods: The Role of Noise' and dated May 2024, is available on
  OpenReview",,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Despite the vast empirical evidence supporting the efficacy of adaptive
optimization methods in deep learning, their theoretical understanding is far
from complete. This work introduces novel SDEs for commonly used adaptive
optimizers: SignSGD, RMSprop(W), and Adam(W). These SDEs offer a quantitatively
accurate description of these optimizers and help illuminate an intricate
relationship between adaptivity, gradient noise, and curvature. Our novel
analysis of SignSGD highlights a noteworthy and precise contrast to SGD in
terms of convergence speed, stationary distribution, and robustness to
heavy-tail noise. We extend this analysis to AdamW and RMSpropW, for which we
observe that the role of noise is much more complex. Crucially, we support our
theoretical analysis with experimental evidence by verifying our insights: this
includes numerically integrating our SDEs using Euler-Maruyama discretization
on various neural network architectures such as MLPs, CNNs, ResNets, and
Transformers. Our SDEs accurately track the behavior of the respective
optimizers, especially when compared to previous SDEs derived for Adam and
RMSprop. We believe our approach can provide valuable insights into best
training practices and novel scaling rules.
","[{'version': 'v1', 'created': 'Sun, 24 Nov 2024 19:07:31 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 23:28:01 GMT'}]",2025-03-12,"[['Compagnoni', 'Enea Monzio', ''], ['Liu', 'Tianlin', ''], ['Islamov', 'Rustem', ''], ['Proske', 'Frank Norbert', ''], ['Orvieto', 'Antonio', ''], ['Lucchi', 'Aurelien', '']]","[{'text': 'SignSGD', 'label': 'BERT'}, {'text': 'RMSprop', 'label': 'BERT'}, {'text': 'Adam(W)', 'label': 'ALBERT'}, {'text': 'SignSGD', 'label': 'ALBERT'}, {'text': 'AdamW', 'label': 'BERT'}, {'text': 'RMSpropW', 'label': 'BERT'}, {'text': 'ResNets', 'label': 'Transformers'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Adam', 'label': 'BERT'}, {'text': 'RMSprop', 'label': 'BERT'}, {'text': 'novel scaling rules', 'label': 'Scaling law'}]",Transformers,Transformers,1.0
2412.00776,Chongyang Zhao,Chongyang Zhao and Dong Gong,"Learning Mamba as a Continual Learner: Meta-learning Selective State
  Space Models for Efficient Continual Learning",,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Continual learning (CL) aims to efficiently learn from a non-stationary data
stream, without storing or recomputing all seen samples. CL enables prediction
on new tasks by incorporating sequential training samples. Building on this
connection between CL and sequential modeling, meta-continual learning (MCL)
aims to meta-learn an efficient continual learner as a sequence prediction
model, with advanced sequence models like Transformers being natural choices.
However, despite decent performance, Transformers rely on a linearly growing
cache to store all past representations, conflicting with CL's objective of not
storing all seen samples and limiting efficiency. In this paper, we focus on
meta-learning sequence-prediction-based continual learners without retaining
all past representations. While attention-free models with fixed-size hidden
states (e.g., Linear Transformers) align with CL's essential goal and
efficiency needs, they have shown limited effectiveness in MCL in previous
literature. Given Mamba's strong sequence modeling performance and
attention-free nature, we explore a key question: Can attention-free models
like Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,
we propose MambaCL, a meta-learned continual learner. To enhance MambaCL's
training, we introduce selectivity regularization, leveraging the connection
between Mamba and Transformers to guide its behavior over sequences.
Furthermore, we study how Mamba and other models perform across various MCL
scenarios through extensive and well-designed experiments. Our results
highlight the promising performance and strong generalization of Mamba and
attention-free models in MCL, demonstrating its potential for efficient
continual learning and adaptation.
","[{'version': 'v1', 'created': 'Sun, 1 Dec 2024 11:43:46 GMT'}, {'version': 'v2', 'created': 'Mon, 3 Feb 2025 09:31:32 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 02:19:22 GMT'}]",2025-03-11,"[['Zhao', 'Chongyang', ''], ['Gong', 'Dong', '']]","[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Linear Transformers', 'label': 'Transformers'}, {'text': 'MCL', 'label': 'LLM'}, {'text': 'selectivity regularization', 'label': 'Fine-tuning'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'MCL', 'label': 'LLM'}]",Transformers,Transformers,1.0
2412.04532,Md Khairul Islam,"Md. Khairul Islam, Judy Fox","WinTSR: A Windowed Temporal Saliency Rescaling Method for Interpreting
  Time Series Deep Learning Models","11 pages, 14 figures, GitHub
  https://github.com/khairulislam/Timeseries-Explained",,,,cs.LG cs.AI,http://creativecommons.org/licenses/by-sa/4.0/,"  Interpreting complex time series forecasting models is challenging due to the
temporal dependencies between time steps and the dynamic relevance of input
features over time. Existing interpretation methods are limited by focusing
mostly on classification tasks, evaluating using custom baseline models instead
of the latest time series models, using simple synthetic datasets, and
requiring training another model. We introduce a novel interpretation method,
\textit{Windowed Temporal Saliency Rescaling (WinTSR)} addressing these
limitations. WinTSR explicitly captures temporal dependencies among the past
time steps and efficiently scales the feature importance with this time
importance. We benchmark WinTSR against 10 recent interpretation techniques
with 5 state-of-the-art deep-learning models of different architectures,
including a time series foundation model. We use 3 real-world datasets for both
time-series classification and regression. Our comprehensive analysis shows
that WinTSR significantly outperforms other local interpretation methods in
overall performance. Finally, we provide a novel, open-source framework to
interpret the latest time series transformers and foundation models.
","[{'version': 'v1', 'created': 'Thu, 5 Dec 2024 17:15:07 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 16:41:01 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 03:16:36 GMT'}]",2025-03-11,"[['Islam', 'Md. Khairul', ''], ['Fox', 'Judy', '']]","[{'text': 'time series foundation model', 'label': 'Foundation Model'}, {'text': 'open-source framework', 'label': 'Open-source LLMs'}, {'text': 'time series transformers', 'label': 'Transformers'}]",Transformers,time series transformers,0.7990843057632446
2412.07752,"Korbinian P\""oppel","Korbinian P\""oppel, Maximilian Beck, Sepp Hochreiter",FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware,,,,,cs.LG cs.AI,http://creativecommons.org/licenses/by/4.0/,"  While Transformers and other sequence-parallelizable neural network
architectures seem like the current state of the art in sequence modeling, they
specifically lack state-tracking capabilities. These are important for
time-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,
as well as modern variants like sLSTM do have these capabilities at the cost of
strictly sequential processing. While this is often seen as a strong
limitation, we show how fast these networks can get with our
hardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the
register level on modern GPUs. We extend traditional RNNs with a
parallelization variant that processes multiple RNNs of smaller hidden state in
parallel, similar to the head-wise processing in Transformers. To enable
flexibility on different GPU variants, we introduce a new optimization
framework for hardware-internal cache sizes, memory and compute handling. It
models the hardware in a setting using polyhedral-like constraints, including
the notion of divisibility. This speeds up the solution process in our
ConstrINT library for general integer constraint satisfaction problems (integer
CSPs). We show that our kernels can achieve 50x speed-ups over a vanilla
PyTorch implementation and allow 40x larger hidden sizes compared to our Triton
implementation. Our open-source kernels and the optimization library are
released here to boost research in the direction of state-tracking enabled RNNs
and sequence modeling: https://github.com/NX-AI/flashrnn
","[{'version': 'v1', 'created': 'Tue, 10 Dec 2024 18:50:37 GMT'}, {'version': 'v2', 'created': 'Mon, 13 Jan 2025 17:34:22 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 11:14:49 GMT'}]",2025-03-14,"[['Pöppel', 'Korbinian', ''], ['Beck', 'Maximilian', ''], ['Hochreiter', 'Sepp', '']]","[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'sLSTM', 'label': 'Transformers'}, {'text': 'RNNs', 'label': 'AI model'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'RNNs', 'label': 'AI model'}]",Transformers,Transformers,1.0
2501.01423,Jingfeng Yao,"Jingfeng Yao, Bin Yang and Xinggang Wang","Reconstruction vs. Generation: Taming Optimization Dilemma in Latent
  Diffusion Models","Models and codes are available at:
  https://github.com/hustvl/LightningDiT",,,,cs.CV cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Latent diffusion models with Transformer architectures excel at generating
high-fidelity images. However, recent studies reveal an optimization dilemma in
this two-stage design: while increasing the per-token feature dimension in
visual tokenizers improves reconstruction quality, it requires substantially
larger diffusion models and more training iterations to achieve comparable
generation performance. Consequently, existing systems often settle for
sub-optimal solutions, either producing visual artifacts due to information
loss within tokenizers or failing to converge fully due to expensive
computation costs. We argue that this dilemma stems from the inherent
difficulty in learning unconstrained high-dimensional latent spaces. To address
this, we propose aligning the latent space with pre-trained vision foundation
models when training the visual tokenizers. Our proposed VA-VAE (Vision
foundation model Aligned Variational AutoEncoder) significantly expands the
reconstruction-generation frontier of latent diffusion models, enabling faster
convergence of Diffusion Transformers (DiT) in high-dimensional latent spaces.
To exploit the full potential of VA-VAE, we build an enhanced DiT baseline with
improved training strategies and architecture designs, termed LightningDiT. The
integrated system achieves state-of-the-art (SOTA) performance on ImageNet
256x256 generation with an FID score of 1.35 while demonstrating remarkable
training efficiency by reaching an FID score of 2.11 in just 64
epochs--representing an over 21 times convergence speedup compared to the
original DiT. Models and codes are available at:
https://github.com/hustvl/LightningDiT.
","[{'version': 'v1', 'created': 'Thu, 2 Jan 2025 18:59:40 GMT'}, {'version': 'v2', 'created': 'Mon, 6 Jan 2025 15:28:11 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 11:43:42 GMT'}]",2025-03-11,"[['Yao', 'Jingfeng', ''], ['Yang', 'Bin', ''], ['Wang', 'Xinggang', '']]","[{'text': 'Diffusion Transformers', 'label': 'Transformers'}, {'text': 'LightningDiT', 'label': 'LLM-based'}]",Transformers,Diffusion Transformers,0.5920959711074829
2501.09096,Badhan Kumar Das,"Badhan Kumar Das, Gengyan Zhao, Han Liu, Thomas J. Re, Dorin
  Comaniciu, Eli Gibson, and Andreas Maier","Self Pre-training with Adaptive Mask Autoencoders for Variable-Contrast
  3D Medical Imaging","5 pages, ISBI 2025 accepted",,,,eess.IV cs.CV,http://creativecommons.org/licenses/by/4.0/,"  The Masked Autoencoder (MAE) has recently demonstrated effectiveness in
pre-training Vision Transformers (ViT) for analyzing natural images. By
reconstructing complete images from partially masked inputs, the ViT encoder
gathers contextual information to predict the missing regions. This capability
to aggregate context is especially important in medical imaging, where
anatomical structures are functionally and mechanically linked to surrounding
regions. However, current methods do not consider variations in the number of
input images, which is typically the case in real-world Magnetic Resonance (MR)
studies. To address this limitation, we propose a 3D Adaptive Masked
Autoencoders (AMAE) architecture that accommodates a variable number of 3D
input contrasts per subject. A magnetic resonance imaging (MRI) dataset of
45,364 subjects was used for pretraining and a subset of 1648 training, 193
validation and 215 test subjects were used for finetuning. The performance
demonstrates that self pre-training of this adaptive masked autoencoders can
enhance the infarct segmentation performance by 2.8%-3.7% for ViT-based
segmentation models.
","[{'version': 'v1', 'created': 'Wed, 15 Jan 2025 19:29:31 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 18:48:15 GMT'}]",2025-03-12,"[['Das', 'Badhan Kumar', ''], ['Zhao', 'Gengyan', ''], ['Liu', 'Han', ''], ['Re', 'Thomas J.', ''], ['Comaniciu', 'Dorin', ''], ['Gibson', 'Eli', ''], ['Maier', 'Andreas', '']]","[{'text': 'Vision Transformers', 'label': 'Transformers'}, {'text': 'finetuning', 'label': 'Fine-tuning'}]",Transformers,Vision Transformers,0.7330732345581055
2501.13353,Aman Urumbekov,"Aman Urumbekov, Zheng Chen","Contrast: A Hybrid Architecture of Transformers and State Space Models
  for Low-Level Vision","10 pages, 6 figures",,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Transformers have become increasingly popular for image super-resolution (SR)
tasks due to their strong global context modeling capabilities. However, their
quadratic computational complexity necessitates the use of window-based
attention mechanisms, which restricts the receptive field and limits effective
context expansion. Recently, the Mamba architecture has emerged as a promising
alternative with linear computational complexity, allowing it to avoid window
mechanisms and maintain a large receptive field. Nevertheless, Mamba faces
challenges in handling long-context dependencies when high pixel-level
precision is required, as in SR tasks. This is due to its hidden state
mechanism, which can compress and store a substantial amount of context but
only in an approximate manner, leading to inaccuracies that transformers do not
suffer from. In this paper, we propose \textbf{Contrast}, a hybrid SR model
that combines \textbf{Con}volutional, \textbf{Tra}nsformer, and \textbf{St}ate
Space components, effectively blending the strengths of transformers and Mamba
to address their individual limitations. By integrating transformer and state
space mechanisms, \textbf{Contrast} compensates for the shortcomings of each
approach, enhancing both global context modeling and pixel-level accuracy. We
demonstrate that combining these two architectures allows us to mitigate the
problems inherent in each, resulting in improved performance on image
super-resolution tasks.
","[{'version': 'v1', 'created': 'Thu, 23 Jan 2025 03:34:14 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 22:07:50 GMT'}]",2025-03-11,"[['Urumbekov', 'Aman', ''], ['Chen', 'Zheng', '']]","[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'window-based\nattention mechanisms', 'label': 'Attention mechanism'}, {'text': 'transformers', 'label': 'Transformers'}, {'text': 'transformers', 'label': 'Transformers'}]",Transformers,Transformers,1.0
2501.13484,Yuxuan Yue,"Zukang Xu, Yuxuan Yue, Xing Hu, Zhihang Yuan, Zixu Jiang, Zhixuan
  Chen, Jiangyong Yu, Chen Xu, Sifan Zhou, Dawei Yang","MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation
  Methods",,,,,cs.LG cs.AI cs.CL,http://creativecommons.org/licenses/by/4.0/,"  Mamba is an efficient sequence model that rivals Transformers and
demonstrates significant potential as a foundational architecture for various
tasks. Quantization is commonly used in neural networks to reduce model size
and computational latency. However, applying quantization to Mamba remains
underexplored, and existing quantization methods, which have been effective for
CNN and Transformer models, appear inadequate for Mamba models (e.g., Quarot
suffers a 21% accuracy drop on Vim-T$^\dagger$ even under W8A8). We have
pioneered the exploration of this issue and identified several key challenges.
First, significant outliers are present in gate projections, output
projections, and matrix multiplications. Second, Mamba's unique parallel scan
further amplifies these outliers, leading to uneven and heavy-tailed data
distributions. Third, even with the application of the Hadamard transform, the
variance across channels in weights and activations still remains inconsistent.
To these ends, we propose MambaQuant, a post-training quantization (PTQ)
framework consisting of: 1) Karhunen-Loeve Transformation (KLT) enhanced
rotation, rendering the rotation matrix adaptable to diverse channel
distributions. 2) Smooth-Fused rotation, which equalizes channel variances and
can merge additional parameters into model weights. Experiments show that
MambaQuant can quantize both weights and activations into 8-bit with less than
1% accuracy loss for Mamba-based vision and language tasks. To the best of our
knowledge, MambaQuant is the first comprehensive PTQ design for the Mamba
family, paving the way for further advancements in its application.
","[{'version': 'v1', 'created': 'Thu, 23 Jan 2025 08:57:33 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Feb 2025 08:05:38 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 06:49:47 GMT'}]",2025-03-12,"[['Xu', 'Zukang', ''], ['Yue', 'Yuxuan', ''], ['Hu', 'Xing', ''], ['Yuan', 'Zhihang', ''], ['Jiang', 'Zixu', ''], ['Chen', 'Zhixuan', ''], ['Yu', 'Jiangyong', ''], ['Xu', 'Chen', ''], ['Zhou', 'Sifan', ''], ['Yang', 'Dawei', '']]","[{'text': 'Mamba', 'label': 'Foundation Model'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Quantization', 'label': 'quantisation'}, {'text': 'quantization', 'label': 'quantisation'}, {'text': 'Mamba', 'label': 'Foundation Model'}, {'text': 'quantization', 'label': 'quantisation'}, {'text': 'Mamba', 'label': 'Foundation Model'}, {'text': 'Mamba', 'label': 'Foundation Model'}, {'text': 'MambaQuant', 'label': 'Foundation Model'}, {'text': 'quantization', 'label': 'quantisation'}, {'text': 'MambaQuant', 'label': 'Foundation Model'}, {'text': 'quantize', 'label': 'quantisation'}, {'text': 'MambaQuant', 'label': 'Foundation Model'}, {'text': 'Mamba', 'label': 'Foundation Model'}]",Transformers,Transformers,1.0
2501.19255,Mian Muhammad Naeem Abid,"Mian Muhammad Naeem Abid, Nancy Mehta, Zongwei Wu, Radu Timofte",ContextFormer: Redefining Efficiency in Semantic Segmentation,,,,,cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Semantic segmentation assigns labels to pixels in images, a critical yet
challenging task in computer vision. Convolutional methods, although capturing
local dependencies well, struggle with long-range relationships. Vision
Transformers (ViTs) excel in global context capture but are hindered by high
computational demands, especially for high-resolution inputs. Most research
optimizes the encoder architecture, leaving the bottleneck underexplored - a
key area for enhancing performance and efficiency. We propose ContextFormer, a
hybrid framework leveraging the strengths of CNNs and ViTs in the bottleneck to
balance efficiency, accuracy, and robustness for real-time semantic
segmentation. The framework's efficiency is driven by three synergistic
modules: the Token Pyramid Extraction Module (TPEM) for hierarchical
multi-scale representation, the Transformer and Branched DepthwiseConv
(Trans-BDC) block for dynamic scale-aware feature modeling, and the Feature
Merging Module (FMM) for robust integration with enhanced spatial and
contextual consistency. Extensive experiments on ADE20K, Pascal Context,
CityScapes, and COCO-Stuff datasets show ContextFormer significantly
outperforms existing models, achieving state-of-the-art mIoU scores, setting a
new benchmark for efficiency and performance. The codes will be made publicly
available upon acceptance.
","[{'version': 'v1', 'created': 'Fri, 31 Jan 2025 16:11:04 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 14:00:08 GMT'}]",2025-03-11,"[['Abid', 'Mian Muhammad Naeem', ''], ['Mehta', 'Nancy', ''], ['Wu', 'Zongwei', ''], ['Timofte', 'Radu', '']]","[{'text': 'Vision\nTransformers', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ADE20K', 'label': 'Large Language Model'}, {'text': 'Pascal Context', 'label': 'Large Language Model'}]",Transformers,"Vision
Transformers",0.7330732345581055
2502.18913,Jiaming Zhou,"Jiaming Zhou, Yujie Guo, Shiwan Zhao, Haoqin Sun, Hui Wang, Jiabei He,
  Aobo Kong, Shiyao Wang, Xi Yang, Yequan Wang, Yonghua Lin, Yong Qin","CS-Dialogue: A 104-Hour Dataset of Spontaneous Mandarin-English
  Code-Switching Dialogues for Speech Recognition",,,,,cs.CL cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Code-switching (CS), the alternation between two or more languages within a
single conversation, presents significant challenges for automatic speech
recognition (ASR) systems. Existing Mandarin-English code-switching datasets
often suffer from limitations in size, spontaneity, and the lack of full-length
dialogue recordings with transcriptions, hindering the development of robust
ASR models for real-world conversational scenarios. This paper introduces
CS-Dialogue, a novel large-scale Mandarin-English code-switching speech dataset
comprising 104 hours of spontaneous conversations from 200 speakers. Unlike
previous datasets, CS-Dialogue provides full-length dialogue recordings with
complete transcriptions, capturing naturalistic code-switching patterns in
continuous speech. We describe the data collection and annotation processes,
present detailed statistics of the dataset, and establish benchmark ASR
performance using state-of-the-art models. Our experiments, using Transformer,
Conformer, and Branchformer, demonstrate the challenges of code-switching ASR,
and show that existing pre-trained models such as Whisper still have the space
to improve. The CS-Dialogue dataset will be made freely available for all
academic purposes.
","[{'version': 'v1', 'created': 'Wed, 26 Feb 2025 07:59:55 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 03:06:01 GMT'}]",2025-03-13,"[['Zhou', 'Jiaming', ''], ['Guo', 'Yujie', ''], ['Zhao', 'Shiwan', ''], ['Sun', 'Haoqin', ''], ['Wang', 'Hui', ''], ['He', 'Jiabei', ''], ['Kong', 'Aobo', ''], ['Wang', 'Shiyao', ''], ['Yang', 'Xi', ''], ['Wang', 'Yequan', ''], ['Lin', 'Yonghua', ''], ['Qin', 'Yong', '']]","[{'text': 'CS-Dialogue', 'label': 'Large Language Model'}, {'text': 'CS-Dialogue', 'label': 'Large Language Model'}, {'text': 'Transformer', 'label': 'Transformers'}, {'text': 'Conformer', 'label': 'Transformers'}, {'text': 'CS-Dialogue', 'label': 'Large Language Model'}]",Transformers,Transformer,0.781291127204895
2503.06368,Leonardo Scabini,"Leonardo Scabini, Kallil M. Zielinski, Emir Konuk, Ricardo T. Fares,
  Lucas C. Ribas, Kevin Smith, and Odemir M. Bruno","VORTEX: Challenging CNNs at Texture Recognition by using Vision
  Transformers with Orderless and Randomized Token Encodings",,,,,cs.CV cs.AI cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Texture recognition has recently been dominated by ImageNet-pre-trained deep
Convolutional Neural Networks (CNNs), with specialized modifications and
feature engineering required to achieve state-of-the-art (SOTA) performance.
However, although Vision Transformers (ViTs) were introduced a few years ago,
little is known about their texture recognition ability. Therefore, in this
work, we introduce VORTEX (ViTs with Orderless and Randomized Token Encodings
for Texture Recognition), a novel method that enables the effective use of ViTs
for texture analysis. VORTEX extracts multi-depth token embeddings from
pre-trained ViT backbones and employs a lightweight module to aggregate
hierarchical features and perform orderless encoding, obtaining a better image
representation for texture recognition tasks. This approach allows seamless
integration with any ViT with the common transformer architecture. Moreover, no
fine-tuning of the backbone is performed, since they are used only as frozen
feature extractors, and the features are fed to a linear SVM. We evaluate
VORTEX on nine diverse texture datasets, demonstrating its ability to achieve
or surpass SOTA performance in a variety of texture analysis scenarios. By
bridging the gap between texture recognition with CNNs and transformer-based
architectures, VORTEX paves the way for adopting emerging transformer
foundation models. Furthermore, VORTEX demonstrates robust computational
efficiency when coupled with ViT backbones compared to CNNs with similar costs.
The method implementation and experimental scripts are publicly available in
our online repository.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 00:36:02 GMT'}]",2025-03-11,"[['Scabini', 'Leonardo', ''], ['Zielinski', 'Kallil M.', ''], ['Konuk', 'Emir', ''], ['Fares', 'Ricardo T.', ''], ['Ribas', 'Lucas C.', ''], ['Smith', 'Kevin', ''], ['Bruno', 'Odemir M.', '']]","[{'text': 'Vision Transformers', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'multi-depth token embeddings', 'label': 'Embedding'}, {'text': 'ViT', 'label': 'Transformers'}, {'text': 'ViT', 'label': 'Transformers'}]",Transformers,Vision Transformers,0.7330732345581055
2503.06369,Sahar Dastani,"Sahar Dastani, Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, David
  Osowiechi, Gustavo Adolfo Vargas Hakim, Farzad Beizaee, Milad
  Cheraghalikhani, Arnab Kumar Mondal, Herve Lombaert, Christian Desrosiers","Spectral State Space Model for Rotation-Invariant Visual Representation
  Learning",,,,,cs.CV,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  State Space Models (SSMs) have recently emerged as an alternative to Vision
Transformers (ViTs) due to their unique ability of modeling global
relationships with linear complexity. SSMs are specifically designed to capture
spatially proximate relationships of image patches. However, they fail to
identify relationships between conceptually related yet not adjacent patches.
This limitation arises from the non-causal nature of image data, which lacks
inherent directional relationships. Additionally, current vision-based SSMs are
highly sensitive to transformations such as rotation. Their predefined scanning
directions depend on the original image orientation, which can cause the model
to produce inconsistent patch-processing sequences after rotation. To address
these limitations, we introduce Spectral VMamba, a novel approach that
effectively captures the global structure within an image by leveraging
spectral information derived from the graph Laplacian of image patches. Through
spectral decomposition, our approach encodes patch relationships independently
of image orientation, achieving rotation invariance with the aid of our
Rotational Feature Normalizer (RFN) module. Our experiments on classification
tasks show that Spectral VMamba outperforms the leading SSM models in vision,
such as VMamba, while maintaining invariance to rotations and a providing a
similar runtime efficiency.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 00:37:43 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 02:10:35 GMT'}]",2025-03-14,"[['Dastani', 'Sahar', ''], ['Bahri', 'Ali', ''], ['Yazdanpanah', 'Moslem', ''], ['Noori', 'Mehrdad', ''], ['Osowiechi', 'David', ''], ['Hakim', 'Gustavo Adolfo Vargas', ''], ['Beizaee', 'Farzad', ''], ['Cheraghalikhani', 'Milad', ''], ['Mondal', 'Arnab Kumar', ''], ['Lombaert', 'Herve', ''], ['Desrosiers', 'Christian', '']]","[{'text': 'Vision\nTransformers', 'label': 'Transformers'}]",Transformers,"Vision
Transformers",0.7330732345581055
2503.06537,Xiaoyang Liu,"Xiaoyang Liu, Yuquan Wang, Zheng Chen, Jiezhang Cao, He Zhang, Yulun
  Zhang and Xiaokang Yang",One-Step Diffusion Model for Image Motion-Deblurring,,,,,cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Currently, methods for single-image deblurring based on CNNs and transformers
have demonstrated promising performance. However, these methods often suffer
from perceptual limitations, poor generalization ability, and struggle with
heavy or complex blur. While diffusion-based methods can partially address
these shortcomings, their multi-step denoising process limits their practical
usage. In this paper, we conduct an in-depth exploration of diffusion models in
deblurring and propose a one-step diffusion model for deblurring (OSDD), a
novel framework that reduces the denoising process to a single step,
significantly improving inference efficiency while maintaining high fidelity.
To tackle fidelity loss in diffusion models, we introduce an enhanced
variational autoencoder (eVAE), which improves structural restoration.
Additionally, we construct a high-quality synthetic deblurring dataset to
mitigate perceptual collapse and design a dynamic dual-adapter (DDA) to enhance
perceptual quality while preserving fidelity. Extensive experiments demonstrate
that our method achieves strong performance on both full and no-reference
metrics. Our code and pre-trained model will be publicly available at
https://github.com/xyLiu339/OSDD.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 09:39:57 GMT'}]",2025-03-11,"[['Liu', 'Xiaoyang', ''], ['Wang', 'Yuquan', ''], ['Chen', 'Zheng', ''], ['Cao', 'Jiezhang', ''], ['Zhang', 'He', ''], ['Zhang', 'Yulun', ''], ['Yang', 'Xiaokang', '']]","[{'text': 'transformers', 'label': 'Transformers'}, {'text': 'OSDD', 'label': 'Transformers'}]",Transformers,transformers,1.0
2503.06625,Chaocan Xue,"Chaocan Xue, Bineng Zhong, Qihua Liang, Yaozong Zheng, Ning Li,
  Yuanliang Xue, Shuxiang Song",Similarity-Guided Layer-Adaptive Vision Transformer for UAV Tracking,,,,,cs.CV,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Vision transformers (ViTs) have emerged as a popular backbone for visual
tracking. However, complete ViT architectures are too cumbersome to deploy for
unmanned aerial vehicle (UAV) tracking which extremely emphasizes efficiency.
In this study, we discover that many layers within lightweight ViT-based
trackers tend to learn relatively redundant and repetitive target
representations. Based on this observation, we propose a similarity-guided
layer adaptation approach to optimize the structure of ViTs. Our approach
dynamically disables a large number of representation-similar layers and
selectively retains only a single optimal layer among them, aiming to achieve a
better accuracy-speed trade-off. By incorporating this approach into existing
ViTs, we tailor previously complete ViT architectures into an efficient
similarity-guided layer-adaptive framework, namely SGLATrack, for real-time UAV
tracking. Extensive experiments on six tracking benchmarks verify the
effectiveness of the proposed approach, and show that our SGLATrack achieves a
state-of-the-art real-time speed while maintaining competitive tracking
precision. Codes and models are available at
https://github.com/GXNU-ZhongLab/SGLATrack.
","[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 14:02:30 GMT'}]",2025-03-11,"[['Xue', 'Chaocan', ''], ['Zhong', 'Bineng', ''], ['Liang', 'Qihua', ''], ['Zheng', 'Yaozong', ''], ['Li', 'Ning', ''], ['Xue', 'Yuanliang', ''], ['Song', 'Shuxiang', '']]","[{'text': 'Vision transformers', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ViT', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ViT', 'label': 'Transformers'}]",Transformers,Vision transformers,0.7330732345581055
