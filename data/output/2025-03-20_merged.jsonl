{"id":2503.10635,"submitter":"Zhiqiang Shen","authors":"Zhaoyi Li and Xiaohan Zhao and Dong-Dong Wu and Jiacheng Cui and\n  Zhiqiang Shen","title":"A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90%\n  Success Rate Against the Strong Black-box Models of GPT-4.5\/4o\/o1","comments":"Code at: https:\/\/github.com\/VILA-Lab\/M-Attack","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Despite promising performance on open-source large vision-language models\n(LVLMs), transfer-based targeted attacks often fail against black-box\ncommercial LVLMs. Analyzing failed adversarial perturbations reveals that the\nlearned perturbations typically originate from a uniform distribution and lack\nclear semantic details, resulting in unintended responses. This critical\nabsence of semantic information leads commercial LVLMs to either ignore the\nperturbation entirely or misinterpret its embedded semantics, thereby causing\nthe attack to fail. To overcome these issues, we notice that identifying core\nsemantic objects is a key objective for models trained with various datasets\nand methodologies. This insight motivates our approach that refines semantic\nclarity by encoding explicit semantic details within local regions, thus\nensuring interoperability and capturing finer-grained features, and by\nconcentrating modifications on semantically rich areas rather than applying\nthem uniformly. To achieve this, we propose a simple yet highly effective\nsolution: at each optimization step, the adversarial image is cropped randomly\nby a controlled aspect ratio and scale, resized, and then aligned with the\ntarget image in the embedding space. Experimental results confirm our\nhypothesis. Our adversarial examples crafted with local-aggregated\nperturbations focused on crucial regions exhibit surprisingly good\ntransferability to commercial LVLMs, including GPT-4.5, GPT-4o,\nGemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning\nmodels like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach\nachieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly\noutperforming all prior state-of-the-art attack methods. Our optimized\nadversarial examples under different configurations and training code are\navailable at https:\/\/github.com\/VILA-Lab\/M-Attack.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:59:55 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Zhaoyi', ''], ['Zhao', 'Xiaohan', ''], ['Wu', 'Dong-Dong', ''], ['Cui', 'Jiacheng', ''], ['Shen', 'Zhiqiang', '']]","extracted_entities":"[{'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'embedding space', 'label': 'Embedding'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'GPT-4.5', 'label': 'GPT-4'}, {'text': 'GPT-4o', 'label': 'GPT-4'}, {'text': 'Gemini-2.0-flash', 'label': 'GPT-2'}, {'text': 'Claude-3.5-sonnet', 'label': 'GPT-4'}, {'text': 'Claude-3.7-sonnet', 'label': 'GPT-2'}, {'text': 'Claude-3.7-thinking', 'label': 'GPT-2'}, {'text': 'GPT-4.5', 'label': 'GPT'}]","assigned_concept":"GPT-4","matched_keyword":"GPT-4.5","similarity_score":0.9148372412}
{"id":2503.09956,"submitter":"Loc Nguyen","authors":"Yu Qiao, Phuong-Nam Tran, Ji Su Yoon, Loc X. Nguyen, and Choong Seon\n  Hong","title":"Exploring Mutual Empowerment Between Wireless Networks and RL-based\n  LLMs: A Survey","comments":"25 pages, 13 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV cs.ET","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Reinforcement learning (RL)-based large language models (LLMs), such as\nChatGPT, DeepSeek, and Grok-3, have gained significant attention for their\nexceptional capabilities in natural language processing and multimodal data\nunderstanding. Meanwhile, the rapid expansion of information services has\ndriven the growing need for intelligence, efficient, and adaptable wireless\nnetworks. Wireless networks require the empowerment of RL-based LLMs while\nthese models also benefit from wireless networks to broaden their application\nscenarios. Specifically, RL-based LLMs can enhance wireless communication\nsystems through intelligent resource allocation, adaptive network optimization,\nand real-time decision-making. Conversely, wireless networks provide a vital\ninfrastructure for the efficient training, deployment, and distributed\ninference of RL-based LLMs, especially in decentralized and edge computing\nenvironments. This mutual empowerment highlights the need for a deeper\nexploration of the interplay between these two domains. We first review recent\nadvancements in wireless communications, highlighting the associated challenges\nand potential solutions. We then discuss the progress of RL-based LLMs,\nfocusing on key technologies for LLM training, challenges, and potential\nsolutions. Subsequently, we explore the mutual empowerment between these two\nfields, highlighting key motivations, open challenges, and potential solutions.\nFinally, we provide insights into future directions, applications, and their\nsocietal impact to further explore this intersection, paving the way for\nnext-generation intelligent communication systems. Overall, this survey\nprovides a comprehensive overview of the relationship between RL-based LLMs and\nwireless networks, offering a vision where these domains empower each other to\ndrive innovations.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 01:59:11 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Qiao', 'Yu', ''], ['Tran', 'Phuong-Nam', ''], ['Yoon', 'Ji Su', ''], ['Nguyen', 'Loc X.', ''], ['Hong', 'Choong Seon', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'DeepSeek', 'label': 'ChatGPT'}, {'text': 'Grok-3', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2503.10306,"submitter":"Tolgahan Bardakci","authors":"Tolgahan Bardakci, Serge Demeyer, Mutlu Beyazit","title":"Test Amplification for REST APIs Using \"Out-of-the-box\" Large Language\n  Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  REST APIs are an indispensable building block in today's cloud-native\napplications, so testing them is critically important. However, writing\nautomated tests for such REST APIs is challenging because one needs strong and\nreadable tests that exercise the boundary values of the protocol embedded in\nthe REST API. In this paper, we report our experience with using \"out of the\nbox\" large language models (ChatGPT and GitHub's Copilot) to amplify REST API\ntest suites. We compare the resulting tests based on coverage and\nunderstandability, and we derive a series of guidelines and lessons learned\nconcerning the prompts that result in the strongest test suite.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 12:30:14 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Bardakci', 'Tolgahan', ''], ['Demeyer', 'Serge', ''], ['Beyazit', 'Mutlu', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'GitHub', 'label': 'Open-source LLMs'}, {'text': 'prompts', 'label': 'Prompting'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2503.10556,"submitter":"Jaan Aru","authors":"Brett Puppart and Jaan Aru","title":"Short-term AI literacy intervention does not reduce over-reliance on\n  incorrect ChatGPT recommendations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY q-bio.NC","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  In this study, we examined whether a short-form AI literacy intervention\ncould reduce the adoption of incorrect recommendations from large language\nmodels. High school seniors were randomly assigned to either a control or an\nintervention group, which received an educational text explaining ChatGPT's\nworking mechanism, limitations, and proper use. Participants solved math\npuzzles with the help of ChatGPT's recommendations, which were incorrect in\nhalf of the cases. Results showed that students adopted incorrect suggestions\n52.1% of the time, indicating widespread over-reliance. The educational\nintervention did not significantly reduce over-reliance. Instead, it led to an\nincrease in ignoring ChatGPT's correct recommendations. We conclude that the\nusage of ChatGPT is associated with over-reliance and it is not trivial to\nincrease AI literacy to counter over-reliance.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:10:33 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Puppart', 'Brett', ''], ['Aru', 'Jaan', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2012.09766,"submitter":"Sofian Chaybouti","authors":"Sofian Chaybouti, Achraf Saghe, Aymen Shabou","title":"MIX : a Multi-task Learning Approach to Solve Open-Domain Question\n  Answering","comments":"8 pages, 7 figures, 3 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper introduces MIX, a multi-task deep learning approach to solve\nopen-ended question-answering. First, we design our system as a multi-stage\npipeline of 3 building blocks: a BM25-based Retriever to reduce the search\nspace, a RoBERTa-based Scorer, and an Extractor to rank retrieved paragraphs\nand extract relevant text spans, respectively. Eventually, we further improve\nthe computational efficiency of our system to deal with the scalability\nchallenge: thanks to multi-task learning, we parallelize the close tasks solved\nby the Scorer and the Extractor. Our system is on par with state-of-the-art\nperformances on the squad-open benchmark while being simpler conceptually.\n","versions":"[{'version': 'v1', 'created': 'Thu, 17 Dec 2020 17:22:30 GMT'}, {'version': 'v2', 'created': 'Fri, 29 Jan 2021 20:06:03 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 13:56:45 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chaybouti', 'Sofian', ''], ['Saghe', 'Achraf', ''], ['Shabou', 'Aymen', '']]","extracted_entities":"[{'text': 'MIX', 'label': 'Few-shot Learning'}, {'text': 'RoBERTa-based', 'label': 'RoBERTa'}, {'text': 'multi-task learning', 'label': 'Few-shot Learning'}]","assigned_concept":"RoBERTa","matched_keyword":"RoBERTa-based","similarity_score":0.8695192933}
{"id":2306.0821,"submitter":"Shuyi Chen","authors":"Shuyi Chen, Kaize Ding, Shixiang Zhu","title":"Uncertainty-Aware Robust Learning on Noisy Graphs","comments":"ICASSP 2025 camera ready","journal-ref":"ICASSP 2025 - IEEE International Conference on Acoustics, Speech,\n  and Signal Processing","doi":"10.1109\/ICASSP49660.2025.10888672","report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Graph neural networks (GNNs) have excelled in various graph learning tasks,\nparticularly node classification. However, their performance is often hampered\nby noisy measurements in real-world graphs, which can corrupt critical patterns\nin the data. To address this, we propose a novel uncertainty-aware graph\nlearning framework inspired by distributionally robust optimization.\nSpecifically, we use a graph neural network-based encoder to embed the node\nfeatures and find the optimal node embeddings by minimizing the worst-case risk\nthrough a minimax formulation. Such an uncertainty-aware learning process leads\nto improved node representations and a more robust graph predictive model that\neffectively mitigates the impact of uncertainty arising from data noise. Our\nexperimental results demonstrate superior predictive performance over baselines\nacross noisy scenarios.\n","versions":"[{'version': 'v1', 'created': 'Wed, 14 Jun 2023 02:45:14 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 14:30:06 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chen', 'Shuyi', ''], ['Ding', 'Kaize', ''], ['Zhu', 'Shixiang', '']]","extracted_entities":"[{'text': 'distributionally robust optimization', 'label': 'Fine-tuning'}, {'text': 'node embeddings', 'label': 'Embedding'}, {'text': 'minimax formulation', 'label': 'Fine-tuning'}]","assigned_concept":"Embedding","matched_keyword":"node embeddings","similarity_score":0.7718001008}
{"id":2308.00137,"submitter":"Hemn Abdalla","authors":"Hemn Barzan Abdalla, Awder Ahmed, Bahtiyar Mehmed, Mehdi Gheisari,\n  Maryam Cheraghy, Yang Liu","title":"An Efficient Recommendation System in E-commerce using Passer learning\n  optimization based on Bi-LSTM","comments":"22 pages, 5 figuers, 4 Tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.MM cs.NE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Online reviews play a crucial role in shaping consumer decisions, especially\nin the context of e-commerce. However, the quality and reliability of these\nreviews can vary significantly. Some reviews contain misleading or unhelpful\ninformation, such as advertisements, fake content, or irrelevant details. These\nissues pose significant challenges for recommendation systems, which rely on\nuser-generated reviews to provide personalized suggestions. This article\nintroduces a recommendation system based on Passer Learning\nOptimization-enhanced Bi-LSTM classifier applicable to e-commerce\nrecommendation systems with improved accuracy and efficiency compared to\nstate-of-the-art models. It achieves as low as 1.24% MSE on the baby dataset.\nThis lifts it as high as 88.58%. Besides, there is also robust performance of\nthe system on digital music and patio lawn garden datasets at F1 of 88.46% and\n92.51%, correspondingly. These results, made possible by advanced graph\nembedding for effective knowledge extraction and fine-tuning of classifier\nparameters, establish the suitability of the proposed model in various\ne-commerce environments.\n","versions":"[{'version': 'v1', 'created': 'Mon, 31 Jul 2023 20:09:25 GMT'}, {'version': 'v2', 'created': 'Wed, 2 Aug 2023 07:34:05 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 14:43:36 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Abdalla', 'Hemn Barzan', ''], ['Ahmed', 'Awder', ''], ['Mehmed', 'Bahtiyar', ''], ['Gheisari', 'Mehdi', ''], ['Cheraghy', 'Maryam', ''], ['Liu', 'Yang', '']]","extracted_entities":"[{'text': 'advanced graph\\nembedding', 'label': 'Embedding'}, {'text': 'fine-tuning of classifier\\nparameters', 'label': 'Fine-tuning'}]","assigned_concept":"Embedding","matched_keyword":"advanced graph\nembedding","similarity_score":0.6591576338}
{"id":2405.10075,"submitter":"Kun Yuan","authors":"Kun Yuan, Vinkle Srivastav, Nassir Navab, Nicolas Padoy","title":"HecVL: Hierarchical Video-Language Pretraining for Zero-shot Surgical\n  Phase Recognition","comments":"Accepted by MICCAI2024","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Natural language could play an important role in developing generalist\nsurgical models by providing a broad source of supervision from raw texts. This\nflexible form of supervision can enable the model's transferability across\ndatasets and tasks as natural language can be used to reference learned visual\nconcepts or describe new ones. In this work, we present HecVL, a novel\nhierarchical video-language pretraining approach for building a generalist\nsurgical model. Specifically, we construct a hierarchical video-text paired\ndataset by pairing the surgical lecture video with three hierarchical levels of\ntexts: at clip-level, atomic actions using transcribed audio texts; at\nphase-level, conceptual text summaries; and at video-level, overall abstract\ntext of the surgical procedure. Then, we propose a novel fine-to-coarse\ncontrastive learning framework that learns separate embedding spaces for the\nthree video-text hierarchies using a single model. By disentangling embedding\nspaces of different hierarchical levels, the learned multi-modal\nrepresentations encode short-term and long-term surgical concepts in the same\nmodel. Thanks to the injected textual semantics, we demonstrate that the HecVL\napproach can enable zero-shot surgical phase recognition without any human\nannotation. Furthermore, we show that the same HecVL model for surgical phase\nrecognition can be transferred across different surgical procedures and medical\ncenters. The code is available at https:\/\/github.com\/CAMMA-public\/SurgVLP\n","versions":"[{'version': 'v1', 'created': 'Thu, 16 May 2024 13:14:43 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 15:27:41 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Yuan', 'Kun', ''], ['Srivastav', 'Vinkle', ''], ['Navab', 'Nassir', ''], ['Padoy', 'Nicolas', '']]","extracted_entities":"[{'text': 'embedding spaces', 'label': 'Embedding'}, {'text': 'embedding\\nspaces', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embedding spaces","similarity_score":0.8289273977}
{"id":2407.03715,"submitter":"Flavio Tonioni","authors":"C\\'edric Debusschere, Flavio Tonioni, Thomas Van Riet","title":"A distance conjecture beyond moduli?","comments":"8+1 pages and references, comments welcome!; v2: 9+2 pages and\n  references, with typos fixed, refs. added, and an extra appendix comparing\n  with hep-th\/2407.02705; v3, JHEP version: 11+2 pages and references, with\n  improved tests of the proposal in sec. 4, including 3 figs. and refs. added","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-th","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The distance conjecture states that for theories with moduli coupled to\ngravity a tower of states becomes light exponentially in the geodesic distance\nin moduli space. This specifies how effective field theories break down for\nlarge field values. However, phenomenological field theories have no moduli,\nbut a scalar potential that deforms dynamical trajectories away from geodesic\ncurves. In this note we speculate on how one should generalise the distance\nconjecture, in asymptotic field regimes, to include a scalar potential. We test\nthe generalised distance conjecture in a few cases, demonstrate a link with\npseudo-\/fake supersymmetry and apply it to the ekpyrotic scenario in cosmology.\nFor the latter we observe that the pre-uplift KKLT potential could provide a\nstringy embedding of ekpyrosis away from the asymptotic regimes in field space.\n","versions":"[{'version': 'v1', 'created': 'Thu, 4 Jul 2024 08:02:44 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Aug 2024 13:49:06 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 12:10:12 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Debusschere', 'C\u00e9dric', ''], ['Tonioni', 'Flavio', ''], ['Van Riet', 'Thomas', '']]","extracted_entities":"[{'text': 'stringy embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"stringy embedding","similarity_score":0.737077713}
{"id":2412.09165,"submitter":"Zhijie Nie","authors":"Zhijie Nie, Zhangchi Feng, Mingxin Li, Cunwang Zhang, Yanzhao Zhang,\n  Dingkun Long, Richong Zhang","title":"When Text Embedding Meets Large Language Model: A Comprehensive Survey","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.IR","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications-such as semantic matching, clustering,\nand information retrieval-continue to rely on text embeddings for their\nefficiency and effectiveness. Therefore, how to combine the LLMs and the text\nembeddings has become one of the hotspots of academic attention in recent\nyears. In this survey, we categorize the interplay between LLMs and text\nembeddings into three overarching themes: (1) LLM-augmented text embedding,\nenhancing traditional embedding methods with LLMs; (2) LLMs as text embedders,\nadapting their innate capabilities for high-quality embedding; and (3) Text\nembedding understanding with LLMs, leveraging LLMs to analyze and interpret\nembeddings. By organizing recent works based on interaction patterns rather\nthan specific downstream applications, we offer a novel and systematic overview\nof contributions from various research and application domains in the era of\nLLMs. Furthermore, we highlight the unresolved challenges that persisted in the\npre-LLM era with pre-trained language models (PLMs) and explore the emerging\nobstacles brought forth by LLMs. Building on this analysis, we outline\nprospective directions for the evolution of text embedding, addressing both\ntheoretical and practical opportunities in the rapidly advancing landscape of\nNLP.\n","versions":"[{'version': 'v1', 'created': 'Thu, 12 Dec 2024 10:50:26 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 16:11:43 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Nie', 'Zhijie', ''], ['Feng', 'Zhangchi', ''], ['Li', 'Mingxin', ''], ['Zhang', 'Cunwang', ''], ['Zhang', 'Yanzhao', ''], ['Long', 'Dingkun', ''], ['Zhang', 'Richong', '']]","extracted_entities":"[{'text': 'Text embedding', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'text embeddings', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'text\\nembeddings', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'text\\nembeddings', 'label': 'Embedding'}, {'text': 'text embedding', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Text\\nembedding', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'text embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"Text embedding","similarity_score":0.8247289658}
{"id":2501.12673,"submitter":"Daniel Ruberman","authors":"Dave Auckly, Daniel Ruberman","title":"Exotic families of embeddings","comments":"25 page, 9 figures. Added acknowledgment to 2nd version","journal-ref":"Frontiers in geometry and topology, Proc. Sympos. Pure Math., 109,\n  71--98, (2024) Amer. Math. Soc., Providence, RI","doi":null,"report-no":null,"categories":"math.GT","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We construct a number of topologically trivial but smoothly non-trivial\nfamilies of embeddings of 3-manifolds in 4-manifolds. These include embeddings\nof homology spheres in $S^4$ that are not isotopic but have diffeomorphic\ncomplements, and families (parameterized by high-dimensional spheres) of\nembeddings of any 3-manifold that embeds in a blown-up K3 surface. In each\ncase, the families are constructed so as to be topologically trivial in an\nappropriate sense. We also illustrate a general technique for converting a\nnon-trivial family of embeddings into a non-trivial family of submanifolds.\n","versions":"[{'version': 'v1', 'created': 'Wed, 22 Jan 2025 06:16:27 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 12:40:53 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Auckly', 'Dave', ''], ['Ruberman', 'Daniel', '']]","extracted_entities":"[{'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2503.09916,"submitter":"Jiaqi Sun","authors":"Jiaqi Sun, Yujia Zheng, Xinshuai Dong, Haoyue Dai, Kun Zhang","title":"Type Information-Assisted Self-Supervised Knowledge Graph Denoising","comments":"Accepted by AISTATS 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Knowledge graphs serve as critical resources supporting intelligent systems,\nbut they can be noisy due to imperfect automatic generation processes. Existing\napproaches to noise detection often rely on external facts, logical rule\nconstraints, or structural embeddings. These methods are often challenged by\nimperfect entity alignment, flexible knowledge graph construction, and\noverfitting on structures. In this paper, we propose to exploit the consistency\nbetween entity and relation type information for noise detection, resulting a\nnovel self-supervised knowledge graph denoising method that avoids those\nproblems. We formalize type inconsistency noise as triples that deviate from\nthe majority with respect to type-dependent reasoning along the topological\nstructure. Specifically, we first extract a compact representation of a given\nknowledge graph via an encoder that models the type dependencies of triples.\nThen, the decoder reconstructs the original input knowledge graph based on the\ncompact representation. It is worth noting that, our proposal has the potential\nto address the problems of knowledge graph compression and completion, although\nthis is not our focus. For the specific task of noise detection, the\ndiscrepancy between the reconstruction results and the input knowledge graph\nprovides an opportunity for denoising, which is facilitated by the type\nconsistency embedded in our method. Experimental validation demonstrates the\neffectiveness of our approach in detecting potential noise in real-world data.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 00:12:27 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Sun', 'Jiaqi', ''], ['Zheng', 'Yujia', ''], ['Dong', 'Xinshuai', ''], ['Dai', 'Haoyue', ''], ['Zhang', 'Kun', '']]","extracted_entities":"[{'text': 'structural embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"structural embeddings","similarity_score":0.8064122796}
{"id":2503.10057,"submitter":"Ho Hin Lee","authors":"Ho Hin Lee, Alberto Santamaria-Pang, Jameson Merkov, Matthew Lungren,\n  Ivan Tarapov","title":"Multi-Modal Mamba Modeling for Survival Prediction (M4Survive): Adapting\n  Joint Foundation Model Representations","comments":"10 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Accurate survival prediction in oncology requires integrating diverse imaging\nmodalities to capture the complex interplay of tumor biology. Traditional\nsingle-modality approaches often fail to leverage the complementary insights\nprovided by radiological and pathological assessments. In this work, we\nintroduce M4Survive (Multi-Modal Mamba Modeling for Survival Prediction), a\nnovel framework that learns joint foundation model representations using\nefficient adapter networks. Our approach dynamically fuses heterogeneous\nembeddings from a foundation model repository (e.g., MedImageInsight,\nBiomedCLIP, Prov-GigaPath, UNI2-h), creating a correlated latent space\noptimized for survival risk estimation. By leveraging Mamba-based adapters,\nM4Survive enables efficient multi-modal learning while preserving computational\nefficiency. Experimental evaluations on benchmark datasets demonstrate that our\napproach outperforms both unimodal and traditional static multi-modal baselines\nin survival prediction accuracy. This work underscores the potential of\nfoundation model-driven multi-modal fusion in advancing precision oncology and\npredictive analytics.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 05:18:32 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Lee', 'Ho Hin', ''], ['Santamaria-Pang', 'Alberto', ''], ['Merkov', 'Jameson', ''], ['Lungren', 'Matthew', ''], ['Tarapov', 'Ivan', '']]","extracted_entities":"[{'text': 'heterogeneous\\nembeddings', 'label': 'Embedding'}, {'text': 'MedImageInsight', 'label': 'Foundation Model'}, {'text': 'BiomedCLIP', 'label': 'Foundation Model'}, {'text': 'UNI2-h', 'label': 'Foundation Model'}, {'text': 'multi-modal learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Embedding","matched_keyword":"heterogeneous\nembeddings","similarity_score":0.7148266435}
{"id":2503.10063,"submitter":"Luke Bauer","authors":"Luke A. Bauer, Wenxuan Bao, and Vincent Bindschaedler","title":"Provably Secure Covert Messaging Using Image-based Diffusion Processes","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We consider the problem of securely and robustly embedding covert messages\ninto an image-based diffusion model's output. The sender and receiver want to\nexchange the maximum amount of information possible per diffusion sampled image\nwhile remaining undetected. The adversary wants to detect that such\ncommunication is taking place by identifying those diffusion samples that\ncontain covert messages. To maximize robustness to transformations of the\ndiffusion sample, a strategy is for the sender and the receiver to embed the\nmessage in the initial latents. We first show that prior work that attempted\nthis is easily broken because their embedding technique alters the latents'\ndistribution. We then propose a straightforward method to embed covert messages\nin the initial latent {\\em without} altering the distribution. We prove that\nour construction achieves indistinguishability to any probabilistic polynomial\ntime adversary. Finally, we discuss and analyze empirically the tradeoffs\nbetween embedding capacity, message recovery rates, and robustness. We find\nthat optimizing the inversion method for error correction is crucial for\nreliability.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 05:24:40 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Bauer', 'Luke A.', ''], ['Bao', 'Wenxuan', ''], ['Bindschaedler', 'Vincent', '']]","extracted_entities":"[{'text': 'embedding technique', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embedding technique","similarity_score":0.9008094072}
{"id":2503.1008,"submitter":"Zhen Qu","authors":"Zhen Qu, Xian Tao, Xinyi Gong, Shichen Qu, Qiyu Chen, Zhengtao Zhang,\n  Xingang Wang, Guiguang Ding","title":"Bayesian Prompt Flow Learning for Zero-Shot Anomaly Detection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recently, vision-language models (e.g. CLIP) have demonstrated remarkable\nperformance in zero-shot anomaly detection (ZSAD). By leveraging auxiliary data\nduring training, these models can directly perform cross-category anomaly\ndetection on target datasets, such as detecting defects on industrial product\nsurfaces or identifying tumors in organ tissues. Existing approaches typically\nconstruct text prompts through either manual design or the optimization of\nlearnable prompt vectors. However, these methods face several challenges: 1)\nhandcrafted prompts require extensive expert knowledge and trial-and-error; 2)\nsingle-form learnable prompts struggle to capture complex anomaly semantics;\nand 3) an unconstrained prompt space limit generalization to unseen categories.\nTo address these issues, we propose Bayesian Prompt Flow Learning (Bayes-PFL),\nwhich models the prompt space as a learnable probability distribution from a\nBayesian perspective. Specifically, a prompt flow module is designed to learn\nboth image-specific and image-agnostic distributions, which are jointly\nutilized to regularize the text prompt space and enhance the model's\ngeneralization on unseen categories. These learned distributions are then\nsampled to generate diverse text prompts, effectively covering the prompt\nspace. Additionally, a residual cross-attention (RCA) module is introduced to\nbetter align dynamic text embeddings with fine-grained image features.\nExtensive experiments on 15 industrial and medical datasets demonstrate our\nmethod's superior performance.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 06:05:35 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Qu', 'Zhen', ''], ['Tao', 'Xian', ''], ['Gong', 'Xinyi', ''], ['Qu', 'Shichen', ''], ['Chen', 'Qiyu', ''], ['Zhang', 'Zhengtao', ''], ['Wang', 'Xingang', ''], ['Ding', 'Guiguang', '']]","extracted_entities":"[{'text': 'text prompts', 'label': 'Prompting'}, {'text': 'handcrafted prompts', 'label': 'Prompting'}, {'text': 'single-form learnable prompts', 'label': 'Prompting'}, {'text': 'Bayesian Prompt Flow Learning', 'label': 'Zero-shot Learning'}, {'text': 'Bayes-PFL', 'label': 'Zero-shot Learning'}, {'text': 'text prompts', 'label': 'Prompting'}, {'text': 'dynamic text embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"dynamic text embeddings","similarity_score":0.6625349522}
{"id":2503.10176,"submitter":"Yuta Sato","authors":"Yuta Sato","title":"Uniform Lyndon interpolation for the pure logic of necessitation with a\n  modal reduction principle","comments":"20 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.LO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We prove the uniform Lyndon interpolation property (ULIP) of some extensions\nof the pure logic of necessitation $\\mathbf{N}$. For any $m, n \\in \\mathbb{N}$,\n$\\mathbf{N}^+\\mathbf{A}_{m,n}$ is the logic obtained from $\\mathbf{N}$ by\nadding a single axiom $\\Box^n \\varphi \\to \\Box^m \\varphi$, which is a\n$\\Diamond$-free modal reduction principle, and a rule $\\frac{\\neg \\Box\n\\varphi}{\\neg \\Box \\Box \\varphi}$, which is required to make the logic complete\nwith respect to its Kripke-like semantics. We first introduce a sequent\ncalculus $\\mathbf{GN}^+\\mathbf{A}_{m,n}$ for $\\mathbf{N}^+\\mathbf{A}_{m,n}$ and\nshow that it enjoys cut elimination, proving Craig and Lyndon interpolation\nproperties as a consequence. We then construct an embedding of\n$\\mathbf{N}^+\\mathbf{A}_{m,n}$ into classical propositional logic\n$\\mathbf{Cl}$, which is then used to prove ULIP of\n$\\mathbf{N}^+\\mathbf{A}_{m,n}$ by reducing it to that of $\\mathbf{Cl}$. We also\nprove ULIP of $\\mathbf{NRA}_{m,n} = \\mathbf{N} + \\Box^n \\varphi \\to \\Box^m\n\\varphi + \\frac{\\neg \\varphi}{\\neg \\Box \\varphi}$ with a similar method.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 08:57:41 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Sato', 'Yuta', '']]","extracted_entities":"[{'text': 'embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embedding","similarity_score":1.0}
{"id":2503.10183,"submitter":"Shunqi Mao","authors":"Shunqi Mao, Chaoyi Zhang, Weidong Cai","title":"Through the Magnifying Glass: Adaptive Perception Magnification for\n  Hallucination-Free VLM Decoding","comments":"19 pages, 5 figures, 9 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Existing vision-language models (VLMs) often suffer from visual\nhallucination, where the generated responses contain inaccuracies that are not\ngrounded in the visual input. Efforts to address this issue without model\nfinetuning primarily mitigate hallucination by reducing biases contrastively or\namplifying the weights of visual embedding during decoding. However, these\napproaches improve visual perception at the cost of impairing the language\nreasoning capability. In this work, we propose the Perception Magnifier (PM), a\nnovel visual decoding method that iteratively isolates relevant visual tokens\nbased on attention and magnifies the corresponding regions, spurring the model\nto concentrate on fine-grained visual details during decoding. Specifically, by\nmagnifying critical regions while preserving the structural and contextual\ninformation at each decoding step, PM allows the VLM to enhance its scrutiny of\nthe visual input, hence producing more accurate and faithful responses.\nExtensive experimental results demonstrate that PM not only achieves superior\nhallucination mitigation but also enhances language generation while preserving\nstrong reasoning capabilities.Code is available at\nhttps:\/\/github.com\/ShunqiM\/PM .\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 09:14:11 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Mao', 'Shunqi', ''], ['Zhang', 'Chaoyi', ''], ['Cai', 'Weidong', '']]","extracted_entities":"[{'text': 'visual embedding', 'label': 'Embedding'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'spurring', 'label': 'Prompting'}]","assigned_concept":"Embedding","matched_keyword":"visual embedding","similarity_score":0.8153178692}
{"id":2503.10297,"submitter":"Peyman Neshaastegaran","authors":"Peyman Neshaastegaran, and Ming Jian","title":"CoDiPhy: A General Framework for Applying Denoising Diffusion Models to\n  the Physical Layer of Wireless Communication Systems","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Generative models, including denoising diffusion models (DM), are gaining\nattention in wireless applications due to their ability to learn complex data\ndistributions. In this paper, we propose CoDiPhy, a novel framework that\nleverages conditional denoising diffusion models to address a wide range of\nwireless physical layer problems. A key challenge of using DM is the need to\nassume or approximate Gaussian signal models. CoDiPhy addresses this by\nincorporating a conditional encoder as a guidance mechanism, mapping problem\nobservations to a latent space and removing the Gaussian constraint. By\ncombining conditional encoding, time embedding layers, and a U-Net-based main\nneural network, CoDiPhy introduces a noise prediction neural network, replacing\nthe conventional approach used in DM. This adaptation enables CoDiPhy to serve\nas an effective solution for a wide range of detection, estimation, and\npredistortion tasks. We demonstrate CoDiPhy's adaptability through two case\nstudies: an OFDM receiver for detection and phase noise compensation for\nestimation. In both cases, CoDiPhy outperforms conventional methods by a\nsignificant margin.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 12:18:25 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Neshaastegaran', 'Peyman', ''], ['Jian', 'Ming', '']]","extracted_entities":"[{'text': 'conditional encoding', 'label': 'Embedding'}, {'text': 'time embedding layers', 'label': 'Embedding'}, {'text': 'noise prediction neural network', 'label': 'Neural Language Model'}]","assigned_concept":"Embedding","matched_keyword":"time embedding layers","similarity_score":0.5304672718}
{"id":2503.1035,"submitter":"Ali Salar","authors":"Ali Salar, Qing Liu, Yingli Tian and Guoying Zhao","title":"Enhancing Facial Privacy Protection via Weakening Diffusion Purification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The rapid growth of social media has led to the widespread sharing of\nindividual portrait images, which pose serious privacy risks due to the\ncapabilities of automatic face recognition (AFR) systems for mass surveillance.\nHence, protecting facial privacy against unauthorized AFR systems is essential.\nInspired by the generation capability of the emerging diffusion models, recent\nmethods employ diffusion models to generate adversarial face images for privacy\nprotection. However, they suffer from the diffusion purification effect,\nleading to a low protection success rate (PSR). In this paper, we first propose\nlearning unconditional embeddings to increase the learning capacity for\nadversarial modifications and then use them to guide the modification of the\nadversarial latent code to weaken the diffusion purification effect. Moreover,\nwe integrate an identity-preserving structure to maintain structural\nconsistency between the original and generated images, allowing human observers\nto recognize the generated image as having the same identity as the original.\nExtensive experiments conducted on two public datasets, i.e., CelebA-HQ and\nLADN, demonstrate the superiority of our approach. The protected faces\ngenerated by our method outperform those produced by existing facial privacy\nprotection approaches in terms of transferability and natural appearance.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 13:27:53 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Salar', 'Ali', ''], ['Liu', 'Qing', ''], ['Tian', 'Yingli', ''], ['Zhao', 'Guoying', '']]","extracted_entities":"[{'text': 'unconditional embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"unconditional embeddings","similarity_score":0.7055432796}
{"id":2503.10358,"submitter":"Zirun Guo","authors":"Zirun Guo, Tao Jin","title":"ConceptGuard: Continual Personalized Text-to-Image Generation with\n  Forgetting and Confusion Mitigation","comments":"Accepted at CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Diffusion customization methods have achieved impressive results with only a\nminimal number of user-provided images. However, existing approaches customize\nconcepts collectively, whereas real-world applications often require sequential\nconcept integration. This sequential nature can lead to catastrophic\nforgetting, where previously learned concepts are lost. In this paper, we\ninvestigate concept forgetting and concept confusion in the continual\ncustomization. To tackle these challenges, we present ConceptGuard, a\ncomprehensive approach that combines shift embedding, concept-binding prompts\nand memory preservation regularization, supplemented by a priority queue which\ncan adaptively update the importance and occurrence order of different\nconcepts. These strategies can dynamically update, unbind and learn the\nrelationship of the previous concepts, thus alleviating concept forgetting and\nconfusion. Through comprehensive experiments, we show that our approach\noutperforms all the baseline methods consistently and significantly in both\nquantitative and qualitative analyses.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 13:39:24 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Guo', 'Zirun', ''], ['Jin', 'Tao', '']]","extracted_entities":"[{'text': 'shift embedding', 'label': 'Embedding'}, {'text': 'concept-binding prompts', 'label': 'Prompting'}]","assigned_concept":"Embedding","matched_keyword":"shift embedding","similarity_score":0.7073588967}
{"id":2503.10399,"submitter":"Andrey Savchenko","authors":"Andrey V. Savchenko","title":"HSEmotion Team at ABAW-8 Competition: Audiovisual Ambivalence\/Hesitancy,\n  Emotional Mimicry Intensity and Facial Expression Recognition","comments":"submitted to ABAW CVPR 2025 Workshop","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This article presents our results for the eighth Affective Behavior Analysis\nin-the-Wild (ABAW) competition. We combine facial emotional descriptors\nextracted by pre-trained models, namely, our EmotiEffLib library, with acoustic\nfeatures and embeddings of texts recognized from speech. The frame-level\nfeatures are aggregated and fed into simple classifiers, e.g., multi-layered\nperceptron (feed-forward neural network with one hidden layer), to predict\nambivalence\/hesitancy and facial expressions. In the latter case, we also use\nthe pre-trained facial expression recognition model to select high-score video\nframes and prevent their processing with a domain-specific video classifier.\nThe video-level prediction of emotional mimicry intensity is implemented by\nsimply aggregating frame-level features and training a multi-layered\nperceptron. Experimental results for three tasks from the ABAW challenge\ndemonstrate that our approach significantly increases validation metrics\ncompared to existing baselines.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 14:21:46 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Savchenko', 'Andrey V.', '']]","extracted_entities":"[{'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2503.10446,"submitter":"Jakaria Islam Emon","authors":"Jakaria Islam Emon, Md Abu Salek and Kazi Tamanna Alam","title":"Whisper Speaker Identification: Leveraging Pre-Trained Multilingual\n  Transformers for Robust Speaker Embeddings","comments":"6 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.AI eess.AS","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Speaker identification in multilingual settings presents unique challenges,\nparticularly when conventional models are predominantly trained on English\ndata. In this paper, we propose WSI (Whisper Speaker Identification), a\nframework that repurposes the encoder of the Whisper automatic speech\nrecognition model pre trained on extensive multilingual data to generate robust\nspeaker embeddings via a joint loss optimization strategy that leverages online\nhard triplet mining and self supervised Normalized Temperature-scaled Cross\nEntropy loss. By capitalizing on Whisper language-agnostic acoustic\nrepresentations, our approach effectively distinguishes speakers across diverse\nlanguages and recording conditions. Extensive evaluations on multiple corpora,\nincluding VoxTube (multilingual), JVS (Japanese), CallHome (German, Spanish,\nChinese, and Japanese), and Voxconverse (English), demonstrate that WSI\nconsistently outperforms state-of-the-art baselines, namely Pyannote Embedding,\nECAPA TDNN, and Xvector, in terms of lower equal error rates and higher AUC\nscores. These results validate our hypothesis that a multilingual pre-trained\nASR encoder, combined with joint loss optimization, substantially improves\nspeaker identification performance in non-English languages.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 15:11:28 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Emon', 'Jakaria Islam', ''], ['Salek', 'Md Abu', ''], ['Alam', 'Kazi Tamanna', '']]","extracted_entities":"[{'text': 'Pyannote Embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"Pyannote Embedding","similarity_score":0.510794282}
{"id":2503.10566,"submitter":"Egor Zverev","authors":"Egor Zverev, Evgenii Kortukov, Alexander Panfilov, Soroush Tabesh,\n  Alexandra Volkova, Sebastian Lapuschkin, Wojciech Samek, Christoph H. Lampert","title":"ASIDE: Architectural Separation of Instructions and Data in Language\n  Models","comments":"ICLR 2025 Workshop on Building Trust in Language Models and\n  Applications","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Despite their remarkable performance, large language models lack elementary\nsafety features, and this makes them susceptible to numerous malicious attacks.\nIn particular, previous work has identified the absence of an intrinsic\nseparation between instructions and data as a root cause for the success of\nprompt injection attacks. In this work, we propose an architectural change,\nASIDE, that allows the model to clearly separate between instructions and data\nby using separate embeddings for them. Instead of training the embeddings from\nscratch, we propose a method to convert an existing model to ASIDE form by\nusing two copies of the original model's embeddings layer, and applying an\northogonal rotation to one of them. We demonstrate the effectiveness of our\nmethod by showing (1) highly increased instruction-data separation scores\nwithout a loss in model capabilities and (2) competitive results on prompt\ninjection benchmarks, even without dedicated safety training. Additionally, we\nstudy the working mechanism behind our method through an analysis of model\nrepresentations.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:17:17 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zverev', 'Egor', ''], ['Kortukov', 'Evgenii', ''], ['Panfilov', 'Alexander', ''], ['Tabesh', 'Soroush', ''], ['Volkova', 'Alexandra', ''], ['Lapuschkin', 'Sebastian', ''], ['Samek', 'Wojciech', ''], ['Lampert', 'Christoph H.', '']]","extracted_entities":"[{'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2404.14812,"submitter":"Yufeng Zhang","authors":"Yufeng Zhang, Xuepeng Wang, Lingxiang Wu, Jinqiao Wang","title":"Enhancing Chain of Thought Prompting in Large Language Models via\n  Reasoning Patterns","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Chain of Thought (CoT) prompting can encourage language models to engage in\nmulti-step logical reasoning. The quality of the provided demonstrations\nsignificantly influences the success of downstream inference tasks. Current\nunsupervised CoT methods primarily select examples based on the semantics of\nthe questions, which can introduce noise and lack interpretability. In this\npaper, we propose leveraging reasoning patterns to enhance CoT prompting\neffectiveness. Reasoning patterns represent the process by which language\nmodels arrive at their final results. By utilizing prior knowledge and\nprompt-based methods from large models, we first construct task-specific\npattern sets. We then select diverse demonstrations based on different\nreasoning patterns. This approach not only mitigates the impact of noise but\nalso provides explicit interpretability to help us understand the mechanisms of\nCoT. Extensive experiments demonstrate that our method is more robust and\nconsistently leads to improvements across various reasoning tasks.\n","versions":"[{'version': 'v1', 'created': 'Tue, 23 Apr 2024 07:50:00 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 03:03:57 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhang', 'Yufeng', ''], ['Wang', 'Xuepeng', ''], ['Wu', 'Lingxiang', ''], ['Wang', 'Jinqiao', '']]","extracted_entities":"[{'text': 'Chain of Thought', 'label': 'Chain of thought'}, {'text': 'CoT', 'label': 'Chain of thought'}, {'text': 'CoT prompting', 'label': 'Prompting'}, {'text': 'Reasoning patterns', 'label': 'Chain of thought'}, {'text': 'reasoning patterns', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain of Thought","similarity_score":0.9999998808}
{"id":2503.06692,"submitter":"Yuchen Yan","authors":"Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Mengdi Zhang, Jian\n  Shao, Yueting Zhuang","title":"InftyThink: Breaking the Length Limits of Long-Context Reasoning in\n  Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Advanced reasoning in large language models has achieved remarkable\nperformance on challenging tasks, but the prevailing long-context reasoning\nparadigm faces critical limitations: quadratic computational scaling with\nsequence length, reasoning constrained by maximum context boundaries, and\nperformance degradation beyond pre-training context windows. Existing\napproaches primarily compress reasoning chains without addressing the\nfundamental scaling problem. To overcome these challenges, we introduce\nInftyThink, a paradigm that transforms monolithic reasoning into an iterative\nprocess with intermediate summarization. By interleaving short reasoning\nsegments with concise progress summaries, our approach enables unbounded\nreasoning depth while maintaining bounded computational costs. This creates a\ncharacteristic sawtooth memory pattern that significantly reduces computational\ncomplexity compared to traditional approaches. Furthermore, we develop a\nmethodology for reconstructing long-context reasoning datasets into our\niterative format, transforming OpenR1-Math into 333K training instances.\nExperiments across multiple model architectures demonstrate that our approach\nreduces computational costs while improving performance, with Qwen2.5-Math-7B\nshowing 3-13% improvements across MATH500, AIME24, and GPQA_diamond benchmarks.\nOur work challenges the assumed trade-off between reasoning depth and\ncomputational efficiency, providing a more scalable approach to complex\nreasoning without architectural modifications.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 16:59:14 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 16:00:47 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Yan', 'Yuchen', ''], ['Shen', 'Yongliang', ''], ['Liu', 'Yang', ''], ['Jiang', 'Jin', ''], ['Zhang', 'Mengdi', ''], ['Shao', 'Jian', ''], ['Zhuang', 'Yueting', '']]","extracted_entities":"[{'text': 'quadratic computational scaling', 'label': 'Scaling law'}, {'text': 'reasoning chains', 'label': 'Chain of thought'}, {'text': 'intermediate summarization', 'label': 'Knowledge distillation'}]","assigned_concept":"Chain of thought","matched_keyword":"reasoning chains","similarity_score":0.5610544086}
{"id":2503.08679,"submitter":"Iv\\'an Arcuschin","authors":"Iv\\'an Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran\n  Rajamanoharan, Neel Nanda, Arthur Conmy","title":"Chain-of-Thought Reasoning In The Wild Is Not Always Faithful","comments":"Accepted to the Reasoning and Planning for Large Language Models\n  Workshop (ICLR 25), 10 main paper pages, 38 appendix pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art\nAI capabilities. However, recent studies have shown that CoT reasoning is not\nalways faithful, i.e. CoT reasoning does not always reflect how models arrive\nat conclusions. So far, most of these studies have focused on unfaithfulness in\nunnatural contexts where an explicit bias has been introduced. In contrast, we\nshow that unfaithful CoT can occur on realistic prompts with no artificial\nbias. Our results reveal non-negligible rates of several forms of unfaithful\nreasoning in frontier models: Sonnet 3.7 (16.3%), DeepSeek R1 (5.3%) and\nChatGPT-4o (7.0%) all answer a notable proportion of question pairs\nunfaithfully. Specifically, we find that models rationalize their implicit\nbiases in answers to binary questions (\"implicit post-hoc rationalization\").\nFor example, when separately presented with the questions \"Is X bigger than Y?\"\nand \"Is Y bigger than X?\", models sometimes produce superficially coherent\narguments to justify answering Yes to both questions or No to both questions,\ndespite such responses being logically contradictory. We also investigate\nrestoration errors (Dziri et al., 2023), where models make and then silently\ncorrect errors in their reasoning, and unfaithful shortcuts, where models use\nclearly illogical reasoning to simplify solving problems in Putnam questions (a\nhard benchmark). Our findings raise challenges for AI safety work that relies\non monitoring CoT to detect undesired behavior.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 17:56:30 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 17:49:58 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Arcuschin', 'Iv\u00e1n', ''], ['Janiak', 'Jett', ''], ['Krzyzanowski', 'Robert', ''], ['Rajamanoharan', 'Senthooran', ''], ['Nanda', 'Neel', ''], ['Conmy', 'Arthur', '']]","extracted_entities":"[{'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'CoT reasoning', 'label': 'Chain of thought'}, {'text': 'CoT reasoning', 'label': 'Chain of thought'}, {'text': 'CoT', 'label': 'Chain of thought'}, {'text': 'realistic prompts', 'label': 'Prompting'}, {'text': 'ChatGPT-4o', 'label': 'ChatGPT'}, {'text': 'models', 'label': 'AI model'}, {'text': 'models', 'label': 'AI model'}, {'text': 'models', 'label': 'AI model'}, {'text': 'AI safety work', 'label': 'AI Ethics'}, {'text': 'CoT', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain-of-Thought","similarity_score":0.9539169669}
{"id":2503.09567,"submitter":"Qiguang Chen","authors":"Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng\n  Wang, Mengkang Hu, Yuhang Zhou, Te Gao, Wanxiang Che","title":"Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning\n  Large Language Models","comments":"Paper are available at https:\/\/long-cot.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Recent advancements in reasoning with large language models (RLLMs), such as\nOpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in\ncomplex domains like mathematics and coding. A central factor in their success\nlies in the application of long chain-of-thought (Long CoT) characteristics,\nwhich enhance reasoning abilities and enable the solution of intricate\nproblems. However, despite these developments, a comprehensive survey on Long\nCoT is still lacking, limiting our understanding of its distinctions from\ntraditional short chain-of-thought (Short CoT) and complicating ongoing debates\non issues like \"overthinking\" and \"test-time scaling.\" This survey seeks to\nfill this gap by offering a unified perspective on Long CoT. (1) We first\ndistinguish Long CoT from Short CoT and introduce a novel taxonomy to\ncategorize current reasoning paradigms. (2) Next, we explore the key\ncharacteristics of Long CoT: deep reasoning, extensive exploration, and\nfeasible reflection, which enable models to handle more complex tasks and\nproduce more efficient, coherent outcomes compared to the shallower Short CoT.\n(3) We then investigate key phenomena such as the emergence of Long CoT with\nthese characteristics, including overthinking, and test-time scaling, offering\ninsights into how these processes manifest in practice. (4) Finally, we\nidentify significant research gaps and highlight promising future directions,\nincluding the integration of multi-modal reasoning, efficiency improvements,\nand enhanced knowledge frameworks. By providing a structured overview, this\nsurvey aims to inspire future research and further the development of logical\nreasoning in artificial intelligence.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 17:35:03 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 04:34:15 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chen', 'Qiguang', ''], ['Qin', 'Libo', ''], ['Liu', 'Jinhao', ''], ['Peng', 'Dengyun', ''], ['Guan', 'Jiannan', ''], ['Wang', 'Peng', ''], ['Hu', 'Mengkang', ''], ['Zhou', 'Yuhang', ''], ['Gao', 'Te', ''], ['Che', 'Wanxiang', '']]","extracted_entities":"[{'text': 'long chain-of-thought', 'label': 'Chain of thought'}, {'text': 'Long CoT', 'label': 'Chain of thought'}, {'text': 'Long\\nCoT', 'label': 'Chain of thought'}, {'text': 'short chain-of-thought', 'label': 'Chain of thought'}, {'text': 'Short CoT', 'label': 'Chain of thought'}, {'text': 'test-time scaling', 'label': 'Scaling law'}, {'text': 'Long CoT', 'label': 'Chain of thought'}, {'text': 'Long CoT', 'label': 'Chain of thought'}, {'text': 'Short CoT', 'label': 'Chain of thought'}, {'text': 'Long CoT', 'label': 'Chain of thought'}, {'text': 'Short CoT', 'label': 'Chain of thought'}, {'text': 'Long CoT', 'label': 'Chain of thought'}, {'text': 'test-time scaling', 'label': 'Scaling law'}]","assigned_concept":"Chain of thought","matched_keyword":"long chain-of-thought","similarity_score":0.8942457438}
{"id":2503.09968,"submitter":"Zihao Zhang","authors":"Zihao Zhang and Aming Wu and Yahong Han","title":"Style Evolving along Chain-of-Thought for Unknown-Domain Object\n  Detection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recently, a task of Single-Domain Generalized Object Detection (Single-DGOD)\nis proposed, aiming to generalize a detector to multiple unknown domains never\nseen before during training. Due to the unavailability of target-domain data,\nsome methods leverage the multimodal capabilities of vision-language models,\nusing textual prompts to estimate cross-domain information, enhancing the\nmodel's generalization capability. These methods typically use a single textual\nprompt, often referred to as the one-step prompt method. However, when dealing\nwith complex styles such as the combination of rain and night, we observe that\nthe performance of the one-step prompt method tends to be relatively weak. The\nreason may be that many scenes incorporate not just a single style but a\ncombination of multiple styles. The one-step prompt method may not effectively\nsynthesize combined information involving various styles. To address this\nlimitation, we propose a new method, i.e., Style Evolving along\nChain-of-Thought, which aims to progressively integrate and expand style\ninformation along the chain of thought, enabling the continual evolution of\nstyles. Specifically, by progressively refining style descriptions and guiding\nthe diverse evolution of styles, this approach enables more accurate simulation\nof various style characteristics and helps the model gradually learn and adapt\nto subtle differences between styles. Additionally, it exposes the model to a\nbroader range of style features with different data distributions, thereby\nenhancing its generalization capability in unseen domains. The significant\nperformance gains over five adverse-weather scenarios and the Real to Art\nbenchmark demonstrate the superiorities of our method.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 02:14:10 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhang', 'Zihao', ''], ['Wu', 'Aming', ''], ['Han', 'Yahong', '']]","extracted_entities":"[{'text': 'textual prompts', 'label': 'Prompting'}, {'text': 'one-step prompt method', 'label': 'Prompting'}, {'text': 'one-step prompt method', 'label': 'Prompting'}, {'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'chain of thought', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"chain of thought","similarity_score":0.9999998808}
{"id":2503.10166,"submitter":"Pengfei Luo","authors":"Pengfei Luo, Jingbo Zhou, Tong Xu, Yuan Xia, Linli Xu, Enhong Chen","title":"ImageScope: Unifying Language-Guided Image Retrieval via Large\n  Multimodal Model Collective Reasoning","comments":"WWW 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR cs.AI cs.MM","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  With the proliferation of images in online content, language-guided image\nretrieval (LGIR) has emerged as a research hotspot over the past decade,\nencompassing a variety of subtasks with diverse input forms. While the\ndevelopment of large multimodal models (LMMs) has significantly facilitated\nthese tasks, existing approaches often address them in isolation, requiring the\nconstruction of separate systems for each task. This not only increases system\ncomplexity and maintenance costs, but also exacerbates challenges stemming from\nlanguage ambiguity and complex image content, making it difficult for retrieval\nsystems to provide accurate and reliable results. To this end, we propose\nImageScope, a training-free, three-stage framework that leverages collective\nreasoning to unify LGIR tasks. The key insight behind the unification lies in\nthe compositional nature of language, which transforms diverse LGIR tasks into\na generalized text-to-image retrieval process, along with the reasoning of LMMs\nserving as a universal verification to refine the results. To be specific, in\nthe first stage, we improve the robustness of the framework by synthesizing\nsearch intents across varying levels of semantic granularity using\nchain-of-thought (CoT) reasoning. In the second and third stages, we then\nreflect on retrieval results by verifying predicate propositions locally, and\nperforming pairwise evaluations globally. Experiments conducted on six LGIR\ndatasets demonstrate that ImageScope outperforms competitive baselines.\nComprehensive evaluations and ablation studies further confirm the\neffectiveness of our design.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 08:43:24 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Luo', 'Pengfei', ''], ['Zhou', 'Jingbo', ''], ['Xu', 'Tong', ''], ['Xia', 'Yuan', ''], ['Xu', 'Linli', ''], ['Chen', 'Enhong', '']]","extracted_entities":"[{'text': 'ImageScope', 'label': 'Embedding'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'chain-of-thought (CoT)', 'label': 'Chain of thought'}, {'text': 'ImageScope', 'label': 'Embedding'}]","assigned_concept":"Chain of thought","matched_keyword":"chain-of-thought (CoT)","similarity_score":0.7026641369}
{"id":2503.10177,"submitter":"Yirong Sun","authors":"Yirong Sun, Yanjun Chen","title":"PRISM: Preference Refinement via Implicit Scene Modeling for 3D\n  Vision-Language Preference-Based Reinforcement Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We propose PRISM, a novel framework designed to overcome the limitations of\n2D-based Preference-Based Reinforcement Learning (PBRL) by unifying 3D point\ncloud modeling and future-aware preference refinement. At its core, PRISM\nadopts a 3D Point Cloud-Language Model (3D-PC-LLM) to mitigate occlusion and\nviewpoint biases, ensuring more stable and spatially consistent preference\nsignals. Additionally, PRISM leverages Chain-of-Thought (CoT) reasoning to\nincorporate long-horizon considerations, thereby preventing the short-sighted\nfeedback often seen in static preference comparisons. In contrast to\nconventional PBRL techniques, this integration of 3D perception and\nfuture-oriented reasoning leads to significant gains in preference agreement\nrates, faster policy convergence, and robust generalization across unseen\nrobotic environments. Our empirical results, spanning tasks such as robotic\nmanipulation and autonomous navigation, highlight PRISM's potential for\nreal-world applications where precise spatial understanding and reliable\nlong-term decision-making are critical. By bridging 3D geometric awareness with\nCoT-driven preference modeling, PRISM establishes a comprehensive foundation\nfor scalable, human-aligned reinforcement learning.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 08:58:10 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Sun', 'Yirong', ''], ['Chen', 'Yanjun', '']]","extracted_entities":"[{'text': 'PRISM', 'label': 'Foundation Model'}, {'text': 'PRISM', 'label': 'Foundation Model'}, {'text': 'PRISM', 'label': 'Foundation Model'}, {'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'PRISM', 'label': 'Foundation Model'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain-of-Thought","similarity_score":0.9539169669}
{"id":2503.10263,"submitter":"Christian Sendlinger","authors":"Christian Sendlinger, Jonas Kellerer, Felix Spanier","title":"KARL -- A Monte Carlo model for atomic and molecular processes in the\n  tritium atmosphere of the KATRIN experiment","comments":"accepted for publication in Computer Physics Communications, 60\n  pages, 28 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.comp-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  A new parallelized simulation code is presented, which uses a Monte Carlo\nmethod to determine particle spectra in the KATRIN source. Reaction chains are\ngenerated from the decay of tritium within the source. The code includes all\nrelevant processes: elastic scattering, ionization, excitation (electric,\nvibrational, rotational), recombination and various clustering processes. The\nmain emphasis of the code is the calculation of particle spectra and particle\ndensities and currents at specific points within the source. It features a new\ntechnique to determine these quantities. It also calculates target fields for\nthe interaction of particles with each other as it is needed for recombination\nprocesses. The code has been designed for the KATRIN experiment but is easily\nadapt-able for other tritium based experiments like Project 8. Geometry and\nbackground tritium gas flow can be given as user input. The code is\nparallelized using MPI and writes output using HDF5. Input to the simulation is\nread from a JSON description.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 11:20:40 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Sendlinger', 'Christian', ''], ['Kellerer', 'Jonas', ''], ['Spanier', 'Felix', '']]","extracted_entities":"[{'text': 'Reaction chains', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"Reaction chains","similarity_score":0.5112974644}
{"id":2503.10351,"submitter":"Sinuo Liu","authors":"Sinuo Liu, Chenyang Lyu, Minghao Wu, Longyue Wang, Weihua Luo, Kaifu\n  Zhang","title":"New Trends for Modern Machine Translation with Large Reasoning Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent advances in Large Reasoning Models (LRMs), particularly those\nleveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility\nfor Machine Translation (MT). This position paper argues that LRMs\nsubstantially transformed traditional neural MT as well as LLMs-based MT\nparadigms by reframing translation as a dynamic reasoning task that requires\ncontextual, cultural, and linguistic understanding and reasoning. We identify\nthree foundational shifts: 1) contextual coherence, where LRMs resolve\nambiguities and preserve discourse structure through explicit reasoning over\ncross-sentence and complex context or even lack of context; 2) cultural\nintentionality, enabling models to adapt outputs by inferring speaker intent,\naudience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can\nperform self-reflection during the inference time to correct the potential\nerrors in translation especially extremely noisy cases, showing better\nrobustness compared to simply mapping X->Y translation. We explore various\nscenarios in translation including stylized translation, document-level\ntranslation and multimodal translation by showcasing empirical examples that\ndemonstrate the superiority of LRMs in translation. We also identify several\ninteresting phenomenons for LRMs for MT including auto-pivot translation as\nwell as the critical challenges such as over-localisation in translation and\ninference efficiency. In conclusion, we think that LRMs redefine translation\nsystems not merely as text converters but as multilingual cognitive agents\ncapable of reasoning about meaning beyond the text. This paradigm shift reminds\nus to think of problems in translation beyond traditional translation scenarios\nin a much broader context with LRMs - what we can achieve on top of it.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 13:27:53 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Liu', 'Sinuo', ''], ['Lyu', 'Chenyang', ''], ['Wu', 'Minghao', ''], ['Wang', 'Longyue', ''], ['Luo', 'Weihua', ''], ['Zhang', 'Kaifu', '']]","extracted_entities":"[{'text': 'Large Reasoning Models', 'label': 'Large Language Model'}, {'text': 'Chain-of-Thought reasoning', 'label': 'Chain of thought'}, {'text': 'contextual coherence', 'label': 'Chain of thought'}, {'text': 'LRMs', 'label': 'Large Language Model'}, {'text': 'LRMs', 'label': 'Large Language Model'}, {'text': 'LRMs', 'label': 'Large Language Model'}, {'text': 'LRMs', 'label': 'Large Language Model'}, {'text': 'LRMs', 'label': 'Large Language Model'}, {'text': 'LRMs', 'label': 'Large Language Model'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain-of-Thought reasoning","similarity_score":0.8011320829}
{"id":2503.10627,"submitter":"Ziyu Guo","authors":"Ziyu Guo, Ray Zhang, Hao Chen, Jialin Gao, Dongzhi Jiang, Jiaze Wang,\n  Pheng-Ann Heng","title":"SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of\n  LMMs on Multi-modal Scientific Problems","comments":"Initially released in September 2024. Project page:\n  https:\/\/sciverse-cuhk.github.io","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The rapid advancement of Large Multi-modal Models (LMMs) has enabled their\napplication in scientific problem-solving, yet their fine-grained capabilities\nremain under-explored. In this paper, we introduce SciVerse, a multi-modal\nscientific evaluation benchmark to thoroughly assess LMMs across 5,735 test\ninstances in five distinct versions. We aim to investigate three key dimensions\nof LMMs: scientific knowledge comprehension, multi-modal content\ninterpretation, and Chain-of-Thought (CoT) reasoning. To unveil whether LMMs\npossess sufficient scientific expertise, we first transform each problem into\nthree versions containing different levels of knowledge required for solving,\ni.e., Knowledge-free, -lite, and -rich. Then, to explore how LMMs interpret\nmulti-modal scientific content, we annotate another two versions, i.e.,\nVision-rich and -only, marking more question information from texts to\ndiagrams. Comparing the results of different versions, SciVerse systematically\nexamines the professional knowledge stock and visual perception skills of LMMs\nin scientific domains. In addition, to rigorously assess CoT reasoning, we\npropose a new scientific CoT evaluation strategy, conducting a step-wise\nassessment on knowledge and logical errors in model outputs. Our extensive\nevaluation of different LMMs on SciVerse reveals critical limitations in their\nscientific proficiency and provides new insights into future developments.\nProject page: https:\/\/sciverse-cuhk.github.io\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:59:32 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Guo', 'Ziyu', ''], ['Zhang', 'Ray', ''], ['Chen', 'Hao', ''], ['Gao', 'Jialin', ''], ['Jiang', 'Dongzhi', ''], ['Wang', 'Jiaze', ''], ['Heng', 'Pheng-Ann', '']]","extracted_entities":"[{'text': 'Large Multi-modal Models', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'Chain-of-Thought (CoT) reasoning', 'label': 'Chain of thought'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'CoT reasoning', 'label': 'Chain of thought'}, {'text': 'LMMs', 'label': 'Large Language Model'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain-of-Thought (CoT) reasoning","similarity_score":0.6324658394}
{"id":2503.10628,"submitter":"Tianjiao Yu","authors":"Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh\n  Juvekar, Tal August, Ismini Lourentzou","title":"Uncertainty in Action: Confidence Elicitation in Embodied Agents","comments":"Project page: https:\/\/plan-lab.github.io\/ece\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Expressing confidence is challenging for embodied agents navigating dynamic\nmultimodal environments, where uncertainty arises from both perception and\ndecision-making processes. We present the first work investigating embodied\nconfidence elicitation in open-ended multimodal environments. We introduce\nElicitation Policies, which structure confidence assessment across inductive,\ndeductive, and abductive reasoning, along with Execution Policies, which\nenhance confidence calibration through scenario reinterpretation, action\nsampling, and hypothetical reasoning. Evaluating agents in calibration and\nfailure prediction tasks within the Minecraft environment, we show that\nstructured reasoning approaches, such as Chain-of-Thoughts, improve confidence\ncalibration. However, our findings also reveal persistent challenges in\ndistinguishing uncertainty, particularly under abductive settings, underscoring\nthe need for more sophisticated embodied confidence elicitation methods.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:59:41 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Yu', 'Tianjiao', ''], ['Shah', 'Vedant', ''], ['Wahed', 'Muntasir', ''], ['Nguyen', 'Kiet A.', ''], ['Juvekar', 'Adheesh', ''], ['August', 'Tal', ''], ['Lourentzou', 'Ismini', '']]","extracted_entities":"[{'text': 'Chain-of-Thoughts', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain-of-Thoughts","similarity_score":0.911457181}
{"id":2503.10639,"submitter":"Rongyao Fang","authors":"Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin\n  Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Xihui Liu, Hongsheng Li","title":"GoT: Unleashing Reasoning Capability of Multimodal Large Language Model\n  for Visual Generation and Editing","comments":"Dataset and models are released in https:\/\/github.com\/rongyaofang\/GoT","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Current image generation and editing methods primarily process textual\nprompts as direct inputs without reasoning about visual composition and\nexplicit operations. We present Generation Chain-of-Thought (GoT), a novel\nparadigm that enables generation and editing through an explicit language\nreasoning process before outputting images. This approach transforms\nconventional text-to-image generation and editing into a reasoning-guided\nframework that analyzes semantic relationships and spatial arrangements. We\ndefine the formulation of GoT and construct large-scale GoT datasets containing\nover 9M samples with detailed reasoning chains capturing semantic-spatial\nrelationships. To leverage the advantages of GoT, we implement a unified\nframework that integrates Qwen2.5-VL for reasoning chain generation with an\nend-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance\nModule. Experiments show our GoT framework achieves excellent performance on\nboth generation and editing tasks, with significant improvements over\nbaselines. Additionally, our approach enables interactive visual generation,\nallowing users to explicitly modify reasoning steps for precise image\nadjustments. GoT pioneers a new direction for reasoning-driven visual\ngeneration and editing, producing images that better align with human intent.\nTo facilitate future research, we make our datasets, code, and pretrained\nmodels publicly available at https:\/\/github.com\/rongyaofang\/GoT.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:59:59 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Fang', 'Rongyao', ''], ['Duan', 'Chengqi', ''], ['Wang', 'Kun', ''], ['Huang', 'Linjiang', ''], ['Li', 'Hao', ''], ['Yan', 'Shilin', ''], ['Tian', 'Hao', ''], ['Zeng', 'Xingyu', ''], ['Zhao', 'Rui', ''], ['Dai', 'Jifeng', ''], ['Liu', 'Xihui', ''], ['Li', 'Hongsheng', '']]","extracted_entities":"[{'text': 'textual\\nprompts', 'label': 'Prompting'}, {'text': 'Generation Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'GoT', 'label': 'Chain of thought'}, {'text': 'GoT', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"Generation Chain-of-Thought","similarity_score":0.7672333717}
{"id":2401.16796,"submitter":"Weibin Liao","authors":"Weibin Liao, Yinghao Zhu, Zhongji Zhang, Yuhang Wang, Zixiang Wang, Xu\n  Chu, Yasha Wang, Liantao Ma","title":"Learnable Prompt as Pseudo-Imputation: Rethinking the Necessity of\n  Traditional EHR Data Imputation in Downstream Clinical Prediction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Analyzing the health status of patients based on Electronic Health Records\n(EHR) is a fundamental research problem in medical informatics. The presence of\nextensive missing values in EHR makes it challenging for deep neural networks\n(DNNs) to directly model the patient's health status. Existing DNNs training\nprotocols, including Impute-then-Regress Procedure and Jointly Optimizing of\nImpute-n-Regress Procedure, require the additional imputation models to\nreconstruction missing values. However, Impute-then-Regress Procedure\nintroduces the risk of injecting imputed, non-real data into downstream\nclinical prediction tasks, resulting in power loss, biased estimation, and\npoorly performing models, while Jointly Optimizing of Impute-n-Regress\nProcedure is also difficult to generalize due to the complex optimization space\nand demanding data requirements. Inspired by the recent advanced literature of\nlearnable prompt in the fields of NLP and CV, in this work, we rethought the\nnecessity of the imputation model in downstream clinical tasks, and proposed\nLearnable Prompt as Pseudo-Imputation (PAI) as a new training protocol to\nassist EHR analysis. PAI no longer introduces any imputed data but constructs a\nlearnable prompt to model the implicit preferences of the downstream model for\nmissing values, resulting in a significant performance improvement for all\nstate-of-the-arts EHR analysis models on four real-world datasets across two\nclinical prediction tasks. Further experimental analysis indicates that PAI\nexhibits higher robustness in situations of data insufficiency and high missing\nrates. More importantly, as a plug-and-play protocol, PAI can be easily\nintegrated into any existing or even imperceptible future EHR analysis models.\n","versions":"[{'version': 'v1', 'created': 'Tue, 30 Jan 2024 07:19:36 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 06:17:29 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Liao', 'Weibin', ''], ['Zhu', 'Yinghao', ''], ['Zhang', 'Zhongji', ''], ['Wang', 'Yuhang', ''], ['Wang', 'Zixiang', ''], ['Chu', 'Xu', ''], ['Wang', 'Yasha', ''], ['Ma', 'Liantao', '']]","extracted_entities":"[{'text': 'Jointly Optimizing of\\nImpute-n-Regress Procedure', 'label': 'Prompting'}, {'text': 'learnable prompt', 'label': 'Prompting'}, {'text': 'Learnable Prompt', 'label': 'Prompting'}, {'text': 'PAI', 'label': 'Prompting'}, {'text': 'PAI', 'label': 'Prompting'}, {'text': 'learnable prompt', 'label': 'Prompting'}, {'text': 'PAI', 'label': 'Prompting'}, {'text': 'PAI', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"learnable prompt","similarity_score":0.6185894608}
{"id":2409.06214,"submitter":"Kim Jaewoo","authors":"Jaewoo Kim, Uehwan Kim","title":"Towards Generalizable Scene Change Detection","comments":"Camera-ready version. Accepted to CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  While current state-of-the-art Scene Change Detection (SCD) approaches\nachieve impressive results in well-trained research data, they become\nunreliable under unseen environments and different temporal conditions;\nin-domain performance drops from 77.6% to 8.0% in a previously unseen\nenvironment and to 4.6% under a different temporal condition -- calling for\ngeneralizable SCD and benchmark. In this work, we propose the Generalizable\nScene Change Detection Framework (GeSCF), which addresses unseen domain\nperformance and temporal consistency -- to meet the growing demand for anything\nSCD. Our method leverages the pre-trained Segment Anything Model (SAM) in a\nzero-shot manner. For this, we design Initial Pseudo-mask Generation and\nGeometric-Semantic Mask Matching -- seamlessly turning user-guided prompt and\nsingle-image based segmentation into scene change detection for a pair of\ninputs without guidance. Furthermore, we define the Generalizable Scene Change\nDetection (GeSCD) benchmark along with novel metrics and an evaluation protocol\nto facilitate SCD research in generalizability. In the process, we introduce\nthe ChangeVPR dataset, a collection of challenging image pairs with diverse\nenvironmental scenarios -- including urban, suburban, and rural settings.\nExtensive experiments across various datasets demonstrate that GeSCF achieves\nan average performance gain of 19.2% on existing SCD datasets and 30.0% on the\nChangeVPR dataset, nearly doubling the prior art performance. We believe our\nwork can lay a solid foundation for robust and generalizable SCD research.\n","versions":"[{'version': 'v1', 'created': 'Tue, 10 Sep 2024 04:45:25 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Feb 2025 05:28:05 GMT'}, {'version': 'v3', 'created': 'Mon, 3 Mar 2025 01:46:42 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 13:55:30 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Kim', 'Jaewoo', ''], ['Kim', 'Uehwan', '']]","extracted_entities":"[{'text': 'user-guided prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"user-guided prompt","similarity_score":0.7089807987}
{"id":2409.2056,"submitter":"Jiachen Li","authors":"Xiaopan Zhang and Hao Qin and Fuquan Wang and Yue Dong and Jiachen Li","title":"LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and\n  Planning with LM-Driven PDDL Planner","comments":"IEEE Conference on Robotics and Automation (ICRA 2025); Project\n  website: https:\/\/lamma-p.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI cs.CV cs.LG cs.MA","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Language models (LMs) possess a strong capability to comprehend natural\nlanguage, making them effective in translating human instructions into detailed\nplans for simple robot tasks. Nevertheless, it remains a significant challenge\nto handle long-horizon tasks, especially in subtask identification and\nallocation for cooperative heterogeneous robot teams. To address this issue, we\npropose a Language Model-Driven Multi-Agent PDDL Planner (LaMMA-P), a novel\nmulti-agent task planning framework that achieves state-of-the-art performance\non long-horizon tasks. LaMMA-P integrates the strengths of the LMs' reasoning\ncapability and the traditional heuristic search planner to achieve a high\nsuccess rate and efficiency while demonstrating strong generalization across\ntasks. Additionally, we create MAT-THOR, a comprehensive benchmark that\nfeatures household tasks with two different levels of complexity based on the\nAI2-THOR environment. The experimental results demonstrate that LaMMA-P\nachieves a 105% higher success rate and 36% higher efficiency than existing\nLM-based multiagent planners. The experimental videos, code, datasets, and\ndetailed prompts used in each module can be found on the project website:\nhttps:\/\/lamma-p.github.io.\n","versions":"[{'version': 'v1', 'created': 'Mon, 30 Sep 2024 17:58:18 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 06:17:58 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhang', 'Xiaopan', ''], ['Qin', 'Hao', ''], ['Wang', 'Fuquan', ''], ['Dong', 'Yue', ''], ['Li', 'Jiachen', '']]","extracted_entities":"[{'text': 'detailed prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"detailed prompts","similarity_score":0.640391469}
{"id":2411.05039,"submitter":"Subhankar Maity","authors":"Aniket Deroy, Subhankar Maity","title":"YouTube Comments Decoded: Leveraging LLMs for Low Resource Language\n  Classification","comments":"Updated and Final Version","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Sarcasm detection is a significant challenge in sentiment analysis,\nparticularly due to its nature of conveying opinions where the intended meaning\ndeviates from the literal expression. This challenge is heightened in social\nmedia contexts where code-mixing, especially in Dravidian languages, is\nprevalent. Code-mixing involves the blending of multiple languages within a\nsingle utterance, often with non-native scripts, complicating the task for\nsystems trained on monolingual data. This shared task introduces a novel gold\nstandard corpus designed for sarcasm and sentiment detection within code-mixed\ntexts, specifically in Tamil-English and Malayalam-English languages. The\nprimary objective of this task is to identify sarcasm and sentiment polarity\nwithin a code-mixed dataset of Tamil-English and Malayalam-English comments and\nposts collected from social media platforms. Each comment or post is annotated\nat the message level for sentiment polarity, with particular attention to the\nchallenges posed by class imbalance, reflecting real-world scenarios.In this\nwork, we experiment with state-of-the-art large language models like GPT-3.5\nTurbo via prompting to classify comments into sarcastic or non-sarcastic\ncategories. We obtained a macro-F1 score of 0.61 for Tamil language. We\nobtained a macro-F1 score of 0.50 for Malayalam language.\n","versions":"[{'version': 'v1', 'created': 'Wed, 6 Nov 2024 17:58:01 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 16:17:21 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Deroy', 'Aniket', ''], ['Maity', 'Subhankar', '']]","extracted_entities":"[{'text': 'prompting', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2501.18883,"submitter":"Jae Yong Lee","authors":"Jae Yong Lee, Sungmin Kang, Shin Yoo","title":"Predictive Prompt Analysis","comments":"Accepted by FSE 2025, 5 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) are machine learning models that have seen\nwidespread adoption due to their capability of handling previously difficult\ntasks. LLMs, due to their training, are sensitive to how exactly a question is\npresented, also known as prompting. However, prompting well is challenging, as\nit has been difficult to uncover principles behind prompting -- generally,\ntrial-and-error is the most common way of improving prompts, despite its\nsignificant computational cost. In this context, we argue it would be useful to\nperform `predictive prompt analysis', in which an automated technique would\nperform a quick analysis of a prompt and predict how the LLM would react to it,\nrelative to a goal provided by the user. As a demonstration of the concept, we\npresent Syntactic Prevalence Analyzer (SPA), a predictive prompt analysis\napproach based on sparse autoencoders (SAEs). SPA accurately predicted how\noften an LLM would generate target syntactic structures during code synthesis,\nwith up to 0.994 Pearson correlation between the predicted and actual\nprevalence of the target structure. At the same time, SPA requires only 0.4\\%\nof the time it takes to run the LLM on a benchmark. As LLMs are increasingly\nused during and integrated into modern software development, our proposed\npredictive prompt analysis concept has the potential to significantly ease the\nuse of LLMs for both practitioners and researchers.\n","versions":"[{'version': 'v1', 'created': 'Fri, 31 Jan 2025 04:34:43 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 07:23:59 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Lee', 'Jae Yong', ''], ['Kang', 'Sungmin', ''], ['Yoo', 'Shin', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2502.06432,"submitter":"Huaqiu Li","authors":"Huaqiu Li, Wang Zhang, Xiaowan Hu, Tao Jiang, Zikang Chen, Haoqian\n  Wang","title":"Prompt-SID: Learning Structural Representation Prompt via Latent\n  Diffusion for Single-Image Denoising","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Many studies have concentrated on constructing supervised models utilizing\npaired datasets for image denoising, which proves to be expensive and\ntime-consuming. Current self-supervised and unsupervised approaches typically\nrely on blind-spot networks or sub-image pairs sampling, resulting in pixel\ninformation loss and destruction of detailed structural information, thereby\nsignificantly constraining the efficacy of such methods. In this paper, we\nintroduce Prompt-SID, a prompt-learning-based single image denoising framework\nthat emphasizes preserving of structural details. This approach is trained in a\nself-supervised manner using downsampled image pairs. It captures\noriginal-scale image information through structural encoding and integrates\nthis prompt into the denoiser. To achieve this, we propose a structural\nrepresentation generation model based on the latent diffusion process and\ndesign a structural attention module within the transformer-based denoiser\narchitecture to decode the prompt. Additionally, we introduce a scale replay\ntraining mechanism, which effectively mitigates the scale gap from images of\ndifferent resolutions. We conduct comprehensive experiments on synthetic,\nreal-world, and fluorescence imaging datasets, showcasing the remarkable\neffectiveness of Prompt-SID. Our code will be released at\nhttps:\/\/github.com\/huaqlili\/Prompt-SID.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Feb 2025 13:09:47 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 12:49:20 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Huaqiu', ''], ['Zhang', 'Wang', ''], ['Hu', 'Xiaowan', ''], ['Jiang', 'Tao', ''], ['Chen', 'Zikang', ''], ['Wang', 'Haoqian', '']]","extracted_entities":"[{'text': 'prompt', 'label': 'Prompting'}, {'text': 'structural attention module', 'label': 'Attention mechanism'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'scale replay\\ntraining mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Prompting","matched_keyword":"prompt","similarity_score":0.7767513394}
{"id":2502.19363,"submitter":"Ru Peng","authors":"Ru Peng, Kexin Yang, Yawen Zeng, Junyang Lin, Dayiheng Liu, Junbo Zhao","title":"DataMan: Data Manager for Pre-training Large Language Models","comments":"ICLR2025 paper","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The performance emergence of large language models (LLMs) driven by data\nscaling laws makes the selection of pre-training data increasingly important.\nHowever, existing methods rely on limited heuristics and human intuition,\nlacking comprehensive and clear guidelines. To address this, we are inspired by\n``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit\nits performance. As its pre-training capabilities are related to perplexity\n(PPL), we derive 14 quality criteria from the causes of text perplexity\nanomalies and introduce 15 common application domains to support domain mixing.\nIn this paper, we train a Data Manager (DataMan) to learn quality ratings and\ndomain recognition from pointwise rating, and use it to annotate a 447B token\npre-training corpus with 14 quality ratings and domain type. Our experiments\nvalidate our approach, using DataMan to select 30B tokens to train a\n1.3B-parameter language model, demonstrating significant improvements in\nin-context learning (ICL), perplexity, and instruction-following ability over\nthe state-of-the-art baseline. The best-performing model, based on the Overall\nScore l=5 surpasses a model trained with 50% more data using uniform sampling.\nWe continue pre-training with high-rated, domain-specific data annotated by\nDataMan to enhance domain-specific ICL performance and thus verify DataMan's\ndomain mixing ability. Our findings emphasize the importance of quality\nranking, the complementary nature of quality criteria, and their low\ncorrelation with perplexity, analyzing misalignment between PPL and ICL\nperformance. We also thoroughly analyzed our pre-training dataset, examining\nits composition, the distribution of quality ratings, and the original document\nsources.\n","versions":"[{'version': 'v1', 'created': 'Wed, 26 Feb 2025 18:01:19 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 15:42:07 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Peng', 'Ru', ''], ['Yang', 'Kexin', ''], ['Zeng', 'Yawen', ''], ['Lin', 'Junyang', ''], ['Liu', 'Dayiheng', ''], ['Zhao', 'Junbo', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'data\\nscaling laws', 'label': 'Scaling law'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2503.08421,"submitter":"Qiming Xia","authors":"Qiming Xia, Wenkai Lin, Haoen Xiang, Xun Huang, Siheng Chen, Zhen\n  Dong, Cheng Wang, Chenglu Wen","title":"Learning to Detect Objects from Multi-Agent LiDAR Scans without Manual\n  Labels","comments":"11 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Unsupervised 3D object detection serves as an important solution for offline\n3D object annotation. However, due to the data sparsity and limited views, the\nclustering-based label fitting in unsupervised object detection often generates\nlow-quality pseudo-labels. Multi-agent collaborative dataset, which involves\nthe sharing of complementary observations among agents, holds the potential to\nbreak through this bottleneck. In this paper, we introduce a novel unsupervised\nmethod that learns to Detect Objects from Multi-Agent LiDAR scans, termed DOtA,\nwithout using labels from external. DOtA first uses the internally shared\nego-pose and ego-shape of collaborative agents to initialize the detector,\nleveraging the generalization performance of neural networks to infer\npreliminary labels. Subsequently,DOtA uses the complementary observations\nbetween agents to perform multi-scale encoding on preliminary labels, then\ndecodes high-quality and low-quality labels. These labels are further used as\nprompts to guide a correct feature learning process, thereby enhancing the\nperformance of the unsupervised object detection task. Extensive experiments on\nthe V2V4Real and OPV2V datasets show that our DOtA outperforms state-of-the-art\nunsupervised 3D object detection methods. Additionally, we also validate the\neffectiveness of the DOtA labels under various collaborative perception\nframeworks.The code is available at https:\/\/github.com\/xmuqimingxia\/DOtA.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 13:34:35 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 01:41:04 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Xia', 'Qiming', ''], ['Lin', 'Wenkai', ''], ['Xiang', 'Haoen', ''], ['Huang', 'Xun', ''], ['Chen', 'Siheng', ''], ['Dong', 'Zhen', ''], ['Wang', 'Cheng', ''], ['Wen', 'Chenglu', '']]","extracted_entities":"[{'text': 'prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompts","similarity_score":0.7638334036}
{"id":2503.08434,"submitter":"Armando Fortes","authors":"Armando Fortes, Tianyi Wei, Shangchen Zhou, Xingang Pan","title":"Bokeh Diffusion: Defocus Blur Control in Text-to-Image Diffusion Models","comments":"Project page: https:\/\/atfortes.github.io\/projects\/bokeh-diffusion\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GR cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent advances in large-scale text-to-image models have revolutionized\ncreative fields by generating visually captivating outputs from textual\nprompts; however, while traditional photography offers precise control over\ncamera settings to shape visual aesthetics -- such as depth-of-field -- current\ndiffusion models typically rely on prompt engineering to mimic such effects.\nThis approach often results in crude approximations and inadvertently altering\nthe scene content. In this work, we propose Bokeh Diffusion, a scene-consistent\nbokeh control framework that explicitly conditions a diffusion model on a\nphysical defocus blur parameter. By grounding depth-of-field adjustments, our\nmethod preserves the underlying scene structure as the level of blur is varied.\nTo overcome the scarcity of paired real-world images captured under different\ncamera settings, we introduce a hybrid training pipeline that aligns\nin-the-wild images with synthetic blur augmentations. Extensive experiments\ndemonstrate that our approach not only achieves flexible, lens-like blur\ncontrol but also supports applications such as real image editing via\ninversion.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 13:49:12 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 08:41:47 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Fortes', 'Armando', ''], ['Wei', 'Tianyi', ''], ['Zhou', 'Shangchen', ''], ['Pan', 'Xingang', '']]","extracted_entities":"[{'text': 'textual\\nprompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"textual\nprompts","similarity_score":0.6302489042}
{"id":2503.08741,"submitter":"Letian Zhang","authors":"Letian Zhang, Quan Cui, Bingchen Zhao, Cheng Yang","title":"Oasis: One Image is All You Need for Multimodal Instruction Data\n  Synthesis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The success of multi-modal large language models (MLLMs) has been largely\nattributed to the large-scale training data. However, the training data of many\nMLLMs is unavailable due to privacy concerns. The expensive and labor-intensive\nprocess of collecting multi-modal data further exacerbates the problem. Is it\npossible to synthesize multi-modal training data automatically without\ncompromising diversity and quality? In this paper, we propose a new method,\nOasis, to synthesize high-quality multi-modal data with only images. Oasis\nbreaks through traditional methods by prompting only images to the MLLMs, thus\nextending the data diversity by a large margin. Our method features a delicate\nquality control method which ensures the data quality. We collected over 500k\ndata and conducted incremental experiments on LLaVA-NeXT. Extensive experiments\ndemonstrate that our method can significantly improve the performance of MLLMs.\nThe image-based synthesis also allows us to focus on the specific-domain\nability of MLLMs. Code and data will be publicly available.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 08:25:40 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 06:15:32 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhang', 'Letian', ''], ['Cui', 'Quan', ''], ['Zhao', 'Bingchen', ''], ['Yang', 'Cheng', '']]","extracted_entities":"[{'text': 'multi-modal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'privacy concerns', 'label': 'AI Ethics'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2503.10037,"submitter":"Sina Malakouti","authors":"Sina Malakouti and Adriana Kovashka","title":"Investigating and Improving Counter-Stereotypical Action Relation in\n  Text-to-Image Diffusion Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Text-to-image diffusion models consistently fail at generating\ncounter-stereotypical action relationships (e.g., \"mouse chasing cat\"),\ndefaulting to frequent stereotypes even when explicitly prompted otherwise.\nThrough systematic investigation, we discover this limitation stems from\ndistributional biases rather than inherent model constraints. Our key insight\nreveals that while models fail on rare compositions when their inversions are\ncommon, they can successfully generate similar intermediate compositions (e.g.,\n\"mouse chasing boy\"). To test this hypothesis, we develop a Role-Bridging\nDecomposition framework that leverages these intermediates to gradually teach\nrare relationships without architectural modifications. We introduce\nActionBench, a comprehensive benchmark specifically designed to evaluate\naction-based relationship generation across stereotypical and\ncounter-stereotypical configurations. Our experiments validate that\nintermediate compositions indeed facilitate counter-stereotypical generation,\nwith both automatic metrics and human evaluations showing significant\nimprovements over existing approaches. This work not only identifies\nfundamental biases in current text-to-image systems but demonstrates a\npromising direction for addressing them through compositional reasoning.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 04:38:02 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Malakouti', 'Sina', ''], ['Kovashka', 'Adriana', '']]","extracted_entities":"[{'text': 'explicitly prompted', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"explicitly prompted","similarity_score":0.7715600133}
{"id":2503.10081,"submitter":"Joonsung Jeon","authors":"Joonsung Jeon, Woo Jae Kim, Suhyeon Ha, Sooel Son, Sung-eui Yoon","title":"AdvPaint: Protecting Images from Inpainting Manipulation via Adversarial\n  Attention Disruption","comments":"Accepted to ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CR","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  The outstanding capability of diffusion models in generating high-quality\nimages poses significant threats when misused by adversaries. In particular, we\nassume malicious adversaries exploiting diffusion models for inpainting tasks,\nsuch as replacing a specific region with a celebrity. While existing methods\nfor protecting images from manipulation in diffusion-based generative models\nhave primarily focused on image-to-image and text-to-image tasks, the challenge\nof preventing unauthorized inpainting has been rarely addressed, often\nresulting in suboptimal protection performance. To mitigate inpainting abuses,\nwe propose ADVPAINT, a novel defensive framework that generates adversarial\nperturbations that effectively disrupt the adversary's inpainting tasks.\nADVPAINT targets the self- and cross-attention blocks in a target diffusion\ninpainting model to distract semantic understanding and prompt interactions\nduring image generation. ADVPAINT also employs a two-stage perturbation\nstrategy, dividing the perturbation region based on an enlarged bounding box\naround the object, enhancing robustness across diverse masks of varying shapes\nand sizes. Our experimental results demonstrate that ADVPAINT's perturbations\nare highly effective in disrupting the adversary's inpainting tasks,\noutperforming existing methods; ADVPAINT attains over a 100-point increase in\nFID and substantial decreases in precision.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 06:05:40 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Jeon', 'Joonsung', ''], ['Kim', 'Woo Jae', ''], ['Ha', 'Suhyeon', ''], ['Son', 'Sooel', ''], ['Yoon', 'Sung-eui', '']]","extracted_entities":"[{'text': 'prompt interactions', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt interactions","similarity_score":0.6690463424}
{"id":2503.10127,"submitter":"Runze He","authors":"Runze He, Bo Cheng, Yuhang Ma, Qingxiang Jia, Shanyuan Liu, Ao Ma,\n  Xiaoyu Wu, Liebucha Wu, Dawei Leng, Yuhui Yin","title":"PlanGen: Towards Unified Layout Planning and Image Generation in\n  Auto-Regressive Vision Language Models","comments":"15 pages, 12 figures, project page:\n  https:\/\/360cvgroup.github.io\/PlanGen","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper, we propose a unified layout planning and image generation\nmodel, PlanGen, which can pre-plan spatial layout conditions before generating\nimages. Unlike previous diffusion-based models that treat layout planning and\nlayout-to-image as two separate models, PlanGen jointly models the two tasks\ninto one autoregressive transformer using only next-token prediction. PlanGen\nintegrates layout conditions into the model as context without requiring\nspecialized encoding of local captions and bounding box coordinates, which\nprovides significant advantages over the previous embed-and-pool operations on\nlayout conditions, particularly when dealing with complex layouts. Unified\nprompting allows PlanGen to perform multitasking training related to layout,\nincluding layout planning, layout-to-image generation, image layout\nunderstanding, etc. In addition, PlanGen can be seamlessly expanded to\nlayout-guided image manipulation thanks to the well-designed modeling, with\nteacher-forcing content manipulation policy and negative layout guidance.\nExtensive experiments verify the effectiveness of our PlanGen in multiple\nlayoutrelated tasks, showing its great potential. Code is available at:\nhttps:\/\/360cvgroup.github.io\/PlanGen.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 07:37:09 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['He', 'Runze', ''], ['Cheng', 'Bo', ''], ['Ma', 'Yuhang', ''], ['Jia', 'Qingxiang', ''], ['Liu', 'Shanyuan', ''], ['Ma', 'Ao', ''], ['Wu', 'Xiaoyu', ''], ['Wu', 'Liebucha', ''], ['Leng', 'Dawei', ''], ['Yin', 'Yuhui', '']]","extracted_entities":"[{'text': 'Unified\\nprompting', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"Unified\nprompting","similarity_score":0.7852942944}
{"id":2503.10229,"submitter":"Andreas Spitz","authors":"Julian Schelb, Orr Borin, David Garcia, Andreas Spitz","title":"R.U.Psycho? Robust Unified Psychometric Testing of Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Generative language models are increasingly being subjected to psychometric\nquestionnaires intended for human testing, in efforts to establish their\ntraits, as benchmarks for alignment, or to simulate participants in social\nscience experiments. While this growing body of work sheds light on the\nlikeness of model responses to those of humans, concerns are warranted\nregarding the rigour and reproducibility with which these experiments may be\nconducted. Instabilities in model outputs, sensitivity to prompt design,\nparameter settings, and a large number of available model versions increase\ndocumentation requirements. Consequently, generalization of findings is often\ncomplex and reproducibility is far from guaranteed. In this paper, we present\nR.U.Psycho, a framework for designing and running robust and reproducible\npsychometric experiments on generative language models that requires limited\ncoding expertise. We demonstrate the capability of our framework on a variety\nof psychometric questionnaires, which lend support to prior findings in the\nliterature. R.U.Psycho is available as a Python package at\nhttps:\/\/github.com\/julianschelb\/rupsycho.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 10:12:34 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Schelb', 'Julian', ''], ['Borin', 'Orr', ''], ['Garcia', 'David', ''], ['Spitz', 'Andreas', '']]","extracted_entities":"[{'text': 'prompt design', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt design","similarity_score":0.624378264}
{"id":2503.10305,"submitter":"Emil Mededovic","authors":"Emil Mededovic, Yuli Wu, Henning Konermann, Marcin Kopaczka, Mareike\n  Schulz, Rene Tolba, Johannes Stegmaier","title":"Eye on the Target: Eye Tracking Meets Rodent Tracking","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Analyzing animal behavior from video recordings is crucial for scientific\nresearch, yet manual annotation remains labor-intensive and prone to\nsubjectivity. Efficient segmentation methods are needed to automate this\nprocess while maintaining high accuracy. In this work, we propose a novel\npipeline that utilizes eye-tracking data from Aria glasses to generate prompt\npoints, which are then used to produce segmentation masks via a fast zero-shot\nsegmentation model. Additionally, we apply post-processing to refine the\nprompts, leading to improved segmentation quality. Through our approach, we\ndemonstrate that combining eye-tracking-based annotation with smart prompt\nrefinement can enhance segmentation accuracy, achieving an improvement of 70.6%\nfrom 38.8 to 66.2 in the Jaccard Index for segmentation results in the rats\ndataset.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 12:27:42 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Mededovic', 'Emil', ''], ['Wu', 'Yuli', ''], ['Konermann', 'Henning', ''], ['Kopaczka', 'Marcin', ''], ['Schulz', 'Mareike', ''], ['Tolba', 'Rene', ''], ['Stegmaier', 'Johannes', '']]","extracted_entities":"[{'text': 'prompt\\npoints', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt\npoints","similarity_score":0.6149590611}
{"id":2503.10512,"submitter":"Devjeet Roy","authors":"Hooman Shahrokhi, Devjeet Raj Roy, Yan Yan, Venera Arnaoudova and\n  Janaradhan Rao Doppa","title":"Conformal Prediction Sets for Deep Generative Models via Reduction to\n  Conformal Regression","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  We consider the problem of generating valid and small prediction sets by\nsampling outputs (e.g., software code and natural language text) from a\nblack-box deep generative model for a given input (e.g., textual prompt). The\nvalidity of a prediction set is determined by a user-defined binary\nadmissibility function depending on the target application. For example,\nrequiring at least one program in the set to pass all test cases in code\ngeneration application. To address this problem, we develop a simple and\neffective conformal inference algorithm referred to as Generative Prediction\nSets (GPS). Given a set of calibration examples and black-box access to a deep\ngenerative model, GPS can generate prediction sets with provable guarantees.\nThe key insight behind GPS is to exploit the inherent structure within the\ndistribution over the minimum number of samples needed to obtain an admissible\noutput to develop a simple conformal regression approach over the minimum\nnumber of samples. Experiments on multiple datasets for code and math word\nproblems using different large language models demonstrate the efficacy of GPS\nover state-of-the-art methods.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 16:16:23 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Shahrokhi', 'Hooman', ''], ['Roy', 'Devjeet Raj', ''], ['Yan', 'Yan', ''], ['Arnaoudova', 'Venera', ''], ['Doppa', 'Janaradhan Rao', '']]","extracted_entities":"[{'text': 'textual prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"textual prompt","similarity_score":0.6434890032}
{"id":2503.10544,"submitter":"Jing Xu","authors":"Jing Xu, Franziska Boenisch, Iyiola Emmanuel Olatunji, and Adam\n  Dziedzic","title":"DP-GPL: Differentially Private Graph Prompt Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Graph Neural Networks (GNNs) have shown remarkable performance in various\napplications. Recently, graph prompt learning has emerged as a powerful GNN\ntraining paradigm, inspired by advances in language and vision foundation\nmodels. Here, a GNN is pre-trained on public data and then adapted to sensitive\ntasks using lightweight graph prompts. However, using prompts from sensitive\ndata poses privacy risks. In this work, we are the first to investigate these\npractical risks in graph prompts by instantiating a membership inference attack\nthat reveals significant privacy leakage. We also find that the standard\nprivacy method, DP-SGD, fails to provide practical privacy-utility trade-offs\nin graph prompt learning, likely due to the small number of sensitive data\npoints used to learn the prompts. As a solution, we propose DP-GPL for\ndifferentially private graph prompt learning based on the PATE framework, that\ngenerates a graph prompt with differential privacy guarantees. Our evaluation\nacross various graph prompt learning methods, GNN architectures, and\npre-training strategies demonstrates that our algorithm achieves high utility\nat strong privacy, effectively mitigating privacy concerns while preserving the\npowerful capabilities of prompted GNNs as powerful foundation models in the\ngraph domain.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 16:58:07 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Xu', 'Jing', ''], ['Boenisch', 'Franziska', ''], ['Olatunji', 'Iyiola Emmanuel', ''], ['Dziedzic', 'Adam', '']]","extracted_entities":"[{'text': 'graph prompt learning', 'label': 'Prompting'}, {'text': 'graph prompts', 'label': 'Prompting'}, {'text': 'graph prompts', 'label': 'Prompting'}, {'text': 'graph prompt learning', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'graph prompt learning', 'label': 'Prompting'}, {'text': 'graph prompt learning', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompts","similarity_score":0.7638334036}
{"id":2503.10586,"submitter":"Chaoqun Wang","authors":"Chaoqun Wang, Jie Yang, Xiaobin Hong, and Ruimao Zhang","title":"Unlock the Power of Unlabeled Data in Language Driving Model","comments":"Accepted by ICRA2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent Vision-based Large Language Models~(VisionLLMs) for autonomous driving\nhave seen rapid advancements. However, such promotion is extremely dependent on\nlarge-scale high-quality annotated data, which is costly and labor-intensive.\nTo address this issue, we propose unlocking the value of abundant yet unlabeled\ndata to improve the language-driving model in a semi-supervised learning\nmanner. Specifically, we first introduce a series of template-based prompts to\nextract scene information, generating questions that create pseudo-answers for\nthe unlabeled data based on a model trained with limited labeled data. Next, we\npropose a Self-Consistency Refinement method to improve the quality of these\npseudo-annotations, which are later used for further training. By utilizing a\npre-trained VisionLLM (e.g., InternVL), we build a strong Language Driving\nModel (LDM) for driving scene question-answering, outperforming previous\nstate-of-the-art methods. Extensive experiments on the DriveLM benchmark show\nthat our approach performs well with just 5% labeled data, achieving\ncompetitive performance against models trained with full datasets. In\nparticular, our LDM achieves 44.85% performance with limited labeled data,\nincreasing to 54.27% when using unlabeled data, while models trained with full\ndatasets reach 60.68% on the DriveLM benchmark.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:36:36 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Chaoqun', ''], ['Yang', 'Jie', ''], ['Hong', 'Xiaobin', ''], ['Zhang', 'Ruimao', '']]","extracted_entities":"[{'text': 'template-based prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"template-based prompts","similarity_score":0.540751338}
{"id":2503.10634,"submitter":"Junkun Chen","authors":"Yanming Zhang, Jun-Kun Chen, Jipeng Lyu, Yu-Xiong Wang","title":"V2Edit: Versatile Video Diffusion Editor for Videos and 3D Scenes","comments":"Project Website: https:\/\/immortalco.github.io\/V2Edit\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper introduces V$^2$Edit, a novel training-free framework for\ninstruction-guided video and 3D scene editing. Addressing the critical\nchallenge of balancing original content preservation with editing task\nfulfillment, our approach employs a progressive strategy that decomposes\ncomplex editing tasks into a sequence of simpler subtasks. Each subtask is\ncontrolled through three key synergistic mechanisms: the initial noise, noise\nadded at each denoising step, and cross-attention maps between text prompts and\nvideo content. This ensures robust preservation of original video elements\nwhile effectively applying the desired edits. Beyond its native video editing\ncapability, we extend V$^2$Edit to 3D scene editing via a\n\"render-edit-reconstruct\" process, enabling high-quality, 3D-consistent edits\neven for tasks involving substantial geometric changes such as object\ninsertion. Extensive experiments demonstrate that our V$^2$Edit achieves\nhigh-quality and successful edits across various challenging video editing\ntasks and complex 3D scene editing tasks, thereby establishing state-of-the-art\nperformance in both domains.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:59:55 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhang', 'Yanming', ''], ['Chen', 'Jun-Kun', ''], ['Lyu', 'Jipeng', ''], ['Wang', 'Yu-Xiong', '']]","extracted_entities":"[{'text': 'cross-attention maps', 'label': 'Attention mechanism'}, {'text': 'text prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"text prompts","similarity_score":0.6106933355}
{"id":2410.06215,"submitter":"Zaid Khan","authors":"Zaid Khan, Elias Stengel-Eskin, Jaemin Cho, Mohit Bansal","title":"DataEnvGym: Data Generation Agents in Teacher Environments with Student\n  Feedback","comments":"ICLR 2025 Spotlight; Project Page: https:\/\/DataEnvGym.github.io","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The process of creating training data to teach models is currently driven by\nhumans, who manually analyze model weaknesses and plan how to create data that\nimproves a student model. Approaches using LLMs as annotators reduce human\neffort, but still require humans to interpret feedback from evaluations and\ncontrol the LLM to produce data the student needs. Automating this\nlabor-intensive process by creating autonomous data generation agents - or\nteachers - is desirable, but requires environments that can simulate the\nfeedback-driven, iterative, closed loop of data creation. To enable rapid,\nscalable testing for such agents and their modules, we introduce DataEnvGym, a\ntestbed of teacher environments for data generation agents. DataEnvGym frames\ndata generation as a sequential decision-making task, involving an agent\nconsisting of a data generation policy (which generates a plan for creating\ntraining data) and a data generation engine (which transforms the plan into\ndata), inside an environment that provides student feedback. The agent's goal\nis to improve student performance. Students are iteratively trained and\nevaluated on generated data, and their feedback (in the form of errors or weak\nskills) is reported to the agent after each iteration. DataEnvGym includes\nmultiple teacher environment instantiations across 3 levels of structure in the\nstate representation and action space. More structured environments are based\non inferred skills and offer more interpretability and curriculum control. We\nsupport 4 domains (math, code, VQA, and tool-use) and test multiple students\nand teachers. Example agents in our teaching environments can iteratively\nimprove students across tasks and settings. Moreover, we show that environments\nteach different skill levels and test variants of key modules, pointing to\nfuture work in improving data generation agents, engines, and feedback\nmechanisms.\n","versions":"[{'version': 'v1', 'created': 'Tue, 8 Oct 2024 17:20:37 GMT'}, {'version': 'v2', 'created': 'Tue, 17 Dec 2024 18:54:45 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 17:30:48 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Khan', 'Zaid', ''], ['Stengel-Eskin', 'Elias', ''], ['Cho', 'Jaemin', ''], ['Bansal', 'Mohit', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2410.1364,"submitter":"Yiming Wang","authors":"Yiming Wang, Pei Zhang, Baosong Yang, Derek F. Wong, Rui Wang","title":"Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation","comments":"Accepted by ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  LLM self-evaluation relies on the LLM's own ability to estimate response\ncorrectness, which can greatly improve its deployment reliability. In this\nresearch track, we propose the Chain-of-Embedding (CoE) in the latent space to\nenable LLMs to perform output-free self-evaluation. CoE consists of all\nprogressive hidden states produced during the inference time, which can be\ntreated as the latent thinking path of LLMs. We find that when LLMs respond\ncorrectly and incorrectly, their CoE features differ, these discrepancies\nassist us in estimating LLM response correctness. Experiments in four diverse\ndomains and seven LLMs fully demonstrate the effectiveness of our method.\nMeanwhile, its label-free design intent without any training and\nmillisecond-level computational cost ensures real-time feedback in large-scale\nscenarios. More importantly, we provide interesting insights into LLM response\ncorrectness from the perspective of hidden state changes inside LLMs.\n","versions":"[{'version': 'v1', 'created': 'Thu, 17 Oct 2024 15:09:24 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 16:16:12 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Yiming', ''], ['Zhang', 'Pei', ''], ['Yang', 'Baosong', ''], ['Wong', 'Derek F.', ''], ['Wang', 'Rui', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'Chain-of-Embedding', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'CoE', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'CoE', 'label': 'Embedding'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2410.20643,"submitter":"Wilson Wongso","authors":"Wilson Wongso, Hao Xue, Flora D. Salim","title":"GenUP: Generative User Profilers as In-Context Learners for Next POI\n  Recommender Systems","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Traditional Point-of-Interest (POI) recommendation systems often lack\ntransparency, interpretability, and scrutability due to their reliance on dense\nvector-based user embeddings. Furthermore, the cold-start problem -- where\nsystems have insufficient data for new users -- limits their ability to\ngenerate accurate recommendations. Existing methods often address this by\nleveraging similar trajectories from other users, but this approach can be\ncomputationally expensive and increases the context length for LLM-based\nmethods, making them difficult to scale. To address these limitations, we\npropose a method that generates natural language (NL) user profiles from\nlarge-scale, location-based social network (LBSN) check-ins, utilizing robust\npersonality assessments and behavioral theories. These NL profiles capture user\npreferences, routines, and behaviors, improving POI prediction accuracy while\noffering enhanced transparency. By incorporating NL profiles as system prompts\nto LLMs, our approach reduces reliance on extensive historical data, while\nremaining flexible, easily updated, and computationally efficient. Our method\nis not only competitive with other LLM-based and complex agentic frameworks but\nis also more scalable for real-world POI recommender systems. Results\ndemonstrate that our approach consistently outperforms baseline methods,\noffering a more interpretable and resource-efficient solution for POI\nrecommendation systems. Our source code is available at:\nhttps:\/\/github.com\/w11wo\/GenUP\/.\n","versions":"[{'version': 'v1', 'created': 'Mon, 28 Oct 2024 00:39:22 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 00:54:57 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wongso', 'Wilson', ''], ['Xue', 'Hao', ''], ['Salim', 'Flora D.', '']]","extracted_entities":"[{'text': 'dense\\nvector-based user embeddings', 'label': 'Embedding'}, {'text': 'system prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2501.108,"submitter":"Emanuele La Malfa","authors":"Oliver Goldstein, Emanuele La Malfa, Felix Drinkall, Samuele Marro,\n  Michael Wooldridge","title":"Jailbreaking Large Language Models in Infinitely Many Ways","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We discuss the ``Infinitely Many Paraphrases'' attacks (IMP), a category of\njailbreaks that leverages the increasing capabilities of a model to handle\nparaphrases and encoded communications to bypass their defensive mechanisms.\nIMPs' viability pairs and grows with a model's capabilities to handle and bind\nthe semantics of simple mappings between tokens and work extremely well in\npractice, posing a concrete threat to the users of the most powerful LLMs in\ncommerce. We show how one can bypass the safeguards of the most powerful open-\nand closed-source LLMs and generate content that explicitly violates their\nsafety policies. One can protect against IMPs by improving the guardrails and\nmaking them scale with the LLMs' capabilities. For two categories of attacks\nthat are straightforward to implement, i.e., bijection and encoding, we discuss\ntwo defensive strategies, one in token and the other in embedding space. We\nconclude with some research questions we believe should be prioritised to\nenhance the defensive mechanisms of LLMs and our understanding of their safety.\n","versions":"[{'version': 'v1', 'created': 'Sat, 18 Jan 2025 15:39:53 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 08:43:27 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Goldstein', 'Oliver', ''], ['La Malfa', 'Emanuele', ''], ['Drinkall', 'Felix', ''], ['Marro', 'Samuele', ''], ['Wooldridge', 'Michael', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}, {'text': 'safety policies', 'label': 'AI Ethics'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'embedding space', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2503.02191,"submitter":"Mia Mohammad Imran","authors":"Mia Mohammad Imran, Robert Zita, Rebekah Copeland, Preetha Chatterjee,\n  Rahat Rizvi Rahman, and Kostadin Damevski","title":"Understanding and Predicting Derailment in Toxic Conversations on GitHub","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Software projects thrive on the involvement and contributions of individuals\nfrom different backgrounds. However, toxic language and negative interactions\ncan hinder the participation and retention of contributors and alienate\nnewcomers. Proactive moderation strategies aim to prevent toxicity from\noccurring by addressing conversations that have derailed from their intended\npurpose. This study aims to understand and predict conversational derailment\nleading to toxicity on GitHub.\n  To facilitate this research, we curate a novel dataset comprising 202 toxic\nconversations from GitHub with annotated derailment points, along with 696\nnon-toxic conversations as a baseline. Based on this dataset, we identify\nunique characteristics of toxic conversations and derailment points, including\nlinguistic markers such as second-person pronouns, negation terms, and tones of\nBitter Frustration and Impatience, as well as patterns in conversational\ndynamics between project contributors and external participants.\n  Leveraging these empirical observations, we propose a proactive moderation\napproach to automatically detect and address potentially harmful conversations\nbefore escalation. By utilizing modern LLMs, we develop a conversation\ntrajectory summary technique that captures the evolution of discussions and\nidentifies early signs of derailment. Our experiments demonstrate that LLM\nprompts tailored to provide summaries of GitHub conversations achieve 70%\nF1-Score in predicting conversational derailment, strongly improving over a set\nof baseline approaches.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 02:01:37 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 03:25:44 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Imran', 'Mia Mohammad', ''], ['Zita', 'Robert', ''], ['Copeland', 'Rebekah', ''], ['Chatterjee', 'Preetha', ''], ['Rahman', 'Rahat Rizvi', ''], ['Damevski', 'Kostadin', '']]","extracted_entities":"[{'text': 'GitHub', 'label': 'Open-source LLMs'}, {'text': 'GitHub', 'label': 'Open-source LLMs'}, {'text': 'modern LLMs', 'label': 'LLM'}, {'text': 'GitHub', 'label': 'Open-source LLMs'}]","assigned_concept":"LLM","matched_keyword":"modern LLMs","similarity_score":0.7401012182}
{"id":2503.08179,"submitter":"Zicheng Ma","authors":"Zicheng Ma and Chuanliu Fan and Zhicong Wang and Zhenyu Chen and\n  Xiaohan Lin and Yanheng Li and Shihao Feng and Jun Zhang and Ziqiang Cao and\n  Yi Qin Gao","title":"ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with\n  Large Language Models","comments":"26 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"q-bio.BM cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models have made remarkable progress in the field of molecular\nscience, particularly in understanding and generating functional small\nmolecules. This success is largely attributed to the effectiveness of molecular\ntokenization strategies. In protein science, the amino acid sequence serves as\nthe sole tokenizer for LLMs. However, many fundamental challenges in protein\nscience are inherently structure-dependent. The absence of structure-aware\ntokens significantly limits the capabilities of LLMs for comprehensive\nbiomolecular comprehension and multimodal generation. To address these\nchallenges, we introduce a novel framework, ProtTeX, which tokenizes the\nprotein sequences, structures, and textual information into a unified discrete\nspace. This innovative approach enables joint training of the LLM exclusively\nthrough the Next-Token Prediction paradigm, facilitating multimodal protein\nreasoning and generation. ProtTeX enables general LLMs to perceive and process\nprotein structures through sequential text input, leverage structural\ninformation as intermediate reasoning components, and generate or manipulate\nstructures via sequential text output. Experiments demonstrate that our model\nachieves significant improvements in protein function prediction, outperforming\nthe state-of-the-art domain expert model with a twofold increase in accuracy.\nOur framework enables high-quality conformational generation and customizable\nprotein design. For the first time, we demonstrate that by adopting the\nstandard training and inference pipelines from the LLM domain, ProtTeX empowers\ndecoder-only LLMs to effectively address diverse spectrum of protein-related\ntasks.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 08:43:05 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 08:46:33 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 13:54:27 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Ma', 'Zicheng', ''], ['Fan', 'Chuanliu', ''], ['Wang', 'Zhicong', ''], ['Chen', 'Zhenyu', ''], ['Lin', 'Xiaohan', ''], ['Li', 'Yanheng', ''], ['Feng', 'Shihao', ''], ['Zhang', 'Jun', ''], ['Cao', 'Ziqiang', ''], ['Gao', 'Yi Qin', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2503.09986,"submitter":"Ling Liang","authors":"Rohan Bhatnagar, Ling Liang, Krish Patel, and Haizhao Yang","title":"From Equations to Insights: Unraveling Symbolic Structures in PDEs with\n  LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Motivated by the remarkable success of artificial intelligence (AI) across\ndiverse fields, the application of AI to solve scientific problems-often\nformulated as partial differential equations (PDEs)-has garnered increasing\nattention. While most existing research concentrates on theoretical properties\n(such as well-posedness, regularity, and continuity) of the solutions,\nalongside direct AI-driven methods for solving PDEs, the challenge of\nuncovering symbolic relationships within these equations remains largely\nunexplored. In this paper, we propose leveraging large language models (LLMs)\nto learn such symbolic relationships. Our results demonstrate that LLMs can\neffectively predict the operators involved in PDE solutions by utilizing the\nsymbolic information in the PDEs. Furthermore, we show that discovering these\nsymbolic relationships can substantially improve both the efficiency and\naccuracy of the finite expression method for finding analytical approximation\nof PDE solutions, delivering a fully interpretable solution pipeline. This work\nopens new avenues for understanding the symbolic structure of scientific\nproblems and advancing their solution processes.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 02:52:20 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Bhatnagar', 'Rohan', ''], ['Liang', 'Ling', ''], ['Patel', 'Krish', ''], ['Yang', 'Haizhao', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2503.10061,"submitter":"Nicholas Roberts","authors":"Nicholas Roberts, Niladri Chatterji, Sharan Narang, Mike Lewis,\n  Dieuwke Hupkes","title":"Compute Optimal Scaling of Skills: Knowledge vs Reasoning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Scaling laws are a critical component of the LLM development pipeline, most\nfamously as a way to forecast training decisions such as 'compute-optimally'\ntrading-off parameter count and dataset size, alongside a more recent growing\nlist of other crucial decisions. In this work, we ask whether compute-optimal\nscaling behaviour can be skill-dependent. In particular, we examine knowledge\nand reasoning-based skills such as knowledge-based QA and code generation, and\nwe answer this question in the affirmative: $\\textbf{scaling laws are\nskill-dependent}$. Next, to understand whether skill-dependent scaling is an\nartefact of the pretraining datamix, we conduct an extensive ablation of\ndifferent datamixes and find that, also when correcting for datamix\ndifferences, $\\textbf{knowledge and code exhibit fundamental differences in\nscaling behaviour}$. We conclude with an analysis of how our findings relate to\nstandard compute-optimal scaling using a validation set, and find that\n$\\textbf{a misspecified validation set can impact compute-optimal parameter\ncount by nearly 50%,}$ depending on its skill composition.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 05:21:22 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Roberts', 'Nicholas', ''], ['Chatterji', 'Niladri', ''], ['Narang', 'Sharan', ''], ['Lewis', 'Mike', ''], ['Hupkes', 'Dieuwke', '']]","extracted_entities":"[{'text': 'Scaling laws', 'label': 'Scaling law'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'scaling laws', 'label': 'Scaling law'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2503.10071,"submitter":"Sunzida Siddique","authors":"Mohd Ariful Haque, Justin Williams, Sunzida Siddique, Md. Hujaifa\n  Islam, Hasmot Ali, Kishor Datta Gupta, and Roy George","title":"Advanced Tool Learning and Selection System (ATLASS): A Closed-Loop\n  Framework Using LLM","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  The combination of LLM agents with external tools enables models to solve\ncomplex tasks beyond their knowledge base. Human-designed tools are inflexible\nand restricted to solutions within the scope of pre-existing tools created by\nexperts. To address this problem, we propose ATLASS, an advanced tool learning\nand selection system designed as a closed-loop framework. It enables the LLM to\nsolve problems by dynamically generating external tools on demand. In this\nframework, agents play a crucial role in orchestrating tool selection,\nexecution, and refinement, ensuring adaptive problem-solving capabilities. The\noperation of ATLASS follows three phases: The first phase, Understanding Tool\nRequirements, involves the Agents determining whether tools are required and\nspecifying their functionality; the second phase, Tool Retrieval\/Generation,\ninvolves the Agents retrieving or generating tools based on their availability;\nand the third phase, Task Solving, involves combining all the component tools\nnecessary to complete the initial task. The Tool Dataset stores the generated\ntools, ensuring reusability and minimizing inference cost. Current LLM-based\ntool generation systems have difficulty creating complex tools that need APIs\nor external packages. In ATLASS, we solve the problem by automatically setting\nup the environment, fetching relevant API documentation online, and using a\nPython interpreter to create a reliable, versatile tool that works in a wider\nrange of situations. OpenAI GPT-4.0 is used as the LLM agent, and safety and\nethical concerns are handled through human feedback before executing generated\ncode. By addressing the limitations of predefined toolsets and enhancing\nadaptability, ATLASS serves as a real-world solution that empowers users with\ndynamically generated tools for complex problem-solving.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 05:39:00 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Haque', 'Mohd Ariful', ''], ['Williams', 'Justin', ''], ['Siddique', 'Sunzida', ''], ['Islam', 'Md. Hujaifa', ''], ['Ali', 'Hasmot', ''], ['Gupta', 'Kishor Datta', ''], ['George', 'Roy', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}, {'text': 'ATLASS', 'label': 'LLM-based'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM-based', 'label': 'LLM-based'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'safety and\\nethical concerns', 'label': 'AI Ethics'}, {'text': 'ATLASS', 'label': 'LLM-based'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2503.10076,"submitter":"Meiqi Wu","authors":"Xinrang Ling, Chen Zhu, Meiqi Wu, Hangyu Li, Xiaokun Feng, Cundian\n  Yang, Aiming Hao, Jiashu Zhu, Jiahong Wu, Xiangxiang Chu","title":"VMBench: A Benchmark for Perception-Aligned Video Motion Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Video generation has advanced rapidly, improving evaluation methods, yet\nassessing video's motion remains a major challenge. Specifically, there are two\nkey issues: 1) current motion metrics do not fully align with human\nperceptions; 2) the existing motion prompts are limited. Based on these\nfindings, we introduce VMBench--a comprehensive Video Motion Benchmark that has\nperception-aligned motion metrics and features the most diverse types of\nmotion. VMBench has several appealing properties: 1) Perception-Driven Motion\nEvaluation Metrics, we identify five dimensions based on human perception in\nmotion video assessment and develop fine-grained evaluation metrics, providing\ndeeper insights into models' strengths and weaknesses in motion quality. 2)\nMeta-Guided Motion Prompt Generation, a structured method that extracts\nmeta-information, generates diverse motion prompts with LLMs, and refines them\nthrough human-AI validation, resulting in a multi-level prompt library covering\nsix key dynamic scene dimensions. 3) Human-Aligned Validation Mechanism, we\nprovide human preference annotations to validate our benchmarks, with our\nmetrics achieving an average 35.3% improvement in Spearman's correlation over\nbaseline methods. This is the first time that the quality of motion in videos\nhas been evaluated from the perspective of human perception alignment.\nAdditionally, we will soon release VMBench at\nhttps:\/\/github.com\/GD-AIGC\/VMBench, setting a new standard for evaluating and\nadvancing motion generation models.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 05:54:42 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Ling', 'Xinrang', ''], ['Zhu', 'Chen', ''], ['Wu', 'Meiqi', ''], ['Li', 'Hangyu', ''], ['Feng', 'Xiaokun', ''], ['Yang', 'Cundian', ''], ['Hao', 'Aiming', ''], ['Zhu', 'Jiashu', ''], ['Wu', 'Jiahong', ''], ['Chu', 'Xiangxiang', '']]","extracted_entities":"[{'text': 'Meta-Guided Motion Prompt Generation', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2503.1012,"submitter":"Bingchen Li","authors":"Bingchen Li, Xin Li, Yiting Lu, Zhibo Chen","title":"Hybrid Agents for Image Restoration","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV eess.IV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Existing Image Restoration (IR) studies typically focus on task-specific or\nuniversal modes individually, relying on the mode selection of users and\nlacking the cooperation between multiple task-specific\/universal restoration\nmodes. This leads to insufficient interaction for unprofessional users and\nlimits their restoration capability for complicated real-world applications. In\nthis work, we present HybridAgent, intending to incorporate multiple\nrestoration modes into a unified image restoration model and achieve\nintelligent and efficient user interaction through our proposed hybrid agents.\nConcretely, we propose the hybrid rule of fast, slow, and feedback restoration\nagents. Here, the slow restoration agent optimizes the powerful multimodal\nlarge language model (MLLM) with our proposed instruction-tuning dataset to\nidentify degradations within images with ambiguous user prompts and invokes\nproper restoration tools accordingly. The fast restoration agent is designed\nbased on a lightweight large language model (LLM) via in-context learning to\nunderstand the user prompts with simple and clear requirements, which can\nobviate the unnecessary time\/resource costs of MLLM. Moreover, we introduce the\nmixed distortion removal mode for our HybridAgents, which is crucial but not\nconcerned in previous agent-based works. It can effectively prevent the error\npropagation of step-by-step image restoration and largely improve the\nefficiency of the agent system. We validate the effectiveness of HybridAgent\nwith both synthetic and real-world IR tasks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 07:28:33 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Bingchen', ''], ['Li', 'Xin', ''], ['Lu', 'Yiting', ''], ['Chen', 'Zhibo', '']]","extracted_entities":"[{'text': 'HybridAgent', 'label': 'LLM-based'}, {'text': 'MLLM', 'label': 'Large Language Model'}, {'text': 'user prompts', 'label': 'Prompting'}, {'text': 'lightweight large language model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'in-context learning', 'label': 'Few-shot Learning'}, {'text': 'user prompts', 'label': 'Prompting'}, {'text': 'MLLM', 'label': 'Large Language Model'}, {'text': 'HybridAgents', 'label': 'LLM-powered'}, {'text': 'HybridAgent', 'label': 'LLM-based'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2503.10357,"submitter":"Viktor Moskvoretskii","authors":"Viktor Moskvoretskii, Alina Lobanova, Ekaterina Neminova, Chris\n  Biemann, Alexander Panchenko, Irina Nikishina","title":"Do I look like a `cat.n.01` to you? A Taxonomy Image Generation\n  Benchmark","comments":"Labeled data and generated image Wordnet are published at\n  https:\/\/huggingface.co\/collections\/VityaVitalich\/generated-image-wordnet-67d2c868ff1414ec2f8e0d3d","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  This paper explores the feasibility of using text-to-image models in a\nzero-shot setup to generate images for taxonomy concepts. While text-based\nmethods for taxonomy enrichment are well-established, the potential of the\nvisual dimension remains unexplored. To address this, we propose a\ncomprehensive benchmark for Taxonomy Image Generation that assesses models'\nabilities to understand taxonomy concepts and generate relevant, high-quality\nimages. The benchmark includes common-sense and randomly sampled WordNet\nconcepts, alongside the LLM generated predictions. The 12 models are evaluated\nusing 9 novel taxonomy-related text-to-image metrics and human feedback.\nMoreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for\nimage generation. Experimental results show that the ranking of models differs\nsignificantly from standard T2I tasks. Playground-v2 and FLUX consistently\noutperform across metrics and subsets and the retrieval-based approach performs\npoorly. These findings highlight the potential for automating the curation of\nstructured data resources.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 13:37:54 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Moskvoretskii', 'Viktor', ''], ['Lobanova', 'Alina', ''], ['Neminova', 'Ekaterina', ''], ['Biemann', 'Chris', ''], ['Panchenko', 'Alexander', ''], ['Nikishina', 'Irina', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}, {'text': 'GPT-4', 'label': 'GPT'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2503.10494,"submitter":"Hanxu Hu","authors":"Hanxu Hu, Jannis Vamvas, Rico Sennrich","title":"Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents","comments":"9 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 15:57:50 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Hu', 'Hanxu', ''], ['Vamvas', 'Jannis', ''], ['Sennrich', 'Rico', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2503.1063,"submitter":"Hang Yin","authors":"Hang Yin, Xiuwei Xu, Lingqing Zhao, Ziwei Wang, Jie Zhou, Jiwen Lu","title":"UniGoal: Towards Universal Zero-shot Goal-oriented Navigation","comments":"Accepted to CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:59:48 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Yin', 'Hang', ''], ['Xu', 'Xiuwei', ''], ['Zhao', 'Lingqing', ''], ['Wang', 'Ziwei', ''], ['Zhou', 'Jie', ''], ['Lu', 'Jiwen', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2502.19339,"submitter":"Tohida Rehman Ms.","authors":"Tohida Rehman, Soumabha Ghosh, Kuntal Das, Souvik Bhattacharjee,\n  Debarshi Kumar Sanyal, Samiran Chattopadhyay","title":"Evaluating LLMs and Pre-trained Models for Text Summarization Across\n  Diverse Datasets","comments":"5 pages, 2 figures, 6 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Text summarization plays a crucial role in natural language processing by\ncondensing large volumes of text into concise and coherent summaries. As\ndigital content continues to grow rapidly and the demand for effective\ninformation retrieval increases, text summarization has become a focal point of\nresearch in recent years. This study offers a thorough evaluation of four\nleading pre-trained and open-source large language models: BART, FLAN-T5,\nLLaMA-3-8B, and Gemma-7B, across five diverse datasets CNN\/DM, Gigaword, News\nSummary, XSum, and BBC News. The evaluation employs widely recognized automatic\nmetrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and METEOR, to assess\nthe models' capabilities in generating coherent and informative summaries. The\nresults reveal the comparative strengths and limitations of these models in\nprocessing various text types.\n","versions":"[{'version': 'v1', 'created': 'Wed, 26 Feb 2025 17:32:07 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 09:40:42 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Rehman', 'Tohida', ''], ['Ghosh', 'Soumabha', ''], ['Das', 'Kuntal', ''], ['Bhattacharjee', 'Souvik', ''], ['Sanyal', 'Debarshi Kumar', ''], ['Chattopadhyay', 'Samiran', '']]","extracted_entities":"[{'text': 'Text summarization', 'label': 'Knowledge distillation'}, {'text': 'text summarization', 'label': 'Knowledge distillation'}, {'text': 'FLAN-T5', 'label': 'Large Language Model'}, {'text': 'Gigaword', 'label': 'Large Language Model'}, {'text': 'ROUGE-1', 'label': 'BERT'}, {'text': 'BERTScore', 'label': 'BERT'}]","assigned_concept":"BERT","matched_keyword":"BERTScore","similarity_score":0.7477546334}
{"id":2503.09257,"submitter":"Haixing Gong","authors":"Haixing Gong, Hui Zou, Xingzhou Liang, Shiyuan Meng, Pinlong Cai,\n  Xingcheng Xu, Jingjing Qu","title":"DeepInnovation AI: A Global Dataset Mapping the AI innovation from\n  Academic Research to Industrial Patents","comments":"32 pages and 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DB cs.AI cs.DL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In the rapidly evolving field of artificial intelligence (AI), mapping\ninnovation patterns and understanding effective technology transfer from\nresearch to applications are essential for economic growth. However, existing\ndata infrastructures suffer from fragmentation, incomplete coverage, and\ninsufficient evaluative capacity. Here, we present DeepInnovationAI, a\ncomprehensive global dataset containing three structured files.\nDeepPatentAI.csv: Contains 2,356,204 patent records with 8 field-specific\nattributes. DeepDiveAI.csv: Encompasses 3,511,929 academic publications with 13\nmetadata fields. These two datasets leverage large language models,\nmultilingual text analysis and dual-layer BERT classifiers to accurately\nidentify AI-related content, while utilizing hypergraph analysis to create\nrobust innovation metrics. Additionally, DeepCosineAI.csv: By applying semantic\nvector proximity analysis, this file presents approximately one hundred million\ncalculated paper-patent similarity pairs to enhance understanding of how\ntheoretical advancements translate into commercial technologies.\nDeepInnovationAI enables researchers, policymakers, and industry leaders to\nanticipate trends and identify collaboration opportunities. With extensive\ntemporal and geographical scope, it supports detailed analysis of technological\ndevelopment patterns and international competition dynamics, establishing a\nfoundation for modeling AI innovation and technology transfer processes.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 10:56:02 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 05:53:58 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Gong', 'Haixing', ''], ['Zou', 'Hui', ''], ['Liang', 'Xingzhou', ''], ['Meng', 'Shiyuan', ''], ['Cai', 'Pinlong', ''], ['Xu', 'Xingcheng', ''], ['Qu', 'Jingjing', '']]","extracted_entities":"[{'text': 'dual-layer BERT classifiers', 'label': 'BERT'}]","assigned_concept":"BERT","matched_keyword":"dual-layer BERT classifiers","similarity_score":0.5805794001}
{"id":2503.10095,"submitter":"Avinash Patil","authors":"Avinash Patil, Amardeep Kour Gedhu","title":"Cognitive-Mental-LLM: Leveraging Reasoning in Large Language Models for\n  Mental Health Prediction via Online Text","comments":"8 pages, 4 Figures, 3 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) have demonstrated potential in predicting mental\nhealth outcomes from online text, yet traditional classification methods often\nlack interpretability and robustness. This study evaluates structured reasoning\ntechniques-Chain-of-Thought (CoT), Self-Consistency (SC-CoT), and\nTree-of-Thought (ToT)-to improve classification accuracy across multiple mental\nhealth datasets sourced from Reddit. We analyze reasoning-driven prompting\nstrategies, including Zero-shot CoT and Few-shot CoT, using key performance\nmetrics such as Balanced Accuracy, F1 score, and Sensitivity\/Specificity. Our\nfindings indicate that reasoning-enhanced techniques improve classification\nperformance over direct prediction, particularly in complex cases. Compared to\nbaselines such as Zero Shot non-CoT Prompting, and fine-tuned pre-trained\ntransformers such as BERT and Mental-RoBerta, and fine-tuned Open Source LLMs\nsuch as Mental Alpaca and Mental-Flan-T5, reasoning-driven LLMs yield notable\ngains on datasets like Dreaddit (+0.52\\% over M-LLM, +0.82\\% over BERT) and\nSDCNL (+4.67\\% over M-LLM, +2.17\\% over BERT). However, performance declines in\nDepression Severity, and CSSRS predictions suggest dataset-specific\nlimitations, likely due to our using a more extensive test set. Among prompting\nstrategies, Few-shot CoT consistently outperforms others, reinforcing the\neffectiveness of reasoning-driven LLMs. Nonetheless, dataset variability\nhighlights challenges in model reliability and interpretability. This study\nprovides a comprehensive benchmark of reasoning-based LLM techniques for mental\nhealth text classification. It offers insights into their potential for\nscalable clinical applications while identifying key challenges for future\nimprovements.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 06:42:37 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Patil', 'Avinash', ''], ['Gedhu', 'Amardeep Kour', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Tree-of-Thought', 'label': 'Chain of thought'}, {'text': 'Zero-shot CoT', 'label': 'Few-shot Learning'}, {'text': 'Few-shot CoT', 'label': 'Few-shot Learning'}, {'text': 'Zero Shot non-CoT Prompting', 'label': 'Prompting'}, {'text': 'BERT', 'label': 'BERT'}, {'text': 'Mental-RoBerta', 'label': 'RoBERTa'}, {'text': 'Mental Alpaca', 'label': 'Open-source LLMs'}, {'text': 'BERT', 'label': 'BERT'}, {'text': 'SDCNL', 'label': 'Large Language Model'}, {'text': 'BERT', 'label': 'BERT'}, {'text': 'Few-shot CoT', 'label': 'Few-shot Learning'}]","assigned_concept":"BERT","matched_keyword":"BERT","similarity_score":1.0}
{"id":2503.10233,"submitter":"Laya Mahmoudi","authors":"Samira Zangooei, Amirhossein Darmani, Hossein Farahmand Nezhad, Laya\n  Mahmoudi","title":"ARLED: Leveraging LED-based ARMAN Model for Abstractive Summarization of\n  Persian Long Documents","comments":"11 pages, 3 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The increasing volume of textual data poses challenges in reading and\ncomprehending large documents, particularly for scholars who need to extract\nuseful information from research articles. Automatic text summarization has\nemerged as a powerful tool to condense lengthy documents into concise and\ninformative summaries. Depending on the approach used, text summarization can\nbe categorized as either extractive or abstractive. While extractive methods\nare commonly used due to their simplicity, they often miss important\ninformation. On the other hand, Abstractive Summarization can generate more\ncoherent and informative summaries by understanding the underlying meaning of\nthe text. Abstractive techniques have gained attention in various languages,\nand recent advancements have been achieved through pre-training models such as\nBERT, BART, and T5. However, the challenge of summarizing long documents\nremains, and alternative models like Longformer have been introduced to address\nthis limitation. In this context, this paper focuses on abstractive\nsummarization in the Persian language. The authors introduce a new dataset of\n300,000 full-text Persian papers obtained from the Ensani website and apply the\nARMAN model, based on the Longformer architecture, to generate summaries. The\nexperimental results demonstrate promising performance in Persian text\nsummarization. The paper provides a comprehensive overview of related work,\ndiscusses the methodology, presents the experimental results, and concludes\nwith future research directions.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 10:16:46 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zangooei', 'Samira', ''], ['Darmani', 'Amirhossein', ''], ['Nezhad', 'Hossein Farahmand', ''], ['Mahmoudi', 'Laya', '']]","extracted_entities":"[{'text': 'Abstractive Summarization', 'label': 'Knowledge distillation'}, {'text': 'BERT', 'label': 'BERT'}, {'text': 'BART', 'label': 'BERT'}]","assigned_concept":"BERT","matched_keyword":"BERT","similarity_score":1.0}
{"id":2412.07752,"submitter":"Korbinian P\\\"oppel","authors":"Korbinian P\\\"oppel, Maximilian Beck, Sepp Hochreiter","title":"FlashRNN: I\/O-Aware Optimization of Traditional RNNs on modern hardware","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https:\/\/github.com\/NX-AI\/flashrnn\n","versions":"[{'version': 'v1', 'created': 'Tue, 10 Dec 2024 18:50:37 GMT'}, {'version': 'v2', 'created': 'Mon, 13 Jan 2025 17:34:22 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 11:14:49 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['P\u00f6ppel', 'Korbinian', ''], ['Beck', 'Maximilian', ''], ['Hochreiter', 'Sepp', '']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'sLSTM', 'label': 'Transformers'}, {'text': 'RNNs', 'label': 'AI model'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'RNNs', 'label': 'AI model'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2503.06369,"submitter":"Sahar Dastani","authors":"Sahar Dastani, Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, David\n  Osowiechi, Gustavo Adolfo Vargas Hakim, Farzad Beizaee, Milad\n  Cheraghalikhani, Arnab Kumar Mondal, Herve Lombaert, Christian Desrosiers","title":"Spectral State Space Model for Rotation-Invariant Visual Representation\n  Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  State Space Models (SSMs) have recently emerged as an alternative to Vision\nTransformers (ViTs) due to their unique ability of modeling global\nrelationships with linear complexity. SSMs are specifically designed to capture\nspatially proximate relationships of image patches. However, they fail to\nidentify relationships between conceptually related yet not adjacent patches.\nThis limitation arises from the non-causal nature of image data, which lacks\ninherent directional relationships. Additionally, current vision-based SSMs are\nhighly sensitive to transformations such as rotation. Their predefined scanning\ndirections depend on the original image orientation, which can cause the model\nto produce inconsistent patch-processing sequences after rotation. To address\nthese limitations, we introduce Spectral VMamba, a novel approach that\neffectively captures the global structure within an image by leveraging\nspectral information derived from the graph Laplacian of image patches. Through\nspectral decomposition, our approach encodes patch relationships independently\nof image orientation, achieving rotation invariance with the aid of our\nRotational Feature Normalizer (RFN) module. Our experiments on classification\ntasks show that Spectral VMamba outperforms the leading SSM models in vision,\nsuch as VMamba, while maintaining invariance to rotations and a providing a\nsimilar runtime efficiency.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 00:37:43 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 02:10:35 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Dastani', 'Sahar', ''], ['Bahri', 'Ali', ''], ['Yazdanpanah', 'Moslem', ''], ['Noori', 'Mehrdad', ''], ['Osowiechi', 'David', ''], ['Hakim', 'Gustavo Adolfo Vargas', ''], ['Beizaee', 'Farzad', ''], ['Cheraghalikhani', 'Milad', ''], ['Mondal', 'Arnab Kumar', ''], ['Lombaert', 'Herve', ''], ['Desrosiers', 'Christian', '']]","extracted_entities":"[{'text': 'Vision\\nTransformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Vision\nTransformers","similarity_score":0.7330732346}
{"id":2503.09942,"submitter":"Yasheng Sun","authors":"Yasheng Sun, Zhiliang Xu, Hang Zhou, Jiazhi Guan, Quanwei Yang,\n  Kaisiyuan Wang, Borong Liang, Yingying Li, Haocheng Feng, Jingdong Wang,\n  Ziwei Liu, Koike Hideki","title":"Cosh-DiT: Co-Speech Gesture Video Synthesis via Hybrid Audio-Visual\n  Diffusion Transformers","comments":"Project Page: https:\/\/sunyasheng.github.io\/projects\/COSH-DIT","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Co-speech gesture video synthesis is a challenging task that requires both\nprobabilistic modeling of human gestures and the synthesis of realistic images\nthat align with the rhythmic nuances of speech. To address these challenges, we\npropose Cosh-DiT, a Co-speech gesture video system with hybrid Diffusion\nTransformers that perform audio-to-motion and motion-to-video synthesis using\ndiscrete and continuous diffusion modeling, respectively. First, we introduce\nan audio Diffusion Transformer (Cosh-DiT-A) to synthesize expressive gesture\ndynamics synchronized with speech rhythms. To capture upper body, facial, and\nhand movement priors, we employ vector-quantized variational autoencoders\n(VQ-VAEs) to jointly learn their dependencies within a discrete latent space.\nThen, for realistic video synthesis conditioned on the generated speech-driven\nmotion, we design a visual Diffusion Transformer (Cosh-DiT-V) that effectively\nintegrates spatial and temporal contexts. Extensive experiments demonstrate\nthat our framework consistently generates lifelike videos with expressive\nfacial expressions and natural, smooth gestures that align seamlessly with\nspeech.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 01:36:05 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Sun', 'Yasheng', ''], ['Xu', 'Zhiliang', ''], ['Zhou', 'Hang', ''], ['Guan', 'Jiazhi', ''], ['Yang', 'Quanwei', ''], ['Wang', 'Kaisiyuan', ''], ['Liang', 'Borong', ''], ['Li', 'Yingying', ''], ['Feng', 'Haocheng', ''], ['Wang', 'Jingdong', ''], ['Liu', 'Ziwei', ''], ['Hideki', 'Koike', '']]","extracted_entities":"[{'text': 'Diffusion\\nTransformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Diffusion\nTransformers","similarity_score":0.5920959711}
{"id":2503.10043,"submitter":"Wenjie Li","authors":"Wenjie Li, Heng Guo, Yuefeng Hou, and Zhanyu Ma","title":"FourierSR: A Fourier Token-based Plugin for Efficient Image\n  Super-Resolution","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Image super-resolution (SR) aims to recover low-resolution images to\nhigh-resolution images, where improving SR efficiency is a high-profile\nchallenge. However, commonly used units in SR, like convolutions and\nwindow-based Transformers, have limited receptive fields, making it challenging\nto apply them to improve SR under extremely limited computational cost. To\naddress this issue, inspired by modeling convolution theorem through token mix,\nwe propose a Fourier token-based plugin called FourierSR to improve SR\nuniformly, which avoids the instability or inefficiency of existing token mix\ntechnologies when applied as plug-ins. Furthermore, compared to convolutions\nand windows-based Transformers, our FourierSR only utilizes Fourier transform\nand multiplication operations, greatly reducing complexity while having global\nreceptive fields. Experimental results show that our FourierSR as a\nplug-and-play unit brings an average PSNR gain of 0.34dB for existing efficient\nSR methods on Manga109 test set at the scale of x4, while the average increase\nin the number of Params and FLOPs is only 0.6% and 1.5% of original sizes. We\nwill release our codes upon acceptance.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 04:50:55 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Wenjie', ''], ['Guo', 'Heng', ''], ['Hou', 'Yuefeng', ''], ['Ma', 'Zhanyu', '']]","extracted_entities":"[{'text': 'window-based Transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"window-based Transformers","similarity_score":0.6468666792}
{"id":2503.10251,"submitter":"Stanislav Budzinskiy","authors":"Stanislav Budzinskiy, Wenyi Fang, Longbin Zeng, Philipp Petersen","title":"Numerical Error Analysis of Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.LG cs.NA stat.ML","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models based on transformer architectures have become integral\nto state-of-the-art natural language processing applications. However, their\ntraining remains computationally expensive and exhibits instabilities, some of\nwhich are expected to be caused by finite-precision computations. We provide a\ntheoretical analysis of the impact of round-off errors within the forward pass\nof a transformer architecture which yields fundamental bounds for these\neffects. In addition, we conduct a series of numerical experiments which\ndemonstrate the practical relevance of our bounds. Our results yield concrete\nguidelines for choosing hyperparameters that mitigate round-off errors, leading\nto more robust and stable inference.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 10:53:17 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Budzinskiy', 'Stanislav', ''], ['Fang', 'Wenyi', ''], ['Zeng', 'Longbin', ''], ['Petersen', 'Philipp', '']]","extracted_entities":"[{'text': 'transformer architectures', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"transformer architectures","similarity_score":0.5942972898}
{"id":2503.10457,"submitter":"Bousselham El Haddaoui Mr","authors":"Bousselham El Haddaoui, Raddouane Chiheb, Rdouan Faizi, Abdellatif El\n  Afia","title":"Sentiment Analysis in SemEval: A Review of Sentiment Identification\n  Approaches","comments":null,"journal-ref":"International Journal of Electrical and Computer Engineering\n  (IJECE), 13(3), 3322-3338 (2023)","doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Social media platforms are becoming the foundations of social interactions\nincluding messaging and opinion expression. In this regard, Sentiment Analysis\ntechniques focus on providing solutions to ensure the retrieval and analysis of\ngenerated data including sentiments, emotions, and discussed topics.\nInternational competitions such as the International Workshop on Semantic\nEvaluation (SemEval) have attracted many researchers and practitioners with a\nspecial research interest in building sentiment analysis systems. In our work,\nwe study top-ranking systems for each SemEval edition during the 2013-2021\nperiod, a total of 658 teams participated in these editions with increasing\ninterest over years. We analyze the proposed systems marking the evolution of\nresearch trends with a focus on the main components of sentiment analysis\nsystems including data acquisition, preprocessing, and classification. Our\nstudy shows an active use of preprocessing techniques, an evolution of features\nengineering and word representation from lexicon-based approaches to word\nembeddings, and the dominance of neural networks and transformers over the\nclassification phase fostering the use of ready-to-use models. Moreover, we\nprovide researchers with insights based on experimented systems which will\nallow rapid prototyping of new systems and help practitioners build for future\nSemEval editions.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 15:25:23 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Haddaoui', 'Bousselham El', ''], ['Chiheb', 'Raddouane', ''], ['Faizi', 'Rdouan', ''], ['Afia', 'Abdellatif El', '']]","extracted_entities":"[{'text': 'word\\nembeddings', 'label': 'Embedding'}, {'text': 'transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"transformers","similarity_score":1.0}
{"id":2503.10571,"submitter":"Yongchang Hao","authors":"Yongchang Hao, Mengyao Zhai, Hossein Hajimirsadeghi, Sepidehsadat\n  Hosseini, Frederick Tung","title":"Radar: Fast Long-Context Decoding for Any Transformer","comments":"Accepted @ ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Transformer models have demonstrated exceptional performance across a wide\nrange of applications. Though forming the foundation of Transformer models, the\ndot-product attention does not scale well to long-context data since its time\nrequirement grows quadratically with context length. In this work, we propose\nRadar, a training-free approach that accelerates inference by dynamically\nsearching for the most important context tokens. For any pre-trained\nTransformer, Radar can reduce the decoding time complexity without training or\nheuristically evicting tokens. Moreover, we provide theoretical justification\nfor our approach, demonstrating that Radar can reliably identify the most\nimportant tokens with high probability. We conduct extensive comparisons with\nthe previous methods on a wide range of tasks. The results demonstrate that\nRadar achieves the state-of-the-art performance across different architectures\nwith reduced time complexity, offering a practical solution for efficient\nlong-context processing of Transformers.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:23:10 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Hao', 'Yongchang', ''], ['Zhai', 'Mengyao', ''], ['Hajimirsadeghi', 'Hossein', ''], ['Hosseini', 'Sepidehsadat', ''], ['Tung', 'Frederick', '']]","extracted_entities":"[{'text': 'Transformer models', 'label': 'Foundation Model'}, {'text': 'dot-product attention', 'label': 'Attention mechanism'}, {'text': 'Radar', 'label': 'Transformers'}, {'text': 'Transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2503.10622,"submitter":"Zhuang Liu","authors":"Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, Zhuang Liu","title":"Transformers without Normalization","comments":"CVPR 2025; Project page: https:\/\/jiachenzhu.github.io\/DyT\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Normalization layers are ubiquitous in modern neural networks and have long\nbeen considered essential. This work demonstrates that Transformers without\nnormalization can achieve the same or better performance using a remarkably\nsimple technique. We introduce Dynamic Tanh (DyT), an element-wise operation\n$DyT($x$) = \\tanh(\\alpha $x$)$, as a drop-in replacement for normalization\nlayers in Transformers. DyT is inspired by the observation that layer\nnormalization in Transformers often produces tanh-like, $S$-shaped input-output\nmappings. By incorporating DyT, Transformers without normalization can match or\nexceed the performance of their normalized counterparts, mostly without\nhyperparameter tuning. We validate the effectiveness of Transformers with DyT\nacross diverse settings, ranging from recognition to generation, supervised to\nself-supervised learning, and computer vision to language models. These\nfindings challenge the conventional understanding that normalization layers are\nindispensable in modern neural networks, and offer new insights into their role\nin deep networks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:59:06 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhu', 'Jiachen', ''], ['Chen', 'Xinlei', ''], ['He', 'Kaiming', ''], ['LeCun', 'Yann', ''], ['Liu', 'Zhuang', '']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'DyT', 'label': 'Transformers'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'DyT', 'label': 'Transformers'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'hyperparameter tuning', 'label': 'Fine-tuning'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'supervised to\\nself-supervised learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2503.10626,"submitter":"Berat Mert Albaba","authors":"Mert Albaba, Chenhao Li, Markos Diomataris, Omid Taheri, Andreas\n  Krause, Michael Black","title":"NIL: No-data Imitation Learning by Leveraging Pre-trained Video\n  Diffusion Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Acquiring physically plausible motor skills across diverse and unconventional\nmorphologies-including humanoid robots, quadrupeds, and animals-is essential\nfor advancing character simulation and robotics. Traditional methods, such as\nreinforcement learning (RL) are task- and body-specific, require extensive\nreward function engineering, and do not generalize well. Imitation learning\noffers an alternative but relies heavily on high-quality expert demonstrations,\nwhich are difficult to obtain for non-human morphologies. Video diffusion\nmodels, on the other hand, are capable of generating realistic videos of\nvarious morphologies, from humans to ants. Leveraging this capability, we\npropose a data-independent approach for skill acquisition that learns 3D motor\nskills from 2D-generated videos, with generalization capability to\nunconventional and non-human forms. Specifically, we guide the imitation\nlearning process by leveraging vision transformers for video-based comparisons\nby calculating pair-wise distance between video embeddings. Along with\nvideo-encoding distance, we also use a computed similarity between segmented\nvideo frames as a guidance reward. We validate our method on locomotion tasks\ninvolving unique body configurations. In humanoid robot locomotion tasks, we\ndemonstrate that 'No-data Imitation Learning' (NIL) outperforms baselines\ntrained on 3D motion-capture data. Our results highlight the potential of\nleveraging generative video models for physically plausible skill learning with\ndiverse morphologies, effectively replacing data collection with data\ngeneration for imitation learning.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:59:24 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Albaba', 'Mert', ''], ['Li', 'Chenhao', ''], ['Diomataris', 'Markos', ''], ['Taheri', 'Omid', ''], ['Krause', 'Andreas', ''], ['Black', 'Michael', '']]","extracted_entities":"[{'text': 'Imitation learning', 'label': 'Few-shot Learning'}, {'text': 'imitation\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'vision transformers', 'label': 'Transformers'}, {'text': 'video embeddings', 'label': 'Embedding'}, {'text': 'Imitation Learning', 'label': 'Few-shot Learning'}, {'text': 'imitation learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Transformers","matched_keyword":"vision transformers","similarity_score":0.7330732346}
{"id":2503.10632,"submitter":"Subhajit Maity","authors":"Subhajit Maity, Killian Hitsman, Xin Li, Aritra Dutta","title":"Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision\n  Transformers?","comments":"Preprint, Appendix included","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of\nlearnable activation functions with the potential to capture more complex\nrelationships from data. Although KANs are useful in finding symbolic\nrepresentations and continual learning of one-dimensional functions, their\neffectiveness in diverse machine learning (ML) tasks, such as vision, remains\nquestionable. Presently, KANs are deployed by replacing multilayer perceptrons\n(MLPs) in deep network architectures, including advanced architectures such as\nvision Transformers (ViTs). In this paper, we are the first to design a general\nlearnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate\non any choice of basis. However, the computing and memory costs of training\nthem motivated us to propose a more modular version, and we designed particular\nlearnable attention, called Fourier-KArAt. Fourier-KArAt and its variants\neither outperform their ViT counterparts or show comparable performance on\nCIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures'\nperformance and generalization capacity by analyzing their loss landscapes,\nweight distributions, optimizer path, attention visualization, and spectral\nbehavior, and contrast them with vanilla ViTs. The goal of this paper is not to\nproduce parameter- and compute-efficient attention, but to encourage the\ncommunity to explore KANs in conjunction with more advanced architectures that\nrequire a careful understanding of learnable activations. Our open-source code\nand implementation details are available on: https:\/\/subhajitmaity.me\/KArAt\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:59:52 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Maity', 'Subhajit', ''], ['Hitsman', 'Killian', ''], ['Li', 'Xin', ''], ['Dutta', 'Aritra', '']]","extracted_entities":"[{'text': 'vision Transformers', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"vision Transformers","similarity_score":0.7330732346}
{"id":2503.10023,"submitter":"Stephanie Hu","authors":"Stephanie Hu, Xiaolu Guo","title":"Using Context to Improve Word Segmentation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  An important step in understanding how children acquire languages is studying\nhow infants learn word segmentation. It has been established in previous\nresearch that infants may use statistical regularities in speech to learn word\nsegmentation. The research of Goldwater et al., demonstrated that incorporating\ncontext in models improves their ability to learn word segmentation. We\nimplemented two of their models, a unigram and bigram model, to examine how\ncontext can improve statistical word segmentation. The results are consistent\nwith our hypothesis that the bigram model outperforms the unigram model at\npredicting word segmentation. Extending the work of Goldwater et al., we also\nexplored basic ways to model how young children might use previously learned\nwords to segment new utterances.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 04:04:55 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Hu', 'Stephanie', ''], ['Guo', 'Xiaolu', '']]","extracted_entities":"[{'text': 'context', 'label': 'contextual Embedding'}, {'text': 'context', 'label': 'contextual Embedding'}]","assigned_concept":"contextual Embedding","matched_keyword":"context","similarity_score":0.50626266}
{"id":2503.10094,"submitter":"Georgios Feretzakis","authors":"Phoebe Koundouri, Conrad Landis, Georgios Feretzakis","title":"Semantic Synergy: Unlocking Policy Insights and Learning Pathways\n  Through Advanced Skill Mapping","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This research introduces a comprehensive system based on state-of-the-art\nnatural language processing, semantic embedding, and efficient search\ntechniques for retrieving similarities and thus generating actionable insights\nfrom raw textual information. The system automatically extracts and aggregates\nnormalized competencies from multiple documents (such as policy files and\ncurricula vitae) and creates strong relationships between recognized\ncompetencies, occupation profiles, and related learning courses. To validate\nits performance, we conducted a multi-tier evaluation that included both\nexplicit and implicit skill references in synthetic and real-world documents.\nThe results showed near-human-level accuracy, with F1 scores exceeding 0.95 for\nexplicit skill detection and above 0.93 for implicit mentions. The system\nthereby establishes a sound foundation for supporting in-depth collaboration\nacross the AE4RIA network. The methodology involves a multi-stage pipeline\nbased on extensive preprocessing and data cleaning, semantic embedding and\nsegmentation via SentenceTransformer, and skill extraction using a FAISS-based\nsearch method. The extracted skills are associated with occupation frameworks\n(as formulated in the ESCO ontology) and with learning paths offered through\nthe Sustainable Development Goals Academy. Moreover, interactive visualization\nsoftware, implemented with Dash and Plotly, presents graphs and tables for\nreal-time exploration and informed decision-making by those involved in\npolicymaking, training and learning supply, career transitions, and\nrecruitment. Overall, this system, backed by rigorous validation, offers\npromising prospects for improved policymaking, human resource development, and\nlifelong learning by providing structured and actionable insights from raw,\ncomplex textual information.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 06:41:26 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Koundouri', 'Phoebe', ''], ['Landis', 'Conrad', ''], ['Feretzakis', 'Georgios', '']]","extracted_entities":"[{'text': 'semantic embedding', 'label': 'contextual Embedding'}, {'text': 'semantic embedding', 'label': 'contextual Embedding'}]","assigned_concept":"contextual Embedding","matched_keyword":"semantic embedding","similarity_score":0.7632060051}
{"id":2503.10125,"submitter":"Yi Wu","authors":"Yi Wu, Lingting Zhu, Lei Liu, Wandi Qiao, Ziqiang Li, Lequan Yu, Bin\n  Li","title":"Proxy-Tuning: Tailoring Multimodal Autoregressive Models for\n  Subject-Driven Image Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.MM","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multimodal autoregressive (AR) models, based on next-token prediction and\ntransformer architecture, have demonstrated remarkable capabilities in various\nmultimodal tasks including text-to-image (T2I) generation. Despite their strong\nperformance in general T2I tasks, our research reveals that these models\ninitially struggle with subject-driven image generation compared to dominant\ndiffusion models. To address this limitation, we introduce Proxy-Tuning,\nleveraging diffusion models to enhance AR models' capabilities in\nsubject-specific image generation. Our method reveals a striking weak-to-strong\nphenomenon: fine-tuned AR models consistently outperform their diffusion model\nsupervisors in both subject fidelity and prompt adherence. We analyze this\nperformance shift and identify scenarios where AR models excel, particularly in\nmulti-subject compositions and contextual understanding. This work not only\ndemonstrates impressive results in subject-driven AR image generation, but also\nunveils the potential of weak-to-strong generalization in the image generation\ndomain, contributing to a deeper understanding of different architectures'\nstrengths and limitations.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 07:32:57 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wu', 'Yi', ''], ['Zhu', 'Lingting', ''], ['Liu', 'Lei', ''], ['Qiao', 'Wandi', ''], ['Li', 'Ziqiang', ''], ['Yu', 'Lequan', ''], ['Li', 'Bin', '']]","extracted_entities":"[{'text': 'Proxy-Tuning', 'label': 'Fine-tuning'}, {'text': 'prompt adherence', 'label': 'Prompting'}, {'text': 'contextual understanding', 'label': 'contextual Embedding'}]","assigned_concept":"contextual Embedding","matched_keyword":"contextual understanding","similarity_score":0.6383735538}
{"id":2503.07384,"submitter":"Gonzalo Mancera","authors":"Gonzalo Mancera, Daniel DeAlcala, Julian Fierrez, Ruben Tolosana,\n  Aythami Morales","title":"Is My Text in Your AI Model? Gradient-based Membership Inference Test\n  applied to LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  This work adapts and studies the gradient-based Membership Inference Test\n(gMINT) to the classification of text based on LLMs. MINT is a general approach\nintended to determine if given data was used for training machine learning\nmodels, and this work focuses on its application to the domain of Natural\nLanguage Processing. Using gradient-based analysis, the MINT model identifies\nwhether particular data samples were included during the language model\ntraining phase, addressing growing concerns about data privacy in machine\nlearning. The method was evaluated in seven Transformer-based models and six\ndatasets comprising over 2.5 million sentences, focusing on text classification\ntasks. Experimental results demonstrate MINTs robustness, achieving AUC scores\nbetween 85% and 99%, depending on data size and model architecture. These\nfindings highlight MINTs potential as a scalable and reliable tool for auditing\nmachine learning models, ensuring transparency, safeguarding sensitive data,\nand fostering ethical compliance in the deployment of AI\/NLP technologies.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Mar 2025 14:32:56 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 12:37:37 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Mancera', 'Gonzalo', ''], ['DeAlcala', 'Daniel', ''], ['Fierrez', 'Julian', ''], ['Tolosana', 'Ruben', ''], ['Morales', 'Aythami', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLMs'}, {'text': 'ethical compliance', 'label': 'AI Ethics'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2503.09994,"submitter":"Yunxiao Wang","authors":"Yunxiao Wang, Meng Liu, Rui Shao, Haoyu Zhang, Bin Wen, Fan Yang,\n  Tingting Gao, Di Zhang, Liqiang Nie","title":"TIME: Temporal-sensitive Multi-dimensional Instruction Tuning and\n  Benchmarking for Video-LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Video large language models have achieved remarkable performance in tasks\nsuch as video question answering, however, their temporal understanding remains\nsuboptimal. To address this limitation, we curate a dedicated instruction\nfine-tuning dataset that focuses on enhancing temporal comprehension across\nfive key dimensions. In order to reduce reliance on costly temporal\nannotations, we introduce a multi-task prompt fine-tuning approach that\nseamlessly integrates temporal-sensitive tasks into existing instruction\ndatasets without requiring additional annotations. Furthermore, we develop a\nnovel benchmark for temporal-sensitive video understanding that not only fills\nthe gaps in dimension coverage left by existing benchmarks but also rigorously\nfilters out potential shortcuts, ensuring a more accurate evaluation. Extensive\nexperimental results demonstrate that our approach significantly enhances the\ntemporal understanding of video-LLMs while avoiding reliance on shortcuts.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 03:05:11 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Yunxiao', ''], ['Liu', 'Meng', ''], ['Shao', 'Rui', ''], ['Zhang', 'Haoyu', ''], ['Wen', 'Bin', ''], ['Yang', 'Fan', ''], ['Gao', 'Tingting', ''], ['Zhang', 'Di', ''], ['Nie', 'Liqiang', '']]","extracted_entities":"[{'text': 'video-LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"video-LLMs","similarity_score":0.7106761336}
{"id":2503.10509,"submitter":"Sahar Admoni","authors":"Sahar Admoni, Omer Ben-Porat, Ofra Amir","title":"SySLLM: Generating Synthesized Policy Summaries for Reinforcement\n  Learning Agents Using Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Policies generated by Reinforcement Learning (RL) algorithms can be difficult\nto describe to users, as they result from the interplay between complex reward\nstructures and neural network-based representations. This combination often\nleads to unpredictable behaviors, making policies challenging to analyze and\nposing significant obstacles to fostering human trust in real-world\napplications. Global policy summarization methods aim to describe agent\nbehavior through a demonstration of actions in a subset of world-states.\nHowever, users can only watch a limited number of demonstrations, restricting\ntheir understanding of policies. Moreover, those methods overly rely on user\ninterpretation, as they do not synthesize observations into coherent patterns.\nIn this work, we present SySLLM (Synthesized Summary using LLMs), a novel\nmethod that employs synthesis summarization, utilizing large language models'\n(LLMs) extensive world knowledge and ability to capture patterns, to generate\ntextual summaries of policies. Specifically, an expert evaluation demonstrates\nthat the proposed approach generates summaries that capture the main insights\ngenerated by experts while not resulting in significant hallucinations.\nAdditionally, a user study shows that SySLLM summaries are preferred over\ndemonstration-based policy summaries and match or surpass their performance in\nobjective agent identification tasks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 16:10:14 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Admoni', 'Sahar', ''], ['Ben-Porat', 'Omer', ''], ['Amir', 'Ofra', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLMs'}, {'text': 'large language models', 'label': 'Large Language Model'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2111.02019,"submitter":"Juho Timonen","authors":"Juho Timonen and Harri L\\\"ahdesm\\\"aki","title":"Scalable mixed-domain Gaussian process modeling and model reduction for\n  longitudinal data","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.CO cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Gaussian process (GP) models that combine both categorical and continuous\ninput variables have found use in analysis of longitudinal data and computer\nexperiments. However, standard inference for these models has the typical cubic\nscaling, and common scalable approximation schemes for GPs cannot be applied\nsince the covariance function is non-continuous. In this work, we derive a\nbasis function approximation scheme for mixed-domain covariance functions,\nwhich scales linearly with respect to the number of observations and total\nnumber of basis functions. The proposed approach is naturally applicable to\nalso Bayesian GP regression with discrete observation models. We demonstrate\nthe scalability of the approach and compare model reduction techniques for\nadditive GP models in a longitudinal data context. We confirm that we can\napproximate the exact GP model accurately in a fraction of the runtime compared\nto fitting the corresponding exact model. In addition, we demonstrate a\nscalable model reduction workflow for obtaining smaller and more interpretable\nmodels when dealing with a large number of candidate predictors.\n","versions":"[{'version': 'v1', 'created': 'Wed, 3 Nov 2021 04:47:37 GMT'}, {'version': 'v2', 'created': 'Fri, 6 Sep 2024 09:06:25 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 00:52:01 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Timonen', 'Juho', ''], ['L\u00e4hdesm\u00e4ki', 'Harri', '']]","extracted_entities":"[{'text': 'cubic\\nscaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"cubic\nscaling","similarity_score":0.5120496154}
{"id":2404.02175,"submitter":"Javier Mar\\'in","authors":"Javier Marin","title":"Symmetries, Scaling Laws and Phase Transitions in Consumer Advertising\n  Response","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.soc-ph cs.LG q-fin.GN","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Understanding how consumers respond to business advertising efforts is\nessential for optimizing marketing investment. This research introduces a new\nmodeling approach based on the concepts of symmetries and scaling laws in\nphysics to describe consumer response to advertising dynamics. Drawing from\nmathematical frameworks used in physics and social sciences, we propose a model\nthat accounts for a key aspect: the saturation effect. The model is validated\nagainst commonly used models, including the Michaelis-Menten and Hill\nequations, showing its ability to better capture nonlinearities in advertising\neffects. We introduce new key parameters like Marketing Sensitivity, Response\nSensitivity, and Behavioral Sensitivit, that offer additional insights into the\ndrivers of audience engagement and advertising performance. Our model provides\na rigorous yet practical tool for understanding audience behavior, contributing\nto the improvement of budget allocation strategies.\n","versions":"[{'version': 'v1', 'created': 'Mon, 1 Apr 2024 11:23:31 GMT'}, {'version': 'v2', 'created': 'Fri, 18 Oct 2024 06:33:19 GMT'}, {'version': 'v3', 'created': 'Sun, 10 Nov 2024 10:10:44 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Mar 2025 18:32:02 GMT'}, {'version': 'v5', 'created': 'Thu, 13 Mar 2025 08:48:26 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Marin', 'Javier', '']]","extracted_entities":"[{'text': 'scaling laws', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling laws","similarity_score":0.9373526573}
{"id":2404.17365,"submitter":"Fleur Hendriks","authors":"Fleur Hendriks (1), Vlado Menkovski (1), Martin Do\\v{s}k\\'a\\v{r} (2),\n  Marc G. D. Geers (1), Ond\\v{r}ej Roko\\v{s} (1) ((1) Eindhoven University of\n  Technology, (2) Czech Technical University in Prague)","title":"Similarity Equivariant Graph Neural Networks for Homogenization of\n  Metamaterials","comments":"60 pages, 22 figures. Published in CMAME (Computer Methods in Applied\n  Mechanics and Engineering)","journal-ref":null,"doi":"10.1016\/j.cma.2025.117867","report-no":null,"categories":"cond-mat.soft cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Soft, porous mechanical metamaterials exhibit pattern transformations that\nmay have important applications in soft robotics, sound reduction and\nbiomedicine. To design these innovative materials, it is important to be able\nto simulate them accurately and quickly, in order to tune their mechanical\nproperties. Since conventional simulations using the finite element method\nentail a high computational cost, in this article we aim to develop a machine\nlearning-based approach that scales favorably to serve as a surrogate model. To\nensure that the model is also able to handle various microstructures, including\nthose not encountered during training, we include the microstructure as part of\nthe network input. Therefore, we introduce a graph neural network that predicts\nglobal quantities (energy, stress stiffness) as well as the pattern\ntransformations that occur (the kinematics). To make our model as accurate and\ndata-efficient as possible, various symmetries are incorporated into the model.\nThe starting point is an E(n)-equivariant graph neural network (which respects\ntranslation, rotation and reflection) that has periodic boundary conditions\n(i.e., it is in-\/equivariant with respect to the choice of RVE), is scale\nin-\/equivariant, can simulate large deformations, and can predict scalars,\nvectors as well as second and fourth order tensors (specifically energy, stress\nand stiffness). The incorporation of scale equivariance makes the model\nequivariant with respect to the similarities group, of which the Euclidean\ngroup E(n) is a subgroup. We show that this network is more accurate and\ndata-efficient than graph neural networks with fewer symmetries. To create an\nefficient graph representation of the finite element discretization, we use\nonly the internal geometrical hole boundaries from the finite element mesh to\nachieve a better speed-up and scaling with the mesh size.\n","versions":"[{'version': 'v1', 'created': 'Fri, 26 Apr 2024 12:30:32 GMT'}, {'version': 'v2', 'created': 'Mon, 9 Dec 2024 15:10:19 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 14:48:27 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Hendriks', 'Fleur', ''], ['Menkovski', 'Vlado', ''], ['Do\u0161k\u00e1\u0159', 'Martin', ''], ['Geers', 'Marc G. D.', ''], ['Roko\u0161', 'Ond\u0159ej', '']]","extracted_entities":"[{'text': 'graph neural network', 'label': 'Neural Language Model'}, {'text': 'graph neural network', 'label': 'Neural Language Model'}, {'text': 'scale equivariance', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scale equivariance","similarity_score":0.5122733116}
{"id":2409.00425,"submitter":"Ning Liu","authors":"Lu Chen, Baopi Liu, and Ning Liu","title":"Phase behaviors and dynamics of active particle systems in double-well\n  potential","comments":"7 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.soft cond-mat.stat-mech physics.bio-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this study, we investigate the behaviors and dynamics of self-propelled\nparticles with active reorientation (AR) in a double-well potential. We explore\nthe competition between AR and external potentials, revealing that\nself-propelled particles exhibit flocking and clustering behaviors in an\nasymmetric potential trap. Through molecular dynamics simulations, we obtain a\nphase diagram that illustrates flocking behavior as a function of active\nreorientation and potential asymmetry. We compare the responses of inactive and\nactive particles to the potential, finding that active reorientation\nsignificantly increases aggregation on one side of the asymmetric potential\nwell. Additionally, by calculating the mean squared displacement and scaling\nexponent, we identify distinct diffusion regimes. Our findings demonstrate that\nactive particles with active reorientation are more sensitive to variations in\ndouble-well potentials.\n","versions":"[{'version': 'v1', 'created': 'Sat, 31 Aug 2024 11:53:02 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 09:17:41 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chen', 'Lu', ''], ['Liu', 'Baopi', ''], ['Liu', 'Ning', '']]","extracted_entities":"[{'text': 'scaling\\nexponent', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling\nexponent","similarity_score":0.6063866615}
{"id":2409.11697,"submitter":"Thieu Vo","authors":"Viet-Hoang Tran and Thieu N. Vo and Tho H. Tran and An T. Nguyen and\n  Tan M. Nguyen","title":"Monomial Matrix Group Equivariant Neural Functional Networks","comments":"10 pages in the main text. Published at NeurIPS 2024. The code is\n  available at https:\/\/github.com\/MathematicalAI-NUS\/Monomial-NFN","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Neural functional networks (NFNs) have recently gained significant attention\ndue to their diverse applications, ranging from predicting network\ngeneralization and network editing to classifying implicit neural\nrepresentation. Previous NFN designs often depend on permutation symmetries in\nneural networks' weights, which traditionally arise from the unordered\narrangement of neurons in hidden layers. However, these designs do not take\ninto account the weight scaling symmetries of $\\ReLU$ networks, and the weight\nsign flipping symmetries of $\\sin$ or $\\Tanh$ networks. In this paper, we\nextend the study of the group action on the network weights from the group of\npermutation matrices to the group of monomial matrices by incorporating\nscaling\/sign-flipping symmetries. Particularly, we encode these\nscaling\/sign-flipping symmetries by designing our corresponding equivariant and\ninvariant layers. We name our new family of NFNs the Monomial Matrix Group\nEquivariant Neural Functional Networks (Monomial-NFN). Because of the expansion\nof the symmetries, Monomial-NFN has much fewer independent trainable parameters\ncompared to the baseline NFNs in the literature, thus enhancing the model's\nefficiency. Moreover, for fully connected and convolutional neural networks, we\ntheoretically prove that all groups that leave these networks invariant while\nacting on their weight spaces are some subgroups of the monomial matrix group.\nWe provide empirical evidence to demonstrate the advantages of our model over\nexisting baselines, achieving competitive performance and efficiency.\n","versions":"[{'version': 'v1', 'created': 'Wed, 18 Sep 2024 04:36:05 GMT'}, {'version': 'v2', 'created': 'Thu, 31 Oct 2024 22:55:21 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 15:36:01 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Tran', 'Viet-Hoang', ''], ['Vo', 'Thieu N.', ''], ['Tran', 'Tho H.', ''], ['Nguyen', 'An T.', ''], ['Nguyen', 'Tan M.', '']]","extracted_entities":"[{'text': 'weight scaling symmetries', 'label': 'Scaling law'}, {'text': 'scaling\/sign-flipping symmetries', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"weight scaling symmetries","similarity_score":0.5615555048}
{"id":2501.12973,"submitter":"Da-Sol Joo","authors":"Da-Sol Joo","title":"A global similarity correction for the RANS modeling of natural\n  convection in unstably stratified flows","comments":"39 pages, 11 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.flu-dyn","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This study proposes a global similarity correction for Reynolds-averaged\nNavier--Stokes (RANS) modeling of buoyancy effects in unstably stratified\nflows. Conventional two-equation RANS models (e.g., the $k$-$\\varepsilon$\nmodel) lack a clear criterion for incorporating unstable buoyancy effects in\ntheir scale-determining equations (e.g., $\\varepsilon$-equation). To address\nthis gap, a global correction function is introduced, derived from a\ngeneralized algebraic formulation that incorporates available potential energy\nas an additional parameter. This function reproduces a global similarity law\ncommonly observed in natural convection flows--for instance, the correlation\namong the Nusselt, Rayleigh, and Prandtl numbers, which can be approximately\nexpressed as a single power law over a wide parameter range. A calibration\nmethod is proposed in which an approximate analytical solution for\nRayleigh--B\\'enard convection is obtained via equilibrium analysis, confirming\nthat the proposed model captures similarity relations not addressed by\nconventional one-point closures. Numerical results show significantly improved\nagreement with experimental data, accurately reproducing Nusselt number\ndependencies over broad ranges of Rayleigh and Prandtl numbers in unstably\nstratified flows, such as Rayleigh--B\\'enard convection and two types of\ninternally heated convection. The method remains fully compatible with standard\nRANS frameworks and reverts to traditional turbulence treatments in\nshear-driven flows where buoyant effects are negligible. By introducing only a\nsingle, simple, algebraic global function in the conventional\n$\\varepsilon$-equation, this approach significantly enhances the accuracy and\nrobustness of buoyancy-driven turbulence simulations.\n","versions":"[{'version': 'v1', 'created': 'Wed, 22 Jan 2025 15:57:40 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 16:27:13 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 03:28:37 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 02:47:37 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Joo', 'Da-Sol', '']]","extracted_entities":"[{'text': 'global similarity law', 'label': 'Scaling law'}, {'text': 'Nusselt', 'label': 'BERT'}, {'text': 'Prandtl', 'label': 'BERT'}, {'text': 'single power law', 'label': 'Scaling law'}, {'text': 'Nusselt', 'label': 'BERT'}, {'text': 'Prandtl', 'label': 'BERT'}]","assigned_concept":"Scaling law","matched_keyword":"global similarity law","similarity_score":0.5060133934}
{"id":2503.09532,"submitter":"Can Ludwig Rager","authors":"Adam Karvonen, Can Rager, Johnny Lin, Curt Tigges, Joseph Bloom, David\n  Chanin, Yeu-Tong Lau, Eoin Farrell, Callum McDougall, Kola Ayonrinde, Matthew\n  Wearden, Arthur Conmy, Samuel Marks, Neel Nanda","title":"SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language\n  Model Interpretability","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Sparse autoencoders (SAEs) are a popular technique for interpreting language\nmodel activations, and there is extensive recent work on improving SAE\neffectiveness. However, most prior work evaluates progress using unsupervised\nproxy metrics with unclear practical relevance. We introduce SAEBench, a\ncomprehensive evaluation suite that measures SAE performance across seven\ndiverse metrics, spanning interpretability, feature disentanglement and\npractical applications like unlearning. To enable systematic comparison, we\nopen-source a suite of over 200 SAEs across eight recently proposed SAE\narchitectures and training algorithms. Our evaluation reveals that gains on\nproxy metrics do not reliably translate to better practical performance. For\ninstance, while Matryoshka SAEs slightly underperform on existing proxy\nmetrics, they substantially outperform other architectures on feature\ndisentanglement metrics; moreover, this advantage grows with SAE scale. By\nproviding a standardized framework for measuring progress in SAE development,\nSAEBench enables researchers to study scaling trends and make nuanced\ncomparisons between different SAE architectures and training methodologies. Our\ninteractive interface enables researchers to flexibly visualize relationships\nbetween metrics across hundreds of open-source SAEs at: https:\/\/saebench.xyz\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 16:49:02 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 03:18:16 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Karvonen', 'Adam', ''], ['Rager', 'Can', ''], ['Lin', 'Johnny', ''], ['Tigges', 'Curt', ''], ['Bloom', 'Joseph', ''], ['Chanin', 'David', ''], ['Lau', 'Yeu-Tong', ''], ['Farrell', 'Eoin', ''], ['McDougall', 'Callum', ''], ['Ayonrinde', 'Kola', ''], ['Wearden', 'Matthew', ''], ['Conmy', 'Arthur', ''], ['Marks', 'Samuel', ''], ['Nanda', 'Neel', '']]","extracted_entities":"[{'text': 'scaling trends', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling trends","similarity_score":0.6064868569}
{"id":2503.10005,"submitter":"Yongqi Li","authors":"Yongqi Li, Xiaowei Zhang","title":"Adaptive Moment Estimation Optimization Algorithm Using Projection\n  Gradient for Deep Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Training deep neural networks is challenging. To accelerate training and\nenhance performance, we propose PadamP, a novel optimization algorithm. PadamP\nis derived by applying the adaptive estimation of the p-th power of the\nsecond-order moments under scale invariance, enhancing projection adaptability\nby modifying the projection discrimination condition. It is integrated into\nAdam-type algorithms, accelerating training, boosting performance, and\nimproving generalization in deep learning. Combining projected gradient\nbenefits with adaptive moment estimation, PadamP tackles unconstrained\nnon-convex problems. Convergence for the non-convex case is analyzed, focusing\non the decoupling of first-order moment estimation coefficients and\nsecond-order moment estimation coefficients. Unlike prior work relying on , our\nproof generalizes the convergence theorem, enhancing practicality. Experiments\nusing VGG-16 and ResNet-18 on CIFAR-10 and CIFAR-100 show PadamP's\neffectiveness, with notable performance on CIFAR-10\/100, especially for VGG-16.\nThe results demonstrate that PadamP outperforms existing algorithms in terms of\nconvergence speed and generalization ability, making it a valuable addition to\nthe field of deep learning optimization.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 03:31:08 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Yongqi', ''], ['Zhang', 'Xiaowei', '']]","extracted_entities":"[{'text': 'scale invariance', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scale invariance","similarity_score":0.5601648688}
{"id":2503.10121,"submitter":"Fernando Moreno","authors":"Fernando Moreno and Emmanuel Jehin","title":"Dust shells and dark linear structures on dust tails of historical and\n  recent long-period comets","comments":"Accepted by A&A","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.EP","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Context. Dust halos or shells, along with linear dark structures along the\naxes of dust tails, are commonly observed in many long-period comets near\nperihelion. Examples range from the recent C\/2023 A3 (Tsuchinshan-ATLAS) to\nhistorical comets such as the Great Comet of 1874, C\/1874 H1 (Coggia).\n  Aims. While dust halos can readily be modeled as spin-modulated activity\noriginating from the comet nucleus, their possible connection to those dark\nlinear features has, to our knowledge, not been investigated. The aim of this\npaper is to shed light on the formation of these remarkable structures by\nmodeling a sample of six long-period comets, using similar dust physical\nproperties and ejection parameters, to explore whether they share a common\norigin.\n  Methods. To model the dust features, we employed a Monte Carlo procedure to\ngenerate synthetic images. The particles ejected from the comet nucleus follow\na power-law size distribution and are released into interplanetary space at\nspeeds determined by the ratio of solar radiation pressure to solar gravity,\nthe heliocentric distance, and, as a new feature of the code, the solar zenith\nangle at the emission point.\n  Results. We demonstrate that, in all the cases analyzed, the dust shells form\nas a result of short-term events characterized by cyclically varying ejection\nof very small particles from large surface areas on the rotating nucleus. These\nevents are triggered as these areas become freshly exposed to solar radiation\nnear perihelion due to the high obliquity of the spin axes of their nuclei. The\ndark linear stripes along the tail axes may arise from a specific dependence of\nthe ejection speeds on the square root of the cosine of the zenith angle, as is\npredicted by hydrodynamical modeling, but their presence is also dependent on\nthe extent of the latitude region of emission that defines the velocity vector\nfield.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 07:28:37 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Moreno', 'Fernando', ''], ['Jehin', 'Emmanuel', '']]","extracted_entities":"[{'text': 'power-law size distribution', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"power-law size distribution","similarity_score":0.551808238}
{"id":2503.10274,"submitter":"Zhichao Zhang","authors":"Yangfan He and Zhichao Zhang","title":"Symplectic Wigner Distribution in the Linear Canonical Transform Domain:\n  Theory and Application","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP cs.IT math.FA math.IT","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper devotes to combine the chirp basis function transformation and\nsymplectic coordinates transformation to yield a novel Wigner distribution (WD)\nassociated with the linear canonical transform (LCT), named as the symplectic\nWD in the LCT domain (SWDL). It incorporates the merits of the symplectic WD\n(SWD) and the WD in the LCT domain (WDL), achieving stronger capability in the\nlinear frequency-modulated (LFM) signal frequency rate feature extraction while\nmaintaining the same level of computational complexity. Some essential\nproperties of the SWDL are derived, including marginal distributions, energy\nconservations, unique reconstruction, Moyal formula, complex conjugate\nsymmetry, time reversal symmetry, scaling property, time translation property,\nfrequency modulation property, and time translation and frequency modulation\nproperty. Heisenberg's uncertainty principles of the SWDL are formulated,\ngiving rise to three kinds of lower bounds attainable respectively by Gaussian\nenveloped complex exponential signal, Gaussian signal and Gaussian enveloped\nchirp signal. The optimal symplectic matrices corresponding to the highest\ntime-frequency resolution are generated by solving the lower bound optimization\n(minimization) problem. The time-frequency resolution of the SWDL is compared\nwith those of the SWD and WDL to demonstrate its superiority in LFM signals\ntime-frequency energy concentration. A synthesis example is also carried out to\nverify the feasibility and reliability of the theoretical analysis.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 11:32:16 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['He', 'Yangfan', ''], ['Zhang', 'Zhichao', '']]","extracted_entities":"[{'text': 'Moyal formula', 'label': 'Scaling law'}, {'text': 'scaling property', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling property","similarity_score":0.7560453415}
{"id":2503.10392,"submitter":"Fengxiang Wang","authors":"Fengxiang Wang, Hongzhen Wang, Yulin Wang, Di Wang, Mingshuo Chen,\n  Haiyan Zhao, Yangang Sun, Shuo Wang, Long Lan, Wenjing Yang, Jing Zhang","title":"RoMA: Scaling up Mamba-based Foundation Models for Remote Sensing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advances in self-supervised learning for Vision Transformers (ViTs)\nhave fueled breakthroughs in remote sensing (RS) foundation models. However,\nthe quadratic complexity of self-attention poses a significant barrier to\nscalability, particularly for large models and high-resolution images. While\nthe linear-complexity Mamba architecture offers a promising alternative,\nexisting RS applications of Mamba remain limited to supervised tasks on small,\ndomain-specific datasets. To address these challenges, we propose RoMA, a\nframework that enables scalable self-supervised pretraining of Mamba-based RS\nfoundation models using large-scale, diverse, unlabeled data. RoMA enhances\nscalability for high-resolution images through a tailored auto-regressive\nlearning strategy, incorporating two key innovations: 1) a rotation-aware\npretraining mechanism combining adaptive cropping with angular embeddings to\nhandle sparsely distributed objects with arbitrary orientations, and 2)\nmulti-scale token prediction objectives that address the extreme variations in\nobject scales inherent to RS imagery. Systematic empirical studies validate\nthat Mamba adheres to RS data and parameter scaling laws, with performance\nscaling reliably as model and data size increase. Furthermore, experiments\nacross scene classification, object detection, and semantic segmentation tasks\ndemonstrate that RoMA-pretrained Mamba models consistently outperform ViT-based\ncounterparts in both accuracy and computational efficiency. The source code and\npretrained models will be released at https:\/\/github.com\/MiliLab\/RoMA.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 14:09:18 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Fengxiang', ''], ['Wang', 'Hongzhen', ''], ['Wang', 'Yulin', ''], ['Wang', 'Di', ''], ['Chen', 'Mingshuo', ''], ['Zhao', 'Haiyan', ''], ['Sun', 'Yangang', ''], ['Wang', 'Shuo', ''], ['Lan', 'Long', ''], ['Yang', 'Wenjing', ''], ['Zhang', 'Jing', '']]","extracted_entities":"[{'text': 'Vision Transformers', 'label': 'Transformers'}, {'text': 'Mamba', 'label': 'Foundation Model'}, {'text': 'Mamba', 'label': 'Foundation Model'}, {'text': 'RoMA', 'label': 'RoBERTa'}, {'text': 'RoMA', 'label': 'RoBERTa'}, {'text': 'angular embeddings', 'label': 'Embedding'}, {'text': 'Mamba', 'label': 'Foundation Model'}, {'text': 'parameter scaling laws', 'label': 'Scaling law'}, {'text': 'Mamba', 'label': 'Foundation Model'}, {'text': 'RoMA', 'label': 'RoBERTa'}]","assigned_concept":"Scaling law","matched_keyword":"parameter scaling laws","similarity_score":0.7594408989}
{"id":2503.10531,"submitter":"Sylvain Prolhac","authors":"Sylvain Prolhac","title":"Current fluctuations for the second class particle : joint statistics","comments":"17 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.stat-mech math-ph math.MP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We consider TASEP with a single second class particle and periodic boundary\nconditions. Using Bethe ansatz, we compute stationary large deviations for the\njoint statistics of the current of first and second class particles. At large\nscales, the generating function of the joint cumulants shows an unexpected\nconnection to current fluctuations of TASEP with open boundaries.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 16:39:21 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Prolhac', 'Sylvain', '']]","extracted_entities":"[{'text': 'large\\nscales', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"large\nscales","similarity_score":0.5389848948}
{"id":2503.10553,"submitter":"Eric Boltersdorf","authors":"Eric Boltersdorf and Thilo vom H\\\"ovel and Jeremy Andrew Mor\\'in\n  Nenoff and Frank Vewinger and Martin Weitz","title":"Two-photon spectroscopy and a verification of the Kennard-Stepanov\n  relation in high-pressure two-species xenon-noble gas mixtures","comments":"8 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.optics","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Between the absorption and the emission spectral lineshapes of dense atomic\nand molecular media, such as dye solutions and alkali-noble buffer gas mixtures\nat high pressure, in many cases there exists a universal scaling, the\nKennard-Stepanov relation, which is a manifestation of detailed balance. This\nrelation plays a crucial role in recent Bose-Einstein condensation experiments\nof visible-spectral-photons in e.g. dye-solution-filled optical microcavities.\nIt has recently been proposed to use high-pressure xenon-noble gas mixtures as\na thermalization medium for vacuum-ultraviolet regime photons, so as to extend\nthe achievable wavelength range of such Bose-Einstein-condensed optical sources\nfrom the visible to the vacuum-ultraviolet regime. In this work, we report\ntwo-photon excitation spectroscopy measurements of ground state ($5p^6$) xenon\natoms subject to up to 80bar of helium or krypton buffer gas pressure,\nrespectively, in the 220nm - 260nm wavelength range. The study of such\ntwo-photon spectra is of interest e.g. for the exploration of possible pumping\nschemes of a future vacuum-ultraviolet photon Bose-Einstein condensate. We have\nalso recorded absorption and emission spectra of the $5p^6 \\leftrightarrow\n5p^56s$ single-photon transition near 147nm wavelength of xenon atoms subject\nto 80bar of krypton buffer gas pressure. We find that the ratio of absorption\nand emission follows a Kennard-Stepanov scaling, which suggests that such gas\nmixtures are promising candidates as a thermalization medium for a\nBose-Einstein condensate of vacuum-ultraviolet photons.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:09:05 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Boltersdorf', 'Eric', ''], ['H\u00f6vel', 'Thilo vom', ''], ['Nenoff', 'Jeremy Andrew Mor\u00edn', ''], ['Vewinger', 'Frank', ''], ['Weitz', 'Martin', '']]","extracted_entities":"[{'text': 'universal scaling', 'label': 'Scaling law'}, {'text': 'Kennard-Stepanov relation', 'label': 'Scaling law'}, {'text': 'Kennard-Stepanov scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"universal scaling","similarity_score":0.6292396784}
{"id":2503.10584,"submitter":"Jun-Kun Zhao","authors":"Jun-Kun Zhao and Li Li","title":"Holographic study of shear viscosity and butterfly velocity for magnetic\n  field-driven quantum criticality","comments":"19 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-th cond-mat.str-el","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We investigate the shear viscosity and butterfly velocity of a magnetic\nfield-induced quantum phase transition in five dimensional\nEinstein-Maxwell-Chern-Simons theory, which is holographically dual to a class\nof strongly coupled quantum field theories with chiral anomalies. Our analysis\nreveals that the ratio of longitudinal shear viscosity to entropy density\n$\\eta_\\parallel\/s$ exhibits a pronounced non-monotonic dependence on\ntemperature $T$ when the magnetic field $B$ is slightly below the critical\nvalue $B_c$ of the quantum phase transition. In particular, it can develop a\ndistinct minimum at an intermediate temperature. This contrasts sharply with\nthe monotonic temperature scaling observed at and above $B_c$, where\n$\\eta_\\parallel\/s$ follows the scaling $T^{2\/3}$ at $B=B_c$ and transitions to\n$T^2$ for $B>B_c$ as $T\\to0$. The non-vanishing of $\\eta_\\parallel\/s$ for\n$B<B_c$ in the zero temperature limit suggests that it could serve as a good\norder parameter of the quantum phase transition. We also find that all\nbutterfly velocities change dramatically near the quantum phase transition, and\nthus their derivatives with respect to $B$ can be independently used to detect\nthe quantum critical point.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:34:44 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhao', 'Jun-Kun', ''], ['Li', 'Li', '']]","extracted_entities":"[{'text': 'monotonic temperature scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"monotonic temperature scaling","similarity_score":0.5399912}
{"id":2405.14529,"submitter":"Simon Damm","authors":"Simon Damm, Mike Laszkiewicz, Johannes Lederer, Asja Fischer","title":"AnomalyDINO: Boosting Patch-based Few-shot Anomaly Detection with DINOv2","comments":"Accepted at WACV 2025 (Oral)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent advances in multimodal foundation models have set new standards in\nfew-shot anomaly detection. This paper explores whether high-quality visual\nfeatures alone are sufficient to rival existing state-of-the-art\nvision-language models. We affirm this by adapting DINOv2 for one-shot and\nfew-shot anomaly detection, with a focus on industrial applications. We show\nthat this approach does not only rival existing techniques but can even\noutmatch them in many settings. Our proposed vision-only approach, AnomalyDINO,\nfollows the well-established patch-level deep nearest neighbor paradigm, and\nenables both image-level anomaly prediction and pixel-level anomaly\nsegmentation. The approach is methodologically simple and training-free and,\nthus, does not require any additional data for fine-tuning or meta-learning.\nThe approach is methodologically simple and training-free and, thus, does not\nrequire any additional data for fine-tuning or meta-learning. Despite its\nsimplicity, AnomalyDINO achieves state-of-the-art results in one- and few-shot\nanomaly detection (e.g., pushing the one-shot performance on MVTec-AD from an\nAUROC of 93.1% to 96.6%). The reduced overhead, coupled with its outstanding\nfew-shot performance, makes AnomalyDINO a strong candidate for fast deployment,\ne.g., in industrial contexts.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 May 2024 13:15:13 GMT'}, {'version': 'v2', 'created': 'Thu, 12 Sep 2024 09:23:32 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 09:32:39 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Damm', 'Simon', ''], ['Laszkiewicz', 'Mike', ''], ['Lederer', 'Johannes', ''], ['Fischer', 'Asja', '']]","extracted_entities":"[{'text': 'multimodal foundation models', 'label': 'Foundation Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2406.04094,"submitter":"Zixi Chen","authors":"Zixi Chen, Xuyang Ren, Yuya Hamamatsu, Gastone Ciuti, Cesare Stefanini","title":"A Generalized Adaptive Jacobian Controller for Soft Robots","comments":"10 pages, 8 figures, 4 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The nonlinearity and hysteresis of soft robot motions have posed challenges\nin control. The Jacobian controller is transferred from rigid robot controllers\nand exhibits conciseness, but the improper assumption of soft robots induces\nthe feasibility only in a small local area. Accurate controllers like neural\nnetworks can deal with delayed and nonlinear motion, achieving high accuracy,\nbut they suffer from the high data amount requirement and black-box property.\nInspired by these approaches, we propose an adaptive generalized Jacobian\ncontroller for soft robots. This controller is constructed by the concise\nformat of the Jacobian controller but includes more states and independent\nmatrices, which is suitable for soft robotics. In addition, the initialization\nleverages the motor babbling strategy and batch optimization from neural\nnetwork controllers. In experiments, we first analyze the online controllers,\nincluding the Jacobian controller, the Gaussian process regression, and our\ncontroller. Real experiments have validated that our controller outperforms the\nRNN controller even with fewer data samples, and it is adaptive to various\nsituations without fine-tuning, like different control frequencies, softness,\nand even manufacturing errors. Future work may include online adjustment of the\ncontroller format and adaptability validation in more scenarios.\n","versions":"[{'version': 'v1', 'created': 'Thu, 6 Jun 2024 14:11:09 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 13:04:28 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chen', 'Zixi', ''], ['Ren', 'Xuyang', ''], ['Hamamatsu', 'Yuya', ''], ['Ciuti', 'Gastone', ''], ['Stefanini', 'Cesare', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2406.13035,"submitter":"Zhongwei Wan","authors":"Zhongwei Wan, Xinjian Wu, Yu Zhang, Yi Xin, Chaofan Tao, Zhihong Zhu,\n  Xin Wang, Siqi Luo, Jing Xiong, Longyue Wang, Mi Zhang","title":"D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Jun 2024 20:01:51 GMT'}, {'version': 'v2', 'created': 'Sun, 23 Jun 2024 08:27:48 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 03:16:43 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wan', 'Zhongwei', ''], ['Wu', 'Xinjian', ''], ['Zhang', 'Yu', ''], ['Xin', 'Yi', ''], ['Tao', 'Chaofan', ''], ['Zhu', 'Zhihong', ''], ['Wang', 'Xin', ''], ['Luo', 'Siqi', ''], ['Xiong', 'Jing', ''], ['Wang', 'Longyue', ''], ['Zhang', 'Mi', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2407.01131,"submitter":"Xuyang Liu","authors":"Xuyang Liu, Ting Liu, Siteng Huang, Yi Xin, Yue Hu, Quanjun Yin,\n  Donglin Wang, Yuanyuan Wu, Honggang Chen","title":"M2IST: Multi-Modal Interactive Side-Tuning for Efficient Referring\n  Expression Comprehension","comments":"Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Referring expression comprehension (REC) is a vision-language task to locate\na target object in an image based on a language expression. Fully fine-tuning\ngeneral-purpose pre-trained vision-language foundation models for REC yields\nimpressive performance but becomes increasingly costly. Parameter-efficient\ntransfer learning (PETL) methods have shown strong performance with fewer\ntunable parameters. However, directly applying PETL to REC faces two\nchallenges: (1) insufficient multi-modal interaction between pre-trained\nvision-language foundation models, and (2) high GPU memory usage due to\ngradients passing through the heavy vision-language foundation models. To this\nend, we present M2IST: Multi-Modal Interactive Side-Tuning with M3ISAs: Mixture\nof Multi-Modal Interactive Side-Adapters. During fine-tuning, we fix the\npre-trained uni-modal encoders and update M3ISAs to enable efficient\nvision-language alignment for REC. Empirical results reveal that M2IST achieves\nbetter performance-efficiency trade-off than full fine-tuning and other PETL\nmethods, requiring only 2.11\\% tunable parameters, 39.61\\% GPU memory, and\n63.46\\% training time while maintaining competitive performance. Our code is\nreleased at https:\/\/github.com\/xuyang-liu16\/M2IST.\n","versions":"[{'version': 'v1', 'created': 'Mon, 1 Jul 2024 09:53:53 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Oct 2024 12:57:42 GMT'}, {'version': 'v3', 'created': 'Sun, 16 Feb 2025 18:44:39 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 08:48:16 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Liu', 'Xuyang', ''], ['Liu', 'Ting', ''], ['Huang', 'Siteng', ''], ['Xin', 'Yi', ''], ['Hu', 'Yue', ''], ['Yin', 'Quanjun', ''], ['Wang', 'Donglin', ''], ['Wu', 'Yuanyuan', ''], ['Chen', 'Honggang', '']]","extracted_entities":"[{'text': 'PETL', 'label': 'Few-shot Learning'}, {'text': 'M3ISAs', 'label': 'Foundation Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'M3ISAs', 'label': 'Foundation Model'}, {'text': 'full fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2409.15658,"submitter":"Siyuan Liu","authors":"Siyuan Liu, Jiawei Du, Sicheng Xiang, Zibo Wang and Dingsheng Luo","title":"Long-horizon Embodied Planning with Implicit Logical Inference and\n  Hallucination Mitigation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Long-horizon embodied planning underpins embodied AI. To accomplish\nlong-horizon tasks, one of the most feasible ways is to decompose abstract\ninstructions into a sequence of actionable steps. Foundation models still face\nlogical errors and hallucinations in long-horizon planning, unless provided\nwith highly relevant examples to the tasks. However, providing highly relevant\nexamples for any random task is unpractical. Therefore, we present ReLEP, a\nnovel framework for Real-time Long-horizon Embodied Planning. ReLEP can\ncomplete a wide range of long-horizon tasks without in-context examples by\nlearning implicit logical inference through fine-tuning. The fine-tuned large\nvision-language model formulates plans as sequences of skill functions. These\nfunctions are selected from a carefully designed skill library. ReLEP is also\nequipped with a Memory module for plan and status recall, and a Robot\nConfiguration module for versatility across robot types. In addition, we\npropose a data generation pipeline to tackle dataset scarcity. When\nconstructing the dataset, we considered the implicit logical relationships,\nenabling the model to learn implicit logical relationships and dispel\nhallucinations. Through comprehensive evaluations across various long-horizon\ntasks, ReLEP demonstrates high success rates and compliance to execution even\non unseen tasks and outperforms state-of-the-art baseline methods.\n","versions":"[{'version': 'v1', 'created': 'Tue, 24 Sep 2024 01:47:23 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 10:15:59 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Liu', 'Siyuan', ''], ['Du', 'Jiawei', ''], ['Xiang', 'Sicheng', ''], ['Wang', 'Zibo', ''], ['Luo', 'Dingsheng', '']]","extracted_entities":"[{'text': 'Foundation models', 'label': 'Foundation Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2410.05116,"submitter":"Shang-Fu Chen","authors":"Ayano Hiranaka, Shang-Fu Chen, Chieh-Hsin Lai, Dongjun Kim, Naoki\n  Murata, Takashi Shibuya, Wei-Hsiang Liao, Shao-Hua Sun, Yuki Mitsufuji","title":"HERO: Human-Feedback Efficient Reinforcement Learning for Online\n  Diffusion Model Finetuning","comments":"Published in International Conference on Learning Representations\n  (ICLR) 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Controllable generation through Stable Diffusion (SD) fine-tuning aims to\nimprove fidelity, safety, and alignment with human guidance. Existing\nreinforcement learning from human feedback methods usually rely on predefined\nheuristic reward functions or pretrained reward models built on large-scale\ndatasets, limiting their applicability to scenarios where collecting such data\nis costly or difficult. To effectively and efficiently utilize human feedback,\nwe develop a framework, HERO, which leverages online human feedback collected\non the fly during model learning. Specifically, HERO features two key\nmechanisms: (1) Feedback-Aligned Representation Learning, an online training\nmethod that captures human feedback and provides informative learning signals\nfor fine-tuning, and (2) Feedback-Guided Image Generation, which involves\ngenerating images from SD's refined initialization samples, enabling faster\nconvergence towards the evaluator's intent. We demonstrate that HERO is 4x more\nefficient in online feedback for body part anomaly correction compared to the\nbest existing method. Additionally, experiments show that HERO can effectively\nhandle tasks like reasoning, counting, personalization, and reducing NSFW\ncontent with only 0.5K online feedback. The code and project page are available\nat https:\/\/hero-dm.github.io\/.\n","versions":"[{'version': 'v1', 'created': 'Mon, 7 Oct 2024 15:12:01 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Mar 2025 17:11:55 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 08:12:07 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Hiranaka', 'Ayano', ''], ['Chen', 'Shang-Fu', ''], ['Lai', 'Chieh-Hsin', ''], ['Kim', 'Dongjun', ''], ['Murata', 'Naoki', ''], ['Shibuya', 'Takashi', ''], ['Liao', 'Wei-Hsiang', ''], ['Sun', 'Shao-Hua', ''], ['Mitsufuji', 'Yuki', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Feedback-Aligned Representation Learning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Feedback-Guided Image Generation', 'label': 'Few-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2410.06846,"submitter":"Mutian He","authors":"Mutian He, Philip N. Garner","title":"Joint Fine-tuning and Conversion of Pretrained Speech and Language\n  Models towards Linear Complexity","comments":"18 pages, 5 figures; ICLR 2025 camera ready. Code:\n  https:\/\/github.com\/idiap\/linearize-distill-pretrained-transformers","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG cs.SD eess.AS","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Architectures such as Linformer and Mamba have recently emerged as\ncompetitive linear time replacements for transformers. However, corresponding\nlarge pretrained models are often unavailable, especially in non-text domains.\nTo remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)\napproach that jointly converts a transformer model to a linear time substitute\nand fine-tunes it to a target task. We also compare several means to guide the\nfine-tuning to optimally retain the desired inference capability from the\noriginal model. The methods differ in their use of the target model and the\ntrajectory of the parameters. In a series of empirical studies on language\nprocessing, language modeling, and speech processing, we show that CALD can\neffectively recover the result of the original model, and that the guiding\nstrategy contributes to the result. Some reasons for the variation are\nsuggested.\n","versions":"[{'version': 'v1', 'created': 'Wed, 9 Oct 2024 13:06:43 GMT'}, {'version': 'v2', 'created': 'Mon, 23 Dec 2024 13:53:32 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Feb 2025 13:08:42 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 16:17:19 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['He', 'Mutian', ''], ['Garner', 'Philip N.', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'guiding\\nstrategy', 'label': 'Prompting'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2410.12854,"submitter":"Weibin Liao","authors":"Weibin Liao, Xu Chu, Yasha Wang","title":"TPO: Aligning Large Language Models with Multi-branch & Multi-step\n  Preference Trees","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In the domain of complex reasoning tasks, such as mathematical reasoning,\nrecent advancements have proposed the use of Direct Preference Optimization\n(DPO) to suppress output of dispreferred responses, thereby enhancing the\nlong-chain reasoning capabilities of large language models (LLMs). To this end,\nthese studies employed LLMs to generate preference trees via Tree-of-thoughts\n(ToT) and sample the paired preference responses required by the DPO algorithm.\nHowever, the DPO algorithm based on binary preference optimization is unable to\nlearn multiple responses with varying degrees of preference\/dispreference that\nprovided by the preference trees, resulting in incomplete preference learning.\nIn this work, we introduce Tree Preference Optimization (TPO), that does not\nsample paired preference responses from the preference tree; instead, it\ndirectly learns from the entire preference tree during the fine-tuning.\nSpecifically, TPO formulates the language model alignment as a Preference List\nRanking problem, where the policy can potentially learn more effectively from a\nranked preference list of responses given the prompt. In addition, to further\nassist LLMs in identifying discriminative steps within long-chain reasoning and\nincrease the relative reward margin in the preference list, TPO utilizes\nAdaptive Step Reward to adjust the reward values of each step in trajectory for\nperforming fine-grained preference optimization. We carry out extensive\nexperiments on mathematical reasoning tasks to evaluate TPO. The experimental\nresults indicate that TPO consistently outperforms DPO across five public large\nlanguage models on four datasets.\n","versions":"[{'version': 'v1', 'created': 'Thu, 10 Oct 2024 22:22:05 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 06:40:44 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Liao', 'Weibin', ''], ['Chu', 'Xu', ''], ['Wang', 'Yasha', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'Tree-of-thoughts', 'label': 'Chain of thought'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'large\\nlanguage models', 'label': 'Large Language Model'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2411.13022,"submitter":"Ya\\c{s}ar Utku Al\\c{c}alar","authors":"Ya\\c{s}ar Utku Al\\c{c}alar, Merve G\\\"ulle, Mehmet Ak\\c{c}akaya","title":"Fast MRI for All: Bridging Equity Gaps via Training without Raw Data\n  Access","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.AI cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Physics-driven deep learning (PD-DL) approaches have become popular for\nimproved reconstruction of fast magnetic resonance imaging (MRI) scans. Though\nPD-DL offers higher acceleration rates than existing clinical fast MRI\ntechniques, their use has been limited outside specialized MRI centers. A key\nchallenge is generalization to underrepresented pathologies or populations,\nnoted in multiple studies, with fine-tuning on target populations suggested for\nimprovement. However, current approaches for PD-DL training require access to\nraw k-space measurements, which is typically only available at specialized MRI\ncenters that have research agreements for such data access. This is especially\nan issue for rural and underserved areas, where commercial MRI scanners only\nprovide access to a final reconstructed image. To tackle these challenges, we\npropose Compressibility-inspired Unsupervised Learning via Parallel Imaging\nFidelity (CUPID) for high-quality PD-DL training using only routine clinical\nreconstructed images exported from an MRI scanner. CUPID evaluates output\nquality with a compressibility-based approach while ensuring that the output\nstays consistent with the clinical parallel imaging reconstruction through\nwell-designed perturbations. Our results show CUPID achieves similar quality to\nestablished PD-DL training that requires k-space data while outperforming\ncompressed sensing (CS) and diffusion-based generative methods. We further\ndemonstrate its effectiveness in a zero-shot training setup for retrospectively\nand prospectively sub-sampled acquisitions, attesting to its minimal training\nburden. As an approach that radically deviates from existing strategies, CUPID\npresents an opportunity to provide equitable access to fast MRI for underserved\npopulations in an attempt to reduce the inequalities associated with this\nexpensive imaging modality.\n","versions":"[{'version': 'v1', 'created': 'Wed, 20 Nov 2024 03:53:41 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 15:54:28 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Al\u00e7alar', 'Ya\u015far Utku', ''], ['G\u00fclle', 'Merve', ''], ['Ak\u00e7akaya', 'Mehmet', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'research agreements', 'label': 'AI Ethics'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2411.17274,"submitter":"Yikun Li","authors":"Yikun Li, Ting Zhang, Ratnadira Widyasari, Yan Naing Tun, Huu Hung\n  Nguyen, Tan Bui, Ivana Clairine Irsan, Yiran Cheng, Xiang Lan, Han Wei Ang,\n  Frank Liauw, Martin Weyssow, Hong Jin Kang, Eng Lieh Ouh, Lwin Khin Shar,\n  David Lo","title":"CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.CR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 8,203\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul.\n","versions":"[{'version': 'v1', 'created': 'Tue, 26 Nov 2024 09:51:55 GMT'}, {'version': 'v2', 'created': 'Thu, 28 Nov 2024 03:52:23 GMT'}, {'version': 'v3', 'created': 'Thu, 16 Jan 2025 04:08:15 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 10:41:04 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Yikun', ''], ['Zhang', 'Ting', ''], ['Widyasari', 'Ratnadira', ''], ['Tun', 'Yan Naing', ''], ['Nguyen', 'Huu Hung', ''], ['Bui', 'Tan', ''], ['Irsan', 'Ivana Clairine', ''], ['Cheng', 'Yiran', ''], ['Lan', 'Xiang', ''], ['Ang', 'Han Wei', ''], ['Liauw', 'Frank', ''], ['Weyssow', 'Martin', ''], ['Kang', 'Hong Jin', ''], ['Ouh', 'Eng Lieh', ''], ['Shar', 'Lwin Khin', ''], ['Lo', 'David', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2412.00196,"submitter":"Xiyuan Gao","authors":"Xiyuan Gao","title":"Spontaneous CP Violation and Flavor Changing Neutral Currents in Minimal\n  SO(10)","comments":"24 pages, 4 figures; version published in PRD","journal-ref":"Phys. Rev. D 111, 055013 (2025)","doi":"10.1103\/PhysRevD.111.055013","report-no":null,"categories":"hep-ph hep-ex","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We explore spontaneous CP violation (SCPV) in the minimal non-supersymmetric\nSO(10) grand unified theory (GUT), with a scalar sector comprising a CP-even\n$45_H$, a $126_H$, and a complex $10_H$. All renormalizable couplings are real\ndue to CP symmetry, and the Kobayashi-Maskawa phase arises solely from complex\nelectroweak vacuum expectation values. The model requires an additional Higgs\ndoublet fine-tuned below 500 GeV and constrains new Yukawa couplings, linking\ncertain flavor-violating (FV) processes. Future proton decay observations may\nreveal correlated FV decay ratios, offering insights into minimal SO(10).\n","versions":"[{'version': 'v1', 'created': 'Fri, 29 Nov 2024 19:00:26 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 09:01:27 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Gao', 'Xiyuan', '']]","extracted_entities":"[{'text': 'fine-tuned below 500 GeV', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuned below 500 GeV","similarity_score":0.5089265108}
{"id":2412.01254,"submitter":"Liangwei Jiang","authors":"Liangwei Jiang, Ruida Li, Zhifeng Zhang, Shuo Fang, Chenguang Ma","title":"EmojiDiff: Advanced Facial Expression Control with High Identity\n  Preservation in Portrait Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper aims to bring fine-grained expression control while maintaining\nhigh-fidelity identity in portrait generation. This is challenging due to the\nmutual interference between expression and identity: (i) fine expression\ncontrol signals inevitably introduce appearance-related semantics (e.g., facial\ncontours, and ratio), which impact the identity of the generated portrait; (ii)\neven coarse-grained expression control can cause facial changes that compromise\nidentity, since they all act on the face. These limitations remain unaddressed\nby previous generation methods, which primarily rely on coarse control signals\nor two-stage inference that integrates portrait animation. Here, we introduce\nEmojiDiff, the first end-to-end solution that enables simultaneous control of\nextremely detailed expression (RGB-level) and high-fidelity identity in\nportrait generation. To address the above challenges, EmojiDiff adopts a\ntwo-stage scheme involving decoupled training and fine-tuning. For decoupled\ntraining, we innovate ID-irrelevant Data Iteration (IDI) to synthesize\ncross-identity expression pairs by dividing and optimizing the processes of\nmaintaining expression and altering identity, thereby ensuring stable and\nhigh-quality data generation. Training the model with this data, we effectively\ndisentangle fine expression features in the expression template from other\nextraneous information (e.g., identity, skin). Subsequently, we present\nID-enhanced Contrast Alignment (ICA) for further fine-tuning. ICA achieves\nrapid reconstruction and joint supervision of identity and expression\ninformation, thus aligning identity representations of images with and without\nexpression control. Experimental results demonstrate that our method remarkably\noutperforms counterparts, achieves precise expression control with highly\nmaintained identity, and generalizes well to various diffusion models.\n","versions":"[{'version': 'v1', 'created': 'Mon, 2 Dec 2024 08:24:11 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 08:32:46 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Jiang', 'Liangwei', ''], ['Li', 'Ruida', ''], ['Zhang', 'Zhifeng', ''], ['Fang', 'Shuo', ''], ['Ma', 'Chenguang', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2412.1678,"submitter":"Changchang Sun","authors":"Changchang Sun and Ren Wang and Yihua Zhang and Jinghan Jia and\n  Jiancheng Liu and Gaowen Liu and Sijia Liu and Yan Yan","title":"Forget Vectors at Play: Universal Input Perturbations Driving Machine\n  Unlearning in Image Classification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Machine unlearning (MU), which seeks to erase the influence of specific\nunwanted data from already-trained models, is becoming increasingly vital in\nmodel editing, particularly to comply with evolving data regulations like the\n``right to be forgotten''. Conventional approaches are predominantly\nmodel-based, typically requiring retraining or fine-tuning the model's weights\nto meet unlearning requirements. In this work, we approach the MU problem from\na novel input perturbation-based perspective, where the model weights remain\nintact throughout the unlearning process. We demonstrate the existence of a\nproactive input-based unlearning strategy, referred to forget vector, which can\nbe generated as an input-agnostic data perturbation and remains as effective as\nmodel-based approximate unlearning approaches. We also explore forget vector\narithmetic, whereby multiple class-specific forget vectors are combined through\nsimple operations (e.g., linear combinations) to generate new forget vectors\nfor unseen unlearning tasks, such as forgetting arbitrary subsets across\nclasses. Extensive experiments validate the effectiveness and adaptability of\nthe forget vector, showcasing its competitive performance relative to\nstate-of-the-art model-based methods. Codes are available at\nhttps:\/\/github.com\/Changchangsun\/Forget-Vector.\n","versions":"[{'version': 'v1', 'created': 'Sat, 21 Dec 2024 21:27:22 GMT'}, {'version': 'v2', 'created': 'Wed, 8 Jan 2025 17:00:18 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 01:25:27 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Sun', 'Changchang', ''], ['Wang', 'Ren', ''], ['Zhang', 'Yihua', ''], ['Jia', 'Jinghan', ''], ['Liu', 'Jiancheng', ''], ['Liu', 'Gaowen', ''], ['Liu', 'Sijia', ''], ['Yan', 'Yan', '']]","extracted_entities":"[{'text': 'Machine unlearning', 'label': 'Zero-shot Learning'}, {'text': 'evolving data regulations', 'label': 'AI Ethics'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2412.17741,"submitter":"Rui Qian","authors":"Rui Qian, Xin Yin, Dejing Dou","title":"Reasoning to Attend: Try to Understand How <SEG> Token Works","comments":"This work has been accepted to CVPR 2025, please refer to\n  https:\/\/github.com\/rui-qian\/READ","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Current Large Multimodal Models (LMMs) empowered visual grounding typically\nrely on $\\texttt{<SEG>}$ tokens as a text prompt to jointly optimize the\nvision-language model (e.g., LLaVA) and the downstream task-specific model\n(e.g., SAM). However, we observe that little research has looked into how it\nworks.In this work, we first visualize the similarity maps, which are obtained\nby computing the semantic similarity between the $\\texttt{<SEG>}$ token and the\nimage token embeddings derived from the last hidden layer in both the LLaVA\nencoder and SAM decoder. Intriguingly, we have found that a striking\nconsistency holds in terms of activation responses in the similarity map, which\nreveals that what the $\\texttt{<SEG>}$ token contributes to is semantic\nsimilarity within image-text pairs. Specifically, the $\\texttt{<SEG>}$ token, a\nplaceholder expanded in text vocabulary, extensively queries among individual\ntokenized image patches to match the semantics of an object from text to the\npaired image, while the Large Language Models (LLMs) are being fine-tuned. Upon\nthe above findings, we present READ, which facilitates LMMs' resilient\n$\\textbf{REA}$soning capability of where to atten$\\textbf{D}$ under the\nguidance of highly activated points borrowed from similarity maps. Remarkably,\nREAD features an intuitive design, Similarity as Points module (SasP), which\ncan be seamlessly applied to $\\texttt{<SEG>}$-like paradigms in a plug-and-play\nfashion. Also, extensive experiments have been conducted on ReasonSeg and\nRefCOCO(+\/g) datasets. To validate whether READ suffers from catastrophic\nforgetting of previous skills after fine-tuning, we further assess its\ngeneration ability on an augmented FP-RefCOCO(+\/g) dataset. All codes and\nmodels are publicly available at https:\/\/github.com\/rui-qian\/READ.\n","versions":"[{'version': 'v1', 'created': 'Mon, 23 Dec 2024 17:44:05 GMT'}, {'version': 'v2', 'created': 'Wed, 25 Dec 2024 10:19:44 GMT'}, {'version': 'v3', 'created': 'Mon, 20 Jan 2025 07:57:50 GMT'}, {'version': 'v4', 'created': 'Wed, 5 Mar 2025 15:55:51 GMT'}, {'version': 'v5', 'created': 'Thu, 6 Mar 2025 04:11:30 GMT'}, {'version': 'v6', 'created': 'Thu, 13 Mar 2025 14:04:12 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Qian', 'Rui', ''], ['Yin', 'Xin', ''], ['Dou', 'Dejing', '']]","extracted_entities":"[{'text': 'text prompt', 'label': 'Prompting'}, {'text': 'SAM', 'label': 'Large Language Model'}, {'text': 'image token embeddings', 'label': 'Embedding'}, {'text': 'SAM', 'label': 'Large Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2501.15187,"submitter":"Zecheng Li","authors":"Zecheng Li, Wengang Zhou, Weichao Zhao, Kepeng Wu, Hezhen Hu, Houqiang\n  Li","title":"Uni-Sign: Toward Unified Sign Language Understanding at Scale","comments":"Accepted by ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Sign language pre-training has gained increasing attention for its ability to\nenhance performance across various sign language understanding (SLU) tasks.\nHowever, existing methods often suffer from a gap between pre-training and\nfine-tuning, leading to suboptimal results. To address this, we propose\nUni-Sign, a unified pre-training framework that eliminates the gap between\npre-training and downstream SLU tasks through a large-scale generative\npre-training strategy and a novel fine-tuning paradigm. First, we introduce\nCSL-News, a large-scale Chinese Sign Language (CSL) dataset containing 1,985\nhours of video paired with textual annotations, which enables effective\nlarge-scale pre-training. Second, Uni-Sign unifies SLU tasks by treating\ndownstream tasks as a single sign language translation (SLT) task during\nfine-tuning, ensuring seamless knowledge transfer between pre-training and\nfine-tuning. Furthermore, we incorporate a prior-guided fusion (PGF) module and\na score-aware sampling strategy to efficiently fuse pose and RGB information,\naddressing keypoint inaccuracies and improving computational efficiency.\nExtensive experiments across multiple SLU benchmarks demonstrate that Uni-Sign\nachieves state-of-the-art performance across multiple downstream SLU tasks.\nDataset and code are available at github.com\/ZechengLi19\/Uni-Sign.\n","versions":"[{'version': 'v1', 'created': 'Sat, 25 Jan 2025 11:51:23 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Jan 2025 09:44:28 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 12:51:29 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Zecheng', ''], ['Zhou', 'Wengang', ''], ['Zhao', 'Weichao', ''], ['Wu', 'Kepeng', ''], ['Hu', 'Hezhen', ''], ['Li', 'Houqiang', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2502.18008,"submitter":"Yashan Wang","authors":"Yashan Wang, Shangda Wu, Jianhuai Hu, Xingjian Du, Yueqi Peng, Yongxin\n  Huang, Shuai Fan, Xiaobing Li, Feng Yu, Maosong Sun","title":"NotaGen: Advancing Musicality in Symbolic Music Generation with Large\n  Language Model Training Paradigms","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.AI eess.AS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce NotaGen, a symbolic music generation model aiming to explore the\npotential of producing high-quality classical sheet music. Inspired by the\nsuccess of Large Language Models (LLMs), NotaGen adopts pre-training,\nfine-tuning, and reinforcement learning paradigms (henceforth referred to as\nthe LLM training paradigms). It is pre-trained on 1.6M pieces of music in ABC\nnotation, and then fine-tuned on approximately 9K high-quality classical\ncompositions conditioned on \"period-composer-instrumentation\" prompts. For\nreinforcement learning, we propose the CLaMP-DPO method, which further enhances\ngeneration quality and controllability without requiring human annotations or\npredefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in\nsymbolic music generation models with different architectures and encoding\nschemes. Furthermore, subjective A\/B tests show that NotaGen outperforms\nbaseline models against human compositions, greatly advancing musical\naesthetics in symbolic music generation.\n","versions":"[{'version': 'v1', 'created': 'Tue, 25 Feb 2025 09:12:07 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 08:18:41 GMT'}, {'version': 'v3', 'created': 'Thu, 27 Feb 2025 07:02:39 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 13:50:00 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Yashan', ''], ['Wu', 'Shangda', ''], ['Hu', 'Jianhuai', ''], ['Du', 'Xingjian', ''], ['Peng', 'Yueqi', ''], ['Huang', 'Yongxin', ''], ['Fan', 'Shuai', ''], ['Li', 'Xiaobing', ''], ['Yu', 'Feng', ''], ['Sun', 'Maosong', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'reinforcement learning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuned', 'label': 'Fine-tuning'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'reinforcement learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.05403,"submitter":"Verena H\\\"aberle","authors":"Verena H\\\"aberle, Xiuqiang He, Linbin Huang, Florian D\\\"orfler, Steven\n  Low","title":"Quantitative Decentralized Stability Certificates for Grid-Forming\n  Converter Control","comments":"12 pages, 13 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We propose a decentralized framework for guaranteeing the small-signal\nstability of future power systems with grid-forming converters. Our approach\nleverages dynamic loop-shifting techniques to compensate for the lack of\npassivity in the network dynamics and establishes decentralized parametric\nstability certificates, depending on the local device-level controls and\nincorporating the effects of the network dynamics. By following practical\ntuning rules, we are able to ensure plug-and-play operation without centralized\ncoordination. Unlike prior works, our approach accommodates coupled frequency\nand voltage dynamics, incorporates network dynamics, and does not rely on\nspecific network configurations or operating points, offering a general and\nscalable solution for the integration of power-electronics-based devices into\nfuture power systems. We validate our theoretical stability results through\nnumerical case studies in a high-fidelity simulation model.\n","versions":"[{'version': 'v1', 'created': 'Fri, 7 Mar 2025 13:26:55 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 13:51:36 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['H\u00e4berle', 'Verena', ''], ['He', 'Xiuqiang', ''], ['Huang', 'Linbin', ''], ['D\u00f6rfler', 'Florian', ''], ['Low', 'Steven', '']]","extracted_entities":"[{'text': 'practical\\ntuning rules', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"practical\ntuning rules","similarity_score":0.691578269}
{"id":2503.08048,"submitter":"Sanghyuk Chun","authors":"Sanghyuk Chun and Sangdoo Yun","title":"LongProLIP: A Probabilistic Vision-Language Model with Long Context Text","comments":"Accepted as a tiny paper at the 1st workshop of \"Quantify Uncertainty\n  and Hallucination in Foundation Models: The Next Frontier in Reliable AI\" at\n  ICLR 2025; code: https:\/\/github.com\/naver-ai\/prolip; models:\n  https:\/\/huggingface.co\/collections\/SanghyukChun\/prolip-6712595dfc87fd8597350291","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recently, Probabilistic Language-Image Pre-Training (ProLIP) has been\nproposed to tackle the multiplicity issue of vision-language (VL) tasks.\nDespite their success in probabilistic representation learning at a scale, the\nProLIP models cannot handle long context texts longer than 64 context length,\nwhich limits their ability to capture rich contextual information from longer\ntext sequences. To address this issue, this paper proposes a fine-tuning\nstrategy for ProLIP to accept longer texts, e.g., 256 text tokens. Experimental\nresults on Urban-1k and the DataComp evaluation suite show that the proposed\nLongProLIP recipe can improve understanding of long contexts while minimizing\nthe negative effect of fine-tuning.We also observe a trade-off between the long\ncontext understanding (measured by Urban-1k) and general zero-shot capability\n(measured by evaluation datasets by DataComp). Code is available at\nhttps:\/\/github.com\/naver-ai\/prolip\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 05:04:43 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 06:05:04 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chun', 'Sanghyuk', ''], ['Yun', 'Sangdoo', '']]","extracted_entities":"[{'text': 'probabilistic representation learning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.09929,"submitter":"Weiwei Zhou","authors":"Weiwei Zhou, Chenkun Ling, Zefeng Cai","title":"Emotion Recognition with CLIP and Sequential Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Human emotion recognition plays a crucial role in facilitating seamless\ninteractions between humans and computers. In this paper, we present our\ninnovative methodology for tackling the Valence-Arousal (VA) Estimation\nChallenge, the Expression Recognition Challenge, and the Action Unit (AU)\nDetection Challenge, all within the framework of the 8th Workshop and\nCompetition on Affective Behavior Analysis in-the-wild (ABAW).\n  Our approach introduces a novel framework aimed at enhancing continuous\nemotion recognition. This is achieved by fine-tuning the CLIP model with the\naff-wild2 dataset, which provides annotated expression labels. The result is a\nfine-tuned model that serves as an efficient visual feature extractor,\nsignificantly improving its robustness. To further boost the performance of\ncontinuous emotion recognition, we incorporate Temporal Convolutional Network\n(TCN) modules alongside Transformer Encoder modules into our system\narchitecture. The integration of these advanced components allows our model to\noutperform baseline performance, demonstrating its ability to recognize human\nemotions with greater accuracy and efficiency.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 01:02:06 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhou', 'Weiwei', ''], ['Ling', 'Chenkun', ''], ['Cai', 'Zefeng', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Transformer Encoder modules', 'label': 'Transformers'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.09938,"submitter":"Dongliang Zhou","authors":"Sen Wang, Dongliang Zhou, Liang Xie, Chao Xu, Ye Yan, Erwei Yin","title":"PanoGen++: Domain-Adapted Text-Guided Panoramic Environment Generation\n  for Vision-and-Language Navigation","comments":"This paper was accepted by Neural Networks","journal-ref":null,"doi":"10.1016\/j.neunet.2025.107320","report-no":null,"categories":"cs.CV cs.MM cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Vision-and-language navigation (VLN) tasks require agents to navigate\nthree-dimensional environments guided by natural language instructions,\noffering substantial potential for diverse applications. However, the scarcity\nof training data impedes progress in this field. This paper introduces\nPanoGen++, a novel framework that addresses this limitation by generating\nvaried and pertinent panoramic environments for VLN tasks. PanoGen++\nincorporates pre-trained diffusion models with domain-specific fine-tuning,\nemploying parameter-efficient techniques such as low-rank adaptation to\nminimize computational costs. We investigate two settings for environment\ngeneration: masked image inpainting and recursive image outpainting. The former\nmaximizes novel environment creation by inpainting masked regions based on\ntextual descriptions, while the latter facilitates agents' learning of spatial\nrelationships within panoramas. Empirical evaluations on room-to-room (R2R),\nroom-for-room (R4R), and cooperative vision-and-dialog navigation (CVDN)\ndatasets reveal significant performance enhancements: a 2.44% increase in\nsuccess rate on the R2R test leaderboard, a 0.63% improvement on the R4R\nvalidation unseen set, and a 0.75-meter enhancement in goal progress on the\nCVDN validation unseen set. PanoGen++ augments the diversity and relevance of\ntraining environments, resulting in improved generalization and efficacy in VLN\ntasks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 01:16:58 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Sen', ''], ['Zhou', 'Dongliang', ''], ['Xie', 'Liang', ''], ['Xu', 'Chao', ''], ['Yan', 'Ye', ''], ['Yin', 'Erwei', '']]","extracted_entities":"[{'text': 'PanoGen++', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'PanoGen++', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'domain-specific fine-tuning', 'label': 'Fine-tuning'}, {'text': 'low-rank adaptation', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"domain-specific fine-tuning","similarity_score":0.6912822723}
{"id":2503.10017,"submitter":"Abhay Kumar Yadav","authors":"Jingxing Li, Yongjae Lee, Abhay Kumar Yadav, Cheng Peng, Rama\n  Chellappa, Deliang Fan","title":"Speedy MASt3R","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Image matching is a key component of modern 3D vision algorithms, essential\nfor accurate scene reconstruction and localization. MASt3R redefines image\nmatching as a 3D task by leveraging DUSt3R and introducing a fast reciprocal\nmatching scheme that accelerates matching by orders of magnitude while\npreserving theoretical guarantees. This approach has gained strong traction,\nwith DUSt3R and MASt3R collectively cited over 250 times in a short span,\nunderscoring their impact. However, despite its accuracy, MASt3R's inference\nspeed remains a bottleneck. On an A40 GPU, latency per image pair is 198.16 ms,\nmainly due to computational overhead from the ViT encoder-decoder and Fast\nReciprocal Nearest Neighbor (FastNN) matching.\n  To address this, we introduce Speedy MASt3R, a post-training optimization\nframework that enhances inference efficiency while maintaining accuracy. It\nintegrates multiple optimization techniques, including FlashMatch-an approach\nleveraging FlashAttention v2 with tiling strategies for improved efficiency,\ncomputation graph optimization via layer and tensor fusion having kernel\nauto-tuning with TensorRT (GraphFusion), and a streamlined FastNN pipeline that\nreduces memory access time from quadratic to linear while accelerating\nblock-wise correlation scoring through vectorized computation (FastNN-Lite).\nAdditionally, it employs mixed-precision inference with FP16\/FP32 hybrid\ncomputations (HybridCast), achieving speedup while preserving numerical\nprecision. Evaluated on Aachen Day-Night, InLoc, 7-Scenes, ScanNet1500, and\nMegaDepth1500, Speedy MASt3R achieves a 54% reduction in inference time (198 ms\nto 91 ms per image pair) without sacrificing accuracy. This advancement enables\nreal-time 3D understanding, benefiting applications like mixed reality\nnavigation and large-scale 3D scene reconstruction.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 03:56:22 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Jingxing', ''], ['Lee', 'Yongjae', ''], ['Yadav', 'Abhay Kumar', ''], ['Peng', 'Cheng', ''], ['Chellappa', 'Rama', ''], ['Fan', 'Deliang', '']]","extracted_entities":"[{'text': 'kernel\\nauto-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"kernel\nauto-tuning","similarity_score":0.5311024189}
{"id":2503.10086,"submitter":"Jiajun Deng","authors":"Jiajun Deng, Yaolong Ju, Jing Yang, Simon Lui, Xunying Liu","title":"Efficient Adapter Tuning for Joint Singing Voice Beat and Downbeat\n  Tracking with Self-supervised Learning Features","comments":"Accepted by ISMIR2024","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.MM eess.AS","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Singing voice beat tracking is a challenging task, due to the lack of musical\naccompaniment that often contains robust rhythmic and harmonic patterns,\nsomething most existing beat tracking systems utilize and can be essential for\nestimating beats. In this paper, a novel temporal convolutional network-based\nbeat-tracking approach featuring self-supervised learning (SSL) representations\nand adapter tuning is proposed to track the beat and downbeat of singing voices\njointly. The SSL DistilHuBERT representations are utilized to capture the\nsemantic information of singing voices and are further fused with the generic\nspectral features to facilitate beat estimation. Sources of variabilities that\nare particularly prominent with the non-homogeneous singing voice data are\nreduced by the efficient adapter tuning. Extensive experiments show that\nfeature fusion and adapter tuning improve the performance individually, and the\ncombination of both leads to significantly better performances than the\nun-adapted baseline system, with up to 31.6% and 42.4% absolute F1-score\nimprovements on beat and downbeat tracking, respectively.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 06:28:15 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Deng', 'Jiajun', ''], ['Ju', 'Yaolong', ''], ['Yang', 'Jing', ''], ['Lui', 'Simon', ''], ['Liu', 'Xunying', '']]","extracted_entities":"[{'text': 'adapter tuning', 'label': 'Fine-tuning'}, {'text': 'adapter tuning', 'label': 'Fine-tuning'}, {'text': 'adapter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"adapter tuning","similarity_score":0.5330156684}
{"id":2503.10102,"submitter":"Junhao Wang","authors":"Junhao Wang","title":"Geometric Parameter Estimations of Perovskite Solar Cells Based on\n  Optical Simulations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper presents a non-invasive approach to estimate the layer thicknesses\nof perovskite solar cells. The thicknesses are predicted by a convolutional\nneural network that leverages the external quantum efficiency of a perovskite\nsolar cell. The network is trained in thickness ranges where the optical\nproperties are constant, and these ranges set the constraints for the network's\napplication. Due to light sensitivity issues with opaque perovskites, the\nconvolutional neural network showed better performance with transparent\nperovskites. To optimize the performance and reduce the root mean square error,\nwe tried different sampling methods, image specifications, and Bayesian\noptimization for hyperparameter tuning. While sampling methods showed marginal\nimprovement, implementing Bayesian optimization demonstrated high accuracy.\nOther minor optimization attempts include experimenting with input\nspecifications and pre-processing approaches. The results confirm the\nfeasibility, efficiency, and effectiveness of a convolution neural network for\npredicting perovskite solar cells' layer thicknesses based on controlled\nexperiments.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 06:54:12 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Junhao', '']]","extracted_entities":"[{'text': 'convolutional\\nneural network', 'label': 'AI model'}, {'text': 'external quantum efficiency', 'label': 'quantisation'}, {'text': 'Bayesian\\noptimization', 'label': 'Fine-tuning'}, {'text': 'hyperparameter tuning', 'label': 'Fine-tuning'}, {'text': 'convolution neural network', 'label': 'AI model'}]","assigned_concept":"Fine-tuning","matched_keyword":"hyperparameter tuning","similarity_score":0.6193697453}
{"id":2503.10129,"submitter":"Namal Jayasuriya","authors":"Namal Jayasuriya, Yi Guo, Wen Hu, Oula Ghannoum","title":"Deep Learning-Based Direct Leaf Area Estimation using Two RGBD Datasets\n  for Model Development","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Estimation of a single leaf area can be a measure of crop growth and a\nphenotypic trait to breed new varieties. It has also been used to measure leaf\narea index and total leaf area. Some studies have used hand-held cameras, image\nprocessing 3D reconstruction and unsupervised learning-based methods to\nestimate the leaf area in plant images. Deep learning works well for object\ndetection and segmentation tasks; however, direct area estimation of objects\nhas not been explored. This work investigates deep learning-based leaf area\nestimation, for RGBD images taken using a mobile camera setup in real-world\nscenarios. A dataset for attached leaves captured with a top angle view and a\ndataset for detached single leaves were collected for model development and\ntesting. First, image processing-based area estimation was tested on manually\nsegmented leaves. Then a Mask R-CNN-based model was investigated, and modified\nto accept RGBD images and to estimate the leaf area. The detached-leaf data set\nwas then mixed with the attached-leaf plant data set to estimate the single\nleaf area for plant images, and another network design with two backbones was\nproposed: one for segmentation and the other for area estimation. Instead of\ntrying all possibilities or random values, an agile approach was used in\nhyperparameter tuning. The final model was cross-validated with 5-folds and\ntested with two unseen datasets: detached and attached leaves. The F1 score\nwith 90% IoA for segmentation result on unseen detached-leaf data was 1.0,\nwhile R-squared of area estimation was 0.81. For unseen plant data\nsegmentation, the F1 score with 90% IoA was 0.59, while the R-squared score was\n0.57. The research suggests using attached leaves with ground truth area to\nimprove the results.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 07:39:09 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Jayasuriya', 'Namal', ''], ['Guo', 'Yi', ''], ['Hu', 'Wen', ''], ['Ghannoum', 'Oula', '']]","extracted_entities":"[{'text': 'hyperparameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"hyperparameter tuning","similarity_score":0.6193697453}
{"id":2503.10157,"submitter":"Liang Zheng","authors":"Aogui Zhang, Xinye Peng, Liang Zheng","title":"Exploring the multiplicity dependence of the flavor hierarchy for hadron\n  productions in high energy pp collisions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph nucl-th","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this work, we perform a systematic study on the multiplicity dependence of\nhadron productions at mid-rapidity ($|y|<0.5$), ranging from the light to the\ncharm sector in pp collisions at $\\sqrt{s}=13$ TeV. This study utilizes a\nmulti-phase transport model (AMPT) coupled with PYTHIA8 initial conditions. We\nhave investigated the baryon to meson ratios as well as the strange to\nnon-strange meson ratios varying with the charged particle density. By tuning\nthe coalescence parameters, the AMPT model provides a reasonable description to\nthe experimental data for inclusive productions of both light and charm\nhadrons, comparable to the string fragmentation model calculations with color\nreconnection effects. Additionally, we have analyzed the relative production of\nhadrons by examining self-normalized particle ratios as a function of the\ncharged hadron density. Our findings suggest that parton evolution effects and\nthe coalescence hadronization process in AMPT model lead to a strong flavor\nhierarchy in the multiplicity dependence of the baryon to meson ratio.\nFurthermore, our investigation on the $p_T$ differential double ratio of baryon\nto meson fraction between high and low multiplicity events indicates distinct\nmodifications to the flavor associated baryon to meson ratio $p_T$ shape in\nhigh multiplicity events when comparing the coalescence hadronization model to\nthe color reconnection model. These observations highlight the importance of\nunderstanding the hadronization process in high-energy proton-proton collisions\nthrough a comprehensive multiplicity dependent multi-flavor analysis.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 08:35:12 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhang', 'Aogui', ''], ['Peng', 'Xinye', ''], ['Zheng', 'Liang', '']]","extracted_entities":"[{'text': 'tuning', 'label': 'Fine-tuning'}, {'text': 'AMPT', 'label': 'AI model'}, {'text': 'AMPT', 'label': 'AI model'}]","assigned_concept":"Fine-tuning","matched_keyword":"tuning","similarity_score":0.8449009061}
{"id":2503.10217,"submitter":"Shilong Wang","authors":"Shilong Wang, Jianchun Liu, Hongli Xu, Jiaming Yan, Xianjun Gao","title":"Efficient Federated Fine-Tuning of Large Language Models with Layer\n  Dropout","comments":"13 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.DC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Fine-tuning plays a crucial role in enabling pre-trained LLMs to evolve from\ngeneral language comprehension to task-specific expertise. To preserve user\ndata privacy, federated fine-tuning is often employed and has emerged as the de\nfacto paradigm. However, federated fine-tuning is prohibitively inefficient due\nto the tension between LLM complexity and the resource constraint of end\ndevices, incurring unaffordable fine-tuning overhead. Existing literature\nprimarily utilizes parameter-efficient fine-tuning techniques to mitigate\ncommunication costs, yet computational and memory burdens continue to pose\nsignificant challenges for developers. This work proposes DropPEFT, an\ninnovative federated PEFT framework that employs a novel stochastic transformer\nlayer dropout method, enabling devices to deactivate a considerable fraction of\nLLMs layers during training, thereby eliminating the associated computational\nload and memory footprint. In DropPEFT, a key challenge is the proper\nconfiguration of dropout ratios for layers, as overhead and training\nperformance are highly sensitive to this setting. To address this challenge, we\nadaptively assign optimal dropout-ratio configurations to devices through an\nexploration-exploitation strategy, achieving efficient and effective\nfine-tuning. Extensive experiments show that DropPEFT can achieve a\n1.3-6.3\\times speedup in model convergence and a 40%-67% reduction in memory\nfootprint compared to state-of-the-art methods.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 09:59:16 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Shilong', ''], ['Liu', 'Jianchun', ''], ['Xu', 'Hongli', ''], ['Yan', 'Jiaming', ''], ['Gao', 'Xianjun', '']]","extracted_entities":"[{'text': 'Fine-tuning', 'label': 'Fine-tuning'}, {'text': 'federated fine-tuning', 'label': 'Fine-tuning'}, {'text': 'federated fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"Fine-tuning","similarity_score":1.0000001192}
{"id":2503.10282,"submitter":"Samih Karroum","authors":"Samih Karroum, Saad Mazhar","title":"HyperArm Bandit Optimization: A Novel approach to Hyperparameter\n  Optimization and an Analysis of Bandit Algorithms in Stochastic and\n  Adversarial Settings","comments":"41 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  This paper explores the application of bandit algorithms in both stochastic\nand adversarial settings, with a focus on theoretical analysis and practical\napplications. The study begins by introducing bandit problems, distinguishing\nbetween stochastic and adversarial variants, and examining key algorithms such\nas Explore-Then-Commit (ETC), Upper Confidence Bound (UCB), and\nExponential-Weight Algorithm for Exploration and Exploitation (EXP3).\nTheoretical regret bounds are analyzed to compare the performance of these\nalgorithms. The paper then introduces a novel framework, HyperArm Bandit\nOptimization (HABO), which applies EXP3 to hyperparameter tuning in machine\nlearning models. Unlike traditional methods that treat entire configurations as\narms, HABO treats individual hyperparameters as super-arms, and its potential\nconfigurations as sub-arms, enabling dynamic resource allocation and efficient\nexploration. Experimental results demonstrate HABO's effectiveness in\nclassification and regression tasks, outperforming Bayesian Optimization in\nterms of computational efficiency and accuracy. The paper concludes with\ninsights into the convergence guarantees of HABO and its potential for scalable\nand robust hyperparameter optimization.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 11:50:28 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Karroum', 'Samih', ''], ['Mazhar', 'Saad', '']]","extracted_entities":"[{'text': 'hyperparameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"hyperparameter tuning","similarity_score":0.6193697453}
{"id":2503.10286,"submitter":"Zhiqi Li","authors":"Zhiqi Li, Chengrui Dong, Yiming Chen, Zhangchi Huang, Peidong Liu","title":"VicaSplat: A Single Run is All You Need for 3D Gaussian Splatting and\n  Camera Estimation from Unposed Video Frames","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present VicaSplat, a novel framework for joint 3D Gaussians reconstruction\nand camera pose estimation from a sequence of unposed video frames, which is a\ncritical yet underexplored task in real-world 3D applications. The core of our\nmethod lies in a novel transformer-based network architecture. In particular,\nour model starts with an image encoder that maps each image to a list of visual\ntokens. All visual tokens are concatenated with additional inserted learnable\ncamera tokens. The obtained tokens then fully communicate with each other\nwithin a tailored transformer decoder. The camera tokens causally aggregate\nfeatures from visual tokens of different views, and further modulate them\nframe-wisely to inject view-dependent features. 3D Gaussian splats and camera\npose parameters can then be estimated via different prediction heads.\nExperiments show that VicaSplat surpasses baseline methods for multi-view\ninputs, and achieves comparable performance to prior two-view approaches.\nRemarkably, VicaSplat also demonstrates exceptional cross-dataset\ngeneralization capability on the ScanNet benchmark, achieving superior\nperformance without any fine-tuning. Project page:\nhttps:\/\/lizhiqi49.github.io\/VicaSplat.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 11:56:05 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Zhiqi', ''], ['Dong', 'Chengrui', ''], ['Chen', 'Yiming', ''], ['Huang', 'Zhangchi', ''], ['Liu', 'Peidong', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.10322,"submitter":"Haoxuan Li","authors":"Haoxuan Li, Sixu Yan, Yuhan Li, Xinggang Wang","title":"Towards Fast, Memory-based and Data-Efficient Vision-Language Policy","comments":"11 pages, 7 figures, 6 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Vision Language Models (VLMs) pretrained on Internet-scale vision-language\ndata have demonstrated the potential to transfer their knowledge to robotic\nlearning. However, the existing paradigm encounters three critical challenges:\n(1) expensive inference cost resulting from large-scale model parameters, (2)\nfrequent domain shifts caused by mismatched data modalities, and (3) limited\ncapacity to handle past or future experiences. In this work, we propose\nLiteVLP, a lightweight, memory-based, and general-purpose vision-language\npolicy generation model. LiteVLP is built upon a pre-trained 1B-parameter VLM\nand fine-tuned on a tiny-scale and conversation-style robotic dataset. Through\nextensive experiments, we demonstrate that LiteVLP outperforms state-of-the-art\nvision-language policy on VIMA-Bench, with minimal training time. Furthermore,\nLiteVLP exhibits superior inference speed while maintaining exceptional high\naccuracy. In long-horizon manipulation tasks, LiteVLP also shows remarkable\nmemory ability, outperforming the best-performing baseline model by 18.8%.\nThese results highlight LiteVLP as a promising model to integrating the\nintelligence of VLMs into robotic learning.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 12:58:40 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Haoxuan', ''], ['Yan', 'Sixu', ''], ['Li', 'Yuhan', ''], ['Wang', 'Xinggang', '']]","extracted_entities":"[{'text': 'Vision Language Models', 'label': 'Large Language Model'}, {'text': 'robotic\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuned', 'label': 'Fine-tuning'}, {'text': 'robotic learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuned","similarity_score":0.8707774282}
{"id":2503.10337,"submitter":"Vivek Chari","authors":"Vivek Chari, Guanghui Qin, Benjamin Van Durme","title":"KV-Distill: Nearly Lossless Learnable Context Compression for LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 13:15:28 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chari', 'Vivek', ''], ['Qin', 'Guanghui', ''], ['Van Durme', 'Benjamin', '']]","extracted_entities":"[{'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'standard Transformers', 'label': 'Transformers'}, {'text': 'KV-Distill', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'fine-tuned', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuned","similarity_score":0.8707774282}
{"id":2503.10342,"submitter":"Qi Zhao","authors":"Qi Zhao and Zhan Ma and Pan Zhou","title":"DreamInsert: Zero-Shot Image-to-Video Object Insertion from A Single\n  Image","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent developments in generative diffusion models have turned many dreams\ninto realities. For video object insertion, existing methods typically require\nadditional information, such as a reference video or a 3D asset of the object,\nto generate the synthetic motion. However, inserting an object from a single\nreference photo into a target background video remains an uncharted area due to\nthe lack of unseen motion information. We propose DreamInsert, which achieves\nImage-to-Video Object Insertion in a training-free manner for the first time.\nBy incorporating the trajectory of the object into consideration, DreamInsert\ncan predict the unseen object movement, fuse it harmoniously with the\nbackground video, and generate the desired video seamlessly. More\nsignificantly, DreamInsert is both simple and effective, achieving zero-shot\ninsertion without end-to-end training or additional fine-tuning on\nwell-designed image-video data pairs. We demonstrated the effectiveness of\nDreamInsert through a variety of experiments. Leveraging this capability, we\npresent the first results for Image-to-Video object insertion in a\ntraining-free manner, paving exciting new directions for future content\ncreation and synthesis. The code will be released soon.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 13:20:54 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhao', 'Qi', ''], ['Ma', 'Zhan', ''], ['Zhou', 'Pan', '']]","extracted_entities":"[{'text': 'DreamInsert', 'label': 'Embedding'}, {'text': 'Image-to-Video Object Insertion', 'label': 'Embedding'}, {'text': 'DreamInsert', 'label': 'Embedding'}, {'text': 'DreamInsert', 'label': 'Embedding'}, {'text': 'zero-shot\\ninsertion', 'label': 'Zero-shot Learning'}, {'text': 'additional fine-tuning', 'label': 'Fine-tuning'}, {'text': 'DreamInsert', 'label': 'Embedding'}]","assigned_concept":"Fine-tuning","matched_keyword":"additional fine-tuning","similarity_score":0.9366964698}
{"id":2503.10408,"submitter":"Jonathan Shaki","authors":"Jonathan Shaki, Emanuele La Malfa, Michael Wooldridge, Sarit Kraus","title":"Understanding the Logical Capabilities of Large Language Models via\n  Out-of-Context Representation Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We study the capabilities of Large Language Models (LLM) on binary relations,\na ubiquitous concept in math employed in most reasoning, math and logic\nbenchmarks. This work focuses on equality, inequality, and inclusion, along\nwith the properties they satisfy, such as ir\/reflexivity, a\/symmetry,\ntransitivity, and logical complexity (e.g., number of reasoning ``hops''). We\npropose an alternative to in-context learning that trains only the\nrepresentations of newly introduced tokens, namely out-of-context\nrepresentation learning. This method mitigates linguistic biases already\npresent in a model and, differently from in-context learning, does not rely on\nexternal information or illustrations. We argue out-of-context representation\nlearning as a better alternative to in-context learning and fine-tuning to\nevaluate the capabilities of LLMs on logic tasks that are the building blocks\nof more complex reasoning benchmarks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 14:32:30 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Shaki', 'Jonathan', ''], ['La Malfa', 'Emanuele', ''], ['Wooldridge', 'Michael', ''], ['Kraus', 'Sarit', '']]","extracted_entities":"[{'text': 'in-context learning', 'label': 'Few-shot Learning'}, {'text': 'out-of-context\\nrepresentation learning', 'label': 'Few-shot Learning'}, {'text': 'in-context learning', 'label': 'Few-shot Learning'}, {'text': 'out-of-context representation\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'in-context learning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.10468,"submitter":"Yifeng Yang","authors":"Yifeng Yang, Lin Zhu, Zewen Sun, Hengyu Liu, Qinying Gu, Nanyang Ye","title":"OODD: Test-time Out-of-Distribution Detection with Dynamic Dictionary","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Out-of-distribution (OOD) detection remains challenging for deep learning\nmodels, particularly when test-time OOD samples differ significantly from\ntraining outliers. We propose OODD, a novel test-time OOD detection method that\ndynamically maintains and updates an OOD dictionary without fine-tuning. Our\napproach leverages a priority queue-based dictionary that accumulates\nrepresentative OOD features during testing, combined with an informative inlier\nsampling strategy for in-distribution (ID) samples. To ensure stable\nperformance during early testing, we propose a dual OOD stabilization mechanism\nthat leverages strategically generated outliers derived from ID data. To our\nbest knowledge, extensive experiments on the OpenOOD benchmark demonstrate that\nOODD significantly outperforms existing methods, achieving a 26.0% improvement\nin FPR95 on CIFAR-100 Far OOD detection compared to the state-of-the-art\napproach. Furthermore, we present an optimized variant of the KNN-based OOD\ndetection framework that achieves a 3x speedup while maintaining detection\nperformance.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 15:41:56 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Yang', 'Yifeng', ''], ['Zhu', 'Lin', ''], ['Sun', 'Zewen', ''], ['Liu', 'Hengyu', ''], ['Gu', 'Qinying', ''], ['Ye', 'Nanyang', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'priority queue-based dictionary', 'label': 'Embedding'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.10508,"submitter":"Daou Zhang","authors":"Yuhan Wang, Cheng Liu, Daou Zhang and Weichao Wu","title":"Hoi2Anomaly: An Explainable Anomaly Detection Approach Guided by\n  Human-Object Interaction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In the domain of Image Anomaly Detection (IAD), Existing methods frequently\nexhibit a paucity of fine-grained, interpretable semantic information,\nresulting in the detection of anomalous entities or activities that are\nsusceptible to machine illusions. This deficiency often leads to the detection\nof anomalous entities or actions that are susceptible to machine illusions and\nlack sufficient explanation. In this thesis, we propose a novel approach to\nanomaly detection, termed Hoi2Anomaly, which aims to achieve precise\ndiscrimination and localization of anomalies. The proposed methodology involves\nthe construction of a multi-modal instruction tuning dataset comprising\nhuman-object interaction (HOI) pairs in anomalous scenarios. Second, we have\ntrained an HOI extractor in threat scenarios to localize and match anomalous\nactions and entities. Finally, explanatory content is generated for the\ndetected anomalous HOI by fine-tuning the visual language pretraining (VLP)\nframework. The experimental results demonstrate that Hoi2Anomaly surpasses\nexisting generative approaches in terms of precision and explainability. We\nwill release Hoi2Anomaly for the advancement of the field of anomaly detection.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 16:09:51 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Yuhan', ''], ['Liu', 'Cheng', ''], ['Zhang', 'Daou', ''], ['Wu', 'Weichao', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.10615,"submitter":"Yang Yi","authors":"Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao\n  Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, Wei Chen","title":"R1-Onevision: Advancing Generalized Multimodal Reasoning through\n  Cross-Modal Formalization","comments":"Code and Model: https:\/\/github.com\/Fancy-MLLM\/R1-onevision","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models have demonstrated remarkable reasoning capability in\ncomplex textual tasks. However, multimodal reasoning, which requires\nintegrating visual and textual information, remains a significant challenge.\nExisting visual-language models often struggle to effectively analyze and\nreason visual content, resulting in suboptimal performance on complex reasoning\ntasks. Moreover, the absence of comprehensive benchmarks hinders the accurate\nassessment of multimodal reasoning capabilities. In this paper, we introduce\nR1-Onevision, a multimodal reasoning model designed to bridge the gap between\nvisual perception and deep reasoning. To achieve this, we propose a cross-modal\nreasoning pipeline that transforms images into formal textural representations,\nenabling precise language-based reasoning. Leveraging this pipeline, we\nconstruct the R1-Onevision dataset which provides detailed, step-by-step\nmultimodal reasoning annotations across diverse domains. We further develop the\nR1-Onevision model through supervised fine-tuning and reinforcement learning to\ncultivate advanced reasoning and robust generalization abilities. To\ncomprehensively evaluate multimodal reasoning performance across different\ngrades, we introduce R1-Onevision-Bench, a benchmark aligned with human\neducational stages, covering exams from junior high school to university and\nbeyond. Experimental results show that R1-Onevision achieves state-of-the-art\nperformance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple\nchallenging multimodal reasoning benchmarks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:56:05 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Yang', 'Yi', ''], ['He', 'Xiaoxuan', ''], ['Pan', 'Hongkun', ''], ['Jiang', 'Xiyan', ''], ['Deng', 'Yan', ''], ['Yang', 'Xingtao', ''], ['Lu', 'Haoyu', ''], ['Yin', 'Dacheng', ''], ['Rao', 'Fengyun', ''], ['Zhu', 'Minfeng', ''], ['Zhang', 'Bo', ''], ['Chen', 'Wei', '']]","extracted_entities":"[{'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'reinforcement learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"supervised fine-tuning","similarity_score":0.7449287176}
{"id":2503.10618,"submitter":"Chen Chen","authors":"Chen Chen, Rui Qian, Wenze Hu, Tsu-Jui Fu, Lezhi Li, Bowen Zhang, Alex\n  Schwing, Wei Liu, Yinfei Yang","title":"DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture\n  Design in Text to Image Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this work, we empirically study Diffusion Transformers (DiTs) for\ntext-to-image generation, focusing on architectural choices, text-conditioning\nstrategies, and training protocols. We evaluate a range of DiT-based\narchitectures--including PixArt-style and MMDiT variants--and compare them with\na standard DiT variant which directly processes concatenated text and noise\ninputs. Surprisingly, our findings reveal that the performance of standard DiT\nis comparable with those specialized models, while demonstrating superior\nparameter-efficiency, especially when scaled up. Leveraging the layer-wise\nparameter sharing strategy, we achieve a further reduction of 66% in model size\ncompared to an MMDiT architecture, with minimal performance impact. Building on\nan in-depth analysis of critical components such as text encoders and\nVariational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With\nsupervised and reward fine-tuning, DiT-Air achieves state-of-the-art\nperformance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly\ncompetitive, surpassing most existing models despite its compact size.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:57:25 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chen', 'Chen', ''], ['Qian', 'Rui', ''], ['Hu', 'Wenze', ''], ['Fu', 'Tsu-Jui', ''], ['Li', 'Lezhi', ''], ['Zhang', 'Bowen', ''], ['Schwing', 'Alex', ''], ['Liu', 'Wei', ''], ['Yang', 'Yinfei', '']]","extracted_entities":"[{'text': 'Diffusion Transformers', 'label': 'Transformers'}, {'text': 'PixArt-style', 'label': 'Transformers'}, {'text': 'MMDiT variants', 'label': 'Transformers'}, {'text': 'supervised and reward fine-tuning', 'label': 'Fine-tuning'}, {'text': 'DiT-Air', 'label': 'Transformer-based model'}]","assigned_concept":"Fine-tuning","matched_keyword":"supervised and reward fine-tuning","similarity_score":0.5981090069}
{"id":2503.10621,"submitter":"Ayesha Ishaq Ms","authors":"Ayesha Ishaq, Jean Lahoud, Ketan More, Omkar Thawakar, Ritesh Thawkar,\n  Dinura Dissanayake, Noor Ahsan, Yuhao Li, Fahad Shahbaz Khan, Hisham\n  Cholakkal, Ivan Laptev, Rao Muhammad Anwer, Salman Khan","title":"DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model\n  for Driving Scenario Understanding","comments":"8 pages, 4 figures, 3 tables, github:\n  https:\/\/github.com\/ayesha-ishaq\/DriveLMM-o1","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While large multimodal models (LMMs) have demonstrated strong performance\nacross various Visual Question Answering (VQA) tasks, certain challenges\nrequire complex multi-step reasoning to reach accurate answers. One\nparticularly challenging task is autonomous driving, which demands thorough\ncognitive processing before decisions can be made. In this domain, a sequential\nand interpretive understanding of visual cues is essential for effective\nperception, prediction, and planning. Nevertheless, common VQA benchmarks often\nfocus on the accuracy of the final answer while overlooking the reasoning\nprocess that enables the generation of accurate responses. Moreover, existing\nmethods lack a comprehensive framework for evaluating step-by-step reasoning in\nrealistic driving scenarios. To address this gap, we propose DriveLMM-o1, a new\ndataset and benchmark specifically designed to advance step-wise visual\nreasoning for autonomous driving. Our benchmark features over 18k VQA examples\nin the training set and more than 4k in the test set, covering diverse\nquestions on perception, prediction, and planning, each enriched with\nstep-by-step reasoning to ensure logical inference in autonomous driving\nscenarios. We further introduce a large multimodal model that is fine-tuned on\nour reasoning dataset, demonstrating robust performance in complex driving\nscenarios. In addition, we benchmark various open-source and closed-source\nmethods on our proposed dataset, systematically comparing their reasoning\ncapabilities for autonomous driving tasks. Our model achieves a +7.49% gain in\nfinal answer accuracy, along with a 3.62% improvement in reasoning score over\nthe previous best open-source model. Our framework, dataset, and model are\navailable at https:\/\/github.com\/ayesha-ishaq\/DriveLMM-o1.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:59:01 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Ishaq', 'Ayesha', ''], ['Lahoud', 'Jean', ''], ['More', 'Ketan', ''], ['Thawakar', 'Omkar', ''], ['Thawkar', 'Ritesh', ''], ['Dissanayake', 'Dinura', ''], ['Ahsan', 'Noor', ''], ['Li', 'Yuhao', ''], ['Khan', 'Fahad Shahbaz', ''], ['Cholakkal', 'Hisham', ''], ['Laptev', 'Ivan', ''], ['Anwer', 'Rao Muhammad', ''], ['Khan', 'Salman', '']]","extracted_entities":"[{'text': 'fine-tuned', 'label': 'Fine-tuning'}, {'text': 'DriveLMM-o1', 'label': 'Open-source LLMs'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuned","similarity_score":0.8707774282}
{"id":2411.13163,"submitter":"Nabeel Seedat","authors":"Nabeel Seedat, Caterina Tozzi, Andrea Hita Ardiaca, Mihaela van der\n  Schaar, James Weatherall, Adam Taylor","title":"Unlocking Historical Clinical Trial Data with ALIGN: A Compositional\n  Large Language Model System for Medical Coding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The reuse of historical clinical trial data has significant potential to\naccelerate medical research and drug development. However, interoperability\nchallenges, particularly with missing medical codes, hinders effective data\nintegration across studies. While Large Language Models (LLMs) offer a\npromising solution for automated coding without labeled data, current\napproaches face challenges on complex coding tasks. We introduce ALIGN, a novel\ncompositional LLM-based system for automated, zero-shot medical coding. ALIGN\nfollows a three-step process: (1) diverse candidate code generation; (2)\nself-evaluation of codes and (3) confidence scoring and uncertainty estimation\nenabling human deferral to ensure reliability. We evaluate ALIGN on harmonizing\nmedication terms into Anatomical Therapeutic Chemical (ATC) and medical history\nterms into Medical Dictionary for Regulatory Activities (MedDRA) codes\nextracted from 22 immunology trials. ALIGN outperformed the LLM baselines,\nwhile also providing capabilities for trustworthy deployment. For MedDRA\ncoding, ALIGN achieved high accuracy across all levels, matching RAG and\nexcelling at the most specific levels (87-90% for HLGT). For ATC coding, ALIGN\ndemonstrated superior performance, particularly at lower hierarchy levels (ATC\nLevel 4), with 72-73% overall accuracy and 86-89% accuracy for common\nmedications, outperforming baselines by 7-22%. ALIGN's uncertainty-based\ndeferral improved accuracy by 17% to 90% accuracy with 30% deferral, notably\nenhancing performance on uncommon medications. ALIGN achieves this\ncost-efficiently at \\$0.0007 and \\$0.02 per code for GPT-4o-mini and GPT-4o,\nreducing barriers to clinical adoption. ALIGN advances automated medical coding\nfor clinical trial data, contributing to enhanced data interoperability and\nreusability, positioning it as a promising tool to improve clinical research\nand accelerate drug development.\n","versions":"[{'version': 'v1', 'created': 'Wed, 20 Nov 2024 09:59:12 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 13:39:09 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Seedat', 'Nabeel', ''], ['Tozzi', 'Caterina', ''], ['Ardiaca', 'Andrea Hita', ''], ['van der Schaar', 'Mihaela', ''], ['Weatherall', 'James', ''], ['Taylor', 'Adam', '']]","extracted_entities":"[{'text': 'RAG', 'label': 'RAG'}, {'text': 'HLGT', 'label': 'GPT'}, {'text': 'GPT-4o-mini', 'label': 'GPT'}, {'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2502.14614,"submitter":"Mingyi Jia","authors":"Mingyi Jia and Junwen Duan and Yan Song and Jianxin Wang","title":"FIND: Fine-grained Information Density Guided Adaptive\n  Retrieval-Augmented Generation for Disease Diagnosis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Retrieval-Augmented Large Language Models (LLMs), which integrate external\nknowledge into LLMs, have shown remarkable performance in various medical\ndomains, including clinical diagnosis. However, existing RAG methods struggle\nto effectively assess task difficulty to make retrieval decisions, thereby\nfailing to meet the clinical requirements for balancing efficiency and\naccuracy. So in this paper, we propose FIND (\\textbf{F}ine-grained\n\\textbf{In}formation \\textbf{D}ensity Guided Adaptive RAG), a novel framework\nthat improves the reliability of RAG in disease diagnosis scenarios. FIND\nincorporates a fine-grained adaptive control module to determine whether\nretrieval is necessary based on the information density of the input. By\noptimizing the retrieval process and implementing a knowledge filtering module,\nFIND ensures that the retrieval is better suited to clinical scenarios.\nExperiments on three Chinese electronic medical record datasets demonstrate\nthat FIND significantly outperforms various baseline methods, highlighting its\neffectiveness in clinical diagnosis tasks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Feb 2025 14:52:36 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 13:13:07 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Jia', 'Mingyi', ''], ['Duan', 'Junwen', ''], ['Song', 'Yan', ''], ['Wang', 'Jianxin', '']]","extracted_entities":"[{'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.10265,"submitter":"Chang Han Low","authors":"Chang Han Low, Ziyue Wang, Tianyi Zhang, Zhitao Zeng, Zhu Zhuo,\n  Evangelos B. Mazomenos, Yueming Jin","title":"SurgRAW: Multi-Agent Workflow with Chain-of-Thought Reasoning for\n  Surgical Intelligence","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Integration of Vision-Language Models (VLMs) in surgical intelligence is\nhindered by hallucinations, domain knowledge gaps, and limited understanding of\ntask interdependencies within surgical scenes, undermining clinical\nreliability. While recent VLMs demonstrate strong general reasoning and\nthinking capabilities, they still lack the domain expertise and task-awareness\nrequired for precise surgical scene interpretation. Although Chain-of-Thought\n(CoT) can structure reasoning more effectively, current approaches rely on\nself-generated CoT steps, which often exacerbate inherent domain gaps and\nhallucinations. To overcome this, we present SurgRAW, a CoT-driven multi-agent\nframework that delivers transparent, interpretable insights for most tasks in\nrobotic-assisted surgery. By employing specialized CoT prompts across five\ntasks: instrument recognition, action recognition, action prediction, patient\ndata extraction, and outcome assessment, SurgRAW mitigates hallucinations\nthrough structured, domain-aware reasoning. Retrieval-Augmented Generation\n(RAG) is also integrated to external medical knowledge to bridge domain gaps\nand improve response reliability. Most importantly, a hierarchical agentic\nsystem ensures that CoT-embedded VLM agents collaborate effectively while\nunderstanding task interdependencies, with a panel discussion mechanism\npromotes logical consistency. To evaluate our method, we introduce\nSurgCoTBench, the first reasoning-based dataset with structured frame-level\nannotations. With comprehensive experiments, we demonstrate the effectiveness\nof proposed SurgRAW with 29.32% accuracy improvement over baseline VLMs on 12\nrobotic procedures, achieving the state-of-the-art performance and advancing\nexplainable, trustworthy, and autonomous surgical assistance.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 11:23:13 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Low', 'Chang Han', ''], ['Wang', 'Ziyue', ''], ['Zhang', 'Tianyi', ''], ['Zeng', 'Zhitao', ''], ['Zhuo', 'Zhu', ''], ['Mazomenos', 'Evangelos B.', ''], ['Jin', 'Yueming', '']]","extracted_entities":"[{'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'SurgRAW', 'label': 'RAG'}, {'text': 'specialized CoT prompts', 'label': 'Prompting'}, {'text': 'SurgRAW', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'SurgRAW', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2501.13859,"submitter":"Shiyu Zhang","authors":"Shiyu Zhang, Cheng Yan, Yang Liu, Chenchen Jing, Lei Zhou, Wenjun Wang","title":"Learning Visual Proxy for Compositional Zero-Shot Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Compositional Zero-Shot Learning (CZSL) aims to recognize novel\nattribute-object compositions by leveraging knowledge from seen compositions.\nExisting methods align textual prototypes with visual features through\nVision-Language Models (VLMs), but they face two key limitations: (1) modality\ngaps hinder the discrimination of semantically similar composition pairs, and\n(2) single-modal textual prototypes lack fine-grained visual cues, creating\nbottlenecks in VLM-based CZSL. In this paper, we introduce Visual Proxy\nLearning, a novel approach that facilitates the learning of distinct visual\ndistributions, effectively reducing the modality gap and improving\ncompositional generalization performance. Specifically, we initialize visual\nproxies for various attributes, objects, and their compositions using text\nrepresentations. By optimizing the visual space, we capture fine-grained visual\ncues and guide the learning of more discriminative visual representations for\nattributes, objects and compositions. Furthermore, we propose an effective\nCross-Modal Joint Learning (CMJL) strategy that imposes cross-modal constraints\nbetween the original text-image space and the fine-grained visual space. This\napproach not only boosts generalization for previously unseen composition pairs\nbut also sharpens the discrimination of similar pairs, fostering more robust\nand precise learning. Extensive experiments demonstrate state-of-the-art\nperformance in closed-world scenarios and competitive open-world results across\nfour established CZSL benchmarks, validating the effectiveness of our approach\nin advancing compositional generalization.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 Jan 2025 17:30:27 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 05:46:59 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 04:04:32 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhang', 'Shiyu', ''], ['Yan', 'Cheng', ''], ['Liu', 'Yang', ''], ['Jing', 'Chenchen', ''], ['Zhou', 'Lei', ''], ['Wang', 'Wenjun', '']]","extracted_entities":"[{'text': 'Compositional Zero-Shot Learning', 'label': 'Zero-shot Learning'}, {'text': 'CZSL', 'label': 'Zero-shot Learning'}, {'text': 'Visual Proxy\\nLearning', 'label': 'Zero-shot Learning'}, {'text': 'CZSL', 'label': 'Few-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"Compositional Zero-Shot Learning","similarity_score":0.8895157576}
{"id":2502.07842,"submitter":"Jiyoon Kim","authors":"Jiyoon Kim, Kang Eun Jeon, Yulhwa Kim, and Jong Hwan Ko","title":"Column-wise Quantization of Weights and Partial Sums for Accurate and\n  Efficient Compute-In-Memory Accelerators","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AR cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Compute-in-memory (CIM) is an efficient method for implementing deep neural\nnetworks (DNNs) but suffers from substantial overhead from analog-to-digital\nconverters (ADCs), especially as ADC precision increases. Low-precision ADCs\ncan reduce this overhead but introduce partial-sum quantization errors\ndegrading accuracy. Additionally, low-bit weight constraints, imposed by cell\nlimitations and the need for multiple cells for higher-bit weights, present\nfurther challenges. While fine-grained partial-sum quantization has been\nstudied to lower ADC resolution effectively, weight granularity, which limits\noverall partial-sum quantized accuracy, remains underexplored. This work\naddresses these challenges by aligning weight and partial-sum quantization\ngranularities at the column-wise level. Our method improves accuracy while\nmaintaining dequantization overhead, simplifies training by removing two-stage\nprocesses, and ensures robustness to memory cell variations via independent\ncolumn-wise scale factors. We also propose an open-source CIM-oriented\nconvolution framework to handle fine-grained weights and partial-sums\nefficiently, incorporating a novel tiling method and group convolution.\nExperimental results on ResNet-20 (CIFAR-10, CIFAR-100) and ResNet-18\n(ImageNet) show accuracy improvements of 0.99%, 2.69%, and 1.01%, respectively,\ncompared to the best-performing related works. Additionally, variation analysis\nreveals the robustness of our method against memory cell variations. These\nfindings highlight the effectiveness of our quantization scheme in enhancing\naccuracy and robustness while maintaining hardware efficiency in CIM-based DNN\nimplementations. Our code is available at\nhttps:\/\/github.com\/jiyoonkm\/ColumnQuant.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Feb 2025 05:32:14 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 11:32:19 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Kim', 'Jiyoon', ''], ['Jeon', 'Kang Eun', ''], ['Kim', 'Yulhwa', ''], ['Ko', 'Jong Hwan', '']]","extracted_entities":"[{'text': 'partial-sum quantization', 'label': 'quantisation'}, {'text': 'partial-sum quantization', 'label': 'quantisation'}, {'text': 'partial-sum quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"partial-sum quantization","similarity_score":0.5884458423}
{"id":2503.0997,"submitter":"Sachin Vaidya","authors":"Sachin Vaidya, Andr\\'e Grossi Fonseca, Mark R. Hirsbrunner, Taylor L.\n  Hughes, Marin Solja\\v{c}i\\'c","title":"Quantized crystalline-electromagnetic responses in insulators","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mes-hall","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce new classes of gapped topological phases characterized by\nquantized crystalline-electromagnetic responses, termed \"multipolar Chern\ninsulators\". These systems are characterized by nonsymmorphic momentum-space\nsymmetries and mirror symmetries, leading to quantization of momentum-weighted\nBerry curvature multipole moments. We construct lattice models for such phases\nand confirm their quantized responses through numerical calculations. These\nsystems exhibit bound charge and momentum densities at lattice and magnetic\ndefects, and currents induced by electric or time-varying strain fields. Our\nwork extends the classification of topological matter by uncovering novel\nsymmetry-protected topological phases with quantized responses.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 02:19:21 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Vaidya', 'Sachin', ''], ['Fonseca', 'Andr\u00e9 Grossi', ''], ['Hirsbrunner', 'Mark R.', ''], ['Hughes', 'Taylor L.', ''], ['Solja\u010di\u0107', 'Marin', '']]","extracted_entities":"[{'text': 'quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization","similarity_score":0.8134455085}
{"id":2503.09975,"submitter":"Joonhyung Lee","authors":"Joonhyung Lee, Shmulik Markovich-Golan, Daniel Ohayon, Yair Hanani,\n  Gunho Park, Byeongwook Kim, Asaf Karnieli, Uri Livne, Haihao Shen, Tai Huang,\n  Se Jung Kwon, Dongsoo Lee","title":"Faster Inference of LLMs using FP8 on the Intel Gaudi","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Low-precision data types are essential in modern neural networks during both\ntraining and inference as they enhance throughput and computational capacity by\nbetter exploiting available hardware resources. Despite the incorporation of\nFP8 in commercially available neural network accelerators, a comprehensive\nexposition of its underlying mechanisms, along with rigorous performance and\naccuracy evaluations, is still lacking. In this work, we contribute in three\nsignificant ways. First, we analyze the implementation details and quantization\noptions associated with FP8 for inference on the Intel Gaudi AI accelerator.\nSecond, we empirically quantify the throughput improvements afforded by the use\nof FP8 at both the operator level and in end-to-end scenarios. Third, we assess\nthe accuracy impact of various FP8 quantization methods. Our experimental\nresults indicate that the Intel Gaudi 2 accelerator consistently achieves high\ncomputational unit utilization, frequently exceeding 90\\% MFU, while incurring\nan accuracy degradation of less than 1\\%.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 02:21:39 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Lee', 'Joonhyung', ''], ['Markovich-Golan', 'Shmulik', ''], ['Ohayon', 'Daniel', ''], ['Hanani', 'Yair', ''], ['Park', 'Gunho', ''], ['Kim', 'Byeongwook', ''], ['Karnieli', 'Asaf', ''], ['Livne', 'Uri', ''], ['Shen', 'Haihao', ''], ['Huang', 'Tai', ''], ['Kwon', 'Se Jung', ''], ['Lee', 'Dongsoo', '']]","extracted_entities":"[{'text': 'quantization\\noptions', 'label': 'quantisation'}, {'text': 'quantization methods', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization\noptions","similarity_score":0.7135424614}
{"id":2503.10199,"submitter":"Ruibiao Song","authors":"Ruibiao Song, Liying Zhang","title":"Optimal Estimation and Uncertainty Quantification for Stochastic Inverse\n  Problems via Variational Bayesian Methods","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.NA","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The Bayesian inversion method demonstrates significant potential for solving\ninverse problems, enabling both point estimation and uncertainty\nquantification. However, Bayesian maximum a posteriori (MAP) estimation may\nbecome unstable when handling data from diverse distributions (e.g., solutions\nof stochastic partial differential equations (SPDEs)). Additionally, Monte\nCarlo sampling methods are computationally expensive. To address these\nchallenges, we propose a novel two-stage optimization method based on optimal\ncontrol theory and variational Bayesian methods. This method not only achieves\nstable solutions for stochastic inverse problems but also efficiently\nquantifies the uncertainty of the solutions. In the first stage, we introduce a\nnew weighting formulation to ensure the stability of the Bayesian MAP\nestimation. In the second stage, we derive the necessary condition to\nefficiently quantify the uncertainty of the solutions, by combining the new\nweighting formula with variational inference. Furthermore, we establish an\nerror estimation theorem that relates the exact solution to the optimally\nestimated solution under different amounts of observed data. Finally, the\nefficiency of the proposed method is demonstrated through numerical examples.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 09:34:33 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Song', 'Ruibiao', ''], ['Zhang', 'Liying', '']]","extracted_entities":"[{'text': 'uncertainty\\nquantification', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"uncertainty\nquantification","similarity_score":0.5714546442}
{"id":2503.10205,"submitter":"Anthony Couthures","authors":"Anthony Couthures, Vineeth S. Varma, Samson Lasaulce,\n  Irinel-Constantin Morarescu","title":"Global synchronization of multi-agent systems with nonlinear\n  interactions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  The paper addresses the synchronization of multi-agent systems with\ncontinuous-time dynamics interacting through a very general class of monotonic\ncontinuous signal functions that covers estimation biases, approximation of\ndiscrete quantization, or state-dependent estimation. Our analysis reveals\nthat, in the setup under consideration, synchronization equilibria are exactly\nthe fixed points of the signal function. We also derive intuitive stability\nconditions based on whether the signal underestimates or overestimates the\nstate of the agents around these fixed points. Moreover, we show that network\ntopology plays a crucial role in asymptotic synchronization. These results\nprovide interesting insights into the interplay between communication\nnonlinearity and network connectivity, paving the way for advanced coordination\nstrategies in complex systems.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 09:43:43 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Couthures', 'Anthony', ''], ['Varma', 'Vineeth S.', ''], ['Lasaulce', 'Samson', ''], ['Morarescu', 'Irinel-Constantin', '']]","extracted_entities":"[{'text': 'discrete quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"discrete quantization","similarity_score":0.713095665}
{"id":2503.1057,"submitter":"Joshua Lackman","authors":"Joshua Lackman","title":"A Simple Description of the Hyperk\\\"{a}hler Structure of the Cotangent\n  Bundle of Projective Space via Quantization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.SG hep-th math-ph math.AG math.MP math.QA","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Quantization identifies the cotangent bundle of projective space with the\n(non-Hermitian) rank-$1$ projections of a Hilbert space. We use this\nidentification to study the natural geometric structures of these cotangent\nbundles and those of Grassmanians. In particular, we show that the quantization\nmap is an isometric and complex embedding\n$T^*\\mathbb{P}\\mathcal{H}\\hookrightarrow\\mathcal{B}(\\mathcal{H})\\backslash\\{0\\}.$\nHere, the metric on the domain is the hyperk\\\"{a}hler metric and the metric on\nthe codomain is the one whose K\\\"{a}hler potential is the Hilbert-Schmidt norm.\nThe K\\\"{a}hler potential pulled back to $T^*\\mathbb{P}\\mathcal{H}$ equals the\ntrace-class norm. Using this, we give a complete, simple and explicit\ndescription of the hyperk\\\"{a}hler structure. Our constructions are functorial,\ncoordinate-free and reduction-free.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:23:09 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Lackman', 'Joshua', '']]","extracted_entities":"[{'text': 'Quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"Quantization","similarity_score":0.8134455085}
{"id":2410.0407,"submitter":"Ruizhe Chen","authors":"Ruizhe Chen, Xiaotian Zhang, Meng Luo, Wenhao Chai, and Zuozhu Liu","title":"PAD: Personalized Alignment of LLMs at Decoding-Time","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment.\n","versions":"[{'version': 'v1', 'created': 'Sat, 5 Oct 2024 08:00:55 GMT'}, {'version': 'v2', 'created': 'Mon, 14 Oct 2024 13:27:36 GMT'}, {'version': 'v3', 'created': 'Wed, 16 Oct 2024 06:15:35 GMT'}, {'version': 'v4', 'created': 'Tue, 29 Oct 2024 12:51:33 GMT'}, {'version': 'v5', 'created': 'Thu, 7 Nov 2024 06:21:14 GMT'}, {'version': 'v6', 'created': 'Tue, 4 Mar 2025 13:51:14 GMT'}, {'version': 'v7', 'created': 'Thu, 13 Mar 2025 13:37:57 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chen', 'Ruizhe', ''], ['Zhang', 'Xiaotian', ''], ['Luo', 'Meng', ''], ['Chai', 'Wenhao', ''], ['Liu', 'Zuozhu', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM-based'}]","assigned_concept":"LLM-based","matched_keyword":"LLMs","similarity_score":0.8177113533}
{"id":2502.07938,"submitter":"Andrianos Michail","authors":"Andrianos Michail, Corina Julia Racl\\'e, Juri Opitz, Simon Clematide","title":"Adapting Multilingual Embedding Models to Historical Luxembourgish","comments":"To appear in LaTeCH-CLfL 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The growing volume of digitized historical texts requires effective semantic\nsearch using text embeddings. However, pre-trained multilingual models face\nchallenges with historical content due to OCR noise and outdated spellings.\nThis study examines multilingual embeddings for cross-lingual semantic search\nin historical Luxembourgish (LB), a low-resource language. We collect\nhistorical Luxembourgish news articles from various periods and use GPT-4o for\nsentence segmentation and translation, generating 20,000 parallel training\nsentences per language pair. Additionally, we create a semantic search\n(Historical LB Bitext Mining) evaluation set and find that existing models\nperform poorly on cross-lingual search for historical Luxembourgish. Using our\nhistorical and additional modern parallel training data, we adapt several\nmultilingual embedding models through contrastive learning or knowledge\ndistillation and increase accuracy significantly for all models. We release our\nadapted models and historical Luxembourgish-German\/French\/English bitexts to\nsupport further research.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Feb 2025 20:35:29 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Feb 2025 10:38:40 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 13:19:30 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Michail', 'Andrianos', ''], ['Racl\u00e9', 'Corina Julia', ''], ['Opitz', 'Juri', ''], ['Clematide', 'Simon', '']]","extracted_entities":"[{'text': 'text embeddings', 'label': 'Embedding'}, {'text': 'multilingual embeddings', 'label': 'Embedding'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'contrastive learning', 'label': 'Few-shot Learning'}, {'text': 'knowledge\\ndistillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge\ndistillation","similarity_score":1.0000002384}
{"id":2503.09601,"submitter":"Itay Chachy","authors":"Itay Chachy and Guy Yariv and Sagie Benaim","title":"RewardSDS: Aligning Score Distillation via Reward-Weighted Sampling","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Score Distillation Sampling (SDS) has emerged as an effective technique for\nleveraging 2D diffusion priors for tasks such as text-to-3D generation. While\npowerful, SDS struggles with achieving fine-grained alignment to user intent.\nTo overcome this, we introduce RewardSDS, a novel approach that weights noise\nsamples based on alignment scores from a reward model, producing a weighted SDS\nloss. This loss prioritizes gradients from noise samples that yield aligned\nhigh-reward output. Our approach is broadly applicable and can extend SDS-based\nmethods. In particular, we demonstrate its applicability to Variational Score\nDistillation (VSD) by introducing RewardVSD. We evaluate RewardSDS and\nRewardVSD on text-to-image, 2D editing, and text-to-3D generation tasks,\nshowing significant improvements over SDS and VSD on a diverse set of metrics\nmeasuring generation quality and alignment to desired reward models, enabling\nstate-of-the-art performance. Project page is available at\nhttps:\/\/itaychachy.github.io\/reward-sds\/.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 17:59:47 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 13:28:22 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chachy', 'Itay', ''], ['Yariv', 'Guy', ''], ['Benaim', 'Sagie', '']]","extracted_entities":"[{'text': 'Score Distillation Sampling', 'label': 'Knowledge distillation'}, {'text': 'Variational Score\\nDistillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Score Distillation Sampling","similarity_score":0.5910027623}
{"id":2503.10152,"submitter":"Shenghao Fu","authors":"Shenghao Fu, Junkai Yan, Qize Yang, Xihan Wei, Xiaohua Xie, Wei-Shi\n  Zheng","title":"A Hierarchical Semantic Distillation Framework for Open-Vocabulary\n  Object Detection","comments":"Accepted to TMM 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Open-vocabulary object detection (OVD) aims to detect objects beyond the\ntraining annotations, where detectors are usually aligned to a pre-trained\nvision-language model, eg, CLIP, to inherit its generalizable recognition\nability so that detectors can recognize new or novel objects. However, previous\nworks directly align the feature space with CLIP and fail to learn the semantic\nknowledge effectively. In this work, we propose a hierarchical semantic\ndistillation framework named HD-OVD to construct a comprehensive distillation\nprocess, which exploits generalizable knowledge from the CLIP model in three\naspects. In the first hierarchy of HD-OVD, the detector learns fine-grained\ninstance-wise semantics from the CLIP image encoder by modeling relations among\nsingle objects in the visual space. Besides, we introduce text space\nnovel-class-aware classification to help the detector assimilate the highly\ngeneralizable class-wise semantics from the CLIP text encoder, representing the\nsecond hierarchy. Lastly, abundant image-wise semantics containing multi-object\nand their contexts are also distilled by an image-wise contrastive\ndistillation. Benefiting from the elaborated semantic distillation in triple\nhierarchies, our HD-OVD inherits generalizable recognition ability from CLIP in\ninstance, class, and image levels. Thus, we boost the novel AP on the OV-COCO\ndataset to 46.4% with a ResNet50 backbone, which outperforms others by a clear\nmargin. We also conduct extensive ablation studies to analyze how each\ncomponent works.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 08:27:18 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Fu', 'Shenghao', ''], ['Yan', 'Junkai', ''], ['Yang', 'Qize', ''], ['Wei', 'Xihan', ''], ['Xie', 'Xiaohua', ''], ['Zheng', 'Wei-Shi', '']]","extracted_entities":"[{'text': 'image-wise contrastive\\ndistillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"image-wise contrastive\ndistillation","similarity_score":0.596357584}
{"id":2503.10637,"submitter":"Rohit Gandikota","authors":"Rohit Gandikota, David Bau","title":"Distilling Diversity and Control in Diffusion Models","comments":"Project Page: https:\/\/distillation.baulab.info","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GR cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Distilled diffusion models suffer from a critical limitation: reduced sample\ndiversity compared to their base counterparts. In this work, we uncover that\ndespite this diversity loss, distilled models retain the fundamental concept\nrepresentations of base models. We demonstrate control distillation - where\ncontrol mechanisms like Concept Sliders and LoRAs trained on base models can be\nseamlessly transferred to distilled models and vice-versa, effectively\ndistilling control without any retraining. This preservation of\nrepresentational structure prompted our investigation into the mechanisms of\ndiversity collapse during distillation. To understand how distillation affects\ndiversity, we introduce Diffusion Target (DT) Visualization, an analysis and\ndebugging tool that reveals how models predict final outputs at intermediate\nsteps. Through DT-Visualization, we identify generation artifacts,\ninconsistencies, and demonstrate that initial diffusion timesteps\ndisproportionately determine output diversity, while later steps primarily\nrefine details. Based on these insights, we introduce diversity distillation -\na hybrid inference approach that strategically employs the base model for only\nthe first critical timestep before transitioning to the efficient distilled\nmodel. Our experiments demonstrate that this simple modification not only\nrestores the diversity capabilities from base to distilled models but\nsurprisingly exceeds it, while maintaining nearly the computational efficiency\nof distilled inference, all without requiring additional training or model\nmodifications. Our code and data are available at\nhttps:\/\/distillation.baulab.info\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:59:56 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Gandikota', 'Rohit', ''], ['Bau', 'David', '']]","extracted_entities":"[{'text': 'base models', 'label': 'Foundation Model'}, {'text': 'distillation', 'label': 'Knowledge distillation'}, {'text': 'base models', 'label': 'Foundation Model'}, {'text': 'distillation', 'label': 'Knowledge distillation'}, {'text': 'diversity distillation', 'label': 'Knowledge distillation'}, {'text': 'distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"distillation","similarity_score":0.7657151222}
{"id":2403.05158,"submitter":"Zuguang Li","authors":"Zuguang Li, Wen Wu, Shaohua Wu, and Wei Wang","title":"Adaptive Split Learning over Energy-Constrained Wireless Edge Networks","comments":"6 pages, 5 figures, 20 conferences","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.NI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Split learning (SL) is a promising approach for training artificial\nintelligence (AI) models, in which devices collaborate with a server to train\nan AI model in a distributed manner, based on a same fixed split point.\nHowever, due to the device heterogeneity and variation of channel conditions,\nthis way is not optimal in training delay and energy consumption. In this\npaper, we design an adaptive split learning (ASL) scheme which can dynamically\nselect split points for devices and allocate computing resource for the server\nin wireless edge networks. We formulate an optimization problem to minimize the\naverage training latency subject to long-term energy consumption constraint.\nThe difficulties in solving this problem are the lack of future information and\nmixed integer programming (MIP). To solve it, we propose an online algorithm\nleveraging the Lyapunov theory, named OPEN, which decomposes it into a new MIP\nproblem only with the current information. Then, a two-layer optimization\nmethod is proposed to solve the MIP problem. Extensive simulation results\ndemonstrate that the ASL scheme can reduce the average training delay and\nenergy consumption by 53.7% and 22.1%, respectively, as compared to the\nexisting SL schemes.\n","versions":"[{'version': 'v1', 'created': 'Fri, 8 Mar 2024 08:51:37 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 13:27:47 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Zuguang', ''], ['Wu', 'Wen', ''], ['Wu', 'Shaohua', ''], ['Wang', 'Wei', '']]","extracted_entities":"[{'text': 'Split learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"Split learning","similarity_score":0.5144664049}
{"id":2411.011,"submitter":"Xinran Miao","authors":"Xinran Miao, Jiwei Zhao, Hyunseung Kang","title":"Transfer Learning Between U.S. Presidential Elections: How Should We\n  Learn From A 2020 Ad Campaign To Inform 2024 Ad Campaigns?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.AP stat.ME","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  For the 2024 U.S. presidential election, would negative, digital ads against\nDonald Trump impact voter turnout in Pennsylvania (PA), a key \"tipping point''\nstate? The gold standard to address this question, a randomized experiment\nwhere voters get randomized to different ads, yields unbiased estimates of the\nad effect, but is very expensive. Instead, we propose a less-than-ideal, but\nsignificantly cheaper and faster framework based on transfer learning, where we\ntransfer knowledge from a past ad experiment in 2020 to evaluate ads for 2024.\nA key component of our framework is a sensitivity analysis that quantifies the\nunobservable differences between 2020 and 2024 elections, where sensitivity\nparameters can be calibrated in a data-driven manner. We propose two estimators\nof the 2024 ad effect: a simple regression estimator with bootstrap, which we\nrecommend for practitioners in this field, and an estimator based on the\nefficient influence function for broader applications. Using our framework, we\nestimate the effect of running a negative, digital ad campaign against Trump on\nvoter turnout in PA for the 2024 election. Our findings indicate effect\nheterogeneity across counties of PA and among important subgroups stratified by\ngender, urbanicity, and education attainment.\n","versions":"[{'version': 'v1', 'created': 'Sat, 2 Nov 2024 01:35:58 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 01:23:02 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Miao', 'Xinran', ''], ['Zhao', 'Jiwei', ''], ['Kang', 'Hyunseung', '']]","extracted_entities":"[{'text': 'transfer learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"transfer learning","similarity_score":0.5694054365}
{"id":2411.14871,"submitter":"Dingyuan Shi","authors":"Dingyuan Shi, Yong Wang, Hangyu Li, Xiangxiang Chu","title":"Preference Alignment for Diffusion Model via Explicit Denoised\n  Distribution Estimation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Diffusion models have shown remarkable success in text-to-image generation,\nmaking preference alignment for these models increasingly important. The\npreference labels are typically available only at the terminal of denoising\ntrajectories, which poses challenges in optimizing the intermediate denoising\nsteps. In this paper, we propose to conduct Denoised Distribution Estimation\n(DDE) that explicitly connects intermediate steps to the terminal denoised\ndistribution. Therefore, preference labels can be used for the entire\ntrajectory optimization. To this end, we design two estimation strategies for\nour DDE. The first is stepwise estimation, which utilizes the conditional\ndenoised distribution to estimate the model denoised distribution. The second\nis single-shot estimation, which converts the model output into the terminal\ndenoised distribution via DDIM modeling. Analytically and empirically, we\nreveal that DDE equipped with two estimation strategies naturally derives a\nnovel credit assignment scheme that prioritizes optimizing the middle part of\nthe denoising trajectory. Extensive experiments demonstrate that our approach\nachieves superior performance, both quantitatively and qualitatively.\n","versions":"[{'version': 'v1', 'created': 'Fri, 22 Nov 2024 11:45:33 GMT'}, {'version': 'v2', 'created': 'Wed, 25 Dec 2024 14:55:08 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 02:36:28 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Shi', 'Dingyuan', ''], ['Wang', 'Yong', ''], ['Li', 'Hangyu', ''], ['Chu', 'Xiangxiang', '']]","extracted_entities":"[{'text': 'single-shot estimation', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"single-shot estimation","similarity_score":0.6626640558}
{"id":2501.17568,"submitter":"Ehsan Aminian","authors":"Ehsan Aminian, Rita P. Ribeiro, Joao Gama","title":"Histogram Approaches for Imbalanced Data Streams Regression","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Imbalanced domains pose a significant challenge in real-world predictive\nanalytics, particularly in the context of regression. While existing research\nhas primarily focused on batch learning from static datasets, limited attention\nhas been given to imbalanced regression in online learning scenarios. Intending\nto address this gap, in prior work, we proposed sampling strategies based on\nChebyshevs inequality as the first methodologies designed explicitly for data\nstreams. However, these approaches operated under the restrictive assumption\nthat rare instances exclusively reside at distribution extremes. This study\nintroduces histogram-based sampling strategies to overcome this constraint,\nproposing flexible solutions for imbalanced regression in evolving data\nstreams. The proposed techniques -- Histogram-based Undersampling (HistUS) and\nHistogram-based Oversampling (HistOS) -- employ incremental online histograms\nto dynamically detect and prioritize rare instances across arbitrary regions of\nthe target distribution to improve predictions in the rare cases. Comprehensive\nexperiments on synthetic and real-world benchmarks demonstrate that HistUS and\nHistOS substantially improve rare-case prediction accuracy, outperforming\nbaseline models while maintaining competitiveness with Chebyshev-based\napproaches.\n","versions":"[{'version': 'v1', 'created': 'Wed, 29 Jan 2025 11:03:02 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 11:38:47 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Aminian', 'Ehsan', ''], ['Ribeiro', 'Rita P.', ''], ['Gama', 'Joao', '']]","extracted_entities":"[{'text': 'batch learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"batch learning","similarity_score":0.5028504729}
{"id":2503.05491,"submitter":"Lo\\\"ic Fosse","authors":"Lo\\\"ic Fosse and Fr\\'ed\\'eric B\\'echet and Beno\\^it Favre and\n  G\\'eraldine Damnati and Gw\\'enol\\'e Lecorv\\'e and Maxime Darrin and Philippe\n  Formont and Pablo Piantanida","title":"Statistical Deficiency for Task Inclusion Estimation","comments":"34 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Tasks are central in machine learning, as they are the most natural objects\nto assess the capabilities of current models. The trend is to build general\nmodels able to address any task. Even though transfer learning and multitask\nlearning try to leverage the underlying task space, no well-founded tools are\navailable to study its structure. This study proposes a theoretically grounded\nsetup to define the notion of task and to compute the {\\bf inclusion} between\ntwo tasks from a statistical deficiency point of view. We propose a tractable\nproxy as information sufficiency to estimate the degree of inclusion between\ntasks, show its soundness on synthetic data, and use it to reconstruct\nempirically the classic NLP pipeline.\n","versions":"[{'version': 'v1', 'created': 'Fri, 7 Mar 2025 15:00:28 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 08:41:29 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Fosse', 'Lo\u00efc', ''], ['B\u00e9chet', 'Fr\u00e9d\u00e9ric', ''], ['Favre', 'Beno\u00eet', ''], ['Damnati', 'G\u00e9raldine', ''], ['Lecorv\u00e9', 'Gw\u00e9nol\u00e9', ''], ['Darrin', 'Maxime', ''], ['Formont', 'Philippe', ''], ['Piantanida', 'Pablo', '']]","extracted_entities":"[{'text': 'transfer learning', 'label': 'Few-shot Learning'}, {'text': 'multitask\\nlearning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"transfer learning","similarity_score":0.5694054365}
{"id":2503.09494,"submitter":"Qi Xu","authors":"Qi Xu and Annie Qu","title":"Representation Retrieval Learning for Heterogeneous Data Integration","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ME","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In the era of big data, large-scale, multi-modal datasets are increasingly\nubiquitous, offering unprecedented opportunities for predictive modeling and\nscientific discovery. However, these datasets often exhibit complex\nheterogeneity, such as covariate shift, posterior drift, and missing\nmodalities, that can hinder the accuracy of existing prediction algorithms. To\naddress these challenges, we propose a novel Representation Retrieval ($R^2$)\nframework, which integrates a representation learning module (the representer)\nwith a sparsity-induced machine learning model (the learner). Moreover, we\nintroduce the notion of \"integrativeness\" for representers, characterized by\nthe effective data sources used in learning representers, and propose a\nSelective Integration Penalty (SIP) to explicitly improve the property.\nTheoretically, we demonstrate that the $R^2$ framework relaxes the conventional\nfull-sharing assumption in multi-task learning, allowing for partially shared\nstructures, and that SIP can improve the convergence rate of the excess risk\nbound. Extensive simulation studies validate the empirical performance of our\nframework, and applications to two real-world datasets further confirm its\nsuperiority over existing approaches.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 15:54:37 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 16:39:15 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Xu', 'Qi', ''], ['Qu', 'Annie', '']]","extracted_entities":"[{'text': 'multi-task learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"multi-task learning","similarity_score":0.5245423317}
{"id":2503.10003,"submitter":"Dongjun Hwang","authors":"Shiwon Kim, Dongjun Hwang, Sungwon Woo, Rita Singh","title":"A New Benchmark for Few-Shot Class-Incremental Learning: Redefining the\n  Upper Bound","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Class-incremental learning (CIL) aims to continuously adapt to emerging\nclasses while retaining knowledge of previously learned ones. Few-shot\nclass-incremental learning (FSCIL) presents an even greater challenge which\nrequires the model to learn incremental classes with only a limited number of\nsamples. In conventional CIL, joint training is widely considered the upper\nbound, serving as both a benchmark and a methodological guide. However, we find\nthat joint training fails to be a meaningful upper bound in FSCIL due to the\ninherent difficulty of inter-task class separation (ICS) caused by severe class\nimbalance. In this work, we introduce a new joint training benchmark tailored\nfor FSCIL by integrating imbalance-aware techniques, effectively bridging the\nperformance gap between base and incremental classes. Furthermore, we point out\ninconsistencies in the experimental setup and evaluation of existing FSCIL\nmethods. To ensure fair comparisons between different FSCIL approaches and\njoint training, we standardize training conditions and propose a unified\nevaluation protocol that simultaneously considers the validation set and\ncomputational complexity. By establishing a reliable upper bound and a\nstandardized evaluation framework for FSCIL, our work provides a clear\nbenchmark and a practical foundation for future research.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 03:25:29 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Kim', 'Shiwon', ''], ['Hwang', 'Dongjun', ''], ['Woo', 'Sungwon', ''], ['Singh', 'Rita', '']]","extracted_entities":"[{'text': 'Class-incremental learning', 'label': 'Few-shot Learning'}, {'text': 'Few-shot\\nclass-incremental learning', 'label': 'Few-shot Learning'}, {'text': 'FSCIL', 'label': 'Few-shot Learning'}, {'text': 'CIL', 'label': 'Zero-shot Learning'}, {'text': 'joint training', 'label': 'Zero-shot Learning'}, {'text': 'joint training', 'label': 'Zero-shot Learning'}, {'text': 'FSCIL', 'label': 'Zero-shot Learning'}, {'text': 'FSCIL', 'label': 'Few-shot Learning'}, {'text': 'FSCIL', 'label': 'Few-shot Learning'}, {'text': 'FSCIL', 'label': 'Few-shot Learning'}, {'text': 'joint training', 'label': 'Zero-shot Learning'}, {'text': 'FSCIL', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"Few-shot\nclass-incremental learning","similarity_score":0.793258667}
{"id":2503.10065,"submitter":"Damien Teney","authors":"Damien Teney, Liangze Jiang, Florin Gogianu, Ehsan Abbasnejad","title":"Do We Always Need the Simplicity Bias? Looking for Optimal Inductive\n  Biases in the Wild","comments":"IEEE\/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Neural architectures tend to fit their data with relatively simple functions.\nThis \"simplicity bias\" is widely regarded as key to their success. This paper\nexplores the limits of this principle. Building on recent findings that the\nsimplicity bias stems from ReLU activations [96], we introduce a method to\nmeta-learn new activation functions and inductive biases better suited to\nspecific tasks.\n  Findings: We identify multiple tasks where the simplicity bias is inadequate\nand ReLUs suboptimal. In these cases, we learn new activation functions that\nperform better by inducing a prior of higher complexity. Interestingly, these\ncases correspond to domains where neural networks have historically struggled:\ntabular data, regression tasks, cases of shortcut learning, and algorithmic\ngrokking tasks. In comparison, the simplicity bias induced by ReLUs proves\nadequate on image tasks where the best learned activations are nearly identical\nto ReLUs and GeLUs.\n  Implications: Contrary to popular belief, the simplicity bias of ReLU\nnetworks is not universally useful. It is near-optimal for image\nclassification, but other inductive biases are sometimes preferable. We showed\nthat activation functions can control these inductive biases, but future\ntailored architectures might provide further benefits. Advances are still\nneeded to characterize a model's inductive biases beyond \"complexity\", and\ntheir adequacy with the data.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 05:28:40 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Teney', 'Damien', ''], ['Jiang', 'Liangze', ''], ['Gogianu', 'Florin', ''], ['Abbasnejad', 'Ehsan', '']]","extracted_entities":"[{'text': 'simplicity bias', 'label': 'Model Bias and Fairness'}, {'text': 'simplicity bias', 'label': 'Model Bias and Fairness'}, {'text': 'simplicity bias', 'label': 'Model Bias and Fairness'}, {'text': 'shortcut learning', 'label': 'Few-shot Learning'}, {'text': 'simplicity bias', 'label': 'Model Bias and Fairness'}, {'text': 'simplicity bias', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Few-shot Learning","matched_keyword":"shortcut learning","similarity_score":0.508158803}
{"id":2503.101,"submitter":"Tianhao Peng","authors":"Tianhao Peng, Xuhong Li, Haitao Yuan, Yuchen Li, Haoyi Xiong","title":"SOLA-GCL: Subgraph-Oriented Learnable Augmentation Method for Graph\n  Contrastive Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Graph contrastive learning has emerged as a powerful technique for learning\ngraph representations that are robust and discriminative. However, traditional\napproaches often neglect the critical role of subgraph structures, particularly\nthe intra-subgraph characteristics and inter-subgraph relationships, which are\ncrucial for generating informative and diverse contrastive pairs. These\nsubgraph features are crucial as they vary significantly across different graph\ntypes, such as social networks where they represent communities, and\nbiochemical networks where they symbolize molecular interactions. To address\nthis issue, our work proposes a novel subgraph-oriented learnable augmentation\nmethod for graph contrastive learning, termed SOLA-GCL, that centers around\nsubgraphs, taking full advantage of the subgraph information for data\naugmentation. Specifically, SOLA-GCL initially partitions a graph into multiple\ndensely connected subgraphs based on their intrinsic properties. To preserve\nand enhance the unique characteristics inherent to subgraphs, a graph view\ngenerator optimizes augmentation strategies for each subgraph, thereby\ngenerating tailored views for graph contrastive learning. This generator uses a\ncombination of intra-subgraph and inter-subgraph augmentation strategies,\nincluding node dropping, feature masking, intra-edge perturbation, inter-edge\nperturbation, and subgraph swapping. Extensive experiments have been conducted\non various graph learning applications, ranging from social networks to\nmolecules, under semi-supervised learning, unsupervised learning, and transfer\nlearning settings to demonstrate the superiority of our proposed approach over\nthe state-of-the-art in GCL.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 06:52:39 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Peng', 'Tianhao', ''], ['Li', 'Xuhong', ''], ['Yuan', 'Haitao', ''], ['Li', 'Yuchen', ''], ['Xiong', 'Haoyi', '']]","extracted_entities":"[{'text': 'semi-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'unsupervised learning', 'label': 'Few-shot Learning'}, {'text': 'transfer\\nlearning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"transfer\nlearning","similarity_score":0.5694054365}
{"id":2503.10214,"submitter":"Zhiwu Wang","authors":"Zhiwu Wang, Yichen Wu, Renzhen Wang, Haokun Lin, Quanziang Wang, Qian\n  Zhao, Deyu Meng","title":"Singular Value Fine-tuning for Few-Shot Class-Incremental Learning","comments":"12 pages, 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Class-Incremental Learning (CIL) aims to prevent catastrophic forgetting of\npreviously learned classes while sequentially incorporating new ones. The more\nchallenging Few-shot CIL (FSCIL) setting further complicates this by providing\nonly a limited number of samples for each new class, increasing the risk of\noverfitting in addition to standard CIL challenges. While catastrophic\nforgetting has been extensively studied, overfitting in FSCIL, especially with\nlarge foundation models, has received less attention. To fill this gap, we\npropose the Singular Value Fine-tuning for FSCIL (SVFCL) and compared it with\nexisting approaches for adapting foundation models to FSCIL, which primarily\nbuild on Parameter Efficient Fine-Tuning (PEFT) methods like prompt tuning and\nLow-Rank Adaptation (LoRA). Specifically, SVFCL applies singular value\ndecomposition to the foundation model weights, keeping the singular vectors\nfixed while fine-tuning the singular values for each task, and then merging\nthem. This simple yet effective approach not only alleviates the forgetting\nproblem but also mitigates overfitting more effectively while significantly\nreducing trainable parameters. Extensive experiments on four benchmark\ndatasets, along with visualizations and ablation studies, validate the\neffectiveness of SVFCL. The code will be made available.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 09:57:28 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Zhiwu', ''], ['Wu', 'Yichen', ''], ['Wang', 'Renzhen', ''], ['Lin', 'Haokun', ''], ['Wang', 'Quanziang', ''], ['Zhao', 'Qian', ''], ['Meng', 'Deyu', '']]","extracted_entities":"[{'text': 'Few-shot CIL', 'label': 'Few-shot Learning'}, {'text': 'FSCIL', 'label': 'Few-shot Learning'}, {'text': 'CIL', 'label': 'Zero-shot Learning'}, {'text': 'FSCIL', 'label': 'Few-shot Learning'}, {'text': 'Singular Value Fine-tuning', 'label': 'Fine-tuning'}, {'text': 'FSCIL', 'label': 'Few-shot Learning'}, {'text': 'FSCIL', 'label': 'Few-shot Learning'}, {'text': 'prompt tuning', 'label': 'Prompting'}]","assigned_concept":"Few-shot Learning","matched_keyword":"Few-shot CIL","similarity_score":0.6532399058}
{"id":2503.10252,"submitter":"Zhi Chen","authors":"Zhi Chen and Zecheng Zhao and Jingcai Guo and Jingjing Li and Zi Huang","title":"SVIP: Semantically Contextualized Visual Patches for Zero-Shot Learning","comments":"Pre-print","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Zero-shot learning (ZSL) aims to recognize unseen classes without labeled\ntraining examples by leveraging class-level semantic descriptors such as\nattributes. A fundamental challenge in ZSL is semantic misalignment, where\nsemantic-unrelated information involved in visual features introduce ambiguity\nto visual-semantic interaction. Unlike existing methods that suppress\nsemantic-unrelated information post hoc either in the feature space or the\nmodel space, we propose addressing this issue at the input stage, preventing\nsemantic-unrelated patches from propagating through the network. To this end,\nwe introduce Semantically contextualized VIsual Patches (SVIP) for ZSL, a\ntransformer-based framework designed to enhance visual-semantic alignment.\nSpecifically, we propose a self-supervised patch selection mechanism that\npreemptively learns to identify semantic-unrelated patches in the input space.\nThis is trained with the supervision from aggregated attention scores across\nall transformer layers, which estimate each patch's semantic score. As removing\nsemantic-unrelated patches from the input sequence may disrupt object\nstructure, we replace them with learnable patch embeddings. With initialization\nfrom word embeddings, we can ensure they remain semantically meaningful\nthroughout feature extraction. Extensive experiments on ZSL benchmarks\ndemonstrate that SVIP achieves state-of-the-art performance results while\nproviding more interpretable and semantically rich feature representations.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 10:59:51 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chen', 'Zhi', ''], ['Zhao', 'Zecheng', ''], ['Guo', 'Jingcai', ''], ['Li', 'Jingjing', ''], ['Huang', 'Zi', '']]","extracted_entities":"[{'text': 'Zero-shot learning', 'label': 'Few-shot Learning'}, {'text': 'Semantically contextualized VIsual Patches', 'label': 'contextual Embedding'}, {'text': 'aggregated attention scores', 'label': 'Attention mechanism'}, {'text': 'learnable patch embeddings', 'label': 'contextual Embedding'}, {'text': 'word embeddings', 'label': 'Embedding'}]","assigned_concept":"Few-shot Learning","matched_keyword":"Zero-shot learning","similarity_score":0.8116950989}
{"id":2503.10492,"submitter":"Pranav Vaidhyanathan","authors":"Lucas Schorling, Pranav Vaidhyanathan, Jonas Schuff, Miguel J.\n  Carballido, Dominik Zumb\\\"uhl, Gerard Milburn, Florian Marquardt, Jakob\n  Foerster, Michael A. Osborne, and Natalia Ares","title":"Meta-learning characteristics and dynamics of quantum systems","comments":"6+1 pages, 4 figures. L. Schorling and P. Vaidhyanathan contributed\n  equally to this work","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.mes-hall cs.LG physics.comp-ph","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  While machine learning holds great promise for quantum technologies, most\ncurrent methods focus on predicting or controlling a specific quantum system.\nMeta-learning approaches, however, can adapt to new systems for which little\ndata is available, by leveraging knowledge obtained from previous data\nassociated with similar systems. In this paper, we meta-learn dynamics and\ncharacteristics of closed and open two-level systems, as well as the Heisenberg\nmodel. Based on experimental data of a Loss-DiVincenzo spin-qubit hosted in a\nGe\/Si core\/shell nanowire for different gate voltage configurations, we predict\nqubit characteristics i.e. $g$-factor and Rabi frequency using meta-learning.\nThe algorithm we introduce improves upon previous state-of-the-art\nmeta-learning methods for physics-based systems by introducing novel techniques\nsuch as adaptive learning rates and a global optimizer for improved robustness\nand increased computational efficiency. We benchmark our method against other\nmeta-learning methods, a vanilla transformer, and a multilayer perceptron, and\ndemonstrate improved performance.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 15:56:58 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Schorling', 'Lucas', ''], ['Vaidhyanathan', 'Pranav', ''], ['Schuff', 'Jonas', ''], ['Carballido', 'Miguel J.', ''], ['Zumb\u00fchl', 'Dominik', ''], ['Milburn', 'Gerard', ''], ['Marquardt', 'Florian', ''], ['Foerster', 'Jakob', ''], ['Osborne', 'Michael A.', ''], ['Ares', 'Natalia', '']]","extracted_entities":"[{'text': 'meta-learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"meta-learning","similarity_score":0.5332470536}
{"id":2503.09927,"submitter":"Julia Ive","authors":"Julia Ive and Olatomiwa Olukoya and Jonathan P. Funnell and James\n  Booker and Sze H M Lam and Ugan Reddy and Kawsar Noor and Richard JB Dobson\n  and Astri M.V. Luoma and Hani J Marcus","title":"Developing and Evaluating an AI-Assisted Prediction Model for Unplanned\n  Intensive Care Admissions following Elective Neurosurgery using Natural\n  Language Processing within an Electronic Healthcare Record System","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Introduction: Timely care in a specialised neuro-intensive therapy unit (ITU)\nreduces mortality and hospital stays, with planned admissions being safer than\nunplanned ones. However, post-operative care decisions remain subjective. This\nstudy used artificial intelligence (AI), specifically natural language\nprocessing (NLP) to analyse electronic health records (EHRs) and predict ITU\nadmissions for elective surgery patients. Methods: This study analysed the EHRs\nof elective neurosurgery patients from University College London Hospital\n(UCLH) using NLP. Patients were categorised into planned high dependency unit\n(HDU) or ITU admission; unplanned HDU or ITU admission; or ward \/ overnight\nrecovery (ONR). The Medical Concept Annotation Tool (MedCAT) was used to\nidentify SNOMED-CT concepts within the clinical notes. We then explored the\nutility of these identified concepts for a range of AI algorithms trained to\npredict ITU admission. Results: The CogStack-MedCAT NLP model, initially\ntrained on hospital-wide EHRs, underwent two refinements: first with data from\npatients with Normal Pressure Hydrocephalus (NPH) and then with data from\nVestibular Schwannoma (VS) patients, achieving a concept detection F1-score of\n0.93. This refined model was then used to extract concepts from EHR notes of\n2,268 eligible neurosurgical patients. We integrated the extracted concepts\ninto AI models, including a decision tree model and a neural time-series model.\nUsing the simpler decision tree model, we achieved a recall of 0.87 (CI 0.82 -\n0.91) for ITU admissions, reducing the proportion of unplanned ITU cases missed\nby human experts from 36% to 4%. Conclusion: The NLP model, refined for\naccuracy, has proven its efficiency in extracting relevant concepts, providing\na reliable basis for predictive AI models to use in clinically valid\napplications.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 00:48:48 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Ive', 'Julia', ''], ['Olukoya', 'Olatomiwa', ''], ['Funnell', 'Jonathan P.', ''], ['Booker', 'James', ''], ['Lam', 'Sze H M', ''], ['Reddy', 'Ugan', ''], ['Noor', 'Kawsar', ''], ['Dobson', 'Richard JB', ''], ['Luoma', 'Astri M. V.', ''], ['Marcus', 'Hani J', '']]","extracted_entities":"[{'text': 'decision tree model', 'label': 'Neural Language Model'}, {'text': 'neural time-series model', 'label': 'Neural Language Model'}, {'text': 'decision tree model', 'label': 'Neural Language Model'}]","assigned_concept":"Neural Language Model","matched_keyword":"neural time-series model","similarity_score":0.6045396328}
{"id":2503.10312,"submitter":"Theo Di Piazza","authors":"Theo Di Piazza and Loic Boussel","title":"PS3C: An Ensemble-Based Two-Step Framework for Classification of Pep\n  Smear Cell Images","comments":"7 pages, 3 figures, Grand Challenge paper accepted at ISBI 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Early detection of cervical cancer is crucial for improving patient outcomes\nand reducing mortality by identifying precancerous lesions as soon as possible.\nAs a result, the use of pap smear screening has significantly increased,\nleading to a growing demand for automated tools that can assist cytologists\nmanaging their rising workload. To address this, the Pep Smear Cell\nClassification Challenge (PS3C) has been organized in association with ISBI in\n2025. This project aims to promote the development of automated tools for pep\nsmear images classification. The analyzed images are grouped into four\ncategories: healthy, unhealthy, both, and rubbish images which are considered\nas unsuitable for diagnosis. In this work, we propose a two-stage ensemble\napproach: first, a neural network determines whether an image is rubbish or\nnot. If not, a second neural network classifies the image as containing a\nhealthy cell, an unhealthy cell, or both.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 12:46:23 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Di Piazza', 'Theo', ''], ['Boussel', 'Loic', '']]","extracted_entities":"[{'text': 'neural network', 'label': 'Neural Language Model'}, {'text': 'neural network', 'label': 'Neural Language Model'}]","assigned_concept":"Neural Language Model","matched_keyword":"neural network","similarity_score":0.5788917542}
{"id":2503.02376,"submitter":"Guo Chen","authors":"Guo Chen, Ling Lin, Chengfeng Zhang, Jie Zhang, Gilles Frapper, and\n  Xianlong Wang","title":"Unsaturated Dinitrogen Difluoride under Pressure: toward high-Energy\n  Density Polymerized NF Chains","comments":"High-energy-density material, First-principles calculation, Ab initio\n  molecular dynamics, NF compound","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci physics.chem-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Based on first-principles calculations and ab initio molecular dynamics\nsimulations, the polymerisation of the unsaturated cis dinitrogen-difluoride\n(cis-N2F2) molecular compound is investigated. The thermodynamic, dynamical and\nthermal stabilities of the nitrogen fluorine NF system are investigated at\nconditions of 0-3000 K and 0-200 GPa. The cis-N2F2 molecule is a suitable\nprecursor to obtain one-dimensional polymerized nitrogen-fluorine (poly-NF)\nchains at a pressure above 90 GPa and at a temperature around 1900 K.\nImportantly, these poly-NF chains can be quenched to room conditions, and\npotentially serve as a High-energy-density materials (HEDM). It has been\nestablished that when Al is utilised as a reducing agent, poly-NF chains\nexhibit a gravimetric energy density of 13.55 kJ\/g, which exceeds that of cubic\ngauche nitrogen (cg-N, 9.70 kJ\/g). This is attributable to the presence of both\npolymerised nitrogen and strong oxidising F atoms.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 08:05:17 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 05:40:22 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chen', 'Guo', ''], ['Lin', 'Ling', ''], ['Zhang', 'Chengfeng', ''], ['Zhang', 'Jie', ''], ['Frapper', 'Gilles', ''], ['Wang', 'Xianlong', '']]","extracted_entities":"[{'text': 'Al', 'label': 'ALBERT'}]","assigned_concept":"ALBERT","matched_keyword":"Al","similarity_score":0.5163906813}
{"id":2503.09947,"submitter":"Xiaobo Xia","authors":"Xiaobo Xia, Xiaofeng Liu, Jiale Liu, Kuai Fang, Lu Lu, Samet Oymak,\n  William S. Currie, Tongliang Liu","title":"Identifying Trustworthiness Challenges in Deep Learning Models for\n  Continental-Scale Water Quality Prediction","comments":"33 pages, 9 figures, 2 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Water quality is foundational to environmental sustainability, ecosystem\nresilience, and public health. Deep learning models, particularly Long\nShort-Term Memory (LSTM) networks, offer transformative potential for\nlarge-scale water quality prediction and scientific insights generation.\nHowever, their widespread adoption in high-stakes decision-making, such as\npollution mitigation and equitable resource allocation, is prevented by\nunresolved trustworthiness challenges including fairness, uncertainty,\ninterpretability, robustness, generalizability, and reproducibility. In this\nwork, we present the first comprehensive evaluation of trustworthiness in a\ncontinental-scale multi-task LSTM model predicting 20 water quality variables\n(encompassing physical\/chemical processes, geochemical weathering, and nutrient\ncycling) across 482 U.S. basins. Our investigation uncovers systematic patterns\nof model performance disparities linked to basin characteristics, the inherent\ncomplexity of biogeochemical processes, and variable predictability,\nemphasizing critical performance fairness concerns. We further propose\nmethodological frameworks for quantitatively evaluating critical aspects of\ntrustworthiness, including uncertainty, interpretability, and robustness,\nidentifying key limitations that could challenge reliable real-world\ndeployment. This work serves as a timely call to action for advancing\ntrustworthy data-driven methods for water resources management and provides a\npathway to offering critical insights for researchers, decision-makers, and\npractitioners seeking to leverage artificial intelligence (AI) responsibly in\nenvironmental management.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 01:50:50 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Xia', 'Xiaobo', ''], ['Liu', 'Xiaofeng', ''], ['Liu', 'Jiale', ''], ['Fang', 'Kuai', ''], ['Lu', 'Lu', ''], ['Oymak', 'Samet', ''], ['Currie', 'William S.', ''], ['Liu', 'Tongliang', '']]","extracted_entities":"[{'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'uncertainty', 'label': 'Model Bias and Fairness'}, {'text': 'interpretability', 'label': 'Model Bias and Fairness'}, {'text': 'robustness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'uncertainty', 'label': 'Model Bias and Fairness'}, {'text': 'interpretability', 'label': 'Model Bias and Fairness'}, {'text': 'robustness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"fairness","similarity_score":0.6551788449}
{"id":2503.10486,"submitter":"Gaurav Kumar Gupta","authors":"Gaurav Kumar Gupta and Pranal Pande","title":"LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3\n  Mini Across Chronic Health Conditions","comments":"12 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) are revolutionizing medical diagnostics by\nenhancing both disease classification and clinical decision-making. In this\nstudy, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek\nR1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We\nassessed their predictive accuracy at both the disease and category levels, as\nwell as the reliability of their confidence scores. DeepSeek R1 achieved a\ndisease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3\nMini, which attained 72% and 75% respectively. Notably, DeepSeek R1\ndemonstrated exceptional performance in Mental Health, Neurological Disorders,\nand Oncology, where it reached 100% accuracy, while O3 Mini excelled in\nAutoimmune Disease classification with 100% accuracy. Both models, however,\nstruggled with Respiratory Disease classification, recording accuracies of only\n40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of\nconfidence scores revealed that DeepSeek R1 provided high-confidence\npredictions in 92% of cases, compared to 68% for O3 Mini. Ethical\nconsiderations regarding bias, model interpretability, and data privacy are\nalso discussed to ensure the responsible integration of LLMs into clinical\npractice. Overall, our findings offer valuable insights into the strengths and\nlimitations of LLM-based diagnostic systems and provide a roadmap for future\nenhancements in AI-driven healthcare.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 15:54:26 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Gupta', 'Gaurav Kumar', ''], ['Pande', 'Pranal', '']]","extracted_entities":"[{'text': 'Ethical\\nconsiderations', 'label': 'AI Ethics'}, {'text': 'bias', 'label': 'Model Bias and Fairness'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"bias","similarity_score":0.6767607331}
{"id":2503.1056,"submitter":"Nicolas Pr\\\"ollochs","authors":"Kirill Solovev, Nicolas Pr\\\"ollochs","title":"References to unbiased sources increase the helpfulness of community\n  fact-checks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Community-based fact-checking is a promising approach to address\nmisinformation on social media at scale. However, an understanding of what\nmakes community-created fact-checks helpful to users is still in its infancy.\nIn this paper, we analyze the determinants of the helpfulness of\ncommunity-created fact-checks. For this purpose, we draw upon a unique dataset\nof real-world community-created fact-checks and helpfulness ratings from X's\n(formerly Twitter) Community Notes platform. Our empirical analysis implies\nthat the key determinant of helpfulness in community-based fact-checking is\nwhether users provide links to external sources to underpin their assertions.\nOn average, the odds for community-created fact-checks to be perceived as\nhelpful are 2.70 times higher if they provide links to external sources.\nFurthermore, we demonstrate that the helpfulness of community-created\nfact-checks varies depending on their level of political bias. Here, we find\nthat community-created fact-checks linking to high-bias sources (of either\npolitical side) are perceived as significantly less helpful. This suggests that\nthe rating mechanism on the Community Notes platform successfully penalizes\none-sidedness and politically motivated reasoning. These findings have\nimportant implications for social media platforms, which can utilize our\nresults to optimize their community-based fact-checking systems.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:12:01 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Solovev', 'Kirill', ''], ['Pr\u00f6llochs', 'Nicolas', '']]","extracted_entities":"[{'text': 'political bias', 'label': 'Model Bias and Fairness'}, {'text': 'rating mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"political bias","similarity_score":0.5630335808}
{"id":2503.10567,"submitter":"Nannan Wu","authors":"Nannan Wu, Zengqiang Yan, Nong Sang, Li Yu, Chang Wen Chen","title":"FedPCA: Noise-Robust Fair Federated Learning via Performance-Capacity\n  Analysis","comments":"Preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Training a model that effectively handles both common and rare data-i.e.,\nachieving performance fairness-is crucial in federated learning (FL). While\nexisting fair FL methods have shown effectiveness, they remain vulnerable to\nmislabeled data. Ensuring robustness in fair FL is therefore essential.\nHowever, fairness and robustness inherently compete, which causes robust\nstrategies to hinder fairness. In this paper, we attribute this competition to\nthe homogeneity in loss patterns exhibited by rare and mislabeled data clients,\npreventing existing loss-based fair and robust FL methods from effectively\ndistinguishing and handling these two distinct client types. To address this,\nwe propose performance-capacity analysis, which jointly considers model\nperformance on each client and its capacity to handle the dataset, measured by\nloss and a newly introduced feature dispersion score. This allows mislabeled\nclients to be identified by their significantly deviated performance relative\nto capacity while preserving rare data clients. Building on this, we introduce\nFedPCA, an FL method that robustly achieves fairness. FedPCA first identifies\nmislabeled clients via a Gaussian Mixture Model on loss-dispersion pairs, then\napplies fairness and robustness strategies in global aggregation and local\ntraining by adjusting client weights and selectively using reliable data.\nExtensive experiments on three datasets demonstrate FedPCA's effectiveness in\ntackling this complex challenge. Code will be publicly available upon\nacceptance.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:18:18 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wu', 'Nannan', ''], ['Yan', 'Zengqiang', ''], ['Sang', 'Nong', ''], ['Yu', 'Li', ''], ['Chen', 'Chang Wen', '']]","extracted_entities":"[{'text': 'fairness-is', 'label': 'Model Bias and Fairness'}, {'text': 'federated learning', 'label': 'Few-shot Learning'}, {'text': 'FL', 'label': 'Few-shot Learning'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"fairness-is","similarity_score":0.7215364575}
{"id":2503.10587,"submitter":"Justin Sahs","authors":"Justin Sahs, Ryan Pyle, Fabio Anselmi, Ankit Patel","title":"The Spectral Bias of Shallow Neural Network Learning is Shaped by the\n  Choice of Non-linearity","comments":"18 pages, 10 figures in main text","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Despite classical statistical theory predicting severe overfitting, modern\nmassively overparameterized neural networks still generalize well. This\nunexpected property is attributed to the network's so-called implicit bias,\nwhich describes its propensity to converge to solutions that generalize\neffectively, among the many possible that correctly label the training data.\nThe aim of our research is to explore this bias from a new perspective,\nfocusing on how non-linear activation functions contribute to shaping it.\nFirst, we introduce a reparameterization which removes a continuous weight\nrescaling symmetry. Second, in the kernel regime, we leverage this\nreparameterization to generalize recent findings that relate shallow Neural\nNetworks to the Radon transform, deriving an explicit formula for the implicit\nbias induced by a broad class of activation functions. Specifically, by\nutilizing the connection between the Radon transform and the Fourier transform,\nwe interpret the kernel regime's inductive bias as minimizing a spectral\nseminorm that penalizes high-frequency components, in a manner dependent on the\nactivation function. Finally, in the adaptive regime, we demonstrate the\nexistence of local dynamical attractors that facilitate the formation of\nclusters of hyperplanes where the input to a neuron's activation function is\nzero, yielding alignment between many neurons' response functions. We confirm\nthese theoretical results with simulations. All together, our work provides a\ndeeper understanding of the mechanisms underlying the generalization\ncapabilities of overparameterized neural networks and its relation with the\nimplicit bias, offering potential pathways for designing more efficient and\nrobust models.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:36:46 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Sahs', 'Justin', ''], ['Pyle', 'Ryan', ''], ['Anselmi', 'Fabio', ''], ['Patel', 'Ankit', '']]","extracted_entities":"[{'text': 'implicit bias', 'label': 'Model Bias and Fairness'}, {'text': 'continuous weight\\nrescaling symmetry', 'label': 'Scaling law'}, {'text': 'Radon transform', 'label': 'BERT'}, {'text': 'implicit\\nbias', 'label': 'Model Bias and Fairness'}, {'text': 'Radon transform', 'label': 'BERT'}, {'text': 'implicit bias', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"implicit bias","similarity_score":0.5583472252}
{"id":2501.14225,"submitter":"Rong Ye","authors":"Rong Ye, Yongxin Zhang, Yikai Zhang, Haoyu Kuang, Zhongyu Wei, Peng\n  Sun","title":"Multi-agent KTO: Reinforcing Strategic Interactions of Large Language\n  Model in Language Game","comments":"Preprint. Code and data will be available at\n  https:\/\/reneeye.github.io\/MaKTO.html","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Achieving Artificial General Intelligence (AGI) requires AI agents that can\nnot only make stratigic decisions but also engage in flexible and meaningful\ncommunication. Inspired by Wittgenstein's language game theory in Philosophical\nInvestigations, we propose that language agents can learn through in-context\ninteraction rather than traditional multi-stage frameworks that separate\ndecision-making from language expression. Using Werewolf, a social deduction\ngame that tests language understanding, strategic interaction, and\nadaptability, we develop the Multi-agent Kahneman & Tversky's Optimization\n(MaKTO). MaKTO engages diverse models in extensive gameplay to generate\nunpaired desirable and unacceptable responses, then employs KTO to refine the\nmodel's decision-making process. In 9-player Werewolf games, MaKTO achieves a\n61% average win rate across various models, outperforming GPT-4o and two-stage\nRL agents by relative improvements of 23.0% and 10.9%, respectively. Notably,\nMaKTO also demonstrates human-like performance, winning 60% against expert\nplayers and showing only 49% detectability in Turing-style blind tests.\n","versions":"[{'version': 'v1', 'created': 'Fri, 24 Jan 2025 04:09:03 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 03:55:17 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Ye', 'Rong', ''], ['Zhang', 'Yongxin', ''], ['Zhang', 'Yikai', ''], ['Kuang', 'Haoyu', ''], ['Wei', 'Zhongyu', ''], ['Sun', 'Peng', '']]","extracted_entities":"[{'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2502.18485,"submitter":"Jiaqi Xu","authors":"Jiaqi Xu, Cuiling Lan, Xuejin Chen, Yan Lu","title":"Deciphering Functions of Neurons in Vision-Language Models","comments":"22 pages, 23 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"q-bio.NC cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The burgeoning growth of open-sourced vision-language models (VLMs) has\ncatalyzed a plethora of applications across diverse domains. Ensuring the\ntransparency and interpretability of these models is critical for fostering\ntrustworthy and responsible AI systems. In this study, our objective is to\ndelve into the internals of VLMs to interpret the functions of individual\nneurons. We observe the activations of neurons with respects to the input\nvisual tokens and text tokens, and reveal some interesting findings.\nParticularly, we found that there are neurons responsible for only visual or\ntext information, or both, respectively, which we refer to them as visual\nneurons, text neurons, and multi-modal neurons, respectively. We build a\nframework that automates the explanation of neurons with the assistant of\nGPT-4o. Meanwhile, for visual neurons, we propose an activation simulator to\nassess the reliability of the explanations for visual neurons. System\nstatistical analyses on top of one representative VLM of LLaVA, uncover the\nbehaviors\/characteristics of different categories of neurons.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Feb 2025 10:00:06 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Feb 2025 06:32:05 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 07:13:38 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Xu', 'Jiaqi', ''], ['Lan', 'Cuiling', ''], ['Chen', 'Xuejin', ''], ['Lu', 'Yan', '']]","extracted_entities":"[{'text': 'VLMs', 'label': 'Open-source LLMs'}, {'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2503.08481,"submitter":"Weijie Zhou","authors":"Weijie Zhou, Manli Tao, Chaoyang Zhao, Haiyun Guo, Honghui Dong, Ming\n  Tang, Jinqiao Wang","title":"PhysVLM: Enabling Visual Language Models to Understand Robotic Physical\n  Reachability","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Understanding the environment and a robot's physical reachability is crucial\nfor task execution. While state-of-the-art vision-language models (VLMs) excel\nin environmental perception, they often generate inaccurate or impractical\nresponses in embodied visual reasoning tasks due to a lack of understanding of\nrobotic physical reachability. To address this issue, we propose a unified\nrepresentation of physical reachability across diverse robots, i.e.,\nSpace-Physical Reachability Map (S-P Map), and PhysVLM, a vision-language model\nthat integrates this reachability information into visual reasoning.\nSpecifically, the S-P Map abstracts a robot's physical reachability into a\ngeneralized spatial representation, independent of specific robot\nconfigurations, allowing the model to focus on reachability features rather\nthan robot-specific parameters. Subsequently, PhysVLM extends traditional VLM\narchitectures by incorporating an additional feature encoder to process the S-P\nMap, enabling the model to reason about physical reachability without\ncompromising its general vision-language capabilities. To train and evaluate\nPhysVLM, we constructed a large-scale multi-robot dataset, Phys100K, and a\nchallenging benchmark, EQA-phys, which includes tasks for six different robots\nin both simulated and real-world environments. Experimental results demonstrate\nthat PhysVLM outperforms existing models, achieving a 14\\% improvement over\nGPT-4o on EQA-phys and surpassing advanced embodied VLMs such as RoboMamba and\nSpatialVLM on the RoboVQA-val and OpenEQA benchmarks. Additionally, the S-P Map\nshows strong compatibility with various VLMs, and its integration into\nGPT-4o-mini yields a 7.1\\% performance improvement.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 14:34:41 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 11:19:12 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhou', 'Weijie', ''], ['Tao', 'Manli', ''], ['Zhao', 'Chaoyang', ''], ['Guo', 'Haiyun', ''], ['Dong', 'Honghui', ''], ['Tang', 'Ming', ''], ['Wang', 'Jinqiao', '']]","extracted_entities":"[{'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'GPT-4o-mini', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2503.1048,"submitter":"Siyin Wang","authors":"Siyin Wang, Zhaoye Fei, Qinyuan Cheng, Shiduo Zhang, Panpan Cai,\n  Jinlan Fu, Xipeng Qiu","title":"World Modeling Makes a Better Planner: Dual Preference Optimization for\n  Embodied Task Planning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CV cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advances in large vision-language models (LVLMs) have shown promise\nfor embodied task planning, yet they struggle with fundamental challenges like\ndependency constraints and efficiency. Existing approaches either solely\noptimize action selection or leverage world models during inference,\noverlooking the benefits of learning to model the world as a way to enhance\nplanning capabilities. We propose Dual Preference Optimization (D$^2$PO), a new\nlearning framework that jointly optimizes state prediction and action selection\nthrough preference learning, enabling LVLMs to understand environment dynamics\nfor better planning. To automatically collect trajectories and stepwise\npreference data without human annotation, we introduce a tree search mechanism\nfor extensive exploration via trial-and-error. Extensive experiments on\nVoTa-Bench demonstrate that our D$^2$PO-based method significantly outperforms\nexisting methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and\nLLaMA-3.2 (11B), achieving superior task success rates with more efficient\nexecution paths.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 15:49:56 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Siyin', ''], ['Fei', 'Zhaoye', ''], ['Cheng', 'Qinyuan', ''], ['Zhang', 'Shiduo', ''], ['Cai', 'Panpan', ''], ['Fu', 'Jinlan', ''], ['Qiu', 'Xipeng', '']]","extracted_entities":"[{'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'preference learning', 'label': 'Few-shot Learning'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2503.10619,"submitter":"Andy Zhou","authors":"Andy Zhou","title":"Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with\n  Tree Search","comments":"Accepted to ICLR 2025 Trustworthy LLM","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.CR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce Siege, a multi-turn adversarial framework that models the\ngradual erosion of Large Language Model (LLM) safety through a tree search\nperspective. Unlike single-turn jailbreaks that rely on one meticulously\nengineered prompt, Siege expands the conversation at each turn in a\nbreadth-first fashion, branching out multiple adversarial prompts that exploit\npartial compliance from previous responses. By tracking these incremental\npolicy leaks and re-injecting them into subsequent queries, Siege reveals how\nminor concessions can accumulate into fully disallowed outputs. Evaluations on\nthe JailbreakBench dataset show that Siege achieves a 100% success rate on\nGPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries\nthan baselines such as Crescendo or GOAT. This tree search methodology offers\nan in-depth view of how model safeguards degrade over successive dialogue\nturns, underscoring the urgency of robust multi-turn testing procedures for\nlanguage models.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:57:32 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhou', 'Andy', '']]","extracted_entities":"[{'text': 'adversarial prompts', 'label': 'Prompting'}, {'text': 'GPT-3', 'label': 'GPT'}, {'text': 'GPT-4', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-3","similarity_score":0.8771116138}
{"id":2312.1596,"submitter":"Jingyao Li","authors":"Jingyao Li, Pengguang Chen, Bin Xia, Hong Xu, Jiaya Jia","title":"MoTCoder: Elevating Large Language Models with Modular of Thought for\n  Challenging Programming Tasks","comments":"Model: https:\/\/huggingface.co\/JingyaoLi\/MoTCoder-15B-v1.0. Code:\n  https:\/\/github.com\/dvlab-research\/MoTCoder","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.PL cs.SE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) have showcased impressive capabilities in\nhandling straightforward programming tasks. However, their performance tends to\nfalter when confronted with more challenging programming problems. We observe\nthat conventional models often generate solutions as monolithic code blocks,\nrestricting their effectiveness in tackling intricate questions. To overcome\nthis limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a\npioneering framework for MoT instruction tuning, designed to promote the\ndecomposition of tasks into logical sub-tasks and sub-modules. Our\ninvestigations reveal that, through the cultivation and utilization of\nsub-modules, MoTCoder significantly improves both the modularity and\ncorrectness of the generated solutions, leading to substantial relative pass@1\nimprovements of 12.9% on APPS and 9.43% on CodeContests. Our codes are\navailable at https:\/\/github.com\/dvlab-research\/MoTCoder.\n","versions":"[{'version': 'v1', 'created': 'Tue, 26 Dec 2023 08:49:57 GMT'}, {'version': 'v2', 'created': 'Fri, 5 Jan 2024 10:33:32 GMT'}, {'version': 'v3', 'created': 'Thu, 22 Aug 2024 06:24:12 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 05:36:12 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Jingyao', ''], ['Chen', 'Pengguang', ''], ['Xia', 'Bin', ''], ['Xu', 'Hong', ''], ['Jia', 'Jiaya', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2402.04863,"submitter":"Yingjie Mao","authors":"Xiaoqi Li, Yingjie Mao, Zexin Lu, Wenkai Li, Zongwei Li","title":"SCLA: Automated Smart Contract Summarization via LLMs and Control Flow\n  Prompt","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Smart contract code summarization is crucial for efficient maintenance and\nvulnerability mitigation. While many studies use Large Language Models (LLMs)\nfor summarization, their performance still falls short compared to fine-tuned\nmodels like CodeT5+ and CodeBERT. Some approaches combine LLMs with data flow\nanalysis but fail to fully capture the hierarchy and control structures of the\ncode, leading to information loss and degraded summarization quality. We\npropose SCLA, an LLM-based method that enhances summarization by integrating a\nControl Flow Graph (CFG) and semantic facts from the code's control flow into a\nsemantically enriched prompt. SCLA uses a control flow extraction algorithm to\nderive control flows from semantic nodes in the Abstract Syntax Tree (AST) and\nconstructs the corresponding CFG. Code semantic facts refer to both explicit\nand implicit information within the AST that is relevant to smart contracts.\nThis method enables LLMs to better capture the structural and contextual\ndependencies of the code. We validate the effectiveness of SCLA through\ncomprehensive experiments on a dataset of 40,000 real-world smart contracts.\nThe experiment shows that SCLA significantly improves summarization quality,\noutperforming the SOTA baselines with improvements of 26.7%, 23.2%, 16.7%, and\n14.7% in BLEU-4, METEOR, ROUGE-L, and BLEURT scores, respectively.\n","versions":"[{'version': 'v1', 'created': 'Wed, 7 Feb 2024 13:58:26 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Feb 2024 06:09:16 GMT'}, {'version': 'v3', 'created': 'Wed, 21 Feb 2024 14:18:32 GMT'}, {'version': 'v4', 'created': 'Sat, 17 Aug 2024 03:41:42 GMT'}, {'version': 'v5', 'created': 'Tue, 20 Aug 2024 02:34:56 GMT'}, {'version': 'v6', 'created': 'Thu, 13 Mar 2025 07:05:15 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Xiaoqi', ''], ['Mao', 'Yingjie', ''], ['Lu', 'Zexin', ''], ['Li', 'Wenkai', ''], ['Li', 'Zongwei', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'CodeT5+', 'label': 'Transformer-based model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'SCLA', 'label': 'LLM-based'}, {'text': 'semantically enriched prompt', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'SCLA', 'label': 'LLM-based'}, {'text': 'SCLA', 'label': 'LLM-based'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2406.04443,"submitter":"Eduard Gorbunov","authors":"Savelii Chezhegov, Yaroslav Klyukin, Andrei Semenov, Aleksandr\n  Beznosikov, Alexander Gasnikov, Samuel Horv\\'ath, Martin Tak\\'a\\v{c}, Eduard\n  Gorbunov","title":"Clipping Improves Adam-Norm and AdaGrad-Norm when the Noise Is\n  Heavy-Tailed","comments":"63 pages, 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG math.OC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Methods with adaptive stepsizes, such as AdaGrad and Adam, are essential for\ntraining modern Deep Learning models, especially Large Language Models.\nTypically, the noise in the stochastic gradients is heavy-tailed for the later\nones. Gradient clipping provably helps to achieve good high-probability\nconvergence for such noises. However, despite the similarity between\nAdaGrad\/Adam and Clip-SGD, the current understanding of the high-probability\nconvergence of AdaGrad\/Adam-type methods is limited in this case. In this work,\nwe prove that AdaGrad\/Adam (and their delayed version) can have provably bad\nhigh-probability convergence if the noise is heavy-tailed. We also show that\ngradient clipping fixes this issue, i.e., we derive new high-probability\nconvergence bounds with polylogarithmic dependence on the confidence level for\nAdaGrad-Norm and Adam-Norm with clipping and with\/without delay for smooth\nconvex\/non-convex stochastic optimization with heavy-tailed noise. Our\nempirical evaluations highlight the superiority of clipped versions of\nAdaGrad\/Adam-Norm in handling the heavy-tailed noise.\n","versions":"[{'version': 'v1', 'created': 'Thu, 6 Jun 2024 18:49:10 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 10:26:57 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chezhegov', 'Savelii', ''], ['Klyukin', 'Yaroslav', ''], ['Semenov', 'Andrei', ''], ['Beznosikov', 'Aleksandr', ''], ['Gasnikov', 'Alexander', ''], ['Horv\u00e1th', 'Samuel', ''], ['Tak\u00e1\u010d', 'Martin', ''], ['Gorbunov', 'Eduard', '']]","extracted_entities":"[{'text': 'Adam', 'label': 'ALBERT'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'Adam', 'label': 'ALBERT'}, {'text': 'Adam', 'label': 'ALBERT'}, {'text': 'Adam-Norm', 'label': 'ALBERT'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2406.08426,"submitter":"Zijin Hong","authors":"Zijin Hong, Zheng Yuan, Qinggang Zhang, Hao Chen, Junnan Dong, Feiran\n  Huang, Xiao Huang","title":"Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.DB","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Generating accurate SQL from users' natural language questions (text-to-SQL)\nremains a long-standing challenge due to the complexities involved in user\nquestion understanding, database schema comprehension, and SQL generation.\nTraditional text-to-SQL systems, which combine human engineering and deep\nneural networks, have made significant progress. Subsequently, pre-trained\nlanguage models (PLMs) have been developed for text-to-SQL tasks, achieving\npromising results. However, as modern databases and user questions grow more\ncomplex, PLMs with a limited parameter size often produce incorrect SQL. This\nnecessitates more sophisticated and tailored optimization methods, which\nrestricts the application of PLM-based systems. Recently, large language models\n(LLMs) have shown significant capabilities in natural language understanding as\nmodel scale increases. Thus, integrating LLM-based solutions can bring unique\nopportunities, improvements, and solutions to text-to-SQL research. In this\nsurvey, we provide a comprehensive review of existing LLM-based text-to-SQL\nstudies. Specifically, we offer a brief overview of the technical challenges\nand evolutionary process of text-to-SQL. Next, we introduce the datasets and\nmetrics designed to evaluate text-to-SQL systems. Subsequently, we present a\nsystematic analysis of recent advances in LLM-based text-to-SQL. Finally, we\nmake a summarization and discuss the remaining challenges in this field and\nsuggest expectations for future research directions.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Jun 2024 17:13:17 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Jun 2024 13:51:30 GMT'}, {'version': 'v3', 'created': 'Tue, 16 Jul 2024 08:06:57 GMT'}, {'version': 'v4', 'created': 'Sun, 23 Feb 2025 22:22:20 GMT'}, {'version': 'v5', 'created': 'Thu, 13 Mar 2025 08:45:35 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Hong', 'Zijin', ''], ['Yuan', 'Zheng', ''], ['Zhang', 'Qinggang', ''], ['Chen', 'Hao', ''], ['Dong', 'Junnan', ''], ['Huang', 'Feiran', ''], ['Huang', 'Xiao', '']]","extracted_entities":"[{'text': 'PLMs', 'label': 'Large Language Model'}, {'text': 'PLMs', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2409.18042,"submitter":"Kai Chen","authors":"Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu,\n  Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, Dingdong Wang, Kun Xiang, Haoyuan\n  Li, Haoli Bai, Jianhua Han, Xiaohui Li, Weike Jin, Nian Xie, Yu Zhang, James\n  T. Kwok, Hengshuang Zhao, Xiaodan Liang, Dit-Yan Yeung, Xiao Chen, Zhenguo\n  Li, Wei Zhang, Qun Liu, Jun Yao, Lanqing Hong, Lu Hou, Hang Xu","title":"EMOVA: Empowering Language Models to See, Hear and Speak with Vivid\n  Emotions","comments":"Accepted by CVPR 2025. Project Page: https:\/\/emova-ollm.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  GPT-4o, an omni-modal model that enables vocal conversations with diverse\nemotions and tones, marks a milestone for omni-modal foundation models.\nHowever, empowering Large Language Models to perceive and generate images,\ntexts, and speeches end-to-end with publicly available data remains challenging\nfor the open-source community. Existing vision-language models rely on external\ntools for speech processing, while speech-language models still suffer from\nlimited or totally without vision-understanding capabilities. To address this\ngap, we propose the EMOVA (EMotionally Omni-present Voice Assistant), to enable\nLarge Language Models with end-to-end speech abilities while maintaining the\nleading vision-language performance. With a semantic-acoustic disentangled\nspeech tokenizer, we surprisingly notice that omni-modal alignment can further\nenhance vision-language and speech abilities compared with the bi-modal aligned\ncounterparts. Moreover, a lightweight style module is introduced for the\nflexible speech style controls including emotions and pitches. For the first\ntime, EMOVA achieves state-of-the-art performance on both the vision-language\nand speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue\nwith vivid emotions.\n","versions":"[{'version': 'v1', 'created': 'Thu, 26 Sep 2024 16:44:02 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Oct 2024 06:25:52 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 14:51:04 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chen', 'Kai', ''], ['Gou', 'Yunhao', ''], ['Huang', 'Runhui', ''], ['Liu', 'Zhili', ''], ['Tan', 'Daxin', ''], ['Xu', 'Jing', ''], ['Wang', 'Chunwei', ''], ['Zhu', 'Yi', ''], ['Zeng', 'Yihan', ''], ['Yang', 'Kuo', ''], ['Wang', 'Dingdong', ''], ['Xiang', 'Kun', ''], ['Li', 'Haoyuan', ''], ['Bai', 'Haoli', ''], ['Han', 'Jianhua', ''], ['Li', 'Xiaohui', ''], ['Jin', 'Weike', ''], ['Xie', 'Nian', ''], ['Zhang', 'Yu', ''], ['Kwok', 'James T.', ''], ['Zhao', 'Hengshuang', ''], ['Liang', 'Xiaodan', ''], ['Yeung', 'Dit-Yan', ''], ['Chen', 'Xiao', ''], ['Li', 'Zhenguo', ''], ['Zhang', 'Wei', ''], ['Liu', 'Qun', ''], ['Yao', 'Jun', ''], ['Hong', 'Lanqing', ''], ['Hou', 'Lu', ''], ['Xu', 'Hang', '']]","extracted_entities":"[{'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'open-source community', 'label': 'Open-source LLMs'}, {'text': 'vision-language models', 'label': 'Large Language Model'}, {'text': 'speech-language models', 'label': 'Large Language Model'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2410.00263,"submitter":"Kun Yuan","authors":"Kun Yuan, Vinkle Srivastav, Nassir Navab, Nicolas Padoy","title":"Procedure-Aware Surgical Video-language Pretraining with Hierarchical\n  Knowledge Augmentation","comments":"Accepted at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024 Spolight)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Surgical video-language pretraining (VLP) faces unique challenges due to the\nknowledge domain gap and the scarcity of multi-modal data. This study aims to\nbridge the gap by addressing issues regarding textual information loss in\nsurgical lecture videos and the spatial-temporal challenges of surgical VLP. We\npropose a hierarchical knowledge augmentation approach and a novel\nProcedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining\n(PeskaVLP) framework to tackle these issues. The knowledge augmentation uses\nlarge language models (LLM) for refining and enriching surgical concepts, thus\nproviding comprehensive language supervision and reducing the risk of\noverfitting. PeskaVLP combines language supervision with visual\nself-supervision, constructing hard negative samples and employing a Dynamic\nTime Warping (DTW) based loss function to effectively comprehend the\ncross-modal procedural alignment. Extensive experiments on multiple public\nsurgical scene understanding and cross-modal retrieval datasets show that our\nproposed method significantly improves zero-shot transferring performance and\noffers a generalist visual representation for further advancements in surgical\nscene understanding.The code is available at\nhttps:\/\/github.com\/CAMMA-public\/SurgVLP\n","versions":"[{'version': 'v1', 'created': 'Mon, 30 Sep 2024 22:21:05 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 15:21:36 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Yuan', 'Kun', ''], ['Srivastav', 'Vinkle', ''], ['Navab', 'Nassir', ''], ['Padoy', 'Nicolas', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2410.01727,"submitter":"Yilmazcan Ozyurt","authors":"Yilmazcan Ozyurt, Stefan Feuerriegel, Mrinmaya Sachan","title":"Automated Knowledge Concept Annotation and Question Representation\n  Learning for Knowledge Tracing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Knowledge tracing (KT) is a popular approach for modeling students' learning\nprogress over time, which can enable more personalized and adaptive learning.\nHowever, existing KT approaches face two major limitations: (1) they rely\nheavily on expert-defined knowledge concepts (KCs) in questions, which is\ntime-consuming and prone to errors; and (2) KT methods tend to overlook the\nsemantics of both questions and the given KCs. In this work, we address these\nchallenges and present KCQRL, a framework for automated knowledge concept\nannotation and question representation learning that can improve the\neffectiveness of any existing KT model. First, we propose an automated KC\nannotation process using large language models (LLMs), which generates question\nsolutions and then annotates KCs in each solution step of the questions.\nSecond, we introduce a contrastive learning approach to generate semantically\nrich embeddings for questions and solution steps, aligning them with their\nassociated KCs via a tailored false negative elimination approach. These\nembeddings can be readily integrated into existing KT models, replacing their\nrandomly initialized embeddings. We demonstrate the effectiveness of KCQRL\nacross 15 KT algorithms on two large real-world Math learning datasets, where\nwe achieve consistent performance improvements.\n","versions":"[{'version': 'v1', 'created': 'Wed, 2 Oct 2024 16:37:19 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 13:09:14 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Ozyurt', 'Yilmazcan', ''], ['Feuerriegel', 'Stefan', ''], ['Sachan', 'Mrinmaya', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2410.04759,"submitter":"Yifan Liu","authors":"Tianhui Cai, Yifan Liu, Zewei Zhou, Haoxuan Ma, Seth Z. Zhao, Zhiwen\n  Wu and Jiaqi Ma","title":"Driving with Regulation: Interpretable Decision-Making for Autonomous\n  Vehicles with Retrieval-Augmented Reasoning via LLM","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This work presents an interpretable decision-making framework for autonomous\nvehicles that integrates traffic regulations, norms, and safety guidelines\ncomprehensively and enables seamless adaptation to different regions. While\ntraditional rule-based methods struggle to incorporate the full scope of\ntraffic rules, we develop a Traffic Regulation Retrieval (TRR) Agent based on\nRetrieval-Augmented Generation (RAG) to automatically retrieve relevant traffic\nrules and guidelines from extensive regulation documents and relevant records\nbased on the ego vehicle's situation. Given the semantic complexity of the\nretrieved rules, we also design a reasoning module powered by a Large Language\nModel (LLM) to interpret these rules, differentiate between mandatory rules and\nsafety guidelines, and assess actions on legal compliance and safety.\nAdditionally, the reasoning is designed to be interpretable, enhancing both\ntransparency and reliability. The framework demonstrates robust performance on\nboth hypothesized and real-world cases across diverse scenarios, along with the\nability to adapt to different regions with ease.\n","versions":"[{'version': 'v1', 'created': 'Mon, 7 Oct 2024 05:27:22 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 04:00:16 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Cai', 'Tianhui', ''], ['Liu', 'Yifan', ''], ['Zhou', 'Zewei', ''], ['Ma', 'Haoxuan', ''], ['Zhao', 'Seth Z.', ''], ['Wu', 'Zhiwen', ''], ['Ma', 'Jiaqi', '']]","extracted_entities":"[{'text': 'Large Language\\nModel', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language\nModel","similarity_score":1.0}
{"id":2411.00915,"submitter":"Liang Mi","authors":"Liang Mi, Weijun Wang, Wenming Tu, Qingfeng He, Rui Kong, Xinyu Fang,\n  Yazhu Dong, Yikang Zhang, Yunchun Li, Meng Li, Haipeng Dai, Guihai Chen,\n  Yunxin Liu","title":"V-LoRA: An Efficient and Flexible System Boosts Vision Applications with\n  LoRA LMM","comments":"EuroSys'2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Multimodal Models (LMMs) have shown significant progress in various\ncomplex vision tasks with the solid linguistic and reasoning capacity inherited\nfrom large language models (LMMs). Low-rank adaptation (LoRA) offers a\npromising method to integrate external knowledge into LMMs, compensating for\ntheir limitations on domain-specific tasks. However, the existing LoRA model\nserving is excessively computationally expensive and causes extremely high\nlatency. In this paper, we present an end-to-end solution that empowers diverse\nvision tasks and enriches vision applications with LoRA LMMs. Our system,\nVaLoRA, enables accurate and efficient vision tasks by 1) an accuracy-aware\nLoRA adapter generation approach that generates LoRA adapters rich in\ndomain-specific knowledge to meet application-specific accuracy requirements,\n2) an adaptive-tiling LoRA adapters batching operator that efficiently computes\nconcurrent heterogeneous LoRA adapters, and 3) a flexible LoRA adapter\norchestration mechanism that manages application requests and LoRA adapters to\nachieve the lowest average response latency. We prototype VaLoRA on five\npopular vision tasks on three LMMs. Experiment results reveal that VaLoRA\nimproves 24-62% of the accuracy compared to the original LMMs and reduces\n20-89% of the latency compared to the state-of-the-art LoRA model serving\nsystems.\n","versions":"[{'version': 'v1', 'created': 'Fri, 1 Nov 2024 13:43:33 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Feb 2025 05:57:42 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 13:26:38 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 08:38:15 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Mi', 'Liang', ''], ['Wang', 'Weijun', ''], ['Tu', 'Wenming', ''], ['He', 'Qingfeng', ''], ['Kong', 'Rui', ''], ['Fang', 'Xinyu', ''], ['Dong', 'Yazhu', ''], ['Zhang', 'Yikang', ''], ['Li', 'Yunchun', ''], ['Li', 'Meng', ''], ['Dai', 'Haipeng', ''], ['Chen', 'Guihai', ''], ['Liu', 'Yunxin', '']]","extracted_entities":"[{'text': 'Large Multimodal Models', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Multimodal Models","similarity_score":0.5739125013}
{"id":2412.16833,"submitter":"Kaiwen Zuo","authors":"Kaiwen Zuo, Yirui Jiang, Fan Mo, Pietro Lio","title":"KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge\n  Graph Enhancement for Medical Diagnosis","comments":"10 pages,5 figures,published to AAAI-25 Bridge Program","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Integrating Large Language Models (LLMs) in healthcare diagnosis demands\nsystematic frameworks that can handle complex medical scenarios while\nmaintaining specialized expertise. We present KG4Diagnosis, a novel\nhierarchical multi-agent framework that combines LLMs with automated knowledge\ngraph construction, encompassing 362 common diseases across medical\nspecialties. Our framework mirrors real-world medical systems through a\ntwo-tier architecture: a general practitioner (GP) agent for initial assessment\nand triage, coordinating with specialized agents for in-depth diagnosis in\nspecific domains. The core innovation lies in our end-to-end knowledge graph\ngeneration methodology, incorporating: (1) semantic-driven entity and relation\nextraction optimized for medical terminology, (2) multi-dimensional decision\nrelationship reconstruction from unstructured medical texts, and (3)\nhuman-guided reasoning for knowledge expansion. KG4Diagnosis serves as an\nextensible foundation for specialized medical diagnosis systems, with\ncapabilities to incorporate new diseases and medical knowledge. The framework's\nmodular design enables seamless integration of domain-specific enhancements,\nmaking it valuable for developing targeted medical diagnosis systems. We\nprovide architectural guidelines and protocols to facilitate adoption across\nmedical contexts.\n","versions":"[{'version': 'v1', 'created': 'Sun, 22 Dec 2024 02:40:59 GMT'}, {'version': 'v2', 'created': 'Fri, 3 Jan 2025 00:07:09 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 03:05:30 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zuo', 'Kaiwen', ''], ['Jiang', 'Yirui', ''], ['Mo', 'Fan', ''], ['Lio', 'Pietro', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'KG4Diagnosis', 'label': 'Foundation Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'KG4Diagnosis', 'label': 'Foundation Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2412.18947,"submitter":"Kaiwen Zuo","authors":"Kaiwen Zuo, Yirui Jiang","title":"MedHallBench: A New Benchmark for Assessing Hallucination in Medical\n  Large Language Models","comments":"Published to AAAI-25 Bridge Program","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Medical Large Language Models (MLLMs) have demonstrated potential in\nhealthcare applications, yet their propensity for hallucinations -- generating\nmedically implausible or inaccurate information -- presents substantial risks\nto patient care. This paper introduces MedHallBench, a comprehensive benchmark\nframework for evaluating and mitigating hallucinations in MLLMs. Our\nmethodology integrates expert-validated medical case scenarios with established\nmedical databases to create a robust evaluation dataset. The framework employs\na sophisticated measurement system that combines automated ACHMI (Automatic\nCaption Hallucination Measurement in Medical Imaging) scoring with rigorous\nclinical expert evaluations and utilizes reinforcement learning methods to\nachieve automatic annotation. Through an optimized reinforcement learning from\nhuman feedback (RLHF) training pipeline specifically designed for medical\napplications, MedHallBench enables thorough evaluation of MLLMs across diverse\nclinical contexts while maintaining stringent accuracy standards. We conducted\ncomparative experiments involving various models, utilizing the benchmark to\nestablish a baseline for widely adopted large language models (LLMs). Our\nfindings indicate that ACHMI provides a more nuanced understanding of the\neffects of hallucinations compared to traditional metrics, thereby highlighting\nits advantages in hallucination assessment. This research establishes a\nfoundational framework for enhancing MLLMs' reliability in healthcare settings\nand presents actionable strategies for addressing the critical challenge of AI\nhallucinations in medical applications.\n","versions":"[{'version': 'v1', 'created': 'Wed, 25 Dec 2024 16:51:29 GMT'}, {'version': 'v2', 'created': 'Fri, 3 Jan 2025 00:16:52 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 02:29:47 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zuo', 'Kaiwen', ''], ['Jiang', 'Yirui', '']]","extracted_entities":"[{'text': 'Medical Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MedHallBench', 'label': 'Foundation Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MedHallBench', 'label': 'Foundation Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Medical Large Language Models","similarity_score":0.7974728346}
{"id":2501.05031,"submitter":"Ronghao Dang","authors":"Ronghao Dang, Yuqian Yuan, Wenqi Zhang, Yifei Xin, Boqiang Zhang, Long\n  Li, Liuyi Wang, Qinyang Zeng, Xin Li, Lidong Bing","title":"ECBench: Can Multi-modal Foundation Models Understand the Egocentric\n  World? A Holistic Embodied Cognition Benchmark","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The enhancement of generalization in robots by large vision-language models\n(LVLMs) is increasingly evident. Therefore, the embodied cognitive abilities of\nLVLMs based on egocentric videos are of great interest. However, current\ndatasets for embodied video question answering lack comprehensive and\nsystematic evaluation frameworks. Critical embodied cognitive issues, such as\nrobotic self-cognition, dynamic scene perception, and hallucination, are rarely\naddressed. To tackle these challenges, we propose ECBench, a high-quality\nbenchmark designed to systematically evaluate the embodied cognitive abilities\nof LVLMs. ECBench features a diverse range of scene video sources, open and\nvaried question formats, and 30 dimensions of embodied cognition. To ensure\nquality, balance, and high visual dependence, ECBench uses class-independent\nmeticulous human annotation and multi-round question screening strategies.\nAdditionally, we introduce ECEval, a comprehensive evaluation system that\nensures the fairness and rationality of the indicators. Utilizing ECBench, we\nconduct extensive evaluations of proprietary, open-source, and task-specific\nLVLMs. ECBench is pivotal in advancing the embodied cognitive capabilities of\nLVLMs, laying a solid foundation for developing reliable core models for\nembodied agents. All data and code are available at\nhttps:\/\/github.com\/Rh-Dang\/ECBench.\n","versions":"[{'version': 'v1', 'created': 'Thu, 9 Jan 2025 07:43:49 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 07:45:55 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Dang', 'Ronghao', ''], ['Yuan', 'Yuqian', ''], ['Zhang', 'Wenqi', ''], ['Xin', 'Yifei', ''], ['Zhang', 'Boqiang', ''], ['Li', 'Long', ''], ['Wang', 'Liuyi', ''], ['Zeng', 'Qinyang', ''], ['Li', 'Xin', ''], ['Bing', 'Lidong', '']]","extracted_entities":"[{'text': 'large vision-language models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large vision-language models","similarity_score":0.7742220759}
{"id":2501.06828,"submitter":"Ruizhe Ou","authors":"Ruizhe Ou, Yuan Hu, Fan Zhang, Jiaxin Chen, Yu Liu","title":"GeoPix: Multi-Modal Large Language Model for Pixel-level Image\n  Understanding in Remote Sensing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multi-modal large language models (MLLMs) have achieved remarkable success in\nimage- and region-level remote sensing (RS) image understanding tasks, such as\nimage captioning, visual question answering, and visual grounding. However,\nexisting RS MLLMs lack the pixel-level dialogue capability, which involves\nresponding to user instructions with segmentation masks for specific instances.\nIn this paper, we propose GeoPix, a RS MLLM that extends image understanding\ncapabilities to the pixel level. This is achieved by equipping the MLLM with a\nmask predictor, which transforms visual features from the vision encoder into\nmasks conditioned on the LLM's segmentation token embeddings. To facilitate the\nsegmentation of multi-scale objects in RS imagery, a class-wise learnable\nmemory module is integrated into the mask predictor to capture and store\nclass-wise geo-context at the instance level across the entire dataset. In\naddition, to address the absence of large-scale datasets for training\npixel-level RS MLLMs, we construct the GeoPixInstruct dataset, comprising\n65,463 images and 140,412 instances, with each instance annotated with text\ndescriptions, bounding boxes, and masks. Furthermore, we develop a two-stage\ntraining strategy to balance the distinct requirements of text generation and\nmasks prediction in multi-modal multi-task optimization. Extensive experiments\nverify the effectiveness and superiority of GeoPix in pixel-level segmentation\ntasks, while also maintaining competitive performance in image- and\nregion-level benchmarks.\n","versions":"[{'version': 'v1', 'created': 'Sun, 12 Jan 2025 14:45:27 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 08:16:01 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Ou', 'Ruizhe', ''], ['Hu', 'Yuan', ''], ['Zhang', 'Fan', ''], ['Chen', 'Jiaxin', ''], ['Liu', 'Yu', '']]","extracted_entities":"[{'text': 'Multi-modal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'segmentation token embeddings', 'label': 'Embedding'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multi-modal large language models","similarity_score":0.7925285697}
{"id":2502.12029,"submitter":"Qi Zhao","authors":"Qi Zhao, Hongyu Yang, Qi Song, Xinwei Yao, Xiangyang Li","title":"KnowPath: Knowledge-enhanced Reasoning via LLM-generated Inference Paths\n  over Knowledge Graphs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious complex tasks, yet they still suffer from hallucinations. Introducing\nexternal knowledge, such as knowledge graph, can enhance the LLMs' ability to\nprovide factual answers. LLMs have the ability to interactively explore\nknowledge graphs. However, most approaches have been affected by insufficient\ninternal knowledge excavation in LLMs, limited generation of trustworthy\nknowledge reasoning paths, and a vague integration between internal and\nexternal knowledge. Therefore, we propose KnowPath, a knowledge-enhanced large\nmodel framework driven by the collaboration of internal and external knowledge.\nIt relies on the internal knowledge of the LLM to guide the exploration of\ninterpretable directed subgraphs in external knowledge graphs, better\nintegrating the two knowledge sources for more accurate reasoning. Extensive\nexperiments on multiple real-world datasets confirm the superiority of\nKnowPath.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Feb 2025 17:02:01 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 13:22:46 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhao', 'Qi', ''], ['Yang', 'Hongyu', ''], ['Song', 'Qi', ''], ['Yao', 'Xinwei', ''], ['Li', 'Xiangyang', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'KnowPath', 'label': 'LLM-based'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2502.12455,"submitter":"Minxuan Lv","authors":"Minxuan Lv, Zhenpeng Su, Leiyu Pan, Yizhe Xiong, Zijia Lin, Hui Chen,\n  Wei Zhou, Jungong Han, Guiguang Ding, Cheng Luo, Di Zhang, Kun Gai, Songlin\n  Hu","title":"DSMoE: Matrix-Partitioned Experts with Dynamic Routing for\n  Computation-Efficient Dense LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  As large language models continue to scale, computational costs and resource\nconsumption have emerged as significant challenges. While existing\nsparsification methods like pruning reduce computational overhead, they risk\nlosing model knowledge through parameter removal. This paper proposes DSMoE\n(Dynamic Sparse Mixture-of-Experts), a novel approach that achieves\nsparsification by partitioning pre-trained FFN layers into computational\nblocks. We implement adaptive expert routing using sigmoid activation and\nstraight-through estimators, enabling tokens to flexibly access different\naspects of model knowledge based on input complexity. Additionally, we\nintroduce a sparsity loss term to balance performance and computational\nefficiency. Extensive experiments on LLaMA models demonstrate that under\nequivalent computational constraints, DSMoE achieves superior performance\ncompared to existing pruning and MoE approaches across language modeling and\ndownstream tasks, particularly excelling in generation tasks. Analysis reveals\nthat DSMoE learns distinctive layerwise activation patterns, providing new\ninsights for future MoE architecture design.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Feb 2025 02:37:26 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 10:40:09 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Lv', 'Minxuan', ''], ['Su', 'Zhenpeng', ''], ['Pan', 'Leiyu', ''], ['Xiong', 'Yizhe', ''], ['Lin', 'Zijia', ''], ['Chen', 'Hui', ''], ['Zhou', 'Wei', ''], ['Han', 'Jungong', ''], ['Ding', 'Guiguang', ''], ['Luo', 'Cheng', ''], ['Zhang', 'Di', ''], ['Gai', 'Kun', ''], ['Hu', 'Songlin', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2502.17599,"submitter":"Zhongwei Wan","authors":"Zhongwei Wan, Hui Shen, Xin Wang, Che Liu, Zheda Mai, Mi Zhang","title":"MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference","comments":"NAACL 2025 Main","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps:\/\/github.com\/AIoT-MLSys-Lab\/MEDA.\n","versions":"[{'version': 'v1', 'created': 'Mon, 24 Feb 2025 19:34:52 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 04:04:08 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wan', 'Zhongwei', ''], ['Shen', 'Hui', ''], ['Wang', 'Xin', ''], ['Liu', 'Che', ''], ['Mai', 'Zheda', ''], ['Zhang', 'Mi', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MEDA', 'label': 'LLM'}, {'text': 'cross-modal attention entropy', 'label': 'Attention mechanism'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2502.19679,"submitter":"Linzhuo Li","authors":"Linzhuo li","title":"Old Experience Helps: Leveraging Survey Methodology to Improve AI Text\n  Annotation Reliability in Social Sciences","comments":"8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DL cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper introduces a framework for assessing the reliability of Large\nLanguage Model (LLM) text annotations in social science research by adapting\nestablished survey methodology principles. Drawing parallels between survey\nrespondent behavior and LLM outputs, the study implements three key\ninterventions: option randomization, position randomization, and reverse\nvalidation. While traditional accuracy metrics may mask model instabilities,\nparticularly in edge cases, the framework provides a more comprehensive\nreliability assessment. Using the F1000 dataset in biomedical science and three\nsizes of Llama models (8B, 70B, and 405B parameters), the paper demonstrates\nthat these survey-inspired interventions can effectively identify unreliable\nannotations that might otherwise go undetected through accuracy metrics alone.\nThe results show that 5-25% of LLM annotations change under these\ninterventions, with larger models exhibiting greater stability. Notably, for\nrare categories approximately 50% of \"correct\" annotations demonstrate low\nreliability when subjected to this framework. The paper then introduce an\ninformation-theoretic reliability score (R-score) based on Kullback-Leibler\ndivergence that quantifies annotation confidence and distinguishes between\nrandom guessing and meaningful annotations at the case level. This approach\ncomplements existing expert validation methods by providing a scalable way to\nassess internal annotation reliability and offers practical guidance for prompt\ndesign and downstream analysis.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 01:42:10 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 03:06:47 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['li', 'Linzhuo', '']]","extracted_entities":"[{'text': 'Large\\nLanguage Model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'prompt\\ndesign', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Large\nLanguage Model","similarity_score":1.0}
{"id":2503.04779,"submitter":"Thanh Le-Cong Le-Cong Thanh","authors":"Thanh Le-Cong, Bach Le, Toby Murray","title":"Can LLMs Reason About Program Semantics? A Comprehensive Evaluation of\n  LLMs on Formal Specification Inference","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.PL cs.AI cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) are increasingly being used to automate\nprogramming tasks. Yet, LLMs' capabilities in reasoning about program semantics\nare still inadequately studied, leaving significant potential for further\nexploration. This paper introduces FormalBench, a comprehensive benchmark\ndesigned to evaluate LLMs' reasoning abilities on program semantics,\nparticularly via the task of synthesizing formal program specifications to\nassist verifying program correctness. This task requires both comprehensive\nreasoning over all possible program executions and the generation of precise,\nsyntactically correct expressions that adhere to formal syntax and semantics.\nUsing this benchmark, we evaluated the ability of LLMs in synthesizing\nconsistent and complete specifications. Our findings show that LLMs perform\nwell with simple control flows but struggle with more complex structures,\nespecially loops, even with advanced prompting. Additionally, LLMs exhibit\nlimited robustness against semantic-preserving transformations. We also\nhighlight common failure patterns and design self-repair prompts, improving\nsuccess rates by 25%.\n","versions":"[{'version': 'v1', 'created': 'Sat, 22 Feb 2025 13:27:31 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 07:41:37 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Le-Cong', 'Thanh', ''], ['Le', 'Bach', ''], ['Murray', 'Toby', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'advanced prompting', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'design self-repair prompts', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.08708,"submitter":"Jingyi Zheng","authors":"Jingyi Zheng, Junfeng Wang, Zhen Sun, Wenhan Dong, Yule Liu, Xinlei He","title":"TH-Bench: Evaluating Evading Attacks via Humanizing AI Text on\n  Machine-Generated Text Detectors","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  As Large Language Models (LLMs) advance, Machine-Generated Texts (MGTs) have\nbecome increasingly fluent, high-quality, and informative. Existing wide-range\nMGT detectors are designed to identify MGTs to prevent the spread of plagiarism\nand misinformation. However, adversaries attempt to humanize MGTs to evade\ndetection (named evading attacks), which requires only minor modifications to\nbypass MGT detectors. Unfortunately, existing attacks generally lack a unified\nand comprehensive evaluation framework, as they are assessed using different\nexperimental settings, model architectures, and datasets. To fill this gap, we\nintroduce the Text-Humanization Benchmark (TH-Bench), the first comprehensive\nbenchmark to evaluate evading attacks against MGT detectors. TH-Bench evaluates\nattacks across three key dimensions: evading effectiveness, text quality, and\ncomputational overhead. Our extensive experiments evaluate 6 state-of-the-art\nattacks against 13 MGT detectors across 6 datasets, spanning 19 domains and\ngenerated by 11 widely used LLMs. Our findings reveal that no single evading\nattack excels across all three dimensions. Through in-depth analysis, we\nhighlight the strengths and limitations of different attacks. More importantly,\nwe identify a trade-off among three dimensions and propose two optimization\ninsights. Through preliminary experiments, we validate their correctness and\neffectiveness, offering potential directions for future research.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Mar 2025 02:55:05 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 10:37:18 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zheng', 'Jingyi', ''], ['Wang', 'Junfeng', ''], ['Sun', 'Zhen', ''], ['Dong', 'Wenhan', ''], ['Liu', 'Yule', ''], ['He', 'Xinlei', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'MGT', 'label': 'Large Language Model'}, {'text': 'MGT', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.09022,"submitter":"Wenjie Qu","authors":"Wenjie Qu, Yuguang Zhou, Yongji Wu, Tingsong Xiao, Binhang Yuan,\n  Yiming Li, Jiaheng Zhang","title":"Prompt Inversion Attack against Collaborative Inference of Large\n  Language Models","comments":"To appear at IEEE Symposium on Security and Privacy 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR","license":"http:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/","abstract":"  Large language models (LLMs) have been widely applied for their remarkable\ncapability of content generation. However, the practical use of open-source\nLLMs is hindered by high resource requirements, making deployment expensive and\nlimiting widespread development. The collaborative inference is a promising\nsolution for this problem, in which users collaborate by each hosting a subset\nof layers and transmitting intermediate activation. Many companies are building\ncollaborative inference platforms to reduce LLM serving costs, leveraging\nusers' underutilized GPUs. Despite widespread interest in collaborative\ninference within academia and industry, the privacy risks associated with LLM\ncollaborative inference have not been well studied. This is largely because of\nthe challenge posed by inverting LLM activation due to its strong\nnon-linearity.\n  In this paper, to validate the severity of privacy threats in LLM\ncollaborative inference, we introduce the concept of prompt inversion attack\n(PIA), where a malicious participant intends to recover the input prompt\nthrough the activation transmitted by its previous participant. Extensive\nexperiments show that our PIA method substantially outperforms existing\nbaselines. For example, our method achieves an 88.4\\% token accuracy on the\nSkytrax dataset with the Llama-65B model when inverting the maximum number of\ntransformer layers, while the best baseline method only achieves 22.8\\%\naccuracy. The results verify the effectiveness of our PIA attack and highlights\nits practical threat to LLM collaborative inference systems.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 03:20:03 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 05:55:55 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Qu', 'Wenjie', ''], ['Zhou', 'Yuguang', ''], ['Wu', 'Yongji', ''], ['Xiao', 'Tingsong', ''], ['Yuan', 'Binhang', ''], ['Li', 'Yiming', ''], ['Zhang', 'Jiaheng', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'input prompt', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.09533,"submitter":"Houyu Zhou","authors":"Nguyen Thach, Fei Liu, Houyu Zhou, Hau Chan","title":"Large Language Models for Multi-Facility Location Mechanism Design","comments":"Under Review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Designing strategyproof mechanisms for multi-facility location that optimize\nsocial costs based on agent preferences had been challenging due to the\nextensive domain knowledge required and poor worst-case guarantees. Recently,\ndeep learning models have been proposed as alternatives. However, these models\nrequire some domain knowledge and extensive hyperparameter tuning as well as\nlacking interpretability, which is crucial in practice when transparency of the\nlearned mechanisms is mandatory. In this paper, we introduce a novel approach,\nnamed LLMMech, that addresses these limitations by incorporating large language\nmodels (LLMs) into an evolutionary framework for generating interpretable,\nhyperparameter-free, empirically strategyproof, and nearly optimal mechanisms.\nOur experimental results, evaluated on various problem settings where the\nsocial cost is arbitrarily weighted across agents and the agent preferences may\nnot be uniformly distributed, demonstrate that the LLM-generated mechanisms\ngenerally outperform existing handcrafted baselines and deep learning models.\nFurthermore, the mechanisms exhibit impressive generalizability to\nout-of-distribution agent preferences and to larger instances with more agents.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 16:49:56 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 05:54:22 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Thach', 'Nguyen', ''], ['Liu', 'Fei', ''], ['Zhou', 'Houyu', ''], ['Chan', 'Hau', '']]","extracted_entities":"[{'text': 'hyperparameter tuning', 'label': 'Fine-tuning'}, {'text': 'large language\\nmodels', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language\nmodels","similarity_score":0.9664971828}
{"id":2503.09925,"submitter":"Mahmoud Srewa","authors":"Mahmoud Srewa, Tianyu Zhao, Salma Elmalaki","title":"PluralLLM: Pluralistic Alignment in LLMs via Federated Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Ensuring Large Language Models (LLMs) align with diverse human preferences\nwhile preserving privacy and fairness remains a challenge. Existing methods,\nsuch as Reinforcement Learning from Human Feedback (RLHF), rely on centralized\ndata collection, making them computationally expensive and privacy-invasive. We\nintroduce PluralLLM a federated learning-based approach that enables multiple\nuser groups to collaboratively train a transformer-based preference predictor\nwithout sharing sensitive data, which can also serve as a reward model for\naligning LLMs. Our method leverages Federated Averaging (FedAvg) to aggregate\npreference updates efficiently, achieving 46% faster convergence, a 4%\nimprovement in alignment scores, and nearly the same group fairness measure as\nin centralized training. Evaluated on a Q\/A preference alignment task,\nPluralLLM demonstrates that federated preference learning offers a scalable and\nprivacy-preserving alternative for aligning LLMs with diverse human values.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 00:45:27 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Srewa', 'Mahmoud', ''], ['Zhao', 'Tianyu', ''], ['Elmalaki', 'Salma', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'privacy and fairness', 'label': 'Model Bias and Fairness'}, {'text': 'Reinforcement Learning from Human Feedback', 'label': 'Few-shot Learning'}, {'text': 'PluralLLM', 'label': 'LLM-based'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'PluralLLM', 'label': 'LLM-based'}, {'text': 'federated preference learning', 'label': 'Few-shot Learning'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.09962,"submitter":"Jiayu Jiang","authors":"Jiayu Jiang, Changxing Ding, Wentao Tan, Junhong Wang, Jin Tao,\n  Xiangmin Xu","title":"Modeling Thousands of Human Annotators for Generalizable Text-to-Image\n  Person Re-identification","comments":"CVPR 2025. Project website: https:\/\/github.com\/sssaury\/HAM","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Text-to-image person re-identification (ReID) aims to retrieve the images of\nan interested person based on textual descriptions. One main challenge for this\ntask is the high cost in manually annotating large-scale databases, which\naffects the generalization ability of ReID models. Recent works handle this\nproblem by leveraging Multi-modal Large Language Models (MLLMs) to describe\npedestrian images automatically. However, the captions produced by MLLMs lack\ndiversity in description styles. To address this issue, we propose a Human\nAnnotator Modeling (HAM) approach to enable MLLMs to mimic the description\nstyles of thousands of human annotators. Specifically, we first extract style\nfeatures from human textual descriptions and perform clustering on them. This\nallows us to group textual descriptions with similar styles into the same\ncluster. Then, we employ a prompt to represent each of these clusters and apply\nprompt learning to mimic the description styles of different human annotators.\nFurthermore, we define a style feature space and perform uniform sampling in\nthis space to obtain more diverse clustering prototypes, which further enriches\nthe diversity of the MLLM-generated captions. Finally, we adopt HAM to\nautomatically annotate a massive-scale database for text-to-image ReID.\nExtensive experiments on this database demonstrate that it significantly\nimproves the generalization ability of ReID models.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 02:08:27 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Jiang', 'Jiayu', ''], ['Ding', 'Changxing', ''], ['Tan', 'Wentao', ''], ['Wang', 'Junhong', ''], ['Tao', 'Jin', ''], ['Xu', 'Xiangmin', '']]","extracted_entities":"[{'text': 'Multi-modal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'prompt', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Multi-modal Large Language Models","similarity_score":0.7925285697}
{"id":2503.09964,"submitter":"Usman Naseem","authors":"Bhavik Chandna, Mariam Aboujenane, Usman Naseem","title":"ExtremeAIGC: Benchmarking LMM Vulnerability to AI-Generated Extremist\n  Content","comments":"Preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Multimodal Models (LMMs) are increasingly vulnerable to AI-generated\nextremist content, including photorealistic images and text, which can be used\nto bypass safety mechanisms and generate harmful outputs. However, existing\ndatasets for evaluating LMM robustness offer limited exploration of extremist\ncontent, often lacking AI-generated images, diverse image generation models,\nand comprehensive coverage of historical events, which hinders a complete\nassessment of model vulnerabilities. To fill this gap, we introduce\nExtremeAIGC, a benchmark dataset and evaluation framework designed to assess\nLMM vulnerabilities against such content. ExtremeAIGC simulates real-world\nevents and malicious use cases by curating diverse text- and image-based\nexamples crafted using state-of-the-art image generation techniques. Our study\nreveals alarming weaknesses in LMMs, demonstrating that even cutting-edge\nsafety measures fail to prevent the generation of extremist material. We\nsystematically quantify the success rates of various attack strategies,\nexposing critical gaps in current defenses and emphasizing the need for more\nrobust mitigation strategies.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 02:10:29 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chandna', 'Bhavik', ''], ['Aboujenane', 'Mariam', ''], ['Naseem', 'Usman', '']]","extracted_entities":"[{'text': 'Large Multimodal Models', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Multimodal Models","similarity_score":0.5739125013}
{"id":2503.10009,"submitter":"Bowen Zhang","authors":"Bowen Zhang, Pengcheng Luo","title":"OR-LLM-Agent: Automating Modeling and Solving of Operations Research\n  Optimization Problem with Reasoning Large Language Model","comments":"11 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI math.OC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Operations Research (OR) has been widely applied in various fields such as\nresource allocation, production planning, and supply chain management. However,\naddressing real-world OR problems requires OR experts to perform mathematical\nmodeling and programmers to develop solution algorithms. This traditional\nmethod, heavily reliant on experts, is costly and has long development cycles,\nseverely limiting the widespread adoption of OR techniques. Few have considered\nusing Artificial Intelligence (AI) to replace professionals to achieve fully\nautomated solutions for OR problems. We propose OR-LLM-Agent, the first AI\nagent that enables end-to-end automation for solving real-world OR problems.\nOR-LLM-Agent leverages the Chain-of-Thought (CoT) reasoning capabilities of\nLarge Language Models (LLMs) to translate natural language problem descriptions\ninto formal mathematical models and automatically generate Gurobi solver code.\nIn OR-LLM-Agent, OR-CodeAgent is designed to automate code execution and repair\nwithin a sandbox environment, facilitating the derivation of the final\nsolution. Due to the lack of dedicated benchmark datasets for evaluating the\nautomated solving of OR problems, we construct a benchmark dataset comprising\n83 real-world OR problems described in natural language. We conduct comparative\nexperiments with state-of-the-art (SOTA) reasoning LLMs, including GPT-o3-mini,\nDeepSeek-R1, and Gemini 2.0 Flash Thinking. The OR-LLM-Agent achieved the\nhighest pass rate of 100% and the highest solution accuracy of 85%,\ndemonstrating the feasibility of automated OR problem-solving. Data and code\nhave been publicly available at https:\/\/github.com\/bwz96sco\/or_llm_agent.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 03:40:50 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhang', 'Bowen', ''], ['Luo', 'Pengcheng', '']]","extracted_entities":"[{'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.10042,"submitter":"Ziyue Wang","authors":"Ziyue Wang, Yurui Dong, Fuwen Luo, Minyuan Ruan, Zhili Cheng, Chi\n  Chen, Peng Li, Yang Liu","title":"How Do Multimodal Large Language Models Handle Complex Multimodal\n  Reasoning? Placing Them in An Extensible Escape Game","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred\ninterest in complex multimodal reasoning tasks in the real-world and virtual\nenvironment, which require coordinating multiple abilities, including visual\nperception, visual reasoning, spatial awareness, and target deduction. However,\nexisting evaluations primarily assess the final task completion, often\ndegrading assessments to isolated abilities such as visual grounding and visual\nquestion answering. Less attention is given to comprehensively and\nquantitatively analyzing reasoning process in multimodal environments, which is\ncrucial for understanding model behaviors and underlying reasoning mechanisms\nbeyond merely task success. To address this, we introduce MM-Escape, an\nextensible benchmark for investigating multimodal reasoning, inspired by\nreal-world escape games. MM-Escape emphasizes intermediate model behaviors\nalongside final task completion. To achieve this, we develop EscapeCraft, a\ncustomizable and open environment that enables models to engage in free-form\nexploration for assessing multimodal reasoning. Extensive experiments show that\nMLLMs, regardless of scale, can successfully complete the simplest room escape\ntasks, with some exhibiting human-like exploration strategies. Yet, performance\ndramatically drops as task difficulty increases. Moreover, we observe that\nperformance bottlenecks vary across models, revealing distinct failure modes\nand limitations in their multimodal reasoning abilities, such as repetitive\ntrajectories without adaptive exploration, getting stuck in corners due to poor\nvisual spatial awareness, and ineffective use of acquired props, such as the\nkey. We hope our work sheds light on new challenges in multimodal reasoning,\nand uncovers potential improvements in MLLMs capabilities.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 04:48:43 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Ziyue', ''], ['Dong', 'Yurui', ''], ['Luo', 'Fuwen', ''], ['Ruan', 'Minyuan', ''], ['Cheng', 'Zhili', ''], ['Chen', 'Chi', ''], ['Li', 'Peng', ''], ['Liu', 'Yang', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2503.10049,"submitter":"Jianzong Wang","authors":"Ziqi Jia, Junjie Li, Xiaoyang Qu, Jianzong Wang","title":"Enhancing Multi-Agent Systems via Reinforcement Learning with LLM-based\n  Planner and Graph-based Policy","comments":"Accepted by the 2025 IEEE International Conference on Robotics &\n  Automation (ICRA 2025)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multi-agent systems (MAS) have shown great potential in executing complex\ntasks, but coordination and safety remain significant challenges. Multi-Agent\nReinforcement Learning (MARL) offers a promising framework for agent\ncollaboration, but it faces difficulties in handling complex tasks and\ndesigning reward functions. The introduction of Large Language Models (LLMs)\nhas brought stronger reasoning and cognitive abilities to MAS, but existing\nLLM-based systems struggle to respond quickly and accurately in dynamic\nenvironments. To address these challenges, we propose LLM-based Graph\nCollaboration MARL (LGC-MARL), a framework that efficiently combines LLMs and\nMARL. This framework decomposes complex tasks into executable subtasks and\nachieves efficient collaboration among multiple agents through graph-based\ncoordination. Specifically, LGC-MARL consists of two main components: an LLM\nplanner and a graph-based collaboration meta policy. The LLM planner transforms\ncomplex task instructions into a series of executable subtasks, evaluates the\nrationality of these subtasks using a critic model, and generates an action\ndependency graph. The graph-based collaboration meta policy facilitates\ncommunication and collaboration among agents based on the action dependency\ngraph, and adapts to new task environments through meta-learning. Experimental\nresults on the AI2-THOR simulation platform demonstrate the superior\nperformance and scalability of LGC-MARL in completing various complex tasks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 05:02:49 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Jia', 'Ziqi', ''], ['Li', 'Junjie', ''], ['Qu', 'Xiaoyang', ''], ['Wang', 'Jianzong', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.10069,"submitter":"Xiangyu Shi","authors":"Xiangyu Shi, Zerui Li, Wenqi Lyu, Jiatong Xia, Feras Dayoub, Yanyuan\n  Qiao, Qi Wu","title":"SmartWay: Enhanced Waypoint Prediction and Backtracking for Zero-Shot\n  Vision-and-Language Navigation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Vision-and-Language Navigation (VLN) in continuous environments requires\nagents to interpret natural language instructions while navigating\nunconstrained 3D spaces. Existing VLN-CE frameworks rely on a two-stage\napproach: a waypoint predictor to generate waypoints and a navigator to execute\nmovements. However, current waypoint predictors struggle with spatial\nawareness, while navigators lack historical reasoning and backtracking\ncapabilities, limiting adaptability. We propose a zero-shot VLN-CE framework\nintegrating an enhanced waypoint predictor with a Multi-modal Large Language\nModel (MLLM)-based navigator. Our predictor employs a stronger vision encoder,\nmasked cross-attention fusion, and an occupancy-aware loss for better waypoint\nquality. The navigator incorporates history-aware reasoning and adaptive path\nplanning with backtracking, improving robustness. Experiments on R2R-CE and\nMP3D benchmarks show our method achieves state-of-the-art (SOTA) performance in\nzero-shot settings, demonstrating competitive results compared to fully\nsupervised methods. Real-world validation on Turtlebot 4 further highlights its\nadaptability.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 05:32:57 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Shi', 'Xiangyu', ''], ['Li', 'Zerui', ''], ['Lyu', 'Wenqi', ''], ['Xia', 'Jiatong', ''], ['Dayoub', 'Feras', ''], ['Qiao', 'Yanyuan', ''], ['Wu', 'Qi', '']]","extracted_entities":"[{'text': 'Multi-modal Large Language\\nModel (MLLM)', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multi-modal Large Language\nModel (MLLM)","similarity_score":0.7299913764}
{"id":2503.10079,"submitter":"Chunyi Li","authors":"Chunyi Li, Xiaozhe Li, Zicheng Zhang, Yuan Tian, Ziheng Jia, Xiaohong\n  Liu, Xiongkuo Min, Jia Wang, Haodong Duan, Kai Chen, Guangtao Zhai","title":"Information Density Principle for MLLM Benchmarks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  With the emergence of Multimodal Large Language Models (MLLMs), hundreds of\nbenchmarks have been developed to ensure the reliability of MLLMs in downstream\ntasks. However, the evaluation mechanism itself may not be reliable. For\ndevelopers of MLLMs, questions remain about which benchmark to use and whether\nthe test results meet their requirements. Therefore, we propose a critical\nprinciple of Information Density, which examines how much insight a benchmark\ncan provide for the development of MLLMs. We characterize it from four key\ndimensions: (1) Fallacy, (2) Difficulty, (3) Redundancy, (4) Diversity. Through\na comprehensive analysis of more than 10,000 samples, we measured the\ninformation density of 19 MLLM benchmarks. Experiments show that using the\nlatest benchmarks in testing can provide more insight compared to previous\nones, but there is still room for improvement in their information density. We\nhope this principle can promote the development and application of future MLLM\nbenchmarks. Project page: https:\/\/github.com\/lcysyzxdxc\/bench4bench\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 05:58:41 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Chunyi', ''], ['Li', 'Xiaozhe', ''], ['Zhang', 'Zicheng', ''], ['Tian', 'Yuan', ''], ['Jia', 'Ziheng', ''], ['Liu', 'Xiaohong', ''], ['Min', 'Xiongkuo', ''], ['Wang', 'Jia', ''], ['Duan', 'Haodong', ''], ['Chen', 'Kai', ''], ['Zhai', 'Guangtao', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2503.10084,"submitter":"Juntai Cao","authors":"Xiang Zhang, Juntai Cao, Jiaqi Wei, Chenyu You, Dujian Ding","title":"Why Does Your CoT Prompt (Not) Work? Theoretical Analysis of Prompt\n  Space Complexity, its Interaction with Answer Space During CoT Reasoning with\n  LLMs: A Recurrent Perspective","comments":"arXiv admin note: substantial text overlap with arXiv:2410.14198","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Despite the remarkable successes of Large Language Models (LLMs), their\nfundamental Transformer architecture possesses inherent theoretical limitations\nthat restrict their capability to handle reasoning tasks with increasing\ncomputational complexity. Chain-of-Thought (CoT) prompting has emerged as a\npractical solution, supported by several theoretical studies. However, current\nCoT-based methods (including ToT, GoT, etc.) generally adopt a\n\"one-prompt-fits-all\" strategy, using fixed templates (e.g., \"think step by\nstep\") across diverse reasoning tasks. This method forces models to navigate an\nextremely complex prompt space to identify effective reasoning paths. The\ncurrent prompt designing research are also heavily relying on trial-and-error\nrather than theoretically informed guidance. In this paper, we provide a\nrigorous theoretical analysis of the complexity and interplay between two\ncrucial spaces: the prompt space (the space of potential prompt structures) and\nthe answer space (the space of reasoning solutions generated by LLMs) in CoT\nreasoning. We demonstrate how reliance on a single universal prompt (e.g. think\nstep by step) can negatively impact the theoretical computability of LLMs,\nillustrating that prompt complexity directly influences the structure and\neffectiveness of the navigation in answer space. Our analysis highlights that\nsometimes human supervision is critical for efficiently navigating the prompt\nspace. We theoretically and empirically show that task-specific prompting\nsignificantly outperforms unsupervised prompt generation, emphasizing the\nnecessity of thoughtful human guidance in CoT prompting.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 06:11:10 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhang', 'Xiang', ''], ['Cao', 'Juntai', ''], ['Wei', 'Jiaqi', ''], ['You', 'Chenyu', ''], ['Ding', 'Dujian', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'think step by\\nstep', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'CoT', 'label': 'Chain of thought'}, {'text': 'think\\nstep by step', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'task-specific prompting', 'label': 'Prompting'}, {'text': 'CoT prompting', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.10093,"submitter":"Qiyuan Deng","authors":"Qiyuan Deng, Xuefeng Bai, Kehai Chen, Yaowei Wang, Liqiang Nie, Min\n  Zhang","title":"Representation-based Reward Modeling for Efficient Safety Alignment of\n  Large Language Model","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Reinforcement Learning (RL) algorithms for safety alignment of Large Language\nModels (LLMs), such as Direct Preference Optimization (DPO), encounter the\nchallenge of distribution shift. Current approaches typically address this\nissue through online sampling from the target policy, which requires\nsignificant computational resources. In this paper, we hypothesize that during\noff-policy training, while the ranking order of output generated by policy\nchanges, their overall distribution remains relatively stable. This stability\nallows the transformation of the sampling process from the target policy into a\nre-ranking of preference data. Building on this hypothesis, We propose a new\nframework that leverages the model's intrinsic safety judgment capability to\nextract reward signals, which are then used to calculate label confidence for\npreferences reordering. Extensive experimental results and theoretical analysis\ndemonstrate that the proposed method effectively addresses the distribution\nshift issue, remarkably enhancing the safety performance while reducing about\n300x computational overheads.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 06:40:34 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Deng', 'Qiyuan', ''], ['Bai', 'Xuefeng', ''], ['Chen', 'Kehai', ''], ['Wang', 'Yaowei', ''], ['Nie', 'Liqiang', ''], ['Zhang', 'Min', '']]","extracted_entities":"[{'text': 'Large Language\\nModels', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language\nModels","similarity_score":0.9664971828}
{"id":2503.10099,"submitter":"Han Liu","authors":"Lin Ao, Han Liu, Huafeng Zhang","title":"AgentDAO: Synthesis of Proposal Transactions Via Abstract DAO Semantics","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While the trend of decentralized governance is obvious (cryptocurrencies and\nblockchains are widely adopted by multiple sovereign countries), initiating\ngovernance proposals within Decentralized Autonomous Organizations (DAOs) is\nstill challenging, i.e., it requires providing a low-level transaction payload,\ntherefore posing significant barriers to broad community participation. To\naddress these challenges, we propose a multi-agent system powered by Large\nLanguage Models with a novel Label-Centric Retrieval algorithm to automate the\ntranslation from natural language inputs into executable proposal transactions.\nThe system incorporates DAOLang, a Domain-Specific Language to simplify the\nspecification of various governance proposals. The key optimization achieved by\nDAOLang is a semantic-aware abstraction of user input that reliably secures\nproposal generation with a low level of token demand. A preliminary evaluation\non real-world applications reflects the potential of DAOLang in terms of\ngenerating complicated types of proposals with existing foundation models, e.g.\nGPT-4o.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 06:52:18 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Ao', 'Lin', ''], ['Liu', 'Han', ''], ['Zhang', 'Huafeng', '']]","extracted_entities":"[{'text': 'Large\\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"Large Language Model","matched_keyword":"Large\nLanguage Models","similarity_score":0.9664971828}
{"id":2503.10135,"submitter":"Jinze Li","authors":"Jinze Li, Yixing Xu, Haiduo Huang, Xuanwu Yin, Dong Li, Edith C.H.\n  Ngai, Emad Barsoum","title":"Gumiho: A Hybrid Architecture to Prioritize Early Tokens in Speculative\n  Decoding","comments":"Paper under review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Speculative decoding (SPD) aims to accelerate the auto-regressive token\ngeneration process of a target Large Language Model (LLM). Some approaches\nemploy a draft model with multiple heads to predict a sequence of future\ntokens, where each head handles a token in the sequence. The target LLM\nverifies the predicted sequence and accepts aligned tokens, enabling efficient\nmulti-token generation. However, existing methods assume that all tokens within\na sequence are equally important, employing identical head structures and\nrelying on a single-generation paradigm, either serial or parallel. To this\nend, we theoretically demonstrate that initial tokens in the draft sequence are\nmore important than later ones. Building on this insight, we propose Gumiho, a\nhybrid model combining serial and parallel heads. Specifically, given the\ncritical importance of early tokens, we employ a sophisticated Transformer\narchitecture for the early draft heads in a serial configuration to improve\naccuracy. For later tokens, we utilize multiple lightweight MLP heads operating\nin parallel to enhance efficiency. By allocating more advanced model structures\nand longer running times to the early heads, Gumiho achieves improved overall\nperformance. The experimental results demonstrate that our method outperforms\nexisting approaches, fully validating its effectiveness.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 07:55:38 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Jinze', ''], ['Xu', 'Yixing', ''], ['Huang', 'Haiduo', ''], ['Yin', 'Xuanwu', ''], ['Li', 'Dong', ''], ['Ngai', 'Edith C. H.', ''], ['Barsoum', 'Emad', '']]","extracted_entities":"[{'text': 'target Large Language Model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"target Large Language Model","similarity_score":0.9097527266}
{"id":2503.1015,"submitter":"Haoyu Huang","authors":"Haoyu Huang, Yongfeng Huang, Junjie Yang, Zhenyu Pan, Yongqiang Chen,\n  Kaili Ma, Hongzhi Chen, James Cheng","title":"Retrieval-Augmented Generation with Hierarchical Knowledge","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Graph-based Retrieval-Augmented Generation (RAG) methods have significantly\nenhanced the performance of large language models (LLMs) in domain-specific\ntasks. However, existing RAG methods do not adequately utilize the naturally\ninherent hierarchical knowledge in human cognition, which limits the\ncapabilities of RAG systems. In this paper, we introduce a new RAG approach,\ncalled HiRAG, which utilizes hierarchical knowledge to enhance the semantic\nunderstanding and structure capturing capabilities of RAG systems in the\nindexing and retrieval processes. Our extensive experiments demonstrate that\nHiRAG achieves significant performance improvements over the state-of-the-art\nbaseline methods. The code of our proposed method is available at\n\\href{https:\/\/github.com\/hhy-huang\/HiRAG}{https:\/\/github.com\/hhy-huang\/HiRAG}.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 08:22:31 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Huang', 'Haoyu', ''], ['Huang', 'Yongfeng', ''], ['Yang', 'Junjie', ''], ['Pan', 'Zhenyu', ''], ['Chen', 'Yongqiang', ''], ['Ma', 'Kaili', ''], ['Chen', 'Hongzhi', ''], ['Cheng', 'James', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'HiRAG', 'label': 'RAG'}, {'text': 'HiRAG', 'label': 'RAG'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.10167,"submitter":"Je Won Yeom","authors":"Hyunbin Jin, Je Won Yeom, Seunghyun Bae, Taesup Kim","title":"\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection\n  Decoding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs) exhibit strong reasoning abilities, often\nattributed to few-shot or zero-shot chain-of-thought (CoT) prompting. While\neffective, these methods require labor-intensive prompt engineering, raising\nthe question of whether reasoning can be induced without reliance on explicit\nprompts. In this work, we unlock the reasoning capabilities of LLMs without\nexplicit prompting. Inspired by zero-shot CoT and CoT-decoding, we propose a\nnovel decoding strategy that systematically nudges LLMs to continue reasoning,\nthereby preventing immature reasoning processes. Specifically, we monitor the\nmodel's generation and inject a designated phrase whenever it is likely to\nconclude its response prematurely, before completing the reasoning process. Our\nexperimental evaluations on diverse reasoning benchmarks demonstrate that our\nproposed strategy substantially improves LLM reasoning capabilities,\nhighlighting the potential of decoding-based interventions as an alternative to\ntraditional prompting techniques.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 08:46:32 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Jin', 'Hyunbin', ''], ['Yeom', 'Je Won', ''], ['Bae', 'Seunghyun', ''], ['Kim', 'Taesup', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'zero-shot CoT', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.10211,"submitter":"HengLyu Liu","authors":"Henglyu Liu, Andong Chen, Kehai Chen, Xuefeng Bai, Meizhi Zhong, Yuan\n  Qiu, Min Zhang","title":"Adaptive Inner Speech-Text Alignment for LLM-based Speech Translation","comments":"12 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.SD eess.AS","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advancement of large language models (LLMs) has led to significant\nbreakthroughs across various tasks, laying the foundation for the development\nof LLM-based speech translation systems. Existing methods primarily focus on\naligning inputs and outputs across modalities while overlooking deeper semantic\nalignment within model representations. To address this limitation, we propose\nan Adaptive Inner Speech-Text Alignment (AI-STA) method to bridge the modality\ngap by explicitly aligning speech and text representations at selected layers\nwithin LLMs. To achieve this, we leverage the optimal transport (OT) theory to\nquantify fine-grained representation discrepancies between speech and text.\nFurthermore, we utilize the cross-modal retrieval technique to identify the\nlayers that are best suited for alignment and perform joint training on these\nlayers. Experimental results on speech translation (ST) tasks demonstrate that\nAI-STA significantly improves the translation performance of large speech-text\nmodels (LSMs), outperforming previous state-of-the-art approaches. Our findings\nhighlight the importance of inner-layer speech-text alignment in LLMs and\nprovide new insights into enhancing cross-modal learning.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 09:54:35 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Liu', 'Henglyu', ''], ['Chen', 'Andong', ''], ['Chen', 'Kehai', ''], ['Bai', 'Xuefeng', ''], ['Zhong', 'Meizhi', ''], ['Qiu', 'Yuan', ''], ['Zhang', 'Min', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'cross-modal learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.10241,"submitter":"Sabrina Patania","authors":"Dimitri Ognibene, Sabrina Patania, Luca Annese, Cansu Koyuturk, Franca\n  Garzotto, Giuseppe Vizzari, Azzurra Ruggeri, Simone Colombani","title":"SCOOP: A Framework for Proactive Collaboration and Social Continual\n  Learning through Natural Language Interaction andCausal Reasoning","comments":"5 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.MA cs.HC cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multimodal information-gathering settings, where users collaborate with AI in\ndynamic environments, are increasingly common. These involve complex processes\nwith textual and multimodal interactions, often requiring additional structural\ninformation via cost-incurring requests. AI helpers lack access to users' true\ngoals, beliefs, and preferences and struggle to integrate diverse information\neffectively.\n  We propose a social continual learning framework for causal knowledge\nacquisition and collaborative decision-making. It focuses on autonomous agents\nlearning through dialogues, question-asking, and interaction in open, partially\nobservable environments. A key component is a natural language oracle that\nanswers the agent's queries about environmental mechanisms and states, refining\ncausal understanding while balancing exploration or learning, and exploitation\nor knowledge use.\n  Evaluation tasks inspired by developmental psychology emphasize causal\nreasoning and question-asking skills. They complement benchmarks by assessing\nthe agent's ability to identify knowledge gaps, generate meaningful queries,\nand incrementally update reasoning. The framework also evaluates how knowledge\nacquisition costs are amortized across tasks within the same environment.\n  We propose two architectures: 1) a system combining Large Language Models\n(LLMs) with the ReAct framework and question-generation, and 2) an advanced\nsystem with a causal world model, symbolic, graph-based, or subsymbolic, for\nreasoning and decision-making. The latter builds a causal knowledge graph for\nefficient inference and adaptability under constraints. Challenges include\nintegrating causal reasoning into ReAct and optimizing exploration and\nquestion-asking in error-prone scenarios. Beyond applications, this framework\nmodels developmental processes combining causal reasoning, question generation,\nand social learning.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 10:32:50 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Ognibene', 'Dimitri', ''], ['Patania', 'Sabrina', ''], ['Annese', 'Luca', ''], ['Koyuturk', 'Cansu', ''], ['Garzotto', 'Franca', ''], ['Vizzari', 'Giuseppe', ''], ['Ruggeri', 'Azzurra', ''], ['Colombani', 'Simone', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.10248,"submitter":"Idan Horowitz","authors":"Idan Horowitz and Ori Plonsky","title":"LLM Agents Display Human Biases but Exhibit Distinct Learning Patterns","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We investigate the choice patterns of Large Language Models (LLMs) in the\ncontext of Decisions from Experience tasks that involve repeated choice and\nlearning from feedback, and compare their behavior to human participants. We\nfind that on the aggregate, LLMs appear to display behavioral biases similar to\nhumans: both exhibit underweighting rare events and correlation effects.\nHowever, more nuanced analyses of the choice patterns reveal that this happens\nfor very different reasons. LLMs exhibit strong recency biases, unlike humans,\nwho appear to respond in more sophisticated ways. While these different\nprocesses may lead to similar behavior on average, choice patterns contingent\non recent events differ vastly between the two groups. Specifically, phenomena\nsuch as ``surprise triggers change\" and the ``wavy recency effect of rare\nevents\" are robustly observed in humans, but entirely absent in LLMs. Our\nfindings provide insights into the limitations of using LLMs to simulate and\npredict humans in learning environments and highlight the need for refined\nanalyses of their behavior when investigating whether they replicate human\ndecision making tendencies.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 10:47:03 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Horowitz', 'Idan', ''], ['Plonsky', 'Ori', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.10291,"submitter":"Weiyun Wang","authors":"Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu\n  Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, Lewei Lu, Haodong\n  Duan, Yu Qiao, Jifeng Dai, Wenhai Wang","title":"VisualPRM: An Effective Process Reward Model for Multimodal Reasoning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce VisualPRM, an advanced multimodal Process Reward Model (PRM)\nwith 8B parameters, which improves the reasoning abilities of existing\nMultimodal Large Language Models (MLLMs) across different model scales and\nfamilies with Best-of-N (BoN) evaluation strategies. Specifically, our model\nimproves the reasoning performance of three types of MLLMs and four different\nmodel scales. Even when applied to the highly capable InternVL2.5-78B, it\nachieves a 5.9-point improvement across seven multimodal reasoning benchmarks.\nExperimental results show that our model exhibits superior performance compared\nto Outcome Reward Models and Self-Consistency during BoN evaluation. To\nfacilitate the training of multimodal PRMs, we construct a multimodal process\nsupervision dataset VisualPRM400K using an automated data pipeline. For the\nevaluation of multimodal PRMs, we propose VisualProcessBench, a benchmark with\nhuman-annotated step-wise correctness labels, to measure the abilities of PRMs\nto detect erroneous steps in multimodal reasoning tasks. We hope that our work\ncan inspire more future research and contribute to the development of MLLMs.\nOur model, data, and benchmark are released in\nhttps:\/\/internvl.github.io\/blog\/2025-03-13-VisualPRM\/.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 12:03:37 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Weiyun', ''], ['Gao', 'Zhangwei', ''], ['Chen', 'Lianjie', ''], ['Chen', 'Zhe', ''], ['Zhu', 'Jinguo', ''], ['Zhao', 'Xiangyu', ''], ['Liu', 'Yangzhou', ''], ['Cao', 'Yue', ''], ['Ye', 'Shenglong', ''], ['Zhu', 'Xizhou', ''], ['Lu', 'Lewei', ''], ['Duan', 'Haodong', ''], ['Qiao', 'Yu', ''], ['Dai', 'Jifeng', ''], ['Wang', 'Wenhai', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2503.1031,"submitter":"Shin Yoo Dr","authors":"Shin Yoo and Robert Feldt and Somin Kim and Naryeong Kim","title":"Capturing Semantic Flow of ML-based Systems","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  ML-based systems are software systems that incorporates machine learning\ncomponents such as Deep Neural Networks (DNNs) or Large Language Models (LLMs).\nWhile such systems enable advanced features such as high performance computer\nvision, natural language processing, and code generation, their internal\nbehaviour remain largely opaque to traditional dynamic analysis such as\ntesting: existing analysis typically concern only what is observable from the\noutside, such as input similarity or class label changes. We propose semantic\nflow, a concept designed to capture the internal behaviour of ML-based system\nand to provide a platform for traditional dynamic analysis techniques to be\nadapted to. Semantic flow combines the idea of control flow with internal\nstates taken from executions of ML-based systems, such as activation values of\na specific layer in a DNN, or embeddings of LLM responses at a specific\ninference step of LLM agents. The resulting representation, summarised as\nsemantic flow graphs, can capture internal decisions that are not explicitly\nrepresented in the traditional control flow of ML-based systems. We propose the\nidea of semantic flow, introduce two examples using a DNN and an LLM agent, and\nfinally sketch its properties and how it can be used to adapt existing dynamic\nanalysis techniques for use in ML-based software systems.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 12:39:04 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Yoo', 'Shin', ''], ['Feldt', 'Robert', ''], ['Kim', 'Somin', ''], ['Kim', 'Naryeong', '']]","extracted_entities":"[{'text': 'ML-based systems', 'label': 'LLM-based'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'ML-based systems', 'label': 'LLM-based'}, {'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.10324,"submitter":"Pingping Zhang Dr","authors":"Yuhao Wang and Yongfeng Lv and Pingping Zhang and Huchuan Lu","title":"IDEA: Inverted Text with Cooperative Deformable Aggregation for\n  Multi-modal Object Re-Identification","comments":"This work is accepted by CVPR2025. More modifications may be\n  performed","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.MM","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multi-modal object Re-IDentification (ReID) aims to retrieve specific objects\nby utilizing complementary information from various modalities. However,\nexisting methods focus on fusing heterogeneous visual features, neglecting the\npotential benefits of text-based semantic information. To address this issue,\nwe first construct three text-enhanced multi-modal object ReID benchmarks. To\nbe specific, we propose a standardized multi-modal caption generation pipeline\nfor structured and concise text annotations with Multi-modal Large Language\nModels (MLLMs). Besides, current methods often directly aggregate multi-modal\ninformation without selecting representative local features, leading to\nredundancy and high complexity. To address the above issues, we introduce IDEA,\na novel feature learning framework comprising the Inverted Multi-modal Feature\nExtractor (IMFE) and Cooperative Deformable Aggregation (CDA). The IMFE\nutilizes Modal Prefixes and an InverseNet to integrate multi-modal information\nwith semantic guidance from inverted text. The CDA adaptively generates\nsampling positions, enabling the model to focus on the interplay between global\nfeatures and discriminative local features. With the constructed benchmarks and\nthe proposed modules, our framework can generate more robust multi-modal\nfeatures under complex scenarios. Extensive experiments on three multi-modal\nobject ReID benchmarks demonstrate the effectiveness of our proposed method.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 13:00:31 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Yuhao', ''], ['Lv', 'Yongfeng', ''], ['Zhang', 'Pingping', ''], ['Lu', 'Huchuan', '']]","extracted_entities":"[{'text': 'Multi-modal Large Language\\nModels', 'label': 'Large Language Model'}, {'text': 'Modal Prefixes', 'label': 'Embedding'}]","assigned_concept":"Large Language Model","matched_keyword":"Multi-modal Large Language\nModels","similarity_score":0.7925285697}
{"id":2503.10325,"submitter":"Jianchun Liu","authors":"Luyao Gao, Jianchun Liu, Hongli Xu, Liusheng Huang","title":"Collaborative Speculative Inference for Efficient LLM Inference Serving","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DC cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Speculative inference is a promising paradigm employing small speculative\nmodels (SSMs) as drafters to generate draft tokens, which are subsequently\nverified in parallel by the target large language model (LLM). This approach\nenhances the efficiency of inference serving by reducing LLM inference latency\nand costs while preserving generation quality. However, existing speculative\nmethods face critical challenges, including inefficient resource utilization\nand limited draft acceptance, which constrain their scalability and overall\neffectiveness. To overcome these obstacles, we present CoSine, a novel\nspeculative inference system that decouples sequential speculative decoding\nfrom parallel verification, enabling efficient collaboration among multiple\nnodes. Specifically, CoSine routes inference requests to specialized drafters\nbased on their expertise and incorporates a confidence-based token fusion\nmechanism to synthesize outputs from cooperating drafters, ensuring\nhigh-quality draft generation. Additionally, CoSine dynamically orchestrates\nthe execution of speculative decoding and verification in a pipelined manner,\nemploying batch scheduling to selectively group requests and adaptive\nspeculation control to minimize idle periods. By optimizing parallel workflows\nthrough heterogeneous node collaboration, CoSine balances draft generation and\nverification throughput in real-time, thereby maximizing resource utilization.\nExperimental results demonstrate that CoSine achieves superior performance\ncompared to state-of-the-art speculative approaches. Notably, with equivalent\nresource costs, CoSine achieves up to a 23.2% decrease in latency and a 32.5%\nincrease in throughput compared to baseline methods.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 13:03:38 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Gao', 'Luyao', ''], ['Liu', 'Jianchun', ''], ['Xu', 'Hongli', ''], ['Huang', 'Liusheng', '']]","extracted_entities":"[{'text': 'target large language model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'CoSine', 'label': 'LLM-based'}, {'text': 'CoSine', 'label': 'LLM-based'}, {'text': 'CoSine', 'label': 'LLM-based'}, {'text': 'CoSine', 'label': 'LLM-based'}, {'text': 'CoSine', 'label': 'LLM-based'}, {'text': 'CoSine', 'label': 'LLM-based'}]","assigned_concept":"Large Language Model","matched_keyword":"target large language model","similarity_score":0.9097527266}
{"id":2503.10367,"submitter":"Yijiang Fan","authors":"Yijiang Fan, Yuren Mao, Longbin Lai, Ying Zhang, Zhengping Qian,\n  Yunjun Gao","title":"G-Boost: Boosting Private SLMs with General LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Due to the limited computational resources, most Large Language Models (LLMs)\ndevelopers can only fine-tune Small Language Models (SLMs) on their own data.\nThese private SLMs typically have limited effectiveness. To boost the\nperformance of private SLMs, this paper proposes to ask general LLMs for help.\nThe general LLMs can be APIs or larger LLMs whose inference cost the developers\ncan afford. Specifically, we propose the G-Boost framework where a private SLM\nadaptively performs collaborative inference with a general LLM under the guide\nof process reward. Experiments demonstrate that our framework can significantly\nboost the performance of private SLMs.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 13:47:03 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Fan', 'Yijiang', ''], ['Mao', 'Yuren', ''], ['Lai', 'Longbin', ''], ['Zhang', 'Ying', ''], ['Qian', 'Zhengping', ''], ['Gao', 'Yunjun', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'APIs', 'label': 'Open-source LLMs'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.10377,"submitter":"Qiaoling Chen","authors":"Qiaoling Chen, Shenggui Li, Wei Gao, Peng Sun, Yonggang Wen, Tianwei\n  Zhang","title":"SPPO:Efficient Long-sequence LLM Training via Adaptive Sequence Pipeline\n  Parallel Offloading","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In recent years, Large Language Models (LLMs) have exhibited remarkable\ncapabilities, driving advancements in real-world applications. However,\ntraining LLMs on increasingly long input sequences imposes significant\nchallenges due to high GPU memory and computational demands. Existing solutions\nface two key limitations: (1) memory reduction techniques, such as activation\nrecomputation and CPU offloading, compromise training efficiency; (2)\ndistributed parallelism strategies require excessive GPU resources, limiting\nthe scalability of input sequence length.\n  To address these gaps, we propose Adaptive Sequence Pipeline Parallel\nOffloading (SPPO), a novel LLM training framework that optimizes memory and\ncomputational resource efficiency for long-sequence training. SPPO introduces\nadaptive offloading, leveraging sequence-aware offloading, and two-level\nactivation management to reduce GPU memory consumption without degrading the\ntraining efficiency. Additionally, SPPO develops an adaptive pipeline\nscheduling approach with a heuristic solver and multiplexed sequence\npartitioning to improve computational resource efficiency. Experimental results\ndemonstrate that SPPO achieves up to 3.38x throughput improvement over\nMegatron-LM and DeepSpeed, realizing efficient training of a 7B LLM with\nsequence lengths of up to 4M tokens on only 128 A100 GPUs.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 13:55:22 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chen', 'Qiaoling', ''], ['Li', 'Shenggui', ''], ['Gao', 'Wei', ''], ['Sun', 'Peng', ''], ['Wen', 'Yonggang', ''], ['Zhang', 'Tianwei', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.10391,"submitter":"Yufan Deng","authors":"Yufan Deng, Xun Guo, Yizhi Wang, Jacob Zhiyuan Fang, Angtian Wang,\n  Shenghai Yuan, Yiding Yang, Bo Liu, Haibin Huang, Chongyang Ma","title":"CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Video generation has witnessed remarkable progress with the advent of deep\ngenerative models, particularly diffusion models. While existing methods excel\nin generating high-quality videos from text prompts or single images,\npersonalized multi-subject video generation remains a largely unexplored\nchallenge. This task involves synthesizing videos that incorporate multiple\ndistinct subjects, each defined by separate reference images, while ensuring\ntemporal and spatial consistency. Current approaches primarily rely on mapping\nsubject images to keywords in text prompts, which introduces ambiguity and\nlimits their ability to model subject relationships effectively. In this paper,\nwe propose CINEMA, a novel framework for coherent multi-subject video\ngeneration by leveraging Multimodal Large Language Model (MLLM). Our approach\neliminates the need for explicit correspondences between subject images and\ntext entities, mitigating ambiguity and reducing annotation effort. By\nleveraging MLLM to interpret subject relationships, our method facilitates\nscalability, enabling the use of large and diverse datasets for training.\nFurthermore, our framework can be conditioned on varying numbers of subjects,\noffering greater flexibility in personalized content creation. Through\nextensive evaluations, we demonstrate that our approach significantly improves\nsubject consistency, and overall video coherence, paving the way for advanced\napplications in storytelling, interactive media, and personalized video\ngeneration.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 14:07:58 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Deng', 'Yufan', ''], ['Guo', 'Xun', ''], ['Wang', 'Yizhi', ''], ['Fang', 'Jacob Zhiyuan', ''], ['Wang', 'Angtian', ''], ['Yuan', 'Shenghai', ''], ['Yang', 'Yiding', ''], ['Liu', 'Bo', ''], ['Huang', 'Haibin', ''], ['Ma', 'Chongyang', '']]","extracted_entities":"[{'text': 'text prompts', 'label': 'Prompting'}, {'text': 'text prompts', 'label': 'Prompting'}, {'text': 'Multimodal Large Language Model', 'label': 'Large Language Model'}, {'text': 'MLLM', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Model","similarity_score":0.7924776673}
{"id":2503.10406,"submitter":"Yijing Lin","authors":"Yijing Lin, Mengqi Huang, Shuhan Zhuang, Zhendong Mao","title":"RealGeneral: Unifying Visual Generation via Temporal In-Context Learning\n  with Video Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Unifying diverse image generation tasks within a single framework remains a\nfundamental challenge in visual generation. While large language models (LLMs)\nachieve unification through task-agnostic data and generation, existing visual\ngeneration models fail to meet these principles. Current approaches either rely\non per-task datasets and large-scale training or adapt pre-trained image models\nwith task-specific modifications, limiting their generalizability. In this\nwork, we explore video models as a foundation for unified image generation,\nleveraging their inherent ability to model temporal correlations. We introduce\nRealGeneral, a novel framework that reformulates image generation as a\nconditional frame prediction task, analogous to in-context learning in LLMs. To\nbridge the gap between video models and condition-image pairs, we propose (1) a\nUnified Conditional Embedding module for multi-modal alignment and (2) a\nUnified Stream DiT Block with decoupled adaptive LayerNorm and attention mask\nto mitigate cross-modal interference. RealGeneral demonstrates effectiveness in\nmultiple important visual generation tasks, e.g., it achieves a 14.5%\nimprovement in subject similarity for customized generation and a 10%\nenhancement in image quality for canny-to-image task. Project page:\nhttps:\/\/lyne1.github.io\/RealGeneral\/\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 14:31:52 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Lin', 'Yijing', ''], ['Huang', 'Mengqi', ''], ['Zhuang', 'Shuhan', ''], ['Mao', 'Zhendong', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'video models', 'label': 'Large Language Model'}, {'text': 'RealGeneral', 'label': 'Open-source LLMs'}, {'text': 'in-context learning', 'label': 'Few-shot Learning'}, {'text': 'video models', 'label': 'Large Language Model'}, {'text': 'Unified Conditional Embedding', 'label': 'Embedding'}, {'text': 'attention mask', 'label': 'Zero-shot Learning'}, {'text': 'RealGeneral', 'label': 'Foundation Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.10432,"submitter":"Can Zheng","authors":"Can Zheng, Jiguang He, Guofa Cai, Zitong Yu, Chung G. Kang","title":"BeamLLM: Vision-Empowered mmWave Beam Prediction with Large Language\n  Models","comments":"6 pages, 7 figures, conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this paper, we propose BeamLLM, a vision-aided millimeter-wave (mmWave)\nbeam prediction framework leveraging large language models (LLMs) to address\nthe challenges of high training overhead and latency in mmWave communication\nsystems. By combining computer vision (CV) with LLMs' cross-modal reasoning\ncapabilities, the framework extracts user equipment (UE) positional features\nfrom RGB images and aligns visual-temporal features with LLMs' semantic space\nthrough reprogramming techniques. Evaluated on a realistic\nvehicle-to-infrastructure (V2I) scenario, the proposed method achieves 61.01%\ntop-1 accuracy and 97.39% top-3 accuracy in standard prediction tasks,\nsignificantly outperforming traditional deep learning models. In few-shot\nprediction scenarios, the performance degradation is limited to 12.56% (top-1)\nand 5.55% (top-3) from time sample 1 to 10, demonstrating superior prediction\ncapability.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 14:55:59 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zheng', 'Can', ''], ['He', 'Jiguang', ''], ['Cai', 'Guofa', ''], ['Yu', 'Zitong', ''], ['Kang', 'Chung G.', '']]","extracted_entities":"[{'text': 'BeamLLM', 'label': 'LLM'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'few-shot\\nprediction scenarios', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.10437,"submitter":"Wanhua Li","authors":"Wanhua Li, Renping Zhou, Jiawei Zhou, Yingwei Song, Johannes Herter,\n  Minghan Qin, Gao Huang, Hanspeter Pfister","title":"4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large\n  Language Models","comments":"CVPR 2025. Project Page: https:\/\/4d-langsplat.github.io","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Learning 4D language fields to enable time-sensitive, open-ended language\nqueries in dynamic scenes is essential for many real-world applications. While\nLangSplat successfully grounds CLIP features into 3D Gaussian representations,\nachieving precision and efficiency in 3D static scenes, it lacks the ability to\nhandle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot\ncapture temporal dynamics in videos. Real-world environments are inherently\ndynamic, with object semantics evolving over time. Building a precise 4D\nlanguage field necessitates obtaining pixel-aligned, object-wise video\nfeatures, which current vision models struggle to achieve. To address these\nchallenges, we propose 4D LangSplat, which learns 4D language fields to handle\ntime-agnostic or time-sensitive open-vocabulary queries in dynamic scenes\nefficiently. 4D LangSplat bypasses learning the language field from vision\nfeatures and instead learns directly from text generated from object-wise video\ncaptions via Multimodal Large Language Models (MLLMs). Specifically, we propose\na multimodal object-wise video prompting method, consisting of visual and text\nprompts that guide MLLMs to generate detailed, temporally consistent,\nhigh-quality captions for objects throughout a video. These captions are\nencoded using a Large Language Model into high-quality sentence embeddings,\nwhich then serve as pixel-aligned, object-specific feature supervision,\nfacilitating open-vocabulary text queries through shared embedding spaces.\nRecognizing that objects in 4D scenes exhibit smooth transitions across states,\nwe further propose a status deformable network to model these continuous\nchanges over time effectively. Our results across multiple benchmarks\ndemonstrate that 4D LangSplat attains precise and efficient results for both\ntime-sensitive and time-agnostic open-vocabulary queries.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 14:58:22 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Wanhua', ''], ['Zhou', 'Renping', ''], ['Zhou', 'Jiawei', ''], ['Song', 'Yingwei', ''], ['Herter', 'Johannes', ''], ['Qin', 'Minghan', ''], ['Huang', 'Gao', ''], ['Pfister', 'Hanspeter', '']]","extracted_entities":"[{'text': '3D Gaussian representations', 'label': 'Embedding'}, {'text': 'visual and text\\nprompts', 'label': 'Prompting'}, {'text': 'Large Language Model', 'label': 'Large Language Model'}, {'text': 'high-quality sentence embeddings', 'label': 'Embedding'}, {'text': 'shared embedding spaces', 'label': 'contextual Embedding'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Model","similarity_score":1.0}
{"id":2503.10497,"submitter":"Weihao Xuan","authors":"Weihao Xuan, Rui Yang, Heli Qi, Qingcheng Zeng, Yunze Xiao, Yun Xing,\n  Junjue Wang, Huitao Li, Xin Li, Kunyu Yu, Nan Liu, Qingyu Chen, Douglas\n  Teodoro, Edison Marrese-Taylor, Shijian Lu, Yusuke Iwasawa, Yutaka Matsuo,\n  Irene Li","title":"MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model\n  Evaluation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Traditional benchmarks struggle to evaluate increasingly sophisticated\nlanguage models in multilingual and culturally diverse contexts. To address\nthis gap, we introduce MMLU-ProX, a comprehensive multilingual benchmark\ncovering 13 typologically diverse languages with approximately 11,829 questions\nper language. Building on the challenging reasoning-focused design of MMLU-Pro,\nour framework employs a semi-automatic translation process: translations\ngenerated by state-of-the-art large language models (LLMs) are rigorously\nevaluated by expert annotators to ensure conceptual accuracy, terminological\nconsistency, and cultural relevance. We comprehensively evaluate 25\nstate-of-the-art LLMs using 5-shot chain-of-thought (CoT) and zero-shot\nprompting strategies, analyzing their performance across linguistic and\ncultural boundaries. Our experiments reveal consistent performance degradation\nfrom high-resource languages to lower-resource ones, with the best models\nachieving over 70% accuracy on English but dropping to around 40% for languages\nlike Swahili, highlighting persistent gaps in multilingual capabilities despite\nrecent advances. MMLU-ProX is an ongoing project; we are expanding our\nbenchmark by incorporating additional languages and evaluating more language\nmodels to provide a more comprehensive assessment of multilingual capabilities.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 15:59:20 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Xuan', 'Weihao', ''], ['Yang', 'Rui', ''], ['Qi', 'Heli', ''], ['Zeng', 'Qingcheng', ''], ['Xiao', 'Yunze', ''], ['Xing', 'Yun', ''], ['Wang', 'Junjue', ''], ['Li', 'Huitao', ''], ['Li', 'Xin', ''], ['Yu', 'Kunyu', ''], ['Liu', 'Nan', ''], ['Chen', 'Qingyu', ''], ['Teodoro', 'Douglas', ''], ['Marrese-Taylor', 'Edison', ''], ['Lu', 'Shijian', ''], ['Iwasawa', 'Yusuke', ''], ['Matsuo', 'Yutaka', ''], ['Li', 'Irene', '']]","extracted_entities":"[{'text': 'state-of-the-art large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': '5-shot chain-of-thought (CoT)', 'label': 'Chain of thought'}, {'text': 'zero-shot\\nprompting strategies', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"state-of-the-art large language models","similarity_score":0.8680342436}
{"id":2503.10501,"submitter":"Xudong Tan","authors":"Xudong Tan, Peng Ye, Chongjun Tu, Jianjian Cao, Yaoxin Yang, Lin\n  Zhang, Dongzhan Zhou, Tao Chen","title":"TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps:\/\/github.com\/ShawnTan86\/TokenCarve.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 16:04:31 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Tan', 'Xudong', ''], ['Ye', 'Peng', ''], ['Tu', 'Chongjun', ''], ['Cao', 'Jianjian', ''], ['Yang', 'Yaoxin', ''], ['Zhang', 'Lin', ''], ['Zhou', 'Dongzhan', ''], ['Chen', 'Tao', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2503.10515,"submitter":"Florian Eichin","authors":"Florian Eichin, Yang Janet Liu, Barbara Plank, Michael A. Hedderich","title":"Probing LLMs for Multilingual Discourse Generalization Through a Unified\n  Label Set","comments":"18 pages, 7 figures, 3 tables, code:\n  https:\/\/github.com\/mainlp\/discourse_probes","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Discourse understanding is essential for many NLP tasks, yet most existing\nwork remains constrained by framework-dependent discourse representations. This\nwork investigates whether large language models (LLMs) capture discourse\nknowledge that generalizes across languages and frameworks. We address this\nquestion along two dimensions: (1) developing a unified discourse relation\nlabel set to facilitate cross-lingual and cross-framework discourse analysis,\nand (2) probing LLMs to assess whether they encode generalizable discourse\nabstractions. Using multilingual discourse relation classification as a\ntestbed, we examine a comprehensive set of 23 LLMs of varying sizes and\nmultilingual capabilities. Our results show that LLMs, especially those with\nmultilingual training corpora, can generalize discourse information across\nlanguages and frameworks. Further layer-wise analyses reveal that language\ngeneralization at the discourse level is most salient in the intermediate\nlayers. Lastly, our error analysis provides an account of challenging relation\nclasses.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 16:20:25 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Eichin', 'Florian', ''], ['Liu', 'Yang Janet', ''], ['Plank', 'Barbara', ''], ['Hedderich', 'Michael A.', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.10529,"submitter":"Zilu Guo","authors":"Zilu Guo, Hongbin Lin, Zhihao Yuan, Chaoda Zheng, Pengshuo Qiu,\n  Dongzhi Jiang, Renrui Zhang, Chun-Mei Feng, Zhen Li","title":"PiSA: A Self-Augmented Data Engine and Training Strategy for 3D\n  Understanding with Large Models","comments":"Technical Report","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  3D Multimodal Large Language Models (MLLMs) have recently made substantial\nadvancements. However, their potential remains untapped, primarily due to the\nlimited quantity and suboptimal quality of 3D datasets. Current approaches\nattempt to transfer knowledge from 2D MLLMs to expand 3D instruction data, but\nstill face modality and domain gaps. To this end, we introduce PiSA-Engine\n(Point-Self-Augmented-Engine), a new framework for generating instruction\npoint-language datasets enriched with 3D spatial semantics. We observe that\nexisting 3D MLLMs offer a comprehensive understanding of point clouds for\nannotation, while 2D MLLMs excel at cross-validation by providing complementary\ninformation. By integrating holistic 2D and 3D insights from off-the-shelf\nMLLMs, PiSA-Engine enables a continuous cycle of high-quality data generation.\nWe select PointLLM as the baseline and adopt this co-evolution training\nframework to develop an enhanced 3D MLLM, termed PointLLM-PiSA. Additionally,\nwe identify limitations in previous 3D benchmarks, which often feature coarse\nlanguage captions and insufficient category diversity, resulting in inaccurate\nevaluations. To address this gap, we further introduce PiSA-Bench, a\ncomprehensive 3D benchmark covering six key aspects with detailed and diverse\nlabels. Experimental results demonstrate PointLLM-PiSA's state-of-the-art\nperformance in zero-shot 3D object captioning and generative classification on\nour PiSA-Bench, achieving significant improvements of 46.45% (+8.33%) and\n63.75% (+16.25%), respectively. We will release the code, datasets, and\nbenchmark.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 16:37:26 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Guo', 'Zilu', ''], ['Lin', 'Hongbin', ''], ['Yuan', 'Zhihao', ''], ['Zheng', 'Chaoda', ''], ['Qiu', 'Pengshuo', ''], ['Jiang', 'Dongzhi', ''], ['Zhang', 'Renrui', ''], ['Feng', 'Chun-Mei', ''], ['Li', 'Zhen', '']]","extracted_entities":"[{'text': '3D Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"3D Multimodal Large Language Models","similarity_score":0.7165836692}
{"id":2503.10546,"submitter":"Mingtong Zhang","authors":"Zixian Liu, Mingtong Zhang, Yunzhu Li","title":"KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation","comments":"Project website: http:\/\/kuda-dynamics.github.io","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  With the rapid advancement of large language models (LLMs) and\nvision-language models (VLMs), significant progress has been made in developing\nopen-vocabulary robotic manipulation systems. However, many existing approaches\noverlook the importance of object dynamics, limiting their applicability to\nmore complex, dynamic tasks. In this work, we introduce KUDA, an\nopen-vocabulary manipulation system that integrates dynamics learning and\nvisual prompting through keypoints, leveraging both VLMs and learning-based\nneural dynamics models. Our key insight is that a keypoint-based target\nspecification is simultaneously interpretable by VLMs and can be efficiently\ntranslated into cost functions for model-based planning. Given language\ninstructions and visual observations, KUDA first assigns keypoints to the RGB\nimage and queries the VLM to generate target specifications. These abstract\nkeypoint-based representations are then converted into cost functions, which\nare optimized using a learned dynamics model to produce robotic trajectories.\nWe evaluate KUDA on a range of manipulation tasks, including free-form language\ninstructions across diverse object categories, multi-object interactions, and\ndeformable or granular objects, demonstrating the effectiveness of our\nframework. The project page is available at http:\/\/kuda-dynamics.github.io.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 16:59:17 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Liu', 'Zixian', ''], ['Zhang', 'Mingtong', ''], ['Li', 'Yunzhu', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'dynamics learning', 'label': 'Few-shot Learning'}, {'text': 'visual prompting', 'label': 'Prompting'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'learning-based\\nneural dynamics models', 'label': 'Neural Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.10602,"submitter":"Jinhao Duan","authors":"Jinhao Duan, Fei Kong, Hao Cheng, James Diffenderfer, Bhavya\n  Kailkhura, Lichao Sun, Xiaofeng Zhu, Xiaoshuang Shi, Kaidi Xu","title":"TruthPrInt: Mitigating LVLM Object Hallucination Via Latent\n  Truthful-Guided Pre-Intervention","comments":"15 pages, 9 figures, the first two authors contributed equally","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Object Hallucination (OH) has been acknowledged as one of the major\ntrustworthy challenges in Large Vision-Language Models (LVLMs). Recent\nadvancements in Large Language Models (LLMs) indicate that internal states,\nsuch as hidden states, encode the \"overall truthfulness\" of generated\nresponses. However, it remains under-explored how internal states in LVLMs\nfunction and whether they could serve as \"per-token\" hallucination indicators,\nwhich is essential for mitigating OH. In this paper, we first conduct an\nin-depth exploration of LVLM internal states in relation to OH issues and\ndiscover that (1) LVLM internal states are high-specificity per-token\nindicators of hallucination behaviors. Moreover, (2) different LVLMs encode\nuniversal patterns of hallucinations in common latent subspaces, indicating\nthat there exist \"generic truthful directions\" shared by various LVLMs. Based\non these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt)\nthat first learns the truthful direction of LVLM decoding and then applies\ntruthful-guided inference-time intervention during LVLM decoding. We further\npropose ComnHallu to enhance both cross-LVLM and cross-data hallucination\ndetection transferability by constructing and aligning hallucination latent\nsubspaces. We evaluate TruthPrInt in extensive experimental settings, including\nin-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks.\nExperimental results indicate that TruthPrInt significantly outperforms\nstate-of-the-art methods. Codes will be available at\nhttps:\/\/github.com\/jinhaoduan\/TruthPrInt.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:46:06 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Duan', 'Jinhao', ''], ['Kong', 'Fei', ''], ['Cheng', 'Hao', ''], ['Diffenderfer', 'James', ''], ['Kailkhura', 'Bhavya', ''], ['Sun', 'Lichao', ''], ['Zhu', 'Xiaofeng', ''], ['Shi', 'Xiaoshuang', ''], ['Xu', 'Kaidi', '']]","extracted_entities":"[{'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'ComnHallu', 'label': 'Llama'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.10613,"submitter":"Advait Gupta","authors":"Advait Gupta, NandaKiran Velaga, Dang Nguyen, Tianyi Zhou","title":"CoSTA$\\ast$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Text-to-image models like stable diffusion and DALLE-3 still struggle with\nmulti-turn image editing. We decompose such a task as an agentic workflow\n(path) of tool use that addresses a sequence of subtasks by AI tools of varying\ncosts. Conventional search algorithms require expensive exploration to find\ntool paths. While large language models (LLMs) possess prior knowledge of\nsubtask planning, they may lack accurate estimations of capabilities and costs\nof tools to determine which to apply in each subtask. Can we combine the\nstrengths of both LLMs and graph search to find cost-efficient tool paths? We\npropose a three-stage approach \"CoSTA*\" that leverages LLMs to create a subtask\ntree, which helps prune a graph of AI tools for the given task, and then\nconducts A* search on the small subgraph to find a tool path. To better balance\nthe total cost and quality, CoSTA* combines both metrics of each tool on every\nsubtask to guide the A* search. Each subtask's output is then evaluated by a\nvision-language model (VLM), where a failure will trigger an update of the\ntool's cost and quality on the subtask. Hence, the A* search can recover from\nfailures quickly to explore other paths. Moreover, CoSTA* can automatically\nswitch between modalities across subtasks for a better cost-quality trade-off.\nWe build a novel benchmark of challenging multi-turn image editing, on which\nCoSTA* outperforms state-of-the-art image-editing models or agents in terms of\nboth cost and quality, and performs versatile trade-offs upon user preference.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:55:45 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Gupta', 'Advait', ''], ['Velaga', 'NandaKiran', ''], ['Nguyen', 'Dang', ''], ['Zhou', 'Tianyi', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.07586,"submitter":"JaeWon Kim","authors":"JaeWon Kim, Jiaying \"Lizzy\" Liu, Cassidy Pyle, Sowmya Somanath,\n  Lindsay Popowski, Hua Shen, Casey Fiesler, Gillian R. Hayes, Alexis Hiniker,\n  Wendy Ju, Florian \"Floyd\" Mueller, Ahmer Arif, Yasmine Kotturi","title":"Design as Hope: Reimagining Futures for Seemingly Doomed Problems","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Design has the power to cultivate hope, especially in the face of seemingly\nintractable societal challenges. This one-day workshop explores how design\nmethodologies -- ranging from problem reframing to participatory, speculative,\nand critical design -- can empower research communities to drive meaningful\nreal-world changes. By aligning design thinking with hope theory -- framework\nof viewing hope as \"goal-directed,\" \"pathways,\" and \"agentic\" thinking\nprocesses -- we aim to examine how researchers can move beyond focusing on harm\nmitigation and instead reimagine alternative futures. Through hands-on\nactivities, participants will engage in problem reframing, develop a taxonomy\nof design methods related to hope, and explore how community-driven design\napproaches can sustain efforts toward societal and individual hope. The\nworkshop also interrogates the ethical and practical boundaries of leveraging\nhope in design research. By the end of the session, participants will leave\nwith concrete strategies for integrating a hopeful design approach into their\nresearch, as well as a network for ongoing collaboration. Ultimately, we\nposition hopeful design not just as a practical tool for action and\nproblem-solving but as a catalyst for cultivating resilience and envisioning\ntransformative futures.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Mar 2025 17:49:10 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 11:27:00 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Kim', 'JaeWon', ''], ['Liu', 'Jiaying \"Lizzy\"', ''], ['Pyle', 'Cassidy', ''], ['Somanath', 'Sowmya', ''], ['Popowski', 'Lindsay', ''], ['Shen', 'Hua', ''], ['Fiesler', 'Casey', ''], ['Hayes', 'Gillian R.', ''], ['Hiniker', 'Alexis', ''], ['Ju', 'Wendy', ''], ['Mueller', 'Florian \"Floyd\"', ''], ['Arif', 'Ahmer', ''], ['Kotturi', 'Yasmine', '']]","extracted_entities":"[{'text': 'ethical and practical boundaries', 'label': 'AI Ethics'}]","assigned_concept":"AI Ethics","matched_keyword":"ethical and practical boundaries","similarity_score":0.5279071331}
{"id":2503.09969,"submitter":"Nathan Drenkow","authors":"Nathan Drenkow and Mitchell Pavlak and Keith Harrigian and Ayah\n  Zirikly and Adarsh Subbaswamy and Mathias Unberath","title":"Detecting Dataset Bias in Medical AI: A Generalized and\n  Modality-Agnostic Auditing Framework","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Data-driven AI is establishing itself at the center of evidence-based\nmedicine. However, reports of shortcomings and unexpected behavior are growing\ndue to AI's reliance on association-based learning. A major reason for this\nbehavior: latent bias in machine learning datasets can be amplified during\ntraining and\/or hidden during testing. We present a data modality-agnostic\nauditing framework for generating targeted hypotheses about sources of bias\nwhich we refer to as Generalized Attribute Utility and Detectability-Induced\nbias Testing (G-AUDIT) for datasets. Our method examines the relationship\nbetween task-level annotations and data properties including protected\nattributes (e.g., race, age, sex) and environment and acquisition\ncharacteristics (e.g., clinical site, imaging protocols). G-AUDIT automatically\nquantifies the extent to which the observed data attributes may enable shortcut\nlearning, or in the case of testing data, hide predictions made based on\nspurious associations. We demonstrate the broad applicability and value of our\nmethod by analyzing large-scale medical datasets for three distinct modalities\nand learning tasks: skin lesion classification in images, stigmatizing language\nclassification in Electronic Health Records (EHR), and mortality prediction for\nICU tabular data. In each setting, G-AUDIT successfully identifies subtle\nbiases commonly overlooked by traditional qualitative methods that focus\nprimarily on social and ethical objectives, underscoring its practical value in\nexposing dataset-level risks and supporting the downstream development of\nreliable AI systems. Our method paves the way for achieving deeper\nunderstanding of machine learning datasets throughout the AI development\nlife-cycle from initial prototyping all the way to regulation, and creates\nopportunities to reduce model bias, enabling safer and more trustworthy AI\nsystems.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 02:16:48 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Drenkow', 'Nathan', ''], ['Pavlak', 'Mitchell', ''], ['Harrigian', 'Keith', ''], ['Zirikly', 'Ayah', ''], ['Subbaswamy', 'Adarsh', ''], ['Unberath', 'Mathias', '']]","extracted_entities":"[{'text': 'association-based learning', 'label': 'Few-shot Learning'}, {'text': 'shortcut\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'social and ethical objectives', 'label': 'AI Ethics'}]","assigned_concept":"AI Ethics","matched_keyword":"social and ethical objectives","similarity_score":0.5102629066}
{"id":2503.09987,"submitter":"Jie Li","authors":"Jie Li, Anusha Withana, Alexandra Diening, Kai Kunze, Masahiko Inami","title":"Beyond Human: Cognitive and Physical Augmentation through AI, Robotics,\n  and XR -- Opportunities and Risks","comments":"Workshop at the Augmented Humans (AHs) International Conference 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC cs.ET","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  As human augmentation technologies evolve, the convergence of AI, robotics,\nand extended reality (XR) is redefining human potential -- enhancing cognition,\nperception, and physical abilities. However, these advancements also introduce\nethical dilemmas, security risks, and concerns over loss of control. This\nworkshop explores both the transformative potential and the unintended\nconsequences of augmentation technologies. Bringing together experts from HCI,\nneuroscience, robotics, and ethics, we will examine real-world applications,\nemerging risks, and governance strategies for responsible augmentation. The\nsession will feature keynote talks and interactive discussions, addressing\ntopics such as AI-enhanced cognition, wearable robotics, neural interfaces, and\nXR-driven augmentation. By fostering multidisciplinary dialogue, this workshop\naims to generate actionable insights for responsible innovation, proposing\nethical frameworks to balance human empowerment with risk mitigation. We invite\nresearchers, practitioners, and industry leaders to contribute their\nperspectives and help shape the future of human augmentation.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 02:53:11 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Jie', ''], ['Withana', 'Anusha', ''], ['Diening', 'Alexandra', ''], ['Kunze', 'Kai', ''], ['Inami', 'Masahiko', '']]","extracted_entities":"[{'text': 'ethics', 'label': 'AI Ethics'}]","assigned_concept":"AI Ethics","matched_keyword":"ethics","similarity_score":0.716448009}
{"id":2503.10242,"submitter":"Shaun Khoo","authors":"Shaun Khoo, Gabriel Chua, Rachel Shong","title":"MinorBench: A hand-built benchmark for content-based risks for children","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) are rapidly entering children's lives - through\nparent-driven adoption, schools, and peer networks - yet current AI ethics and\nsafety research do not adequately address content-related risks specific to\nminors. In this paper, we highlight these gaps with a real-world case study of\nan LLM-based chatbot deployed in a middle school setting, revealing how\nstudents used and sometimes misused the system. Building on these findings, we\npropose a new taxonomy of content-based risks for minors and introduce\nMinorBench, an open-source benchmark designed to evaluate LLMs on their ability\nto refuse unsafe or inappropriate queries from children. We evaluate six\nprominent LLMs under different system prompts, demonstrating substantial\nvariability in their child-safety compliance. Our results inform practical\nsteps for more robust, child-focused safety mechanisms and underscore the\nurgency of tailoring AI systems to safeguard young users.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 10:34:43 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Khoo', 'Shaun', ''], ['Chua', 'Gabriel', ''], ['Shong', 'Rachel', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'AI ethics', 'label': 'AI Ethics'}, {'text': 'LLM-based chatbot', 'label': 'ChatGPT'}, {'text': 'MinorBench', 'label': 'Open-source LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'system prompts', 'label': 'Prompting'}]","assigned_concept":"AI Ethics","matched_keyword":"AI ethics","similarity_score":1.0}
{"id":2503.02597,"submitter":"Wei-Yao Wang","authors":"Wei-Yao Wang, Zhao Wang, Helen Suzuki, Yoshiyuki Kobayashi","title":"Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual\n  Attention for Multimodal LLMs","comments":"Preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in perceiving and reasoning over multimodal inquiries, ushering in a\nnew research era for foundation models. However, vision-language misalignment\nin MLLMs has emerged as a critical challenge, where the textual responses\ngenerated by these models are not factually aligned with the given text-image\ninputs. Existing efforts to address vision-language misalignment have focused\non developing specialized vision-language connectors or leveraging visual\ninstruction tuning from diverse domains. In this paper, we tackle this issue\nfrom a fundamental yet unexplored perspective by revisiting the core\narchitecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs\nconsisting of a causal attention mechanism, which limits the ability of the\nearlier modalities (e.g., images) to incorporate information from the latter\nmodalities (e.g., text). To address this problem, we propose \\MapleLeaf AKI, a\nnovel MLLM that unlocks causal attention into modality-mutual attention (MMA)\nto enable image tokens to attend to text tokens. This simple yet effective\ndesign allows AKI to achieve superior performance in 12 multimodal\nunderstanding benchmarks (+7.2% on average) without introducing additional\nparameters and increasing training time. Our MMA design is intended to be\ngeneric, allowing for application across various modalities, and scalable to\naccommodate diverse multimodal scenarios. The code and model are publicly\navailable at https:\/\/github.com\/sony\/aki to encourage further advancements in\nMLLMs across various directions.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 13:18:33 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 01:48:08 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Wei-Yao', ''], ['Wang', 'Zhao', ''], ['Suzuki', 'Helen', ''], ['Kobayashi', 'Yoshiyuki', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'visual\\ninstruction tuning', 'label': 'Fine-tuning'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'decoder-only LLMs', 'label': 'LLMs'}, {'text': 'causal attention mechanism', 'label': 'Attention mechanism'}, {'text': 'causal attention', 'label': 'Attention mechanism'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2503.0916,"submitter":"Hao Feng","authors":"Hao Feng and Zhi Zuo and Jia-Hui Pan and Ka-Hei Hui and Yihua Shao and\n  Qi Dou and Wei Xie and Zhengzhe Liu","title":"WonderVerse: Extendable 3D Scene Generation with Video Generative Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We introduce \\textit{WonderVerse}, a simple but effective framework for\ngenerating extendable 3D scenes. Unlike existing methods that rely on iterative\ndepth estimation and image inpainting, often leading to geometric distortions\nand inconsistencies, WonderVerse leverages the powerful world-level priors\nembedded within video generative foundation models to create highly immersive\nand geometrically coherent 3D environments. Furthermore, we propose a new\ntechnique for controllable 3D scene extension to substantially increase the\nscale of the generated environments. Besides, we introduce a novel abnormal\nsequence detection module that utilizes camera trajectory to address geometric\ninconsistency in the generated videos. Finally, WonderVerse is compatible with\nvarious 3D reconstruction methods, allowing both efficient and high-quality\ngeneration. Extensive experiments on 3D scene generation demonstrate that our\nWonderVerse, with an elegant and simple pipeline, delivers extendable and\nhighly-realistic 3D scenes, markedly outperforming existing works that rely on\nmore complex architectures.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 08:44:51 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 15:29:28 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Feng', 'Hao', ''], ['Zuo', 'Zhi', ''], ['Pan', 'Jia-Hui', ''], ['Hui', 'Ka-Hei', ''], ['Shao', 'Yihua', ''], ['Dou', 'Qi', ''], ['Xie', 'Wei', ''], ['Liu', 'Zhengzhe', '']]","extracted_entities":"[{'text': 'video generative foundation models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"video generative foundation models","similarity_score":0.5319663882}
{"id":2503.10215,"submitter":"Benjamin Heymann","authors":"Benjamin Heymann","title":"Adaptive Preference Aggregation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.GT","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  AI alignment, the challenge of ensuring AI systems act in accordance with\nhuman values, has emerged as a critical problem in the development of systems\nsuch as foundation models and recommender systems. Still, the current dominant\napproach, reinforcement learning with human feedback (RLHF) faces known\ntheoretical limitations in aggregating diverse human preferences. Social choice\ntheory provides a framework to aggregate preferences, but was not developed for\nthe multidimensional applications typical of AI. Leveraging insights from a\nrecently published urn process, this work introduces a preference aggregation\nstrategy that adapts to the user's context and that inherits the good\nproperties of the maximal lottery, a Condorcet-consistent solution concept.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 09:57:41 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Heymann', 'Benjamin', '']]","extracted_entities":"[{'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'reinforcement learning with human feedback', 'label': 'Few-shot Learning'}, {'text': 'Social choice\\ntheory', 'label': 'AI Ethics'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2503.10247,"submitter":"Zhijie Zhu","authors":"Zhijie Zhu, Lei Fan, Maurice Pagnucco, Yang Song","title":"Interpretable Image Classification via Non-parametric Part Prototype\n  Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Classifying images with an interpretable decision-making process is a\nlong-standing problem in computer vision. In recent years, Prototypical Part\nNetworks has gained traction as an approach for self-explainable neural\nnetworks, due to their ability to mimic human visual reasoning by providing\nexplanations based on prototypical object parts. However, the quality of the\nexplanations generated by these methods leaves room for improvement, as the\nprototypes usually focus on repetitive and redundant concepts. Leveraging\nrecent advances in prototype learning, we present a framework for part-based\ninterpretable image classification that learns a set of semantically\ndistinctive object parts for each class, and provides diverse and comprehensive\nexplanations. The core of our method is to learn the part-prototypes in a\nnon-parametric fashion, through clustering deep features extracted from\nfoundation vision models that encode robust semantic information. To\nquantitatively evaluate the quality of explanations provided by ProtoPNets, we\nintroduce Distinctiveness Score and Comprehensiveness Score. Through evaluation\non CUB-200-2011, Stanford Cars and Stanford Dogs datasets, we show that our\nframework compares favourably against existing ProtoPNets while achieving\nbetter interpretability. Code is available at:\nhttps:\/\/github.com\/zijizhu\/proto-non-param.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 10:46:53 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhu', 'Zhijie', ''], ['Fan', 'Lei', ''], ['Pagnucco', 'Maurice', ''], ['Song', 'Yang', '']]","extracted_entities":"[{'text': 'foundation vision models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation vision models","similarity_score":0.7662065029}
{"id":2503.10538,"submitter":"Teresa Head-Gordon","authors":"Eric C.-Y. Yuan, Yunsheng Liu, Junmin Chen, Peichen Zhong, Sanjeev\n  Raja, Tobias Kreiman, Santiago Vargas, Wenbin Xu, Martin Head-Gordon, Chao\n  Yang, Samuel M. Blau, Aditi Krishnapriyan, Teresa Head-Gordon","title":"Foundation Models for Atomistic Simulation of Chemistry and Materials","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.chem-ph","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Given the power of large language and large vision models, it is of profound\nand fundamental interest to ask if a foundational model based on data and\nparameter scaling laws and pre-training strategies is possible for learned\nsimulations of chemistry and materials. The scaling of large and diverse\ndatasets and highly expressive architectures for chemical and materials\nsciences should result in a foundation model that is more efficient and broadly\ntransferable, robust to out-of-distribution challenges, and easily fine-tuned\nto a variety of downstream observables, when compared to specific training from\nscratch on targeted applications in atomistic simulation. In this Perspective\nwe aim to cover the rapidly advancing field of machine learned interatomic\npotentials (MLIP), and to illustrate a path to create chemistry and materials\nMLIP foundation models at larger scale.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 16:52:12 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Yuan', 'Eric C. -Y.', ''], ['Liu', 'Yunsheng', ''], ['Chen', 'Junmin', ''], ['Zhong', 'Peichen', ''], ['Raja', 'Sanjeev', ''], ['Kreiman', 'Tobias', ''], ['Vargas', 'Santiago', ''], ['Xu', 'Wenbin', ''], ['Head-Gordon', 'Martin', ''], ['Yang', 'Chao', ''], ['Blau', 'Samuel M.', ''], ['Krishnapriyan', 'Aditi', ''], ['Head-Gordon', 'Teresa', '']]","extracted_entities":"[{'text': 'data and\\nparameter scaling laws', 'label': 'Scaling law'}, {'text': 'foundation model', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation model","similarity_score":1.0}
{"id":2312.10052,"submitter":"Zhongliang Zeng","authors":"Dongdong Li, Zhongliang Zeng, Zhe Wang, Hai Yang","title":"ESTformer: Transformer Utilizing Spatiotemporal Dependencies for\n  Electroencaphalogram Super-resolution","comments":"Accepted by Knowledge-Based Systems","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Towards practical applications of Electroencephalography (EEG), lightweight\nacquisition devices garner significant attention. However, EEG channel\nselection methods are commonly data-sensitive and cannot establish a unified\nsound paradigm for EEG acquisition devices. Through reverse conceptualisation,\nwe formulated EEG applications in an EEG super-resolution (SR) manner, but\nsuffered from high computation costs, extra interpolation bias, and few\ninsights into spatiotemporal dependency modelling. To this end, we propose\nESTformer, an EEG SR framework that utilises spatiotemporal dependencies based\non the transformer. ESTformer applies positional encoding methods and a\nmultihead self-attention mechanism to the space and time dimensions, which can\nlearn spatial structural correlations and temporal functional variations.\nESTformer, with the fixed mask strategy, adopts a mask token to upsample\nlow-resolution (LR) EEG data in the case of disturbance from mathematical\ninterpolation methods. On this basis, we designed various transformer blocks to\nconstruct a spatial interpolation module (SIM) and a temporal reconstruction\nmodule (TRM). Finally, ESTformer cascades the SIM and TRM to capture and model\nthe spatiotemporal dependencies for EEG SR with fidelity. Extensive\nexperimental results on two EEG datasets show the effectiveness of ESTformer\nagainst previous state-of-the-art methods, demonstrating the versatility of the\nTransformer for EEG SR tasks. The superiority of the SR data was verified in an\nEEG-based person identification and emotion recognition task, achieving a 2% to\n38% improvement compared with the LR data at different sampling scales.\n","versions":"[{'version': 'v1', 'created': 'Sun, 3 Dec 2023 12:26:32 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 07:17:58 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Dongdong', ''], ['Zeng', 'Zhongliang', ''], ['Wang', 'Zhe', ''], ['Yang', 'Hai', '']]","extracted_entities":"[{'text': 'multihead self-attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"multihead self-attention mechanism","similarity_score":0.7830716372}
{"id":2412.02171,"submitter":"Tianyi Wang","authors":"Tianyi Wang, Zichen Wang, Cong Wang, Yuanchao Shu, Ruilong Deng, Peng\n  Cheng, Jiming Chen (Zhejiang University, Hangzhou, China)","title":"Can't Slow me Down: Learning Robust and Hardware-Adaptive Object\n  Detectors against Latency Attacks for Edge Devices","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Object detection is a fundamental enabler for many real-time downstream\napplications such as autonomous driving, augmented reality and supply chain\nmanagement. However, the algorithmic backbone of neural networks is brittle to\nimperceptible perturbations in the system inputs, which were generally known as\nmisclassifying attacks. By targeting the real-time processing capability, a new\nclass of latency attacks are reported recently. They exploit new attack\nsurfaces in object detectors by creating a computational bottleneck in the\npost-processing module, that leads to cascading failure and puts the real-time\ndownstream tasks at risks. In this work, we take an initial attempt to defend\nagainst this attack via background-attentive adversarial training that is also\ncognizant of the underlying hardware capabilities. We first draw system-level\nconnections between latency attack and hardware capacity across heterogeneous\nGPU devices. Based on the particular adversarial behaviors, we utilize\nobjectness loss as a proxy and build background attention into the adversarial\ntraining pipeline, and achieve a reasonable balance between clean and robust\naccuracy. The extensive experiments demonstrate the defense effectiveness of\nrestoring real-time processing capability from $13$ FPS to $43$ FPS on Jetson\nOrin NX, with a better trade-off between the clean and robust accuracy.\n","versions":"[{'version': 'v1', 'created': 'Tue, 3 Dec 2024 05:00:26 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 07:31:19 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Tianyi', '', 'Zhejiang University, Hangzhou, China'], ['Wang', 'Zichen', '', 'Zhejiang University, Hangzhou, China'], ['Wang', 'Cong', '', 'Zhejiang University, Hangzhou, China'], ['Shu', 'Yuanchao', '', 'Zhejiang University, Hangzhou, China'], ['Deng', 'Ruilong', '', 'Zhejiang University, Hangzhou, China'], ['Cheng', 'Peng', '', 'Zhejiang University, Hangzhou, China'], ['Chen', 'Jiming', '', 'Zhejiang University, Hangzhou, China']]","extracted_entities":"[{'text': 'background-attentive adversarial training', 'label': 'Few-shot Learning'}, {'text': 'background attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"background attention","similarity_score":0.623593092}
{"id":2412.03021,"submitter":"Tianyu Chang","authors":"Tianyu Chang, Xiaohao Chen, Zhichao Wei, Xuanpu Zhang, Qing-Guo Chen,\n  Weihua Luo, Peipei Song and Xun Yang","title":"PEMF-VTO: Point-Enhanced Video Virtual Try-on via Mask-free Paradigm","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Video Virtual Try-on aims to seamlessly transfer a reference garment onto a\ntarget person in a video while preserving both visual fidelity and temporal\ncoherence. Existing methods typically rely on inpainting masks to define the\ntry-on area, enabling accurate garment transfer for simple scenes (e.g.,\nin-shop videos). However, these mask-based approaches struggle with complex\nreal-world scenarios, as overly large and inconsistent masks often destroy\nspatial-temporal information, leading to distorted results. Mask-free methods\nalleviate this issue but face challenges in accurately determining the try-on\narea, especially for videos with dynamic body movements. To address these\nlimitations, we propose PEMF-VTO, a novel Point-Enhanced Mask-Free Video\nVirtual Try-On framework that leverages sparse point alignments to explicitly\nguide garment transfer. Our key innovation is the introduction of\npoint-enhanced guidance, which provides flexible and reliable control over both\nspatial-level garment transfer and temporal-level video coherence.\nSpecifically, we design a Point-Enhanced Transformer (PET) with two core\ncomponents: Point-Enhanced Spatial Attention (PSA), which uses frame-cloth\npoint alignments to precisely guide garment transfer, and Point-Enhanced\nTemporal Attention (PTA), which leverages frame-frame point correspondences to\nenhance temporal coherence and ensure smooth transitions across frames.\nExtensive experiments demonstrate that our PEMF-VTO outperforms\nstate-of-the-art methods, generating more natural, coherent, and visually\nappealing try-on videos, particularly for challenging in-the-wild scenarios.\n","versions":"[{'version': 'v1', 'created': 'Wed, 4 Dec 2024 04:24:15 GMT'}, {'version': 'v2', 'created': 'Thu, 5 Dec 2024 02:57:24 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 14:22:12 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Chang', 'Tianyu', ''], ['Chen', 'Xiaohao', ''], ['Wei', 'Zhichao', ''], ['Zhang', 'Xuanpu', ''], ['Chen', 'Qing-Guo', ''], ['Luo', 'Weihua', ''], ['Song', 'Peipei', ''], ['Yang', 'Xun', '']]","extracted_entities":"[{'text': 'Point-Enhanced Spatial Attention', 'label': 'Attention mechanism'}, {'text': 'Point-Enhanced\\nTemporal Attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Point-Enhanced\nTemporal Attention","similarity_score":0.605589509}
{"id":2412.07589,"submitter":"Jianzong Wu","authors":"Jianzong Wu, Chao Tang, Jingbo Wang, Yanhong Zeng, Xiangtai Li, Yunhai\n  Tong","title":"DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for\n  Customized Manga Generation","comments":"[CVPR 2025] The project page is\n  https:\/\/jianzongwu.github.io\/projects\/diffsensei\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Story visualization, the task of creating visual narratives from textual\ndescriptions, has seen progress with text-to-image generation models. However,\nthese models often lack effective control over character appearances and\ninteractions, particularly in multi-character scenes. To address these\nlimitations, we propose a new task: \\textbf{customized manga generation} and\nintroduce \\textbf{DiffSensei}, an innovative framework specifically designed\nfor generating manga with dynamic multi-character control. DiffSensei\nintegrates a diffusion-based image generator with a multimodal large language\nmodel (MLLM) that acts as a text-compatible identity adapter. Our approach\nemploys masked cross-attention to seamlessly incorporate character features,\nenabling precise layout control without direct pixel transfer. Additionally,\nthe MLLM-based adapter adjusts character features to align with panel-specific\ntext cues, allowing flexible adjustments in character expressions, poses, and\nactions. We also introduce \\textbf{MangaZero}, a large-scale dataset tailored\nto this task, containing 43,264 manga pages and 427,147 annotated panels,\nsupporting the visualization of varied character interactions and movements\nacross sequential frames. Extensive experiments demonstrate that DiffSensei\noutperforms existing models, marking a significant advancement in manga\ngeneration by enabling text-adaptable character customization. The project page\nis https:\/\/jianzongwu.github.io\/projects\/diffsensei\/.\n","versions":"[{'version': 'v1', 'created': 'Tue, 10 Dec 2024 15:24:12 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 06:23:03 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wu', 'Jianzong', ''], ['Tang', 'Chao', ''], ['Wang', 'Jingbo', ''], ['Zeng', 'Yanhong', ''], ['Li', 'Xiangtai', ''], ['Tong', 'Yunhai', '']]","extracted_entities":"[{'text': 'MLLM', 'label': 'Large Language Model'}, {'text': 'masked cross-attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"masked cross-attention","similarity_score":0.6562166214}
{"id":2501.08137,"submitter":"Marcella Astrid","authors":"Marcella Astrid, Enjie Ghorbel, Djamila Aouada","title":"Audio-Visual Deepfake Detection With Local Temporal Inconsistencies","comments":"Accepted in ICASSP 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CR cs.MM cs.SD eess.AS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper proposes an audio-visual deepfake detection approach that aims to\ncapture fine-grained temporal inconsistencies between audio and visual\nmodalities. To achieve this, both architectural and data synthesis strategies\nare introduced. From an architectural perspective, a temporal distance map,\ncoupled with an attention mechanism, is designed to capture these\ninconsistencies while minimizing the impact of irrelevant temporal\nsubsequences. Moreover, we explore novel pseudo-fake generation techniques to\nsynthesize local inconsistencies. Our approach is evaluated against\nstate-of-the-art methods using the DFDC and FakeAVCeleb datasets, demonstrating\nits effectiveness in detecting audio-visual deepfakes.\n","versions":"[{'version': 'v1', 'created': 'Tue, 14 Jan 2025 14:15:10 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Jan 2025 09:14:14 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Mar 2025 10:22:54 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 11:02:33 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Astrid', 'Marcella', ''], ['Ghorbel', 'Enjie', ''], ['Aouada', 'Djamila', '']]","extracted_entities":"[{'text': 'attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanism","similarity_score":1.0}
{"id":2501.10736,"submitter":"Shanwen Wang","authors":"Shanwen Wang, Xin Sun, Changrui Chen, Danfeng Hong, Jungong Han","title":"Semi-supervised Semantic Segmentation for Remote Sensing Images via\n  Multi-scale Uncertainty Consistency and Cross-Teacher-Student Attention","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Semi-supervised learning offers an appealing solution for remote sensing (RS)\nimage segmentation to relieve the burden of labor-intensive pixel-level\nlabeling. However, RS images pose unique challenges, including rich multi-scale\nfeatures and high inter-class similarity. To address these problems, this paper\nproposes a novel semi-supervised Multi-Scale Uncertainty and\nCross-Teacher-Student Attention (MUCA) model for RS image semantic segmentation\ntasks. Specifically, MUCA constrains the consistency among feature maps at\ndifferent layers of the network by introducing a multi-scale uncertainty\nconsistency regularization. It improves the multi-scale learning capability of\nsemi-supervised algorithms on unlabeled data. Additionally, MUCA utilizes a\nCross-Teacher-Student attention mechanism to guide the student network, guiding\nthe student network to construct more discriminative feature representations\nthrough complementary features from the teacher network. This design\neffectively integrates weak and strong augmentations (WA and SA) to further\nboost segmentation performance. To verify the effectiveness of our model, we\nconduct extensive experiments on ISPRS-Potsdam and LoveDA datasets. The\nexperimental results show the superiority of our method over state-of-the-art\nsemi-supervised methods. Notably, our model excels in distinguishing highly\nsimilar objects, showcasing its potential for advancing semi-supervised RS\nimage segmentation tasks.\n","versions":"[{'version': 'v1', 'created': 'Sat, 18 Jan 2025 11:57:20 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 14:18:36 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Shanwen', ''], ['Sun', 'Xin', ''], ['Chen', 'Changrui', ''], ['Hong', 'Danfeng', ''], ['Han', 'Jungong', '']]","extracted_entities":"[{'text': 'Semi-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'Cross-Teacher-Student attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Cross-Teacher-Student attention mechanism","similarity_score":0.6716021299}
{"id":2503.04823,"submitter":"Yuheng Kuang","authors":"Yuheng Kuang, Zhengning Wang, Jianping Zhang, Zhenyu Shi, Yuding Zhang","title":"DA-STGCN: 4D Trajectory Prediction Based on Spatiotemporal Feature\n  Extraction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The importance of four-dimensional (4D) trajectory prediction within air\ntraffic management systems is on the rise. Key operations such as conflict\ndetection and resolution, aircraft anomaly monitoring, and the management of\ncongested flight paths are increasingly reliant on this foundational\ntechnology, underscoring the urgent demand for intelligent solutions. The\ndynamics in airport terminal zones and crowded airspaces are intricate and\never-changing; however, current methodologies do not sufficiently account for\nthe interactions among aircraft. To tackle these challenges, we propose\nDA-STGCN, an innovative spatiotemporal graph convolutional network that\nintegrates a dual attention mechanism. Our model reconstructs the adjacency\nmatrix through a self-attention approach, enhancing the capture of node\ncorrelations, and employs graph attention to distill spatiotemporal\ncharacteristics, thereby generating a probabilistic distribution of predicted\ntrajectories. This novel adjacency matrix, reconstructed with the\nself-attention mechanism, is dynamically optimized throughout the network's\ntraining process, offering a more nuanced reflection of the inter-node\nrelationships compared to traditional algorithms. The performance of the model\nis validated on two ADS-B datasets, one near the airport terminal area and the\nother in dense airspace. Experimental results demonstrate a notable improvement\nover current 4D trajectory prediction methods, achieving a 20% and 30%\nreduction in the Average Displacement Error (ADE) and Final Displacement Error\n(FDE), respectively. The incorporation of a Dual-Attention module has been\nshown to significantly enhance the extraction of node correlations, as verified\nby ablation experiments.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 03:42:49 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 03:39:44 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Kuang', 'Yuheng', ''], ['Wang', 'Zhengning', ''], ['Zhang', 'Jianping', ''], ['Shi', 'Zhenyu', ''], ['Zhang', 'Yuding', '']]","extracted_entities":"[{'text': 'dual attention mechanism', 'label': 'Attention mechanism'}, {'text': 'self-attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"self-attention mechanism","similarity_score":0.8757837415}
{"id":2503.07933,"submitter":"Yirui Wang","authors":"Qinji Yu, Yirui Wang, Ke Yan, Dandan Zheng, Dashan Ai, Dazhou Guo,\n  Zhanghexuan Ji, Yanzhou Su, Yun Bian, Na Shen, Xiaowei Ding, Le Lu, Xianghua\n  Ye, Dakai Jin","title":"From Slices to Sequences: Autoregressive Tracking Transformer for\n  Cohesive and Consistent 3D Lymph Node Detection in CT Scans","comments":"Technical report (11 pages plus supplementary)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Lymph node (LN) assessment is an essential task in the routine radiology\nworkflow, providing valuable insights for cancer staging, treatment planning\nand beyond. Identifying scatteredly-distributed and low-contrast LNs in 3D CT\nscans is highly challenging, even for experienced clinicians. Previous lesion\nand LN detection methods demonstrate effectiveness of 2.5D approaches (i.e,\nusing 2D network with multi-slice inputs), leveraging pretrained 2D model\nweights and showing improved accuracy as compared to separate 2D or 3D\ndetectors. However, slice-based 2.5D detectors do not explicitly model\ninter-slice consistency for LN as a 3D object, requiring heuristic post-merging\nsteps to generate final 3D LN instances, which can involve tuning a set of\nparameters for each dataset. In this work, we formulate 3D LN detection as a\ntracking task and propose LN-Tracker, a novel LN tracking transformer, for\njoint end-to-end detection and 3D instance association. Built upon DETR-based\ndetector, LN-Tracker decouples transformer decoder's query into the track and\ndetection groups, where the track query autoregressively follows previously\ntracked LN instances along the z-axis of a CT scan. We design a new transformer\ndecoder with masked attention module to align track query's content to the\ncontext of current slice, meanwhile preserving detection query's high accuracy\nin current slice. An inter-slice similarity loss is introduced to encourage\ncohesive LN association between slices. Extensive evaluation on four lymph node\ndatasets shows LN-Tracker's superior performance, with at least 2.7% gain in\naverage sensitivity when compared to other top 3D\/2.5D detectors. Further\nvalidation on public lung nodule and prostate tumor detection tasks confirms\nthe generalizability of LN-Tracker as it achieves top performance on both\ntasks.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 00:22:05 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 00:01:12 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Yu', 'Qinji', ''], ['Wang', 'Yirui', ''], ['Yan', 'Ke', ''], ['Zheng', 'Dandan', ''], ['Ai', 'Dashan', ''], ['Guo', 'Dazhou', ''], ['Ji', 'Zhanghexuan', ''], ['Su', 'Yanzhou', ''], ['Bian', 'Yun', ''], ['Shen', 'Na', ''], ['Ding', 'Xiaowei', ''], ['Lu', 'Le', ''], ['Ye', 'Xianghua', ''], ['Jin', 'Dakai', '']]","extracted_entities":"[{'text': 'masked attention module', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"masked attention module","similarity_score":0.58659935}
{"id":2503.0901,"submitter":"Jingkai Sun","authors":"Qiang Zhang, Zhang Zhang, Wei Cui, Jingkai Sun, Jiahang Cao, Yijie\n  Guo, Gang Han, Wen Zhao, Jiaxu Wang, Chenghao Sun, Lingfeng Zhang, Hao Cheng,\n  Yujie Chen, Lin Wang, Jian Tang, Renjing Xu","title":"HumanoidPano: Hybrid Spherical Panoramic-LiDAR Cross-Modal Perception\n  for Humanoid Robots","comments":"Technical Report","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The perceptual system design for humanoid robots poses unique challenges due\nto inherent structural constraints that cause severe self-occlusion and limited\nfield-of-view (FOV). We present HumanoidPano, a novel hybrid cross-modal\nperception framework that synergistically integrates panoramic vision and LiDAR\nsensing to overcome these limitations. Unlike conventional robot perception\nsystems that rely on monocular cameras or standard multi-sensor configurations,\nour method establishes geometrically-aware modality alignment through a\nspherical vision transformer, enabling seamless fusion of 360 visual context\nwith LiDAR's precise depth measurements. First, Spherical Geometry-aware\nConstraints (SGC) leverage panoramic camera ray properties to guide\ndistortion-regularized sampling offsets for geometric alignment. Second,\nSpatial Deformable Attention (SDA) aggregates hierarchical 3D features via\nspherical offsets, enabling efficient 360{\\deg}-to-BEV fusion with\ngeometrically complete object representations. Third, Panoramic Augmentation\n(AUG) combines cross-view transformations and semantic alignment to enhance\nBEV-panoramic feature consistency during data augmentation. Extensive\nevaluations demonstrate state-of-the-art performance on the 360BEV-Matterport\nbenchmark. Real-world deployment on humanoid platforms validates the system's\ncapability to generate accurate BEV segmentation maps through panoramic-LiDAR\nco-perception, directly enabling downstream navigation tasks in complex\nenvironments. Our work establishes a new paradigm for embodied perception in\nhumanoid robotics.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 02:59:21 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 03:42:53 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhang', 'Qiang', ''], ['Zhang', 'Zhang', ''], ['Cui', 'Wei', ''], ['Sun', 'Jingkai', ''], ['Cao', 'Jiahang', ''], ['Guo', 'Yijie', ''], ['Han', 'Gang', ''], ['Zhao', 'Wen', ''], ['Wang', 'Jiaxu', ''], ['Sun', 'Chenghao', ''], ['Zhang', 'Lingfeng', ''], ['Cheng', 'Hao', ''], ['Chen', 'Yujie', ''], ['Wang', 'Lin', ''], ['Tang', 'Jian', ''], ['Xu', 'Renjing', '']]","extracted_entities":"[{'text': 'Spatial Deformable Attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Spatial Deformable Attention","similarity_score":0.6878522635}
{"id":2503.0959,"submitter":"Md Mohaiminul Islam","authors":"Md Mohaiminul Islam, Tushar Nagarajan, Huiyu Wang, Gedas Bertasius,\n  Lorenzo Torresani","title":"BIMBA: Selective-Scan Compression for Long-Range Video Question\n  Answering","comments":"Accepted by CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Video Question Answering (VQA) in long videos poses the key challenge of\nextracting relevant information and modeling long-range dependencies from many\nredundant frames. The self-attention mechanism provides a general solution for\nsequence modeling, but it has a prohibitive cost when applied to a massive\nnumber of spatiotemporal tokens in long videos. Most prior methods rely on\ncompression strategies to lower the computational cost, such as reducing the\ninput length via sparse frame sampling or compressing the output sequence\npassed to the large language model (LLM) via space-time pooling. However, these\nnaive approaches over-represent redundant information and often miss salient\nevents or fast-occurring space-time patterns. In this work, we introduce BIMBA,\nan efficient state-space model to handle long-form videos. Our model leverages\nthe selective scan algorithm to learn to effectively select critical\ninformation from high-dimensional video and transform it into a reduced token\nsequence for efficient LLM processing. Extensive experiments demonstrate that\nBIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks,\nincluding PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and\nVideo-MME. Code, and models are publicly available at\nhttps:\/\/sites.google.com\/view\/bimba-mllm.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 17:57:32 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 17:14:31 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Islam', 'Md Mohaiminul', ''], ['Nagarajan', 'Tushar', ''], ['Wang', 'Huiyu', ''], ['Bertasius', 'Gedas', ''], ['Torresani', 'Lorenzo', '']]","extracted_entities":"[{'text': 'self-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]","assigned_concept":"Attention mechanism","matched_keyword":"self-attention mechanism","similarity_score":0.8757837415}
{"id":2503.09951,"submitter":"Xinglong Sun","authors":"Xinglong Sun and Haijiang Sun and Shan Jiang and Jiacheng Wang and\n  Jiasong Wang","title":"Target-aware Bidirectional Fusion Transformer for Aerial Object Tracking","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The trackers based on lightweight neural networks have achieved great success\nin the field of aerial remote sensing, most of which aggregate multi-stage deep\nfeatures to lift the tracking quality. However, existing algorithms usually\nonly generate single-stage fusion features for state decision, which ignore\nthat diverse kinds of features are required for identifying and locating the\nobject, limiting the robustness and precision of tracking. In this paper, we\npropose a novel target-aware Bidirectional Fusion transformer (BFTrans) for UAV\ntracking. Specifically, we first present a two-stream fusion network based on\nlinear self and cross attentions, which can combine the shallow and the deep\nfeatures from both forward and backward directions, providing the adjusted\nlocal details for location and global semantics for recognition. Besides, a\ntarget-aware positional encoding strategy is designed for the above fusion\nmodel, which is helpful to perceive the object-related attributes during the\nfusion phase. Finally, the proposed method is evaluated on several popular UAV\nbenchmarks, including UAV-123, UAV20L and UAVTrack112. Massive experimental\nresults demonstrate that our approach can exceed other state-of-the-art\ntrackers and run with an average speed of 30.5 FPS on embedded platform, which\nis appropriate for practical drone deployments.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 01:53:29 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Sun', 'Xinglong', ''], ['Sun', 'Haijiang', ''], ['Jiang', 'Shan', ''], ['Wang', 'Jiacheng', ''], ['Wang', 'Jiasong', '']]","extracted_entities":"[{'text': 'cross attentions', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross attentions","similarity_score":0.6792199612}
{"id":2503.09961,"submitter":"Xin Zhu","authors":"Xin Zhu, Hongyi Pan, Ahmet Enis Cetin","title":"Edge-Fog Computing-Enabled EEG Data Compression via Asymmetrical\n  Variational Discrete Cosine Transform Network","comments":"Accepted by the IEEE Internet of Things Journal","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The large volume of electroencephalograph (EEG) data produced by\nbrain-computer interface (BCI) systems presents challenges for rapid\ntransmission over bandwidth-limited channels in Internet of Things (IoT)\nnetworks. To address the issue, we propose a novel multi-channel asymmetrical\nvariational discrete cosine transform (DCT) network for EEG data compression\nwithin an edge-fog computing framework. At the edge level, low-complexity DCT\ncompression units are designed using parallel trainable hard-thresholding and\nscaling operators to remove redundant data and extract the effective latent\nspace representation. At the fog level, an adaptive filter bank is applied to\nmerge important features from adjacent channels into each individual channel by\nleveraging inter-channel correlations. Then, the inverse DCT reconstructed\nmulti-head attention is developed to capture both local and global dependencies\nand reconstruct the original signals. Furthermore, by applying the principles\nof variational inference, a new evidence lower bound is formulated as the loss\nfunction, driving the model to balance compression efficiency and\nreconstruction accuracy. Experimental results on two public datasets\ndemonstrate that the proposed method achieves superior compression performance\nwithout sacrificing any useful information for BCI detection compared with\nstate-of-the-art techniques, indicating a feasible solution for EEG data\ncompression.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 02:07:17 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zhu', 'Xin', ''], ['Pan', 'Hongyi', ''], ['Cetin', 'Ahmet Enis', '']]","extracted_entities":"[{'text': 'multi-head attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"multi-head attention","similarity_score":0.69643116}
{"id":2503.1002,"submitter":"Ali Abedi","authors":"Ali Abedi, Q. M. Jonathan Wu, Ning Zhang, Farhad Pourpanah","title":"One-Shot Federated Unsupervised Domain Adaptation with Scaled Entropy\n  Attention and Multi-Source Smoothed Pseudo Labeling","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Federated Learning (FL) is a promising approach for privacy-preserving\ncollaborative learning. However, it faces significant challenges when dealing\nwith domain shifts, especially when each client has access only to its source\ndata and cannot share it during target domain adaptation. Moreover, FL methods\noften require high communication overhead due to multiple rounds of model\nupdates between clients and the server. We propose a one-shot Federated\nUnsupervised Domain Adaptation (FUDA) method to address these limitations.\nSpecifically, we introduce Scaled Entropy Attention (SEA) for model aggregation\nand Multi-Source Pseudo Labeling (MSPL) for target domain adaptation. SEA uses\nscaled prediction entropy on target domain to assign higher attention to\nreliable models. This improves the global model quality and ensures balanced\nweighting of contributions. MSPL distills knowledge from multiple source models\nto generate pseudo labels and manage noisy labels using smoothed soft-label\ncross-entropy (SSCE). Our approach outperforms state-of-the-art methods across\nfour standard benchmarks while reducing communication and computation costs,\nmaking it highly suitable for real-world applications. The implementation code\nwill be made publicly available upon publication.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 03:59:51 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Abedi', 'Ali', ''], ['Wu', 'Q. M. Jonathan', ''], ['Zhang', 'Ning', ''], ['Pourpanah', 'Farhad', '']]","extracted_entities":"[{'text': 'Federated Learning', 'label': 'Few-shot Learning'}, {'text': 'FL', 'label': 'Zero-shot Learning'}, {'text': 'Scaled Entropy Attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Scaled Entropy Attention","similarity_score":0.5619481802}
{"id":2503.10052,"submitter":"Minjun Kim","authors":"Minje Kim, Minjun Kim, Xu Yang","title":"DTA: Dual Temporal-channel-wise Attention for Spiking Neural Networks","comments":"Accepted by IEEE\/CVF Winter Conference on Applications of Computer\n  Vision (WACV) 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.NE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Spiking Neural Networks (SNNs) present a more energy-efficient alternative to\nArtificial Neural Networks (ANNs) by harnessing spatio-temporal dynamics and\nevent-driven spikes. Effective utilization of temporal information is crucial\nfor SNNs, leading to the exploration of attention mechanisms to enhance this\ncapability. Conventional attention operations either apply identical operation\nor employ non-identical operations across target dimensions. We identify that\nthese approaches provide distinct perspectives on temporal information. To\nleverage the strengths of both operations, we propose a novel Dual\nTemporal-channel-wise Attention (DTA) mechanism that integrates both\nidentical\/non-identical attention strategies. To the best of our knowledge,\nthis is the first attempt to concentrate on both the correlation and dependency\nof temporal-channel using both identical and non-identical attention\noperations. Experimental results demonstrate that the DTA mechanism achieves\nstate-of-the-art performance on both static datasets (CIFAR10, CIFAR100,\nImageNet-1k) and dynamic dataset (CIFAR10-DVS), elevating spike representation\nand capturing complex temporal-channel relationship. We open-source our code:\nhttps:\/\/github.com\/MnJnKIM\/DTA-SNN.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 05:09:48 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Kim', 'Minje', ''], ['Kim', 'Minjun', ''], ['Yang', 'Xu', '']]","extracted_entities":"[{'text': 'Conventional attention operations', 'label': 'Attention mechanism'}, {'text': 'Dual\\nTemporal-channel-wise Attention (DTA) mechanism', 'label': 'Attention mechanism'}, {'text': 'DTA mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Conventional attention operations","similarity_score":0.7730885744}
{"id":2503.10112,"submitter":"Yanfeng Li","authors":"Yanfeng Li and Kahou Chan and Yue Sun and Chantong Lam and Tong Tong\n  and Zitong Yu and Keren Fu and Xiaohong Liu and Tao Tan","title":"MoEdit: On Learning Quantity Perception for Multi-object Image Editing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multi-object images are prevalent in various real-world scenarios, including\naugmented reality, advertisement design, and medical imaging. Efficient and\nprecise editing of these images is critical for these applications. With the\nadvent of Stable Diffusion (SD), high-quality image generation and editing have\nentered a new era. However, existing methods often struggle to consider each\nobject both individually and part of the whole image editing, both of which are\ncrucial for ensuring consistent quantity perception, resulting in suboptimal\nperceptual performance. To address these challenges, we propose MoEdit, an\nauxiliary-free multi-object image editing framework. MoEdit facilitates\nhigh-quality multi-object image editing in terms of style transfer, object\nreinvention, and background regeneration, while ensuring consistent quantity\nperception between inputs and outputs, even with a large number of objects. To\nachieve this, we introduce the Feature Compensation (FeCom) module, which\nensures the distinction and separability of each object attribute by minimizing\nthe in-between interlacing. Additionally, we present the Quantity Attention\n(QTTN) module, which perceives and preserves quantity consistency by effective\ncontrol in editing, without relying on auxiliary tools. By leveraging the SD\nmodel, MoEdit enables customized preservation and modification of specific\nconcepts in inputs with high quality. Experimental results demonstrate that our\nMoEdit achieves State-Of-The-Art (SOTA) performance in multi-object image\nediting. Data and codes will be available at\nhttps:\/\/github.com\/Tear-kitty\/MoEdit.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 07:13:54 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Yanfeng', ''], ['Chan', 'Kahou', ''], ['Sun', 'Yue', ''], ['Lam', 'Chantong', ''], ['Tong', 'Tong', ''], ['Yu', 'Zitong', ''], ['Fu', 'Keren', ''], ['Liu', 'Xiaohong', ''], ['Tan', 'Tao', '']]","extracted_entities":"[{'text': 'Quantity Attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Quantity Attention","similarity_score":0.7404478788}
{"id":2503.10149,"submitter":"Zhenxuan Zeng","authors":"Zhenxuan Zeng, Qiao Wu, Xiyu Zhang, Lin Yuanbo Wu, Pei An, Jiaqi Yang,\n  Ji Wang, Peng Wang","title":"Unlocking Generalization Power in LiDAR Point Cloud Registration","comments":"Accepted by CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In real-world environments, a LiDAR point cloud registration method with\nrobust generalization capabilities (across varying distances and datasets) is\ncrucial for ensuring safety in autonomous driving and other LiDAR-based\napplications. However, current methods fall short in achieving this level of\ngeneralization. To address these limitations, we propose UGP, a pruned\nframework designed to enhance generalization power for LiDAR point cloud\nregistration. The core insight in UGP is the elimination of cross-attention\nmechanisms to improve generalization, allowing the network to concentrate on\nintra-frame feature extraction. Additionally, we introduce a progressive\nself-attention module to reduce ambiguity in large-scale scenes and integrate\nBird's Eye View (BEV) features to incorporate semantic information about scene\nelements. Together, these enhancements significantly boost the network's\ngeneralization performance. We validated our approach through various\ngeneralization experiments in multiple outdoor scenes. In cross-distance\ngeneralization experiments on KITTI and nuScenes, UGP achieved state-of-the-art\nmean Registration Recall rates of 94.5% and 91.4%, respectively. In\ncross-dataset generalization from nuScenes to KITTI, UGP achieved a\nstate-of-the-art mean Registration Recall of 90.9%. Code will be available at\nhttps:\/\/github.com\/peakpang\/UGP.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 08:20:59 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Zeng', 'Zhenxuan', ''], ['Wu', 'Qiao', ''], ['Zhang', 'Xiyu', ''], ['Wu', 'Lin Yuanbo', ''], ['An', 'Pei', ''], ['Yang', 'Jiaqi', ''], ['Wang', 'Ji', ''], ['Wang', 'Peng', '']]","extracted_entities":"[{'text': 'cross-attention\\nmechanisms', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross-attention\nmechanisms","similarity_score":0.8177332282}
{"id":2503.10259,"submitter":"Yunpeng Qu","authors":"Yunpeng Qu, Kun Yuan, Qizhi Xie, Ming Sun, Chao Zhou, Jian Wang","title":"KVQ: Boosting Video Quality Assessment via Saliency-guided Local\n  Perception","comments":"11 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Video Quality Assessment (VQA), which intends to predict the perceptual\nquality of videos, has attracted increasing attention. Due to factors like\nmotion blur or specific distortions, the quality of different regions in a\nvideo varies. Recognizing the region-wise local quality within a video is\nbeneficial for assessing global quality and can guide us in adopting\nfine-grained enhancement or transcoding strategies. Due to the heavy cost of\nannotating region-wise quality, the lack of ground truth constraints from\nrelevant datasets further complicates the utilization of local perception.\nInspired by the Human Visual System (HVS) that links global quality to the\nlocal texture of different regions and their visual saliency, we propose a\nKaleidoscope Video Quality Assessment (KVQ) framework, which aims to\neffectively assess both saliency and local texture, thereby facilitating the\nassessment of global quality. Our framework extracts visual saliency and\nallocates attention using Fusion-Window Attention (FWA) while incorporating a\nLocal Perception Constraint (LPC) to mitigate the reliance of regional texture\nperception on neighboring areas. KVQ obtains significant improvements across\nmultiple scenarios on five VQA benchmarks compared to SOTA methods.\nFurthermore, to assess local perception, we establish a new Local Perception\nVisual Quality (LPVQ) dataset with region-wise annotations. Experimental\nresults demonstrate the capability of KVQ in perceiving local distortions. KVQ\nmodels and the LPVQ dataset will be available at\nhttps:\/\/github.com\/qyp2000\/KVQ.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 11:16:58 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Qu', 'Yunpeng', ''], ['Yuan', 'Kun', ''], ['Xie', 'Qizhi', ''], ['Sun', 'Ming', ''], ['Zhou', 'Chao', ''], ['Wang', 'Jian', '']]","extracted_entities":"[{'text': 'Fusion-Window Attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Fusion-Window Attention","similarity_score":0.5567828417}
{"id":2503.10289,"submitter":"Zebin He","authors":"Zebin He, Mingxin Yang, Shuhui Yang, Yixuan Tang, Tao Wang, Kaihao\n  Zhang, Guanying Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, Wenhan Luo","title":"MaterialMVP: Illumination-Invariant Material Generation via Multi-view\n  PBR Diffusion","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Physically-based rendering (PBR) has become a cornerstone in modern computer\ngraphics, enabling realistic material representation and lighting interactions\nin 3D scenes. In this paper, we present MaterialMVP, a novel end-to-end model\nfor generating PBR textures from 3D meshes and image prompts, addressing key\nchallenges in multi-view material synthesis. Our approach leverages Reference\nAttention to extract and encode informative latent from the input reference\nimages, enabling intuitive and controllable texture generation. We also\nintroduce a Consistency-Regularized Training strategy to enforce stability\nacross varying viewpoints and illumination conditions, ensuring\nillumination-invariant and geometrically consistent results. Additionally, we\npropose Dual-Channel Material Generation, which separately optimizes albedo and\nmetallic-roughness (MR) textures while maintaining precise spatial alignment\nwith the input images through Multi-Channel Aligned Attention. Learnable\nmaterial embeddings are further integrated to capture the distinct properties\nof albedo and MR. Experimental results demonstrate that our model generates PBR\ntextures with realistic behavior across diverse lighting scenarios,\noutperforming existing methods in both consistency and quality for scalable 3D\nasset creation.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 11:57:30 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['He', 'Zebin', ''], ['Yang', 'Mingxin', ''], ['Yang', 'Shuhui', ''], ['Tang', 'Yixuan', ''], ['Wang', 'Tao', ''], ['Zhang', 'Kaihao', ''], ['Chen', 'Guanying', ''], ['Liu', 'Yuhong', ''], ['Jiang', 'Jie', ''], ['Guo', 'Chunchao', ''], ['Luo', 'Wenhan', '']]","extracted_entities":"[{'text': 'image prompts', 'label': 'Prompting'}, {'text': 'Reference\\nAttention', 'label': 'Attention mechanism'}, {'text': 'Multi-Channel Aligned Attention', 'label': 'Attention mechanism'}, {'text': 'Learnable\\nmaterial embeddings', 'label': 'contextual Embedding'}]","assigned_concept":"Attention mechanism","matched_keyword":"Reference\nAttention","similarity_score":0.6288903952}
{"id":2503.10421,"submitter":"Zhenwei Wang","authors":"Zhenwei Wang, Ruibin Bai, Tiehua Zhang","title":"Towards Constraint-Based Adaptive Hypergraph Learning for Solving\n  Vehicle Routing: An End-to-End Solution","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.NE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The application of learning based methods to vehicle routing problems has\nemerged as a pivotal area of research in combinatorial optimization. These\nproblems are characterized by vast solution spaces and intricate constraints,\nmaking traditional approaches such as exact mathematical models or heuristic\nmethods prone to high computational overhead or reliant on the design of\ncomplex heuristic operators to achieve optimal or near optimal solutions.\nMeanwhile, although some recent learning-based methods can produce good\nperformance for VRP with straightforward constraint scenarios, they often fail\nto effectively handle hard constraints that are common in practice. This study\nintroduces a novel end-to-end framework that combines constraint-oriented\nhypergraphs with reinforcement learning to address vehicle routing problems. A\ncentral innovation of this work is the development of a constraint-oriented\ndynamic hyperedge reconstruction strategy within an encoder, which\nsignificantly enhances hypergraph representation learning. Additionally, the\ndecoder leverages a double-pointer attention mechanism to iteratively generate\nsolutions. The proposed model is trained by incorporating asynchronous\nparameter updates informed by hypergraph constraints and optimizing a dual loss\nfunction comprising constraint loss and policy gradient loss. The experiment\nresults on benchmark datasets demonstrate that the proposed approach not only\neliminates the need for sophisticated heuristic operators but also achieves\nsubstantial improvements in solution quality.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 14:42:44 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Zhenwei', ''], ['Bai', 'Ruibin', ''], ['Zhang', 'Tiehua', '']]","extracted_entities":"[{'text': 'reinforcement learning', 'label': 'Few-shot Learning'}, {'text': 'hypergraph representation learning', 'label': 'Few-shot Learning'}, {'text': 'double-pointer attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"double-pointer attention mechanism","similarity_score":0.7746528387}
{"id":2503.10445,"submitter":"Huiyun Tang","authors":"Huiyun Tang, Bj\\\"orn Rohles, Yuwei Chuai, Gabriele Lenzini, Anastasia\n  Sergeeva","title":"More Than Just Warnings:Exploring the Ways of Communicating Credibility\n  Assessment on Social Media","comments":"27 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Reducing the spread of misinformation is challenging. AI-based fact\nverification systems offer a promising solution by addressing the high costs\nand slow pace of traditional fact-checking. However, the problem of how to\neffectively communicate the results to users remains unsolved. Warning labels\nmay seem an easy solution, but they fail to account for fuzzy misinformation\nthat is not entirely fake. Additionally, users' limited attention spans and\nsocial media information should be taken into account while designing the\npresentation. The online experiment (n = 537) investigates the impact of\nsources and granularity on users' perception of information veracity and the\nsystem's usefulness and trustworthiness. Findings show that fine-grained\nindicators enhance nuanced opinions, information awareness, and the intention\nto use fact-checking systems. Source differences had minimal impact on opinions\nand perceptions, except for informativeness. Qualitative findings suggest the\nproposed indicators promote critical thinking. We discuss implications for\ndesigning concise, user-friendly AI fact-checking feedback.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 15:10:55 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Tang', 'Huiyun', ''], ['Rohles', 'Bj\u00f6rn', ''], ['Chuai', 'Yuwei', ''], ['Lenzini', 'Gabriele', ''], ['Sergeeva', 'Anastasia', '']]","extracted_entities":"[{'text': 'limited attention spans', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"limited attention spans","similarity_score":0.6633863449}
{"id":2503.1051,"submitter":"Rajiv Krishnakumar","authors":"Rajiv Krishnakumar, Julien Baglio, Frederik F. Fl\\\"other, Christian\n  Ruiz, Stefan Habringer, Nicole H. Romano","title":"Extreme Learning Machines for Attention-based Multiple Instance Learning\n  in Whole-Slide Image Classification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"q-bio.QM cs.LG quant-ph","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Whole-slide image classification represents a key challenge in computational\npathology and medicine. Attention-based multiple instance learning (MIL) has\nemerged as an effective approach for this problem. However, the effect of\nattention mechanism architecture on model performance is not well-documented\nfor biomedical imagery. In this work, we compare different methods and\nimplementations of MIL, including deep learning variants. We introduce a new\nmethod using higher-dimensional feature spaces for deep MIL. We also develop a\nnovel algorithm for whole-slide image classification where extreme machine\nlearning is combined with attention-based MIL to improve sensitivity and reduce\ntraining complexity. We apply our algorithms to the problem of detecting\ncirculating rare cells (CRCs), such as erythroblasts, in peripheral blood. Our\nresults indicate that nonlinearities play a key role in the classification, as\nremoving them leads to a sharp decrease in stability in addition to a decrease\nin average area under the curve (AUC) of over 4%. We also demonstrate a\nconsiderable increase in robustness of the model with improvements of over 10%\nin average AUC when higher-dimensional feature spaces are leveraged. In\naddition, we show that extreme learning machines can offer clear improvements\nin terms of training efficiency by reducing the number of trained parameters by\na factor of 5 whilst still maintaining the average AUC to within 1.5% of the\ndeep MIL model. Finally, we discuss options of enriching the classical\ncomputing framework with quantum algorithms in the future. This work can thus\nhelp pave the way towards more accurate and efficient single-cell diagnostics,\none of the building blocks of precision medicine.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 16:14:08 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Krishnakumar', 'Rajiv', ''], ['Baglio', 'Julien', ''], ['Fl\u00f6ther', 'Frederik F.', ''], ['Ruiz', 'Christian', ''], ['Habringer', 'Stefan', ''], ['Romano', 'Nicole H.', '']]","extracted_entities":"[{'text': 'attention mechanism', 'label': 'Attention mechanism'}, {'text': 'attention-based MIL', 'label': 'Few-shot Learning'}, {'text': 'quantum algorithms', 'label': 'quantisation'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanism","similarity_score":1.0}
{"id":2503.10523,"submitter":"Yongqi Wang","authors":"Jun Yu, Yongqi Wang, Lei Wang, Yang Zheng, Shengfan Xu","title":"Interactive Multimodal Fusion with Temporal Modeling","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper presents our method for the estimation of valence-arousal (VA) in\nthe 8th Affective Behavior Analysis in-the-Wild (ABAW) competition. Our\napproach integrates visual and audio information through a multimodal\nframework. The visual branch uses a pre-trained ResNet model to extract spatial\nfeatures from facial images. The audio branches employ pre-trained VGG models\nto extract VGGish and LogMel features from speech signals. These features\nundergo temporal modeling using Temporal Convolutional Networks (TCNs). We then\napply cross-modal attention mechanisms, where visual features interact with\naudio features through query-key-value attention structures. Finally, the\nfeatures are concatenated and passed through a regression layer to predict\nvalence and arousal. Our method achieves competitive performance on the\nAff-Wild2 dataset, demonstrating effective multimodal fusion for VA estimation\nin-the-wild.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 16:31:56 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Yu', 'Jun', ''], ['Wang', 'Yongqi', ''], ['Wang', 'Lei', ''], ['Zheng', 'Yang', ''], ['Xu', 'Shengfan', '']]","extracted_entities":"[{'text': 'cross-modal attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'query-key-value attention structures', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross-modal attention mechanisms","similarity_score":0.7600411177}
{"id":2503.10568,"submitter":"Haopeng Li","authors":"Haopeng Li, Jinyue Yang, Guoqi Li, Huan Wang","title":"Autoregressive Image Generation with Randomized Parallel Decoding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:19:51 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Li', 'Haopeng', ''], ['Yang', 'Jinyue', ''], ['Li', 'Guoqi', ''], ['Wang', 'Huan', '']]","extracted_entities":"[{'text': 'causal attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"causal attention mechanism","similarity_score":0.8178867102}
{"id":2503.10579,"submitter":"Chaoqun Wang","authors":"Chaoqun Wang, Xiaobin Hong, Wenzhong Li, and Ruimao Zhang","title":"Semantic-Supervised Spatial-Temporal Fusion for LiDAR-based 3D Object\n  Detection","comments":"Accepted by ICRA2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  LiDAR-based 3D object detection presents significant challenges due to the\ninherent sparsity of LiDAR points. A common solution involves long-term\ntemporal LiDAR data to densify the inputs. However, efficiently leveraging\nspatial-temporal information remains an open problem. In this paper, we propose\na novel Semantic-Supervised Spatial-Temporal Fusion (ST-Fusion) method, which\nintroduces a novel fusion module to relieve the spatial misalignment caused by\nthe object motion over time and a feature-level semantic supervision to\nsufficiently unlock the capacity of the proposed fusion module. Specifically,\nthe ST-Fusion consists of a Spatial Aggregation (SA) module and a Temporal\nMerging (TM) module. The SA module employs a convolutional layer with\nprogressively expanding receptive fields to aggregate the object features from\nthe local regions to alleviate the spatial misalignment, the TM module\ndynamically extracts object features from the preceding frames based on the\nattention mechanism for a comprehensive sequential presentation. Besides, in\nthe semantic supervision, we propose a Semantic Injection method to enrich the\nsparse LiDAR data via injecting the point-wise semantic labels, using it for\ntraining a teacher model and providing a reconstruction target at the feature\nlevel supervised by the proposed object-aware loss. Extensive experiments on\nvarious LiDAR-based detectors demonstrate the effectiveness and universality of\nour proposal, yielding an improvement of approximately +2.8% in NDS based on\nthe nuScenes benchmark.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:30:20 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Wang', 'Chaoqun', ''], ['Hong', 'Xiaobin', ''], ['Li', 'Wenzhong', ''], ['Zhang', 'Ruimao', '']]","extracted_entities":"[{'text': 'attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanism","similarity_score":1.0}
{"id":2503.10589,"submitter":"Yuwei Guo","authors":"Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng\n  Yang, Dahua Lin, Lu Jiang","title":"Long Context Tuning for Video Generation","comments":"Project Page: https:\/\/guoyww.github.io\/projects\/long-context-video\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps:\/\/guoyww.github.io\/projects\/long-context-video\/ for more details.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:40:07 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Guo', 'Yuwei', ''], ['Yang', 'Ceyuan', ''], ['Yang', 'Ziyan', ''], ['Ma', 'Zhibei', ''], ['Lin', 'Zhijie', ''], ['Yang', 'Zhenheng', ''], ['Lin', 'Dahua', ''], ['Jiang', 'Lu', '']]","extracted_entities":"[{'text': 'Long Context Tuning', 'label': 'Fine-tuning'}, {'text': 'full attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'interleaved 3D position embedding', 'label': 'contextual Embedding'}, {'text': 'bidirectional attention', 'label': 'Attention mechanism'}, {'text': 'context-causal attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"full attention mechanisms","similarity_score":0.8964736462}
{"id":2503.10625,"submitter":"Lingteng Qiu","authors":"Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei\n  Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, Liefeng Bo","title":"LHM: Large Animatable Human Reconstruction Model from a Single Image in\n  Seconds","comments":"Project Page: https:\/\/lingtengqiu.github.io\/LHM\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Animatable 3D human reconstruction from a single image is a challenging\nproblem due to the ambiguity in decoupling geometry, appearance, and\ndeformation. Recent advances in 3D human reconstruction mainly focus on static\nhuman modeling, and the reliance of using synthetic 3D scans for training\nlimits their generalization ability. Conversely, optimization-based video\nmethods achieve higher fidelity but demand controlled capture conditions and\ncomputationally intensive refinement processes. Motivated by the emergence of\nlarge reconstruction models for efficient static reconstruction, we propose LHM\n(Large Animatable Human Reconstruction Model) to infer high-fidelity avatars\nrepresented as 3D Gaussian splatting in a feed-forward pass. Our model\nleverages a multimodal transformer architecture to effectively encode the human\nbody positional features and image features with attention mechanism, enabling\ndetailed preservation of clothing geometry and texture. To further boost the\nface identity preservation and fine detail recovery, we propose a head feature\npyramid encoding scheme to aggregate multi-scale features of the head regions.\nExtensive experiments demonstrate that our LHM generates plausible animatable\nhuman in seconds without post-processing for face and hands, outperforming\nexisting methods in both reconstruction accuracy and generalization ability.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:59:21 GMT'}]","update_date":"2025-03-14","authors_parsed":"[['Qiu', 'Lingteng', ''], ['Gu', 'Xiaodong', ''], ['Li', 'Peihao', ''], ['Zuo', 'Qi', ''], ['Shen', 'Weichao', ''], ['Zhang', 'Junfei', ''], ['Qiu', 'Kejie', ''], ['Yuan', 'Weihao', ''], ['Chen', 'Guanying', ''], ['Dong', 'Zilong', ''], ['Bo', 'Liefeng', '']]","extracted_entities":"[{'text': 'attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanism","similarity_score":1.0}
