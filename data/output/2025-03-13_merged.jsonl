{"id":2311.11482,"submitter":"Yifan Zhang","authors":"Yifan Zhang, Yang Yuan, Andrew Chi-Chih Yao","title":"Meta Prompting for AI Systems","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We introduce Meta Prompting (MP), a prompting paradigm designed to enhance\nthe utilization of large language models (LLMs) and AI systems in complex\nproblem-solving and data interaction. Grounded in type theory and category\ntheory, Meta Prompting prioritizes structural and syntactical considerations\nover traditional content-centric methods. In this work, we formally define Meta\nPrompting, delineate its distinctions from few-shot prompting, and demonstrate\nits effectiveness across various AI applications. In particular, we show that\nMeta Prompting can decompose intricate reasoning tasks into simpler\nsub-problems, thereby improving token efficiency and enabling fairer\ncomparisons with conventional few-shot techniques. Furthermore, we extend this\nframework to prompting tasks, allowing LLMs to recursively self-generate\nrefined prompts in a metaprogramming-like manner. Empirical evaluations reveal\nthat a Qwen-72B base language model equipped with Meta Prompting-without\nadditional instruction tuning-achieves a PASS@1 accuracy of 46.3% on MATH\nproblems, surpassing a supervised fine-tuned counterpart, 83.5% accuracy on\nGSM8K, and a 100% success rate on Game of 24 tasks using GPT-4. The code is\navailable at https:\/\/github.com\/meta-prompting\/meta-prompting.\n","versions":"[{'version': 'v1', 'created': 'Mon, 20 Nov 2023 01:51:13 GMT'}, {'version': 'v2', 'created': 'Thu, 25 Jan 2024 13:54:42 GMT'}, {'version': 'v3', 'created': 'Tue, 30 Jan 2024 01:15:59 GMT'}, {'version': 'v4', 'created': 'Thu, 1 Feb 2024 04:12:52 GMT'}, {'version': 'v5', 'created': 'Tue, 2 Apr 2024 03:36:57 GMT'}, {'version': 'v6', 'created': 'Sat, 15 Jun 2024 08:19:24 GMT'}, {'version': 'v7', 'created': 'Wed, 26 Feb 2025 05:39:39 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Zhang', 'Yifan', ''], ['Yuan', 'Yang', ''], ['Yao', 'Andrew Chi-Chih', '']]","extracted_entities":"[{'text': 'Meta Prompting', 'label': 'Prompting'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Meta Prompting', 'label': 'Prompting'}, {'text': 'Meta\\nPrompting', 'label': 'Prompting'}, {'text': 'Meta Prompting', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Meta Prompting-without', 'label': 'Prompting'}, {'text': 'GPT-4', 'label': 'GPT-4'}]","assigned_concept":"GPT-4","matched_keyword":"GPT-4","similarity_score":1.0}
{"id":2403.11807,"submitter":"Jen-Tse Huang","authors":"Jen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang,\n  Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Michael R. Lyu","title":"How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments","comments":"Accepted to ICLR 2025; 11 pages of main text; 26 pages of appendices;\n  Included models: GPT-3.5-{0613, 1106, 0125}, GPT-4-0125, GPT-4o-0806,\n  Gemini-{1.0, 1.5)-Pro, LLaMA-3.1-{7, 70, 405}B, Mixtral-8x{7, 22}B,\n  Qwen-2-72B","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Decision-making is a complex process requiring diverse abilities, making it\nan excellent framework for evaluating Large Language Models (LLMs). Researchers\nhave examined LLMs' decision-making through the lens of Game Theory. However,\nexisting evaluation mainly focus on two-player scenarios where an LLM competes\nagainst another. Additionally, previous benchmarks suffer from test set leakage\ndue to their static design. We introduce GAMA($\\gamma$)-Bench, a new framework\nfor evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes\neight classical game theory scenarios and a dynamic scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. $\\gamma$-Bench allows\nflexible game settings and adapts the scoring system to different game\nparameters, enabling comprehensive evaluation of robustness, generalizability,\nand strategies for improvement. Our results indicate that GPT-3.5 demonstrates\nstrong robustness but limited generalizability, which can be enhanced using\nmethods like Chain-of-Thought. We also evaluate 13 LLMs from 6 model families,\nincluding GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2.\nGemini-1.5-Pro outperforms others, scoring of $69.8$ out of $100$, followed by\nLLaMA-3.1-70B ($65.9$) and Mixtral-8x22B ($62.4$). Our code and experimental\nresults are publicly available at https:\/\/github.com\/CUHK-ARISE\/GAMABench.\n","versions":"[{'version': 'v1', 'created': 'Mon, 18 Mar 2024 14:04:47 GMT'}, {'version': 'v2', 'created': 'Thu, 25 Apr 2024 15:04:41 GMT'}, {'version': 'v3', 'created': 'Tue, 3 Sep 2024 01:14:30 GMT'}, {'version': 'v4', 'created': 'Mon, 30 Sep 2024 20:57:58 GMT'}, {'version': 'v5', 'created': 'Sun, 9 Feb 2025 13:37:46 GMT'}, {'version': 'v6', 'created': 'Thu, 27 Feb 2025 13:57:52 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Huang', 'Jen-tse', ''], ['Li', 'Eric John', ''], ['Lam', 'Man Ho', ''], ['Liang', 'Tian', ''], ['Wang', 'Wenxuan', ''], ['Yuan', 'Youliang', ''], ['Jiao', 'Wenxiang', ''], ['Wang', 'Xing', ''], ['Tu', 'Zhaopeng', ''], ['Lyu', 'Michael R.', '']]","extracted_entities":"[{'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'GPT-4', 'label': 'GPT-4'}, {'text': 'Mixtral', 'label': 'GPT-4'}]","assigned_concept":"GPT-4","matched_keyword":"GPT-4","similarity_score":1.0}
{"id":2306.00693,"submitter":"Ning Ding","authors":"Ning Ding, Yehui Tang, Zhongqian Fu, Chao Xu, Kai Han, Yunhe Wang","title":"GPT4Image: Large Pre-trained Models Help Vision Models Learn Better on\n  Perception Task","comments":"GitHub:\n  https:\/\/github.com\/huawei-noah\/Efficient-Computing\/tree\/master\/GPT4Image\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The upsurge in pre-trained large models started by ChatGPT has swept across\nthe entire deep learning community. Such powerful models demonstrate advanced\ngenerative ability and multimodal understanding capability, which quickly set\nnew state of the arts on a variety of benchmarks. The pre-trained LLM usually\nplays the role as a universal AI model that can conduct various tasks like\narticle analysis and image comprehension. However, due to the prohibitively\nhigh memory and computational cost of implementing such a large model, the\nconventional models (such as CNN and ViT) are still essential for many visual\nperception tasks. In this paper, we propose to enhance the representation\nability of ordinary vision models on perception tasks (e.g. image\nclassification) by taking advantage of the off-the-shelf large pre-trained\nmodels. We present a new learning framework, dubbed GPT4Image, where the\nknowledge of the large pre-trained models are extracted to help CNNs and ViTs\nlearn better representations and achieve higher performance. Firstly, we curate\na high quality description set by prompting a multimodal LLM to generate\ndescriptions for training images. Then, these detailed descriptions are fed\ninto a pre-trained encoder to extract text embeddings that encodes the rich\nsemantics of images. During training, text embeddings will serve as extra\nsupervising signal and be aligned with image representations learned by vision\nmodels. The alignment process helps vision models achieve better performance\nwith the aid of pre-trained LLMs. We conduct extensive experiments to verify\nthe effectiveness of the proposed algorithm on various visual perception tasks\nfor heterogeneous model architectures.\n","versions":"[{'version': 'v1', 'created': 'Thu, 1 Jun 2023 14:02:45 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Jun 2023 13:59:25 GMT'}, {'version': 'v3', 'created': 'Thu, 27 Feb 2025 12:49:05 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Ding', 'Ning', ''], ['Tang', 'Yehui', ''], ['Fu', 'Zhongqian', ''], ['Xu', 'Chao', ''], ['Han', 'Kai', ''], ['Wang', 'Yunhe', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'GPT4Image', 'label': 'GPT'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'text embeddings', 'label': 'Embedding'}, {'text': 'text embeddings', 'label': 'Embedding'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2311.05769,"submitter":"Ross Deans Kristensen-McLachlan","authors":"Ross Deans Kristensen-McLachlan, Miceal Canavan, M\\'arton Kardos, Mia\n  Jacobsen, Lene Aar{\\o}e","title":"Are Chatbots Reliable Text Annotators? Sometimes","comments":"Accepted for publication in PNAS Nexus (accepted Feb. 2025)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Recent research highlights the significant potential of ChatGPT for text\nannotation in social science research. However, ChatGPT is a closed-source\nproduct which has major drawbacks with regards to transparency,\nreproducibility, cost, and data protection. Recent advances in open-source (OS)\nlarge language models (LLMs) offer an alternative without these drawbacks.\nThus, it is important to evaluate the performance of OS LLMs relative to\nChatGPT and standard approaches to supervised machine learning classification.\nWe conduct a systematic comparative evaluation of the performance of a range of\nOS LLMs alongside ChatGPT, using both zero- and few-shot learning as well as\ngeneric and custom prompts, with results compared to supervised classification\nmodels. Using a new dataset of tweets from US news media, and focusing on\nsimple binary text annotation tasks, we find significant variation in the\nperformance of ChatGPT and OS models across the tasks, and that the supervised\nclassifier using DistilBERT generally outperforms both. Given the unreliable\nperformance of ChatGPT and the significant challenges it poses to Open Science\nwe advise caution when using ChatGPT for substantive text annotation tasks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 9 Nov 2023 22:28:14 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 09:57:48 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Kristensen-McLachlan', 'Ross Deans', ''], ['Canavan', 'Miceal', ''], ['Kardos', 'M\u00e1rton', ''], ['Jacobsen', 'Mia', ''], ['Aar\u00f8e', 'Lene', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'OS', 'label': 'Open-source LLMs'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'zero- and few-shot learning', 'label': 'Zero-shot Learning'}, {'text': 'custom prompts', 'label': 'Prompting'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'DistilBERT', 'label': 'DistilBERT'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2312.04059,"submitter":"Zhuoran Huang","authors":"Zhuoran Huang, Michael P. Berry, Christina Chwyl, Gary Hsieh, Jing\n  Wei, Evan M. Forman","title":"Comparing Large Language Model AI and Human-Generated Coaching Messages\n  for Behavioral Weight Loss","comments":"12 pages, 5 figures","journal-ref":"Journal of Technology in Behavioral Science (2025)","doi":"10.1007\/s41347-025-00491-5","report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Automated coaching messages for weight control can save time and costs, but\ntheir repetitive, generic nature may limit their effectiveness compared to\nhuman coaching. Large language model (LLM) based artificial intelligence (AI)\nchatbots, like ChatGPT, could offer more personalized and novel messages to\naddress repetition with their data-processing abilities. While LLM AI\ndemonstrates promise to encourage healthier lifestyles, studies have yet to\nexamine the feasibility and acceptability of LLM-based BWL coaching. 87 adults\nin a weight-loss trial rated ten coaching messages' helpfulness (five\nhuman-written, five ChatGPT-generated) using a 5-point Likert scale, providing\nadditional open-ended feedback to justify their ratings. Participants also\nidentified which messages they believed were AI-generated. The evaluation\noccurred in two phases: messages in Phase 1 were perceived as impersonal and\nnegative, prompting revisions for Phase 2 messages. In Phase 1, AI-generated\nmessages were rated less helpful than human-written ones, with 66 percent\nreceiving a helpfulness rating of 3 or higher. However, in Phase 2, the AI\nmessages matched the human-written ones regarding helpfulness, with 82% scoring\nthree or above. Additionally, 50% were misidentified as human-written,\nsuggesting AI's sophistication in mimicking human-generated content. A thematic\nanalysis of open-ended feedback revealed that participants appreciated AI's\nempathy and personalized suggestions but found them more formulaic, less\nauthentic, and too data-focused. This study reveals the preliminary feasibility\nand acceptability of LLM AIs, like ChatGPT, in crafting potentially effective\nweight control coaching messages. Our findings also underscore areas for future\nenhancement.\n","versions":"[{'version': 'v1', 'created': 'Thu, 7 Dec 2023 05:45:24 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 18:38:02 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Huang', 'Zhuoran', ''], ['Berry', 'Michael P.', ''], ['Chwyl', 'Christina', ''], ['Hsieh', 'Gary', ''], ['Wei', 'Jing', ''], ['Forman', 'Evan M.', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'ChatGPT-generated', 'label': 'ChatGPT'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2403.15297,"submitter":"Tiansi Dong","authors":"Tiansi Dong, Mateja Jamnik, Pietro Li\\`o","title":"Sphere Neural-Networks for Rational Reasoning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by\ntheir planetary popularity, their capability of human-like communication, and\nalso by their steadily improved reasoning performance. However, it remains\nunclear whether LLMs reason. It is an open problem how traditional neural\nnetworks can be qualitatively extended to go beyond the statistic paradigm and\nachieve high-level cognition. Here, we present a novel qualitative extension by\ngeneralising computational building blocks from vectors to spheres. We propose\nSphere Neural Networks (SphNNs) for human-like reasoning through model\nconstruction and inspection, and develop SphNN for syllogistic reasoning, a\nmicrocosm of human rationality. SphNN is a hierarchical neuro-symbolic\nKolmogorov-Arnold geometric GNN, and uses a neuro-symbolic transition map of\nneighbourhood spatial relations to transform the current sphere configuration\ntowards the target. SphNN is the first neural model that can determine the\nvalidity of long-chained syllogistic reasoning in one epoch without training\ndata, with the worst computational complexity of O(N). SphNN can evolve into\nvarious types of reasoning, such as spatio-temporal reasoning, logical\nreasoning with negation and disjunction, event reasoning, neuro-symbolic\nunification, and humour understanding (the highest level of cognition). All\nthese suggest a new kind of Herbert A. Simon's scissors with two neural blades.\nSphNNs will tremendously enhance interdisciplinary collaborations to develop\nthe two neural blades and realise deterministic neural reasoning and\nhuman-bounded rationality and elevate LLMs to reliable psychological AI. This\nwork suggests that the non-zero radii of spheres are the missing components\nthat prevent traditional deep-learning systems from reaching the realm of\nrational reasoning and cause LLMs to be trapped in the swamp of hallucination.\n","versions":"[{'version': 'v1', 'created': 'Fri, 22 Mar 2024 15:44:59 GMT'}, {'version': 'v2', 'created': 'Wed, 17 Apr 2024 20:02:20 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Jun 2024 19:45:42 GMT'}, {'version': 'v4', 'created': 'Tue, 25 Feb 2025 15:48:11 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Dong', 'Tiansi', ''], ['Jamnik', 'Mateja', ''], ['Li\u00f2', 'Pietro', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'SphNNs', 'label': 'Neural Language Model'}, {'text': 'SphNN', 'label': 'Neural Language Model'}, {'text': 'long-chained syllogistic reasoning', 'label': 'Chain of thought'}, {'text': 'spatio-temporal reasoning', 'label': 'Chain of thought'}, {'text': 'event reasoning', 'label': 'Chain of thought'}, {'text': 'neuro-symbolic\\nunification', 'label': 'Chain of thought'}, {'text': 'SphNNs', 'label': 'Neural Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2403.16354,"submitter":"Nicolas Van Kempen","authors":"Kyla Levin and Nicolas van Kempen and Emery D. Berger and Stephen N.\n  Freund","title":"ChatDBG: An AI-Powered Debugging Assistant","comments":"19 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI cs.LG cs.PL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Debugging is a critical but challenging task for programmers. This paper\nproposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large\nlanguage models (LLMs) to significantly enhance the capabilities and\nuser-friendliness of conventional debuggers. ChatDBG lets programmers engage in\na collaborative dialogue with the debugger, allowing them to pose complex\nquestions about program state, perform root cause analysis for crashes or\nassertion failures, and explore open-ended queries like `why is x null?'. To\nhandle these queries, ChatDBG grants the LLM autonomy to \"take the wheel\": it\ncan act as an independent agent capable of querying and controlling the\ndebugger to navigate through stacks and inspect program state. It then reports\nits findings and yields back control to the programmer. By leveraging the\nreal-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable\nonly through the use of domain-specific reasoning. Our ChatDBG prototype\nintegrates with standard debuggers including LLDB and GDB for native code and\nPdb for Python. Our evaluation across a diverse set of code, including C\/C++\ncode with known bugs and a suite of Python code including standalone scripts\nand Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root\ncauses, explain bugs, and generate accurate fixes for a wide range of\nreal-world errors. For the Python programs, a single query led to an actionable\nbug fix 67% of the time; one additional follow-up query increased the success\nrate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded more\nthan 65,000 times.\n","versions":"[{'version': 'v1', 'created': 'Mon, 25 Mar 2024 01:12:57 GMT'}, {'version': 'v2', 'created': 'Tue, 24 Sep 2024 15:07:24 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 22:18:54 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Levin', 'Kyla', ''], ['van Kempen', 'Nicolas', ''], ['Berger', 'Emery D.', ''], ['Freund', 'Stephen N.', '']]","extracted_entities":"[{'text': 'ChatDBG', 'label': 'ChatGPT'}, {'text': 'ChatDBG', 'label': 'ChatGPT'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ChatDBG', 'label': 'ChatGPT'}, {'text': 'ChatDBG', 'label': 'ChatGPT'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ChatDBG', 'label': 'ChatGPT'}, {'text': 'ChatDBG', 'label': 'ChatGPT'}, {'text': 'ChatDBG', 'label': 'ChatGPT'}, {'text': 'ChatDBG', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatDBG","similarity_score":0.6821680069}
{"id":2403.16362,"submitter":"Yihao Qin","authors":"Yihao Qin, Shangwen Wang, Yiling Lou, Jinhao Dong, Kaixin Wang,\n  Xiaoling Li, Xiaoguang Mao","title":"AgentFL: Scaling LLM-based Fault Localization to Project-Level Context","comments":"Added a comment to refer to the published version","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Fault Localization (FL) is an essential step during the debugging process.\nWith the strong capabilities of code comprehension, the recent Large Language\nModels (LLMs) have demonstrated promising performance in diagnosing bugs in the\ncode. Nevertheless, due to LLMs' limited performance in handling long contexts,\nexisting LLM-based fault localization remains on localizing bugs within a small\ncode scope (i.e., a method or a class), which struggles to diagnose bugs for a\nlarge code scope (i.e., an entire software system). To address the limitation,\nthis paper presents AgentFL, a multi-agent system based on ChatGPT for\nautomated fault localization. By simulating the behavior of a human developer,\nAgentFL models the FL task as a three-step process, which involves\ncomprehension, navigation, and confirmation. Within each step, AgentFL hires\nagents with diversified expertise, each of which utilizes different tools to\nhandle specific tasks. Particularly, we adopt a series of auxiliary strategies\nsuch as Test Behavior Tracking, Document-Guided Search, and Multi-Round\nDialogue to overcome the challenges in each step. The evaluation on the widely\nused Defects4J-V1.2.0 benchmark shows that AgentFL can localize 157 out of 395\nbugs within Top-1, which outperforms the other LLM-based approaches and\nexhibits complementarity to the state-of-the-art learning-based techniques.\nAdditionally, we confirm the indispensability of the components in AgentFL with\nthe ablation study and demonstrate the usability of AgentFL through a user\nstudy. Finally, the cost analysis shows that AgentFL spends an average of only\n0.074 dollars and 97 seconds for a single bug.\n","versions":"[{'version': 'v1', 'created': 'Mon, 25 Mar 2024 01:58:19 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 03:23:06 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Qin', 'Yihao', ''], ['Wang', 'Shangwen', ''], ['Lou', 'Yiling', ''], ['Dong', 'Jinhao', ''], ['Wang', 'Kaixin', ''], ['Li', 'Xiaoling', ''], ['Mao', 'Xiaoguang', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2404.01833,"submitter":"Ahmed Salem","authors":"Mark Russinovich and Ahmed Salem and Ronen Eldan","title":"Great, Now Write an Article About That: The Crescendo Multi-Turn LLM\n  Jailbreak Attack","comments":"Accepted at USENIX Security 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) have risen significantly in popularity and are\nincreasingly being adopted across multiple applications. These LLMs are heavily\naligned to resist engaging in illegal or unethical topics as a means to avoid\ncontributing to responsible AI harms. However, a recent line of attacks, known\nas jailbreaks, seek to overcome this alignment. Intuitively, jailbreak attacks\naim to narrow the gap between what the model can do and what it is willing to\ndo. In this paper, we introduce a novel jailbreak attack called Crescendo.\nUnlike existing jailbreak methods, Crescendo is a simple multi-turn jailbreak\nthat interacts with the model in a seemingly benign manner. It begins with a\ngeneral prompt or question about the task at hand and then gradually escalates\nthe dialogue by referencing the model's replies progressively leading to a\nsuccessful jailbreak. We evaluate Crescendo on various public systems,\nincluding ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b and LlaMA-3 70b Chat,\nand Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo,\nwith it achieving high attack success rates across all evaluated models and\ntasks. Furthermore, we present Crescendomation, a tool that automates the\nCrescendo attack and demonstrate its efficacy against state-of-the-art models\nthrough our evaluations. Crescendomation surpasses other state-of-the-art\njailbreaking techniques on the AdvBench subset dataset, achieving 29-61% higher\nperformance on GPT-4 and 49-71% on Gemini-Pro. Finally, we also demonstrate\nCrescendo's ability to jailbreak multimodal models.\n","versions":"[{'version': 'v1', 'created': 'Tue, 2 Apr 2024 10:45:49 GMT'}, {'version': 'v2', 'created': 'Tue, 24 Sep 2024 13:51:39 GMT'}, {'version': 'v3', 'created': 'Wed, 26 Feb 2025 13:41:41 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Russinovich', 'Mark', ''], ['Salem', 'Ahmed', ''], ['Eldan', 'Ronen', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'Gemini Pro', 'label': 'ChatGPT'}, {'text': 'GPT-4', 'label': 'GPT-2'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2405.20681,"submitter":"Xiaojin Zhang","authors":"Xiaojin Zhang, Yahao Pang, Yan Kang, Wei Chen, Lixin Fan, Hai Jin,\n  Qiang Yang","title":"No Free Lunch Theorem for Privacy-Preserving LLM Inference","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Individuals and businesses have been significantly benefited by Large\nLanguage Models (LLMs) including PaLM, Gemini and ChatGPT in various ways. For\nexample, LLMs enhance productivity, reduce costs, and enable us to focus on\nmore valuable tasks. Furthermore, LLMs possess the capacity to sift through\nextensive datasets, uncover underlying patterns, and furnish critical insights\nthat propel the frontiers of technology and science. However, LLMs also pose\nprivacy concerns. Users' interactions with LLMs may expose their sensitive\npersonal or company information. A lack of robust privacy safeguards and legal\nframeworks could permit the unwarranted intrusion or improper handling of\nindividual data, thereby risking infringements of privacy and the theft of\npersonal identities. To ensure privacy, it is essential to minimize the\ndependency between shared prompts and private information. Various\nrandomization approaches have been proposed to protect prompts' privacy, but\nthey may incur utility loss compared to unprotected LLMs prompting. Therefore,\nit is essential to evaluate the balance between the risk of privacy leakage and\nloss of utility when conducting effective protection mechanisms. The current\nstudy develops a framework for inferring privacy-protected Large Language\nModels (LLMs) and lays down a solid theoretical basis for examining the\ninterplay between privacy preservation and utility. The core insight is\nencapsulated within a theorem that is called as the NFL (abbreviation of the\nword No-Free-Lunch) Theorem.\n","versions":"[{'version': 'v1', 'created': 'Fri, 31 May 2024 08:22:53 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 01:55:21 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Zhang', 'Xiaojin', ''], ['Pang', 'Yahao', ''], ['Kang', 'Yan', ''], ['Chen', 'Wei', ''], ['Fan', 'Lixin', ''], ['Jin', 'Hai', ''], ['Yang', 'Qiang', '']]","extracted_entities":"[{'text': 'Large\\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'Gemini', 'label': 'ChatGPT'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Large Language\\nModels', 'label': 'Large Language Model'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2407.0841,"submitter":"Robbie Holland","authors":"Robbie Holland, Thomas R. P. Taylor, Christopher Holmes, Sophie Riedl,\n  Julia Mai, Maria Patsiamanidi, Dimitra Mitsopoulou, Paul Hager, Philip\n  M\\\"uller, Hendrik P. N. Scholl, Hrvoje Bogunovi\\'c, Ursula Schmidt-Erfurth,\n  Daniel Rueckert, Sobha Sivaprasad, Andrew J. Lotery, Martin J. Menten (on\n  behalf of the PINNACLE consortium)","title":"Specialized curricula for training vision-language models in retinal\n  image analysis","comments":"Under review at npj Digital Medicine","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Clinicians spend a significant amount of time reviewing medical images and\ntranscribing their findings regarding patient diagnosis, referral and treatment\nin text form. Vision-language models (VLMs), which automatically interpret\nimages and summarize their findings as text, have enormous potential to\nalleviate clinical workloads and increase patient access to high-quality\nmedical care. While foundational models have stirred considerable interest in\nthe medical community, it is unclear whether their general capabilities\ntranslate to real-world clinical utility. In this work, we demonstrate that\nOpenAI's ChatGPT-4o model, in addition to two foundation VLMs designed for\nmedical use, markedly underperform compared to practicing ophthalmologists on\nspecialist tasks crucial to the care of patients with age-related macular\ndegeneration (AMD). To address this, we initially identified the essential\ncapabilities required for image-based clinical decision-making, and then\ndeveloped a curriculum to selectively train VLMs in these skills. The resulting\nmodel, RetinaVLM, can be instructed to write reports that significantly\noutperform those written by leading foundation medical VLMs and ChatGPT-4o in\ndisease staging (F1 score of 0.63 vs. 0.33) and patient referral (0.67 vs.\n0.50), and approaches the diagnostic performance of junior ophthalmologists\n(who achieve 0.77 and 0.78 on the respective tasks). Furthermore, in a\nsingle-blind reader study two senior ophthalmologists with up to 32 years of\nexperience found RetinaVLM's reports were found to be substantially more\naccurate than those by ChatGPT-4o (64.3% vs. 14.3%). These results reinforce\nthat our curriculum-based approach provides a blueprint towards specializing\nfoundation medical VLMs for real-world clinical tasks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 11 Jul 2024 11:31:48 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 01:54:59 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Holland', 'Robbie', '', 'on\\n  behalf of the PINNACLE consortium'], ['Taylor', 'Thomas R. P.', '', 'on\\n  behalf of the PINNACLE consortium'], ['Holmes', 'Christopher', '', 'on\\n  behalf of the PINNACLE consortium'], ['Riedl', 'Sophie', '', 'on\\n  behalf of the PINNACLE consortium'], ['Mai', 'Julia', '', 'on\\n  behalf of the PINNACLE consortium'], ['Patsiamanidi', 'Maria', '', 'on\\n  behalf of the PINNACLE consortium'], ['Mitsopoulou', 'Dimitra', '', 'on\\n  behalf of the PINNACLE consortium'], ['Hager', 'Paul', '', 'on\\n  behalf of the PINNACLE consortium'], ['M\u00fcller', 'Philip', '', 'on\\n  behalf of the PINNACLE consortium'], ['Scholl', 'Hendrik P. N.', '', 'on\\n  behalf of the PINNACLE consortium'], ['Bogunovi\u0107', 'Hrvoje', '', 'on\\n  behalf of the PINNACLE consortium'], ['Schmidt-Erfurth', 'Ursula', '', 'on\\n  behalf of the PINNACLE consortium'], ['Rueckert', 'Daniel', '', 'on\\n  behalf of the PINNACLE consortium'], ['Sivaprasad', 'Sobha', '', 'on\\n  behalf of the PINNACLE consortium'], ['Lotery', 'Andrew J.', '', 'on\\n  behalf of the PINNACLE consortium'], ['Menten', 'Martin J.', '', 'on\\n  behalf of the PINNACLE consortium']]","extracted_entities":"[{'text': 'OpenAI', 'label': 'Open-source LLMs'}, {'text': 'ChatGPT-4o', 'label': 'ChatGPT'}, {'text': 'RetinaVLM', 'label': 'Foundation Model'}, {'text': 'ChatGPT-4o', 'label': 'ChatGPT'}, {'text': 'RetinaVLM', 'label': 'Foundation Model'}, {'text': 'ChatGPT-4o', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT-4o","similarity_score":0.8508302569}
{"id":1802.04233,"submitter":"Thomas Lasko","authors":"Jacek M. Bajor, Diego A. Mesa, Travis J. Osterman, Thomas A. Lasko","title":"Embedding Complexity In the Data Representation Instead of In the Model:\n  A Case Study Using Heterogeneous Medical Data","comments":"9 pages, 5 figures. This version only removed conference submission\n  info","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.AP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Electronic Health Records have become popular sources of data for secondary\nresearch, but their use is hampered by the amount of effort it takes to\novercome the sparsity, irregularity, and noise that they contain. Modern\nlearning architectures can remove the need for expert-driven feature\nengineering, but not the need for expert-driven preprocessing to abstract away\nthe inherent messiness of clinical data. This preprocessing effort is often the\ndominant component of a typical clinical prediction project. In this work we\npropose using semantic embedding methods to directly couple the raw, messy\nclinical data to downstream learning architectures with truly minimal\npreprocessing. We examine this step from the perspective of capturing and\nencoding complex data dependencies in the data representation instead of in the\nmodel, which has the nice benefit of allowing downstream processing to be done\nwith fast, lightweight, and simple models accessible to researchers without\nmachine learning expertise. We demonstrate with three typical clinical\nprediction tasks that the highly compressed, embedded data representations\ncapture a large amount of useful complexity, although in some cases the\ncompression is not completely lossless.\n","versions":"[{'version': 'v1', 'created': 'Mon, 12 Feb 2018 18:31:24 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 22:25:00 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Bajor', 'Jacek M.', ''], ['Mesa', 'Diego A.', ''], ['Osterman', 'Travis J.', ''], ['Lasko', 'Thomas A.', '']]","extracted_entities":"[{'text': 'semantic embedding methods', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"semantic embedding methods","similarity_score":0.7070268393}
{"id":2301.07275,"submitter":"Yinqian Sun","authors":"Yinqian Sun, Feifei Zhao, Zhuoya Zhao and Yi Zeng","title":"Multi-compartment Neuron and Population Encoding Powered Spiking Neural\n  Network for Deep Distributional Reinforcement Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.NE cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Inspired by the brain's information processing using binary spikes, spiking\nneural networks (SNNs) offer significant reductions in energy consumption and\nare more adept at incorporating multi-scale biological characteristics. In\nSNNs, spiking neurons serve as the fundamental information processing units.\nHowever, in most models, these neurons are typically simplified, focusing\nprimarily on the leaky integrate-and-fire (LIF) point neuron model while\nneglecting the structural properties of biological neurons. This simplification\nhampers the computational and learning capabilities of SNNs. In this paper, we\npropose a brain-inspired deep distributional reinforcement learning algorithm\nbased on SNNs, which integrates a bio-inspired multi-compartment neuron (MCN)\nmodel with a population coding approach. The proposed MCN model simulates the\nstructure and function of apical dendritic, basal dendritic, and somatic\ncompartments, achieving computational power comparable to that of biological\nneurons. Additionally, we introduce an implicit fractional embedding method\nbased on population coding of spiking neurons. We evaluated our model on Atari\ngames, and the experimental results demonstrate that it surpasses the vanilla\nFQF model, which utilizes traditional artificial neural networks (ANNs), as\nwell as the Spiking-FQF models that are based on ANN-to-SNN conversion methods.\nAblation studies further reveal that the proposed multi-compartment neuron\nmodel and the quantile fraction implicit population spike representation\nsignificantly enhance the performance of MCS-FQF while also reducing power\nconsumption.\n","versions":"[{'version': 'v1', 'created': 'Wed, 18 Jan 2023 02:45:38 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 02:49:20 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Sun', 'Yinqian', ''], ['Zhao', 'Feifei', ''], ['Zhao', 'Zhuoya', ''], ['Zeng', 'Yi', '']]","extracted_entities":"[{'text': 'implicit fractional embedding method', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"implicit fractional embedding method","similarity_score":0.5433442593}
{"id":2402.19097,"submitter":"Alexander Shabalin","authors":"Alexander Shabalin, Viacheslav Meshchaninov, Egor Chimbulatov,\n  Vladislav Lapikov, Roman Kim, Grigory Bartosh, Dmitry Molchanov, Sergey\n  Markov, Dmitry Vetrov","title":"TEncDM: Understanding the Properties of the Diffusion Model in the Space\n  of Language Model Encodings","comments":"15 pages, 13 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper presents the Text Encoding Diffusion Model (TEncDM), a novel\napproach to diffusion modeling that operates in the space of pre-trained\nlanguage model encodings. In contrast to traditionally used embeddings,\nencodings integrate contextual information. In our approach, we also employ a\ntransformer-based decoder, specifically designed to incorporate context in the\ntoken prediction process. We conduct a comprehensive examination of the\ninfluence of the encoder, decoder, noise scheduler, and self-conditioning on\nzero-shot generation. Furthermore, we compare TEncDM with previous approaches\non three conditional text generation tasks: QQP, XSum, and Wiki-Auto. The\nresults show that TEncDM exhibits superior performance compared to existing\nnon-autoregressive diffusion models. Our code is available at\nhttps:\/\/github.com\/M0RJIQUE\/tencdm.\n","versions":"[{'version': 'v1', 'created': 'Thu, 29 Feb 2024 12:25:45 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Aug 2024 09:35:24 GMT'}, {'version': 'v3', 'created': 'Wed, 18 Dec 2024 16:30:58 GMT'}, {'version': 'v4', 'created': 'Mon, 24 Feb 2025 13:06:32 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Shabalin', 'Alexander', ''], ['Meshchaninov', 'Viacheslav', ''], ['Chimbulatov', 'Egor', ''], ['Lapikov', 'Vladislav', ''], ['Kim', 'Roman', ''], ['Bartosh', 'Grigory', ''], ['Molchanov', 'Dmitry', ''], ['Markov', 'Sergey', ''], ['Vetrov', 'Dmitry', '']]","extracted_entities":"[{'text': 'embeddings', 'label': 'Embedding'}, {'text': 'encodings', 'label': 'Embedding'}, {'text': 'zero-shot generation', 'label': 'Zero-shot Learning'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2404.0647,"submitter":"Rohan Sarkar","authors":"Rohan Sarkar, Avinash Kak","title":"A Dataset and Framework for Learning State-invariant Object\n  Representations","comments":"This work has been submitted to the IEEE for possible publication","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.IR cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We add one more invariance - the state invariance - to the more commonly used\nother invariances for learning object representations for recognition and\nretrieval. By state invariance, we mean robust with respect to changes in the\nstructural form of the objects, such as when an umbrella is folded, or when an\nitem of clothing is tossed on the floor. In this work, we present a novel\ndataset, ObjectsWithStateChange, which captures state and pose variations in\nthe object images recorded from arbitrary viewpoints. We believe that this\ndataset will facilitate research in fine-grained object recognition and\nretrieval of 3D objects that are capable of state changes. The goal of such\nresearch would be to train models capable of learning discriminative object\nembeddings that remain invariant to state changes while also staying invariant\nto transformations induced by changes in viewpoint, pose, illumination, etc. A\nmajor challenge in this regard is that instances of different objects (both\nwithin and across different categories) under various state changes may share\nsimilar visual characteristics and therefore may be close to one another in the\nlearned embedding space, which would make it more difficult to discriminate\nbetween them. To address this, we propose a curriculum learning strategy that\nprogressively selects object pairs with smaller inter-object distances in the\nlearned embedding space during the training phase. This approach gradually\nsamples harder-to-distinguish examples of visually similar objects, both within\nand across different categories. Our ablation related to the role played by\ncurriculum learning indicates an improvement in object recognition accuracy of\n7.9% and retrieval mAP of 9.2% over the state-of-the-art on our new dataset, as\nwell as three other challenging multi-view datasets such as ModelNet40,\nObjectPI, and FG3D.\n","versions":"[{'version': 'v1', 'created': 'Tue, 9 Apr 2024 17:17:48 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 17:26:00 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Sarkar', 'Rohan', ''], ['Kak', 'Avinash', '']]","extracted_entities":"[{'text': 'discriminative object\\nembeddings', 'label': 'Embedding'}, {'text': 'curriculum learning', 'label': 'Few-shot Learning'}, {'text': 'curriculum learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Embedding","matched_keyword":"discriminative object\nembeddings","similarity_score":0.6400979161}
{"id":2404.07575,"submitter":"Tien-Hong Lo","authors":"Tien-Hong Lo, Fu-An Chao, Tzu-I Wu, Yao-Ting Sung, Berlin Chen","title":"An Effective Automated Speaking Assessment Approach to Mitigating Data\n  Scarcity and Imbalanced Distribution","comments":"Accepted to NAACL 2024 Findings","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.AI eess.AS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Automated speaking assessment (ASA) typically involves automatic speech\nrecognition (ASR) and hand-crafted feature extraction from the ASR transcript\nof a learner's speech. Recently, self-supervised learning (SSL) has shown\nstellar performance compared to traditional methods. However, SSL-based ASA\nsystems are faced with at least three data-related challenges: limited\nannotated data, uneven distribution of learner proficiency levels and\nnon-uniform score intervals between different CEFR proficiency levels. To\naddress these challenges, we explore the use of two novel modeling strategies:\nmetric-based classification and loss reweighting, leveraging distinct SSL-based\nembedding features. Extensive experimental results on the ICNALE benchmark\ndataset suggest that our approach can outperform existing strong baselines by a\nsizable margin, achieving a significant improvement of more than 10% in CEFR\nprediction accuracy.\n","versions":"[{'version': 'v1', 'created': 'Thu, 11 Apr 2024 09:06:49 GMT'}, {'version': 'v2', 'created': 'Fri, 12 Apr 2024 01:22:47 GMT'}, {'version': 'v3', 'created': 'Thu, 27 Feb 2025 07:19:22 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Lo', 'Tien-Hong', ''], ['Chao', 'Fu-An', ''], ['Wu', 'Tzu-I', ''], ['Sung', 'Yao-Ting', ''], ['Chen', 'Berlin', '']]","extracted_entities":"[{'text': 'self-supervised learning', 'label': 'Embedding'}, {'text': 'SSL-based\\nembedding features', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"SSL-based\nembedding features","similarity_score":0.5272803307}
{"id":2404.19227,"submitter":"Anudeep Das","authors":"Anudeep Das, Vasisht Duddu, Rui Zhang, N. Asokan","title":"Espresso: Robust Concept Filtering in Text-to-Image Models","comments":"ACM Conference on Data and Application Security and Privacy\n  (CODASPY), 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Diffusion based text-to-image models are trained on large datasets scraped\nfrom the Internet, potentially containing unacceptable concepts (e.g.,\ncopyright-infringing or unsafe). We need concept removal techniques (CRTs)\nwhich are i) effective in preventing the generation of images with unacceptable\nconcepts, ii) utility-preserving on acceptable concepts, and, iii) robust\nagainst evasion with adversarial prompts. No prior CRT satisfies all these\nrequirements simultaneously. We introduce Espresso, the first robust concept\nfilter based on Contrastive Language-Image Pre-Training (CLIP). We identify\nunacceptable concepts by using the distance between the embedding of a\ngenerated image to the text embeddings of both unacceptable and acceptable\nconcepts. This lets us fine-tune for robustness by separating the text\nembeddings of unacceptable and acceptable concepts while preserving utility. We\npresent a pipeline to evaluate various CRTs to show that Espresso is more\neffective and robust than prior CRTs, while retaining utility.\n","versions":"[{'version': 'v1', 'created': 'Tue, 30 Apr 2024 03:13:06 GMT'}, {'version': 'v2', 'created': 'Wed, 1 May 2024 18:30:14 GMT'}, {'version': 'v3', 'created': 'Wed, 8 May 2024 00:22:32 GMT'}, {'version': 'v4', 'created': 'Fri, 7 Jun 2024 14:28:24 GMT'}, {'version': 'v5', 'created': 'Mon, 9 Sep 2024 16:51:21 GMT'}, {'version': 'v6', 'created': 'Sun, 15 Dec 2024 16:20:37 GMT'}, {'version': 'v7', 'created': 'Wed, 26 Feb 2025 14:53:47 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Das', 'Anudeep', ''], ['Duddu', 'Vasisht', ''], ['Zhang', 'Rui', ''], ['Asokan', 'N.', '']]","extracted_entities":"[{'text': 'adversarial prompts', 'label': 'Prompting'}, {'text': 'text embeddings', 'label': 'Embedding'}, {'text': 'fine-tune for robustness', 'label': 'Fine-tuning'}, {'text': 'text\\nembeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"text embeddings","similarity_score":0.812117815}
{"id":2405.16865,"submitter":"Dehong Xu","authors":"Dehong Xu, Ruiqi Gao, Wen-Hao Zhang, Xue-Xin Wei, Ying Nian Wu","title":"On Conformal Isometry of Grid Cells: Learning Distance-Preserving\n  Position Embedding","comments":"arXiv admin note: text overlap with arXiv:2310.19192","journal-ref":null,"doi":null,"report-no":null,"categories":"q-bio.NC cs.LG stat.ML","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper investigates the conformal isometry hypothesis as a potential\nexplanation for the hexagonal periodic patterns in grid cell response maps. We\nposit that grid cell activities form a high-dimensional vector in neural space,\nencoding the agent's position in 2D physical space. As the agent moves, this\nvector rotates within a 2D manifold in the neural space, driven by a recurrent\nneural network. The conformal hypothesis proposes that this neural manifold is\na conformal isometric embedding of 2D physical space, where local physical\ndistance is preserved by the embedding up to a scaling factor (or unit of\nmetric). Such distance-preserving position embedding is indispensable for path\nplanning in navigation, especially planning local straight path segments. We\nconduct numerical experiments to show that this hypothesis leads to the\nhexagonal grid firing patterns by learning maximally distance-preserving\nposition embedding, agnostic to the choice of the recurrent neural network.\nFurthermore, we present a theoretical explanation of why hexagon periodic\npatterns emerge by minimizing our loss function by showing that hexagon flat\ntorus is maximally distance preserving.\n","versions":"[{'version': 'v1', 'created': 'Mon, 27 May 2024 06:31:39 GMT'}, {'version': 'v2', 'created': 'Thu, 10 Oct 2024 06:27:11 GMT'}, {'version': 'v3', 'created': 'Thu, 9 Jan 2025 19:39:12 GMT'}, {'version': 'v4', 'created': 'Thu, 27 Feb 2025 07:31:38 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Xu', 'Dehong', ''], ['Gao', 'Ruiqi', ''], ['Zhang', 'Wen-Hao', ''], ['Wei', 'Xue-Xin', ''], ['Wu', 'Ying Nian', '']]","extracted_entities":"[{'text': 'embedding', 'label': 'Embedding'}, {'text': 'embedding', 'label': 'Embedding'}, {'text': 'scaling factor', 'label': 'Scaling law'}, {'text': 'maximally distance-preserving\\nposition embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embedding","similarity_score":1.0}
{"id":2406.10354,"submitter":"Barbora Barancikova","authors":"Barbora Barancikova, Zhuoyue Huang, Cristopher Salvi","title":"SigDiffusions: Score-Based Diffusion Models for Time Series via\n  Log-Signature Embeddings","comments":"Published at ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Score-based diffusion models have recently emerged as state-of-the-art\ngenerative models for a variety of data modalities. Nonetheless, it remains\nunclear how to adapt these models to generate long multivariate time series.\nViewing a time series as the discretisation of an underlying continuous\nprocess, we introduce SigDiffusion, a novel diffusion model operating on\nlog-signature embeddings of the data. The forward and backward processes\ngradually perturb and denoise log-signatures while preserving their algebraic\nstructure. To recover a signal from its log-signature, we provide new\nclosed-form inversion formulae expressing the coefficients obtained by\nexpanding the signal in a given basis (e.g. Fourier or orthogonal polynomials)\nas explicit polynomial functions of the log-signature. Finally, we show that\ncombining SigDiffusions with these inversion formulae results in high-quality\nlong time series generation, competitive with the current state-of-the-art on\nvarious datasets of synthetic and real-world examples.\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Jun 2024 18:04:06 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 19:38:40 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Barancikova', 'Barbora', ''], ['Huang', 'Zhuoyue', ''], ['Salvi', 'Cristopher', '']]","extracted_entities":"[{'text': 'log-signature embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"log-signature embeddings","similarity_score":0.5123707056}
{"id":2407.09774,"submitter":"Sixiao Zheng","authors":"Sixiao Zheng, Yanwei Fu","title":"ContextualStory: Consistent Visual Storytelling with Spatially-Enhanced\n  and Storyline Context","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.MM","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Visual storytelling involves generating a sequence of coherent frames from a\ntextual storyline while maintaining consistency in characters and scenes.\nExisting autoregressive methods, which rely on previous frame-sentence pairs,\nstruggle with high memory usage, slow generation speeds, and limited context\nintegration. To address these issues, we propose ContextualStory, a novel\nframework designed to generate coherent story frames and extend frames for\nvisual storytelling. ContextualStory utilizes Spatially-Enhanced Temporal\nAttention to capture spatial and temporal dependencies, handling significant\ncharacter movements effectively. Additionally, we introduce a Storyline\nContextualizer to enrich context in storyline embedding, and a StoryFlow\nAdapter to measure scene changes between frames for guiding the model.\nExtensive experiments on PororoSV and FlintstonesSV datasets demonstrate that\nContextualStory significantly outperforms existing SOTA methods in both story\nvisualization and continuation. Code is available at\nhttps:\/\/github.com\/sixiaozheng\/ContextualStory.\n","versions":"[{'version': 'v1', 'created': 'Sat, 13 Jul 2024 05:02:42 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Aug 2024 14:17:31 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 14:02:08 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Zheng', 'Sixiao', ''], ['Fu', 'Yanwei', '']]","extracted_entities":"[{'text': 'ContextualStory', 'label': 'contextual Embedding'}, {'text': 'ContextualStory', 'label': 'contextual Embedding'}, {'text': 'Spatially-Enhanced Temporal\\nAttention', 'label': 'Attention mechanism'}, {'text': 'Storyline\\nContextualizer', 'label': 'contextual Embedding'}, {'text': 'storyline embedding', 'label': 'Embedding'}, {'text': 'ContextualStory', 'label': 'contextual Embedding'}, {'text': 'ContextualStory', 'label': 'contextual Embedding'}]","assigned_concept":"Embedding","matched_keyword":"storyline embedding","similarity_score":0.642375946}
{"id":2305.16735,"submitter":"Xiaochun Meng","authors":"James W. Taylor and Xiaochun Meng","title":"Angular Combining of Forecasts of Probability Distributions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ME stat.AP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  When multiple forecasts are available for a probability distribution,\nforecast combining enables a pragmatic synthesis of the information to extract\nthe wisdom of the crowd. The linear opinion pool has been widely used, whereby\nthe combining is applied to the probabilities of the distributional forecasts.\nHowever, it has been argued that this will tend to deliver overdispersed\ndistributions, prompting the combination to be applied, instead, to the\nquantiles of the distributional forecasts. Results from different applications\nare mixed, leaving it as an empirical question whether to combine probabilities\nor quantiles. In this paper, we present an alternative approach. Looking at the\ndistributional forecasts, combining the probabilities can be viewed as vertical\ncombining, with quantile combining seen as horizontal combining. Our proposal\nis to allow combining to take place on an angle between the extreme cases of\nvertical and horizontal combining. We term this angular combining. The angle is\na parameter that can be optimized using a proper scoring rule. For\nimplementation, we provide a pragmatic numerical approach and a simulation\nalgorithm. Among our theoretical results, we show that, as with vertical and\nhorizontal averaging, angular averaging results in a distribution with mean\nequal to the average of the means of the distributions that are being combined.\nWe also show that angular averaging produces a distribution with lower variance\nthan vertical averaging, and, under certain assumptions, greater variance than\nhorizontal averaging. We provide empirical results for distributional forecasts\nof Covid mortality, macroeconomic survey data, and electricity prices.\n","versions":"[{'version': 'v1', 'created': 'Fri, 26 May 2023 08:38:44 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 00:30:25 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Taylor', 'James W.', ''], ['Meng', 'Xiaochun', '']]","extracted_entities":"[{'text': 'prompting', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2306.04347,"submitter":"Andreas Opedal","authors":"Andreas Opedal, Niklas Stoehr, Abulhair Saparov, Mrinmaya Sachan","title":"World Models for Math Story Problems","comments":"ACL Findings 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Solving math story problems is a complex task for students and NLP models\nalike, requiring them to understand the world as described in the story and\nreason over it to compute an answer. Recent years have seen impressive\nperformance on automatically solving these problems with large pre-trained\nlanguage models and innovative techniques to prompt them. However, it remains\nunclear if these models possess accurate representations of mathematical\nconcepts. This leads to lack of interpretability and trustworthiness which\nimpedes their usefulness in various applications. In this paper, we consolidate\nprevious work on categorizing and representing math story problems and develop\nMathWorld, which is a graph-based semantic formalism specific for the domain of\nmath story problems. With MathWorld, we can assign world models to math story\nproblems which represent the situations and actions introduced in the text and\ntheir mathematical relationships. We combine math story problems from several\nexisting datasets and annotate a corpus of 1,019 problems and 3,204 logical\nforms with MathWorld. Using this data, we demonstrate the following use cases\nof MathWorld: (1) prompting language models with synthetically generated\nquestion-answer pairs to probe their reasoning and world modeling abilities,\nand (2) generating new problems by using the world models as a design space.\n","versions":"[{'version': 'v1', 'created': 'Wed, 7 Jun 2023 11:25:20 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 10:11:39 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Opedal', 'Andreas', ''], ['Stoehr', 'Niklas', ''], ['Saparov', 'Abulhair', ''], ['Sachan', 'Mrinmaya', '']]","extracted_entities":"[{'text': 'prompting', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2405.17082,"submitter":"Cong Wang","authors":"Cong Wang, Kuan Tian, Yonghang Guan, Fei Shen, Zhiwei Jiang, Qing Gu,\n  Jun Zhang","title":"Ensembling Diffusion Models via Adaptive Feature Aggregation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The success of the text-guided diffusion model has inspired the development\nand release of numerous powerful diffusion models within the open-source\ncommunity. These models are typically fine-tuned on various expert datasets,\nshowcasing diverse denoising capabilities. Leveraging multiple high-quality\nmodels to produce stronger generation ability is valuable, but has not been\nextensively studied. Existing methods primarily adopt parameter merging\nstrategies to produce a new static model. However, they overlook the fact that\nthe divergent denoising capabilities of the models may dynamically change\nacross different states, such as when experiencing different prompts, initial\nnoises, denoising steps, and spatial locations. In this paper, we propose a\nnovel ensembling method, Adaptive Feature Aggregation (AFA), which dynamically\nadjusts the contributions of multiple models at the feature level according to\nvarious states (i.e., prompts, initial noises, denoising steps, and spatial\nlocations), thereby keeping the advantages of multiple diffusion models, while\nsuppressing their disadvantages. Specifically, we design a lightweight\nSpatial-Aware Block-Wise (SABW) feature aggregator that adaptive aggregates the\nblock-wise intermediate features from multiple U-Net denoisers into a unified\none. The core idea lies in dynamically producing an individual attention map\nfor each model's features by comprehensively considering various states. It is\nworth noting that only SABW is trainable with about 50 million parameters,\nwhile other models are frozen. Both the quantitative and qualitative\nexperiments demonstrate the effectiveness of our proposed Adaptive Feature\nAggregation method.\n","versions":"[{'version': 'v1', 'created': 'Mon, 27 May 2024 11:55:35 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 07:35:14 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Wang', 'Cong', ''], ['Tian', 'Kuan', ''], ['Guan', 'Yonghang', ''], ['Shen', 'Fei', ''], ['Jiang', 'Zhiwei', ''], ['Gu', 'Qing', ''], ['Zhang', 'Jun', '']]","extracted_entities":"[{'text': 'open-source\\ncommunity', 'label': 'Open-source LLMs'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'denoising steps', 'label': 'Attention mechanism'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'denoising steps', 'label': 'Attention mechanism'}]","assigned_concept":"Prompting","matched_keyword":"prompts","similarity_score":0.7638334036}
{"id":2406.00036,"submitter":"Yinghao Zhu","authors":"Yinghao Zhu, Changyu Ren, Zixiang Wang, Xiaochen Zheng, Shiyun Xie,\n  Junlan Feng, Xi Zhu, Zhoujun Li, Liantao Ma, Chengwei Pan","title":"EMERGE: Enhancing Multimodal Electronic Health Records Predictive\n  Modeling with Retrieval-Augmented Generation","comments":"CIKM 2024 Full Research Paper; arXiv admin note: text overlap with\n  arXiv:2402.07016","journal-ref":null,"doi":"10.1145\/3627673.3679582","report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The integration of multimodal Electronic Health Records (EHR) data has\nsignificantly advanced clinical predictive capabilities. Existing models, which\nutilize clinical notes and multivariate time-series EHR data, often fall short\nof incorporating the necessary medical context for accurate clinical tasks,\nwhile previous approaches with knowledge graphs (KGs) primarily focus on\nstructured knowledge extraction. In response, we propose EMERGE, a\nRetrieval-Augmented Generation (RAG) driven framework to enhance multimodal EHR\npredictive modeling. We extract entities from both time-series data and\nclinical notes by prompting Large Language Models (LLMs) and align them with\nprofessional PrimeKG, ensuring consistency. In addition to triplet\nrelationships, we incorporate entities' definitions and descriptions for richer\nsemantics. The extracted knowledge is then used to generate task-relevant\nsummaries of patients' health statuses. Finally, we fuse the summary with other\nmodalities using an adaptive multimodal fusion network with cross-attention.\nExtensive experiments on the MIMIC-III and MIMIC-IV datasets' in-hospital\nmortality and 30-day readmission tasks demonstrate the superior performance of\nthe EMERGE framework over baseline models. Comprehensive ablation studies and\nanalysis highlight the efficacy of each designed module and robustness to data\nsparsity. EMERGE contributes to refining the utilization of multimodal EHR data\nin healthcare, bridging the gap with nuanced medical contexts essential for\ninformed clinical predictions. We have publicly released the code at\nhttps:\/\/github.com\/yhzhu99\/EMERGE.\n","versions":"[{'version': 'v1', 'created': 'Mon, 27 May 2024 10:53:15 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 13:18:09 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Zhu', 'Yinghao', ''], ['Ren', 'Changyu', ''], ['Wang', 'Zixiang', ''], ['Zheng', 'Xiaochen', ''], ['Xie', 'Shiyun', ''], ['Feng', 'Junlan', ''], ['Zhu', 'Xi', ''], ['Li', 'Zhoujun', ''], ['Ma', 'Liantao', ''], ['Pan', 'Chengwei', '']]","extracted_entities":"[{'text': 'EMERGE', 'label': 'RAG'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'cross-attention', 'label': 'Attention mechanism'}, {'text': 'EMERGE', 'label': 'RAG'}, {'text': 'EMERGE', 'label': 'RAG'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2406.06608,"submitter":"Sander Schulhoff","authors":"Sander Schulhoff, Michael Ilie, Nishant Balepur, Konstantine Kahadze,\n  Amanda Liu, Chenglei Si, Yinheng Li, Aayush Gupta, HyoJung Han, Sevien\n  Schulhoff, Pranav Sandeep Dulepet, Saurav Vidyadhara, Dayeon Ki, Sweta\n  Agrawal, Chau Pham, Gerson Kroiz, Feileen Li, Hudson Tao, Ashay Srivastava,\n  Hevander Da Costa, Saloni Gupta, Megan L. Rogers, Inna Goncearenco, Giuseppe\n  Sarli, Igor Galynker, Denis Peskoff, Marine Carpuat, Jules White, Shyamal\n  Anadkat, Alexander Hoyle, Philip Resnik","title":"The Prompt Report: A Systematic Survey of Prompt Engineering Techniques","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Generative Artificial Intelligence (GenAI) systems are increasingly being\ndeployed across diverse industries and research domains. Developers and\nend-users interact with these systems through the use of prompting and prompt\nengineering. Although prompt engineering is a widely adopted and extensively\nresearched area, it suffers from conflicting terminology and a fragmented\nontological understanding of what constitutes an effective prompt due to its\nrelatively recent emergence. We establish a structured understanding of prompt\nengineering by assembling a taxonomy of prompting techniques and analyzing\ntheir applications. We present a detailed vocabulary of 33 vocabulary terms, a\ntaxonomy of 58 LLM prompting techniques, and 40 techniques for other\nmodalities. Additionally, we provide best practices and guidelines for prompt\nengineering, including advice for prompting state-of-the-art (SOTA) LLMs such\nas ChatGPT. We further present a meta-analysis of the entire literature on\nnatural language prefix-prompting. As a culmination of these efforts, this\npaper presents the most comprehensive survey on prompt engineering to date.\n","versions":"[{'version': 'v1', 'created': 'Thu, 6 Jun 2024 18:10:11 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Jun 2024 01:28:09 GMT'}, {'version': 'v3', 'created': 'Mon, 15 Jul 2024 03:17:50 GMT'}, {'version': 'v4', 'created': 'Mon, 23 Dec 2024 18:38:36 GMT'}, {'version': 'v5', 'created': 'Mon, 30 Dec 2024 19:33:09 GMT'}, {'version': 'v6', 'created': 'Wed, 26 Feb 2025 18:59:01 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Schulhoff', 'Sander', ''], ['Ilie', 'Michael', ''], ['Balepur', 'Nishant', ''], ['Kahadze', 'Konstantine', ''], ['Liu', 'Amanda', ''], ['Si', 'Chenglei', ''], ['Li', 'Yinheng', ''], ['Gupta', 'Aayush', ''], ['Han', 'HyoJung', ''], ['Schulhoff', 'Sevien', ''], ['Dulepet', 'Pranav Sandeep', ''], ['Vidyadhara', 'Saurav', ''], ['Ki', 'Dayeon', ''], ['Agrawal', 'Sweta', ''], ['Pham', 'Chau', ''], ['Kroiz', 'Gerson', ''], ['Li', 'Feileen', ''], ['Tao', 'Hudson', ''], ['Srivastava', 'Ashay', ''], ['Da Costa', 'Hevander', ''], ['Gupta', 'Saloni', ''], ['Rogers', 'Megan L.', ''], ['Goncearenco', 'Inna', ''], ['Sarli', 'Giuseppe', ''], ['Galynker', 'Igor', ''], ['Peskoff', 'Denis', ''], ['Carpuat', 'Marine', ''], ['White', 'Jules', ''], ['Anadkat', 'Shyamal', ''], ['Hoyle', 'Alexander', ''], ['Resnik', 'Philip', '']]","extracted_entities":"[{'text': 'prompting', 'label': 'Prompting'}, {'text': 'prompt\\nengineering', 'label': 'Prompting'}, {'text': 'prompt engineering', 'label': 'Prompting'}, {'text': 'prompt\\nengineering', 'label': 'Prompting'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'prompting techniques', 'label': 'Prompting'}, {'text': 'prompt\\nengineering', 'label': 'Prompting'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'prompt engineering', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2406.10126,"submitter":"Chen Hou","authors":"Chen Hou, Zhibo Chen","title":"Training-free Camera Control for Video Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We propose a training-free and robust solution to offer camera movement\ncontrol for off-the-shelf video diffusion models. Unlike previous work, our\nmethod does not require any supervised finetuning on camera-annotated datasets\nor self-supervised training via data augmentation. Instead, it can be\nplug-and-play with most pretrained video diffusion models and generate\ncamera-controllable videos with a single image or text prompt as input. The\ninspiration for our work comes from the layout prior that intermediate latents\nencode for the generated results, thus rearranging noisy pixels in them will\ncause the output content to relocate as well. As camera moving could also be\nseen as a type of pixel rearrangement caused by perspective change, videos can\nbe reorganized following specific camera motion if their noisy latents change\naccordingly. Building on this, we propose CamTrol, which enables robust camera\ncontrol for video diffusion models. It is achieved by a two-stage process.\nFirst, we model image layout rearrangement through explicit camera movement in\n3D point cloud space. Second, we generate videos with camera motion by\nleveraging the layout prior of noisy latents formed by a series of rearranged\nimages. Extensive experiments have demonstrated its superior performance in\nboth video generation and camera motion alignment compared with other finetuned\nmethods. Furthermore, we show the capability of CamTrol to generalize to\nvarious base models, as well as its impressive applications in scalable motion\ncontrol, dealing with complicated trajectories and unsupervised 3D video\ngeneration. Videos available at https:\/\/lifedecoder.github.io\/CamTrol\/.\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Jun 2024 15:33:00 GMT'}, {'version': 'v2', 'created': 'Fri, 6 Sep 2024 10:25:23 GMT'}, {'version': 'v3', 'created': 'Mon, 16 Dec 2024 03:13:09 GMT'}, {'version': 'v4', 'created': 'Tue, 25 Feb 2025 00:32:29 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Hou', 'Chen', ''], ['Chen', 'Zhibo', '']]","extracted_entities":"[{'text': 'supervised finetuning', 'label': 'Fine-tuning'}, {'text': 'text prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"text prompt","similarity_score":0.6277507544}
{"id":2408.02454,"submitter":"Daeun Song","authors":"Daeun Song, Jing Liang, Xuesu Xiao, Dinesh Manocha","title":"VL-TGS: Trajectory Generation and Selection using Vision Language Models\n  in Mapless Outdoor Environments","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present a multi-modal trajectory generation and selection algorithm for\nreal-world mapless outdoor navigation in human-centered environments. Such\nenvironments contain rich features like crosswalks, grass, and curbs, which are\neasily interpretable by humans, but not by mobile robots. We aim to compute\nsuitable trajectories that (1) satisfy the environment-specific traversability\nconstraints and (2) generate human-like paths while navigating on crosswalks,\nsidewalks, etc. Our formulation uses a Conditional Variational Autoencoder\n(CVAE) generative model enhanced with traversability constraints to generate\nmultiple candidate trajectories for global navigation. We develop a visual\nprompting approach and leverage the Visual Language Model's (VLM) zero-shot\nability of semantic understanding and logical reasoning to choose the best\ntrajectory given the contextual information about the task. We evaluate our\nmethod in various outdoor scenes with wheeled robots and compare the\nperformance with other global navigation algorithms. In practice, we observe an\naverage improvement of 20.81% in satisfying traversability constraints and\n28.51% in terms of human-like navigation in four different outdoor navigation\nscenarios.\n","versions":"[{'version': 'v1', 'created': 'Mon, 5 Aug 2024 13:25:27 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Aug 2024 13:39:27 GMT'}, {'version': 'v3', 'created': 'Wed, 4 Dec 2024 09:26:27 GMT'}, {'version': 'v4', 'created': 'Tue, 25 Feb 2025 17:32:32 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Song', 'Daeun', ''], ['Liang', 'Jing', ''], ['Xiao', 'Xuesu', ''], ['Manocha', 'Dinesh', '']]","extracted_entities":"[{'text': 'visual\\nprompting approach', 'label': 'Prompting'}, {'text': 'Visual Language Model', 'label': 'Neural Language Model'}]","assigned_concept":"Prompting","matched_keyword":"visual\nprompting approach","similarity_score":0.7160181999}
{"id":2408.12594,"submitter":"Xingtong Yu","authors":"Xingtong Yu, Jie Zhang, Yuan Fang, Renhe Jiang","title":"Non-Homophilic Graph Pre-Training and Prompt Learning","comments":"Accepted by KDD 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Graphs are ubiquitous for modeling complex relationships between objects\nacross various fields. Graph neural networks (GNNs) have become a mainstream\ntechnique for graph-based applications, but their performance heavily relies on\nabundant labeled data. To reduce labeling requirement, pre-training and prompt\nlearning has become a popular alternative. However, most existing prompt\nmethods do not differentiate homophilic and heterophilic characteristics of\nreal-world graphs. In particular, many real-world graphs are non-homophilic,\nnot strictly or uniformly homophilic with mixing homophilic and heterophilic\npatterns, exhibiting varying non-homophilic characteristics across graphs and\nnodes. In this paper, we propose ProNoG, a novel pre-training and prompt\nlearning framework for such non-homophilic graphs. First, we analyze existing\ngraph pre-training methods, providing theoretical insights into the choice of\npre-training tasks. Second, recognizing that each node exhibits unique\nnon-homophilic characteristics, we propose a conditional network to\ncharacterize the node-specific patterns in downstream tasks. Finally, we\nthoroughly evaluate and analyze ProNoG through extensive experiments on ten\npublic datasets.\n","versions":"[{'version': 'v1', 'created': 'Thu, 22 Aug 2024 17:57:31 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Aug 2024 08:23:53 GMT'}, {'version': 'v3', 'created': 'Fri, 30 Aug 2024 10:55:58 GMT'}, {'version': 'v4', 'created': 'Sat, 7 Dec 2024 04:28:09 GMT'}, {'version': 'v5', 'created': 'Thu, 2 Jan 2025 04:43:11 GMT'}, {'version': 'v6', 'created': 'Wed, 26 Feb 2025 06:58:51 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Yu', 'Xingtong', ''], ['Zhang', 'Jie', ''], ['Fang', 'Yuan', ''], ['Jiang', 'Renhe', '']]","extracted_entities":"[{'text': 'prompt\\nlearning', 'label': 'Prompting'}, {'text': 'prompt\\nlearning', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt\nlearning","similarity_score":0.5904975533}
{"id":2309.02926,"submitter":"Guozhu Meng","authors":"Tong Liu, Zizhuang Deng, Guozhu Meng, Yuekang Li, Kai Chen","title":"Demystifying RCE Vulnerabilities in LLM-Integrated Apps","comments":null,"journal-ref":"Proceedings of the 2024 on ACM SIGSAC Conference on Computer and\n  Communications Security (CCS '24)","doi":"10.1145\/3658644.3690338","report-no":null,"categories":"cs.CR","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  LLMs show promise in transforming software development, with a growing\ninterest in integrating them into more intelligent apps. Frameworks like\nLangChain aid LLM-integrated app development, offering code execution\nutility\/APIs for custom actions. However, these capabilities theoretically\nintroduce Remote Code Execution (RCE) vulnerabilities, enabling remote code\nexecution through prompt injections. No prior research systematically\ninvestigates these frameworks' RCE vulnerabilities or their impact on\napplications and exploitation consequences. Therefore, there is a huge research\ngap in this field. In this study, we propose LLMSmith to detect, validate and\nexploit the RCE vulnerabilities in LLM-integrated frameworks and apps. To\nachieve this goal, we develop two novel techniques, including 1) a lightweight\nstatic analysis to examine LLM integration mechanisms, and construct call\nchains to identify RCE vulnerabilities in frameworks; 2) a systematical\nprompt-based exploitation method to verify and exploit the found\nvulnerabilities in LLM-integrated apps. This technique involves various\nstrategies to control LLM outputs, trigger RCE vulnerabilities and launch\nsubsequent attacks. Our research has uncovered a total of 20 vulnerabilities in\n11 LLM-integrated frameworks, comprising 19 RCE vulnerabilities and 1 arbitrary\nfile read\/write vulnerability. Of these, 17 have been confirmed by the\nframework developers, with 11 vulnerabilities being assigned CVE IDs. For the\n51 apps potentially affected by RCE, we successfully executed attacks on 17\napps, 16 of which are vulnerable to RCE and 1 to SQL injection. Furthermore, we\nconduct a comprehensive analysis of these vulnerabilities and construct\npractical attacks to demonstrate the hazards in reality. Last, we propose\nseveral mitigation measures for both framework and app developers to counteract\nsuch attacks.\n","versions":"[{'version': 'v1', 'created': 'Wed, 6 Sep 2023 11:39:37 GMT'}, {'version': 'v2', 'created': 'Sun, 8 Oct 2023 05:28:14 GMT'}, {'version': 'v3', 'created': 'Wed, 20 Nov 2024 06:01:23 GMT'}, {'version': 'v4', 'created': 'Thu, 27 Feb 2025 02:22:07 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Liu', 'Tong', ''], ['Deng', 'Zizhuang', ''], ['Meng', 'Guozhu', ''], ['Li', 'Yuekang', ''], ['Chen', 'Kai', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMSmith', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2311.01314,"submitter":"Ghazaleh Haratinezhad Torbati","authors":"Ghazaleh Haratinezhad Torbati, Anna Tigunova, Andrew Yates, Gerhard\n  Weikum","title":"Recommendations by Concise User Profiles from Review Text","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recommender systems perform well for popular items and users with ample\ninteractions (likes, ratings etc.). This work addresses the difficult and\nunderexplored case of users who have very sparse interactions but post\ninformative review texts. This setting naturally calls for encoding\nuser-specific text with large language models (LLM). However, feeding the full\ntext of all reviews through an LLM has a weak signal-to-noise ratio and incurs\nhigh costs of processed tokens. This paper addresses these two issues. It\npresents a light-weight framework, called CUP, which first computes concise\nuser profiles and feeds only these into the training of transformer-based\nrecommenders. For user profiles, we devise various techniques to select the\nmost informative cues from noisy reviews. Experiments, with book reviews data,\nshow that fine-tuning a small language model with judiciously constructed\nprofiles achieves the best performance, even in comparison to LLM-generated\nrankings.\n","versions":"[{'version': 'v1', 'created': 'Thu, 2 Nov 2023 15:31:12 GMT'}, {'version': 'v2', 'created': 'Wed, 13 Dec 2023 14:31:27 GMT'}, {'version': 'v3', 'created': 'Thu, 27 Feb 2025 16:36:08 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Torbati', 'Ghazaleh Haratinezhad', ''], ['Tigunova', 'Anna', ''], ['Yates', 'Andrew', ''], ['Weikum', 'Gerhard', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2311.07978,"submitter":"David Adelani","authors":"Jessica Ojo, Odunayo Ogundepo, Akintunde Oladipo, Kelechi Ogueji,\n  Jimmy Lin, Pontus Stenetorp, David Ifeoluwa Adelani","title":"AfroBench: How Good are Large Language Models on African Languages?","comments":"Under review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large-scale multilingual evaluations, such as MEGA, often include only a\nhandful of African languages due to the scarcity of high-quality evaluation\ndata and the limited discoverability of existing African datasets. This lack of\nrepresentation hinders comprehensive LLM evaluation across a diverse range of\nlanguages and tasks. To address these challenges, we introduce AfroBench -- a\nmulti-task benchmark for evaluating the performance of LLMs across 64 African\nlanguages, 15 tasks and 22 datasets. AfroBench consists of nine natural\nlanguage understanding datasets, six text generation datasets, six knowledge\nand question answering tasks, and one mathematical reasoning task. We present\nresults comparing the performance of prompting LLMs to fine-tuned baselines\nbased on BERT and T5-style models. Our results suggest large gaps in\nperformance between high-resource languages, such as English, and African\nlanguages across most tasks; but performance also varies based on the\navailability of monolingual data resources. Our findings confirm that\nperformance on African languages continues to remain a hurdle for current LLMs,\nunderscoring the need for additional efforts to close this gap.\n  https:\/\/mcgill-nlp.github.io\/AfroBench\/\n","versions":"[{'version': 'v1', 'created': 'Tue, 14 Nov 2023 08:10:14 GMT'}, {'version': 'v2', 'created': 'Tue, 30 Apr 2024 16:04:16 GMT'}, {'version': 'v3', 'created': 'Wed, 26 Feb 2025 15:16:47 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Ojo', 'Jessica', ''], ['Ogundepo', 'Odunayo', ''], ['Oladipo', 'Akintunde', ''], ['Ogueji', 'Kelechi', ''], ['Lin', 'Jimmy', ''], ['Stenetorp', 'Pontus', ''], ['Adelani', 'David Ifeoluwa', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'BERT', 'label': 'BERT'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2311.09802,"submitter":"Sen Yang","authors":"Sen Yang, Xin Li, Leyang Cui, Lidong Bing, Wai Lam","title":"Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs","comments":"To appear in Findings of NAACL2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Two lines of approaches are adopted for complex reasoning with LLMs. One line\nof work prompts LLMs with various reasoning structures, while the structural\noutputs can be naturally regarded as intermediate reasoning steps. Another line\nof work adopt LLM-free declarative solvers to do the reasoning task, rendering\nhigher reasoning accuracy but lacking interpretability due to the black-box\nnature of the solvers. Aiming to resolve the trade-off between answer accuracy\nand interpretability, we present a simple extension to the latter line of work.\nSpecifically, we showcase that the intermediate search logs generated by Prolog\ninterpreters can be accessed and interpreted into human-readable reasoning\nproofs. As long as LLMs correctly translate problem descriptions into Prolog\nrepresentations, the corresponding reasoning proofs are ensured to be causal\nand reliable. On two logical reasoning and one arithmetic reasoning datasets,\nour framework obtains significant improvements in terms of both answer accuracy\nand reasoning proof accuracy. Our code is released at\nhttps:\/\/github.com\/DAMO-NLP-SG\/CaRing\n","versions":"[{'version': 'v1', 'created': 'Thu, 16 Nov 2023 11:26:21 GMT'}, {'version': 'v2', 'created': 'Thu, 26 Sep 2024 08:15:50 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 08:33:46 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Yang', 'Sen', ''], ['Li', 'Xin', ''], ['Cui', 'Leyang', ''], ['Bing', 'Lidong', ''], ['Lam', 'Wai', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2401.08807,"submitter":"Lezhi Ma","authors":"Lezhi Ma, Shangqing Liu, Yi Li, Xiaofei Xie and Lei Bu","title":"SpecGen: Automated Generation of Formal Program Specifications via Large\n  Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Formal program specifications play a crucial role in various stages of\nsoftware development. However, manually crafting formal program specifications\nis rather difficult, making the job time-consuming and labor-intensive. It is\neven more challenging to write specifications that correctly and\ncomprehensively describe the semantics of complex programs. To reduce the\nburden on software developers, automated specification generation methods have\nemerged. However, existing methods usually rely on predefined templates or\ngrammar, making them struggle to accurately describe the behavior and\nfunctionality of complex real-world programs. To tackle this challenge, we\nintroduce SpecGen, a novel technique for formal program specification\ngeneration based on Large Language Models. Our key insight is to overcome the\nlimitations of existing methods by leveraging the code comprehension capability\nof LLMs. The process of SpecGen consists of two phases. The first phase employs\na conversational approach that guides the LLM to generate appropriate\nspecifications for a given program. The second phase, designed for where the\nLLM fails to generate correct specifications, applies four mutation operators\nto the model-generated specifications and selects verifiable specifications\nfrom the mutated ones through a novel heuristic selection strategy. We evaluate\nSpecGen on two datasets, including the SV-COMP Java category benchmark and a\nmanually constructed dataset. Experimental results demonstrate that SpecGen\nsucceeds in generating verifiable specifications for 279 out of 385 programs,\noutperforming the existing purely LLM-based approaches and conventional\nspecification generation tools like Houdini and Daikon. Further investigations\non the quality of generated specifications indicate that SpecGen can\ncomprehensively articulate the behaviors of the input program.\n","versions":"[{'version': 'v1', 'created': 'Tue, 16 Jan 2024 20:13:50 GMT'}, {'version': 'v2', 'created': 'Sun, 24 Mar 2024 03:01:48 GMT'}, {'version': 'v3', 'created': 'Mon, 18 Nov 2024 07:30:06 GMT'}, {'version': 'v4', 'created': 'Sat, 7 Dec 2024 07:50:25 GMT'}, {'version': 'v5', 'created': 'Tue, 25 Feb 2025 07:20:36 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Ma', 'Lezhi', ''], ['Liu', 'Shangqing', ''], ['Li', 'Yi', ''], ['Xie', 'Xiaofei', ''], ['Bu', 'Lei', '']]","extracted_entities":"[{'text': 'SpecGen', 'label': 'LLM'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'SpecGen', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'SpecGen', 'label': 'LLM'}, {'text': 'SpecGen', 'label': 'LLM'}, {'text': 'SpecGen', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2402.01881,"submitter":"Siyi Liu","authors":"Siyi Liu, Chen Gao, Yong Li","title":"Large Language Model Agent for Hyper-Parameter Optimization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Hyperparameter optimization is critical in modern machine learning, requiring\nexpert knowledge, numerous trials, and high computational and human resources.\nDespite the advancements in Automated Machine Learning (AutoML), challenges in\nterms of trial efficiency, setup complexity, and interoperability still\npersist. To address these issues, we introduce a novel paradigm leveraging\nLarge Language Models (LLMs) to automate hyperparameter optimization across\ndiverse machine learning tasks, which is named AgentHPO (short for LLM\nAgent-based Hyperparameter Optimization). Specifically, AgentHPO processes the\ntask information autonomously, conducts experiments with specific\nhyperparameters (HPs), and iteratively optimizes them based on historical\ntrials. This human-like optimization process largely reduces the number of\nrequired trials, simplifies the setup process, and enhances interpretability\nand user trust, compared to traditional AutoML methods. Extensive empirical\nexperiments conducted on 12 representative machine-learning tasks indicate that\nAgentHPO not only matches but also often surpasses the best human trials in\nterms of performance while simultaneously providing explainable results.\nFurther analysis sheds light on the strategies employed by the LLM in\noptimizing these tasks, highlighting its effectiveness and adaptability in\nvarious scenarios.\n","versions":"[{'version': 'v1', 'created': 'Fri, 2 Feb 2024 20:12:05 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Feb 2024 15:03:09 GMT'}, {'version': 'v3', 'created': 'Wed, 26 Feb 2025 13:57:13 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Liu', 'Siyi', ''], ['Gao', 'Chen', ''], ['Li', 'Yong', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'AgentHPO', 'label': 'LLM-based'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'AgentHPO', 'label': 'LLM'}, {'text': 'AgentHPO', 'label': 'LLM-based'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2405.19799,"submitter":"Jiahui Xu","authors":"Jiahui Xu, Feng Jiang, Anningzhe Gao, Luis Fernando D'Haro, Haizhou Li","title":"Unsupervised Mutual Learning of Discourse Parsing and Topic Segmentation\n  in Dialogue","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In dialogue systems, discourse plays a crucial role in managing\nconversational focus and coordinating interactions. It consists of two key\nstructures: rhetorical structure and topic structure. The former captures the\nlogical flow of conversations, while the latter detects transitions between\ntopics. Together, they improve the ability of a dialogue system to track\nconversation dynamics and generate contextually relevant high-quality\nresponses. These structures are typically identified through discourse parsing\nand topic segmentation, respectively. However, existing supervised methods rely\non costly manual annotations, while unsupervised methods often focus on a\nsingle task, overlooking the deep linguistic interplay between rhetorical and\ntopic structures. To address these issues, we first introduce a unified\nrepresentation that integrates rhetorical and topic structures, ensuring\nsemantic consistency between them. Under the unified representation, we further\npropose two linguistically grounded hypotheses based on discourse theories: (1)\nLocal Discourse Coupling, where rhetorical cues dynamically enhance topic-aware\ninformation flow, and (2) Global Topology Constraint, where topic structure\npatterns probabilistically constrain rhetorical relation distributions.\nBuilding on the unified representation and two hypotheses, we propose an\nunsupervised mutual learning framework (UMLF) that jointly models rhetorical\nand topic structures, allowing them to mutually reinforce each other without\nrequiring additional annotations. We evaluate our approach on two rhetorical\ndatasets and three topic segmentation datasets. Experimental results\ndemonstrate that our method surpasses all strong baselines built on pre-trained\nlanguage models. Furthermore, when applied to LLMs, our framework achieves\nnotable improvements, demonstrating its effectiveness in improving discourse\nstructure modeling.\n","versions":"[{'version': 'v1', 'created': 'Thu, 30 May 2024 08:10:50 GMT'}, {'version': 'v2', 'created': 'Mon, 3 Jun 2024 08:13:10 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Feb 2025 09:22:19 GMT'}, {'version': 'v4', 'created': 'Mon, 24 Feb 2025 09:50:00 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Xu', 'Jiahui', ''], ['Jiang', 'Feng', ''], ['Gao', 'Anningzhe', ''], [\"D'Haro\", 'Luis Fernando', ''], ['Li', 'Haizhou', '']]","extracted_entities":"[{'text': 'unified representation', 'label': 'contextual Embedding'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2405.20777,"submitter":"Thibaud Gloaguen","authors":"Thibaud Gloaguen, Nikola Jovanovi\\'c, Robin Staab, Martin Vechev","title":"Black-Box Detection of Language Model Watermarks","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Watermarking has emerged as a promising way to detect LLM-generated text, by\naugmenting LLM generations with later detectable signals. Recent work has\nproposed multiple families of watermarking schemes, several of which focus on\npreserving the LLM distribution. This distribution-preservation property is\nmotivated by the fact that it is a tractable proxy for retaining LLM\ncapabilities, as well as the inherently implied undetectability of the\nwatermark by downstream users. Yet, despite much discourse around\nundetectability, no prior work has investigated the practical detectability of\nany of the current watermarking schemes in a realistic black-box setting. In\nthis work we tackle this for the first time, developing rigorous statistical\ntests to detect the presence, and estimate parameters, of all three popular\nwatermarking scheme families, using only a limited number of black-box queries.\nWe experimentally confirm the effectiveness of our methods on a range of\nschemes and a diverse set of open-source models. Further, we validate the\nfeasibility of our tests on real-world APIs. Our findings indicate that current\nwatermarking schemes are more detectable than previously believed.\n","versions":"[{'version': 'v1', 'created': 'Tue, 28 May 2024 08:41:30 GMT'}, {'version': 'v2', 'created': 'Sat, 13 Jul 2024 15:47:35 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 14:06:41 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Gloaguen', 'Thibaud', ''], ['Jovanovi\u0107', 'Nikola', ''], ['Staab', 'Robin', ''], ['Vechev', 'Martin', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2406.05315,"submitter":"Mehrdad Khatir","authors":"Mehrdad Khatir, Sanchit Kabra, Chandan K. Reddy","title":"Aligned at the Start: Conceptual Groupings in LLM Embeddings","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  This paper shifts focus to the often-overlooked input embeddings - the\ninitial representations fed into transformer blocks. Using fuzzy graph,\nk-nearest neighbor (k-NN), and community detection, we analyze embeddings from\ndiverse LLMs, finding significant categorical community structure aligned with\npredefined concepts and categories aligned with humans. We observe these\ngroupings exhibit within-cluster organization (such as hierarchies, topological\nordering, etc.), hypothesizing a fundamental structure that precedes contextual\nprocessing. To further investigate the conceptual nature of these groupings, we\nexplore cross-model alignments across different LLM categories within their\ninput embeddings, observing a medium to high degree of alignment. Furthermore,\nprovide evidence that manipulating these groupings can play a functional role\nin mitigating ethnicity bias in LLM tasks.\n","versions":"[{'version': 'v1', 'created': 'Sat, 8 Jun 2024 01:27:19 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Feb 2025 23:26:33 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 17:53:06 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Khatir', 'Mehrdad', ''], ['Kabra', 'Sanchit', ''], ['Reddy', 'Chandan K.', '']]","extracted_entities":"[{'text': 'input embeddings', 'label': 'contextual Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'input embeddings', 'label': 'contextual Embedding'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2406.09288,"submitter":"Jinbin Zhang","authors":"Jinbin Zhang, Nasib Ullah, Rohit Babbar","title":"Large Language Model as a Teacher for Zero-shot Tagging at Extreme\n  Scales","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Extreme Multi-label Text Classification (XMC) entails selecting the most\nrelevant labels for an instance from a vast label set. Extreme Zero-shot XMC\n(EZ-XMC) extends this challenge by operating without annotated data, relying\nonly on raw text instances and a predefined label set, making it particularly\ncritical for addressing cold-start problems in large-scale recommendation and\ncategorization systems. State-of-the-art methods, such as MACLR and RTS,\nleverage lightweight bi-encoders but rely on suboptimal pseudo labels for\ntraining, such as document titles (MACLR) or document segments (RTS), which may\nnot align well with the intended tagging or categorization tasks. On the other\nhand, LLM-based approaches, like ICXML, achieve better label-instance alignment\nbut are computationally expensive and impractical for real-world EZ-XMC\napplications due to their heavy inference costs. In this paper, we introduce\nLMTX (Large language Model as Teacher for eXtreme classification), a novel\nframework that bridges the gap between these two approaches. LMTX utilizes an\nLLM to identify high-quality pseudo labels during training, while employing a\nlightweight bi-encoder for efficient inference. This design eliminates the need\nfor LLMs at inference time, offering the benefits of improved label alignment\nwithout sacrificing computational efficiency. Our approach achieves superior\nperformance and efficiency over both LLM and non-LLM based approaches,\nestablishing a new state-of-the-art in EZ-XMC.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Jun 2024 16:26:37 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 13:10:05 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Zhang', 'Jinbin', ''], ['Ullah', 'Nasib', ''], ['Babbar', 'Rohit', '']]","extracted_entities":"[{'text': 'Extreme Zero-shot XMC', 'label': 'Zero-shot Learning'}, {'text': 'MACLR', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2406.12221,"submitter":"Xueru Wen","authors":"Xueru Wen, Jie Lou, Xinyu Lu, Ji Yuqiu, Xinyan Guan, Yaojie Lu, Hongyu\n  Lin, Ben He, Xianpei Han, Debing Zhang, Le Sun","title":"On-Policy Self-Alignment with Fine-grained Knowledge Feedback for\n  Hallucination Mitigation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Hallucination occurs when large language models exhibit behavior that\ndeviates from the boundaries of their knowledge during response generation. To\naddress this critical issue, previous learning-based methods attempt to\nfinetune models but are limited by off-policy sampling and coarse-grained\nfeedback. In this paper, we present \\textit{\\b{R}einforcement \\b{L}earning\n\\b{f}or \\b{H}allucination} (RLFH), an on-policy self-alignment approach that\nenables LLMs to actively explore their knowledge boundaries and self-correct\ngeneration behavior through fine-grained feedback signals. RLFH introduces a\nself-assessment framework where the policy serves as its own judge. Through\nthis framework, responses are automatically decomposed into atomic facts and\ntheir truthfulness and informativeness are assessed against external knowledge\nsources. The resulting fine-grained feedback at the statement level are then\nconverted into token-level dense reward signals. This enables online\nreinforcement learning to achieve precise and timely optimization without human\nintervention. Comprehensive evaluations on HotpotQA, SQuADv2, and Biography\nbenchmarks validate RLFH's effectiveness in hallucination mitigation.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Jun 2024 02:43:49 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Feb 2025 05:20:32 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Feb 2025 11:00:17 GMT'}, {'version': 'v4', 'created': 'Tue, 25 Feb 2025 06:05:45 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Wen', 'Xueru', ''], ['Lou', 'Jie', ''], ['Lu', 'Xinyu', ''], ['Yuqiu', 'Ji', ''], ['Guan', 'Xinyan', ''], ['Lu', 'Yaojie', ''], ['Lin', 'Hongyu', ''], ['He', 'Ben', ''], ['Han', 'Xianpei', ''], ['Zhang', 'Debing', ''], ['Sun', 'Le', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}, {'text': 'online\\nreinforcement learning', 'label': 'Few-shot Learning'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2408.02487,"submitter":"Weiwei Xu","authors":"Weiwei Xu, Kai Gao, Hao He, Minghui Zhou","title":"LiCoEval: Evaluating LLMs on License Compliance in Code Generation","comments":"The 47th International Conference on Software Engineering(ICSE 2025)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Recent advances in Large Language Models (LLMs) have revolutionized code\ngeneration, leading to widespread adoption of AI coding tools by developers.\nHowever, LLMs can generate license-protected code without providing the\nnecessary license information, leading to potential intellectual property\nviolations during software production. This paper addresses the critical, yet\nunderexplored, issue of license compliance in LLM-generated code by\nestablishing a benchmark to evaluate the ability of LLMs to provide accurate\nlicense information for their generated code. To establish this benchmark, we\nconduct an empirical study to identify a reasonable standard for \"striking\nsimilarity\" that excludes the possibility of independent creation, indicating a\ncopy relationship between the LLM output and certain open-source code. Based on\nthis standard, we propose LiCoEval, to evaluate the license compliance\ncapabilities of LLMs, i.e., the ability to provide accurate license or\ncopyright information when they generate code with striking similarity to\nalready existing copyrighted code. Using LiCoEval, we evaluate 14 popular LLMs,\nfinding that even top-performing LLMs produce a non-negligible proportion\n(0.88% to 2.01%) of code strikingly similar to existing open-source\nimplementations. Notably, most LLMs fail to provide accurate license\ninformation, particularly for code under copyleft licenses. These findings\nunderscore the urgent need to enhance LLM compliance capabilities in code\ngeneration tasks. Our study provides a foundation for future research and\ndevelopment to improve license compliance in AI-assisted software development,\ncontributing to both the protection of open-source software copyrights and the\nmitigation of legal risks for LLM users.\n","versions":"[{'version': 'v1', 'created': 'Mon, 5 Aug 2024 14:09:30 GMT'}, {'version': 'v2', 'created': 'Tue, 12 Nov 2024 10:03:37 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Feb 2025 08:58:05 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Xu', 'Weiwei', ''], ['Gao', 'Kai', ''], ['He', 'Hao', ''], ['Zhou', 'Minghui', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'existing open-source\\nimplementations', 'label': 'Open-source LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2405.17428,"submitter":"Wei Ping","authors":"Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad\n  Shoeybi, Bryan Catanzaro, Wei Ping","title":"NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding\n  Models","comments":"ICLR 2025 (Spotlight). We open-source the model at:\n  https:\/\/huggingface.co\/nvidia\/NV-Embed-v2","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.IR cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Decoder-only LLM-based embedding models are beginning to outperform BERT or\nT5-based embedding models in general-purpose text embedding tasks, including\ndense vector-based retrieval. In this work, we introduce NV-Embed,\nincorporating architectural designs, training procedures, and curated datasets\nto significantly enhance the performance of LLM as a versatile embedding model,\nwhile maintaining its simplicity and reproducibility. For model architecture,\nwe propose a latent attention layer to obtain pooled embeddings, which\nconsistently improves retrieval and downstream task accuracy compared to mean\npooling or using the last <EOS> token embedding from LLMs. To enhance\nrepresentation learning, we remove the causal attention mask of LLMs during\ncontrastive training. For training algorithm, we introduce a two-stage\ncontrastive instruction-tuning method. It first applies contrastive training\nwith instructions on retrieval datasets, utilizing in-batch negatives and\ncurated hard negative examples. At stage-2, it blends various non-retrieval\ninto instruction tuning, which not only enhances non-retrieval task accuracy\nbut also improves retrieval performance. For training data, we utilize the\nhard-negative mining, synthetic data generation and existing public available\ndatasets to boost the performance of embedding model. By combining these\ntechniques, our NV-Embed-v1 and NV-Embed-v2 models obtained the No.1 position\non the MTEB leaderboard (as of May 24 and August 30, 2024, respectively) across\n56 tasks, demonstrating the sustained effectiveness of the proposed methods\nover time. It also achieved the highest scores in the Long Doc section and the\nsecond-highest scores in the QA section of the AIR Benchmark, which covers a\nrange of out-of-domain information retrieval topics beyond those in MTEB. We\nfurther provide the analysis of model compression techniques for generalist\nembedding models.\n","versions":"[{'version': 'v1', 'created': 'Mon, 27 May 2024 17:59:45 GMT'}, {'version': 'v2', 'created': 'Thu, 9 Jan 2025 22:27:06 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Feb 2025 00:35:18 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Lee', 'Chankyu', ''], ['Roy', 'Rajarshi', ''], ['Xu', 'Mengyao', ''], ['Raiman', 'Jonathan', ''], ['Shoeybi', 'Mohammad', ''], ['Catanzaro', 'Bryan', ''], ['Ping', 'Wei', '']]","extracted_entities":"[{'text': 'BERT', 'label': 'BERT'}, {'text': 'pooled embeddings', 'label': 'Embedding'}, {'text': 'last <EOS> token embedding', 'label': 'Embedding'}, {'text': 'representation learning', 'label': 'Few-shot Learning'}, {'text': 'causal attention mask', 'label': 'Attention mechanism'}, {'text': 'instruction tuning', 'label': 'Fine-tuning'}]","assigned_concept":"BERT","matched_keyword":"BERT","similarity_score":1.0}
{"id":2306.00396,"submitter":"Qihang Fan","authors":"Qihang Fan and Huaibo Huang and Xiaoqiang Zhou and Ran He","title":"Lightweight Vision Transformer with Bidirectional Interaction","comments":"The paper is accepted by NeurIPS2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advancements in vision backbones have significantly improved their\nperformance by simultaneously modeling images' local and global contexts.\nHowever, the bidirectional interaction between these two contexts has not been\nwell explored and exploited, which is important in the human visual system.\nThis paper proposes a Fully Adaptive Self-Attention (FASA) mechanism for vision\ntransformer to model the local and global information as well as the\nbidirectional interaction between them in context-aware ways. Specifically,\nFASA employs self-modulated convolutions to adaptively extract local\nrepresentation while utilizing self-attention in down-sampled space to extract\nglobal representation. Subsequently, it conducts a bidirectional adaptation\nprocess between local and global representation to model their interaction. In\naddition, we introduce a fine-grained downsampling strategy to enhance the\ndown-sampled self-attention mechanism for finer-grained global perception\ncapability. Based on FASA, we develop a family of lightweight vision backbones,\nFully Adaptive Transformer (FAT) family. Extensive experiments on multiple\nvision tasks demonstrate that FAT achieves impressive performance. Notably, FAT\naccomplishes a 77.6% accuracy on ImageNet-1K using only 4.5M parameters and\n0.7G FLOPs, which surpasses the most advanced ConvNets and Transformers with\nsimilar model size and computational costs. Moreover, our model exhibits faster\nspeed on modern GPU compared to other models. Code will be available at\nhttps:\/\/github.com\/qhfan\/FAT.\n","versions":"[{'version': 'v1', 'created': 'Thu, 1 Jun 2023 06:56:41 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 03:16:17 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Fan', 'Qihang', ''], ['Huang', 'Huaibo', ''], ['Zhou', 'Xiaoqiang', ''], ['He', 'Ran', '']]","extracted_entities":"[{'text': 'Fully Adaptive Self-Attention (FASA)', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'down-sampled self-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'FASA', 'label': 'Attention mechanism'}, {'text': 'Transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2308.09372,"submitter":"Tobias Nauen","authors":"Tobias Christian Nauen, Sebastian Palacio, Federico Raue, Andreas\n  Dengel","title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in\n  Vision Transformers","comments":"v3: new models, analysis of scaling behaviors; v4: WACV 2025 camera\n  ready version, appendix added","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Self-attention in Transformers comes with a high computational cost because\nof their quadratic computational complexity, but their effectiveness in\naddressing problems in language and vision has sparked extensive research aimed\nat enhancing their efficiency. However, diverse experimental conditions,\nspanning multiple input domains, prevent a fair comparison based solely on\nreported results, posing challenges for model selection. To address this gap in\ncomparability, we perform a large-scale benchmark of more than 45 models for\nimage classification, evaluating key efficiency aspects, including accuracy,\nspeed, and memory usage. Our benchmark provides a standardized baseline for\nefficiency-oriented transformers. We analyze the results based on the Pareto\nfront -- the boundary of optimal models. Surprisingly, despite claims of other\nmodels being more efficient, ViT remains Pareto optimal across multiple\nmetrics. We observe that hybrid attention-CNN models exhibit remarkable\ninference memory- and parameter-efficiency. Moreover, our benchmark shows that\nusing a larger model in general is more efficient than using higher resolution\nimages. Thanks to our holistic evaluation, we provide a centralized resource\nfor practitioners and researchers, facilitating informed decisions when\nselecting or developing efficient transformers.\n","versions":"[{'version': 'v1', 'created': 'Fri, 18 Aug 2023 08:06:49 GMT'}, {'version': 'v2', 'created': 'Fri, 12 Apr 2024 09:21:33 GMT'}, {'version': 'v3', 'created': 'Fri, 19 Jul 2024 10:44:53 GMT'}, {'version': 'v4', 'created': 'Mon, 24 Feb 2025 10:51:07 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Nauen', 'Tobias Christian', ''], ['Palacio', 'Sebastian', ''], ['Raue', 'Federico', ''], ['Dengel', 'Andreas', '']]","extracted_entities":"[{'text': 'Self-attention', 'label': 'Attention mechanism'}, {'text': 'Transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2401.11647,"submitter":"Ye Lin Tun","authors":"Ye Lin Tun, Chu Myaet Thwal, Huy Q. Le, Minh N. H. Nguyen, Choong Seon\n  Hong","title":"LW-FedSSL: Resource-efficient Layer-wise Federated Self-supervised\n  Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Many studies integrate federated learning (FL) with self-supervised learning\n(SSL) to take advantage of raw data distributed across edge devices. However,\nedge devices often struggle with high computational and communication costs\nimposed by SSL and FL algorithms. With the deployment of more complex and\nlarge-scale models, such as Transformers, these challenges are exacerbated. To\ntackle this, we propose the Layer-Wise Federated Self-Supervised Learning\n(LW-FedSSL) approach, which allows edge devices to incrementally train a small\npart of the model at a time. Specifically, in LW-FedSSL, training is decomposed\ninto multiple stages, with each stage responsible for only a specific layer (or\na block of layers) of the model. Since only a portion of the model is active\nfor training at any given time, LW-FedSSL significantly reduces computational\nrequirements. Additionally, only the active model portion needs to be exchanged\nbetween the FL server and clients, reducing the communication overhead. This\nenables LW-FedSSL to jointly address both computational and communication\nchallenges in FL. Depending on the SSL algorithm used, it can achieve up to a\n$3.34 \\times$ reduction in memory usage, $4.20 \\times$ fewer computational\noperations (GFLOPs), and a $5.07 \\times$ lower communication cost while\nmaintaining performance comparable to its end-to-end training counterpart.\nFurthermore, we explore a progressive training strategy called Prog-FedSSL,\nwhich offers a $1.84\\times$ reduction in GFLOPs and a $1.67\\times$ reduction in\ncommunication costs while maintaining the same memory requirements as\nend-to-end training. While the resource efficiency of Prog-FedSSL is lower than\nthat of LW-FedSSL, its performance improvements make it a viable candidate for\nFL environments with more lenient resource constraints.\n","versions":"[{'version': 'v1', 'created': 'Mon, 22 Jan 2024 01:57:31 GMT'}, {'version': 'v2', 'created': 'Tue, 30 Apr 2024 00:51:18 GMT'}, {'version': 'v3', 'created': 'Mon, 21 Oct 2024 02:11:09 GMT'}, {'version': 'v4', 'created': 'Wed, 26 Feb 2025 05:20:00 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Tun', 'Ye Lin', ''], ['Thwal', 'Chu Myaet', ''], ['Le', 'Huy Q.', ''], ['Nguyen', 'Minh N. H.', ''], ['Hong', 'Choong Seon', '']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2402.12365,"submitter":"Benedikt Alkin","authors":"Benedikt Alkin and Andreas F\\\"urst and Simon Schmid and Lukas Gruber\n  and Markus Holzleitner and Johannes Brandstetter","title":"Universal Physics Transformers: A Framework For Efficiently Scaling\n  Neural Operators","comments":"Published at NeurIPS 2024, Github: https:\/\/ml-jku.github.io\/UPT\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI physics.flu-dyn","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Neural operators, serving as physics surrogate models, have recently gained\nincreased interest. With ever increasing problem complexity, the natural\nquestion arises: what is an efficient way to scale neural operators to larger\nand more complex simulations - most importantly by taking into account\ndifferent types of simulation datasets. This is of special interest since, akin\nto their numerical counterparts, different techniques are used across\napplications, even if the underlying dynamics of the systems are similar.\nWhereas the flexibility of transformers has enabled unified architectures\nacross domains, neural operators mostly follow a problem specific design, where\nGNNs are commonly used for Lagrangian simulations and grid-based models\npredominate Eulerian simulations. We introduce Universal Physics Transformers\n(UPTs), an efficient and unified learning paradigm for a wide range of\nspatio-temporal problems. UPTs operate without grid- or particle-based latent\nstructures, enabling flexibility and scalability across meshes and particles.\nUPTs efficiently propagate dynamics in the latent space, emphasized by inverse\nencoding and decoding techniques. Finally, UPTs allow for queries of the latent\nspace representation at any point in space-time. We demonstrate diverse\napplicability and efficacy of UPTs in mesh-based fluid simulations, and\nsteady-state Reynolds averaged Navier-Stokes simulations, and Lagrangian-based\ndynamics.\n","versions":"[{'version': 'v1', 'created': 'Mon, 19 Feb 2024 18:52:13 GMT'}, {'version': 'v2', 'created': 'Tue, 30 Apr 2024 17:15:35 GMT'}, {'version': 'v3', 'created': 'Tue, 8 Oct 2024 12:52:04 GMT'}, {'version': 'v4', 'created': 'Thu, 10 Oct 2024 07:48:24 GMT'}, {'version': 'v5', 'created': 'Thu, 27 Feb 2025 10:24:17 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Alkin', 'Benedikt', ''], ['F\u00fcrst', 'Andreas', ''], ['Schmid', 'Simon', ''], ['Gruber', 'Lukas', ''], ['Holzleitner', 'Markus', ''], ['Brandstetter', 'Johannes', '']]","extracted_entities":"[{'text': 'Universal Physics Transformers', 'label': 'Transformers'}, {'text': 'UPTs', 'label': 'Transformers'}, {'text': 'UPTs', 'label': 'Transformers'}, {'text': 'UPTs', 'label': 'Transformers'}, {'text': 'UPTs', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Universal Physics Transformers","similarity_score":0.7450098991}
{"id":2404.14979,"submitter":"Junsong Zhang","authors":"Junsong Zhang, Zisong Chen, Chunyu Lin, Lang Nie, Zhijie Shen, Kang\n  Liao, Junda Huang, Yao Zhao","title":"SGFormer: Spherical Geometry Transformer for 360 Depth Estimation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Panoramic distortion poses a significant challenge in 360 depth estimation,\nparticularly pronounced at the north and south poles. Existing methods either\nadopt a bi-projection fusion strategy to remove distortions or model long-range\ndependencies to capture global structures, which can result in either unclear\nstructure or insufficient local perception. In this paper, we propose a\nspherical geometry transformer, named SGFormer, to address the above issues,\nwith an innovative step to integrate spherical geometric priors into vision\ntransformers. To this end, we retarget the transformer decoder to a spherical\nprior decoder (termed SPDecoder), which endeavors to uphold the integrity of\nspherical structures during decoding. Concretely, we leverage bipolar\nre-projection, circular rotation, and curve local embedding to preserve the\nspherical characteristics of equidistortion, continuity, and surface distance,\nrespectively. Furthermore, we present a query-based global conditional position\nembedding to compensate for spatial structure at varying resolutions. It not\nonly boosts the global perception of spatial position but also sharpens the\ndepth structure across different patches. Finally, we conduct extensive\nexperiments on popular benchmarks, demonstrating our superiority over\nstate-of-the-art solutions.\n","versions":"[{'version': 'v1', 'created': 'Tue, 23 Apr 2024 12:36:24 GMT'}, {'version': 'v2', 'created': 'Tue, 8 Oct 2024 03:09:38 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Feb 2025 15:14:30 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Zhang', 'Junsong', ''], ['Chen', 'Zisong', ''], ['Lin', 'Chunyu', ''], ['Nie', 'Lang', ''], ['Shen', 'Zhijie', ''], ['Liao', 'Kang', ''], ['Huang', 'Junda', ''], ['Zhao', 'Yao', '']]","extracted_entities":"[{'text': 'vision\\ntransformers', 'label': 'Transformers'}, {'text': 'curve local embedding', 'label': 'Embedding'}, {'text': 'query-based global conditional position\\nembedding', 'label': 'Embedding'}]","assigned_concept":"Transformers","matched_keyword":"vision\ntransformers","similarity_score":0.7330732346}
{"id":2405.15618,"submitter":"William Tong","authors":"William L. Tong and Cengiz Pehlevan","title":"MLPs Learn In-Context on Regression and Classification Tasks","comments":"Published at ICLR 2025. 30 pages, 10 figures, code available at\n  https:\/\/github.com\/wtong98\/mlp-icl","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.NE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In-context learning (ICL), the remarkable ability to solve a task from only\ninput exemplars, is often assumed to be a unique hallmark of Transformer\nmodels. By examining commonly employed synthetic ICL tasks, we demonstrate that\nmulti-layer perceptrons (MLPs) can also learn in-context. Moreover, MLPs, and\nthe closely related MLP-Mixer models, learn in-context comparably with\nTransformers under the same compute budget in this setting. We further show\nthat MLPs outperform Transformers on a series of classical tasks from\npsychology designed to test relational reasoning, which are closely related to\nin-context classification. These results underscore a need for studying\nin-context learning beyond attention-based architectures, while also\nchallenging prior arguments against MLPs' ability to solve relational tasks.\nAltogether, our results highlight the unexpected competence of MLPs in a\nsynthetic setting, and support the growing interest in all-MLP alternatives to\nTransformer architectures. It remains unclear how MLPs perform against\nTransformers at scale on real-world tasks, and where a performance gap may\noriginate. We encourage further exploration of these architectures in more\ncomplex settings to better understand the potential comparative advantage of\nattention-based schemes.\n","versions":"[{'version': 'v1', 'created': 'Fri, 24 May 2024 15:04:36 GMT'}, {'version': 'v2', 'created': 'Thu, 26 Sep 2024 16:05:30 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Feb 2025 16:27:38 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Tong', 'William L.', ''], ['Pehlevan', 'Cengiz', '']]","extracted_entities":"[{'text': 'In-context learning', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'MLPs', 'label': 'Neural Language Model'}, {'text': 'in-context', 'label': 'contextual Embedding'}, {'text': 'MLPs', 'label': 'Neural Language Model'}, {'text': 'in-context', 'label': 'contextual Embedding'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'MLPs', 'label': 'Neural Language Model'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'in-context', 'label': 'contextual Embedding'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}, {'text': 'MLPs', 'label': 'Neural Language Model'}, {'text': 'MLPs', 'label': 'Neural Language Model'}, {'text': 'MLPs', 'label': 'Neural Language Model'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2406.13815,"submitter":"Alireza Aghelan","authors":"Alireza Aghelan, Ali Amiryan, Abolfazl Zarghani, Modjtaba Rouhani","title":"IG-CFAT: An Improved GAN-Based Framework for Effectively Exploiting\n  Transformers in Real-World Image Super-Resolution","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In the field of single image super-resolution (SISR), transformer-based\nmodels, have demonstrated significant advancements. However, the potential and\nefficiency of these models in applied fields such as real-world image\nsuper-resolution have been less noticed and there are substantial opportunities\nfor improvement. Recently, composite fusion attention transformer (CFAT),\noutperformed previous state-of-the-art (SOTA) models in classic image\nsuper-resolution. In this paper, we propose a novel GAN-based framework by\nincorporating the CFAT model to effectively exploit the performance of\ntransformers in real-world image super-resolution. In our proposed approach, we\nintegrate a semantic-aware discriminator to reconstruct fine details more\naccurately and employ an adaptive degradation model to better simulate\nreal-world degradations. Moreover, we introduce a new combination of loss\nfunctions by adding wavelet loss to loss functions of GAN-based models to\nbetter recover high-frequency details. Empirical results demonstrate that\nIG-CFAT significantly outperforms existing SOTA models in both quantitative and\nqualitative metrics. Our proposed model revolutionizes the field of real-world\nimage super-resolution and demonstrates substantially better performance in\nrecovering fine details and generating realistic textures. The introduction of\nIG-CFAT offers a robust and adaptable solution for real-world image\nsuper-resolution tasks.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Jun 2024 20:21:26 GMT'}, {'version': 'v2', 'created': 'Mon, 22 Jul 2024 20:50:09 GMT'}, {'version': 'v3', 'created': 'Tue, 26 Nov 2024 17:31:53 GMT'}, {'version': 'v4', 'created': 'Tue, 25 Feb 2025 17:52:38 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Aghelan', 'Alireza', ''], ['Amiryan', 'Ali', ''], ['Zarghani', 'Abolfazl', ''], ['Rouhani', 'Modjtaba', '']]","extracted_entities":"[{'text': 'transformers', 'label': 'Transformers'}, {'text': 'IG-CFAT', 'label': 'Transformer-based model'}, {'text': 'IG-CFAT', 'label': 'Transformer-based model'}]","assigned_concept":"Transformers","matched_keyword":"transformers","similarity_score":1.0}
{"id":2407.10099,"submitter":"Yang Liu","authors":"Yang Liu and Zhiyong Zhang","title":"STGFormer: Spatio-Temporal GraphFormer for 3D Human Pose Estimation in\n  Video","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The current methods of video-based 3D human pose estimation have achieved\nsignificant progress.However, they still face pressing challenges, such as the\nunderutilization of spatiotemporal bodystructure features in transformers and\nthe inadequate granularity of spatiotemporal interaction modeling in graph\nconvolutional networks, which leads to pervasive depth ambiguity in monocular\n3D human pose estimation. To address these limitations, this paper presents the\nSpatio-Temporal GraphFormer framework (STGFormer) for 3D human pose estimation\nin videos. First, we introduce a Spatio-Temporal criss-cross Graph (STG)\nattention mechanism, designed to more effectively leverage the inherent graph\npriors of the human body within continuous sequence distributions while\ncapturing spatiotemporal long-range dependencies. Next, we present a dual-path\nModulated Hop-wise Regular GCN (MHR-GCN) to independently process temporal and\nspatial dimensions in parallel, preserving features rich in temporal dynamics\nand the original or high-dimensional representations of spatial structures.\nFurthermore, the module leverages modulation to optimize parameter efficiency\nand incorporates spatiotemporal hop-wise skip connections to capture\nhigher-order information. Finally, we demonstrate that our method achieves\nstate-of-the-art performance on the Human3.6M and MPIINF-3DHP datasets.\n","versions":"[{'version': 'v1', 'created': 'Sun, 14 Jul 2024 06:45:27 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 07:56:48 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Liu', 'Yang', ''], ['Zhang', 'Zhiyong', '']]","extracted_entities":"[{'text': 'transformers', 'label': 'Transformers'}, {'text': 'Spatio-Temporal criss-cross Graph (STG)\\nattention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Transformers","matched_keyword":"transformers","similarity_score":1.0}
{"id":2408.02922,"submitter":"Xinyi Zhang","authors":"Xinyi Zhang, Qiqi Bao, Qinpeng Cui, Wenming Yang, Qingmin Liao","title":"Pose Magic: Efficient and Temporally Consistent Human Pose Estimation\n  with a Hybrid Mamba-GCN Network","comments":"This work has been accepted by AAAI 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Current state-of-the-art (SOTA) methods in 3D Human Pose Estimation (HPE) are\nprimarily based on Transformers. However, existing Transformer-based 3D HPE\nbackbones often encounter a trade-off between accuracy and computational\nefficiency. To resolve the above dilemma, in this work, we leverage recent\nadvances in state space models and utilize Mamba for high-quality and efficient\nlong-range modeling. Nonetheless, Mamba still faces challenges in precisely\nexploiting local dependencies between joints. To address these issues, we\npropose a new attention-free hybrid spatiotemporal architecture named Hybrid\nMamba-GCN (Pose Magic). This architecture introduces local enhancement with GCN\nby capturing relationships between neighboring joints, thus producing new\nrepresentations to complement Mamba's outputs. By adaptively fusing\nrepresentations from Mamba and GCN, Pose Magic demonstrates superior capability\nin learning the underlying 3D structure. To meet the requirements of real-time\ninference, we also provide a fully causal version. Extensive experiments show\nthat Pose Magic achieves new SOTA results ($\\downarrow 0.9 mm$) while saving\n$74.1\\%$ FLOPs. In addition, Pose Magic exhibits optimal motion consistency and\nthe ability to generalize to unseen sequence lengths.\n","versions":"[{'version': 'v1', 'created': 'Tue, 6 Aug 2024 03:15:18 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Aug 2024 06:44:24 GMT'}, {'version': 'v3', 'created': 'Wed, 26 Feb 2025 03:17:49 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Zhang', 'Xinyi', ''], ['Bao', 'Qiqi', ''], ['Cui', 'Qinpeng', ''], ['Yang', 'Wenming', ''], ['Liao', 'Qingmin', '']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'GCN', 'label': 'Transformers'}, {'text': 'GCN', 'label': 'Transformers'}, {'text': 'Pose Magic', 'label': 'Transformer-based model'}, {'text': 'Pose Magic', 'label': 'Transformer-based model'}, {'text': 'Pose Magic', 'label': 'Transformer-based model'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2208.10662,"submitter":"Alzayat Saleh","authors":"Alzayat Saleh, Marcus Sheaves, Dean Jerry, Mostafa Rahimi Azghadi","title":"How to Track and Segment Fish without Human Annotations: A\n  Self-Supervised Deep Learning Approach","comments":"22 pages, 11 figures. Published at Pattern Analysis and Applications\n  journal","journal-ref":"Pattern Anal Applic 27, 4 (2024)","doi":"10.1007\/s10044-024-01227-6","report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Tracking fish movements and sizes of fish is crucial to understanding their\necology and behaviour. Knowing where fish migrate, how they interact with their\nenvironment, and how their size affects their behaviour can help ecologists\ndevelop more effective conservation and management strategies to protect fish\npopulations and their habitats. Deep learning is a promising tool to analyze\nfish ecology from underwater videos. However, training deep neural networks\n(DNNs) for fish tracking and segmentation requires high-quality labels, which\nare expensive to obtain. We propose an alternative unsupervised approach that\nrelies on spatial and temporal variations in video data to generate noisy\npseudo-ground-truth labels. We train a multitask DNN using these pseudo-labels.\nOur framework consists of three stages: (1) an optical flow model generates the\npseudo labels using spatial and temporal consistency between frames, (2) a\nself-supervised model refines the pseudo-labels incrementally, and (3) a\nsegmentation network uses the refined labels for training. Consequently, we\nperform extensive experiments to validate our method on three public underwater\nvideo datasets and demonstrate its effectiveness for video annotation and\nsegmentation. We also evaluate its robustness to different imaging conditions\nand discuss its limitations.\n","versions":"[{'version': 'v1', 'created': 'Tue, 23 Aug 2022 01:01:27 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 04:20:12 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Saleh', 'Alzayat', ''], ['Sheaves', 'Marcus', ''], ['Jerry', 'Dean', ''], ['Azghadi', 'Mostafa Rahimi', '']]","extracted_entities":"[{'text': 'Deep learning', 'label': 'Few-shot Learning'}, {'text': 'optical flow model', 'label': 'AI model'}, {'text': 'self-supervised model', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"self-supervised model","similarity_score":0.5818687677}
{"id":2403.01529,"submitter":"Cong Li","authors":"Cong Li","title":"Deep Incremental Model Informed Reinforcement Learning for Continuous\n  Robotic Control","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.SY eess.SY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Model-based reinforcement learning attempts to use an available or learned\nmodel to improve the data efficiency of reinforcement learning. This work\nproposes a one-step lookback approach that jointly learns the deep incremental\nmodel and the policy to realize the sample-efficient continuous robotic\ncontrol, wherein the control-theoretical knowledge is utilized to decrease the\nmodel learning difficulty and facilitate efficient training. Specifically, we\nuse one-step backward data to facilitate the deep incremental model, an\nalternative structured representation of the robotic evolution model, that\naccurately predicts the robotic movement but with low sample complexity. This\nis because the formulated deep incremental model degrades the model learning\ndifficulty into a parametric matrix learning problem, which is especially\nfavourable to high-dimensional robotic applications. The imagined data from the\nlearned deep incremental model is used to supplement training data to enhance\nthe sample efficiency. Comparative numerical simulations on benchmark\ncontinuous robotics control problems are conducted to validate the efficiency\nof our proposed one-step lookback approach.\n","versions":"[{'version': 'v1', 'created': 'Sun, 3 Mar 2024 15:00:54 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 10:24:17 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Li', 'Cong', '']]","extracted_entities":"[{'text': 'Model-based reinforcement learning', 'label': 'Few-shot Learning'}, {'text': 'deep incremental\\nmodel', 'label': 'AI model'}, {'text': 'deep incremental model', 'label': 'AI model'}, {'text': 'deep incremental model', 'label': 'AI model'}, {'text': 'deep incremental model', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"deep incremental\nmodel","similarity_score":0.5037730336}
{"id":2404.17042,"submitter":"Manuel Cuerno","authors":"Manuel Cuerno, Fernando Galaz-Garc\\'ia, Sergio Galaz-Garc\\'ia and\n  Telmo P\\'erez-Izquierdo","title":"Finding patterns of meaning: Reassessing Construal Clustering via\n  Bipolar Class Analysis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Empirical research on \\textit{construals}--social affinity groups that share\nsimilar patterns of meaning--has advanced significantly in recent years. This\nprogress is largely driven by the development of \\textit{Construal Clustering\nMethods} (CCMs), which group survey respondents into construal clusters based\non similarities in their response patterns. We identify key limitations of\nexisting CCMs, which affect their accuracy when applied to the typical\nstructures of available data, and introduce Bipolar Class Analysis (BCA), a CCM\ndesigned to address these shortcomings. BCA measures similarity in response\nshifts between expressions of support and rejection across survey respondents,\naddressing conceptual and measurement challenges in existing methods. We\nformally define BCA and demonstrate its advantages through extensive simulation\nanalyses, where it consistently outperforms existing CCMs in accurately\nidentifying construals. Along the way, we develop a novel data-generation\nprocess that approximates more closely how individuals map latent opinions onto\nobservable survey responses, as well as a new metric to evaluate the\nperformance of CCMs. Additionally, we find that applying BCA to previously\nstudied real-world datasets reveals substantively different construal patterns\ncompared to those generated by existing CCMs in prior empirical analyses.\nFinally, we discuss limitations of BCA and outline directions for future\nresearch.\n","versions":"[{'version': 'v1', 'created': 'Thu, 25 Apr 2024 21:07:06 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 13:48:55 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Cuerno', 'Manuel', ''], ['Galaz-Garc\u00eda', 'Fernando', ''], ['Galaz-Garc\u00eda', 'Sergio', ''], ['P\u00e9rez-Izquierdo', 'Telmo', '']]","extracted_entities":"[{'text': 'CCMs', 'label': 'LLMs'}, {'text': 'CCMs', 'label': 'LLMs'}, {'text': 'CCMs', 'label': 'LLMs'}, {'text': 'CCMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"CCMs","similarity_score":0.5260807276}
{"id":2405.0155,"submitter":"Mohammad Jobayer Hossain","authors":"Anika Tabassum Raisa, Syed Nazmus Sakib, Mohammad Jobayer Hossain,\n  Kaiser Ahmed Rocky, Abu Kowsar","title":"Advances in multijunction solar cells: an overview","comments":"21 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.app-ph physics.optics","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The advanced multijunction solar cell (MJSC) has emerged as a frontrunner in\nphotovoltaic literature due to its superior photoconversion efficiency (PCE)\nowing to its complex fabrication procedure and high costs. This article aims to\nsystematically review the advancements of III-V MJSCs by focusing on\ncomputational modelling and experimental fabrication methodologies. In\naddition, it addresses the technical barriers that have hindered the\nprogression of MJSC technology while also evaluating the current status and\nprospects of these cells. The findings indicate that III-V MJSCs hold\nsignificant promise for space applications. However, advancements in materials\nscience, growth techniques, and structural optimization are crucial for\nreducing fabrication costs to make these cells more viable for terrestrial use.\nIn this context, alternatives such as perovskite\/Si or perovskite\/chalcogenide\ntandem solar cells emerge as viable options. By synthesizing insights from a\nthorough analysis of recent literature, this review serves as a valuable\nresource for researchers, industry practitioners, and newcomers seeking to\ndeepen their understanding of the research methodologies, growth techniques,\nand the associated challenges and opportunities within the realm of MJSCs.\n","versions":"[{'version': 'v1', 'created': 'Wed, 28 Feb 2024 17:11:27 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 14:01:36 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Raisa', 'Anika Tabassum', ''], ['Sakib', 'Syed Nazmus', ''], ['Hossain', 'Mohammad Jobayer', ''], ['Rocky', 'Kaiser Ahmed', ''], ['Kowsar', 'Abu', '']]","extracted_entities":"[{'text': 'MJSCs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"MJSCs","similarity_score":0.5712216496}
{"id":2407.06723,"submitter":"Yu-Guan Hsieh","authors":"Yu-Guan Hsieh, Cheng-Yu Hsieh, Shih-Ying Yeh, Louis B\\'ethune, Hadi\n  Pour Ansari, Pavan Kumar Anasosalu Vasu, Chun-Liang Li, Ranjay Krishna, Oncel\n  Tuzel, Marco Cuturi","title":"Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting\n  Region Captions","comments":"59 pages, 42 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Humans describe complex scenes with compositionality, using simple text\ndescriptions enriched with links and relationships. While vision-language\nresearch has aimed to develop models with compositional understanding\ncapabilities, this is not reflected yet in existing datasets which, for the\nmost part, still use plain text to describe images. In this work, we propose a\nnew annotation strategy, graph-based captioning (GBC) that describes an image\nusing a labeled graph structure, with nodes of various types. The nodes in GBC\nare created through a two-stage process: first, identifying and describing\nentity nodes; second, linking these nodes by highlighting \\textit{compositions}\nand \\textit{relations} among them. Since \\textit{all} GBC nodes hold plain text\ndescriptions, GBC retains the flexibility found in natural language, but can\nalso encode hierarchical information in its edges. We demonstrate that GBC can\nbe produced automatically, using off-the-shelf multimodal LLMs and object\ndetection models, by building a new dataset GBC10M that gathers GBC annotations\nfor about 10M images of the CC12M dataset. Through CLIP training on GBC10M, we\nshow that leveraging GBC nodes' annotations -- particularly those in\ncomposition and relation nodes -- significantly boosts the model's performance\nacross various benchmarks compared to when other annotations are used. To\nfurther explore the opportunities provided by GBC, we also investigate the use\nof GBC as middleware for text-to-image generation, and show the extra benefits\nof incorporating the graph structure in this task. Our code and datasets are\nreleased at https:\/\/github.com\/apple\/ml-gbc and\nhttps:\/\/huggingface.co\/graph-based-captions.\n","versions":"[{'version': 'v1', 'created': 'Tue, 9 Jul 2024 09:55:04 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 22:54:53 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Hsieh', 'Yu-Guan', ''], ['Hsieh', 'Cheng-Yu', ''], ['Yeh', 'Shih-Ying', ''], ['B\u00e9thune', 'Louis', ''], ['Ansari', 'Hadi Pour', ''], ['Vasu', 'Pavan Kumar Anasosalu', ''], ['Li', 'Chun-Liang', ''], ['Krishna', 'Ranjay', ''], ['Tuzel', 'Oncel', ''], ['Cuturi', 'Marco', '']]","extracted_entities":"[{'text': 'multimodal LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"multimodal LLMs","similarity_score":0.6805375218}
{"id":2408.12083,"submitter":"Wade Naylor Dr","authors":"Anna Chrysostomou, Alan S. Cornell, and Wade Naylor","title":"Dominant misconceptions and alluvial flows between Engineering and\n  Physical Science students","comments":"28 pages, 8 figures, 8 tables, APA7; BibTeX issues resolved","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.ed-ph physics.data-an","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this article we assess the comprehension of physics concepts by Physical\nScience and Engineering students enrolled in their first semester at the\nUniversity of Johannesburg (UJ), South Africa ($2022$). We employ different\ngraphical measures to explore similarities and differences using the results of\nboth pre- and post-test data from the Force Concept Inventory assessment tool,\nfrom which we calculate dominant misconceptions (DMs) and gains. We also use\nalluvial diagrams to track the choices made by these two groups of students\nfrom pre- to post-test stages. In our analysis, we find that DM results\nindicate that participating Engineering students outperformed Physical Science\nstudents on average, however, the same types of normalised DMs persist at the\npost-test level. We call these DMs \"persistent misconceptions.\" This is very\nuseful when tracking persistent misconceptions, where when using repeated\nmeasures and alluvial diagrams with smaller groups of students, we find that\nPhysical Science students tend to make more chaotic choices.\n","versions":"[{'version': 'v1', 'created': 'Thu, 22 Aug 2024 02:43:04 GMT'}, {'version': 'v2', 'created': 'Mon, 26 Aug 2024 13:07:48 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 12:32:49 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Chrysostomou', 'Anna', ''], ['Cornell', 'Alan S.', ''], ['Naylor', 'Wade', '']]","extracted_entities":"[{'text': 'DMs', 'label': 'LLMs'}, {'text': 'DMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"DMs","similarity_score":0.7158042192}
{"id":2310.11355,"submitter":"Tara Kalsi","authors":"Tara Kalsi, Alessandro Romito, Henning Schomerus","title":"Spectral chaos bounds from scaling theory of maximally efficient\n  quantum-dynamical scrambling","comments":"Accepted for publication in Quantum","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.stat-mech","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  A key conjecture about the evolution of complex quantum systems towards an\nergodic steady state, known as scrambling, is that this process acquires\nuniversal features when it is most efficient. We develop a single-parameter\nscaling theory for the spectral statistics in this scenario, which embodies\nexact self-similarity of the spectral correlations along the complete\nscrambling dynamics. We establish that the scaling predictions are matched by a\nprivileged stochastic process and serve as bounds for other dynamical\nscrambling scenarios, allowing one to quantify inefficient or incomplete\nscrambling on all time scales.\n","versions":"[{'version': 'v1', 'created': 'Tue, 17 Oct 2023 15:41:50 GMT'}, {'version': 'v2', 'created': 'Fri, 16 Feb 2024 16:19:39 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 18:21:31 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Kalsi', 'Tara', ''], ['Romito', 'Alessandro', ''], ['Schomerus', 'Henning', '']]","extracted_entities":"[{'text': 'single-parameter\\nscaling theory', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"single-parameter\nscaling theory","similarity_score":0.5990965366}
{"id":2402.05881,"submitter":"Matteo Nerini","authors":"Matteo Nerini, Golsa Ghiaasi, Bruno Clerckx","title":"Localized and Distributed Beyond Diagonal Reconfigurable Intelligent\n  Surfaces with Lossy Interconnections: Modeling and Optimization","comments":"Accepted by IEEE for publication","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IT eess.SP math.IT","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Reconfigurable intelligent surface (RIS) is a key technology to control the\ncommunication environment in future wireless networks. Recently, beyond\ndiagonal RIS (BD-RIS) emerged as a generalization of RIS achieving larger\ncoverage through additional tunable impedance components interconnecting the\nRIS elements. However, conventional RIS and BD-RIS can effectively serve only\nusers in their proximity, resulting in limited coverage. To overcome this\nlimitation, in this paper, we investigate distributed RIS, whose elements are\ndistributed over a wide region, in opposition to localized RIS commonly\nconsidered in the literature. The scaling laws of distributed BD-RIS reveal\nthat it offers significant gains over distributed conventional RIS and\nlocalized BD-RIS, enabled by its interconnections allowing signal propagation\nwithin the BD-RIS. To assess the practical performance of distributed BD-RIS,\nwe model and optimize BD-RIS with lossy interconnections through transmission\nline theory. Our model accounts for phase changes and losses over the BD-RIS\ninterconnections arising when the interconnection lengths are not much smaller\nthan the wavelength. Numerical results show that the performance of localized\nBD-RIS is only slightly impacted by losses, given the short interconnection\nlengths. Besides, distributed BD-RIS can achieve orders of magnitude of gains\nover conventional RIS, even in the presence of low losses.\n","versions":"[{'version': 'v1', 'created': 'Thu, 8 Feb 2024 18:15:41 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 17:35:36 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Nerini', 'Matteo', ''], ['Ghiaasi', 'Golsa', ''], ['Clerckx', 'Bruno', '']]","extracted_entities":"[{'text': 'scaling laws', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling laws","similarity_score":0.9373526573}
{"id":2404.16704,"submitter":"Gaoyong Sun","authors":"Chen-Chang Zeng, Zhen Cai, Guang-Heng Wang and Gaoyong Sun","title":"Fidelity and criticality in the nonreciprocal Aubry-Andr{\\'e}-Harper\n  model","comments":"6 pages, 5 figures","journal-ref":"Europhysics Letters 149, 38001 (2025)","doi":"10.1209\/0295-5075\/ada8d6","report-no":null,"categories":"cond-mat.dis-nn","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We study the critical behaviors of the ground and first excited states in the\none-dimensional nonreciprocal Aubry-Andr{\\'e}-Harper model using both the\nself-normal and biorthogonal fidelity susceptibilities. We demonstrate that\nfidelity susceptibility serves as a probe for the phase transition in the\nnonreciprocal AAH model. For ground states, characterized by real eigenenergies\nacross the entire regime, both fidelity susceptibilities near the critical\npoints scale as $N^{2}$, akin to the Hermitian AAH model. However, for the\nfirst-excited states, the fidelity susceptibilities exhibit distinct scaling\nlaws, contingent upon whether the lattice consists of even or odd sites. For\neven lattices, both the self-normal and biorthogonal fidelity susceptibilities\nnear the critical points continue to scale as $N^{2}$. In contrast, for odd\nlattices, the biorthogonal fidelity susceptibilities diverge, while the\nself-normal fidelity susceptibilities exhibit linear behavior, indicating a\nnovel scaling law.\n","versions":"[{'version': 'v1', 'created': 'Thu, 25 Apr 2024 16:07:55 GMT'}, {'version': 'v2', 'created': 'Wed, 1 May 2024 00:28:44 GMT'}, {'version': 'v3', 'created': 'Wed, 26 Feb 2025 05:45:53 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Zeng', 'Chen-Chang', ''], ['Cai', 'Zhen', ''], ['Wang', 'Guang-Heng', ''], ['Sun', 'Gaoyong', '']]","extracted_entities":"[{'text': 'Hermitian AAH model', 'label': 'AI model'}, {'text': 'novel scaling law', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"novel scaling law","similarity_score":0.9090615511}
{"id":2406.06418,"submitter":"Oliver Hahn","authors":"Oliver Hahn, Giulia Ferrini, Ryuji Takagi","title":"Bridging magic and non-Gaussian resources via Gottesman-Kitaev-Preskill\n  encoding","comments":null,"journal-ref":"PRX Quantum 6, 010330 (2025)","doi":"10.1103\/PRXQuantum.6.010330","report-no":null,"categories":"quant-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Although the similarity between non-stabilizer states -- also known as magic\nstates -- in discrete-variable systems and non-Gaussian states in\ncontinuous-variable systems has widely been recognized, the precise connections\nbetween these two notions have still been unclear. We establish a fundamental\nlink between these two quantum resources via the Gottesman-Kitaev-Preskill\n(GKP) encoding. We show that the negativity of the continuous-variable Wigner\nfunction for an encoded GKP state coincides with a magic measure we introduce,\nwhich matches the negativity of the discrete Wigner function for odd\ndimensions. We also provide a continuous-variable representation of the\nstabilizer R\\'enyi entropy -- a recent proposal for a magic measure for\nmulti-qubit states. With this in hand, we give a classical simulation algorithm\nwith runtime scaling with the resource contents, quantified by our magic\nmeasures. We also employ our results to prove that implementing a multi-qubit\nlogical non-Clifford operation in the GKP code subspace requires a non-Gaussian\noperation even at the limit of perfect encoding, despite the fact that the\nideal GKP states already come with a large amount of non-Gaussianity.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Jun 2024 16:09:45 GMT'}, {'version': 'v2', 'created': 'Wed, 16 Oct 2024 01:17:41 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Feb 2025 00:48:28 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Hahn', 'Oliver', ''], ['Ferrini', 'Giulia', ''], ['Takagi', 'Ryuji', '']]","extracted_entities":"[{'text': 'runtime scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"runtime scaling","similarity_score":0.5401808023}
{"id":2112.0646,"submitter":"Juyong Jiang","authors":"Juyong Jiang, Peiyan Zhang, Yingtao Luo, Chaozhuo Li, Jae Boum Kim,\n  Kai Zhang, Senzhang Wang, Sunghun Kim, Philip S. Yu","title":"Improving Sequential Recommendations via Bidirectional Temporal Data\n  Augmentation with Pre-training","comments":"Accepted by TKDE","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Sequential recommendation systems are integral to discerning temporal user\npreferences. Yet, the task of learning from abbreviated user interaction\nsequences poses a notable challenge. Data augmentation has been identified as a\npotent strategy to enhance the informational richness of these sequences.\nTraditional augmentation techniques, such as item randomization, may disrupt\nthe inherent temporal dynamics. Although recent advancements in reverse\nchronological pseudo-item generation have shown promise, they can introduce\ntemporal discrepancies when assessed in a natural chronological context. In\nresponse, we introduce a sophisticated approach, Bidirectional temporal data\nAugmentation with pre-training (BARec). Our approach leverages bidirectional\ntemporal augmentation and knowledge-enhanced fine-tuning to synthesize\nauthentic pseudo-prior items that retain user preferences and capture deeper\nitem semantic correlations, thus boosting the model's expressive power. Our\ncomprehensive experimental analysis on five benchmark datasets confirms the\nsuperiority of BARec across both short and elongated sequence contexts.\nMoreover, theoretical examination and case study offer further insight into the\nmodel's logical processes and interpretability. The source code for our study\nis publicly available at https:\/\/github.com\/juyongjiang\/BARec.\n","versions":"[{'version': 'v1', 'created': 'Mon, 13 Dec 2021 07:33:28 GMT'}, {'version': 'v2', 'created': 'Sun, 1 May 2022 06:01:36 GMT'}, {'version': 'v3', 'created': 'Tue, 5 Jul 2022 09:25:36 GMT'}, {'version': 'v4', 'created': 'Thu, 7 Jul 2022 02:33:02 GMT'}, {'version': 'v5', 'created': 'Tue, 26 Mar 2024 03:44:29 GMT'}, {'version': 'v6', 'created': 'Mon, 24 Feb 2025 18:44:15 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Jiang', 'Juyong', ''], ['Zhang', 'Peiyan', ''], ['Luo', 'Yingtao', ''], ['Li', 'Chaozhuo', ''], ['Kim', 'Jae Boum', ''], ['Zhang', 'Kai', ''], ['Wang', 'Senzhang', ''], ['Kim', 'Sunghun', ''], ['Yu', 'Philip S.', '']]","extracted_entities":"[{'text': 'knowledge-enhanced fine-tuning', 'label': 'Fine-tuning'}, {'text': 'BARec', 'label': 'Generative Pre-trained Transformer (GPT)'}]","assigned_concept":"Fine-tuning","matched_keyword":"knowledge-enhanced fine-tuning","similarity_score":0.7157994509}
{"id":2206.13618,"submitter":"Silpa Babu","authors":"Silpa Babu, Sajan Goud Lingala, Namrata Vaswani","title":"Fast Low Rank column-wise Compressive Sensing for Accelerated Dynamic\n  MRI","comments":"I have a duplication submission in arXiv (arXiv:2212.09664)","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This work develops a fast, memory-efficient, and general algorithm for\naccelerated\/undersampled dynamic MRI by assuming an approximate LR model on the\nmatrix formed by the vectorized images of the sequence. By general, we mean\nthat our algorithm can be used for multiple accelerated dynamic MRI\napplications and multiple sampling rates (acceleration rates) and patterns with\na single choice of parameters (no parameter tuning). We show that our proposed\nalgorithms, alternating Gradient Descent (GD) and minimization for MRI\n(altGDmin-MRI and altGDmin-MRI2), outperform many existing approaches while\nalso being faster than all of them, on average. This claim is based on\ncomparisons on 8 different retrospectively undersampled single- or multi-coil\ndynamic MRI applications, undersampled using either 1D Cartesian or 2D\npseudo-radial undersampling at multiple sampling rates. All comparisons used\nthe same set of algorithm parameters. Our second contribution is a mini-batch\nand a fully online extension that can process new measurements and return\nreconstructions either as soon as measurements of a new image frame arrive, or\nafter a short delay.\n","versions":"[{'version': 'v1', 'created': 'Mon, 27 Jun 2022 20:31:06 GMT'}, {'version': 'v2', 'created': 'Wed, 10 Aug 2022 18:07:01 GMT'}, {'version': 'v3', 'created': 'Thu, 27 Feb 2025 15:52:19 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Babu', 'Silpa', ''], ['Lingala', 'Sajan Goud', ''], ['Vaswani', 'Namrata', '']]","extracted_entities":"[{'text': 'no parameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"no parameter tuning","similarity_score":0.6061486006}
{"id":2310.03249,"submitter":"Mohamed Aghzal","authors":"Mohamed Aghzal, Erion Plaku, Ziyu Yao","title":"Can Large Language Models be Good Path Planners? A Benchmark and\n  Investigation on Spatial-temporal Reasoning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs) have achieved remarkable success across a wide\nspectrum of tasks; however, they still face limitations in scenarios that\ndemand long-term planning and spatial reasoning. To facilitate this line of\nresearch, in this work, we propose a new benchmark, termed $\\textbf{P}$ath\n$\\textbf{P}$lanning from $\\textbf{N}$atural $\\textbf{L}$anguage\n($\\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by\nformulating ''path planning'' tasks that require an LLM to navigate to target\nlocations while avoiding obstacles and adhering to constraints. Leveraging this\nbenchmark, we systematically investigate LLMs including GPT-4 via different\nfew-shot prompting methodologies as well as BART and T5 of various sizes via\nfine-tuning. Our experimental results show the promise of few-shot GPT-4 in\nspatial reasoning, when it is prompted to reason and act interleavedly,\nalthough it still fails to perform long-term temporal reasoning. In contrast,\nwhile fine-tuned LLMs achieved impressive results on in-distribution reasoning\ntasks, they struggled to generalize to larger environments or environments with\nmore obstacles.\n","versions":"[{'version': 'v1', 'created': 'Thu, 5 Oct 2023 01:42:16 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Feb 2024 20:18:54 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 00:58:13 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Aghzal', 'Mohamed', ''], ['Plaku', 'Erion', ''], ['Yao', 'Ziyu', '']]","extracted_entities":"[{'text': 'GPT-4', 'label': 'GPT-4'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'GPT-4', 'label': 'GPT-4'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2402.17073,"submitter":"Abhishek Dalvi","authors":"Abhishek Dalvi, Vasant Honavar","title":"Hyperdimensional Representation Learning for Node Classification and\n  Link Prediction","comments":"Accepted by WSDM 2025","journal-ref":null,"doi":"10.1145\/3701551.3703492","report-no":null,"categories":"cs.LG cs.AI cs.SI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We introduce Hyperdimensional Graph Learner (HDGL), a novel method for node\nclassification and link prediction in graphs. HDGL maps node features into a\nvery high-dimensional space (\\textit{hyperdimensional} or HD space for short)\nusing the \\emph{injectivity} property of node representations in a family of\nGraph Neural Networks (GNNs) and then uses HD operators such as\n\\textit{bundling} and \\textit{binding} to aggregate information from the local\nneighborhood of each node yielding latent node representations that can support\nboth node classification and link prediction tasks. HDGL, unlike GNNs that rely\non computationally expensive iterative optimization and hyperparameter tuning,\nrequires only a single pass through the data set. We report results of\nexperiments using widely used benchmark datasets which demonstrate that, on the\nnode classification task, HDGL achieves accuracy that is competitive with that\nof the state-of-the-art GNN methods at substantially reduced computational\ncost; and on the link prediction task, HDGL matches the performance of DeepWalk\nand related methods, although it falls short of computationally demanding\nstate-of-the-art GNNs.\n","versions":"[{'version': 'v1', 'created': 'Mon, 26 Feb 2024 23:15:01 GMT'}, {'version': 'v2', 'created': 'Sat, 20 Jul 2024 03:46:13 GMT'}, {'version': 'v3', 'created': 'Thu, 27 Feb 2025 00:21:39 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Dalvi', 'Abhishek', ''], ['Honavar', 'Vasant', '']]","extracted_entities":"[{'text': 'hyperparameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"hyperparameter tuning","similarity_score":0.6193697453}
{"id":2403.12117,"submitter":"Josua Stadelmaier","authors":"Josua Stadelmaier (University of T\\\"ubingen), Brandon Malone (NEC\n  OncoImmunity), Ralf Eggeling (University of T\\\"ubingen)","title":"Transfer Learning for T-Cell Response Prediction","comments":"25 pages, 10 figures. Source code, compiled data, final model, and a\n  video presentation are available under\n  https:\/\/github.com\/JosuaStadelmaier\/T-cell-response-prediction","journal-ref":null,"doi":null,"report-no":null,"categories":"q-bio.CB cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We study the prediction of T-cell response for specific given peptides, which\ncould, among other applications, be a crucial step towards the development of\npersonalized cancer vaccines. It is a challenging task due to limited,\nheterogeneous training data featuring a multi-domain structure; such data\nentail the danger of shortcut learning, where models learn general\ncharacteristics of peptide sources, such as the source organism, rather than\nspecific peptide characteristics associated with T-cell response.\n  Using a transformer model for T-cell response prediction, we show that the\ndanger of inflated predictive performance is not merely theoretical but occurs\nin practice. Consequently, we propose a domain-aware evaluation scheme. We then\nstudy different transfer learning techniques to deal with the multi-domain\nstructure and shortcut learning. We demonstrate a per-source fine tuning\napproach to be effective across a wide range of peptide sources and further\nshow that our final model is competitive with existing state-of-the-art\napproaches for predicting T-cell responses for human peptides.\n","versions":"[{'version': 'v1', 'created': 'Mon, 18 Mar 2024 17:32:19 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 21:40:40 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Stadelmaier', 'Josua', '', 'University of T\u00fcbingen'], ['Malone', 'Brandon', '', 'NEC\\n  OncoImmunity'], ['Eggeling', 'Ralf', '', 'University of T\u00fcbingen']]","extracted_entities":"[{'text': 'shortcut learning', 'label': 'Few-shot Learning'}, {'text': 'shortcut learning', 'label': 'Few-shot Learning'}, {'text': 'per-source fine tuning\\napproach', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"per-source fine tuning\napproach","similarity_score":0.7015017867}
{"id":2403.18466,"submitter":"Alessandro Gabbana","authors":"Giulio Ortali, Alessandro Gabbana, Nicola Demo, Gianluigi Rozza,\n  Federico Toschi","title":"Kinetic data-driven approach to turbulence subgrid modeling","comments":null,"journal-ref":"Phys. Rev. Research 7, 013202 (2025)","doi":"10.1103\/PhysRevResearch.7.013202","report-no":null,"categories":"physics.flu-dyn math-ph math.MP physics.comp-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Numerical simulations of turbulent flows are well known to pose extreme\ncomputational challenges due to the huge number of dynamical degrees of freedom\nrequired to correctly describe the complex multi-scale statistical correlations\nof the velocity. On the other hand, kinetic mesoscale approaches based on the\nBoltzmann equation, have the potential to describe a broad range of flows,\nstretching well beyond the special case of gases close to equilibrium, which\nresults in the ordinary Navier-Stokes dynamics. Here we demonstrate that, by\nproperly tuning, a kinetic approach can statistically reproduce the\nquantitative dynamics of the larger scales in turbulence, thereby providing an\nalternative, computationally efficient and physically rooted approach towards\nsubgrid scale (SGS) modeling in turbulence. More specifically we show that by\nleveraging on data from fully resolved Direct Numerical Simulation (DNS) we can\nlearn a collision operator for the discretized Boltzmann equation solver (the\nlattice Boltzmann method), which effectively implies a turbulence subgrid\nclosure model. The mesoscopic nature of our formulation makes the learning\nproblem fully local in both space and time, leading to reduced computational\ncosts and enhanced generalization capabilities. We show that the model offers\nsuperior performance compared to traditional methods, such as the Smagorinsky\nmodel, being less dissipative and, therefore, being able to more closely\ncapture the intermittency of higher-order velocity correlations. This\nfoundational work lays the basis for extending the proposed framework to\ndifferent turbulent flow settings and -- most importantly -- to develop new\nclasses of hybrid data-driven kinetic-based models capable of faithfully\ncapturing the complex macroscopic dynamics of diverse physical systems such as\nemulsions, non-Newtonian fluid and multiphase systems.\n","versions":"[{'version': 'v1', 'created': 'Wed, 27 Mar 2024 11:22:26 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 22:05:19 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Ortali', 'Giulio', ''], ['Gabbana', 'Alessandro', ''], ['Demo', 'Nicola', ''], ['Rozza', 'Gianluigi', ''], ['Toschi', 'Federico', '']]","extracted_entities":"[{'text': 'properly tuning', 'label': 'Fine-tuning'}, {'text': 'Smagorinsky\\nmodel', 'label': 'Foundation Model'}]","assigned_concept":"Fine-tuning","matched_keyword":"properly tuning","similarity_score":0.8581801653}
{"id":2404.11922,"submitter":"Hans Jarett Ong","authors":"Hans Jarett J. Ong, Brian Godwin S. Lim, Renzo Roel P. Tan, Kazushi\n  Ikeda","title":"Redefining the Shortest Path Problem Formulation of the Linear\n  Non-Gaussian Acyclic Model: Pairwise Likelihood Ratios, Prior Knowledge, and\n  Path Enumeration","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ME","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Effective causal discovery is essential for learning the causal graph from\nobservational data. The linear non-Gaussian acyclic model (LiNGAM) operates\nunder the assumption of a linear data generating process with non-Gaussian\nnoise in determining the causal graph. Its assumption of unmeasured confounders\nbeing absent, however, poses practical limitations. In response, empirical\nresearch has shown that the reformulation of LiNGAM as a shortest path problem\n(LiNGAM-SPP) addresses this limitation. Within LiNGAM-SPP, mutual information\nis chosen to serve as the measure of independence. A challenge is introduced -\nparameter tuning is now needed due to its reliance on kNN mutual information\nestimators. The paper proposes a threefold enhancement to the LiNGAM-SPP\nframework.\n  First, the need for parameter tuning is eliminated by using the pairwise\nlikelihood ratio in lieu of kNN-based mutual information. This substitution is\nvalidated on a general data generating process and benchmark real-world data\nsets, outperforming existing methods especially when given a larger set of\nfeatures. The incorporation of prior knowledge is then enabled by a\nnode-skipping strategy implemented on the graph representation of all causal\norderings to eliminate violations based on the provided input of relative\norderings. Flexibility relative to existing approaches is achieved. Last among\nthe three enhancements is the utilization of the distribution of paths in the\ngraph representation of all causal orderings. From this, crucial properties of\nthe true causal graph such as the presence of unmeasured confounders and\nsparsity may be inferred. To some extent, the expected performance of the\ncausal discovery algorithm may be predicted. The refinements above advance the\npracticality and performance of LiNGAM-SPP, showcasing the potential of\ngraph-search-based methodologies in advancing causal discovery.\n","versions":"[{'version': 'v1', 'created': 'Thu, 18 Apr 2024 05:59:28 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 07:32:41 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Ong', 'Hans Jarett J.', ''], ['Lim', 'Brian Godwin S.', ''], ['Tan', 'Renzo Roel P.', ''], ['Ikeda', 'Kazushi', '']]","extracted_entities":"[{'text': 'parameter tuning', 'label': 'Fine-tuning'}, {'text': 'parameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"parameter tuning","similarity_score":0.6959539652}
{"id":2404.16496,"submitter":"Domniki Ladopoulou","authors":"Filippo Fiocchi, Domna Ladopoulou and Petros Dellaportas","title":"Probabilistic Multi-Layer Perceptrons for Wind Farm Condition Monitoring","comments":"10 pages, 9 figures, 3 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.AP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We provide a condition monitoring system for wind farms, based on normal\nbehaviour modelling using a probabilistic multi-layer perceptron with transfer\nlearning via fine-tuning. The model predicts the output power of the wind\nturbine under normal behaviour based on features retrieved from supervisory\ncontrol and data acquisition (SCADA) systems. Its advantages are that (i) it\ncan be trained with SCADA data of at least a few years, (ii) it can incorporate\nall SCADA data of all wind turbines in a wind farm as features, (iii) it\nassumes that the output power follows a normal density with heteroscedastic\nvariance and (iv) it can predict the output of one wind turbine by borrowing\nstrength from the data of all other wind turbines in a farm. Probabilistic\nguidelines for condition monitoring are given via a cumulative sum (CUSUM)\ncontrol chart, which is specifically designed based on a real-data\nclassification exercise and, hence, is adapted to the needs of a wind farm. We\nillustrate the performance of our model in a real SCADA data example which\nprovides evidence that it outperforms other probabilistic prediction models.\n","versions":"[{'version': 'v1', 'created': 'Thu, 25 Apr 2024 10:41:12 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 11:14:25 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Fiocchi', 'Filippo', ''], ['Ladopoulou', 'Domna', ''], ['Dellaportas', 'Petros', '']]","extracted_entities":"[{'text': 'transfer\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'model', 'label': 'AI model'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2405.14804,"submitter":"Xin Xu","authors":"Xin Xu, Tong Xiao, Zitong Chao, Zhenya Huang, Can Yang, Yang Wang","title":"Can LLMs Solve longer Math Word Problems Better?","comments":"Accepted to ICLR 2025","journal-ref":"International Conference on Learning Representations (ICLR 2025)","doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Math Word Problems (MWPs) play a vital role in assessing the capabilities of\nLarge Language Models (LLMs), yet current research primarily focuses on\nquestions with concise contexts. The impact of longer contexts on mathematical\nreasoning remains under-explored. This study pioneers the investigation of\nContext Length Generalizability (CoLeG), which refers to the ability of LLMs to\nsolve MWPs with extended narratives. We introduce Extended Grade-School Math\n(E-GSM), a collection of MWPs featuring lengthy narratives, and propose two\nnovel metrics to evaluate the efficacy and resilience of LLMs in tackling these\nproblems. Our analysis of existing zero-shot prompting techniques with\nproprietary LLMs along with open-source LLMs reveals a general deficiency in\nCoLeG. To alleviate these issues, we propose tailored approaches for different\ncategories of LLMs. For proprietary LLMs, we introduce a new instructional\nprompt designed to mitigate the impact of long contexts. For open-source LLMs,\nwe develop a novel auxiliary task for fine-tuning to enhance CoLeG. Our\ncomprehensive results demonstrate the effectiveness of our proposed methods,\nshowing improved performance on E-GSM. Additionally, we conduct an in-depth\nanalysis to differentiate the effects of semantic understanding and reasoning\nefficacy, showing that our methods improves the latter. We also establish the\ngeneralizability of our methods across several other MWP benchmarks. Our\nfindings highlight the limitations of current LLMs and offer practical\nsolutions correspondingly, paving the way for further exploration of model\ngeneralizability and training methodologies.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 May 2024 17:13:50 GMT'}, {'version': 'v2', 'created': 'Thu, 23 Jan 2025 15:47:09 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Feb 2025 07:58:27 GMT'}, {'version': 'v4', 'created': 'Wed, 26 Feb 2025 02:21:40 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Xu', 'Xin', ''], ['Xiao', 'Tong', ''], ['Chao', 'Zitong', ''], ['Huang', 'Zhenya', ''], ['Yang', 'Can', ''], ['Wang', 'Yang', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'open-source LLMs', 'label': 'Open-source LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'instructional\\nprompt', 'label': 'Prompting'}, {'text': 'open-source LLMs', 'label': 'Open-source LLMs'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2406.1212,"submitter":"Yulai Zhao","authors":"Yulai Zhao, Masatoshi Uehara, Gabriele Scalia, Sunyuan Kung, Tommaso\n  Biancalani, Sergey Levine, Ehsan Hajiramezanali","title":"Adding Conditional Control to Diffusion Models with Reinforcement\n  Learning","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI stat.ML","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Diffusion models are powerful generative models that allow for precise\ncontrol over the characteristics of the generated samples. While these\ndiffusion models trained on large datasets have achieved success, there is\noften a need to introduce additional controls in downstream fine-tuning\nprocesses, treating these powerful models as pre-trained diffusion models. This\nwork presents a novel method based on reinforcement learning (RL) to add such\ncontrols using an offline dataset comprising inputs and labels. We formulate\nthis task as an RL problem, with the classifier learned from the offline\ndataset and the KL divergence against pre-trained models serving as the reward\nfunctions. Our method, $\\textbf{CTRL}$ ($\\textbf{C}$onditioning\npre-$\\textbf{T}$rained diffusion models with $\\textbf{R}$einforcement\n$\\textbf{L}$earning), produces soft-optimal policies that maximize the\nabovementioned reward functions. We formally demonstrate that our method\nenables sampling from the conditional distribution with additional controls\nduring inference. Our RL-based approach offers several advantages over existing\nmethods. Compared to classifier-free guidance, it improves sample efficiency\nand can greatly simplify dataset construction by leveraging conditional\nindependence between the inputs and additional controls. Additionally, unlike\nclassifier guidance, it eliminates the need to train classifiers from\nintermediate states to additional controls. The code is available at\nhttps:\/\/github.com\/zhaoyl18\/CTRL.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Jun 2024 22:00:26 GMT'}, {'version': 'v2', 'created': 'Fri, 7 Feb 2025 04:08:17 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 02:16:23 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Zhao', 'Yulai', ''], ['Uehara', 'Masatoshi', ''], ['Scalia', 'Gabriele', ''], ['Kung', 'Sunyuan', ''], ['Biancalani', 'Tommaso', ''], ['Levine', 'Sergey', ''], ['Hajiramezanali', 'Ehsan', '']]","extracted_entities":"[{'text': 'Diffusion models', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'diffusion models', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'downstream fine-tuning\\nprocesses', 'label': 'Fine-tuning'}, {'text': 'diffusion models', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'reinforcement learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"downstream fine-tuning\nprocesses","similarity_score":0.6032344103}
{"id":2406.14115,"submitter":"Feng Jiang","authors":"Ziche Liu, Rui Ke, Yajiao Liu, Feng Jiang, Haizhou Li","title":"Take the essence and discard the dross: A Rethinking on Data Selection\n  for Fine-Tuning Large Language Models","comments":"Accepted by the NAACL 2025 main conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Data selection for fine-tuning large language models (LLMs) aims to choose a\nhigh-quality subset from existing datasets, allowing the trained model to\noutperform baselines trained on the full dataset. However, the expanding body\nof research lacks a clear, unified framework, and the variability in\nexperimental settings complicates systematic comparisons. While existing\nsurveys comprehensively overview the stages and methods of data selection, they\noften overlook an in-depth exploration of the fine-tuning phase. In this paper,\nwe conduct a focused review of recent data selection techniques for fine-tuning\nLLMs, analyzing a dozen key studies. We introduce a novel three-stage scheme -\ncomprising feature extraction, criteria design, and selector evaluation - to\nsystematically categorize and evaluate these methods. Additionally, we propose\na unified comparison approach that incorporates ratio-based efficiency and\nranking-based feasibility metrics to address inconsistencies across\nexperiments. Our findings reveal that methods emphasizing more targeted quality\nmeasurement achieve higher efficiency but at the cost of feasibility. Finally,\nwe discuss trends and highlight four key challenges in fine-tuning data\nselection, offering potential directions for future research.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Jun 2024 08:58:58 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 07:59:00 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Liu', 'Ziche', ''], ['Ke', 'Rui', ''], ['Liu', 'Yajiao', ''], ['Jiang', 'Feng', ''], ['Li', 'Haizhou', '']]","extracted_entities":"[{'text': 'feature extraction', 'label': 'Fine-tuning'}, {'text': 'criteria design', 'label': 'Fine-tuning'}, {'text': 'selector evaluation', 'label': 'Fine-tuning'}, {'text': 'ratio-based efficiency', 'label': 'Fine-tuning'}, {'text': 'fine-tuning data\\nselection', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning data\nselection","similarity_score":0.5828591585}
{"id":2406.16793,"submitter":"Yushun Zhang","authors":"Yushun Zhang, Congliang Chen, Ziniu Li, Tian Ding, Chenwei Wu,\n  Diederik P. Kingma, Yinyu Ye, Zhi-Quan Luo, Ruoyu Sun","title":"Adam-mini: Use Fewer Learning Rates To Gain More","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We propose Adam-mini, an optimizer that achieves on par or better performance\nthan AdamW with 50% less memory footprint. Adam-mini reduces memory by cutting\ndown the learning rate resources in Adam (i.e., $1\/\\sqrt{v}$). By investigating\nthe Hessian structure of neural nets, we find Adam's $v$ might not function at\nits full potential as effectively as we expected. We find that $\\geq$ 99.9% of\nthese learning rates in $v$ could be harmlessly removed if we (1) carefully\npartition the parameters into blocks following our new principle on Hessian\nstructure; (2) assign a single but good learning rate to each parameter block.\nWe then provide one simple way to find good learning rates and propose\nAdam-mini. Empirically, we verify that Adam-mini performs on par or better than\nAdamW on various language models sized from 39M to 13B for pre-training,\nsupervised fine-tuning, and RLHF. The reduced memory footprint of Adam-mini\nalso alleviates communication overheads among GPUs, thereby increasing\nthroughput. For instance, Adam-mini achieves 49.6% higher throughput than AdamW\nwhen pre-training Llama 2-7B on $2\\times$ A800-80GB GPUs, which saves 33%\nwall-clock time for pre-training.\n","versions":"[{'version': 'v1', 'created': 'Mon, 24 Jun 2024 16:56:41 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Jun 2024 17:45:06 GMT'}, {'version': 'v3', 'created': 'Wed, 26 Jun 2024 13:03:16 GMT'}, {'version': 'v4', 'created': 'Mon, 1 Jul 2024 17:46:19 GMT'}, {'version': 'v5', 'created': 'Wed, 3 Jul 2024 16:38:17 GMT'}, {'version': 'v6', 'created': 'Mon, 11 Nov 2024 16:59:58 GMT'}, {'version': 'v7', 'created': 'Mon, 24 Feb 2025 11:29:08 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Zhang', 'Yushun', ''], ['Chen', 'Congliang', ''], ['Li', 'Ziniu', ''], ['Ding', 'Tian', ''], ['Wu', 'Chenwei', ''], ['Kingma', 'Diederik P.', ''], ['Ye', 'Yinyu', ''], ['Luo', 'Zhi-Quan', ''], ['Sun', 'Ruoyu', '']]","extracted_entities":"[{'text': 'Adam-mini', 'label': 'ALBERT'}, {'text': 'Adam-mini', 'label': 'ALBERT'}, {'text': 'Hessian structure', 'label': 'BERT'}, {'text': 'Hessian\\nstructure', 'label': 'BERT'}, {'text': 'Adam-mini', 'label': 'ALBERT'}, {'text': 'Adam-mini', 'label': 'ALBERT'}, {'text': 'AdamW', 'label': 'ALBERT'}, {'text': 'pre-training', 'label': 'Few-shot Learning'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Adam-mini', 'label': 'ALBERT'}, {'text': 'Adam-mini', 'label': 'ALBERT'}, {'text': 'AdamW', 'label': 'ALBERT'}, {'text': 'Llama 2-7B', 'label': 'Llama'}]","assigned_concept":"Fine-tuning","matched_keyword":"supervised fine-tuning","similarity_score":0.7449287176}
{"id":2407.1952,"submitter":"Wu Tz-Ying","authors":"Tz-Ying Wu, Kyle Min, Subarna Tripathi, Nuno Vasconcelos","title":"Ego-VPA: Egocentric Video Understanding with Parameter-efficient\n  Adaptation","comments":"Accepted to WACV 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Video understanding typically requires fine-tuning the large backbone when\nadapting to new domains. In this paper, we leverage the egocentric video\nfoundation models (Ego-VFMs) based on video-language pre-training and propose a\nparameter-efficient adaptation for egocentric video tasks, namely Ego-VPA. It\nemploys a local sparse approximation for each video frame\/text feature using\nthe basis prompts, and the selected basis prompts are used to synthesize\nvideo\/text prompts. Since the basis prompts are shared across frames and\nmodalities, it models context fusion and cross-modal transfer in an efficient\nfashion. Experiments show that Ego-VPA excels in lightweight adaptation (with\nonly 0.84% learnable parameters), largely improving over baselines and reaching\nthe performance of full fine-tuning.\n","versions":"[{'version': 'v1', 'created': 'Sun, 28 Jul 2024 16:01:32 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 02:37:53 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Wu', 'Tz-Ying', ''], ['Min', 'Kyle', ''], ['Tripathi', 'Subarna', ''], ['Vasconcelos', 'Nuno', '']]","extracted_entities":"[{'text': 'egocentric video\\nfoundation models', 'label': 'Foundation Model'}, {'text': 'Ego-VFMs', 'label': 'Foundation Model'}, {'text': 'basis prompts', 'label': 'Prompting'}, {'text': 'basis prompts', 'label': 'Prompting'}, {'text': 'video\/text prompts', 'label': 'Prompting'}, {'text': 'basis prompts', 'label': 'Prompting'}, {'text': 'context fusion', 'label': 'contextual Embedding'}, {'text': 'full fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"full fine-tuning","similarity_score":0.9569243193}
{"id":2408.09886,"submitter":"Haixia Bi","authors":"Sihan Yang, Xuande Mi, Jiadong Feng, Haixia Bi, Hai Zhang and Jian Sun","title":"Improved Baselines with Synchronized Encoding for Universal Medical\n  Image Segmentation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large foundation models, known for their strong zero-shot generalization\ncapabilities, can be applied to a wide range of downstream tasks. However,\ndeveloping foundation models for medical image segmentation poses a significant\nchallenge due to the domain gap between natural and medical images. While\nfine-tuning techniques based on the Segment Anything Model (SAM) have been\nexplored, they primarily focus on scaling up data or refining inference\nstrategies without incorporating domain-specific architectural designs,\nlimiting their zero-shot performance. To optimize segmentation performance\nunder standard inference settings and provide a strong baseline for future\nresearch, we introduce SyncSAM, which employs a synchronized dual-branch\nencoder that integrates convolution and Transformer features in a synchronized\nmanner to enhance medical image encoding, and a multi-scale dual-branch decoder\nto preserve image details. SyncSAM is trained on two of the largest medical\nimage segmentation datasets, SA-Med2D-20M and IMed-361M, resulting in a series\nof pre-trained models for universal medical image segmentation. Experimental\nresults demonstrate that SyncSAM not only achieves state-of-the-art performance\non test sets but also exhibits strong zero-shot capabilities on unseen\ndatasets. The code and model weights are available at\nhttps:\/\/github.com\/Hhankyangg\/SyncSAM.\n","versions":"[{'version': 'v1', 'created': 'Mon, 19 Aug 2024 11:01:00 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 15:24:27 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Yang', 'Sihan', ''], ['Mi', 'Xuande', ''], ['Feng', 'Jiadong', ''], ['Bi', 'Haixia', ''], ['Zhang', 'Hai', ''], ['Sun', 'Jian', '']]","extracted_entities":"[{'text': 'fine-tuning techniques', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning techniques","similarity_score":0.8813601136}
{"id":2404.06004,"submitter":"Kento Tatsuno","authors":"Kento Tatsuno, Daisuke Miyashita, Taiga Ikeda, Kiyoshi Ishiyama,\n  Kazunari Sumiyoshi and Jun Deguchi","title":"AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free\n  Information Retrieval","comments":"6 pages, 8 figures and 5 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR cs.CL cs.DS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Graph-based approximate nearest neighbor search (ANNS) algorithms work\neffectively against large-scale vector retrieval. Among such methods, DiskANN\nachieves good recall-speed tradeoffs using both DRAM and storage. DiskANN\nadopts product quantization (PQ) to reduce memory usage, which is still\nproportional to the scale of datasets. In this paper, we propose All-in-Storage\nANNS with Product Quantization (AiSAQ), which offloads compressed vectors to\nthe SSD index. Our method achieves $\\sim$10 MB memory usage in query search\nwith billion-scale datasets without critical latency degradation. AiSAQ also\nreduces the index load time for query search preparation, which enables fast\nswitch between muitiple billion-scale indices.This method can be applied to\nretrievers of retrieval-augmented generation (RAG) and be scaled out with\nmultiple-server systems for emerging datasets. Our DiskANN-based implementation\nis available on GitHub.\n","versions":"[{'version': 'v1', 'created': 'Tue, 9 Apr 2024 04:20:27 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 07:47:30 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Tatsuno', 'Kento', ''], ['Miyashita', 'Daisuke', ''], ['Ikeda', 'Taiga', ''], ['Ishiyama', 'Kiyoshi', ''], ['Sumiyoshi', 'Kazunari', ''], ['Deguchi', 'Jun', '']]","extracted_entities":"[{'text': 'product quantization', 'label': 'quantisation'}, {'text': 'Product Quantization', 'label': 'quantisation'}, {'text': 'AiSAQ', 'label': 'quantisation'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'GitHub', 'label': 'Open-source LLMs'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2405.13576,"submitter":"Jiajie Jin","authors":"Jiajie Jin, Yutao Zhu, Guanting Dong, Yuyao Zhang, Xinyu Yang,\n  Chenghao Zhang, Tong Zhao, Zhao Yang, Zhicheng Dou, Ji-Rong Wen","title":"FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation\n  Research","comments":"The paper is accepted by WWW2025 Resource Track","journal-ref":null,"doi":"10.1145\/3701716.3715313.","report-no":null,"categories":"cs.CL cs.IR","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  With the advent of large language models (LLMs) and multimodal large language\nmodels (MLLMs), the potential of retrieval-augmented generation (RAG) has\nattracted considerable research attention. Various novel algorithms and models\nhave been introduced to enhance different aspects of RAG systems. However, the\nabsence of a standardized framework for implementation, coupled with the\ninherently complex RAG process, makes it challenging and time-consuming for\nresearchers to compare and evaluate these approaches in a consistent\nenvironment. Existing RAG toolkits, such as LangChain and LlamaIndex, while\navailable, are often heavy and inflexibly, failing to meet the customization\nneeds of researchers. In response to this challenge, we develop \\ours{}, an\nefficient and modular open-source toolkit designed to assist researchers in\nreproducing and comparing existing RAG methods and developing their own\nalgorithms within a unified framework. Our toolkit has implemented 16 advanced\nRAG methods and gathered and organized 38 benchmark datasets. It has various\nfeatures, including a customizable modular framework, multimodal RAG\ncapabilities, a rich collection of pre-implemented RAG works, comprehensive\ndatasets, efficient auxiliary pre-processing scripts, and extensive and\nstandard evaluation metrics. Our toolkit and resources are available at\nhttps:\/\/github.com\/RUC-NLPIR\/FlashRAG.\n","versions":"[{'version': 'v1', 'created': 'Wed, 22 May 2024 12:12:40 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 02:46:52 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Jin', 'Jiajie', ''], ['Zhu', 'Yutao', ''], ['Dong', 'Guanting', ''], ['Zhang', 'Yuyao', ''], ['Yang', 'Xinyu', ''], ['Zhang', 'Chenghao', ''], ['Zhao', 'Tong', ''], ['Yang', 'Zhao', ''], ['Dou', 'Zhicheng', ''], ['Wen', 'Ji-Rong', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'multimodal large language\\nmodels', 'label': 'Large Language Model'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'LangChain', 'label': 'Llama'}, {'text': 'LlamaIndex', 'label': 'Llama'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2406.14497,"submitter":"Zhiruo Wang","authors":"Zora Zhiruo Wang, Akari Asai, Xinyan Velocity Yu, Frank F. Xu, Yiqing\n  Xie, Graham Neubig, Daniel Fried","title":"CodeRAG-Bench: Can Retrieval Augment Code Generation?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  While language models (LMs) have proven remarkably adept at generating code,\nmany programs are challenging for LMs to generate using their parametric\nknowledge alone. Providing external contexts such as library documentation can\nfacilitate generating accurate and functional code. Despite the success of\nretrieval-augmented generation (RAG) in various text-oriented tasks, its\npotential for improving code generation remains under-explored. In this work,\nwe conduct a systematic, large-scale analysis by asking: in what scenarios can\nretrieval benefit code generation models? and what challenges remain? We first\ncurate a comprehensive evaluation benchmark, CodeRAG-Bench, encompassing three\ncategories of code generation tasks, including basic programming, open-domain,\nand repository-level problems. We aggregate documents from five sources for\nmodels to retrieve contexts: competition solutions, online tutorials, library\ndocumentation, StackOverflow posts, and GitHub repositories. We examine\ntop-performing models on CodeRAG-Bench by providing contexts retrieved from one\nor multiple sources. While notable gains are made in final code generation by\nretrieving high-quality contexts across various settings, our analysis reveals\nroom for improvement -- current retrievers still struggle to fetch useful\ncontexts especially with limited lexical overlap, and generators fail to\nimprove with limited context lengths or abilities to integrate additional\ncontexts. We hope CodeRAG-Bench serves as an effective testbed to encourage\nfurther development of advanced code-oriented RAG methods.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Jun 2024 16:59:52 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 22:10:36 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Wang', 'Zora Zhiruo', ''], ['Asai', 'Akari', ''], ['Yu', 'Xinyan Velocity', ''], ['Xu', 'Frank F.', ''], ['Xie', 'Yiqing', ''], ['Neubig', 'Graham', ''], ['Fried', 'Daniel', '']]","extracted_entities":"[{'text': 'RAG', 'label': 'RAG'}, {'text': 'GitHub repositories', 'label': 'Open-source LLMs'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2406.19271,"submitter":"Praneeth Vadlapati","authors":"Praneeth Vadlapati","title":"AutoPureData: Automated Filtering of Undesirable Web Data to Update LLM\n  Knowledge","comments":"Final version","journal-ref":"Journal of Mathematical & Computer Applications, 3 (2024) E121","doi":"10.47363\/JMCA\/2024(3)E121","report-no":null,"categories":"cs.CL cs.AI cs.IR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Up-to-date and reliable language models are consistently sought after and are\nessential in various applications. Typically, models are trained on a fixed\ndataset and then deployed globally. However, the knowledge of the models\nbecomes outdated. Enabling automatic updation of AI knowledge using web data\ninvolves significant concerns regarding the model's safety and quality due to a\nthreat from unsafe and undesirable text across the web. The purity of new data\nwas essential for updating knowledge of language models to maintain their\nreliability. This paper proposes AutoPureData, a system that automatically\ncollects and purifies web data. The system loaded a sample of web data.\nUtilizing existing trusted AI models, it successfully eliminated unsafe text\nwith an accuracy of 97% and undesirable text with an accuracy of 86%,\ndemonstrating the system's effectiveness in purifying the data. The system\nensures that only meaningful and safe text can be used to update LLM knowledge.\nThe pure text was then optimized and stored in a vector database for future\nquerying. It was found that LLM can fetch new data from the vector DB. The LLM\nwrites the RAG query in English, even if the user's query is in another\nlanguage, proving that the system can perform cross-lingual retrieval. This\npaper proposes a method to maintain the accuracy and relevance of up-to-date\nlanguage models by ensuring that only purified data was used to update LLM\nknowledge. This work contributes to updating knowledge of chatbots using\nmeaningful and safe text, enhancing their utility across various industries,\nand potentially reducing the risks associated with outputs caused by unsafe or\nimpure data. Code is available at github.com\/Pro-GenAI\/AutoPureData.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Jun 2024 15:37:57 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 07:17:52 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Vadlapati', 'Praneeth', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'chatbots', 'label': 'ChatGPT'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2305.14749,"submitter":"Chaitanya K. Joshi","authors":"Chaitanya K. Joshi, Arian R. Jamasb, Ramon Vi\\~nas, Charles Harris,\n  Simon V. Mathis, Alex Morehead, Rishabh Anand, Pietro Li\\`o","title":"gRNAde: Geometric Deep Learning for 3D RNA inverse design","comments":"ICLR 2025 camera-ready version (Spotlight presentation). Previously\n  titled 'Multi-State RNA Design with Geometric Multi-Graph Neural Networks',\n  presented at ICML 2023 Computational Biology Workshop","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG q-bio.BM q-bio.QM","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Computational RNA design tasks are often posed as inverse problems, where\nsequences are designed based on adopting a single desired secondary structure\nwithout considering 3D conformational diversity. We introduce gRNAde, a\ngeometric RNA design pipeline operating on 3D RNA backbones to design sequences\nthat explicitly account for structure and dynamics. gRNAde uses a multi-state\nGraph Neural Network and autoregressive decoding to generates candidate RNA\nsequences conditioned on one or more 3D backbone structures where the\nidentities of the bases are unknown. On a single-state fixed backbone re-design\nbenchmark of 14 RNA structures from the PDB identified by Das et al. (2010),\ngRNAde obtains higher native sequence recovery rates (56% on average) compared\nto Rosetta (45% on average), taking under a second to produce designs compared\nto the reported hours for Rosetta. We further demonstrate the utility of gRNAde\non a new benchmark of multi-state design for structurally flexible RNAs, as\nwell as zero-shot ranking of mutational fitness landscapes in a retrospective\nanalysis of a recent ribozyme. Experimental wet lab validation on 10 different\nstructured RNA backbones finds that gRNAde has a success rate of 50% at\ndesigning pseudoknotted RNA structures, a significant advance over 35% for\nRosetta. Open source code and tutorials are available at:\nhttps:\/\/github.com\/chaitjo\/geometric-rna-design\n","versions":"[{'version': 'v1', 'created': 'Wed, 24 May 2023 05:46:56 GMT'}, {'version': 'v2', 'created': 'Thu, 25 May 2023 14:53:11 GMT'}, {'version': 'v3', 'created': 'Sun, 28 May 2023 22:44:27 GMT'}, {'version': 'v4', 'created': 'Sun, 31 Mar 2024 10:03:17 GMT'}, {'version': 'v5', 'created': 'Sat, 25 May 2024 23:11:45 GMT'}, {'version': 'v6', 'created': 'Sun, 6 Oct 2024 06:39:41 GMT'}, {'version': 'v7', 'created': 'Tue, 25 Feb 2025 08:17:35 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Joshi', 'Chaitanya K.', ''], ['Jamasb', 'Arian R.', ''], ['Vi\u00f1as', 'Ramon', ''], ['Harris', 'Charles', ''], ['Mathis', 'Simon V.', ''], ['Morehead', 'Alex', ''], ['Anand', 'Rishabh', ''], ['Li\u00f2', 'Pietro', '']]","extracted_entities":"[{'text': 'zero-shot ranking', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot ranking","similarity_score":0.709821105}
{"id":2403.02774,"submitter":"Philipp Hess","authors":"Philipp Hess, Michael Aich, Baoxiang Pan, and Niklas Boers","title":"Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System\n  Model Fields with Generative Machine Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.ao-ph cs.CV cs.LG physics.geo-ph","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Accurate and high-resolution Earth system model (ESM) simulations are\nessential to assess the ecological and socio-economic impacts of anthropogenic\nclimate change, but are computationally too expensive to be run at sufficiently\nhigh spatial resolution. Recent machine learning approaches have shown\npromising results in downscaling ESM simulations, outperforming\nstate-of-the-art statistical approaches. However, existing methods require\ncomputationally costly retraining for each ESM and extrapolate poorly to\nclimates unseen during training. We address these shortcomings by learning a\nconsistency model (CM) that efficiently and accurately downscales arbitrary ESM\nsimulations without retraining in a zero-shot manner. Our approach yields\nprobabilistic downscaled fields at a resolution only limited by the\nobservational reference data. We show that the CM outperforms state-of-the-art\ndiffusion models at a fraction of computational cost while maintaining high\ncontrollability on the downscaling task. Further, our method generalizes to\nclimate states unseen during training without explicitly formulated physical\nconstraints.\n","versions":"[{'version': 'v1', 'created': 'Tue, 5 Mar 2024 08:41:41 GMT'}, {'version': 'v2', 'created': 'Thu, 2 Jan 2025 11:30:30 GMT'}, {'version': 'v3', 'created': 'Tue, 14 Jan 2025 11:14:57 GMT'}, {'version': 'v4', 'created': 'Wed, 26 Feb 2025 11:43:09 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Hess', 'Philipp', ''], ['Aich', 'Michael', ''], ['Pan', 'Baoxiang', ''], ['Boers', 'Niklas', '']]","extracted_entities":"[{'text': 'zero-shot manner', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot manner","similarity_score":0.6993873119}
{"id":2405.1466,"submitter":"Zhuowei Li","authors":"Zhuowei Li, Zihao Xu, Ligong Han, Yunhe Gao, Song Wen, Di Liu, Hao\n  Wang, Dimitris N. Metaxas","title":"Implicit In-context Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In-context Learning (ICL) empowers large language models (LLMs) to swiftly\nadapt to unseen tasks at inference-time by prefixing a few demonstration\nexamples before queries. Despite its versatility, ICL incurs substantial\ncomputational and memory overheads compared to zero-shot learning and is\nsensitive to the selection and order of demonstration examples. In this work,\nwe introduce Implicit In-context Learning (I2CL), an innovative paradigm that\nreduces the inference cost of ICL to that of zero-shot learning with minimal\ninformation loss. I2CL operates by first generating a condensed vector\nrepresentation, namely a context vector, extracted from the demonstration\nexamples. It then conducts an inference-time intervention through injecting a\nlinear combination of the context vector and query activations back into the\nmodel's residual streams. Empirical evaluation on nine real-world tasks across\nthree model architectures demonstrates that I2CL achieves few-shot level\nperformance at zero-shot inference cost, and it exhibits robustness against\nvariations in demonstration examples. Furthermore, I2CL facilitates a novel\nrepresentation of task-ids, enhancing task similarity detection and fostering\neffective transfer learning. We also perform a comprehensive analysis and\nablation study on I2CL, offering deeper insights into its internal mechanisms.\nCode is available at https:\/\/github.com\/LzVv123456\/I2CL.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 May 2024 14:57:52 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 14:49:33 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Li', 'Zhuowei', ''], ['Xu', 'Zihao', ''], ['Han', 'Ligong', ''], ['Gao', 'Yunhe', ''], ['Wen', 'Song', ''], ['Liu', 'Di', ''], ['Wang', 'Hao', ''], ['Metaxas', 'Dimitris N.', '']]","extracted_entities":"[{'text': 'ICL', 'label': 'Zero-shot Learning'}, {'text': 'zero-shot learning', 'label': 'Zero-shot Learning'}, {'text': 'ICL', 'label': 'Zero-shot Learning'}, {'text': 'zero-shot learning', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot learning","similarity_score":1.0000001192}
{"id":2406.08772,"submitter":"Xuannan Liu","authors":"Xuannan Liu and Zekun Li and Peipei Li and Huaibo Huang and Shuhan Xia\n  and Xing Cui and Linzhi Huang and Weihong Deng and Zhaofeng He","title":"MMFakeBench: A Mixed-Source Multimodal Misinformation Detection\n  Benchmark for LVLMs","comments":"Accepted by ICLR 2025, Project page:\n  https:\/\/liuxuannan.github.io\/MMFakeBench.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Current multimodal misinformation detection (MMD) methods often assume a\nsingle source and type of forgery for each sample, which is insufficient for\nreal-world scenarios where multiple forgery sources coexist. The lack of a\nbenchmark for mixed-source misinformation has hindered progress in this field.\nTo address this, we introduce MMFakeBench, the first comprehensive benchmark\nfor mixed-source MMD. MMFakeBench includes 3 critical sources: textual veracity\ndistortion, visual veracity distortion, and cross-modal consistency distortion,\nalong with 12 sub-categories of misinformation forgery types. We further\nconduct an extensive evaluation of 6 prevalent detection methods and 15 Large\nVision-Language Models (LVLMs) on MMFakeBench under a zero-shot setting. The\nresults indicate that current methods struggle under this challenging and\nrealistic mixed-source MMD setting. Additionally, we propose MMD-Agent, a novel\napproach to integrate the reasoning, action, and tool-use capabilities of LVLM\nagents, significantly enhancing accuracy and generalization. We believe this\nstudy will catalyze future research into more realistic mixed-source multimodal\nmisinformation and provide a fair evaluation of misinformation detection\nmethods.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Jun 2024 03:04:28 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Aug 2024 05:00:04 GMT'}, {'version': 'v3', 'created': 'Thu, 27 Feb 2025 03:19:48 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Liu', 'Xuannan', ''], ['Li', 'Zekun', ''], ['Li', 'Peipei', ''], ['Huang', 'Huaibo', ''], ['Xia', 'Shuhan', ''], ['Cui', 'Xing', ''], ['Huang', 'Linzhi', ''], ['Deng', 'Weihong', ''], ['He', 'Zhaofeng', '']]","extracted_entities":"[{'text': 'zero-shot setting', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot setting","similarity_score":0.6837423444}
{"id":2405.13929,"submitter":"Aleksandr Nikolich","authors":"Aleksandr Nikolich, Konstantin Korolev, Sergei Bratchikov, Igor\n  Kiselev, Artem Shelmanov","title":"Vikhr: Constructing a State-of-the-art Bilingual Open-Source\n  Instruction-Following Large Language Model for Russian","comments":"Accepted at WMRL @ EMNLP-2024","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  There has been a surge in developing various Large Language Models (LLMs).\nHowever, text generation for languages other than English often faces\nsignificant challenges, including poor generation quality and reduced\ncomputational performance due to the disproportionate representation of tokens\nin the model's vocabulary. In this work, we address these issues by developing\na pipeline for adapting English-oriented pre-trained models to other languages\nand constructing efficient bilingual LLMs. Using this pipeline, we construct\nVikhr, a state-of-the-art bilingual open-source instruction-following LLM\ndesigned specifically for the Russian language. \"Vikhr\" refers to the name of\nthe Mistral LLM series and means a \"strong gust of wind.\" Unlike previous\nRussian-language models that typically rely on LoRA adapters on top of\nEnglish-oriented models, sacrificing performance for lower training costs,\nVikhr features an adapted tokenizer vocabulary and undergoes continued\npre-training and instruction tuning of all weights. This not only enhances the\nmodel's performance but also significantly improves its computational and\ncontextual efficiency. The remarkable performance of Vikhr across various\nRussian-language benchmarks can also be attributed to our efforts in expanding\ninstruction datasets and corpora for continued pre-training. Vikhr not only\nsets a new state of the art among open-source LLMs for Russian but even\noutperforms some proprietary closed-source models on certain benchmarks. The\nmodel weights, instruction sets, and code are publicly available.\n","versions":"[{'version': 'v1', 'created': 'Wed, 22 May 2024 18:58:58 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Jun 2024 17:32:23 GMT'}, {'version': 'v3', 'created': 'Sat, 26 Oct 2024 08:47:36 GMT'}, {'version': 'v4', 'created': 'Wed, 13 Nov 2024 10:57:21 GMT'}, {'version': 'v5', 'created': 'Mon, 24 Feb 2025 13:24:20 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Nikolich', 'Aleksandr', ''], ['Korolev', 'Konstantin', ''], ['Bratchikov', 'Sergei', ''], ['Kiselev', 'Igor', ''], ['Shelmanov', 'Artem', '']]","extracted_entities":"[{'text': 'Mistral', 'label': 'Mistral'}, {'text': 'instruction tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Mistral","matched_keyword":"Mistral","similarity_score":1.0}
{"id":2407.21018,"submitter":"Yuhui Xu","authors":"Yuhui Xu, Zhanming Jie, Hanze Dong, Lei Wang, Xudong Lu, Aojun Zhou,\n  Amrita Saha, Caiming Xiong, Doyen Sahoo","title":"ThinK: Thinner Key Cache by Query-Driven Pruning","comments":"ICLR 2025 (Spotlight)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps:\/\/github.com\/SalesforceAIResearch\/ThinK.\n","versions":"[{'version': 'v1', 'created': 'Tue, 30 Jul 2024 17:59:08 GMT'}, {'version': 'v2', 'created': 'Thu, 3 Oct 2024 03:03:29 GMT'}, {'version': 'v3', 'created': 'Thu, 27 Feb 2025 12:30:43 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Xu', 'Yuhui', ''], ['Jie', 'Zhanming', ''], ['Dong', 'Hanze', ''], ['Wang', 'Lei', ''], ['Lu', 'Xudong', ''], ['Zhou', 'Aojun', ''], ['Saha', 'Amrita', ''], ['Xiong', 'Caiming', ''], ['Sahoo', 'Doyen', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'attention weights', 'label': 'Attention mechanism'}, {'text': 'Mistral', 'label': 'Mistral'}]","assigned_concept":"Mistral","matched_keyword":"Mistral","similarity_score":1.0}
{"id":2408.1469,"submitter":"James Liu","authors":"James Liu, Pragaash Ponnusamy, Tianle Cai, Han Guo, Yoon Kim, Ben\n  Athiwaratkun","title":"Training-Free Activation Sparsity in Large Language Models","comments":"Rev. 2: ICLR 2025 Acceptance (Spotlight)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Activation sparsity can enable practical inference speedups in large language\nmodels (LLMs) by reducing the compute and memory-movement required for matrix\nmultiplications during the forward pass. However, existing methods face\nlimitations that inhibit widespread adoption. Some approaches are tailored\ntowards older models with ReLU-based sparsity, while others require extensive\ncontinued pre-training on up to hundreds of billions of tokens. This paper\ndescribes TEAL, a simple training-free method that applies magnitude-based\nactivation sparsity to hidden states throughout the entire model. TEAL achieves\n40-50% model-wide sparsity with minimal performance degradation across Llama-2,\nLlama-3, and Mistral families, with sizes varying from 7B to 70B. We improve\nexisting sparse kernels and demonstrate wall-clock decoding speed-ups of up to\n1.53$\\times$ and 1.8$\\times$ at 40% and 50% model-wide sparsity. TEAL is\ncompatible with weight quantization, enabling further efficiency gains.\n","versions":"[{'version': 'v1', 'created': 'Mon, 26 Aug 2024 23:30:15 GMT'}, {'version': 'v2', 'created': 'Fri, 11 Oct 2024 20:02:15 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Feb 2025 21:00:50 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Liu', 'James', ''], ['Ponnusamy', 'Pragaash', ''], ['Cai', 'Tianle', ''], ['Guo', 'Han', ''], ['Kim', 'Yoon', ''], ['Athiwaratkun', 'Ben', '']]","extracted_entities":"[{'text': 'TEAL', 'label': 'LLM-based'}, {'text': 'Llama-3', 'label': 'GPT-3'}, {'text': 'Mistral', 'label': 'Mistral'}, {'text': 'weight quantization', 'label': 'quantisation'}]","assigned_concept":"Mistral","matched_keyword":"Mistral","similarity_score":1.0}
{"id":1804.10646,"submitter":"Ben Webster","authors":"Michael McBreen and Ben Webster","title":"Homological Mirror Symmetry for Hypertoric Varieties I","comments":"41 pages. v4: Final published version","journal-ref":"Geom. Topol. 28 (2024) 1005-1063","doi":"10.2140\/gt.2024.28.1005","report-no":null,"categories":"math.AG math.RT math.SG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We consider homological mirror symmetry in the context of hypertoric\nvarieties, showing that appropriate categories of B-branes (that is, coherent\nsheaves) on an additive hypertoric variety match a category of A-branes on a\nDolbeault hypertoric manifold for the same underlying combinatorial data. For\ntechnical reasons, the category of A-branes we consider is the modules over a\ndeformation quantization (that is, DQ-modules). We consider objects in this\ncategory equipped with an analogue of a Hodge structure, which corresponds to a\n$\\mathbb{G}_m$-action on the dual side of the mirror symmetry.\n  This result is based on hands-on calculations in both categories. We analyze\ncoherent sheaves by constructing a tilting generator, using the characteristic\n$p$ approach of Kaledin; the result is a sum of line bundles, which can be\ndescribed using a simple combinatorial rule. The endomorphism algebra $H$ of\nthis tilting generator has a simple quadratic presentation in the grading\ninduced by $\\mathbb{G}_m$-equivariance. In fact, we can confirm it is Koszul,\nand compute its Koszul dual $H^!$.\n  We then show that this same algebra appears as an Ext-algebra of simple\nA-branes in a Dolbeault hypertoric manifold. The $\\mathbb{G}_m$-equivariant\ngrading on coherent sheaves matches a Hodge grading in this category.\n","versions":"[{'version': 'v1', 'created': 'Fri, 27 Apr 2018 18:52:14 GMT'}, {'version': 'v2', 'created': 'Fri, 19 Oct 2018 19:19:20 GMT'}, {'version': 'v3', 'created': 'Sat, 2 Oct 2021 03:11:14 GMT'}, {'version': 'v4', 'created': 'Thu, 27 Feb 2025 15:44:11 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['McBreen', 'Michael', ''], ['Webster', 'Ben', '']]","extracted_entities":"[{'text': 'deformation quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"deformation quantization","similarity_score":0.6073967218}
{"id":2212.05948,"submitter":"Farhad Shirani Chaharsooghi","authors":"Marian Temprana Alonso, Xuyang Liu, Hamidreza Aghasi, Farhad Shirani","title":"Non-Linear Analog Processing in MIMO Systems with Coarse Quantization","comments":"arXiv admin note: substantial text overlap with arXiv:2208.04450","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IT cs.SY eess.SP eess.SY math.IT","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Analog to digital converters (ADCs) are a major contributor to the power\nconsumption of multiple-input multiple-output (MIMO) receivers in large\nbandwidth millimeter-wave systems. Prior works have considered two mitigating\nsolutions to reduce the ADC power consumption: i) decreasing the number of ADCs\nvia analog and hybrid beamforming, and ii) decreasing the ADC resolution, i.e.,\nutilizing one-bit and few-bit ADCs. These mitigating solutions lead to\nperformance loss in terms of achievable rates due to increased quantization\nerror. In this work, the use of nonlinear analog operators such as envelope\ndetectors and polynomial operators, prior to sampling and quantization is\nconsidered, as a way to reduce the aforementioned rate-loss. The receiver\narchitecture consists of linear combiners, nonlinear analog operators, and\nfew-bit ADCs. The fundamental performance limits of the resulting communication\nsystem, in terms of achievable rates, are investigated under various\nassumptions on the set of implementable analog operators. Extensive numerical\nevaluations are provided to evaluate the set of achievable rates and the power\nconsumption of the proposed receiver architectures. Circuit simulations and\nmeasurement results, based on both 22 nm FDSOI CMOS technology and 65 nm Bulk\nCMOS transistor technologies, are provided to justify the power efficiency of\nthe proposed receiver architectures.\n","versions":"[{'version': 'v1', 'created': 'Mon, 12 Dec 2022 15:04:43 GMT'}, {'version': 'v2', 'created': 'Fri, 15 Mar 2024 19:47:37 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 02:50:39 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Alonso', 'Marian Temprana', ''], ['Liu', 'Xuyang', ''], ['Aghasi', 'Hamidreza', ''], ['Shirani', 'Farhad', '']]","extracted_entities":"[{'text': 'quantization\\nerror', 'label': 'quantisation'}, {'text': 'quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization","similarity_score":0.8134455085}
{"id":2303.14731,"submitter":"Mrinal Kanti Roychowdhury","authors":"Amit Priyadarshi, Mrinal K. Roychowdhury, Manuj Verma","title":"Quantization dimensions for inhomogeneous bi-Lipschitz Iterated Function\n  Systems","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Let $\\nu$ be a Borel probability measure on a $d$-dimensional Euclidean space\n$\\mathbb{R}^d$, $d\\geq 1$, with a compact support, and let $(p_0, p_1, p_2,\n\\ldots, p_N)$ be a probability vector with $p_j>0$ for $0\\leq j\\leq N$. Let\n$\\{S_j: 1\\leq j\\leq N\\}$ be a set of contractive mappings on $\\mathbb{R}^d$.\nThen, a Borel probability measure $\\mu$ on $\\mathbb R^d$ such that\n$\\mu=\\sum_{j=1}^N p_j\\mu\\circ S_j^{-1}+p_0\\nu$ is called an inhomogeneous\nmeasure, also known as a condensation measure on $\\mathbb{R}^d$. For a given\n$r\\in (0, +\\infty)$, the quantization dimension of order $r$, if it exists,\ndenoted by $D_r(\\mu)$, of a Borel probability measure $\\mu$ on $\\mathbb{R}^d$\nrepresents the speed at which the $n$th quantization error of order $r$\napproaches to zero as the number of elements $n$ in an optimal set of $n$-means\nfor $\\mu$ tends to infinity. In this paper, we investigate the quantization\ndimension for such a condensation measure.\n","versions":"[{'version': 'v1', 'created': 'Sun, 26 Mar 2023 14:22:15 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 18:46:48 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Priyadarshi', 'Amit', ''], ['Roychowdhury', 'Mrinal K.', ''], ['Verma', 'Manuj', '']]","extracted_entities":"[{'text': 'quantization dimension', 'label': 'quantisation'}, {'text': 'quantization\\ndimension', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization dimension","similarity_score":0.63712883}
{"id":2307.08423,"submitter":"Xuan Zhang","authors":"Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen\n  Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, Keir Adams, Maurice Weiler,\n  Xiner Li, Tianfan Fu, Yucheng Wang, Alex Strasser, Haiyang Yu, YuQing Xie,\n  Xiang Fu, Shenglong Xu, Yi Liu, Yuanqi Du, Alexandra Saxton, Hongyi Ling,\n  Hannah Lawrence, Hannes St\\\"ark, Shurui Gui, Carl Edwards, Nicholas Gao,\n  Adriana Ladera, Tailin Wu, Elyssa F. Hofgard, Aria Mansouri Tehrani, Rui\n  Wang, Ameya Daigavane, Montgomery Bohde, Jerry Kurtin, Qian Huang, Tuong\n  Phung, Minkai Xu, Chaitanya K. Joshi, Simon V. Mathis, Kamyar\n  Azizzadenesheli, Ada Fang, Al\\'an Aspuru-Guzik, Erik Bekkers, Michael\n  Bronstein, Marinka Zitnik, Anima Anandkumar, Stefano Ermon, Pietro Li\\`o,\n  Rose Yu, Stephan G\\\"unnemann, Jure Leskovec, Heng Ji, Jimeng Sun, Regina\n  Barzilay, Tommi Jaakkola, Connor W. Coley, Xiaoning Qian, Xiaofeng Qian, Tess\n  Smidt, Shuiwang Ji","title":"Artificial Intelligence for Science in Quantum, Atomistic, and Continuum\n  Systems","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG physics.comp-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Advances in artificial intelligence (AI) are fueling a new paradigm of\ndiscoveries in natural sciences. Today, AI has started to advance natural\nsciences by improving, accelerating, and enabling our understanding of natural\nphenomena at a wide range of spatial and temporal scales, giving rise to a new\narea of research known as AI for science (AI4Science). Being an emerging\nresearch paradigm, AI4Science is unique in that it is an enormous and highly\ninterdisciplinary area. Thus, a unified and technical treatment of this field\nis needed yet challenging. This work aims to provide a technically thorough\naccount of a subarea of AI4Science; namely, AI for quantum, atomistic, and\ncontinuum systems. These areas aim at understanding the physical world from the\nsubatomic (wavefunctions and electron density), atomic (molecules, proteins,\nmaterials, and interactions), to macro (fluids, climate, and subsurface) scales\nand form an important subarea of AI4Science. A unique advantage of focusing on\nthese areas is that they largely share a common set of challenges, thereby\nallowing a unified and foundational treatment. A key common challenge is how to\ncapture physics first principles, especially symmetries, in natural systems by\ndeep learning methods. We provide an in-depth yet intuitive account of\ntechniques to achieve equivariance to symmetry transformations. We also discuss\nother common technical challenges, including explainability,\nout-of-distribution generalization, knowledge transfer with foundation and\nlarge language models, and uncertainty quantification. To facilitate learning\nand education, we provide categorized lists of resources that we found to be\nuseful. We strive to be thorough and unified and hope this initial effort may\ntrigger more community interests and efforts to further advance AI4Science.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Jul 2023 12:14:14 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Nov 2023 18:25:03 GMT'}, {'version': 'v3', 'created': 'Sun, 13 Oct 2024 15:56:41 GMT'}, {'version': 'v4', 'created': 'Wed, 26 Feb 2025 18:45:58 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Zhang', 'Xuan', ''], ['Wang', 'Limei', ''], ['Helwig', 'Jacob', ''], ['Luo', 'Youzhi', ''], ['Fu', 'Cong', ''], ['Xie', 'Yaochen', ''], ['Liu', 'Meng', ''], ['Lin', 'Yuchao', ''], ['Xu', 'Zhao', ''], ['Yan', 'Keqiang', ''], ['Adams', 'Keir', ''], ['Weiler', 'Maurice', ''], ['Li', 'Xiner', ''], ['Fu', 'Tianfan', ''], ['Wang', 'Yucheng', ''], ['Strasser', 'Alex', ''], ['Yu', 'Haiyang', ''], ['Xie', 'YuQing', ''], ['Fu', 'Xiang', ''], ['Xu', 'Shenglong', ''], ['Liu', 'Yi', ''], ['Du', 'Yuanqi', ''], ['Saxton', 'Alexandra', ''], ['Ling', 'Hongyi', ''], ['Lawrence', 'Hannah', ''], ['St\u00e4rk', 'Hannes', ''], ['Gui', 'Shurui', ''], ['Edwards', 'Carl', ''], ['Gao', 'Nicholas', ''], ['Ladera', 'Adriana', ''], ['Wu', 'Tailin', ''], ['Hofgard', 'Elyssa F.', ''], ['Tehrani', 'Aria Mansouri', ''], ['Wang', 'Rui', ''], ['Daigavane', 'Ameya', ''], ['Bohde', 'Montgomery', ''], ['Kurtin', 'Jerry', ''], ['Huang', 'Qian', ''], ['Phung', 'Tuong', ''], ['Xu', 'Minkai', ''], ['Joshi', 'Chaitanya K.', ''], ['Mathis', 'Simon V.', ''], ['Azizzadenesheli', 'Kamyar', ''], ['Fang', 'Ada', ''], ['Aspuru-Guzik', 'Al\u00e1n', ''], ['Bekkers', 'Erik', ''], ['Bronstein', 'Michael', ''], ['Zitnik', 'Marinka', ''], ['Anandkumar', 'Anima', ''], ['Ermon', 'Stefano', ''], ['Li\u00f2', 'Pietro', ''], ['Yu', 'Rose', ''], ['G\u00fcnnemann', 'Stephan', ''], ['Leskovec', 'Jure', ''], ['Ji', 'Heng', ''], ['Sun', 'Jimeng', ''], ['Barzilay', 'Regina', ''], ['Jaakkola', 'Tommi', ''], ['Coley', 'Connor W.', ''], ['Qian', 'Xiaoning', ''], ['Qian', 'Xiaofeng', ''], ['Smidt', 'Tess', ''], ['Ji', 'Shuiwang', '']]","extracted_entities":"[{'text': 'uncertainty quantification', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"uncertainty quantification","similarity_score":0.5714546442}
{"id":2312.0795,"submitter":"Xiaoyu Liu","authors":"Xin Ding, Xiaoyu Liu, Zhijun Tu, Yun Zhang, Wei Li, Jie Hu, Hanting\n  Chen, Yehui Tang, Zhiwei Xiong, Baoqun Yin, Yunhe Wang","title":"CBQ: Cross-Block Quantization for Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Post-training quantization (PTQ) has played a key role in compressing large\nlanguage models (LLMs) with ultra-low costs. However, existing PTQ methods only\nfocus on handling the outliers within one layer or one block, which ignores the\ndependency of blocks and leads to severe performance degradation in low-bit\nsettings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ\nmethod for LLMs. CBQ employs a cross-block dependency using a homologous\nreconstruction scheme, establishing long-range dependencies across multiple\nblocks to minimize error accumulation. Furthermore, CBQ incorporates a\ncoarse-to-fine preprocessing (CFP) strategy for suppressing weight and\nactivation outliers, coupled with an adaptive LoRA-Rounding technique for\nprecise weight quantization. These innovations enable CBQ to not only handle\nextreme outliers effectively but also improve overall quantization accuracy.\nExtensive experiments show that CBQ achieves superior low-bit quantization\n(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across\nvarious LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model\nwithin only 4.3 hours on a single GPU, achieving a commendable tradeoff between\nperformance and quantization efficiency.\n","versions":"[{'version': 'v1', 'created': 'Wed, 13 Dec 2023 07:56:27 GMT'}, {'version': 'v2', 'created': 'Fri, 2 Feb 2024 06:55:52 GMT'}, {'version': 'v3', 'created': 'Wed, 27 Mar 2024 04:51:51 GMT'}, {'version': 'v4', 'created': 'Mon, 15 Apr 2024 10:57:16 GMT'}, {'version': 'v5', 'created': 'Tue, 25 Feb 2025 09:14:18 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Ding', 'Xin', ''], ['Liu', 'Xiaoyu', ''], ['Tu', 'Zhijun', ''], ['Zhang', 'Yun', ''], ['Li', 'Wei', ''], ['Hu', 'Jie', ''], ['Chen', 'Hanting', ''], ['Tang', 'Yehui', ''], ['Xiong', 'Zhiwei', ''], ['Yin', 'Baoqun', ''], ['Wang', 'Yunhe', '']]","extracted_entities":"[{'text': 'Post-training quantization', 'label': 'quantisation'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'precise weight quantization', 'label': 'quantisation'}, {'text': 'low-bit quantization', 'label': 'quantisation'}, {'text': 'W4A4', 'label': 'GPT'}, {'text': 'W4A8', 'label': 'GPT'}, {'text': 'W2A16', 'label': 'GPT'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"quantisation","matched_keyword":"Post-training quantization","similarity_score":0.6493542194}
{"id":2402.02593,"submitter":"Vivswan Shah","authors":"Vivswan Shah and Nathan Youngblood","title":"Leveraging Continuously Differentiable Activation Functions for Learning\n  in Quantized Noisy Environments","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Real-world analog systems intrinsically suffer from noise that can impede\nmodel convergence and accuracy on a variety of deep learning models. We\ndemonstrate that differentiable activations like GELU and SiLU enable robust\npropagation of gradients which help to mitigate analog quantization error that\nis ubiquitous to all analog systems. We perform analysis and training of\nconvolutional, linear, and transformer networks in the presence of quantized\nnoise. Here, we are able to demonstrate that continuously differentiable\nactivation functions are significantly more noise resilient over conventional\nrectified activations. As in the case of ReLU, the error in gradients are 100x\nhigher than those in GELU near zero. Our findings provide guidance for\nselecting appropriate activations to realize performant and reliable hardware\nimplementations across several machine learning domains such as computer\nvision, signal processing, and beyond. Code available at:\n\\href{https:\/\/github.com\/Vivswan\/GeLUReLUInterpolation}{https:\/\/github.com\/Vivswan\/GeLUReLUInterpolation}.}\n","versions":"[{'version': 'v1', 'created': 'Sun, 4 Feb 2024 20:01:22 GMT'}, {'version': 'v2', 'created': 'Mon, 27 Jan 2025 21:15:38 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 09:50:27 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Shah', 'Vivswan', ''], ['Youngblood', 'Nathan', '']]","extracted_entities":"[{'text': 'analog quantization error', 'label': 'quantisation'}, {'text': 'quantized\\nnoise', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantized\nnoise","similarity_score":0.5843287706}
{"id":2404.05368,"submitter":"Vojtech Mrazek","authors":"Jan Klhufek, Miroslav Safar, Vojtech Mrazek, Zdenek Vasicek, Lukas\n  Sekanina","title":"Exploring Quantization and Mapping Synergy in Hardware-Aware Deep Neural\n  Network Accelerators","comments":"To appear at the 2024 27th International Symposium on Design &\n  Diagnostics of Electronic Circuits & Systems (DDECS)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AR cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Energy efficiency and memory footprint of a convolutional neural network\n(CNN) implemented on a CNN inference accelerator depend on many factors,\nincluding a weight quantization strategy (i.e., data types and bit-widths) and\nmapping (i.e., placement and scheduling of DNN elementary operations on\nhardware units of the accelerator). We show that enabling rich mixed\nquantization schemes during the implementation can open a previously hidden\nspace of mappings that utilize the hardware resources more effectively. CNNs\nutilizing quantized weights and activations and suitable mappings can\nsignificantly improve trade-offs among the accuracy, energy, and memory\nrequirements compared to less carefully optimized CNN implementations. To find,\nanalyze, and exploit these mappings, we: (i) extend a general-purpose\nstate-of-the-art mapping tool (Timeloop) to support mixed quantization, which\nis not currently available; (ii) propose an efficient multi-objective\noptimization algorithm to find the most suitable bit-widths and mapping for\neach DNN layer executed on the accelerator; and (iii) conduct a detailed\nexperimental evaluation to validate the proposed method. On two CNNs\n(MobileNetV1 and MobileNetV2) and two accelerators (Eyeriss and Simba) we show\nthat for a given quality metric (such as the accuracy on ImageNet), energy\nsavings are up to 37% without any accuracy drop.\n","versions":"[{'version': 'v1', 'created': 'Mon, 8 Apr 2024 10:10:30 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 13:11:04 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Klhufek', 'Jan', ''], ['Safar', 'Miroslav', ''], ['Mrazek', 'Vojtech', ''], ['Vasicek', 'Zdenek', ''], ['Sekanina', 'Lukas', '']]","extracted_entities":"[{'text': 'weight quantization strategy', 'label': 'quantisation'}, {'text': 'mixed quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"mixed quantization","similarity_score":0.6759530306}
{"id":2405.11563,"submitter":"Kwangjae Lee","authors":"Kwangjae Lee, Jung Hoon Lee, and Wan Choi","title":"User-Centric Association and Feedback Bit Allocation for FDD Cell-Free\n  Massive MIMO","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IT math.IT","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper, we introduce a novel approach to user-centric association and\nfeedback bit allocation for the downlink of a cell-free massive MIMO (CF-mMIMO)\nsystem, operating under limited feedback constraints. In CF-mMIMO systems\nemploying frequency division duplexing, each access point (AP) relies on\nchannel information provided by its associated user equipments (UEs) for\nbeamforming design. Since the uplink control channel is typically shared among\nUEs, we take account of each AP's total feedback budget, which is distributed\namong its associated UEs. By employing the Saleh-Valenzuela multi-resolvable\npath channel model with different average path gains, we first identify\nnecessary feedback information for each UE, along with an appropriate codebook\nstructure. This structure facilitates adaptive quantization of multiple paths\nbased on their dominance. We then formulate a joint optimization problem\naddressing user-centric UE-AP association and feedback bit allocation. To\naddress this challenge, we analyze the impact of feedback bit allocation and\nderive our proposed scheme from the solution of an alternative optimization\nproblem aimed at devising long-term policies, explicitly considering the\neffects of feedback bit allocation. Numerical results show that our proposed\nscheme effectively enhances the performance of conventional approaches in\nCF-mMIMO systems.\n","versions":"[{'version': 'v1', 'created': 'Sun, 19 May 2024 14:29:02 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 02:30:09 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Lee', 'Kwangjae', ''], ['Lee', 'Jung Hoon', ''], ['Choi', 'Wan', '']]","extracted_entities":"[{'text': 'Saleh-Valenzuela multi-resolvable\\npath channel model', 'label': 'AI model'}, {'text': 'adaptive quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"adaptive quantization","similarity_score":0.6232469082}
{"id":2406.08155,"submitter":"Pingzhi Li","authors":"Pingzhi Li, Xiaolong Jin, Zhen Tan, Yu Cheng, Tianlong Chen","title":"QuantMoE-Bench: Examining Post-Training Quantization for\n  Mixture-of-Experts","comments":"Our code for reproducing all our experiments is provided at\n  https:\/\/github.com\/UNITES-Lab\/moe-quantization","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Mixture-of-Experts (MoE) is a promising way to scale up the learning capacity\nof large language models. It increases the number of parameters while keeping\nFLOPs nearly constant during inference through sparse activation. Yet, it still\nsuffers from significant memory overheads due to the vast parameter size,\nnecessitating model compression techniques. Post-training quantization offers a\npowerful approach for model compression. Existing methods adopt a fixed\nquantization precision for the entire MoE model. This rigid setup can lead to\nsuboptimal performance, without considering the inherent sparse structure. For\nexample, MoE's sparse routing mechanism leads to different activation patterns,\nwhere shared experts are accessed by all tokens while token-conditioned experts\nare selectively activated. This activation disparity suggests different\nquantization requirements, with consistently activated shared experts\npotentially needing higher precision to maintain model quality. In this paper,\nwe study a fine-grained precision setup for MoE quantization. We explore MoE\nstructure-aware quantization heuristics, ranging from coarse (e.g., MoE layers)\nto fine granularity (e.g., linear layers). Our investigations reveal critical\nprinciples, where different MoE structures require varying numbers of bits for\neffective quantization. Conclusions are supported by extensive benchmarking\nacross two representative MoE models and six tasks including commonsense\nreasoning and natural language understanding. We further show that an MoE\nquantized in a fined-grained mixed precision achieved state-of-the-art 65.35%\nperformance on average compared to the baseline 64.30% (i.e., GPTQ). Moreover,\nbased on the findings, we introduce novel data-driven techniques for optimizing\nbit allocation in MoE quantization, including the outlier-aware linear layer\nscorer and MoE block importance predictor.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Jun 2024 12:44:48 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 18:29:54 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Li', 'Pingzhi', ''], ['Jin', 'Xiaolong', ''], ['Tan', 'Zhen', ''], ['Cheng', 'Yu', ''], ['Chen', 'Tianlong', '']]","extracted_entities":"[{'text': 'model compression techniques', 'label': 'quantisation'}, {'text': 'Post-training quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"Post-training quantization","similarity_score":0.6493542194}
{"id":2406.13489,"submitter":"Daniele Proverbio","authors":"Uros Sutulovic, Daniele Proverbio, Rami Katz, Giulia Giordano","title":"Efficient gPC-based quantification of probabilistic robustness for\n  systems in neuroscience","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"q-bio.QM","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Robustness analysis is very important in biology and neuroscience, to unravel\nbehavioral patterns of systems that are conserved despite large parametric\nuncertainties. To make studies of probabilistic robustness more efficient and\nscalable in addressing complex neuroscience models, we propose an alternative\nto computationally expensive Monte Carlo (MC) methods by introducing and\nanalysing the generalised polynomial chaos (gPC) framework for uncertainty\nquantification. We consider both intrusive and non-intrusive gPC approaches,\nwhich turn out to be scalable and allow for a fast comprehensive exploration of\nparameter spaces. Focusing on widely used models of neural dynamics as case\nstudies, we explore the trade-off between efficiency and accuracy of gPC\nmethods, and we select effective computational settings to investigate\nparametric uncertainties in models that feature multiple dynamic regimes.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Jun 2024 12:19:03 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 12:29:10 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Sutulovic', 'Uros', ''], ['Proverbio', 'Daniele', ''], ['Katz', 'Rami', ''], ['Giordano', 'Giulia', '']]","extracted_entities":"[{'text': 'uncertainty\\nquantification', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"uncertainty\nquantification","similarity_score":0.5714546442}
{"id":2407.03312,"submitter":"Maike Holthuijzen","authors":"Maike F. Holthuijzen, Robert B. Gramacy, Cayelan C. Carey, Dave M.\n  Higdon, R. Quinn Thomas","title":"Synthesizing data products, mathematical models, and observational\n  measurements for lake temperature forecasting","comments":"20 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.AP","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  We present a novel forecasting framework for lake water temperature, which is\ncrucial for managing lake ecosystems and drinking water resources. The General\nLake Model (GLM) has been previously used for this purpose, but, similar to\nmany process-based simulation models, it: requires a large number of inputs,\nmany of which are stochastic; presents challenges for uncertainty\nquantification (UQ); and can exhibit model bias. To address these issues, we\npropose a Gaussian process (GP) surrogate-based forecasting approach that\nefficiently handles large, high-dimensional data and accounts for\ninput-dependent variability and systematic GLM bias. We validate the proposed\napproach and compare it with other forecasting methods, including a\nclimatological model and raw GLM simulations. Our results demonstrate that our\nbias-corrected GP surrogate (GPBC) can outperform competing approaches in terms\nof forecast accuracy and UQ up to two weeks into the future.\n","versions":"[{'version': 'v1', 'created': 'Wed, 3 Jul 2024 17:54:57 GMT'}, {'version': 'v2', 'created': 'Mon, 9 Dec 2024 17:26:55 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Feb 2025 15:58:48 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Holthuijzen', 'Maike F.', ''], ['Gramacy', 'Robert B.', ''], ['Carey', 'Cayelan C.', ''], ['Higdon', 'Dave M.', ''], ['Thomas', 'R. Quinn', '']]","extracted_entities":"[{'text': 'uncertainty\\nquantification (UQ)', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"uncertainty\nquantification (UQ)","similarity_score":0.5492583513}
{"id":2407.17329,"submitter":"Erell Gachon","authors":"Erell Gachon, J\\'er\\'emie Bigot, Elsa Cazelles, Audrey Bidet,\n  Jean-Philippe Vial, Pierre-Yves Dumas, Aguirre Mimoun","title":"Low dimensional representation of multi-patient flow cytometry datasets\n  using optimal transport for minimal residual disease detection in leukemia","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG math.ST stat.ME stat.TH","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Representing and quantifying Minimal Residual Disease (MRD) in Acute Myeloid\nLeukemia (AML), a type of cancer that affects the blood and bone marrow, is\nessential in the prognosis and follow-up of AML patients. As traditional\ncytological analysis cannot detect leukemia cells below 5\\%, the analysis of\nflow cytometry dataset is expected to provide more reliable results. In this\npaper, we explore statistical learning methods based on optimal transport (OT)\nto achieve a relevant low-dimensional representation of multi-patient flow\ncytometry measurements (FCM) datasets considered as high-dimensional\nprobability distributions. Using the framework of OT, we justify the use of the\nK-means algorithm for dimensionality reduction of multiple large-scale point\nclouds through mean measure quantization by merging all the data into a single\npoint cloud. After this quantization step, the visualization of the intra and\ninter-patients FCM variability is carried out by embedding low-dimensional\nquantized probability measures into a linear space using either Wasserstein\nPrincipal Component Analysis (PCA) through linearized OT or log-ratio PCA of\ncompositional data. Using a publicly available FCM dataset and a FCM dataset\nfrom Bordeaux University Hospital, we demonstrate the benefits of our approach\nover the popular kernel mean embedding technique for statistical learning from\nmultiple high-dimensional probability distributions. We also highlight the\nusefulness of our methodology for low-dimensional projection and clustering\npatient measurements according to their level of MRD in AML from FCM. In\nparticular, our OT-based approach allows a relevant and informative\ntwo-dimensional representation of the results of the FlowSom algorithm, a\nstate-of-the-art method for the detection of MRD in AML using multi-patient\nFCM.\n","versions":"[{'version': 'v1', 'created': 'Wed, 24 Jul 2024 14:53:01 GMT'}, {'version': 'v2', 'created': 'Mon, 23 Sep 2024 08:09:01 GMT'}, {'version': 'v3', 'created': 'Wed, 26 Feb 2025 08:11:39 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Gachon', 'Erell', ''], ['Bigot', 'J\u00e9r\u00e9mie', ''], ['Cazelles', 'Elsa', ''], ['Bidet', 'Audrey', ''], ['Vial', 'Jean-Philippe', ''], ['Dumas', 'Pierre-Yves', ''], ['Mimoun', 'Aguirre', '']]","extracted_entities":"[{'text': 'mean measure quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"mean measure quantization","similarity_score":0.5048162937}
{"id":2407.20827,"submitter":"Nicolas Fabre","authors":"Thomas Pousset, Maxime Federico, Romain All\\'eaume and Nicolas Fabre","title":"Kramers-Kronig detection in the quantum regime","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We investigate the quantization of Kramers-Kronig detection technique\ninitially developped for classical optical communications. It consists in\nmixing the unknown field with a strong monochromatic local oscillator on an\nunbalanced beamsplitter. A single output of the beamsplitter undergoes a direct\ndetection of the optical intensity by means of a single photodiode. When the\nmeasured output verifies signal processing constraints, namely, the minimal\nphase and the single sideband constraints, Kramers-Kronig detection\nreconstructs the phase of the signal from the intensity measurements via a\ndigitally computed Hilbert transform. The local oscillator being known,\nKramers-Kronig detection allows for reconstructing the quadratures of the\nunknown field. We show that this result holds in the quantum regime up to first\norder in the local oscillator amplitude and thus that Kramers-Kronig detection\nacts as a coherent detection able to measure both quadratures, making it a\nGaussian measurement similar to double homodyne detection. We also study in\ndetails the phase information measured by Kramers-Kronig detection for bosonic\ncoherent states, monomode pure states and mixed states. Finally, we propose and\ninvestigate a spectral tomography protocol for single-photon states that is\ninspired by Kramers-Kronig detection and relies on a spectral engineering of\nthe single-photon.\n","versions":"[{'version': 'v1', 'created': 'Tue, 30 Jul 2024 13:47:31 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 12:56:35 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Pousset', 'Thomas', ''], ['Federico', 'Maxime', ''], ['All\u00e9aume', 'Romain', ''], ['Fabre', 'Nicolas', '']]","extracted_entities":"[{'text': 'quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization","similarity_score":0.8134455085}
{"id":2408.00391,"submitter":"Alexander Schenkel","authors":"Cameron Kemp, Robert Laugwitz, Alexander Schenkel","title":"Infinitesimal 2-braidings from 2-shifted Poisson structures","comments":"v2: 39 pages. Final version accepted for publication in Journal of\n  Geometry and Physics","journal-ref":null,"doi":"10.1016\/j.geomphys.2025.105456","report-no":null,"categories":"math.QA math-ph math.AG math.MP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  It is shown that every $2$-shifted Poisson structure on a finitely generated\nsemi-free commutative differential graded algebra $A$ defines a very explicit\ninfinitesimal $2$-braiding on the homotopy $2$-category of the symmetric\nmonoidal dg-category of finitely generated semi-free $A$-dg-modules. This\nprovides a concrete realization, to first order in the deformation parameter\n$\\hbar$, of the abstract deformation quantization results in derived algebraic\ngeometry due to Calaque, Pantev, To\\\"en, Vaqui\\'e and Vezzosi. Of particular\ninterest is the case when $A$ is the Chevalley-Eilenberg algebra of a Lie\n$N$-algebra, where the braided monoidal deformations developed in this paper\nmay be interpreted as candidates for representation categories of `higher\nquantum groups'.\n","versions":"[{'version': 'v1', 'created': 'Thu, 1 Aug 2024 08:59:34 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 09:21:58 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Kemp', 'Cameron', ''], ['Laugwitz', 'Robert', ''], ['Schenkel', 'Alexander', '']]","extracted_entities":"[{'text': 'deformation quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"deformation quantization","similarity_score":0.6073967218}
{"id":2301.00389,"submitter":"Zhi Yuan Wu","authors":"Zhiyuan Wu, Sheng Sun, Yuwei Wang, Min Liu, Quyang Pan, Xuefeng Jiang,\n  Bo Gao","title":"FedICT: Federated Multi-task Distillation for Multi-access Edge\n  Computing","comments":"Accepted by IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS","journal-ref":null,"doi":"10.1109\/TPDS.2023.3289444","report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The growing interest in intelligent services and privacy protection for\nmobile devices has given rise to the widespread application of federated\nlearning in Multi-access Edge Computing (MEC). Diverse user behaviors call for\npersonalized services with heterogeneous Machine Learning (ML) models on\ndifferent devices. Federated Multi-task Learning (FMTL) is proposed to train\nrelated but personalized ML models for different devices, whereas previous\nworks suffer from excessive communication overhead during training and neglect\nthe model heterogeneity among devices in MEC. Introducing knowledge\ndistillation into FMTL can simultaneously enable efficient communication and\nmodel heterogeneity among clients, whereas existing methods rely on a public\ndataset, which is impractical in reality. To tackle this dilemma, Federated\nMultI-task Distillation for Multi-access Edge CompuTing (FedICT) is proposed.\nFedICT direct local-global knowledge aloof during bi-directional distillation\nprocesses between clients and the server, aiming to enable multi-task clients\nwhile alleviating client drift derived from divergent optimization directions\nof client-side local models. Specifically, FedICT includes Federated Prior\nKnowledge Distillation (FPKD) and Local Knowledge Adjustment (LKA). FPKD is\nproposed to reinforce the clients' fitting of local data by introducing prior\nknowledge of local data distributions. Moreover, LKA is proposed to correct the\ndistillation loss of the server, making the transferred local knowledge better\nmatch the generalized representation. Experiments on three datasets show that\nFedICT significantly outperforms all compared benchmarks in various data\nheterogeneous and model architecture settings, achieving improved accuracy with\nless than 1.2% training communication overhead compared with FedAvg and no more\nthan 75% training communication round compared with FedGKT.\n","versions":"[{'version': 'v1', 'created': 'Sun, 1 Jan 2023 11:50:58 GMT'}, {'version': 'v2', 'created': 'Tue, 15 Aug 2023 14:33:46 GMT'}, {'version': 'v3', 'created': 'Thu, 27 Feb 2025 08:43:49 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Wu', 'Zhiyuan', ''], ['Sun', 'Sheng', ''], ['Wang', 'Yuwei', ''], ['Liu', 'Min', ''], ['Pan', 'Quyang', ''], ['Jiang', 'Xuefeng', ''], ['Gao', 'Bo', '']]","extracted_entities":"[{'text': 'Federated Multi-task Learning', 'label': 'Few-shot Learning'}, {'text': 'FMTL', 'label': 'Few-shot Learning'}, {'text': 'Federated\\nMultI-task Distillation', 'label': 'Knowledge distillation'}, {'text': 'Federated Prior\\nKnowledge Distillation', 'label': 'Knowledge distillation'}, {'text': 'FPKD', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Federated Prior\nKnowledge Distillation","similarity_score":0.7949930429}
{"id":2308.01134,"submitter":"Farzin Salek","authors":"Farzin Salek, Andreas Winter","title":"New Protocols for Conference Key and Multipartite Entanglement\n  Distillation","comments":"Final version accepted with journal","journal-ref":null,"doi":"10.1109\/TIT.2025.3546794","report-no":null,"categories":"quant-ph cs.IT math.IT","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We approach two interconnected problems of quantum information processing in\nnetworks: Conference key agreement and entanglement distillation, both in the\nso-called source model where the given resource is a multipartite quantum state\nand the players interact over public classical channels to generate the desired\ncorrelation. The first problem is the distillation of a conference key when the\nsource state is shared between a number of legal players and an eavesdropper;\nthe eavesdropper, apart from starting off with this quantum side information,\nalso observes the public communication between the players. The second is the\ndistillation of Greenberger-Horne-Zeilinger (GHZ) states by means of local\noperations and classical communication (LOCC) from the given mixed state. These\nproblem settings extend our previous paper [IEEE Trans. Inf. Theory\n68(2):976-988, 2022], and we generalise its results: using a quantum version of\nthe task of communication for omniscience, we derive novel lower bounds on the\ndistillable conference key from any multipartite quantum state by means of\nnon-interacting communication protocols. Secondly, we establish novel lower\nbounds on the yield of GHZ states from multipartite mixed states. Namely, we\npresent two methods to produce bipartite entanglement between sufficiently many\nnodes so as to produce GHZ states. Next, we show that the conference key\nagreement protocol can be made coherent under certain conditions, enabling the\ndirect generation of multipartite GHZ states.\n","versions":"[{'version': 'v1', 'created': 'Wed, 2 Aug 2023 13:23:29 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 22:46:59 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Salek', 'Farzin', ''], ['Winter', 'Andreas', '']]","extracted_entities":"[{'text': 'entanglement distillation', 'label': 'Knowledge distillation'}, {'text': 'distillation', 'label': 'Knowledge distillation'}, {'text': 'bipartite entanglement', 'label': 'quantisation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"distillation","similarity_score":0.7657151222}
{"id":2312.17273,"submitter":"Zhaisheng Ding","authors":"Zhaisheng Ding, Haiyan Li, Ruichao Hou, Yanyu Liu and Shidong Xie","title":"X Modality Assisting RGBT Object Tracking","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Developing robust multi-modal feature representations is crucial for\nenhancing object tracking performance. In pursuit of this objective, a novel X\nModality Assisting Network (X-Net) is introduced, which explores the impact of\nthe fusion paradigm by decoupling visual object tracking into three distinct\nlevels, thereby facilitating subsequent processing. Initially, to overcome the\nchallenges associated with feature learning due to significant discrepancies\nbetween RGB and thermal modalities, a plug-and-play pixel-level generation\nmodule (PGM) based on knowledge distillation learning is proposed. This module\neffectively generates the X modality, bridging the gap between the two patterns\nwhile minimizing noise interference. Subsequently, to optimize sample feature\nrepresentation and promote cross-modal interactions, a feature-level\ninteraction module (FIM) is introduced, integrating a mixed feature interaction\ntransformer and a spatial dimensional feature translation strategy. Finally, to\naddress random drifting caused by missing instance features, a flexible online\noptimization strategy called the decision-level refinement module (DRM) is\nproposed, which incorporates optical flow and refinement mechanisms. The\nefficacy of X-Net is validated through experiments on three benchmarks,\ndemonstrating its superiority over state-of-the-art trackers. Notably, X-Net\nachieves performance gains of 0.47%\/1.2% in the average of precise rate and\nsuccess rate, respectively. Additionally, the research content, data, and code\nare pledged to be made publicly accessible at\nhttps:\/\/github.com\/DZSYUNNAN\/XNet.\n","versions":"[{'version': 'v1', 'created': 'Wed, 27 Dec 2023 05:38:54 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 15:06:13 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Ding', 'Zhaisheng', ''], ['Li', 'Haiyan', ''], ['Hou', 'Ruichao', ''], ['Liu', 'Yanyu', ''], ['Xie', 'Shidong', '']]","extracted_entities":"[{'text': 'knowledge distillation learning', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge distillation learning","similarity_score":0.9332133532}
{"id":2402.08159,"submitter":"Dennis Hein","authors":"Dennis Hein, Grant Stevens, Adam Wang, and Ge Wang","title":"PFCM: Poisson flow consistency models for low-dose CT image denoising","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  X-ray computed tomography (CT) is widely used for medical diagnosis and\ntreatment planning; however, concerns about ionizing radiation exposure drive\nefforts to optimize image quality at lower doses. This study introduces Poisson\nFlow Consistency Models (PFCM), a novel family of deep generative models that\ncombines the robustness of PFGM++ with the efficient single-step sampling of\nconsistency models. PFCM are derived by generalizing consistency distillation\nto PFGM++ through a change-of-variables and an updated noise distribution. As a\ndistilled version of PFGM++, PFCM inherit the ability to trade off robustness\nfor rigidity via the hyperparameter $D \\in (0,\\infty)$. A fact that we exploit\nto adapt this novel generative model for the task of low-dose CT image\ndenoising, via a ``task-specific'' sampler that ``hijacks'' the generative\nprocess by replacing an intermediate state with the low-dose CT image. While\nthis ``hijacking'' introduces a severe mismatch -- the noise characteristics of\nlow-dose CT images are different from that of intermediate states in the\nPoisson flow process -- we show that the inherent robustness of PFCM at small\n$D$ effectively mitigates this issue. The resulting sampler achieves excellent\nperformance in terms of LPIPS, SSIM, and PSNR on the Mayo low-dose CT dataset.\nBy contrast, an analogous sampler based on standard consistency models is found\nto be significantly less robust under the same conditions, highlighting the\nimportance of a tunable $D$ afforded by our novel framework. To highlight\ngeneralizability, we show effective denoising of clinical images from a\nprototype photon-counting system reconstructed using a sharper kernel and at a\nrange of energy levels.\n","versions":"[{'version': 'v1', 'created': 'Tue, 13 Feb 2024 01:39:56 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 12:57:19 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Hein', 'Dennis', ''], ['Stevens', 'Grant', ''], ['Wang', 'Adam', ''], ['Wang', 'Ge', '']]","extracted_entities":"[{'text': 'PFCM', 'label': 'AI model'}, {'text': 'consistency distillation', 'label': 'Knowledge distillation'}, {'text': 'PFCM', 'label': 'AI model'}]","assigned_concept":"Knowledge distillation","matched_keyword":"consistency distillation","similarity_score":0.6887969971}
{"id":2403.05061,"submitter":"Geonho Bang","authors":"Geonho Bang, Kwangjin Choi, Jisong Kim, Dongsuk Kum, Jun Won Choi","title":"RadarDistill: Boosting Radar-based Object Detection Performance via\n  Knowledge Distillation from LiDAR Features","comments":"Accepted to CVPR 2024. Code available at\n  https:\/\/github.com\/geonhobang\/RadarDistill","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The inherent noisy and sparse characteristics of radar data pose challenges\nin finding effective representations for 3D object detection. In this paper, we\npropose RadarDistill, a novel knowledge distillation (KD) method, which can\nimprove the representation of radar data by leveraging LiDAR data. RadarDistill\nsuccessfully transfers desirable characteristics of LiDAR features into radar\nfeatures using three key components: Cross-Modality Alignment (CMA),\nActivation-based Feature Distillation (AFD), and Proposal-based Feature\nDistillation (PFD). CMA enhances the density of radar features by employing\nmultiple layers of dilation operations, effectively addressing the challenge of\ninefficient knowledge transfer from LiDAR to radar. AFD selectively transfers\nknowledge based on regions of the LiDAR features, with a specific focus on\nareas where activation intensity exceeds a predefined threshold. PFD similarly\nguides the radar network to selectively mimic features from the LiDAR network\nwithin the object proposals. Our comparative analyses conducted on the nuScenes\ndatasets demonstrate that RadarDistill achieves state-of-the-art (SOTA)\nperformance for radar-only object detection task, recording 20.5% in mAP and\n43.7% in NDS. Also, RadarDistill significantly improves the performance of the\ncamera-radar fusion model.\n","versions":"[{'version': 'v1', 'created': 'Fri, 8 Mar 2024 05:15:48 GMT'}, {'version': 'v2', 'created': 'Fri, 5 Apr 2024 00:43:16 GMT'}, {'version': 'v3', 'created': 'Wed, 26 Feb 2025 13:41:29 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Bang', 'Geonho', ''], ['Choi', 'Kwangjin', ''], ['Kim', 'Jisong', ''], ['Kum', 'Dongsuk', ''], ['Choi', 'Jun Won', '']]","extracted_entities":"[{'text': 'RadarDistill', 'label': 'Knowledge distillation'}, {'text': 'RadarDistill', 'label': 'Knowledge distillation'}, {'text': 'CMA', 'label': 'Knowledge distillation'}, {'text': 'Activation-based Feature Distillation', 'label': 'Knowledge distillation'}, {'text': 'AFD', 'label': 'Knowledge distillation'}, {'text': 'Proposal-based Feature\\nDistillation', 'label': 'Knowledge distillation'}, {'text': 'PFD', 'label': 'Knowledge distillation'}, {'text': 'CMA', 'label': 'Knowledge distillation'}, {'text': 'AFD', 'label': 'Knowledge distillation'}, {'text': 'PFD', 'label': 'Knowledge distillation'}, {'text': 'RadarDistill', 'label': 'Knowledge distillation'}, {'text': 'RadarDistill', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Activation-based Feature Distillation","similarity_score":0.6461375952}
{"id":2404.02241,"submitter":"Enshu Liu","authors":"Enshu Liu, Junyi Zhu, Zinan Lin, Xuefei Ning, Shuaiqi Wang, Matthew B.\n  Blaschko, Sergey Yekhanin, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang","title":"Linear Combination of Saved Checkpoints Makes Consistency and Diffusion\n  Models Better","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Diffusion Models (DM) and Consistency Models (CM) are two types of popular\ngenerative models with good generation quality on various tasks. When training\nDM and CM, intermediate weight checkpoints are not fully utilized and only the\nlast converged checkpoint is used. In this work, we find that high-quality\nmodel weights often lie in a basin which cannot be reached by SGD but can be\nobtained by proper checkpoint averaging. Based on these observations, we\npropose LCSC, a simple but effective and efficient method to enhance the\nperformance of DM and CM, by combining checkpoints along the training\ntrajectory with coefficients deduced from evolutionary search. We demonstrate\nthe value of LCSC through two use cases: $\\textbf{(a) Reducing training cost.}$\nWith LCSC, we only need to train DM\/CM with fewer number of iterations and\/or\nlower batch sizes to obtain comparable sample quality with the fully trained\nmodel. For example, LCSC achieves considerable training speedups for CM\n(23$\\times$ on CIFAR-10 and 15$\\times$ on ImageNet-64). $\\textbf{(b) Enhancing\npre-trained models.}$ Assuming full training is already done, LCSC can further\nimprove the generation quality or speed of the final converged models. For\nexample, LCSC achieves better performance using 1 number of function evaluation\n(NFE) than the base model with 2 NFE on consistency distillation, and decreases\nthe NFE of DM from 15 to 9 while maintaining the generation quality on\nCIFAR-10. Our code is available at\nhttps:\/\/github.com\/imagination-research\/LCSC.\n","versions":"[{'version': 'v1', 'created': 'Tue, 2 Apr 2024 18:59:39 GMT'}, {'version': 'v2', 'created': 'Mon, 8 Apr 2024 02:06:37 GMT'}, {'version': 'v3', 'created': 'Wed, 26 Feb 2025 12:28:39 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Liu', 'Enshu', ''], ['Zhu', 'Junyi', ''], ['Lin', 'Zinan', ''], ['Ning', 'Xuefei', ''], ['Wang', 'Shuaiqi', ''], ['Blaschko', 'Matthew B.', ''], ['Yekhanin', 'Sergey', ''], ['Yan', 'Shengen', ''], ['Dai', 'Guohao', ''], ['Yang', 'Huazhong', ''], ['Wang', 'Yu', '']]","extracted_entities":"[{'text': 'consistency distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"consistency distillation","similarity_score":0.6887969971}
{"id":2404.0491,"submitter":"Hou-I Liu","authors":"Hou-I Liu, Christine Wu, Jen-Hao Cheng, Wenhao Chai, Shian-Yun Wang,\n  Gaowen Liu, Jenq-Neng Hwang, Hong-Han Shuai and Wen-Huang Cheng","title":"MonoTAKD: Teaching Assistant Knowledge Distillation for Monocular 3D\n  Object Detection","comments":"Accepted by CVPR 2025. Our code will be available at\n  https:\/\/github.com\/hoiliu-0801\/MonoTAKD","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Monocular 3D object detection (Mono3D) holds noteworthy promise for\nautonomous driving applications owing to the cost-effectiveness and rich visual\ncontext of monocular camera sensors. However, depth ambiguity poses a\nsignificant challenge, as it requires extracting precise 3D scene geometry from\na single image, resulting in suboptimal performance when transferring knowledge\nfrom a LiDAR-based teacher model to a camera-based student model. To address\nthis issue, we introduce {\\em Monocular Teaching Assistant Knowledge\nDistillation (MonoTAKD)} to enhance 3D perception in Mono3D. Our approach\npresents a robust camera-based teaching assistant model that effectively\nbridges the representation gap between different modalities for teacher and\nstudent models, addressing the challenge of inaccurate depth estimation. By\ndefining 3D spatial cues as residual features that capture the differences\nbetween the teacher and the teaching assistant models, we leverage these cues\ninto the student model, improving its 3D perception capabilities. Experimental\nresults show that our MonoTAKD achieves state-of-the-art performance on the\nKITTI3D dataset. Additionally, we evaluate the performance on nuScenes and\nKITTI raw datasets to demonstrate the generalization of our model to multi-view\n3D and unsupervised data settings. Our code will be available at\nhttps:\/\/github.com\/hoiliu-0801\/MonoTAKD.\n","versions":"[{'version': 'v1', 'created': 'Sun, 7 Apr 2024 10:39:04 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 02:56:48 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Liu', 'Hou-I', ''], ['Wu', 'Christine', ''], ['Cheng', 'Jen-Hao', ''], ['Chai', 'Wenhao', ''], ['Wang', 'Shian-Yun', ''], ['Liu', 'Gaowen', ''], ['Hwang', 'Jenq-Neng', ''], ['Shuai', 'Hong-Han', ''], ['Cheng', 'Wen-Huang', '']]","extracted_entities":"[{'text': 'Monocular Teaching Assistant Knowledge\\nDistillation', 'label': 'Knowledge distillation'}, {'text': 'student model', 'label': 'AI model'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Monocular Teaching Assistant Knowledge\nDistillation","similarity_score":0.7319251895}
{"id":2404.17335,"submitter":"Xin Zhang","authors":"Xin Zhang, Liangxiu Han, Tam Sobeih, Lianghao Han, and Darren Dancey","title":"A Novel Spike Transformer Network for Depth Estimation from Event\n  Cameras via Cross-modality Knowledge Distillation","comments":"16 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Depth estimation is a critical task in computer vision, with applications in\nautonomous navigation, robotics, and augmented reality. Event cameras, which\nencode temporal changes in light intensity as asynchronous binary spikes, offer\nunique advantages such as low latency, high dynamic range, and energy\nefficiency. However, their unconventional spiking output and the scarcity of\nlabelled datasets pose significant challenges to traditional image-based depth\nestimation methods. To address these challenges, we propose a novel\nenergy-efficient Spike-Driven Transformer Network (SDT) for depth estimation,\nleveraging the unique properties of spiking data. The proposed SDT introduces\nthree key innovations: (1) a purely spike-driven transformer architecture that\nincorporates spike-based attention and residual mechanisms, enabling precise\ndepth estimation with minimal energy consumption; (2) a fusion depth estimation\nhead that combines multi-stage features for fine-grained depth prediction while\nensuring computational efficiency; and (3) a cross-modality knowledge\ndistillation framework that utilises a pre-trained vision foundation model\n(DINOv2) to enhance the training of the spiking network despite limited data\navailability.This work represents the first exploration of transformer-based\nspiking neural networks for depth estimation, providing a significant step\nforward in energy-efficient neuromorphic computing for real-world vision\napplications.\n","versions":"[{'version': 'v1', 'created': 'Fri, 26 Apr 2024 11:32:53 GMT'}, {'version': 'v2', 'created': 'Wed, 1 May 2024 08:54:54 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 10:47:58 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Zhang', 'Xin', ''], ['Han', 'Liangxiu', ''], ['Sobeih', 'Tam', ''], ['Han', 'Lianghao', ''], ['Dancey', 'Darren', '']]","extracted_entities":"[{'text': 'spike-based attention and residual mechanisms', 'label': 'Attention mechanism'}, {'text': 'cross-modality knowledge\\ndistillation', 'label': 'Knowledge distillation'}, {'text': 'DINOv2', 'label': 'Foundation Model'}]","assigned_concept":"Knowledge distillation","matched_keyword":"cross-modality knowledge\ndistillation","similarity_score":0.8367550373}
{"id":2408.12526,"submitter":"Weiyan Wang","authors":"Weiyan Wang, Yilun Jin, Yiming Zhang, Victor Junqiu Wei, Han Tian, Li\n  Chen, Jinbao Xue, Yangyu Tao, Di Wang, Kai Chen","title":"Exploiting Student Parallelism for Efficient GPU Inference of BERT-like\n  Models in Online Services","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Due to high accuracy, BERT-like models have been widely adopted by text\nmining and web searching. However, large BERT-like models suffer from\ninefficient online inference, facing the following two problems on GPUs: (1)\ntheir high accuracy relies on the large model depth, which linearly increases\nthe sequential computation on GPUs; (2) stochastic and dynamic online workloads\ncause extra costs from batching and paddings. Therefore, we present \\sys for\nthe real-world setting of GPU inference on online workloads. At its core, \\sys\nadopts stacking distillation and boosting ensemble, distilling the original\ndeep model into a group of shallow but virtually stacked student models running\nin parallel. This enables \\sys to achieve a lower model depth (e.g., two\nlayers) than the others and the lowest inference latency while maintaining\naccuracy. In addition, adaptive student pruning realizes dynamic student\nnumbers according to changing online workloads. Especially for occasional\nworkload bursts, it can temporarily decrease the student number with minimal\naccuracy loss to improve system throughput. We conduct comprehensive\nexperiments to verify the effectiveness, whose results show that \\sys\noutperforms the baselines by $4.1\\times\\sim 1.6\\times$ in latency while\nmaintaining accuracy and achieves up to $22.27\\times$ higher throughput for\nworkload bursts.\n","versions":"[{'version': 'v1', 'created': 'Thu, 22 Aug 2024 16:31:32 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 12:08:13 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Wang', 'Weiyan', ''], ['Jin', 'Yilun', ''], ['Zhang', 'Yiming', ''], ['Wei', 'Victor Junqiu', ''], ['Tian', 'Han', ''], ['Chen', 'Li', ''], ['Xue', 'Jinbao', ''], ['Tao', 'Yangyu', ''], ['Wang', 'Di', ''], ['Chen', 'Kai', '']]","extracted_entities":"[{'text': 'stacking distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"stacking distillation","similarity_score":0.592784524}
{"id":2401.11323,"submitter":"Yu Bai","authors":"Yu Bai, Heyan Huang, Cesare Spinoso-Di Piano, Marc-Antoine Rondeau,\n  Sanxing Chen, Yang Gao, Jackie Chi Kit Cheung","title":"Identifying and Analyzing Performance-Critical Tokens in Large Language\n  Models","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In-context learning (ICL) has emerged as an effective solution for few-shot\nlearning with large language models (LLMs). However, how LLMs leverage\ndemonstrations to specify a task and learn a corresponding computational\nfunction through ICL is underexplored. Drawing from the way humans learn from\ncontent-label mappings in demonstrations, we categorize the tokens in an ICL\nprompt into content, stopword, and template tokens. Our goal is to identify the\ntypes of tokens whose representations directly influence LLM's performance, a\nproperty we refer to as being performance-critical. By ablating representations\nfrom the attention of the test example, we find that the representations of\ninformative content tokens have less influence on performance compared to\ntemplate and stopword tokens, which contrasts with the human attention to\ninformative words. We give evidence that the representations of\nperformance-critical tokens aggregate information from the content tokens.\nMoreover, we demonstrate experimentally that lexical meaning, repetition, and\nstructural cues are the main distinguishing characteristics of these tokens.\nOur work sheds light on how large language models learn to perform tasks from\ndemonstrations and deepens our understanding of the roles different types of\ntokens play in large language models.\n","versions":"[{'version': 'v1', 'created': 'Sat, 20 Jan 2024 20:55:21 GMT'}, {'version': 'v2', 'created': 'Fri, 16 Feb 2024 16:43:35 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 03:35:56 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Bai', 'Yu', ''], ['Huang', 'Heyan', ''], ['Piano', 'Cesare Spinoso-Di', ''], ['Rondeau', 'Marc-Antoine', ''], ['Chen', 'Sanxing', ''], ['Gao', 'Yang', ''], ['Cheung', 'Jackie Chi Kit', '']]","extracted_entities":"[{'text': 'In-context learning', 'label': 'contextual Embedding'}, {'text': 'few-shot\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ICL', 'label': 'Few-shot Learning'}, {'text': 'ICL\\nprompt', 'label': 'Prompting'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}]","assigned_concept":"Few-shot Learning","matched_keyword":"few-shot\nlearning","similarity_score":1.0}
{"id":2402.18599,"submitter":"Mohammad Rostami","authors":"Mohammad Rostami, Atik Faysal, Huaxia Wang and Avimanyu Sahoo","title":"Meta-Task: A Method-Agnostic Framework for Learning to Regularize in\n  Few-Shot Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Overfitting is a significant challenge in Few-Shot Learning (FSL), where\nmodels trained on small, variable datasets tend to memorize rather than\ngeneralize to unseen tasks. Regularization is crucial in FSL to prevent\noverfitting and enhance generalization performance. To address this issue, we\nintroduce Meta-Task, a novel, method-agnostic framework that leverages both\nlabeled and unlabeled data to enhance generalization through auxiliary tasks\nfor regularization. Specifically, Meta-Task introduces a Task-Decoder, which is\na simple example of the broader framework that refines hidden representations\nby reconstructing input images from embeddings, effectively mitigating\noverfitting.\n  Our framework's method-agnostic design ensures its broad applicability across\nvarious FSL settings. We validate Meta-Task's effectiveness on standard\nbenchmarks, including Mini-ImageNet, Tiered-ImageNet, and FC100, where it\nconsistently improves existing state-of-the-art meta-learning techniques,\ndemonstrating superior performance, faster convergence, reduced generalization\nerror, and lower variance-all without extensive hyperparameter tuning. These\nresults underline Meta-Task's practical applicability and efficiency in\nreal-world, resource-constrained scenarios.\n","versions":"[{'version': 'v1', 'created': 'Tue, 27 Feb 2024 21:15:40 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 23:07:40 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Rostami', 'Mohammad', ''], ['Faysal', 'Atik', ''], ['Wang', 'Huaxia', ''], ['Sahoo', 'Avimanyu', '']]","extracted_entities":"[{'text': 'Few-Shot Learning', 'label': 'Few-shot Learning'}, {'text': 'FSL', 'label': 'Few-shot Learning'}, {'text': 'FSL', 'label': 'Few-shot Learning'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'FSL', 'label': 'Few-shot Learning'}, {'text': 'hyperparameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"Few-Shot Learning","similarity_score":1.0}
{"id":2403.18998,"submitter":"Yuqing Wang","authors":"Yuqing Wang and Mika V. M\\\"antyl\\\"a and Serge Demeyer and Mutlu\n  Beyazit and Joanna Kisaakye and Jesse Nyyss\\\"ol\\\"a","title":"Cross-System Categorization of Abnormal Traces in Microservice-Based\n  Systems via Meta-Learning","comments":"Accepted at ACM International Conference on the Foundations of\n  Software Engineering (FSE) 2025","journal-ref":null,"doi":"10.1145\/3715742","report-no":null,"categories":"cs.SE cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Microservice-based systems (MSS) may fail with various fault types. While\nexisting AIOps methods excel at detecting abnormal traces and locating the\nresponsible service(s), human efforts are still required for diagnosing\nspecific fault types and failure causes.This paper presents TraFaultDia, a\nnovel AIOps framework to automatically classify abnormal traces into fault\ncategories for MSS. We treat the classification process as a series of\nmulti-class classification tasks, where each task represents an attempt to\nclassify abnormal traces into specific fault categories for a MSS. TraFaultDia\nleverages meta-learning to train on several abnormal trace classification tasks\nwith a few labeled instances from a MSS, enabling quick adaptation to new,\nunseen abnormal trace classification tasks with a few labeled instances across\nMSS. TraFaultDia's use cases are scalable depending on how fault categories are\nbuilt from anomalies within MSS. We evaluated TraFaultDia on two MSS,\nTrainTicket and OnlineBoutique, with open datasets where each fault category is\nlinked to faulty system components (service\/pod) and a root cause. TraFaultDia\nautomatically classifies abnormal traces into these fault categories, thus\nenabling the automatic identification of faulty system components and root\ncauses without manual analysis. TraFaultDia achieves 93.26% and 85.20% accuracy\non 50 new classification tasks for TrainTicket and OnlineBoutique,\nrespectively, when trained within the same MSS with 10 labeled instances per\ncategory. In the cross-system context, when TraFaultDia is applied to a MSS\ndifferent from the one it is trained on, TraFaultDia gets an average accuracy\nof 92.19% and 84.77% for the same set of 50 new, unseen abnormal trace\nclassification tasks of the respective systems, also with 10 labeled instances\nprovided for each fault category per task in each system.\n","versions":"[{'version': 'v1', 'created': 'Wed, 27 Mar 2024 20:38:04 GMT'}, {'version': 'v2', 'created': 'Sun, 31 Mar 2024 16:15:58 GMT'}, {'version': 'v3', 'created': 'Fri, 12 Apr 2024 10:09:16 GMT'}, {'version': 'v4', 'created': 'Tue, 25 Feb 2025 08:50:14 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Wang', 'Yuqing', ''], ['M\u00e4ntyl\u00e4', 'Mika V.', ''], ['Demeyer', 'Serge', ''], ['Beyazit', 'Mutlu', ''], ['Kisaakye', 'Joanna', ''], ['Nyyss\u00f6l\u00e4', 'Jesse', '']]","extracted_entities":"[{'text': 'meta-learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"meta-learning","similarity_score":0.5332470536}
{"id":2406.0066,"submitter":"Yingcun Xia","authors":"Haoran Zhan, Jingli Wang, Yingcun Xia","title":"Non-asymptotic Properties of Generalized Mondrian Forests in Statistical\n  Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.ST stat.TH","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Random Forests have been extensively used in regression and classification,\ninspiring the development of various forest-based methods. Among these,\nMondrian Forests, derived from the Mondrian process, mark a significant\nadvancement. Expanding on Mondrian Forests, this paper presents a general\nframework for statistical learning, encompassing a range of common learning\ntasks such as least squares regression, $\\ell_1$ regression, quantile\nregression, and classification. Under mild assumptions on the loss functions,\nwe provide upper bounds on the regret\/risk functions for the estimators and\ndemonstrate their statistical consistency.\n","versions":"[{'version': 'v1', 'created': 'Sun, 2 Jun 2024 08:09:22 GMT'}, {'version': 'v2', 'created': 'Sun, 22 Dec 2024 06:06:44 GMT'}, {'version': 'v3', 'created': 'Thu, 27 Feb 2025 13:49:00 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Zhan', 'Haoran', ''], ['Wang', 'Jingli', ''], ['Xia', 'Yingcun', '']]","extracted_entities":"[{'text': 'statistical learning', 'label': 'Few-shot Learning'}, {'text': 'least squares regression', 'label': 'Zero-shot Learning'}, {'text': 'quantile\\nregression', 'label': 'Zero-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"statistical learning","similarity_score":0.5051944256}
{"id":2407.08056,"submitter":"Nikolaos Dimitriadis","authors":"Nikolaos Dimitriadis, Pascal Frossard, Francois Fleuret","title":"Pareto Low-Rank Adapters: Efficient Multi-Task Learning with Preferences","comments":"Accepted at ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multi-task trade-offs in machine learning can be addressed via Pareto Front\nLearning (PFL) methods that parameterize the Pareto Front (PF) with a single\nmodel. PFL permits to select the desired operational point during inference,\ncontrary to traditional Multi-Task Learning (MTL) that optimizes for a single\ntrade-off decided prior to training. However, recent PFL methodologies suffer\nfrom limited scalability, slow convergence, and excessive memory requirements,\nwhile exhibiting inconsistent mappings from preference to objective space. We\nintroduce PaLoRA, a novel parameter-efficient method that addresses these\nlimitations in two ways. First, we augment any neural network architecture with\ntask-specific low-rank adapters and continuously parameterize the PF in their\nconvex hull. Our approach steers the original model and the adapters towards\nlearning general and task-specific features, respectively. Second, we propose a\ndeterministic sampling schedule of preference vectors that reinforces this\ndivision of labor, enabling faster convergence and strengthening the validity\nof the mapping from preference to objective space throughout training. Our\nexperiments show that PaLoRA outperforms state-of-the-art MTL and PFL baselines\nacross various datasets, scales to large networks, reducing the memory overhead\n$23.8-31.7$ times compared with competing PFL baselines in scene understanding\nbenchmarks.\n","versions":"[{'version': 'v1', 'created': 'Wed, 10 Jul 2024 21:25:51 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 17:56:57 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Dimitriadis', 'Nikolaos', ''], ['Frossard', 'Pascal', ''], ['Fleuret', 'Francois', '']]","extracted_entities":"[{'text': 'Pareto Front\\nLearning', 'label': 'Few-shot Learning'}, {'text': 'PFL', 'label': 'Few-shot Learning'}, {'text': 'Multi-Task Learning', 'label': 'Few-shot Learning'}, {'text': 'PFL', 'label': 'Few-shot Learning'}, {'text': 'task-specific low-rank adapters', 'label': 'Transformers'}]","assigned_concept":"Few-shot Learning","matched_keyword":"Multi-Task Learning","similarity_score":0.5245423317}
{"id":2408.15753,"submitter":"Omer Rochman-Sharabi","authors":"Omer Rochman Sharabi and Sacha Lewin and Gilles Louppe","title":"A Neural Material Point Method for Particle-based Emulation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Mesh-free Lagrangian methods are widely used for simulating fluids, solids,\nand their complex interactions due to their ability to handle large\ndeformations and topological changes. These physics simulators, however,\nrequire substantial computational resources for accurate simulations. To\naddress these issues, deep learning emulators promise faster and scalable\nsimulations, yet they often remain expensive and difficult to train, limiting\ntheir practical use. Inspired by the Material Point Method (MPM), we present\nNeuralMPM, a neural emulation framework for particle-based simulations.\nNeuralMPM interpolates Lagrangian particles onto a fixed-size grid, computes\nupdates on grid nodes using image-to-image neural networks, and interpolates\nback to the particles. Similarly to MPM, NeuralMPM benefits from the regular\nvoxelized representation to simplify the computation of the state dynamics,\nwhile avoiding the drawbacks of mesh-based Eulerian methods. We demonstrate the\nadvantages of NeuralMPM on several datasets, including fluid dynamics and\nfluid-solid interactions. Compared to existing methods, NeuralMPM reduces\ntraining times from days to hours, while achieving comparable or superior\nlong-term accuracy, making it a promising approach for practical forward and\ninverse problems. A project page is available at https:\/\/neuralmpm.isach.be\n","versions":"[{'version': 'v1', 'created': 'Wed, 28 Aug 2024 12:39:51 GMT'}, {'version': 'v2', 'created': 'Sun, 13 Oct 2024 08:44:12 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 12:41:06 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Sharabi', 'Omer Rochman', ''], ['Lewin', 'Sacha', ''], ['Louppe', 'Gilles', '']]","extracted_entities":"[{'text': 'NeuralMPM', 'label': 'Neural Language Model'}, {'text': 'NeuralMPM', 'label': 'Neural Language Model'}, {'text': 'NeuralMPM', 'label': 'Neural Language Model'}, {'text': 'NeuralMPM', 'label': 'Neural Language Model'}, {'text': 'NeuralMPM', 'label': 'Neural Language Model'}, {'text': 'neuralmpm', 'label': 'Neural Language Model'}]","assigned_concept":"Neural Language Model","matched_keyword":"NeuralMPM","similarity_score":0.5694018602}
{"id":2407.11598,"submitter":"Susanne Pumpluen","authors":"Susanne Pumpluen","title":"A classification of the division algebras that are isotopic to a cyclic\n  Galois field extension","comments":"The last section of previous version has been removed, the rest is\n  rewritten","journal-ref":null,"doi":null,"report-no":null,"categories":"math.RA","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We classify all division algebras that are principal Albert isotopes of a\ncyclic Galois field extension of degree $n>2$ up to isomorphisms. We achieve a\n``tight'' classification when the cyclic Galois field extension is cubic. The\nclassification is ``tight'' in the sense that the list of algebras has features\nthat make it easy to distinguish non-isomorphic ones.\n","versions":"[{'version': 'v1', 'created': 'Tue, 16 Jul 2024 10:54:38 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 18:21:00 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Pumpluen', 'Susanne', '']]","extracted_entities":"[{'text': 'Albert', 'label': 'ALBERT'}]","assigned_concept":"ALBERT","matched_keyword":"Albert","similarity_score":1.0}
{"id":2307.13658,"submitter":"Nicholas Perello","authors":"Przemyslaw Grabowicz, Adrian Byrne, Cyrus Cousins, Nicholas Perello,\n  Yair Zick","title":"Towards an AI Accountability Policy","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We propose establishing an office to oversee AI systems by introducing a\ntiered system of explainability and benchmarking requirements for commercial AI\nsystems. We examine how complex high-risk technologies have been successfully\nregulated at the national level. Specifically, we draw parallels to the\nexisting regulation for the U.S. medical device industry and the pharmaceutical\nindustry (regulated by the FDA), the proposed legislation for AI in the\nEuropean Union (the AI Act), and the existing U.S. anti-discrimination\nlegislation. To promote accountability and user trust, AI accountability\nmechanisms shall introduce standarized measures for each category of intended\nhigh-risk use of AI systems to enable structured comparisons among such AI\nsystems. We suggest using explainable AI techniques, such as input influence\nmeasures, as well as fairness statistics and other performance measures of\nhigh-risk AI systems. We propose to standardize internal benchmarking and\nautomated audits to transparently characterize high-risk AI systems. The\nresults of such audits and benchmarks shall be clearly and transparently\ncommunicated and explained to enable meaningful comparisons of competing AI\nsystems via a public AI registry. Such standardized audits, benchmarks, and\ncertificates shall be specific to intended high-risk use of respective AI\nsystems and could constitute conformity assessment for AI systems, e.g., in the\nEuropean Union's AI Act.\n","versions":"[{'version': 'v1', 'created': 'Tue, 25 Jul 2023 17:09:28 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 18:17:19 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Grabowicz', 'Przemyslaw', ''], ['Byrne', 'Adrian', ''], ['Cousins', 'Cyrus', ''], ['Perello', 'Nicholas', ''], ['Zick', 'Yair', '']]","extracted_entities":"[{'text': 'AI Act', 'label': 'AI Ethics'}, {'text': 'AI accountability\\nmechanisms', 'label': 'AI Ethics'}, {'text': 'fairness statistics', 'label': 'Model Bias and Fairness'}, {'text': 'AI Act', 'label': 'AI Ethics'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"fairness statistics","similarity_score":0.7303280234}
{"id":2401.1699,"submitter":"Johan De Aguas","authors":"Johan de Aguas and Johan Pensar and Tom\\'as Varnet P\\'erez and Guido\n  Biele","title":"Recovery and inference of causal effects with sequential adjustment for\n  confounding and attrition","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ME","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Confounding bias and selection bias bring two significant challenges to the\nvalidity of conclusions drawn from applied causal inference. The latter can\nstem from informative missingness, such as in cases of attrition. We introduce\nthe Sequential Adjustment Criteria (SAC), which extend available graphical\nconditions for recovering causal effects from confounding and attrition using\nsequential regressions, allowing for the inclusion of post-exposure and\nforbidden variables in the adjustment sets. We propose an estimator for the\nrecovered Average Treatment Effect (ATE) based on Targeted Minimum-Loss\nEstimation (TMLE), which exhibits multiple robustness under certain technical\nconditions. This approach ensures consistency even in scenarios where the\nDouble Inverse Probability Weighting (DIPW) and the naive plug-in sequential\nregressions approaches fall short. Through a simulation study, we assess the\nperformance of the proposed estimator against alternative methods across\ndifferent graph setups and model specification scenarios. As a motivating\napplication, we examine the effect of pharmacological treatment for\nAttention-Deficit\/Hyperactivity Disorder (ADHD) upon the scores obtained by\ndiagnosed Norwegian schoolchildren in national tests using observational data\n($n=9\\,352$). Our findings align with the accumulated clinical evidence,\naffirming a positive but small impact of medication on academic achievement.\n","versions":"[{'version': 'v1', 'created': 'Tue, 30 Jan 2024 13:22:21 GMT'}, {'version': 'v2', 'created': 'Mon, 30 Sep 2024 15:49:06 GMT'}, {'version': 'v3', 'created': 'Wed, 2 Oct 2024 19:01:30 GMT'}, {'version': 'v4', 'created': 'Tue, 25 Feb 2025 09:47:49 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['de Aguas', 'Johan', ''], ['Pensar', 'Johan', ''], ['P\u00e9rez', 'Tom\u00e1s Varnet', ''], ['Biele', 'Guido', '']]","extracted_entities":"[{'text': 'Confounding bias', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"Confounding bias","similarity_score":0.5946897268}
{"id":2404.18598,"submitter":"Tianyidan Xie","authors":"Tianyidan Xie, Rui Ma, Qian Wang, Xiaoqian Ye, Feixuan Liu, Ying Tai,\n  Zhenyu Zhang, Lanjun Wang, Zili Yi","title":"Anywhere: A Multi-Agent Framework for User-Guided, Reliable, and Diverse\n  Foreground-Conditioned Image Generation","comments":"18 pages, 15 figures, project page:\n  https:\/\/anywheremultiagent.github.io, Accepted at AAAI 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.GR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advancements in image-conditioned image generation have demonstrated\nsubstantial progress. However, foreground-conditioned image generation remains\nunderexplored, encountering challenges such as compromised object integrity,\nforeground-background inconsistencies, limited diversity, and reduced control\nflexibility. These challenges arise from current end-to-end inpainting models,\nwhich suffer from inaccurate training masks, limited foreground semantic\nunderstanding, data distribution biases, and inherent interference between\nvisual and textual prompts. To overcome these limitations, we present Anywhere,\na multi-agent framework that departs from the traditional end-to-end approach.\nIn this framework, each agent is specialized in a distinct aspect, such as\nforeground understanding, diversity enhancement, object integrity protection,\nand textual prompt consistency. Our framework is further enhanced with the\nability to incorporate optional user textual inputs, perform automated quality\nassessments, and initiate re-generation as needed. Comprehensive experiments\ndemonstrate that this modular design effectively overcomes the limitations of\nexisting end-to-end models, resulting in higher fidelity, quality, diversity\nand controllability in foreground-conditioned image generation. Additionally,\nthe Anywhere framework is extensible, allowing it to benefit from future\nadvancements in each individual agent.\n","versions":"[{'version': 'v1', 'created': 'Mon, 29 Apr 2024 11:13:37 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 15:59:18 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Xie', 'Tianyidan', ''], ['Ma', 'Rui', ''], ['Wang', 'Qian', ''], ['Ye', 'Xiaoqian', ''], ['Liu', 'Feixuan', ''], ['Tai', 'Ying', ''], ['Zhang', 'Zhenyu', ''], ['Wang', 'Lanjun', ''], ['Yi', 'Zili', '']]","extracted_entities":"[{'text': 'data distribution biases', 'label': 'Model Bias and Fairness'}, {'text': 'visual and textual prompts', 'label': 'Prompting'}, {'text': 'textual prompt consistency', 'label': 'Prompting'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"data distribution biases","similarity_score":0.5468835831}
{"id":2405.0593,"submitter":"Siyuan Li","authors":"Siyuan Li, Xi Lin, Yaju Liu, Xiang Chen, Jianhua Li","title":"Trustworthy AI-Generative Content for Intelligent Network Service:\n  Robustness, Security, and Fairness","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI cs.NI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  AI-generated content (AIGC) models, represented by large language models\n(LLM), have revolutionized content creation. High-speed next-generation\ncommunication technology is an ideal platform for providing powerful AIGC\nnetwork services. At the same time, advanced AIGC techniques can also make\nfuture network services more intelligent, especially various online content\ngeneration services. However, the significant untrustworthiness concerns of\ncurrent AIGC models, such as robustness, security, and fairness, greatly affect\nthe credibility of intelligent network services, especially in ensuring secure\nAIGC services. This paper proposes TrustGAIN, a trustworthy AIGC framework that\nincorporates robust, secure, and fair network services. We first discuss the\nrobustness to adversarial attacks faced by AIGC models in network systems and\nthe corresponding protection issues. Subsequently, we emphasize the importance\nof avoiding unsafe and illegal services and ensuring the fairness of the AIGC\nnetwork services. Then as a case study, we propose a novel sentiment\nanalysis-based detection method to guide the robust detection of unsafe content\nin network services. We conduct our experiments on fake news, malicious code,\nand unsafe review datasets to represent LLM application scenarios. Our results\nindicate that TrustGAIN is an exploration of future networks that can support\ntrustworthy AIGC network services.\n","versions":"[{'version': 'v1', 'created': 'Thu, 9 May 2024 17:16:20 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 08:09:23 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Li', 'Siyuan', ''], ['Lin', 'Xi', ''], ['Liu', 'Yaju', ''], ['Chen', 'Xiang', ''], ['Li', 'Jianhua', '']]","extracted_entities":"[{'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fair', 'label': 'Model Bias and Fairness'}, {'text': 'robustness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'LLM', 'label': 'Large Language Model'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"fairness","similarity_score":0.6551788449}
{"id":2405.17814,"submitter":"Hanjun Luo","authors":"Hanjun Luo, Ziye Deng, Ruizhe Chen, and Zuozhu Liu","title":"FAIntbench: A Holistic and Precise Benchmark for Bias Evaluation in\n  Text-to-Image Models","comments":"Accepted by ICML DMLR 2024","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The rapid development and reduced barriers to entry for Text-to-Image (T2I)\nmodels have raised concerns about the biases in their outputs, but existing\nresearch lacks a holistic definition and evaluation framework of biases,\nlimiting the enhancement of debiasing techniques. To address this issue, we\nintroduce FAIntbench, a holistic and precise benchmark for biases in T2I\nmodels. In contrast to existing benchmarks that evaluate bias in limited\naspects, FAIntbench evaluate biases from four dimensions: manifestation of\nbias, visibility of bias, acquired attributes, and protected attributes. We\napplied FAIntbench to evaluate seven recent large-scale T2I models and\nconducted human evaluation, whose results demonstrated the effectiveness of\nFAIntbench in identifying various biases. Our study also revealed new research\nquestions about biases, including the side-effect of distillation. The findings\npresented here are preliminary, highlighting the potential of FAIntbench to\nadvance future research aimed at mitigating the biases in T2I models. Our\nbenchmark is publicly available to ensure the reproducibility.\n","versions":"[{'version': 'v1', 'created': 'Tue, 28 May 2024 04:18:00 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Jun 2024 04:23:06 GMT'}, {'version': 'v3', 'created': 'Sat, 8 Jun 2024 13:41:36 GMT'}, {'version': 'v4', 'created': 'Mon, 22 Jul 2024 16:38:07 GMT'}, {'version': 'v5', 'created': 'Wed, 18 Sep 2024 04:40:40 GMT'}, {'version': 'v6', 'created': 'Mon, 24 Feb 2025 08:49:32 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Luo', 'Hanjun', ''], ['Deng', 'Ziye', ''], ['Chen', 'Ruizhe', ''], ['Liu', 'Zuozhu', '']]","extracted_entities":"[{'text': 'manifestation of\\nbias', 'label': 'Model Bias and Fairness'}, {'text': 'visibility of bias', 'label': 'Model Bias and Fairness'}, {'text': 'acquired attributes', 'label': 'Model Bias and Fairness'}, {'text': 'protected attributes', 'label': 'Model Bias and Fairness'}, {'text': 'side-effect of distillation', 'label': 'Knowledge distillation'}, {'text': 'publicly available', 'label': 'Open-source LLMs'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"visibility of bias","similarity_score":0.6506025791}
{"id":2406.11458,"submitter":"Maayan Ehrenberg","authors":"Maayan Ehrenberg, Roy Ganz, Nir Rosenfeld","title":"Adversaries With Incentives: A Strategic Alternative to Adversarial\n  Robustness","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.GT","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Adversarial training aims to defend against *adversaries*: malicious\nopponents whose sole aim is to harm predictive performance in any way possible\n- a rather harsh perspective, which we assert results in unnecessarily\nconservative models. Instead, we propose to model opponents as simply pursuing\ntheir own goals, rather than working directly against the classifier. Employing\ntools from strategic modeling, our approach uses knowledge or beliefs regarding\nthe opponent's possible incentives as inductive bias for learning. Our method\nof *strategic training* is designed to defend against opponents within an\n*incentive uncertainty set*: this resorts to adversarial learning when the set\nis maximal, but offers potential gains when it can be appropriately reduced. We\nconduct a series of experiments that show how even mild knowledge regarding the\nadversary's incentives can be useful, and that the degree of potential gains\ndepends on how incentives relate to the structure of the learning task.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Jun 2024 12:20:59 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 18:14:27 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Ehrenberg', 'Maayan', ''], ['Ganz', 'Roy', ''], ['Rosenfeld', 'Nir', '']]","extracted_entities":"[{'text': 'Adversarial training', 'label': 'Few-shot Learning'}, {'text': 'inductive bias', 'label': 'Model Bias and Fairness'}, {'text': 'adversarial learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"inductive bias","similarity_score":0.5208441019}
{"id":2408.09327,"submitter":"Jiancheng Dong","authors":"Jiancheng Dong, Lei Jiang, Wei Jin, Lu Cheng","title":"Threshold Filtering Packing for Supervised Fine-Tuning: Training Related\n  Samples within Packs","comments":"14 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Packing for Supervised Fine-Tuning (SFT) in autoregressive models involves\nconcatenating data points of varying lengths until reaching the designed\nmaximum length to facilitate GPU processing. However, randomly concatenating\ndata points can lead to cross-contamination of sequences due to the significant\ndifference in their subject matter. The mainstream approaches in SFT ensure\nthat each token in the attention calculation phase only focuses on tokens\nwithin its own short sequence, without providing additional learning signals\nfor the preceding context. To address these challenges, we introduce Threshold\nFiltering Packing (TFP), a method that selects samples with related context\nwhile maintaining sufficient diversity within the same pack. Our experiments\nshow that TFP offers a simple-to-implement and scalable approach that\nsignificantly enhances SFT performance, with observed improvements of up to 7\\%\non GSM8K, 4\\% on HumanEval. Furthermore, results from bias benchmark datasets\nhighlight TFP's promising performance in improving fairness while also boosting\nprediction accuracy by 15\\%.\n","versions":"[{'version': 'v1', 'created': 'Sun, 18 Aug 2024 01:59:41 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 05:16:14 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Dong', 'Jiancheng', ''], ['Jiang', 'Lei', ''], ['Jin', 'Wei', ''], ['Cheng', 'Lu', '']]","extracted_entities":"[{'text': 'fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"fairness","similarity_score":0.6551788449}
{"id":2402.08658,"submitter":"David Haag","authors":"David Haag, Devender Kumar, Sebastian Gruber, Dominik Hofer, Mahdi\n  Sareban, Gunnar Treff, Josef Niebauer, Christopher Bull, Albrecht Schmidt,\n  Jan David Smeddinck","title":"The Last JITAI? Exploring Large Language Models for Issuing Just-in-Time\n  Adaptive Interventions: Fostering Physical Activity in a Conceptual Cardiac\n  Rehabilitation Setting","comments":null,"journal-ref":null,"doi":"10.1145\/3706598.3713307","report-no":null,"categories":"cs.HC cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We evaluated the viability of using Large Language Models (LLMs) to trigger\nand personalize content in Just-in-Time Adaptive Interventions (JITAIs) in\ndigital health. As an interaction pattern representative of context-aware\ncomputing, JITAIs are being explored for their potential to support sustainable\nbehavior change, adapting interventions to an individual's current context and\nneeds. Challenging traditional JITAI implementation models, which face severe\nscalability and flexibility limitations, we tested GPT-4 for suggesting JITAIs\nin the use case of heart-healthy activity in cardiac rehabilitation. Using\nthree personas representing patients affected by CVD with varying severeness\nand five context sets per persona, we generated 450 JITAI decisions and\nmessages. These were systematically evaluated against those created by 10\nlaypersons (LayPs) and 10 healthcare professionals (HCPs). GPT-4-generated\nJITAIs surpassed human-generated intervention suggestions, outperforming both\nLayPs and HCPs across all metrics (i.e., appropriateness, engagement,\neffectiveness, and professionalism). These results highlight the potential of\nLLMs to enhance JITAI implementations in personalized health interventions,\ndemonstrating how generative AI could revolutionize context-aware computing.\n","versions":"[{'version': 'v1', 'created': 'Tue, 13 Feb 2024 18:39:36 GMT'}, {'version': 'v2', 'created': 'Mon, 15 Apr 2024 09:08:44 GMT'}, {'version': 'v3', 'created': 'Wed, 26 Feb 2025 08:57:38 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Haag', 'David', ''], ['Kumar', 'Devender', ''], ['Gruber', 'Sebastian', ''], ['Hofer', 'Dominik', ''], ['Sareban', 'Mahdi', ''], ['Treff', 'Gunnar', ''], ['Niebauer', 'Josef', ''], ['Bull', 'Christopher', ''], ['Schmidt', 'Albrecht', ''], ['Smeddinck', 'Jan David', '']]","extracted_entities":"[{'text': 'GPT-4', 'label': 'GPT'}, {'text': 'GPT-4-generated', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4","similarity_score":0.8572875857}
{"id":2405.1486,"submitter":"Joshua Engels","authors":"Joshua Engels, Eric J. Michaud, Isaac Liao, Wes Gurnee, Max Tegmark","title":"Not All Language Model Features Are One-Dimensionally Linear","comments":"Accepted to ICLR 2025. Code and data at\n  https:\/\/github.com\/JoshEngels\/MultiDimensionalFeatures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent work has proposed that language models perform computation by\nmanipulating one-dimensional representations of concepts (\"features\") in\nactivation space. In contrast, we explore whether some language model\nrepresentations may be inherently multi-dimensional. We begin by developing a\nrigorous definition of irreducible multi-dimensional features based on whether\nthey can be decomposed into either independent or non-co-occurring\nlower-dimensional features. Motivated by these definitions, we design a\nscalable method that uses sparse autoencoders to automatically find\nmulti-dimensional features in GPT-2 and Mistral 7B. These auto-discovered\nfeatures include strikingly interpretable examples, e.g. circular features\nrepresenting days of the week and months of the year. We identify tasks where\nthese exact circles are used to solve computational problems involving modular\narithmetic in days of the week and months of the year. Next, we provide\nevidence that these circular features are indeed the fundamental unit of\ncomputation in these tasks with intervention experiments on Mistral 7B and\nLlama 3 8B, and we examine the continuity of the days of the week feature in\nMistral 7B. Overall, our work argues that understanding multi-dimensional\nfeatures is necessary to mechanistically decompose some model behaviors.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 May 2024 17:59:04 GMT'}, {'version': 'v2', 'created': 'Tue, 8 Oct 2024 14:23:17 GMT'}, {'version': 'v3', 'created': 'Thu, 27 Feb 2025 03:03:59 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Engels', 'Joshua', ''], ['Michaud', 'Eric J.', ''], ['Liao', 'Isaac', ''], ['Gurnee', 'Wes', ''], ['Tegmark', 'Max', '']]","extracted_entities":"[{'text': 'GPT-2', 'label': 'GPT'}, {'text': 'Mistral 7B', 'label': 'Mistral'}, {'text': 'Mistral 7B', 'label': 'Mistral'}, {'text': 'Llama 3 8B', 'label': 'Llama'}, {'text': 'Mistral 7B', 'label': 'Mistral'}]","assigned_concept":"GPT","matched_keyword":"GPT-2","similarity_score":0.8734456301}
{"id":2407.07064,"submitter":"Nicolas E. Diaz Ferreyra PhD","authors":"Catherine Tony, Nicol\\'as E. D\\'iaz Ferreyra, Markus Mutas, Salem\n  Dhiff and Riccardo Scandariato","title":"Prompting Techniques for Secure Code Generation: A Systematic\n  Investigation","comments":"Work partially supported by the EU-funded project Sec4AI4Sec:\n  Cybersecurity for AI-Augmented Systems (grant no. 101120393) - ACCEPTED at\n  ACM Transactions on Software Engineering and Methodology (Feb. 2025)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI cs.CR cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) are gaining momentum in software development\nwith prompt-driven programming enabling developers to create code from natural\nlanguage (NL) instructions. However, studies have questioned their ability to\nproduce secure code and, thereby, the quality of prompt-generated software.\nAlongside, various prompting techniques that carefully tailor prompts have\nemerged to elicit optimal responses from LLMs. Still, the interplay between\nsuch prompting strategies and secure code generation remains under-explored and\ncalls for further investigations. OBJECTIVE: In this study, we investigate the\nimpact of different prompting techniques on the security of code generated from\nNL instructions by LLMs. METHOD: First we perform a systematic literature\nreview to identify the existing prompting techniques that can be used for code\ngeneration tasks. A subset of these techniques are evaluated on GPT-3, GPT-3.5,\nand GPT-4 models for secure code generation. For this, we used an existing\ndataset consisting of 150 NL security-relevant code-generation prompts.\nRESULTS: Our work (i) classifies potential prompting techniques for code\ngeneration (ii) adapts and evaluates a subset of the identified techniques for\nsecure code generation tasks and (iii) observes a reduction in security\nweaknesses across the tested LLMs, especially after using an existing technique\ncalled Recursive Criticism and Improvement (RCI), contributing valuable\ninsights to the ongoing discourse on LLM-generated code security.\n","versions":"[{'version': 'v1', 'created': 'Tue, 9 Jul 2024 17:38:03 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 14:28:11 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Tony', 'Catherine', ''], ['Ferreyra', 'Nicol\u00e1s E. D\u00edaz', ''], ['Mutas', 'Markus', ''], ['Dhiff', 'Salem', ''], ['Scandariato', 'Riccardo', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompting techniques', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompting techniques', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompting techniques', 'label': 'Prompting'}, {'text': 'GPT-3', 'label': 'GPT'}, {'text': 'GPT-3', 'label': 'GPT'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'prompting techniques', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"GPT","matched_keyword":"GPT-3","similarity_score":0.8771116138}
{"id":2210.05715,"submitter":"Joseba Fernandez de Landa","authors":"Joseba Fernandez de Landa and Rodrigo Agerri","title":"Language Independent Stance Detection: Social Interaction-based\n  Embeddings and Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  The large majority of the research performed on stance detection has been\nfocused on developing more or less sophisticated text classification systems,\neven when many benchmarks are based on social network data such as Twitter.\nThis paper aims to take on the stance detection task by placing the emphasis\nnot so much on the text itself but on the interaction data available on social\nnetworks. More specifically, we propose a new method to leverage social\ninformation such as friends and retweets by generating Relational Embeddings,\nnamely, dense vector representations of interaction pairs. Our experiments on\nseven publicly available datasets and four different languages (Basque,\nCatalan, Italian, and Spanish) show that combining our relational embeddings\nwith discriminative textual methods helps to substantially improve performance,\nobtaining state-of-the-art results for six out of seven evaluation settings,\noutperforming strong baselines based on Large Language Models, or other popular\ninteraction-based approaches such as DeepWalk or node2vec.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Oct 2022 18:13:43 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 09:17:32 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['de Landa', 'Joseba Fernandez', ''], ['Agerri', 'Rodrigo', '']]","extracted_entities":"[{'text': 'Relational Embeddings', 'label': 'Embedding'}, {'text': 'seven publicly available datasets', 'label': 'Open-source LLMs'}, {'text': 'relational embeddings', 'label': 'Embedding'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2308.12219,"submitter":"Jiasheng Ye","authors":"Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, Quanquan Gu","title":"Diffusion Language Models Can Perform Many Tasks with Scaling and\n  Instruction-Finetuning","comments":"add results on reasoning and multimodality; add discussions on latest\n  progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  The recent surge of generative AI has been fueled by the generative power of\ndiffusion probabilistic models and the scalable capabilities of large language\nmodels. Despite their potential, it remains elusive whether diffusion language\nmodels can solve general language tasks comparable to their autoregressive\ncounterparts. This paper demonstrates that scaling diffusion models w.r.t.\ndata, sizes, and tasks can effectively make them strong language learners. We\nbuild competent diffusion language models at scale by first acquiring knowledge\nfrom massive data via masked language modeling pretraining thanks to their\nintrinsic connections. We then reprogram pretrained masked language models into\ndiffusion language models via diffusive adaptation, wherein task-specific\nfinetuning and instruction finetuning are explored to unlock their versatility\nin solving general language tasks. Experiments show that scaling diffusion\nlanguage models consistently improves performance across downstream language\ntasks. We further discover that instruction finetuning can elicit zero-shot and\nfew-shot in-context learning abilities that help tackle many unseen tasks by\nfollowing natural language instructions, and show promise in advanced and\nchallenging abilities such as reasoning.\n","versions":"[{'version': 'v1', 'created': 'Wed, 23 Aug 2023 16:01:12 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Aug 2023 16:32:31 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 05:09:09 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Ye', 'Jiasheng', ''], ['Zheng', 'Zaixiang', ''], ['Bao', 'Yu', ''], ['Qian', 'Lihua', ''], ['Gu', 'Quanquan', '']]","extracted_entities":"[{'text': 'large language\\nmodels', 'label': 'Large Language Model'}, {'text': 'diffusion language\\nmodels', 'label': 'Large Language Model'}, {'text': 'diffusion language models', 'label': 'Large Language Model'}, {'text': 'diffusion language models', 'label': 'Large Language Model'}, {'text': 'task-specific\\nfinetuning', 'label': 'Fine-tuning'}, {'text': 'instruction finetuning', 'label': 'Fine-tuning'}, {'text': 'diffusion\\nlanguage models', 'label': 'Large Language Model'}, {'text': 'instruction finetuning', 'label': 'Fine-tuning'}]","assigned_concept":"Large Language Model","matched_keyword":"large language\nmodels","similarity_score":0.9664971828}
{"id":2310.04579,"submitter":"Tao Li","authors":"Tao Li, Juan Guevara, Xinhong Xie, and Quanyan Zhu","title":"Self-Confirming Transformer for Belief-Conditioned Adaptation in Offline\n  Multi-Agent Reinforcement Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.MA","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Offline reinforcement learning (RL) suffers from the distribution shift\nbetween the offline dataset and the online environment. In multi-agent RL\n(MARL), this distribution shift may arise from the nonstationary opponents in\nthe online testing who display distinct behaviors from those recorded in the\noffline dataset. Hence, the key to the broader deployment of offline MARL is\nthe online adaptation to nonstationary opponents. Recent advances in foundation\nmodels, e.g., large language models, have demonstrated the generalization\nability of the transformer, an emerging neural network architecture, in\nsequence modeling, of which offline RL is a special case. One naturally wonders\n\\textit{whether offline-trained transformer-based RL policies adapt to\nnonstationary opponents online}. We propose a novel auto-regressive training to\nequip transformer agents with online adaptability based on the idea of\nself-augmented pre-conditioning. The transformer agent first learns offline to\npredict the opponent's action based on past observations. When deployed online,\nsuch a fictitious opponent play, referred to as the belief, is fed back to the\ntransformer, together with other environmental feedback, to generate future\nactions conditional on the belief. Motivated by self-confirming equilibrium in\ngame theory, the training loss consists of belief consistency loss, requiring\nthe beliefs to match the opponent's actual actions and best response loss,\nmandating the agent to behave optimally under the belief. We evaluate the\nonline adaptability of the proposed self-confirming transformer (SCT) in a\nstructured environment, iterated prisoner's dilemma games, to demonstrate SCT's\nbelief consistency and equilibrium behaviors as well as more involved\nmulti-particle environments to showcase its superior performance against\nnonstationary opponents over prior transformers and offline MARL baselines.\n","versions":"[{'version': 'v1', 'created': 'Fri, 6 Oct 2023 20:43:08 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 06:49:47 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Li', 'Tao', ''], ['Guevara', 'Juan', ''], ['Xie', 'Xinhong', ''], ['Zhu', 'Quanyan', '']]","extracted_entities":"[{'text': 'foundation\\nmodels', 'label': 'Foundation Model'}, {'text': 'large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2402.05374,"submitter":"Youngsik Yun","authors":"Youngsik Yun and Jihie Kim","title":"CIC: A Framework for Culturally-Aware Image Captioning","comments":"Accepted by IJCAI 2024","journal-ref":null,"doi":"10.24963\/ijcai.2024\/180","report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Image Captioning generates descriptive sentences from images using\nVision-Language Pre-trained models (VLPs) such as BLIP, which has improved\ngreatly. However, current methods lack the generation of detailed descriptive\ncaptions for the cultural elements depicted in the images, such as the\ntraditional clothing worn by people from Asian cultural groups. In this paper,\nwe propose a new framework, Culturally-aware Image Captioning (CIC), that\ngenerates captions and describes cultural elements extracted from cultural\nvisual elements in images representing cultures. Inspired by methods combining\nvisual modality and Large Language Models (LLMs) through appropriate prompts,\nour framework (1) generates questions based on cultural categories from images,\n(2) extracts cultural visual elements from Visual Question Answering (VQA)\nusing generated questions, and (3) generates culturally-aware captions using\nLLMs with the prompts. Our human evaluation conducted on 45 participants from 4\ndifferent cultural groups with a high understanding of the corresponding\nculture shows that our proposed framework generates more culturally descriptive\ncaptions when compared to the image captioning baseline based on VLPs.\nResources can be found at https:\/\/shane3606.github.io\/cic..\n","versions":"[{'version': 'v1', 'created': 'Thu, 8 Feb 2024 03:12:25 GMT'}, {'version': 'v2', 'created': 'Thu, 2 May 2024 02:41:50 GMT'}, {'version': 'v3', 'created': 'Mon, 19 Aug 2024 00:52:51 GMT'}, {'version': 'v4', 'created': 'Mon, 9 Dec 2024 15:39:30 GMT'}, {'version': 'v5', 'created': 'Mon, 24 Feb 2025 06:56:33 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Yun', 'Youngsik', ''], ['Kim', 'Jihie', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'prompts', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2402.13758,"submitter":"Zheheng Luo","authors":"Zheheng Luo, Qianqian Xie, Sophia Ananiadou","title":"Factual consistency evaluation of summarization in the Era of large\n  language models","comments":"published on ESWA","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Factual inconsistency with source documents in automatically generated\nsummaries can lead to misinformation or pose risks. Existing factual\nconsistency (FC) metrics are constrained by their performance, efficiency, and\nexplainability. Recent advances in Large language models (LLMs) have\ndemonstrated remarkable potential in text evaluation but their effectiveness in\nassessing FC in summarization remains underexplored. Prior research has mostly\nfocused on proprietary LLMs, leaving essential factors that affect their\nassessment capabilities unexplored. Additionally, current FC evaluation\nbenchmarks are restricted to news articles, casting doubt on the generality of\nthe FC methods tested on them. In this paper, we first address the gap by\nintroducing TreatFact a dataset of LLM-generated summaries of clinical texts,\nannotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC\nevaluation across news and clinical domains and analyse the impact of model\nsize, prompts, pre-training and fine-tuning data. Our findings reveal that\ndespite proprietary models prevailing on the task, open-source LLMs lag behind.\nNevertheless, there is potential for enhancing the performance of open-source\nLLMs through increasing model size, expanding pre-training data, and developing\nwell-curated fine-tuning data. Experiments on TreatFact suggest that both\nprevious methods and LLM-based evaluators are unable to capture factual\ninconsistencies in clinical summaries, posing a new challenge for FC\nevaluation.\n","versions":"[{'version': 'v1', 'created': 'Wed, 21 Feb 2024 12:35:19 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 09:26:10 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Luo', 'Zheheng', ''], ['Xie', 'Qianqian', ''], ['Ananiadou', 'Sophia', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2402.16319,"submitter":"Runyu Peng","authors":"Runyu Peng, Yunhua Zhou, Qipeng Guo, Yang Gao, Hang Yan, Xipeng Qiu,\n  Dahua Lin","title":"Data-free Weight Compress and Denoise for Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) are reshaping the research landscape in\nartificial intelligence, particularly as model parameters scale up\nsignificantly, unlocking remarkable capabilities across various domains.\nNevertheless, the scalability of model parameters faces constraints due to\nlimitations in GPU memory and computational speed. To address these\nconstraints, various weight compression methods have emerged, such as Pruning\nand Quantization. Given the low-rank nature of weight matrices in language\nmodels, the reduction of weights through matrix decomposition undoubtedly holds\nsignificant potential and promise. In this paper, drawing upon the intrinsic\nstructure of LLMs, we propose a novel approach termed Data-free Joint Rank-k\nApproximation for compressing the parameter matrices. Significantly, our method\nis characterized by without necessitating additional involvement of any corpus,\nwhile simultaneously preserving orthogonality in conjunction with pruning and\nquantization methods. We achieve a model pruning of 80% parameters while\nretaining 93.43% of the original performance without any calibration data.\nAdditionally, we explore the fundamental properties of the weight matrix of\nLLMs undergone Rank-k Approximation and conduct comprehensive experiments to\nelucidate our hypothesis.\n","versions":"[{'version': 'v1', 'created': 'Mon, 26 Feb 2024 05:51:47 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 07:41:18 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Peng', 'Runyu', ''], ['Zhou', 'Yunhua', ''], ['Guo', 'Qipeng', ''], ['Gao', 'Yang', ''], ['Yan', 'Hang', ''], ['Qiu', 'Xipeng', ''], ['Lin', 'Dahua', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'Pruning', 'label': 'quantisation'}, {'text': 'Quantization', 'label': 'quantisation'}, {'text': 'pruning', 'label': 'quantisation'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2404.08948,"submitter":"Chenhui Cui","authors":"Chenhui Cui, Tao Li, Junjie Wang, Chunyang Chen, Dave Towey, Rubing\n  Huang","title":"Large Language Models for Mobile GUI Text Input Generation: An Empirical\n  Study","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Mobile applications have become an essential part of our daily lives, making\nensuring their quality an important activity. Graphical User Interface (GUI)\ntesting is a quality assurance method that has frequently been used for mobile\napps. When conducting GUI testing, it is important to generate effective text\ninputs for the text-input components. Some GUIs require these text inputs to be\nable to move from one page to the next: This can be a challenge to achieving\ncomplete UI exploration. Recently, Large Language Models (LLMs) have\ndemonstrated excellent text-generation capabilities. To the best of our\nknowledge, there has not yet been any empirical study to evaluate different\npre-trained LLMs' effectiveness at generating text inputs for mobile GUI\ntesting. This paper reports on a large-scale empirical study that extensively\ninvestigates the effectiveness of nine state-of-the-art LLMs in Android\ntext-input generation for UI pages. We collected 114 UI pages from 62\nopen-source Android apps and extracted contextual information from the UI pages\nto construct prompts for LLMs to generate text inputs. The experimental results\nshow that some LLMs can generate more effective and higher-quality text inputs,\nachieving a 50.58% to 66.67% page-pass-through rate (PPTR). We also found that\nusing more complete UI contextual information can increase the PPTRs of LLMs\nfor generating text inputs. We conducted an experiment to evaluate the\nbug-detection capabilities of LLMs by directly generating invalid text inputs.\nWe collected 37 real-world bugs related to text inputs. The results show that\nusing LLMs to directly generate invalid text inputs for bug detection is\ninsufficient: The bug-detection rates of the nine LLMs are all less than 23%.\nIn addition, we also describe six insights gained regarding the use of LLMs for\nAndroid testing: These insights will benefit the Android testing community.\n","versions":"[{'version': 'v1', 'created': 'Sat, 13 Apr 2024 09:56:50 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 06:23:35 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Cui', 'Chenhui', ''], ['Li', 'Tao', ''], ['Wang', 'Junjie', ''], ['Chen', 'Chunyang', ''], ['Towey', 'Dave', ''], ['Huang', 'Rubing', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2404.09656,"submitter":"Boris Shaposhnikov","authors":"Alexey Gorbatovski, Boris Shaposhnikov, Alexey Malakhov, Nikita\n  Surnachev, Yaroslav Aksenov, Ian Maksimov, Nikita Balagansky, Daniil Gavrilov","title":"Learn Your Reference Model for Real Good Alignment","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Despite the fact that offline methods for Large Language Models (LLMs)\nalignment do not require a direct reward model, they remain susceptible to\noveroptimization. This issue arises when the trained model deviates excessively\nfrom the reference policy, leading to a decrease in sample quality. We propose\na new paradigm of offline alignment methods, called Trust Region (including\nvariants TR-DPO, TR-IPO, TR-KTO), which dynamically updates the reference\npolicy throughout the training process. Our results show that TR alignment\nmethods effectively mitigate overoptimization, enabling models to maintain\nstrong performance even when substantially deviating from the initial reference\npolicy. We demonstrate the efficacy of these approaches not only through toy\nexamples that exhibit reduced overoptimization, but also through direct,\nside-by-side comparisons in specific tasks such as helpful and harmless\ndialogue, as well as summarization, where they surpass conventional methods.\nAdditionally, we report significant improvements in general-purpose assistant\nsetups with the Llama3 model on the AlpacaEval 2 and Arena-Hard benchmarks,\nhighlighting the advantages of Trust Region methods over classical approaches.\n","versions":"[{'version': 'v1', 'created': 'Mon, 15 Apr 2024 10:44:31 GMT'}, {'version': 'v2', 'created': 'Tue, 21 May 2024 15:04:12 GMT'}, {'version': 'v3', 'created': 'Fri, 11 Oct 2024 13:42:12 GMT'}, {'version': 'v4', 'created': 'Tue, 25 Feb 2025 10:19:35 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Gorbatovski', 'Alexey', ''], ['Shaposhnikov', 'Boris', ''], ['Malakhov', 'Alexey', ''], ['Surnachev', 'Nikita', ''], ['Aksenov', 'Yaroslav', ''], ['Maksimov', 'Ian', ''], ['Balagansky', 'Nikita', ''], ['Gavrilov', 'Daniil', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'summarization', 'label': 'Knowledge distillation'}, {'text': 'Llama3', 'label': 'Llama'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2404.10171,"submitter":"Thanh-Dung Le","authors":"Boammani Aser Lompo, Thanh-Dung Le, Philippe Jouvet, Rita Noumeir","title":"Are Medium-Sized Transformers Models still Relevant for Medical Records\n  Processing?","comments":"Under revision","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  As large language models (LLMs) become the standard in many NLP applications,\nwe explore the potential of medium-sized pretrained transformer models as a\nviable alternative for medical record processing. Medical records generated by\nhealthcare professionals during patient admissions remain underutilized due to\nchallenges such as complex medical terminology, the limited ability of\npretrained models to interpret numerical data, and the scarcity of annotated\ntraining datasets. Objective: This study aims to classify numerical values\nextracted from medical records into seven distinct physiological categories\nusing CamemBERT-bio. Previous research has suggested that transformer-based\nmodels may underperform compared to traditional NLP approaches in this context.\nMethods: To enhance the performance of CamemBERT-bio, we propose two key\ninnovations: (1) incorporating keyword embeddings to refine the model's\nattention mechanisms and (2) adopting a number-agnostic strategy by removing\nnumerical values from the text to encourage context-driven learning.\nAdditionally, we assess the criticality of extracted numerical data by\nverifying whether values fall within established standard ranges. Results: Our\nfindings demonstrate significant performance improvements, with CamemBERT-bio\nachieving an F1 score of 0.89 - an increase of over 20% compared to the 0.73 F1\nscore of traditional methods and only 0.06 units lower than GPT-4. These\nresults were obtained despite the use of small and imbalanced training\ndatasets.\n","versions":"[{'version': 'v1', 'created': 'Mon, 15 Apr 2024 22:50:42 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 19:45:11 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Lompo', 'Boammani Aser', ''], ['Le', 'Thanh-Dung', ''], ['Jouvet', 'Philippe', ''], ['Noumeir', 'Rita', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'keyword embeddings', 'label': 'contextual Embedding'}, {'text': 'attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'context-driven learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2405.11874,"submitter":"Zhiyu Li","authors":"Qingchen Yu, Zifan Zheng, Shichao Song, Zhiyu Li, Feiyu Xiong, Bo\n  Tang, Ding Chen","title":"xFinder: Large Language Models as Automated Evaluators for Reliable\n  Evaluation","comments":"Accepted by ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The continuous advancement of large language models (LLMs) has brought\nincreasing attention to the critical issue of developing fair and reliable\nmethods for evaluating their performance. Particularly, the emergence of\ncheating phenomena, such as test set leakage and prompt format overfitting,\nposes significant challenges to the reliable evaluation of LLMs. As evaluation\nframeworks commonly use Regular Expression (RegEx) for answer extraction,\nmodels may adjust their responses to fit formats easily handled by RegEx.\nNevertheless, the key answer extraction module based on RegEx frequently\nsuffers from extraction errors. Furthermore, recent studies proposing\nfine-tuned LLMs as judge models for automated evaluation face challenges in\nterms of generalization ability and fairness. This paper comprehensively\nanalyzes the entire LLM evaluation chain and demonstrates that optimizing the\nkey answer extraction module improves extraction accuracy and enhances\nevaluation reliability. Our findings suggest that improving the key answer\nextraction module can lead to higher judgment accuracy and improved evaluation\nefficiency compared to the judge models. To address these issues, we propose\nxFinder, a novel evaluator for answer extraction and matching in LLM\nevaluation. As part of this process, we create a specialized dataset, the\n\\textbf{K}ey \\textbf{A}nswer \\textbf{F}inder (KAF) dataset, to ensure effective\nmodel training and evaluation. Generalization tests and real-world evaluations\nshow that the smallest xFinder model, with only 500 million parameters,\nachieves an average extraction accuracy of 93.42\\%. In contrast, RegEx accuracy\nin the best evaluation framework is 74.38\\%. The final judgment accuracy of\nxFinder reaches 97.61\\%, outperforming existing evaluation frameworks and judge\nmodels.\n","versions":"[{'version': 'v1', 'created': 'Mon, 20 May 2024 08:30:13 GMT'}, {'version': 'v2', 'created': 'Thu, 23 May 2024 07:00:45 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Feb 2025 11:04:02 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Yu', 'Qingchen', ''], ['Zheng', 'Zifan', ''], ['Song', 'Shichao', ''], ['Li', 'Zhiyu', ''], ['Xiong', 'Feiyu', ''], ['Tang', 'Bo', ''], ['Chen', 'Ding', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2405.14117,"submitter":"Chen Yuheng","authors":"Yuheng Chen, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao","title":"Knowledge Localization: Mission Not Accomplished? Enter Query\n  Localization!","comments":"ICLR 2025 Spotlight","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs) store extensive factual knowledge, but the\nmechanisms behind how they store and express this knowledge remain unclear. The\nKnowledge Neuron (KN) thesis is a prominent theory for explaining these\nmechanisms. This theory is based on the Knowledge Localization (KL) assumption,\nwhich suggests that a fact can be localized to a few knowledge storage units,\nnamely knowledge neurons.\n  However, this assumption has two limitations: first, it may be too rigid\nregarding knowledge storage, and second, it neglects the role of the attention\nmodule in knowledge expression.\n  In this paper, we first re-examine the KL assumption and demonstrate that its\nlimitations do indeed exist. To address these, we then present two new\nfindings, each targeting one of the limitations: one focusing on knowledge\nstorage and the other on knowledge expression. We summarize these findings as\n\\textbf{Query Localization} (QL) assumption and argue that the KL assumption\ncan be viewed as a simplification of the QL assumption. Based on QL assumption,\nwe further propose the Consistency-Aware KN modification method, which improves\nthe performance of knowledge modification, further validating our new\nassumption. We conduct 39 sets of experiments, along with additional\nvisualization experiments, to rigorously confirm our conclusions. Code is\navailable at https:\/\/github.com\/heng840\/KnowledgeLocalization.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 May 2024 02:44:12 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 12:29:11 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Chen', 'Yuheng', ''], ['Cao', 'Pengfei', ''], ['Chen', 'Yubo', ''], ['Liu', 'Kang', ''], ['Zhao', 'Jun', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'attention\\nmodule', 'label': 'Attention mechanism'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2405.15349,"submitter":"Deng Jingcheng","authors":"Jingcheng Deng, Zihao Wei, Liang Pang, Hanxing Ding, Huawei Shen,\n  Xueqi Cheng","title":"Everything is Editable: Extend Knowledge Editing to Unstructured Data in\n  Large Language Models","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent knowledge editing methods have primarily focused on modifying\nstructured knowledge in large language models. However, this task setting\noverlooks the fact that a significant portion of real-world knowledge is stored\nin an unstructured format, characterized by long-form content, noise, and a\ncomplex yet comprehensive nature. Techniques like \"local layer key-value\nstorage\" and \"term-driven optimization\", as used in previous methods like\nMEMIT, are not effective for handling unstructured knowledge. To address these\nchallenges, we propose a novel Unstructured Knowledge Editing method, namely\nUnKE, which extends previous assumptions in the layer dimension and token\ndimension. Firstly, in the layer dimension, we propose non-local block\nkey-value storage to replace local layer key-value storage, increasing the\nrepresentation ability of key-value pairs and incorporating attention layer\nknowledge. Secondly, in the token dimension, we replace \"term-driven\noptimization\" with \"cause-driven optimization\", which edits the last token\ndirectly while preserving context, avoiding the need to locate terms and\npreventing the loss of context information. Results on newly proposed\nunstructured knowledge editing dataset (UnKEBench) and traditional structured\ndatasets demonstrate that UnKE achieves remarkable performance, surpassing\nstrong baselines. In addition, UnKE has robust batch editing and sequential\nediting capabilities.\n","versions":"[{'version': 'v1', 'created': 'Fri, 24 May 2024 08:42:40 GMT'}, {'version': 'v2', 'created': 'Fri, 18 Oct 2024 04:32:49 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Feb 2025 03:33:47 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Deng', 'Jingcheng', ''], ['Wei', 'Zihao', ''], ['Pang', 'Liang', ''], ['Ding', 'Hanxing', ''], ['Shen', 'Huawei', ''], ['Cheng', 'Xueqi', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2405.20318,"submitter":"Zhijing Jin","authors":"Roberto Ceraolo, Dmitrii Kharlapenko, Ahmad Khan, Am\\'elie Reymond,\n  Rada Mihalcea, Bernhard Sch\\\"olkopf, Mrinmaya Sachan, Zhijing Jin","title":"Quriosity: Analyzing Human Questioning Behavior and Causal Inquiry\n  through Curiosity-Driven Queries","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG stat.ML","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Recent progress in Large Language Model (LLM) technology has changed our role\nin interacting with these models. Instead of primarily testing these models\nwith questions we already know answers to, we are now using them for queries\nwhere the answers are unknown to us, driven by human curiosity. This shift\nhighlights the growing need to understand curiosity-driven human questions -\nthose that are more complex, open-ended, and reflective of real-world needs. To\nthis end, we present Quriosity, a collection of 13.5K naturally occurring\nquestions from three diverse sources: human-to-search-engine queries,\nhuman-to-human interactions, and human-to-LLM conversations. Our comprehensive\ncollection enables a rich understanding of human curiosity across various\ndomains and contexts. Our analysis reveals a significant presence of causal\nquestions (up to 42%) in the dataset, for which we develop an iterative prompt\nimprovement framework to identify all causal queries and examine their unique\nlinguistic properties, cognitive complexity and source distribution. Our paper\npaves the way for future work on causal question identification and open-ended\nchatbot interactions.\n","versions":"[{'version': 'v1', 'created': 'Thu, 30 May 2024 17:55:28 GMT'}, {'version': 'v2', 'created': 'Thu, 24 Oct 2024 09:21:38 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 16:42:25 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Ceraolo', 'Roberto', ''], ['Kharlapenko', 'Dmitrii', ''], ['Khan', 'Ahmad', ''], ['Reymond', 'Am\u00e9lie', ''], ['Mihalcea', 'Rada', ''], ['Sch\u00f6lkopf', 'Bernhard', ''], ['Sachan', 'Mrinmaya', ''], ['Jin', 'Zhijing', '']]","extracted_entities":"[{'text': 'Large Language Model', 'label': 'Large Language Model'}, {'text': 'iterative prompt\\nimprovement framework', 'label': 'Prompting'}, {'text': 'open-ended\\nchatbot interactions', 'label': 'ChatGPT'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Model","similarity_score":1.0}
{"id":2406.00023,"submitter":"Jing Li","authors":"Jing Li, Zhijie Sun, Dachao Lin, Xuan He, Binfan Zheng, Yi Lin,\n  Rongqian Zhao, Xin Chen","title":"Expert-Token Resonance MoE: Bidirectional Routing with Efficiency\n  Affinity-Driven Active Selection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Mixture-of-Experts (MoE) architectures have emerged as a paradigm-shifting\napproach for large language models (LLMs), offering unprecedented computational\nefficiency. However, these architectures grapple with challenges of token\ndistribution imbalance and expert homogenization, impeding optimal semantic\ngeneralization. We propose a novel expert routing framework that incorporates:\n(1) An efficient routing mechanism with lightweight computation. (2) An\nadaptive bidirectional selection mechanism leveraging resonance between experts\nand tokens. (3) A module that determines the lower bounds of expert capacity\nbased on dynamic token distribution analysis, specifically designed to address\ndrop-and-pad strategies. It is also integrated with orthogonal feature\nextraction module and an optimized loss function for expert localization. This\nframework effectively reduces expert homogeneity while enhancing the\nperformance of the expert selection module. Additionally, we introduce a local\nexpert strategy that simultaneously improves load balancing and reduces network\ncommunication overhead. It achieves a 40\\% reduction in token processed by each\nexpert without compromising model convergence or efficacy. When coupled with\ncommunication optimizations, the training efficiency improvements of 5.4\\% to\n46.6\\% can be observed. After supervised fine-tuning, it exhibits performance\ngains of 9.7\\% to 14.1\\% across GDAD, GPQA, and TeleQnA benchmarks.\n","versions":"[{'version': 'v1', 'created': 'Fri, 24 May 2024 02:50:44 GMT'}, {'version': 'v2', 'created': 'Fri, 30 Aug 2024 11:32:48 GMT'}, {'version': 'v3', 'created': 'Wed, 26 Feb 2025 03:28:51 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Li', 'Jing', ''], ['Sun', 'Zhijie', ''], ['Lin', 'Dachao', ''], ['He', 'Xuan', ''], ['Zheng', 'Binfan', ''], ['Lin', 'Yi', ''], ['Zhao', 'Rongqian', ''], ['Chen', 'Xin', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2406.00034,"submitter":"Yinghao Zhu","authors":"Tianlong Wang, Xianfeng Jiao, Yinghao Zhu, Zhongzhi Chen, Yifan He, Xu\n  Chu, Junyi Gao, Yasha Wang, Liantao Ma","title":"Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement\n  Method for Diverse Hallucinations Categories","comments":"ACM TheWebConf 2025 Conference (WWW 2025) Research Track","journal-ref":null,"doi":"10.1145\/3696410.3714640","report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent studies have indicated that Large Language Models (LLMs) harbor an\ninherent understanding of truthfulness, yet often fail to consistently express\nit and generate false statements. This gap between \"knowing\" and \"telling\"\nposes a challenge for ensuring the truthfulness of generated content. Inspired\nby recent work on the practice of encoding human-interpretable concepts\nlinearly within large language models, we treat truthfulness as a specially\nlinearly encoded concept within LLMs, and introduce Adaptive Activation\nSteering (ACT), a tuning-free method that adaptively shifts LLM's activations\nin the \"truthful\" direction during inference. ACT addresses diverse categories\nof hallucinations by utilizing diverse truthfulness-related steering vectors\nand adjusting the steering intensity adaptively. Applied as an add-on across\nvarious models, ACT significantly improves truthfulness in LLaMA ($\\uparrow$\n142%), LLaMA2 ($\\uparrow$ 24%), Alpaca ($\\uparrow$ 36%), Vicuna ($\\uparrow$\n28%), LLaMA2-Chat ($\\uparrow$ 19%), and LLaMA3($\\uparrow$ 34%). Furthermore, we\nverify ACT's scalability across larger models (13B, 33B, 65B), underscoring the\nadaptability of ACT to large-scale language models. Our code is available at\nhttps:\/\/github.com\/tianlwang\/ACT.\n","versions":"[{'version': 'v1', 'created': 'Sun, 26 May 2024 21:39:53 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 14:07:05 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Wang', 'Tianlong', ''], ['Jiao', 'Xianfeng', ''], ['Zhu', 'Yinghao', ''], ['Chen', 'Zhongzhi', ''], ['He', 'Yifan', ''], ['Chu', 'Xu', ''], ['Gao', 'Junyi', ''], ['Wang', 'Yasha', ''], ['Ma', 'Liantao', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Alpaca', 'label': 'Llama'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2406.05127,"submitter":"Xiangtai Li Dr","authors":"Shengqiong Wu, Hao Fei, Xiangtai Li, Jiayi Ji, Hanwang Zhang, Tat-Seng\n  Chua, Shuicheng Yan","title":"Towards Semantic Equivalence of Tokenization in Multimodal LLM","comments":"ICLR-2025. The project page: https:\/\/chocowu.github.io\/SeTok-web\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multimodal Large Language Models (MLLMs) have demonstrated exceptional\ncapabilities in processing vision-language tasks. One of the crux of MLLMs lies\nin vision tokenization, which involves efficiently transforming input visual\nsignals into feature representations that are most beneficial for LLMs.\nHowever, existing vision tokenizers, essential for semantic alignment between\nvision and language, remain problematic. Existing methods aggressively fragment\nvisual input, corrupting the visual semantic integrity. To address this, this\npaper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),\nwhich groups visual features into semantic units via a dynamic clustering\nalgorithm, flexibly determining the number of tokens based on image complexity.\nThe resulting vision tokens effectively preserve semantic integrity and capture\nboth low-frequency and high-frequency visual features. The proposed MLLM\n(Setokim) equipped with SeTok significantly demonstrates superior performance\nacross various tasks, as evidenced by our experimental results. The project\npage is at https:\/\/chocowu.github.io\/SeTok-web\/.\n","versions":"[{'version': 'v1', 'created': 'Fri, 7 Jun 2024 17:55:43 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Jun 2024 17:35:45 GMT'}, {'version': 'v3', 'created': 'Wed, 9 Oct 2024 12:01:24 GMT'}, {'version': 'v4', 'created': 'Wed, 26 Feb 2025 02:55:53 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Wu', 'Shengqiong', ''], ['Fei', 'Hao', ''], ['Li', 'Xiangtai', ''], ['Ji', 'Jiayi', ''], ['Zhang', 'Hanwang', ''], ['Chua', 'Tat-Seng', ''], ['Yan', 'Shuicheng', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2406.05516,"submitter":"Hengguan Huang","authors":"Hengguan Huang, Xing Shen, Songtao Wang, Lingfa Meng, Dianbo Liu, Hao\n  Wang, Samir Bhatt","title":"Verbalized Probabilistic Graphical Modeling","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Human cognition excels at transcending sensory input and forming latent\nrepresentations that structure our understanding of the world. Although Large\nLanguage Models (LLMs) can produce chain-of-thought reasoning, they lack a\nprincipled framework to capture latent structures and model uncertainty,\nespecially in compositional reasoning tasks. We propose Verbalized\nProbabilistic Graphical Modeling (vPGM), a Bayesian prompting framework that\nguides LLMs to simulate key principles of Probabilistic Graphical Models (PGMs)\nin natural language. Unlike many traditional probabilistic methods requiring\nsubstantial domain expertise or specialized training, vPGM bypasses\nexpert-driven model design, making it well-suited for scenarios with limited\nassumptions or scarce data. We evaluated our model on several compositional\nreasoning tasks, both close-ended and open-ended. Our results indicate that the\nmodel effectively enhances confidence calibration and text generation quality.\n","versions":"[{'version': 'v1', 'created': 'Sat, 8 Jun 2024 16:35:31 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 13:56:16 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Huang', 'Hengguan', ''], ['Shen', 'Xing', ''], ['Wang', 'Songtao', ''], ['Meng', 'Lingfa', ''], ['Liu', 'Dianbo', ''], ['Wang', 'Hao', ''], ['Bhatt', 'Samir', '']]","extracted_entities":"[{'text': 'Large\\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'chain-of-thought reasoning', 'label': 'Chain of thought'}, {'text': 'Bayesian prompting', 'label': 'Prompting'}, {'text': 'vPGM', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Large\nLanguage Models","similarity_score":0.9664971828}
{"id":2406.09179,"submitter":"Qizhou Wang","authors":"Qizhou Wang, Bo Han, Puning Yang, Jianing Zhu, Tongliang Liu, Masashi\n  Sugiyama","title":"Towards Effective Evaluations and Comparisons for LLM Unlearning Methods","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The imperative to eliminate undesirable data memorization underscores the\nsignificance of machine unlearning for large language models (LLMs). Recent\nresearch has introduced a series of promising unlearning methods, notably\nboosting the practical significance of the field. Nevertheless, adopting a\nproper evaluation framework to reflect the true unlearning efficacy is also\nessential yet has not received adequate attention. This paper seeks to refine\nthe evaluation of LLM unlearning by addressing two key challenges -- a) the\nrobustness of evaluation metrics and b) the trade-offs between competing goals.\nThe first challenge stems from findings that current metrics are susceptible to\nvarious red teaming scenarios. It indicates that they may not reflect the true\nextent of knowledge retained by LLMs but rather tend to mirror superficial\nmodel behaviors, thus prone to attacks. We address this issue by devising and\nassessing a series of candidate metrics, selecting the most robust ones under\nvarious types of attacks. The second challenge arises from the conflicting\ngoals of eliminating unwanted knowledge while retaining those of others. This\ntrade-off between unlearning and retention often fails to conform the Pareto\nfrontier, rendering it subtle to compare the efficacy between methods that\nexcel only in either unlearning or retention. We handle this issue by proposing\na calibration method that can restore the original performance on non-targeted\ndata after unlearning, thereby allowing us to focus exclusively on assessing\nthe strength of unlearning. Our evaluation framework notably enhances the\neffectiveness when assessing and comparing various LLM unlearning methods,\nfurther allowing us to benchmark existing works, identify their proper\nhyper-parameters, and explore new tricks to enhance their practical efficacy.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Jun 2024 14:41:00 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 03:42:38 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Wang', 'Qizhou', ''], ['Han', 'Bo', ''], ['Yang', 'Puning', ''], ['Zhu', 'Jianing', ''], ['Liu', 'Tongliang', ''], ['Sugiyama', 'Masashi', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2406.1203,"submitter":"Yongting Zhang","authors":"Yongting Zhang, Lu Chen, Guodong Zheng, Yifeng Gao, Rui Zheng, Jinlan\n  Fu, Zhenfei Yin, Senjie Jin, Yu Qiao, Xuanjing Huang, Feng Zhao, Tao Gui,\n  Jing Shao","title":"SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision\n  Language Model","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The emergence of Vision Language Models (VLMs) has brought unprecedented\nadvances in understanding multimodal information. The combination of textual\nand visual semantics in VLMs is highly complex and diverse, making the safety\nalignment of these models challenging. Furthermore, due to the limited study on\nthe safety alignment of VLMs, there is a lack of large-scale, high-quality\ndatasets. To address these limitations, we propose a Safety Preference\nAlignment dataset for Vision Language Models named SPA-VL. In terms of breadth,\nSPA-VL covers 6 harmfulness domains, 13 categories, and 53 subcategories, and\ncontains 100,788 samples of the quadruple (question, image, chosen response,\nrejected response). In terms of depth, the responses are collected from 12\nopen-source (e.g., QwenVL) and closed-source (e.g., Gemini) VLMs to ensure\ndiversity. The construction of preference data is fully automated, and the\nexperimental results indicate that models trained with alignment techniques on\nthe SPA-VL dataset exhibit substantial improvements in harmlessness and\nhelpfulness while maintaining core capabilities. SPA-VL, as a large-scale,\nhigh-quality, and diverse dataset, represents a significant milestone in\nensuring that VLMs achieve both harmlessness and helpfulness.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Jun 2024 18:57:37 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 04:18:50 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Zhang', 'Yongting', ''], ['Chen', 'Lu', ''], ['Zheng', 'Guodong', ''], ['Gao', 'Yifeng', ''], ['Zheng', 'Rui', ''], ['Fu', 'Jinlan', ''], ['Yin', 'Zhenfei', ''], ['Jin', 'Senjie', ''], ['Qiao', 'Yu', ''], ['Huang', 'Xuanjing', ''], ['Zhao', 'Feng', ''], ['Gui', 'Tao', ''], ['Shao', 'Jing', '']]","extracted_entities":"[{'text': 'Vision Language Models', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'Vision Language Models', 'label': 'Large Language Model'}, {'text': 'QwenVL', 'label': 'Open-source LLMs'}, {'text': 'Gemini', 'label': 'Open-source LLMs'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Vision Language Models","similarity_score":0.5921616554}
{"id":2406.18849,"submitter":"Zhongqi Wang","authors":"Jie Zhang, Zhongqi Wang, Mengqi Lei, Zheng Yuan, Bei Yan, Shiguang\n  Shan, Xilin Chen","title":"Dysca: A Dynamic and Scalable Benchmark for Evaluating Perception\n  Ability of LVLMs","comments":"Accepted by ICLR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Currently many benchmarks have been proposed to evaluate the perception\nability of the Large Vision-Language Models (LVLMs). However, most benchmarks\nconduct questions by selecting images from existing datasets, resulting in the\npotential data leakage. Besides, these benchmarks merely focus on evaluating\nLVLMs on the realistic style images and clean scenarios, leaving the\nmulti-stylized images and noisy scenarios unexplored. In response to these\nchallenges, we propose a dynamic and scalable benchmark named Dysca for\nevaluating LVLMs by leveraging synthesis images. Specifically, we leverage\nStable Diffusion and design a rule-based method to dynamically generate novel\nimages, questions and the corresponding answers. We consider 51 kinds of image\nstyles and evaluate the perception capability in 20 subtasks. Moreover, we\nconduct evaluations under 4 scenarios (i.e., Clean, Corruption, Print Attacking\nand Adversarial Attacking) and 3 question types (i.e., Multi-choices,\nTrue-or-false and Free-form). Thanks to the generative paradigm, Dysca serves\nas a scalable benchmark for easily adding new subtasks and scenarios. A total\nof 24 advanced open-source LVLMs and 2 close-source LVLMs are evaluated on\nDysca, revealing the drawbacks of current LVLMs. The benchmark is released at\nhttps:\/\/github.com\/Robin-WZQ\/Dysca.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Jun 2024 02:40:35 GMT'}, {'version': 'v2', 'created': 'Fri, 26 Jul 2024 03:18:35 GMT'}, {'version': 'v3', 'created': 'Fri, 24 Jan 2025 13:58:49 GMT'}, {'version': 'v4', 'created': 'Mon, 24 Feb 2025 01:56:43 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Zhang', 'Jie', ''], ['Wang', 'Zhongqi', ''], ['Lei', 'Mengqi', ''], ['Yuan', 'Zheng', ''], ['Yan', 'Bei', ''], ['Shan', 'Shiguang', ''], ['Chen', 'Xilin', '']]","extracted_entities":"[{'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'Stable Diffusion', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Vision-Language Models","similarity_score":0.7742220759}
{"id":2406.19859,"submitter":"Zhi-Qi Cheng","authors":"Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Qi He, Wangmeng\n  Xiang, Hanyuan Chen, Jin-Peng Lan, Xianhui Lin, Kang Zhu, Bin Luo, Yifeng\n  Geng, Xuansong Xie, Alexander G. Hauptmann","title":"MetaDesigner: Advancing Artistic Typography Through AI-Driven,\n  User-Centric, and Multilingual WordArt Synthesis","comments":"Accepted by ICLR 2025, Project:\n  https:\/\/modelscope.cn\/studios\/WordArt\/WordArt","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.HC cs.MM","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  MetaDesigner introduces a transformative framework for artistic typography\nsynthesis, powered by Large Language Models (LLMs) and grounded in a\nuser-centric design paradigm. Its foundation is a multi-agent system comprising\nthe Pipeline, Glyph, and Texture agents, which collectively orchestrate the\ncreation of customizable WordArt, ranging from semantic enhancements to\nintricate textural elements. A central feedback mechanism leverages insights\nfrom both multimodal models and user evaluations, enabling iterative refinement\nof design parameters. Through this iterative process, MetaDesigner dynamically\nadjusts hyperparameters to align with user-defined stylistic and thematic\npreferences, consistently delivering WordArt that excels in visual quality and\ncontextual resonance. Empirical evaluations underscore the system's versatility\nand effectiveness across diverse WordArt applications, yielding outputs that\nare both aesthetically compelling and context-sensitive.\n","versions":"[{'version': 'v1', 'created': 'Fri, 28 Jun 2024 11:58:26 GMT'}, {'version': 'v2', 'created': 'Thu, 4 Jul 2024 15:47:40 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Feb 2025 20:28:02 GMT'}, {'version': 'v4', 'created': 'Thu, 27 Feb 2025 08:36:29 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['He', 'Jun-Yan', ''], ['Cheng', 'Zhi-Qi', ''], ['Li', 'Chenyang', ''], ['Sun', 'Jingdong', ''], ['He', 'Qi', ''], ['Xiang', 'Wangmeng', ''], ['Chen', 'Hanyuan', ''], ['Lan', 'Jin-Peng', ''], ['Lin', 'Xianhui', ''], ['Zhu', 'Kang', ''], ['Luo', 'Bin', ''], ['Geng', 'Yifeng', ''], ['Xie', 'Xuansong', ''], ['Hauptmann', 'Alexander G.', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2407.00047,"submitter":"Archit Patke","authors":"Archit Patke, Dhemath Reddy, Saurabh Jha, Haoran Qiu, Christian Pinto,\n  Chandra Narayanaswami, Zbigniew Kalbarczyk, Ravishankar Iyer","title":"Queue management for slo-oriented large language model serving","comments":null,"journal-ref":null,"doi":"10.1145\/3698038.369852","report-no":null,"categories":"cs.DC cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language model (LLM) serving is becoming an increasingly critical\nworkload for cloud providers. Existing LLM serving systems focus on interactive\nrequests, such as chatbots and coding assistants, with tight latency SLO\nrequirements. However, when such systems execute batch requests that have\nrelaxed SLOs along with interactive requests, it leads to poor multiplexing and\ninefficient resource utilization. To address these challenges, we propose QLM,\na queue management system for LLM serving. QLM maintains batch and interactive\nrequests across different models and SLOs in a request queue. Optimal ordering\nof the request queue is critical to maintain SLOs while ensuring high resource\nutilization. To generate this optimal ordering, QLM uses a Request Waiting Time\n(RWT) Estimator that estimates the waiting times for requests in the request\nqueue. These estimates are used by a global scheduler to orchestrate LLM\nServing Operations (LSOs) such as request pulling, request eviction, load\nbalancing, and model swapping. Evaluation on heterogeneous GPU devices and\nmodels with real-world LLM serving dataset shows that QLM improves SLO\nattainment by 40-90% and throughput by 20-400% while maintaining or improving\ndevice utilization compared to other state-of-the-art LLM serving systems.\nQLM's evaluation is based on the production requirements of a cloud provider.\nQLM is publicly available at https:\/\/www.github.com\/QLM-project\/QLM.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Jun 2024 21:17:34 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 17:54:13 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Patke', 'Archit', ''], ['Reddy', 'Dhemath', ''], ['Jha', 'Saurabh', ''], ['Qiu', 'Haoran', ''], ['Pinto', 'Christian', ''], ['Narayanaswami', 'Chandra', ''], ['Kalbarczyk', 'Zbigniew', ''], ['Iyer', 'Ravishankar', '']]","extracted_entities":"[{'text': 'Large language model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'chatbots', 'label': 'ChatGPT'}, {'text': 'coding assistants', 'label': 'ChatGPT'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language model","similarity_score":1.0}
{"id":2407.00075,"submitter":"Anton Xue","authors":"Anton Xue, Avishree Khare, Rajeev Alur, Surbhi Goel, Eric Wong","title":"Logicbreaks: A Framework for Understanding Subversion of Rule-based\n  Inference","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.CR cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We study how to subvert large language models (LLMs) from following\nprompt-specified rules. We first formalize rule-following as inference in\npropositional Horn logic, a mathematical system in which rules have the form\n\"if $P$ and $Q$, then $R$\" for some propositions $P$, $Q$, and $R$. Next, we\nprove that although small transformers can faithfully follow such rules,\nmaliciously crafted prompts can still mislead both theoretical constructions\nand models learned from data. Furthermore, we demonstrate that popular attack\nalgorithms on LLMs find adversarial prompts and induce attention patterns that\nalign with our theory. Our novel logic-based framework provides a foundation\nfor studying LLMs in rule-based settings, enabling a formal analysis of tasks\nlike logical reasoning and jailbreak attacks.\n","versions":"[{'version': 'v1', 'created': 'Fri, 21 Jun 2024 19:18:16 GMT'}, {'version': 'v2', 'created': 'Tue, 1 Oct 2024 20:42:41 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Feb 2025 19:08:08 GMT'}, {'version': 'v4', 'created': 'Thu, 27 Feb 2025 17:49:33 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Xue', 'Anton', ''], ['Khare', 'Avishree', ''], ['Alur', 'Rajeev', ''], ['Goel', 'Surbhi', ''], ['Wong', 'Eric', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'small transformers', 'label': 'Transformers'}, {'text': 'maliciously crafted prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'attention patterns', 'label': 'Attention mechanism'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2407.00468,"submitter":"Liang Chen","authors":"Jinsheng Huang, Liang Chen, Taian Guo, Fu Zeng, Yusheng Zhao, Bohan\n  Wu, Ye Yuan, Haozhe Zhao, Zhihui Guo, Yichi Zhang, Jingyang Yuan, Wei Ju,\n  Luchen Liu, Tianyu Liu, Baobao Chang, Ming Zhang","title":"MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and\n  Efficient Evaluation","comments":"18 pages, code released at https:\/\/github.com\/chenllliang\/MMEvalPro,\n  Homepage at https:\/\/mmevalpro.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Multimodal Models (LMMs) exhibit impressive cross-modal understanding\nand reasoning abilities, often assessed through multiple-choice questions\n(MCQs) that include an image, a question, and several options. However, many\nbenchmarks used for such evaluations suffer from systematic biases. Remarkably,\nLarge Language Models (LLMs) without any visual perception capabilities achieve\nnon-trivial performance, undermining the credibility of these evaluations. To\naddress this issue while maintaining the efficiency of MCQ evaluations, we\npropose MMEvalPro, a benchmark designed to avoid Type-I errors through a\ntrilogy evaluation pipeline and more rigorous metrics. For each original\nquestion from existing benchmarks, human annotators augment it by creating one\nperception question and one knowledge anchor question through a meticulous\nannotation process. MMEvalPro comprises $2,138$ question triplets, totaling\n$6,414$ distinct questions. Two-thirds of these questions are manually labeled\nby human experts, while the rest are sourced from existing benchmarks (MMMU,\nScienceQA, and MathVista). Compared with the existing benchmarks, our\nexperiments with the latest LLMs and LMMs demonstrate that MMEvalPro is more\nchallenging (the best LMM lags behind human performance by $31.73\\%$, compared\nto an average gap of $8.03\\%$ in previous benchmarks) and more trustworthy (the\nbest LLM trails the best LMM by $23.09\\%$, whereas the gap for previous\nbenchmarks is just $14.64\\%$). Our in-depth analysis explains the reason for\nthe large performance gap and justifies the trustworthiness of evaluation,\nunderscoring its significant potential for advancing future research.\n","versions":"[{'version': 'v1', 'created': 'Sat, 29 Jun 2024 15:28:45 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 15:10:56 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Huang', 'Jinsheng', ''], ['Chen', 'Liang', ''], ['Guo', 'Taian', ''], ['Zeng', 'Fu', ''], ['Zhao', 'Yusheng', ''], ['Wu', 'Bohan', ''], ['Yuan', 'Ye', ''], ['Zhao', 'Haozhe', ''], ['Guo', 'Zhihui', ''], ['Zhang', 'Yichi', ''], ['Yuan', 'Jingyang', ''], ['Ju', 'Wei', ''], ['Liu', 'Luchen', ''], ['Liu', 'Tianyu', ''], ['Chang', 'Baobao', ''], ['Zhang', 'Ming', '']]","extracted_entities":"[{'text': 'Large Multimodal Models', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ScienceQA', 'label': 'Open-source LLMs'}, {'text': 'LMMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2407.02936,"submitter":"Zike Yuan","authors":"Zike Yuan, Ming Liu, Hui Wang, Bing Qin","title":"GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large\n  Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Evaluating the graph comprehension and reasoning abilities of Large Language\nModels (LLMs) is challenging and often incomplete. Existing benchmarks focus\nprimarily on pure graph understanding, lacking a comprehensive evaluation\nacross all graph types and detailed capability definitions. This paper presents\nGraCoRe, a benchmark for systematically assessing LLMs' graph comprehension and\nreasoning. GraCoRe uses a three-tier hierarchical taxonomy to categorize and\ntest models on pure graph and heterogeneous graphs, subdividing capabilities\ninto 10 distinct areas tested through 19 tasks. Our benchmark includes 11\ndatasets with 5,140 graphs of varying complexity. We evaluate four\nclosed-source and eight open-source LLMs, conducting thorough analyses from\nboth ability and task perspectives. Key findings reveal that OpenAI o1 model\nhas amazing comprehension and reasoning capabilities, semantic enrichment\nenhances reasoning performance, node ordering impacts task success, and the\nability to process longer texts does not necessarily improve graph\ncomprehension or reasoning.GraCoRe is open-sourced at\nhttps:\/\/github.com\/ZIKEYUAN\/GraCoRe\n","versions":"[{'version': 'v1', 'created': 'Wed, 3 Jul 2024 09:12:38 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 09:17:32 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Yuan', 'Zike', ''], ['Liu', 'Ming', ''], ['Wang', 'Hui', ''], ['Qin', 'Bing', '']]","extracted_entities":"[{'text': 'Large Language\\nModels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'semantic enrichment', 'label': 'Embedding'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language\nModels","similarity_score":0.9664971828}
{"id":2407.06172,"submitter":"Jin Peng Zhou","authors":"Jin Peng Zhou, Christian K. Belardi, Ruihan Wu, Travis Zhang, Carla P.\n  Gomes, Wen Sun, Kilian Q. Weinberger","title":"On Speeding Up Language Model Evaluation","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Developing prompt-based methods with Large Language Models (LLMs) requires\nmaking numerous decisions, which give rise to a combinatorial search problem\nover hyper-parameters. This exhaustive evaluation can be time-consuming and\ncostly. In this paper, we propose an $\\textit{adaptive}$ approach to explore\nthis space. We are exploiting the fact that often only few samples are needed\nto identify clearly superior or inferior settings, and that many evaluation\ntests are highly correlated. We lean on multi-armed bandits to sequentially\nidentify the next (method, validation sample)-pair to evaluate and utilize\nlow-rank matrix factorization to fill in missing evaluations. We carefully\nassess the efficacy of our approach on several competitive benchmark problems\nand show that it can identify the top-performing method using only 5-15% of the\ntypical resources -- resulting in 85-95% LLM cost savings. Our code is\navailable at https:\/\/github.com\/kilian-group\/banditeval.\n","versions":"[{'version': 'v1', 'created': 'Mon, 8 Jul 2024 17:48:42 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Aug 2024 22:31:35 GMT'}, {'version': 'v3', 'created': 'Wed, 26 Feb 2025 21:53:59 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Zhou', 'Jin Peng', ''], ['Belardi', 'Christian K.', ''], ['Wu', 'Ruihan', ''], ['Zhang', 'Travis', ''], ['Gomes', 'Carla P.', ''], ['Sun', 'Wen', ''], ['Weinberger', 'Kilian Q.', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2407.13522,"submitter":"Abhishek Kumar Singh","authors":"Abhishek Kumar Singh, Vishwajeet kumar, Rudra Murthy, Jaydeep Sen,\n  Ashish Mittal, Ganesh Ramakrishnan","title":"INDIC QA BENCHMARK: A Multilingual Benchmark to Evaluate Question\n  Answering capability of LLMs for Indic Languages","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/","abstract":"  Large Language Models (LLMs) perform well on unseen tasks in English, but\ntheir abilities in non English languages are less explored due to limited\nbenchmarks and training data. To bridge this gap, we introduce the Indic QA\nBenchmark, a large dataset for context grounded question answering in 11 major\nIndian languages, covering both extractive and abstractive tasks. Evaluations\nof multilingual LLMs, including instruction finetuned versions, revealed weak\nperformance in low resource languages due to a strong English language bias in\ntheir training data. We also investigated the Translate Test paradigm,where\ninputs are translated to English for processing and the results are translated\nback into the source language for output. This approach outperformed\nmultilingual LLMs, particularly in low resource settings. By releasing Indic\nQA, we aim to promote further research into LLMs question answering\ncapabilities in low resource languages. This benchmark offers a critical\nresource to address existing limitations and foster multilingual understanding.\n","versions":"[{'version': 'v1', 'created': 'Thu, 18 Jul 2024 13:57:16 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 05:37:48 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Singh', 'Abhishek Kumar', ''], ['kumar', 'Vishwajeet', ''], ['Murthy', 'Rudra', ''], ['Sen', 'Jaydeep', ''], ['Mittal', 'Ashish', ''], ['Ramakrishnan', 'Ganesh', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2407.14845,"submitter":"Arun Verma","authors":"Ze Yu Zhang, Arun Verma, Finale Doshi-Velez, Bryan Kian Hsiang Low","title":"Understanding the Relationship between Prompts and Response Uncertainty\n  in Large Language Models","comments":"22 pages, Preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs) are widely used in decision-making, but their\nreliability, especially in critical tasks like healthcare, is not\nwell-established. Therefore, understanding how LLMs reason and make decisions\nis crucial for their safe deployment. This paper investigates how the\nuncertainty of responses generated by LLMs relates to the information provided\nin the input prompt. Leveraging the insight that LLMs learn to infer latent\nconcepts during pretraining, we propose a prompt-response concept model that\nexplains how LLMs generate responses and helps understand the relationship\nbetween prompts and response uncertainty. We show that the uncertainty\ndecreases as the prompt's informativeness increases, similar to epistemic\nuncertainty. Our detailed experimental results on real-world datasets validate\nour proposed model.\n","versions":"[{'version': 'v1', 'created': 'Sat, 20 Jul 2024 11:19:58 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Aug 2024 02:23:12 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 17:06:21 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Zhang', 'Ze Yu', ''], ['Verma', 'Arun', ''], ['Doshi-Velez', 'Finale', ''], ['Low', 'Bryan Kian Hsiang', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompt', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2407.15073,"submitter":"Hao Duong Le","authors":"Hao Duong Le, Xin Xia and Zhang Chen","title":"Multi-Agent Causal Discovery Using Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Causal discovery aims to identify causal relationships between variables and\nis a critical research area in machine learning. Traditional methods focus on\nstatistical or machine learning algorithms to uncover causal links from\nstructured data, often overlooking the valuable contextual information provided\nby metadata. Large language models (LLMs) have shown promise in creating\nunified causal discovery frameworks by incorporating both structured data and\nmetadata. However, their potential in multi-agent settings remains largely\nunexplored. To address this gap, we introduce the Multi-Agent Causal Discovery\nFramework (MAC), which consists of two key modules: the Debate-Coding Module\n(DCM) and the Meta-Debate Module (MDM). The DCM begins with a multi-agent\ndebating and coding process, where agents use both structured data and metadata\nto collaboratively select the most suitable statistical causal discovery (SCD)\nmethod. The selected SCD is then applied to the structured data to generate an\ninitial causal graph. This causal graph is transformed into causal metadata\nthrough the Meta Fusion mechanism. With all the metadata, MDM then refines the\ncausal structure by leveraging a multi-agent debating framework. Extensive\nexperiments across five datasets demonstrate that MAC outperforms both\ntraditional statistical causal discovery methods and existing LLM-based\napproaches, achieving state-of-the-art performance.\n","versions":"[{'version': 'v1', 'created': 'Sun, 21 Jul 2024 06:21:47 GMT'}, {'version': 'v2', 'created': 'Thu, 10 Oct 2024 02:48:42 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 02:47:56 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Le', 'Hao Duong', ''], ['Xia', 'Xin', ''], ['Chen', 'Zhang', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Meta Fusion mechanism', 'label': 'Embedding'}, {'text': 'MDM', 'label': 'LLM'}, {'text': 'MAC', 'label': 'LLM'}, {'text': 'existing LLM-based\\napproaches', 'label': 'LLM'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2408.07413,"submitter":"Chenhui Hu","authors":"Chenhui Hu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao","title":"Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge\n  Editing for Large Language Models","comments":"To be published in AAAI 2025 (Oral)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Knowledge editing aims to update outdated or incorrect knowledge in large\nlanguage models (LLMs). However, current knowledge editing methods have limited\nscalability for lifelong editing. This study explores the fundamental reason\nwhy knowledge editing fails in lifelong editing. We begin with the closed-form\nsolution derived from linear associative memory, which underpins\nstate-of-the-art knowledge editing methods. We extend the solution from single\nediting to lifelong editing, and through rigorous mathematical derivation,\nidentify an interference term in the final solution, suggesting that editing\nknowledge may impact irrelevant knowledge. Further analysis of the interference\nterm reveals a close relationship with superposition between knowledge\nrepresentations. When knowledge superposition does not exist in language\nmodels, the interference term vanishes, allowing for lossless knowledge\nediting. Experiments across numerous language models reveal that knowledge\nsuperposition is universal, exhibiting high kurtosis, zero mean, and\nheavy-tailed distributions with clear scaling laws. Ultimately, by combining\ntheory and experiments, we demonstrate that knowledge superposition is the\nfundamental reason for the failure of lifelong editing. Moreover, this is the\nfirst study to investigate knowledge editing from the perspective of\nsuperposition and provides a comprehensive observation of superposition across\nnumerous real-world language models. Code available at\nhttps:\/\/github.com\/ChenhuiHu\/knowledge_in_superposition.\n","versions":"[{'version': 'v1', 'created': 'Wed, 14 Aug 2024 09:43:32 GMT'}, {'version': 'v2', 'created': 'Sun, 12 Jan 2025 06:07:15 GMT'}, {'version': 'v3', 'created': 'Wed, 26 Feb 2025 09:13:06 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Hu', 'Chenhui', ''], ['Cao', 'Pengfei', ''], ['Chen', 'Yubo', ''], ['Liu', 'Kang', ''], ['Zhao', 'Jun', '']]","extracted_entities":"[{'text': 'large\\nlanguage models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'clear scaling laws', 'label': 'Scaling law'}]","assigned_concept":"Large Language Model","matched_keyword":"large\nlanguage models","similarity_score":0.9664971828}
{"id":2408.10573,"submitter":"Junhao Chen","authors":"Junhao Chen and Bowen Wang and Zhouqiang Jiang and Yuta Nakashima","title":"Putting People in LLMs' Shoes: Generating Better Answers via Question\n  Rewriter","comments":"7 pages, 4 figures, 5 tables and accepted at AAAI 2025 Main\n  Conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) have demonstrated significant capabilities,\nparticularly in the domain of question answering (QA). However, their\neffectiveness in QA is often undermined by the vagueness of user questions. To\naddress this issue, we introduce single-round instance-level prompt\noptimization, referred to as question rewriter. By enhancing the\nintelligibility of human questions for black-box LLMs, our question rewriter\nimproves the quality of generated answers. The rewriter is optimized using\ndirect preference optimization based on feedback collected from automatic\ncriteria for evaluating generated answers; therefore, its training does not\nrequire costly human annotations. The experiments across multiple black-box\nLLMs and long-form question answering (LFQA) datasets demonstrate the efficacy\nof our method. This paper provides a practical framework for training question\nrewriters and sets a precedent for future explorations in prompt optimization\nwithin LFQA tasks. Code is available at\nhttps:\/\/github.com\/3244we\/Question-Rewriter.\n","versions":"[{'version': 'v1', 'created': 'Tue, 20 Aug 2024 06:24:47 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 03:13:27 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Chen', 'Junhao', ''], ['Wang', 'Bowen', ''], ['Jiang', 'Zhouqiang', ''], ['Nakashima', 'Yuta', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'single-round instance-level prompt\\noptimization', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'direct preference optimization', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompt optimization', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2408.10593,"submitter":"Eui Jun Hwang","authors":"Eui Jun Hwang, Sukmin Cho, Junmyeong Lee, Jong C. Park","title":"An Efficient Sign Language Translation Using Spatial Configuration and\n  Motion Dynamics with LLMs","comments":"Accepted to NAACL 2025 main","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Gloss-free Sign Language Translation (SLT) converts sign videos directly into\nspoken language sentences without relying on glosses. Recently, Large Language\nModels (LLMs) have shown remarkable translation performance in gloss-free\nmethods by harnessing their powerful natural language generation capabilities.\nHowever, these methods often rely on domain-specific fine-tuning of visual\nencoders to achieve optimal results. By contrast, this paper emphasizes the\nimportance of capturing the spatial configurations and motion dynamics inherent\nin sign language. With this in mind, we introduce Spatial and Motion-based Sign\nLanguage Translation (SpaMo), a novel LLM-based SLT framework. The core idea of\nSpaMo is simple yet effective. We first extract spatial and motion features\nusing off-the-shelf visual encoders and then input these features into an LLM\nwith a language prompt. Additionally, we employ a visual-text alignment process\nas a warm-up before the SLT supervision. Our experiments demonstrate that SpaMo\nachieves state-of-the-art performance on two popular datasets, PHOENIX14T and\nHow2Sign.\n","versions":"[{'version': 'v1', 'created': 'Tue, 20 Aug 2024 07:10:40 GMT'}, {'version': 'v2', 'created': 'Sun, 15 Dec 2024 06:18:53 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 06:04:45 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Hwang', 'Eui Jun', ''], ['Cho', 'Sukmin', ''], ['Lee', 'Junmyeong', ''], ['Park', 'Jong C.', '']]","extracted_entities":"[{'text': 'Large Language\\nModels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'domain-specific fine-tuning', 'label': 'Fine-tuning'}, {'text': 'language prompt', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language\nModels","similarity_score":0.9664971828}
{"id":2408.11843,"submitter":"Ruizhe Chen","authors":"Ruizhe Chen, Yichen Li, Jianfei Yang, Joey Tianyi Zhou, Jian Wu,\n  Zuozhu Liu","title":"Identifying and Mitigating Social Bias Knowledge in Language Models","comments":"NAACL 2025 Findings. arXiv admin note: substantial text overlap with\n  arXiv:2405.09341","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Generating fair and accurate predictions plays a pivotal role in deploying\nlarge language models (LLMs) in the real world. However, existing debiasing\nmethods inevitably generate unfair or incorrect predictions as they are\ndesigned and evaluated to achieve parity across different social groups but\nleave aside individual commonsense facts, resulting in modified knowledge that\nelicits unreasonable or undesired predictions. In this paper, we first\nestablish a new bias mitigation benchmark, BiaScope, which systematically\nassesses performance by leveraging newly constructed datasets and metrics on\nknowledge retention and generalization. Then, we propose a novel debiasing\napproach, Fairness Stamp (FAST), which enables fine-grained calibration of\nindividual social biases. FAST identifies the decisive layer responsible for\nstoring social biases and then calibrates its outputs by integrating a small\nmodular network, considering both bias mitigation and knowledge-preserving\ndemands. Comprehensive experiments demonstrate that FAST surpasses\nstate-of-the-art baselines with superior debiasing performance while not\ncompromising the overall model capability for knowledge retention and\ndownstream predictions. This highlights the potential of fine-grained debiasing\nstrategies to achieve fairness in LLMs.\n","versions":"[{'version': 'v1', 'created': 'Wed, 7 Aug 2024 17:14:58 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 10:11:06 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Chen', 'Ruizhe', ''], ['Li', 'Yichen', ''], ['Yang', 'Jianfei', ''], ['Zhou', 'Joey Tianyi', ''], ['Wu', 'Jian', ''], ['Liu', 'Zuozhu', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'fine-grained calibration', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2408.13704,"submitter":"Yicheng Wang","authors":"Yicheng Wang, Jiayi Yuan, Yu-Neng Chuang, Zhuoer Wang, Yingchi Liu,\n  Mark Cusick, Param Kulkarni, Zhengping Ji, Yasser Ibrahim, Xia Hu","title":"DHP Benchmark: Are LLMs Good NLG Evaluators?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) are increasingly serving as evaluators in\nNatural Language Generation (NLG) tasks; this is often referred to as\n``LLM-as-a-judge'' paradigm. However, the capabilities of LLMs in evaluating\nNLG quality remain underexplored. Current studies depend on human assessments\nand simple metrics that fail to capture the discernment of LLMs across diverse\nNLG tasks. To address this gap, we propose the Discernment of Hierarchical\nPerturbation (DHP) benchmarking framework, which provides quantitative\ndiscernment scores for LLMs. This framework leverages hierarchically perturbed\ntext data and statistical tests to systematically measure the NLG evaluation\ncapabilities of LLMs. We re-established six evaluation datasets for this\nbenchmark, covering four NLG tasks: Summarization, Story Completion, Question\nAnswering, and Translation. Our comprehensive benchmarking of five major LLM\nfamilies provides critical insight into their strengths and limitations as NLG\nevaluators. Our dataset is available at\nhttps:\/\/huggingface.co\/datasets\/YCWANGVINCE\/DHP_Benchmark.\n","versions":"[{'version': 'v1', 'created': 'Sun, 25 Aug 2024 02:01:38 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 01:51:06 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Wang', 'Yicheng', ''], ['Yuan', 'Jiayi', ''], ['Chuang', 'Yu-Neng', ''], ['Wang', 'Zhuoer', ''], ['Liu', 'Yingchi', ''], ['Cusick', 'Mark', ''], ['Kulkarni', 'Param', ''], ['Ji', 'Zhengping', ''], ['Ibrahim', 'Yasser', ''], ['Hu', 'Xia', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2403.07066,"submitter":"Benedikt Maier","authors":"Philip Harris, Michael Kagan, Jeffrey Krupa, Benedikt Maier, Nathaniel\n  Woodward","title":"Re-Simulation-based Self-Supervised Learning for Pre-Training Foundation\n  Models","comments":"14 pages, 8 figures","journal-ref":"Phys. Rev. D 111 (2025) 3, 032010","doi":"10.1103\/PhysRevD.111.032010","report-no":null,"categories":"hep-ph cs.LG hep-ex","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Self-Supervised Learning (SSL) is at the core of training modern large\nmachine learning models, providing a scheme for learning powerful\nrepresentations that can be used in a variety of downstream tasks. However, SSL\nstrategies must be adapted to the type of training data and downstream tasks\nrequired. We propose RS3L (\"Re-simulation-based self-supervised representation\nlearning\"), a novel simulation-based SSL strategy that employs a method of\nre-simulation to drive data augmentation for contrastive learning in the\nphysical sciences, particularly, in fields that rely on stochastic simulators.\nBy intervening in the middle of the simulation process and re-running\nsimulation components downstream of the intervention, we generate multiple\nrealizations of an event, thus producing a set of augmentations covering all\nphysics-driven variations available in the simulator. Using experiments from\nhigh-energy physics, we explore how this strategy may enable the development of\na foundation model; we show how RS3L pre-training enables powerful performance\nin downstream tasks such as discrimination of a variety of objects and\nuncertainty mitigation. In addition to our results, we make the RS3L dataset\npublicly available for further studies on how to improve SSL strategies.\n","versions":"[{'version': 'v1', 'created': 'Mon, 11 Mar 2024 18:00:47 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 00:08:55 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Harris', 'Philip', ''], ['Kagan', 'Michael', ''], ['Krupa', 'Jeffrey', ''], ['Maier', 'Benedikt', ''], ['Woodward', 'Nathaniel', '']]","extracted_entities":"[{'text': 'Self-Supervised Learning', 'label': 'Few-shot Learning'}, {'text': 'foundation model', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation model","similarity_score":1.0}
{"id":2405.17842,"submitter":"Akio Hayakawa","authors":"Akio Hayakawa, Masato Ishii, Takashi Shibuya, Yuki Mitsufuji","title":"MMDisCo: Multi-Modal Discriminator-Guided Cooperative Diffusion for\n  Joint Audio and Video Generation","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG cs.MM cs.SD eess.AS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This study aims to construct an audio-video generative model with minimal\ncomputational cost by leveraging pre-trained single-modal generative models for\naudio and video. To achieve this, we propose a novel method that guides\nsingle-modal models to cooperatively generate well-aligned samples across\nmodalities. Specifically, given two pre-trained base diffusion models, we train\na lightweight joint guidance module to adjust scores separately estimated by\nthe base models to match the score of joint distribution over audio and video.\nWe show that this guidance can be computed using the gradient of the optimal\ndiscriminator, which distinguishes real audio-video pairs from fake ones\nindependently generated by the base models. Based on this analysis, we\nconstruct a joint guidance module by training this discriminator. Additionally,\nwe adopt a loss function to stabilize the discriminator's gradient and make it\nwork as a noise estimator, as in standard diffusion models. Empirical\nevaluations on several benchmark datasets demonstrate that our method improves\nboth single-modal fidelity and multimodal alignment with relatively few\nparameters. The code is available at: https:\/\/github.com\/SonyResearch\/MMDisCo.\n","versions":"[{'version': 'v1', 'created': 'Tue, 28 May 2024 05:43:03 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 09:34:26 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Hayakawa', 'Akio', ''], ['Ishii', 'Masato', ''], ['Shibuya', 'Takashi', ''], ['Mitsufuji', 'Yuki', '']]","extracted_entities":"[{'text': 'base models', 'label': 'Foundation Model'}, {'text': 'base models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"base models","similarity_score":0.5099614859}
{"id":2406.04508,"submitter":"Dujian Ding","authors":"Dujian Ding, Bicheng Xu, Laks V.S. Lakshmanan","title":"OCCAM: Towards Cost-Efficient and Accuracy-Aware Classification\n  Inference","comments":"ICLR 2025 (main conference)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Classification tasks play a fundamental role in various applications,\nspanning domains such as healthcare, natural language processing and computer\nvision. With the growing popularity and capacity of machine learning models,\npeople can easily access trained classifiers as a service online or offline.\nHowever, model use comes with a cost and classifiers of higher capacity (such\nas large foundation models) usually incur higher inference costs. To harness\nthe respective strengths of different classifiers, we propose a principled\napproach, OCCAM, to compute the best classifier assignment strategy over\nclassification queries (termed as the optimal model portfolio) so that the\naggregated accuracy is maximized, under user-specified cost budgets. Our\napproach uses an unbiased and low-variance accuracy estimator and effectively\ncomputes the optimal solution by solving an integer linear programming problem.\nOn a variety of real-world datasets, OCCAM achieves 40% cost reduction with\nlittle to no accuracy drop.\n","versions":"[{'version': 'v1', 'created': 'Thu, 6 Jun 2024 21:05:39 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 03:15:20 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Ding', 'Dujian', ''], ['Xu', 'Bicheng', ''], ['Lakshmanan', 'Laks V. S.', '']]","extracted_entities":"[{'text': 'large foundation models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"large foundation models","similarity_score":0.8749241829}
{"id":2104.05914,"submitter":"Yang Li","authors":"Yang Li, Di Wang, and Jos\\'e M. F. Moura","title":"GSA-Forecaster: Forecasting Graph-Based Time-Dependent Data with Graph\n  Sequence Attention","comments":null,"journal-ref":"ACM Transactions on Knowledge Discovery from Data (TKDD), 2025","doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Forecasting graph-based, time-dependent data has broad practical applications\nbut presents challenges. Effective models must capture both spatial and\ntemporal dependencies in the data, while also incorporating auxiliary\ninformation to enhance prediction accuracy. In this paper, we identify\nlimitations in current state-of-the-art models regarding temporal dependency\nhandling. To overcome this, we introduce GSA-Forecaster, a new deep learning\nmodel designed for forecasting in graph-based, time-dependent contexts.\nGSA-Forecaster utilizes graph sequence attention, a new attention mechanism\nproposed in this paper, to effectively manage temporal dependencies.\nGSA-Forecaster integrates the data's graph structure directly into its\narchitecture, addressing spatial dependencies. Additionally, it incorporates\nauxiliary information to refine its predictions further. We validate its\nperformance using real-world graph-based, time-dependent datasets, where it\ndemonstrates superior effectiveness compared to existing state-of-the-art\nmodels.\n","versions":"[{'version': 'v1', 'created': 'Tue, 13 Apr 2021 03:19:10 GMT'}, {'version': 'v2', 'created': 'Sat, 16 Oct 2021 05:13:33 GMT'}, {'version': 'v3', 'created': 'Mon, 29 Aug 2022 17:10:07 GMT'}, {'version': 'v4', 'created': 'Thu, 27 Feb 2025 14:22:25 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Li', 'Yang', ''], ['Wang', 'Di', ''], ['Moura', 'Jos\u00e9 M. F.', '']]","extracted_entities":"[{'text': 'graph sequence attention', 'label': 'Attention mechanism'}, {'text': 'new attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"new attention mechanism","similarity_score":0.9210948348}
{"id":2305.18564,"submitter":"Amru Hussein","authors":"Hind Al Baba, Bilal Al Taki, Amru Hussein","title":"Remark on the local well-posedness of compressible non-Newtonian fluids\n  with initial vacuum","comments":"17 pages","journal-ref":null,"doi":"10.1007\/s00021-024-00901-3","report-no":null,"categories":"math.AP math-ph math.MP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We discuss in this short note the local-in-time strong well-posedness of the\ncompressible Navier-Stokes system for non-Newtonian fluids on the three\ndimensional torus. We show that the result established recently by Kalousek,\nM\\'{a}cha, and Ne\\v{c}asova in \\doi{10.1007\/s00208-021-02301-8} can be extended\nto the case where vanishing density is allowed initially. Our proof builds on\nthe framework developed by Cho, Choe, and Kim in\n\\doi{10.1016\/j.matpur.2003.11.004} for compressible Navier-Stokes equations in\nthe case of Newtonian fluids. To adapt their method, special attention is given\nto the elliptic regularity of a challenging nonlinear elliptic system. We show\nparticular results in this direction, however, the main result of this paper is\nproven in the general case when elliptic $W^{2,p}$-regularity is imposed as an\nassumption. Also, we give a finite time blow-up criterion.\n","versions":"[{'version': 'v1', 'created': 'Mon, 29 May 2023 18:54:32 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Jun 2023 10:38:08 GMT'}, {'version': 'v3', 'created': 'Mon, 8 Apr 2024 09:15:17 GMT'}, {'version': 'v4', 'created': 'Wed, 26 Feb 2025 14:33:14 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Baba', 'Hind Al', ''], ['Taki', 'Bilal Al', ''], ['Hussein', 'Amru', '']]","extracted_entities":"[{'text': 'special attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"special attention","similarity_score":0.6752905846}
{"id":2309.11523,"submitter":"Qihang Fan","authors":"Qihang Fan, Huaibo Huang, Mingrui Chen, Hongmin Liu and Ran He","title":"RMT: Retentive Networks Meet Vision Transformers","comments":"The paper is accepted by CVPR2024. Code is available at\n  https:\/\/github.com\/qhfan\/RMT","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Vision Transformer (ViT) has gained increasing attention in the computer\nvision community in recent years. However, the core component of ViT,\nSelf-Attention, lacks explicit spatial priors and bears a quadratic\ncomputational complexity, thereby constraining the applicability of ViT. To\nalleviate these issues, we draw inspiration from the recent Retentive Network\n(RetNet) in the field of NLP, and propose RMT, a strong vision backbone with\nexplicit spatial prior for general purposes. Specifically, we extend the\nRetNet's temporal decay mechanism to the spatial domain, and propose a spatial\ndecay matrix based on the Manhattan distance to introduce the explicit spatial\nprior to Self-Attention. Additionally, an attention decomposition form that\nadeptly adapts to explicit spatial prior is proposed, aiming to reduce the\ncomputational burden of modeling global information without disrupting the\nspatial decay matrix. Based on the spatial decay matrix and the attention\ndecomposition form, we can flexibly integrate explicit spatial prior into the\nvision backbone with linear complexity. Extensive experiments demonstrate that\nRMT exhibits exceptional performance across various vision tasks. Specifically,\nwithout extra training data, RMT achieves **84.8%** and **86.1%** top-1 acc on\nImageNet-1k with **27M\/4.5GFLOPs** and **96M\/18.2GFLOPs**. For downstream\ntasks, RMT achieves **54.5** box AP and **47.2** mask AP on the COCO detection\ntask, and **52.8** mIoU on the ADE20K semantic segmentation task. Code is\navailable at https:\/\/github.com\/qhfan\/RMT\n","versions":"[{'version': 'v1', 'created': 'Wed, 20 Sep 2023 00:57:48 GMT'}, {'version': 'v2', 'created': 'Wed, 11 Oct 2023 14:51:59 GMT'}, {'version': 'v3', 'created': 'Fri, 27 Oct 2023 15:30:06 GMT'}, {'version': 'v4', 'created': 'Sat, 4 Nov 2023 04:55:31 GMT'}, {'version': 'v5', 'created': 'Sat, 2 Dec 2023 06:23:09 GMT'}, {'version': 'v6', 'created': 'Thu, 27 Feb 2025 03:14:35 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Fan', 'Qihang', ''], ['Huang', 'Huaibo', ''], ['Chen', 'Mingrui', ''], ['Liu', 'Hongmin', ''], ['He', 'Ran', '']]","extracted_entities":"[{'text': 'Self-Attention', 'label': 'Attention mechanism'}, {'text': 'RMT', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'temporal decay mechanism', 'label': 'Attention mechanism'}, {'text': 'Self-Attention', 'label': 'Attention mechanism'}, {'text': 'RMT', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'RMT', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'RMT', 'label': 'Generative Pre-trained Transformer (GPT)'}]","assigned_concept":"Attention mechanism","matched_keyword":"Self-Attention","similarity_score":0.731767118}
{"id":2312.13509,"submitter":"Youssef Mourchid","authors":"Youssef Mourchid, Rim Slama","title":"MR-STGN: Multi-Residual Spatio Temporal Graph Network Using Attention\n  Fusion for Patient Action Assessment","comments":null,"journal-ref":null,"doi":"10.1109\/MMSP59012.2023.10337711","report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Accurate assessment of patient actions plays a crucial role in healthcare as\nit contributes significantly to disease progression monitoring and treatment\neffectiveness. However, traditional approaches to assess patient actions often\nrely on manual observation and scoring, which are subjective and\ntime-consuming. In this paper, we propose an automated approach for patient\naction assessment using a Multi-Residual Spatio Temporal Graph Network\n(MR-STGN) that incorporates both angular and positional 3D skeletons. The\nMR-STGN is specifically designed to capture the spatio-temporal dynamics of\npatient actions. It achieves this by integrating information from multiple\nresidual layers, with each layer extracting features at distinct levels of\nabstraction. Furthermore, we integrate an attention fusion mechanism into the\nnetwork, which facilitates the adaptive weighting of various features. This\nempowers the model to concentrate on the most pertinent aspects of the\npatient's movements, offering precise instructions regarding specific body\nparts or movements that require attention. Ablation studies are conducted to\nanalyze the impact of individual components within the proposed model. We\nevaluate our model on the UI-PRMD dataset demonstrating its performance in\naccurately predicting real-time patient action scores, surpassing\nstate-of-the-art methods.\n","versions":"[{'version': 'v1', 'created': 'Thu, 21 Dec 2023 01:09:52 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 13:16:39 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Mourchid', 'Youssef', ''], ['Slama', 'Rim', '']]","extracted_entities":"[{'text': 'attention fusion mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention fusion mechanism","similarity_score":0.8294988871}
{"id":2401.0615,"submitter":"Youssef Mourchid","authors":"Youssef Mourchid, Rim Slama","title":"D-STGCNT: A Dense Spatio-Temporal Graph Conv-GRU Network based on\n  transformer for assessment of patient physical rehabilitation","comments":"15 pages, Computers in Biology and Medicine Journal","journal-ref":null,"doi":"10.1016\/j.compbiomed.2023.107420","report-no":null,"categories":"eess.IV cs.AI cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  This paper tackles the challenge of automatically assessing physical\nrehabilitation exercises for patients who perform the exercises without\nclinician supervision. The objective is to provide a quality score to ensure\ncorrect performance and achieve desired results. To achieve this goal, a new\ngraph-based model, the Dense Spatio-Temporal Graph Conv-GRU Network with\nTransformer, is introduced. This model combines a modified version of STGCN and\ntransformer architectures for efficient handling of spatio-temporal data. The\nkey idea is to consider skeleton data respecting its non-linear structure as a\ngraph and detecting joints playing the main role in each rehabilitation\nexercise. Dense connections and GRU mechanisms are used to rapidly process\nlarge 3D skeleton inputs and effectively model temporal dynamics. The\ntransformer encoder's attention mechanism focuses on relevant parts of the\ninput sequence, making it useful for evaluating rehabilitation exercises. The\nevaluation of our proposed approach on the KIMORE and UI-PRMD datasets\nhighlighted its potential, surpassing state-of-the-art methods in terms of\naccuracy and computational time. This resulted in faster and more accurate\nlearning and assessment of rehabilitation exercises. Additionally, our model\nprovides valuable feedback through qualitative illustrations, effectively\nhighlighting the significance of joints in specific exercises.\n","versions":"[{'version': 'v1', 'created': 'Thu, 21 Dec 2023 00:38:31 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2025 13:32:19 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Mourchid', 'Youssef', ''], ['Slama', 'Rim', '']]","extracted_entities":"[{'text': 'attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanism","similarity_score":1.0}
{"id":2402.11,"submitter":"Yangyifei Luo","authors":"Yangyifei Luo, Zhuo Chen, Lingbing Guo, Qian Li, Wenxuan Zeng, Zhixin\n  Cai, Jianxin Li","title":"ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment","comments":"Ongoing work; 16 pages, 9 Tables, 8 Figures; Code:\n  https:\/\/github.com\/lyyf2002\/ASGEA","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Entity alignment (EA) aims to identify entities across different knowledge\ngraphs that represent the same real-world objects. Recent embedding-based EA\nmethods have achieved state-of-the-art performance in EA yet faced\ninterpretability challenges as they purely rely on the embedding distance and\nneglect the logic rules behind a pair of aligned entities. In this paper, we\npropose the Align-Subgraph Entity Alignment (ASGEA) framework to exploit logic\nrules from Align-Subgraphs. ASGEA uses anchor links as bridges to construct\nAlign-Subgraphs and spreads along the paths across KGs, which distinguishes it\nfrom the embedding-based methods. Furthermore, we design an interpretable\nPath-based Graph Neural Network, ASGNN, to effectively identify and integrate\nthe logic rules across KGs. We also introduce a node-level multi-modal\nattention mechanism coupled with multi-modal enriched anchors to augment the\nAlign-Subgraph. Our experimental results demonstrate the superior performance\nof ASGEA over the existing embedding-based methods in both EA and Multi-Modal\nEA (MMEA) tasks.\n","versions":"[{'version': 'v1', 'created': 'Fri, 16 Feb 2024 17:03:05 GMT'}, {'version': 'v2', 'created': 'Tue, 5 Mar 2024 13:57:28 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Feb 2025 03:55:35 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Luo', 'Yangyifei', ''], ['Chen', 'Zhuo', ''], ['Guo', 'Lingbing', ''], ['Li', 'Qian', ''], ['Zeng', 'Wenxuan', ''], ['Cai', 'Zhixin', ''], ['Li', 'Jianxin', '']]","extracted_entities":"[{'text': 'ASGEA', 'label': 'Embedding'}, {'text': 'ASGNN', 'label': 'Neural Language Model'}, {'text': 'node-level multi-modal\\nattention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"node-level multi-modal\nattention mechanism","similarity_score":0.7282567024}
{"id":2404.02747,"submitter":"Haozhe Liu","authors":"Haozhe Liu, Wentian Zhang, Jinheng Xie, Francesco Faccio, Mengmeng Xu,\n  Tao Xiang, Mike Zheng Shou, Juan-Manuel Perez-Rua, J\\\"urgen Schmidhuber","title":"Faster Diffusion via Temporal Attention Decomposition","comments":"Accepted by TMLR: https:\/\/openreview.net\/forum?id=xXs2GKXPnH","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https:\/\/github.com\/HaozheLiu-ST\/T-GATE.\n","versions":"[{'version': 'v1', 'created': 'Wed, 3 Apr 2024 13:44:41 GMT'}, {'version': 'v2', 'created': 'Wed, 17 Jul 2024 23:09:10 GMT'}, {'version': 'v3', 'created': 'Wed, 26 Feb 2025 10:49:33 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Liu', 'Haozhe', ''], ['Zhang', 'Wentian', ''], ['Xie', 'Jinheng', ''], ['Faccio', 'Francesco', ''], ['Xu', 'Mengmeng', ''], ['Xiang', 'Tao', ''], ['Shou', 'Mike Zheng', ''], ['Perez-Rua', 'Juan-Manuel', ''], ['Schmidhuber', 'J\u00fcrgen', '']]","extracted_entities":"[{'text': 'attention mechanism', 'label': 'Attention mechanism'}, {'text': 'cross-attention', 'label': 'Attention mechanism'}, {'text': 'Cross-attention', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanism","similarity_score":1.0}
{"id":2405.15932,"submitter":"Soumyabrata Kundu","authors":"Soumyabrata Kundu and Risi Kondor","title":"Steerable Transformers","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this work we introduce Steerable Transformers, an extension of the Vision\nTransformer mechanism that maintains equivariance to the special Euclidean\ngroup $\\mathrm{SE}(d)$. We propose an equivariant attention mechanism that\noperates on features extracted by steerable convolutions. Operating in Fourier\nspace, our network utilizes Fourier space non-linearities. Our experiments in\nboth two and three dimensions show that adding steerable transformer layers to\nsteerable convolutional networks enhances performance.\n","versions":"[{'version': 'v1', 'created': 'Fri, 24 May 2024 20:43:19 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 17:10:11 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Kundu', 'Soumyabrata', ''], ['Kondor', 'Risi', '']]","extracted_entities":"[{'text': 'Steerable Transformers', 'label': 'Transformers'}, {'text': 'Vision\\nTransformer mechanism', 'label': 'Attention mechanism'}, {'text': 'equivariant attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"equivariant attention mechanism","similarity_score":0.7890619636}
{"id":2405.18548,"submitter":"Marco S\\\"alzer","authors":"Marco S\\\"alzer, Eric Alsmann, Martin Lange","title":"Transformer Encoder Satisfiability: Complexity and Impact on Formal\n  Reasoning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LO cs.AI cs.CC cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We analyse the complexity of the satisfiability problem, or similarly\nfeasibility problem, (trSAT) for transformer encoders (TE), which naturally\noccurs in formal verification or interpretation, collectively referred to as\nformal reasoning. We find that trSAT is undecidable when considering TE as they\nare commonly studied in the expressiveness community. Furthermore, we identify\npractical scenarios where trSAT is decidable and establish corresponding\ncomplexity bounds. Beyond trivial cases, we find that quantized TE, those\nrestricted by fixed-width arithmetic, lead to the decidability of trSAT due to\ntheir limited attention capabilities. However, the problem remains difficult,\nas we establish scenarios where trSAT is NEXPTIME-hard and others where it is\nsolvable in NEXPTIME for quantized TE. To complement our complexity results, we\nplace our findings and their implications in the broader context of formal\nreasoning.\n","versions":"[{'version': 'v1', 'created': 'Tue, 28 May 2024 19:30:43 GMT'}, {'version': 'v2', 'created': 'Sat, 12 Oct 2024 16:20:02 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Feb 2025 06:37:14 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['S\u00e4lzer', 'Marco', ''], ['Alsmann', 'Eric', ''], ['Lange', 'Martin', '']]","extracted_entities":"[{'text': 'quantized TE', 'label': 'quantisation'}, {'text': 'limited attention capabilities', 'label': 'Attention mechanism'}, {'text': 'TE', 'label': 'BERT'}]","assigned_concept":"Attention mechanism","matched_keyword":"limited attention capabilities","similarity_score":0.7302443385}
{"id":2406.05784,"submitter":"Seemab Latif","authors":"Huma Ameer, Seemab Latif, Mehwish Fatima","title":"Optimizing Multi-Stuttered Speech Classification: Leveraging Whisper's\n  Encoder for Efficient Parameter Reduction in Automated Assessment","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.LG eess.AS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The automated classification of stuttered speech has significant implications\nfor timely assessments providing assistance to speech language pathologists.\nDespite notable advancements in the field, the cases in which multiple\ndisfluencies occur in speech require attention. We have taken a progressive\napproach to fill this gap by classifying multi-stuttered speech more\nefficiently. The problem has been addressed by firstly curating a dataset of\nmulti-stuttered disfluencies from open source dataset SEP-28k audio clips.\nSecondly, employing Whisper, a state-of-the-art speech recognition model has\nbeen leveraged by using its encoder and taking the problem as multi label\nclassification. Thirdly, using a 6 encoder layer Whisper and experimenting with\nvarious layer freezing strategies, a computationally efficient configuration of\nthe model was identified. The proposed configuration achieved micro, macro, and\nweighted F1-scores of 0.88, 0.85, and 0.87, correspondingly on an external test\ndataset i.e. Fluency-Bank. In addition, through layer freezing strategies, we\nwere able to achieve the aforementioned results by fine-tuning a single encoder\nlayer, consequently, reducing the model's trainable parameters from 20.27\nmillion to 3.29 million. This research study unveils the contribution of the\nlast encoder layer in the identification of disfluencies in stuttered speech.\nConsequently, it has led to a computationally efficient approach, 83.7% less\nparameters to train, making the proposed approach more adaptable for various\ndialects and languages.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Jun 2024 13:42:51 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Jun 2024 06:13:36 GMT'}, {'version': 'v3', 'created': 'Sat, 20 Jul 2024 16:00:30 GMT'}, {'version': 'v4', 'created': 'Wed, 26 Feb 2025 17:31:34 GMT'}]","update_date":"2025-02-27","authors_parsed":"[['Ameer', 'Huma', ''], ['Latif', 'Seemab', ''], ['Fatima', 'Mehwish', '']]","extracted_entities":"[{'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'SEP-28k', 'label': 'Open-source LLMs'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention","similarity_score":0.7383304834}
{"id":2406.13474,"submitter":"Junhan Kim","authors":"Junhan Kim, Ho-young Kim, Eulrang Cho, Chungman Lee, Joonyoung Kim,\n  Yongkweon Jeon","title":"BoA: Attention-aware Post-training Quantization without Backpropagation","comments":"19 pages, under review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Post-training quantization (PTQ) is a promising solution for deploying large\nlanguage models (LLMs) on resource-constrained devices. Early methods developed\nfor smaller networks like ResNet rely on gradient-based optimization, which\nbecomes impractical for hyper-scale LLMs with billions of parameters. While\nrecently proposed backpropagation-free or transformation-based methods\nalleviate this issue, their performance remains limited by either a lack of\ninter-layer dependency consideration or the use of naive nearest-rounding-based\ninteger weight assignment to save the heavy computational cost of weight\noptimization. We thus introduce a novel backpropagation-free PTQ algorithm that\noptimizes integer weights by considering inter-layer dependencies. The key\ninnovation is the development of attention-aware Hessian matrices that capture\ninter-layer interactions within the attention module. Extensive experiments\ndemonstrate that our approach not only outperforms existing weight quantization\nmethods but also shows good synergy with conventional methods to suppress\nactivation outliers, leading to state-of-the-art weight-activation quantization\nperformance.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Jun 2024 11:53:21 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 14:29:08 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Kim', 'Junhan', ''], ['Kim', 'Ho-young', ''], ['Cho', 'Eulrang', ''], ['Lee', 'Chungman', ''], ['Kim', 'Joonyoung', ''], ['Jeon', 'Yongkweon', '']]","extracted_entities":"[{'text': 'Post-training quantization', 'label': 'quantisation'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'attention module', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention module","similarity_score":0.6878248453}
{"id":2406.15079,"submitter":"Darko Drakuli\\'c","authors":"Darko Drakulic, Sofia Michel, Jean-Marc Andreoli","title":"GOAL: A Generalist Combinatorial Optimization Agent Learner","comments":"Accepted to ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Machine Learning-based heuristics have recently shown impressive performance\nin solving a variety of hard combinatorial optimization problems (COPs).\nHowever, they generally rely on a separate neural model, specialized and\ntrained for each single problem. Any variation of a problem requires adjustment\nof its model and re-training from scratch. In this paper, we propose GOAL (for\nGeneralist combinatorial Optimization Agent Learner), a generalist model\ncapable of efficiently solving multiple COPs and which can be fine-tuned to\nsolve new COPs. GOAL consists of a single backbone plus light-weight\nproblem-specific adapters for input and output processing. The backbone is\nbased on a new form of mixed-attention blocks which allows to handle problems\ndefined on graphs with arbitrary combinations of node, edge and instance-level\nfeatures. Additionally, problems which involve heterogeneous types of nodes or\nedges are handled through a novel multi-type transformer architecture, where\nthe attention blocks are duplicated to attend the meaningful combinations of\ntypes while relying on the same shared parameters. We train GOAL on a set of\nrouting, scheduling and classic graph problems and show that it is only\nslightly inferior to the specialized baselines while being the first multi-task\nmodel that solves a wide range of COPs. Finally we showcase the strong transfer\nlearning capacity of GOAL by fine-tuning it on several new problems. Our code\nis available at https:\/\/github.com\/naver\/goal-co\/.\n","versions":"[{'version': 'v1', 'created': 'Fri, 21 Jun 2024 11:55:20 GMT'}, {'version': 'v2', 'created': 'Thu, 24 Oct 2024 16:52:15 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Feb 2025 11:44:20 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Drakulic', 'Darko', ''], ['Michel', 'Sofia', ''], ['Andreoli', 'Jean-Marc', '']]","extracted_entities":"[{'text': 'GOAL', 'label': 'Neural Language Model'}, {'text': 'GOAL', 'label': 'Neural Language Model'}, {'text': 'mixed-attention blocks', 'label': 'Attention mechanism'}, {'text': 'attention blocks', 'label': 'Attention mechanism'}, {'text': 'GOAL', 'label': 'Neural Language Model'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention blocks","similarity_score":0.7334718704}
{"id":2407.01636,"submitter":"Zenglin Shi","authors":"Jie Chu, Tong Su, Pei Liu, Yunpeng Wu, Le Zhang, Zenglin Shi, and Meng\n  Wang","title":"Learning Dual Transformers for All-In-One Image Restoration from a\n  Frequency Perspective","comments":"14 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  This work aims to tackle the all-in-one image restoration task, which seeks\nto handle multiple types of degradation with a single model. The primary\nchallenge is to extract degradation representations from the input degraded\nimages and use them to guide the model's adaptation to specific degradation\ntypes. Building on the insight that various degradations affect image content\ndifferently across frequency bands, we propose a new dual-transformer approach\ncomprising two components: a frequency-aware Degradation estimation transformer\n(Dformer) and a degradation-adaptive Restoration transformer (Rformer). The\nDformer captures the essential characteristics of various degradations by\ndecomposing the input into different frequency components. By understanding how\ndegradations affect these frequency components, the Dformer learns robust\npriors that effectively guide the restoration process. The Rformer then employs\na degradation-adaptive self-attention module to selectively focus on the most\naffected frequency components, guided by the learned degradation\nrepresentations. Extensive experimental results demonstrate that our approach\noutperforms existing methods in five representative restoration tasks,\nincluding denoising, deraining, dehazing, deblurring, and low-light\nenhancement. Additionally, our method offers benefits for handling, real-world\ndegradations, spatially variant degradations, and unseen degradation levels.\n","versions":"[{'version': 'v1', 'created': 'Sun, 30 Jun 2024 13:14:44 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 01:27:49 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Chu', 'Jie', ''], ['Su', 'Tong', ''], ['Liu', 'Pei', ''], ['Wu', 'Yunpeng', ''], ['Zhang', 'Le', ''], ['Shi', 'Zenglin', ''], ['Wang', 'Meng', '']]","extracted_entities":"[{'text': 'degradation-adaptive self-attention module', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"degradation-adaptive self-attention module","similarity_score":0.5256145}
{"id":2407.04916,"submitter":"Tianling Liu","authors":"Tianling Liu and Hongying Liu and Fanhua Shang and Lequan Yu and Tong\n  Han and Liang Wan","title":"Completed Feature Disentanglement Learning for Multimodal MRIs Analysis","comments":"Accept by IEEE JBHI 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multimodal MRIs play a crucial role in clinical diagnosis and treatment.\nFeature disentanglement (FD)-based methods, aiming at learning superior feature\nrepresentations for multimodal data analysis, have achieved significant success\nin multimodal learning (MML). Typically, existing FD-based methods separate\nmultimodal data into modality-shared and modality-specific features, and employ\nconcatenation or attention mechanisms to integrate these features. However, our\npreliminary experiments indicate that these methods could lead to a loss of\nshared information among subsets of modalities when the inputs contain more\nthan two modalities, and such information is critical for prediction accuracy.\nFurthermore, these methods do not adequately interpret the relationships\nbetween the decoupled features at the fusion stage. To address these\nlimitations, we propose a novel Complete Feature Disentanglement (CFD) strategy\nthat recovers the lost information during feature decoupling. Specifically, the\nCFD strategy not only identifies modality-shared and modality-specific\nfeatures, but also decouples shared features among subsets of multimodal\ninputs, termed as modality-partial-shared features. We further introduce a new\nDynamic Mixture-of-Experts Fusion (DMF) module that dynamically integrates\nthese decoupled features, by explicitly learning the local-global relationships\namong the features. The effectiveness of our approach is validated through\nclassification tasks on three multimodal MRI datasets. Extensive experimental\nresults demonstrate that our approach outperforms other state-of-the-art MML\nmethods with obvious margins, showcasing its superior performance.\n","versions":"[{'version': 'v1', 'created': 'Sat, 6 Jul 2024 01:49:38 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Feb 2025 04:49:25 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Liu', 'Tianling', ''], ['Liu', 'Hongying', ''], ['Shang', 'Fanhua', ''], ['Yu', 'Lequan', ''], ['Han', 'Tong', ''], ['Wan', 'Liang', '']]","extracted_entities":"[{'text': 'attention mechanisms', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanisms","similarity_score":0.9558142424}
{"id":2407.18772,"submitter":"Serina Chang","authors":"Serina Chang, Zhiyin Lin, Benjamin Yan, Swapnil Bembde, Qi Xiu, Chi\n  Heem Wong, Yu Qin, Frank Kloster, Alex Luo, Raj Palleti, Jure Leskovec","title":"Learning production functions for supply chains with graph neural\n  networks","comments":"This is the extended version of a paper accepted to AAAI 2025, AI for\n  Social Impact Track (oral)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CY cs.SI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The global economy relies on the flow of goods over supply chain networks,\nwith nodes as firms and edges as transactions between firms. While we may\nobserve these external transactions, they are governed by unseen production\nfunctions, which determine how firms internally transform the input products\nthey receive into output products that they sell. In this setting, it can be\nextremely valuable to infer these production functions, to improve supply chain\nvisibility and to forecast future transactions more accurately. However,\nexisting graph neural networks (GNNs) cannot capture these hidden relationships\nbetween nodes' inputs and outputs. Here, we introduce a new class of models for\nthis setting by combining temporal GNNs with a novel inventory module, which\nlearns production functions via attention weights and a special loss function.\nWe evaluate our models extensively on real supply chains data and data\ngenerated from our new open-source simulator, SupplySim. Our models\nsuccessfully infer production functions, outperforming the strongest baseline\nby 6%-50% (across datasets), and forecast future transactions, outperforming\nthe strongest baseline by 11%-62%\n","versions":"[{'version': 'v1', 'created': 'Fri, 26 Jul 2024 14:32:18 GMT'}, {'version': 'v2', 'created': 'Sat, 19 Oct 2024 18:02:08 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Feb 2025 22:32:49 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Chang', 'Serina', ''], ['Lin', 'Zhiyin', ''], ['Yan', 'Benjamin', ''], ['Bembde', 'Swapnil', ''], ['Xiu', 'Qi', ''], ['Wong', 'Chi Heem', ''], ['Qin', 'Yu', ''], ['Kloster', 'Frank', ''], ['Luo', 'Alex', ''], ['Palleti', 'Raj', ''], ['Leskovec', 'Jure', '']]","extracted_entities":"[{'text': 'attention weights', 'label': 'Attention mechanism'}, {'text': 'SupplySim', 'label': 'Open-source LLMs'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention weights","similarity_score":0.7039312124}
{"id":2407.19271,"submitter":"Zhijie Sui","authors":"Gang Pan, Chen Wang, Zhijie Sui, Shuai Guo, Yaozhi Lv, Honglie Li, Di\n  Sun, Zixia Xia","title":"Sewer Image Super-Resolution with Depth Priors and Its Lightweight\n  Network","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV eess.IV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The Quick-view (QV) technique serves as a primary method for detecting\ndefects within sewerage systems. However, the effectiveness of QV is impeded by\nthe limited visual range of its hardware, resulting in suboptimal image quality\nfor distant portions of the sewer network. Image super-resolution is an\neffective way to improve image quality and has been applied in a variety of\nscenes. However, research on super-resolution for sewer images remains\nconsiderably unexplored. In response, this study leverages the inherent depth\nrelationships present within QV images and introduces a novel Depth-guided,\nReference-based Super-Resolution framework denoted as DSRNet. It comprises two\ncore components: a depth extraction module and a depth information matching\nmodule (DMM). DSRNet utilizes the adjacent frames of the low-resolution image\nas reference images and helps them recover texture information based on the\ncorrelation. By combining these modules, the integration of depth priors\nsignificantly enhances both visual quality and performance benchmarks. Besides,\nin pursuit of computational efficiency and compactness, a super-resolution\nknowledge distillation model based on an attention mechanism is introduced.\nThis mechanism facilitates the acquisition of feature similarity between a more\ncomplex teacher model and a streamlined student model, with the latter being a\nlightweight version of DSRNet. Experimental results demonstrate that DSRNet\nsignificantly improves PSNR and SSIM compared with other methods. This study\nalso conducts experiments on sewer defect semantic segmentation, object\ndetection, and classification on the Pipe dataset and Sewer-ML dataset.\nExperiments show that the method can improve the performance of low-resolution\nsewer images in these tasks.\n","versions":"[{'version': 'v1', 'created': 'Sat, 27 Jul 2024 14:45:34 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Aug 2024 06:34:00 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Feb 2025 13:06:46 GMT'}]","update_date":"2025-02-26","authors_parsed":"[['Pan', 'Gang', ''], ['Wang', 'Chen', ''], ['Sui', 'Zhijie', ''], ['Guo', 'Shuai', ''], ['Lv', 'Yaozhi', ''], ['Li', 'Honglie', ''], ['Sun', 'Di', ''], ['Xia', 'Zixia', '']]","extracted_entities":"[{'text': 'super-resolution\\nknowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanism","similarity_score":1.0}
{"id":2408.03885,"submitter":"Xiaoqi Wang","authors":"Xiaoqi Wang, Yun Zhang","title":"No-Reference Image Quality Assessment with Global-Local Progressive\n  Integration and Semantic-Aligned Quality Transfer","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV eess.IV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Accurate measurement of image quality without reference signals remains a\nfundamental challenge in low-level visual perception applications. In this\npaper, we propose a global-local progressive integration model that addresses\nthis challenge through three key contributions: 1) We develop a\ndual-measurement framework that combines vision Transformer (ViT)-based global\nfeature extractor and convolutional neural networks (CNNs)-based local feature\nextractor to comprehensively capture and quantify image distortion\ncharacteristics at different granularities. 2) We propose a progressive feature\nintegration scheme that utilizes multi-scale kernel configurations to align\nglobal and local features, and progressively aggregates them via an interactive\nstack of channel-wise self-attention and spatial interaction modules for\nmulti-grained quality-aware representations. 3) We introduce a semantic-aligned\nquality transfer method that extends the training data by automatically\nlabeling the quality scores of diverse image content with subjective opinion\nscores. Experimental results demonstrate that our model yields 5.04% and 5.40%\nimprovements in Spearman's rank-order correlation coefficient (SROCC) for\ncross-authentic and cross-synthetic dataset generalization tests, respectively.\nFurthermore, the proposed semantic-aligned quality transfer further yields\n2.26% and 13.23% performance gains in evaluations on single-synthetic and\ncross-synthetic datasets.\n","versions":"[{'version': 'v1', 'created': 'Wed, 7 Aug 2024 16:34:32 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 09:19:26 GMT'}]","update_date":"2025-02-25","authors_parsed":"[['Wang', 'Xiaoqi', ''], ['Zhang', 'Yun', '']]","extracted_entities":"[{'text': 'channel-wise self-attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"channel-wise self-attention","similarity_score":0.6421891451}
{"id":2408.12588,"submitter":"Xuanlei Zhao","authors":"Xuanlei Zhao and Xiaolong Jin and Kai Wang and Yang You","title":"Real-Time Video Generation with Pyramid Attention Broadcast","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.DC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present Pyramid Attention Broadcast (PAB), a real-time, high quality and\ntraining-free approach for DiT-based video generation. Our method is founded on\nthe observation that attention difference in the diffusion process exhibits a\nU-shaped pattern, indicating significant redundancy. We mitigate this by\nbroadcasting attention outputs to subsequent steps in a pyramid style. It\napplies different broadcast strategies to each attention based on their\nvariance for best efficiency. We further introduce broadcast sequence parallel\nfor more efficient distributed inference. PAB demonstrates up to 10.5x speedup\nacross three models compared to baselines, achieving real-time generation for\nup to 720p videos. We anticipate that our simple yet effective method will\nserve as a robust baseline and facilitate future research and application for\nvideo generation.\n","versions":"[{'version': 'v1', 'created': 'Thu, 22 Aug 2024 17:54:21 GMT'}, {'version': 'v2', 'created': 'Wed, 29 Jan 2025 16:02:14 GMT'}, {'version': 'v3', 'created': 'Thu, 27 Feb 2025 07:00:30 GMT'}]","update_date":"2025-02-28","authors_parsed":"[['Zhao', 'Xuanlei', ''], ['Jin', 'Xiaolong', ''], ['Wang', 'Kai', ''], ['You', 'Yang', '']]","extracted_entities":"[{'text': 'attention difference', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention difference","similarity_score":0.7565040588}
