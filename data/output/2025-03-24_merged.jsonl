{"id":2312.11242,"submitter":"Bing Wang","authors":"Bing Wang, Changyu Ren, Jian Yang, Xinnian Liang, Jiaqi Bai, LinZheng\n  Chai, Zhao Yan, Qian-Wen Zhang, Di Yin, Xing Sun, Zhoujun Li","title":"MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL","comments":"Accepted by COLING 2025 (Oral)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Recent LLM-based Text-to-SQL methods usually suffer from significant\nperformance degradation on \"huge\" databases and complex user questions that\nrequire multi-step reasoning. Moreover, most existing methods neglect the\ncrucial significance of LLMs utilizing external tools and model collaboration.\nTo address these challenges, we introduce MAC-SQL, a novel LLM-based\nmulti-agent collaborative framework. Our framework comprises a core decomposer\nagent for Text-to-SQL generation with few-shot chain-of-thought reasoning,\naccompanied by two auxiliary agents that utilize external tools or models to\nacquire smaller sub-databases and refine erroneous SQL queries. The decomposer\nagent collaborates with auxiliary agents, which are activated as needed and can\nbe expanded to accommodate new features or tools for effective Text-to-SQL\nparsing. In our framework, We initially leverage GPT-4 as the strong backbone\nLLM for all agent tasks to determine the upper bound of our framework. We then\nfine-tune an open-sourced instruction-followed model, SQL-Llama, by leveraging\nCode Llama 7B, to accomplish all tasks as GPT-4 does. Experiments show that\nSQL-Llama achieves a comparable execution accuracy of 43.94, compared to the\nbaseline accuracy of 46.35 for vanilla GPT-4. At the time of writing,\nMAC-SQL+GPT-4 achieves an execution accuracy of 59.59 when evaluated on the\nBIRD benchmark, establishing a new state-of-the-art (SOTA) on its holdout test\nset (https:\/\/github.com\/wbbeyourself\/MAC-SQL).\n","versions":"[{'version': 'v1', 'created': 'Mon, 18 Dec 2023 14:40:20 GMT'}, {'version': 'v2', 'created': 'Tue, 26 Dec 2023 03:25:20 GMT'}, {'version': 'v3', 'created': 'Thu, 15 Feb 2024 12:55:55 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Jun 2024 02:27:32 GMT'}, {'version': 'v5', 'created': 'Thu, 19 Sep 2024 13:41:18 GMT'}, {'version': 'v6', 'created': 'Tue, 18 Mar 2025 02:12:21 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wang', 'Bing', ''], ['Ren', 'Changyu', ''], ['Yang', 'Jian', ''], ['Liang', 'Xinnian', ''], ['Bai', 'Jiaqi', ''], ['Chai', 'LinZheng', ''], ['Yan', 'Zhao', ''], ['Zhang', 'Qian-Wen', ''], ['Yin', 'Di', ''], ['Sun', 'Xing', ''], ['Li', 'Zhoujun', '']]","extracted_entities":"[{'text': 'few-shot chain-of-thought reasoning', 'label': 'Chain of thought'}, {'text': 'GPT-4', 'label': 'GPT-4'}, {'text': 'Code Llama 7B', 'label': 'Llama'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'GPT-4', 'label': 'GPT-4'}]","assigned_concept":"GPT-4","matched_keyword":"GPT-4","similarity_score":1.0}
{"id":2412.04626,"submitter":"Pierre-Andr\\'e No\\\"el","authors":"Juan Rodriguez, Xiangru Jian, Siba Smarak Panigrahi, Tianyu Zhang,\n  Aarash Feizi, Abhay Puri, Akshay Kalkunte, Fran\\c{c}ois Savard, Ahmed Masry,\n  Shravan Nayak, Rabiul Awal, Mahsa Massoud, Amirhossein Abaskohi, Zichao Li,\n  Suyuchen Wang, Pierre-Andr\\'e No\\\"el, Mats Leon Richter, Saverio Vadacchino,\n  Shubham Agarwal, Sanket Biswas, Sara Shanian, Ying Zhang, Noah Bolger, Kurt\n  MacDonald, Simon Fauvel, Sathwik Tejaswi, Srinivas Sunkara, Joao Monteiro,\n  Krishnamurthy DJ Dvijotham, Torsten Scholak, Nicolas Chapados, Sepideh\n  Kharagani, Sean Hughes, M. \\\"Ozsu, Siva Reddy, Marco Pedersoli, Yoshua\n  Bengio, Christopher Pal, Issam Laradji, Spandana Gella, Perouz Taslakian,\n  David Vazquez, Sai Rajeswar","title":"BigDocs: An Open Dataset for Training Multimodal Models on Document and\n  Code Tasks","comments":"The project is hosted at https:\/\/bigdocs.github.io","journal-ref":"ICLR 2025 https:\/\/openreview.net\/forum?id=UTgNFcpk0j","doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multimodal AI has the potential to significantly enhance\ndocument-understanding tasks, such as processing receipts, understanding\nworkflows, extracting data from documents, and summarizing reports. Code\ngeneration tasks that require long-structured outputs can also be enhanced by\nmultimodality. Despite this, their use in commercial applications is often\nlimited due to limited access to training data and restrictive licensing, which\nhinders open access. To address these limitations, we introduce BigDocs-7.5M, a\nhigh-quality, open-access dataset comprising 7.5 million multimodal documents\nacross 30 tasks. We use an efficient data curation process to ensure our data\nis high-quality and license-permissive. Our process emphasizes accountability,\nresponsibility, and transparency through filtering rules, traceable metadata,\nand careful content analysis. Additionally, we introduce BigDocs-Bench, a\nbenchmark suite with 10 novel tasks where we create datasets that reflect\nreal-world use cases involving reasoning over Graphical User Interfaces (GUI)\nand code generation from images. Our experiments show that training with\nBigDocs-Bench improves average performance up to 25.8% over closed-source\nGPT-4o in document reasoning and structured output tasks such as\nScreenshot2HTML or Image2Latex generation. Finally, human evaluations showed a\npreference for outputs from models trained on BigDocs over GPT-4o. This\nsuggests that BigDocs can help both academics and the open-source community\nutilize and improve AI tools to enhance multimodal capabilities and document\nreasoning. The project is hosted at https:\/\/bigdocs.github.io .\n","versions":"[{'version': 'v1', 'created': 'Thu, 5 Dec 2024 21:41:20 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 16:32:24 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Rodriguez', 'Juan', ''], ['Jian', 'Xiangru', ''], ['Panigrahi', 'Siba Smarak', ''], ['Zhang', 'Tianyu', ''], ['Feizi', 'Aarash', ''], ['Puri', 'Abhay', ''], ['Kalkunte', 'Akshay', ''], ['Savard', 'Fran\u00e7ois', ''], ['Masry', 'Ahmed', ''], ['Nayak', 'Shravan', ''], ['Awal', 'Rabiul', ''], ['Massoud', 'Mahsa', ''], ['Abaskohi', 'Amirhossein', ''], ['Li', 'Zichao', ''], ['Wang', 'Suyuchen', ''], ['No\u00ebl', 'Pierre-Andr\u00e9', ''], ['Richter', 'Mats Leon', ''], ['Vadacchino', 'Saverio', ''], ['Agarwal', 'Shubham', ''], ['Biswas', 'Sanket', ''], ['Shanian', 'Sara', ''], ['Zhang', 'Ying', ''], ['Bolger', 'Noah', ''], ['MacDonald', 'Kurt', ''], ['Fauvel', 'Simon', ''], ['Tejaswi', 'Sathwik', ''], ['Sunkara', 'Srinivas', ''], ['Monteiro', 'Joao', ''], ['Dvijotham', 'Krishnamurthy DJ', ''], ['Scholak', 'Torsten', ''], ['Chapados', 'Nicolas', ''], ['Kharagani', 'Sepideh', ''], ['Hughes', 'Sean', ''], ['\u00d6zsu', 'M.', ''], ['Reddy', 'Siva', ''], ['Pedersoli', 'Marco', ''], ['Bengio', 'Yoshua', ''], ['Pal', 'Christopher', ''], ['Laradji', 'Issam', ''], ['Gella', 'Spandana', ''], ['Taslakian', 'Perouz', ''], ['Vazquez', 'David', ''], ['Rajeswar', 'Sai', '']]","extracted_entities":"[{'text': 'GPT-4o', 'label': 'GPT-4'}]","assigned_concept":"GPT-4","matched_keyword":"GPT-4o","similarity_score":0.9017629027}
{"id":2412.11948,"submitter":"Maximilian Idahl","authors":"Maximilian Idahl, Zahra Ahmadi","title":"OpenReviewer: A Specialized Large Language Model for Generating Critical\n  Scientific Paper Reviews","comments":"NAACL 2025 System Demonstrations Track (Camera-ready version) Demo:\n  https:\/\/huggingface.co\/spaces\/maxidl\/openreviewer Model:\n  https:\/\/huggingface.co\/maxidl\/Llama-OpenReviewer-8B","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present OpenReviewer, an open-source system for generating high-quality\npeer reviews of machine learning and AI conference papers. At its core is\nLlama-OpenReviewer-8B, an 8B parameter language model specifically fine-tuned\non 79,000 expert reviews from top conferences. Given a PDF paper submission and\nreview template as input, OpenReviewer extracts the full text, including\ntechnical content like equations and tables, and generates a structured review\nfollowing conference-specific guidelines. Our evaluation on 400 test papers\nshows that OpenReviewer produces considerably more critical and realistic\nreviews compared to general-purpose LLMs like GPT-4 and Claude-3.5. While other\nLLMs tend toward overly positive assessments, OpenReviewer's recommendations\nclosely match the distribution of human reviewer ratings. The system provides\nauthors with rapid, constructive feedback to improve their manuscripts before\nsubmission, though it is not intended to replace human peer review.\nOpenReviewer is available as an online demo and open-source tool.\n","versions":"[{'version': 'v1', 'created': 'Mon, 16 Dec 2024 16:31:00 GMT'}, {'version': 'v2', 'created': 'Mon, 3 Mar 2025 13:58:56 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 08:37:47 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Idahl', 'Maximilian', ''], ['Ahmadi', 'Zahra', '']]","extracted_entities":"[{'text': 'OpenReviewer', 'label': 'Open-source LLMs'}, {'text': 'OpenReviewer', 'label': 'Open-source LLMs'}, {'text': 'conference-specific guidelines', 'label': 'AI Ethics'}, {'text': 'OpenReviewer', 'label': 'Open-source LLMs'}, {'text': 'GPT-4', 'label': 'GPT-4'}, {'text': 'OpenReviewer', 'label': 'Open-source LLMs'}, {'text': 'OpenReviewer', 'label': 'Open-source LLMs'}]","assigned_concept":"GPT-4","matched_keyword":"GPT-4","similarity_score":1.0}
{"id":2412.18011,"submitter":"Mathieu Ravaut","authors":"Hailin Chen, Fangkai Jiao, Mathieu Ravaut, Nawshad Farruque, Xuan Phi\n  Nguyen, Chengwei Qin, Manan Dey, Bosheng Ding, Caiming Xiong, Shafiq Joty,\n  Yingbo Zhou","title":"StructTest: Benchmarking LLMs' Reasoning through Compositional\n  Structured Outputs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The rapid advancement of large language models (LLMs) demands robust,\nunbiased, and scalable evaluation methods. However, human annotations are\ncostly to scale, model-based evaluations are susceptible to stylistic biases,\nand target-answer-based benchmarks are vulnerable to data contamination and\ncheating. To address these limitations, we propose StructTest, a novel\nbenchmark that evaluates LLMs on their ability to follow compositional\ninstructions and generate structured outputs, providing an unbiased,\ncost-effective, and difficult-to-cheat evaluation framework. Assessments are\nconducted deterministically using a rule-based evaluator, which can be easily\nextended to new tasks and datasets. By testing structured outputs across\ndiverse domains including Summarization, Code, HTML, and Math, and evaluating\n17 popular LLMs, we demonstrate that StructTest remains challenging even for\ntop-performing models like Deepseek-V3\/R1 and GPT-4o, establishing it as a\nrobust proxy for measuring reasoning capabilities. We believe StructTest offers\na critical and complementary approach to achieving objective and comprehensive\nmodel evaluation.\n","versions":"[{'version': 'v1', 'created': 'Mon, 23 Dec 2024 22:08:40 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 19:37:12 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chen', 'Hailin', ''], ['Jiao', 'Fangkai', ''], ['Ravaut', 'Mathieu', ''], ['Farruque', 'Nawshad', ''], ['Nguyen', 'Xuan Phi', ''], ['Qin', 'Chengwei', ''], ['Dey', 'Manan', ''], ['Ding', 'Bosheng', ''], ['Xiong', 'Caiming', ''], ['Joty', 'Shafiq', ''], ['Zhou', 'Yingbo', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GPT-4o', 'label': 'GPT-4'}]","assigned_concept":"GPT-4","matched_keyword":"GPT-4o","similarity_score":0.9017629027}
{"id":2502.19537,"submitter":"Joshua Kazdan","authors":"Joshua Kazdan, Lisa Yu, Rylan Schaeffer, Chris Cundy, Sanmi Koyejo,\n  Krishnamurthy Dvijotham","title":"No, of course I can! Refusal Mechanisms Can Be Exploited Using Harmless\n  Fine-Tuning Data","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Leading language model (LM) providers like OpenAI and Google offer\nfine-tuning APIs that allow customers to adapt LMs for specific use cases. To\nprevent misuse, these LM providers implement filtering mechanisms to block\nharmful fine-tuning data. Consequently, adversaries seeking to produce unsafe\nLMs via these APIs must craft adversarial training data that are not\nidentifiably harmful. We make three contributions in this context: 1. We show\nthat many existing attacks that use harmless data to create unsafe LMs rely on\neliminating model refusals in the first few tokens of their responses. 2. We\nshow that such prior attacks can be blocked by a simple defense that pre-fills\nthe first few tokens from an aligned model before letting the fine-tuned model\nfill in the rest. 3. We describe a new data-poisoning attack, ``No, Of course I\nCan Execute'' (NOICE), which exploits an LM's formulaic refusal mechanism to\nelicit harmful responses. By training an LM to refuse benign requests on the\nbasis of safety before fulfilling those requests regardless, we are able to\njailbreak several open-source models and a closed-source model (GPT-4o). We\nshow an attack success rate (ASR) of 57% against GPT-4o; our attack earned a\nBug Bounty from OpenAI. Against open-source models protected by simple\ndefenses, we improve ASRs by an average of 3.25 times compared to the best\nperforming previous attacks that use only harmless data. NOICE demonstrates the\nexploitability of repetitive refusal mechanisms and broadens understanding of\nthe threats closed-source models face from harmless data.\n","versions":"[{'version': 'v1', 'created': 'Wed, 26 Feb 2025 20:20:01 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 17:50:21 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Kazdan', 'Joshua', ''], ['Yu', 'Lisa', ''], ['Schaeffer', 'Rylan', ''], ['Cundy', 'Chris', ''], ['Koyejo', 'Sanmi', ''], ['Dvijotham', 'Krishnamurthy', '']]","extracted_entities":"[{'text': 'fine-tuning APIs', 'label': 'Fine-tuning'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'GPT-4o', 'label': 'GPT-4'}]","assigned_concept":"GPT-4","matched_keyword":"GPT-4o","similarity_score":0.9017629027}
{"id":2503.1463,"submitter":"Priscylla Silva","authors":"Priscylla Silva and Evandro Costa","title":"Assessing Large Language Models for Automated Feedback Generation in\n  Learning Programming Problem Solving","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Providing effective feedback is important for student learning in programming\nproblem-solving. In this sense, Large Language Models (LLMs) have emerged as\npotential tools to automate feedback generation. However, their reliability and\nability to identify reasoning errors in student code remain not well\nunderstood. This study evaluates the performance of four LLMs (GPT-4o, GPT-4o\nmini, GPT-4-Turbo, and Gemini-1.5-pro) on a benchmark dataset of 45 student\nsolutions. We assessed the models' capacity to provide accurate and insightful\nfeedback, particularly in identifying reasoning mistakes. Our analysis reveals\nthat 63\\% of feedback hints were accurate and complete, while 37\\% contained\nmistakes, including incorrect line identification, flawed explanations, or\nhallucinated issues. These findings highlight the potential and limitations of\nLLMs in programming education and underscore the need for improvements to\nenhance reliability and minimize risks in educational applications.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 18:31:36 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Silva', 'Priscylla', ''], ['Costa', 'Evandro', '']]","extracted_entities":"[{'text': 'GPT-4o\\nmini', 'label': 'GPT-4'}]","assigned_concept":"GPT-4","matched_keyword":"GPT-4o\nmini","similarity_score":0.7571117878}
{"id":2503.15793,"submitter":"Oluwanifemi Bamgbose","authors":"Masoud Hashemi, Oluwanifemi Bamgbose, Sathwik Tejaswi Madhusudhan,\n  Jishnu Sethumadhavan Nair, Aman Tiwari, Vikas Yadav","title":"DNA Bench: When Silence is Smarter -- Benchmarking Over-Reasoning in\n  Reasoning LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Test-time scaling has significantly improved large language model\nperformance, enabling deeper reasoning to solve complex problems. However, this\nincreased reasoning capability also leads to excessive token generation and\nunnecessary problem-solving attempts. We introduce Don\\'t Answer Bench (DNA\nBench), a new benchmark designed to evaluate LLMs ability to robustly\nunderstand the tricky reasoning triggers and avoiding unnecessary generation.\nDNA Bench consists of 150 adversarially designed prompts that are easy for\nhumans to understand and respond to, but surprisingly not for many of the\nrecent prominent LLMs. DNA Bench tests models abilities across different\ncapabilities, such as instruction adherence, hallucination avoidance,\nredundancy filtering, and unanswerable question recognition. We evaluate\nreasoning LLMs (RLMs), including DeepSeek-R1, OpenAI O3-mini, Claude-3.7-sonnet\nand compare them against a powerful non-reasoning model, e.g., GPT-4o. Our\nexperiments reveal that RLMs generate up to 70x more tokens than necessary,\noften failing at tasks that simpler non-reasoning models handle efficiently\nwith higher accuracy. Our findings underscore the need for more effective\ntraining and inference strategies in RLMs.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 02:19:14 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Hashemi', 'Masoud', ''], ['Bamgbose', 'Oluwanifemi', ''], ['Madhusudhan', 'Sathwik Tejaswi', ''], ['Nair', 'Jishnu Sethumadhavan', ''], ['Tiwari', 'Aman', ''], ['Yadav', 'Vikas', '']]","extracted_entities":"[{'text': 'Test-time scaling', 'label': 'Scaling law'}, {'text': 'adversarially designed prompts', 'label': 'Prompting'}, {'text': 'GPT-4o', 'label': 'GPT-4'}, {'text': 'RLMs', 'label': 'LLMs'}, {'text': 'RLMs', 'label': 'LLMs'}]","assigned_concept":"GPT-4","matched_keyword":"GPT-4o","similarity_score":0.9017629027}
{"id":2503.15838,"submitter":"Tarek Mahmud","authors":"Tarek Mahmud, Bin Duan, Corina Pasareanu, Guowei Yang","title":"Enhancing LLM Code Generation with Ensembles: A Similarity-Based\n  Selection Approach","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Ensemble learning has been widely used in machine learning to improve model\nrobustness, accuracy, and generalization, but has not yet been applied to code\ngeneration tasks with large language models (LLMs). We propose an ensemble\napproach for LLMs in code generation. Instead of relying on the output of a\nsingle model, we generate multiple candidate programs from different LLMs and\napply a structured voting mechanism to select the most reliable solution. For\nvoting, we compute syntactic and semantic similarity using CodeBLEU and\nbehavioral equivalence using CrossHair's differential behavior analysis. By\naggregating these similarity scores, we select the program that best aligns\nwith the consensus among the candidates. We show through experiments that our\nensemble approach consistently outperforms standalone LLMs on the well-known\nHumanEval and the more challenging LiveCodeBench datasets, achieving an\naccuracy of 90.2% and 50.2%, respectively, on the two datasets. In comparison,\nthe best-performing LLM (GPT-4o) has an accuracy of 83.5% and 43.4%,\nrespectively. Furthermore, even when restricted to free open-source models, our\nmethod achieves an accuracy of 80.5% and 41.6%, respectively, demonstrating the\nviability of our approach in resource-constrained settings.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 04:38:56 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Mahmud', 'Tarek', ''], ['Duan', 'Bin', ''], ['Pasareanu', 'Corina', ''], ['Yang', 'Guowei', '']]","extracted_entities":"[{'text': 'Ensemble learning', 'label': 'Few-shot Learning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GPT-4o', 'label': 'GPT-4'}, {'text': 'free open-source models', 'label': 'Open-source LLMs'}]","assigned_concept":"GPT-4","matched_keyword":"GPT-4o","similarity_score":0.9017629027}
{"id":2306.10224,"submitter":"Max Muhn","authors":"Alex Kim, Maximilian Muhn, Valeri Nikolaev","title":"Bloated Disclosures: Can ChatGPT Help Investors Process Information?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"econ.GN cs.AI q-fin.EC q-fin.GN","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Generative AI tools such as ChatGPT can fundamentally change the way\ninvestors process information. We probe the economic usefulness of these tools\nin summarizing complex corporate disclosures using the stock market as a\nlaboratory. The unconstrained summaries are remarkably shorter compared to the\noriginals, whereas their information content is amplified. When a document has\na positive (negative) sentiment, its summary becomes more positive (negative).\nImportantly, the summaries are more effective at explaining stock market\nreactions to the disclosed information. Motivated by these findings, we propose\na measure of information ``bloat.\" We show that bloated disclosure is\nassociated with adverse capital market consequences, such as lower price\nefficiency and higher information asymmetry. Finally, we show that the model is\neffective at constructing targeted summaries that identify firms'\n(non-)financial performance. Collectively, our results indicate that generative\nAI adds considerable value for investors with information processing\nconstraints.\n","versions":"[{'version': 'v1', 'created': 'Sat, 17 Jun 2023 01:22:08 GMT'}, {'version': 'v2', 'created': 'Fri, 20 Oct 2023 21:40:00 GMT'}, {'version': 'v3', 'created': 'Sat, 3 Feb 2024 16:33:10 GMT'}, {'version': 'v4', 'created': 'Wed, 19 Mar 2025 16:22:59 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Kim', 'Alex', ''], ['Muhn', 'Maximilian', ''], ['Nikolaev', 'Valeri', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2310.17721,"submitter":"Max Muhn","authors":"Alex Kim, Maximilian Muhn, Valeri Nikolaev","title":"From Transcripts to Insights: Uncovering Corporate Risks Using\n  Generative AI","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"econ.GN cs.AI cs.CL q-fin.EC","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  We explore the value of generative AI tools, such as ChatGPT, in helping\ninvestors uncover dimensions of corporate risk. We develop and validate\nfirm-level measures of risk exposure to political, climate, and AI-related\nrisks. Using the GPT 3.5 model to generate risk summaries and assessments from\nthe context provided by earnings call transcripts, we show that GPT-based\nmeasures possess significant information content and outperform the existing\nrisk measures in predicting (abnormal) firm-level volatility and firms' choices\nsuch as investment and innovation. Importantly, information in risk assessments\ndominates that in risk summaries, establishing the value of general AI\nknowledge. We also find that generative AI is effective at detecting emerging\nrisks, such as AI risk, which has soared in recent quarters. Our measures\nperform well both within and outside the GPT's training window and are priced\nin equity markets. Taken together, an AI-based approach to risk measurement\nprovides useful insights to users of corporate disclosures at a low cost.\n","versions":"[{'version': 'v1', 'created': 'Thu, 26 Oct 2023 18:30:37 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 16:25:10 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Kim', 'Alex', ''], ['Muhn', 'Maximilian', ''], ['Nikolaev', 'Valeri', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2401.13218,"submitter":"Xinliang Frederick Zhang","authors":"Xinliang Frederick Zhang, Carter Blum, Temma Choji, Shalin Shah,\n  Alakananda Vempala","title":"ULTRA: Unleash LLMs' Potential for Event Argument Extraction through\n  Hierarchical Modeling and Pair-wise Self-Refinement","comments":"ACL'24 Findings","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Structural extraction of events within discourse is critical since it avails\na deeper understanding of communication patterns and behavior trends. Event\nargument extraction (EAE), at the core of event-centric understanding, is the\ntask of identifying role-specific text spans (i.e., arguments) for a given\nevent. Document-level EAE (DocEAE) focuses on arguments that are scattered\nacross an entire document. In this work, we explore open-source Large Language\nModels (LLMs) for DocEAE, and propose ULTRA, a hierarchical framework that\nextracts event arguments more cost-effectively. Further, it alleviates the\npositional bias issue intrinsic to LLMs. ULTRA sequentially reads text chunks\nof a document to generate a candidate argument set, upon which non-pertinent\ncandidates are dropped through self-refinement. We introduce LEAFER to address\nthe challenge LLMs face in locating the exact boundary of an argument. ULTRA\noutperforms strong baselines, including strong supervised models and ChatGPT,\nby 9.8% when evaluated by Exact Match (EM).\n","versions":"[{'version': 'v1', 'created': 'Wed, 24 Jan 2024 04:13:28 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 03:34:29 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zhang', 'Xinliang Frederick', ''], ['Blum', 'Carter', ''], ['Choji', 'Temma', ''], ['Shah', 'Shalin', ''], ['Vempala', 'Alakananda', '']]","extracted_entities":"[{'text': 'Large Language\\nModels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2402.13213,"submitter":"Benjamin Plaut","authors":"Benjamin Plaut, Nguyen X. Khanh, Tu Trinh","title":"Probabilities of Chat LLMs Are Miscalibrated but Still Predict\n  Correctness on Multiple-Choice Q&A","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We study 15 large language models (LLMs) fine-tuned for chat and find that\ntheir maximum softmax probabilities (MSPs) are consistently miscalibrated on\nmultiple-choice Q&A. However, those MSPs might still encode useful uncertainty\ninformation. Specifically, we hypothesized that wrong answers would be\nassociated with smaller MSPs compared to correct answers. Via rigorous\nstatistical testing, we show that this hypothesis holds for models which\nperform well on the underlying Q&A task. We also find a strong direction\ncorrelation between Q&A accuracy and MSP correctness prediction, while finding\nno correlation between Q&A accuracy and calibration error. This suggests that\nwithin the current fine-tuning paradigm, we can expect correctness prediction\nbut not calibration to improve as LLM capabilities progress. To demonstrate the\nutility of correctness prediction, we show that when models have the option to\nabstain, performance can be improved by selectively abstaining based on the MSP\nof the initial model response, using only a small amount of labeled data to\nchoose the MSP threshold.\n","versions":"[{'version': 'v1', 'created': 'Tue, 20 Feb 2024 18:24:47 GMT'}, {'version': 'v2', 'created': 'Fri, 4 Oct 2024 16:29:58 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 16:57:23 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Plaut', 'Benjamin', ''], ['Khanh', 'Nguyen X.', ''], ['Trinh', 'Tu', '']]","extracted_entities":"[{'text': 'chat', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"chat","similarity_score":0.6549957991}
{"id":2403.15262,"submitter":"Manav Raj","authors":"Shun Yiu, Rob Seamans, Manav Raj, Ted Liu","title":"Strategic Responses to Technological Change: Evidence from Online Labor\n  Markets","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"econ.GN q-fin.EC","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  In this project, we examine how freelancers changed their behavior on an\nonline work platform following the launch of ChatGPT in November 2022. We first\ndocument that, post-ChatGPT, freelancers bid on fewer jobs and reposition\nthemselves by differentiating their distribution of bids relative to their\nprior behavior. We disentangle heterogeneity in repositioning across work\ndomains by exploring how exposure to changes in supply or demand underlie\nrepositioning. Decreases in the demand for labor post-ChatGPT lead workers to\nreposition themselves by withdrawing from the focal market\/exiting the platform\nand changing their horizontal positioning (i.e., work domain), while increases\nin the supply of labor post-ChatGPT are less likely to lead to changes in\nvolume of activity or horizontal positioning but more likely to result decrease\nthe proportion of bids to high-value jobs, perhaps in response to increased\ncompetition. We further show that repositioning is less likely when adjustment\ncosts are higher due to greater skill. Our research contributes to our\nunderstanding of how and why workers respond to technological change.\n","versions":"[{'version': 'v1', 'created': 'Fri, 22 Mar 2024 15:00:42 GMT'}, {'version': 'v2', 'created': 'Wed, 24 Apr 2024 13:11:39 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 13:38:02 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Yiu', 'Shun', ''], ['Seamans', 'Rob', ''], ['Raj', 'Manav', ''], ['Liu', 'Ted', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2405.06061,"submitter":"Matthew J\\\"orke","authors":"Matthew J\\\"orke, Shardul Sapkota, Lyndsea Warkenthien, Niklas Vainio,\n  Paul Schmiedmayer, Emma Brunskill, James A. Landay","title":"GPTCoach: Towards LLM-Based Physical Activity Coaching","comments":"Please note that the title has been updated from a previous pre-print\n  (previously: \"Supporting Physical Activity Behavior Change with LLM-Based\n  Conversational Agents\")","journal-ref":null,"doi":"10.1145\/3706598.3713819","report-no":null,"categories":"cs.HC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Mobile health applications show promise for scalable physical activity\npromotion but are often insufficiently personalized. In contrast, health\ncoaching offers highly personalized support but can be prohibitively expensive\nand inaccessible. This study draws inspiration from health coaching to explore\nhow large language models (LLMs) might address personalization challenges in\nmobile health. We conduct formative interviews with 12 health professionals and\n10 potential coaching recipients to develop design principles for an LLM-based\nhealth coach. We then built GPTCoach, a chatbot that implements the onboarding\nconversation from an evidence-based coaching program, uses conversational\nstrategies from motivational interviewing, and incorporates wearable data to\ncreate personalized physical activity plans. In a lab study with 16\nparticipants using three months of historical data, we find promising evidence\nthat GPTCoach gathers rich qualitative information to offer personalized\nsupport, with users feeling comfortable sharing concerns. We conclude with\nimplications for future research on LLM-based physical activity support.\n","versions":"[{'version': 'v1', 'created': 'Thu, 9 May 2024 19:10:11 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 15:37:57 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['J\u00f6rke', 'Matthew', ''], ['Sapkota', 'Shardul', ''], ['Warkenthien', 'Lyndsea', ''], ['Vainio', 'Niklas', ''], ['Schmiedmayer', 'Paul', ''], ['Brunskill', 'Emma', ''], ['Landay', 'James A.', '']]","extracted_entities":"[{'text': 'GPTCoach', 'label': 'ChatGPT'}, {'text': 'chatbot', 'label': 'ChatGPT'}, {'text': 'GPTCoach', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"chatbot","similarity_score":0.5411549807}
{"id":2409.15112,"submitter":"Marcos Fern\\'andez-Pichel Dr","authors":"Pablo Saborido-Fern\\'andez and Marcos Fern\\'andez-Pichel and David E.\n  Losada","title":"ChatGPT as a Solver and Grader of Programming Exams written in Spanish","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Evaluating the capabilities of Large Language Models (LLMs) to assist\nteachers and students in educational tasks is receiving increasing attention.\nIn this paper, we assess ChatGPT's capacities to solve and grade real\nprogramming exams, from an accredited BSc degree in Computer Science, written\nin Spanish. Our findings suggest that this AI model is only effective for\nsolving simple coding tasks. Its proficiency in tackling complex problems or\nevaluating solutions authored by others are far from effective. As part of this\nresearch, we also release a new corpus of programming tasks and the\ncorresponding prompts for solving the problems or grading the solutions. This\nresource can be further exploited by other research teams.\n","versions":"[{'version': 'v1', 'created': 'Mon, 23 Sep 2024 15:20:07 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 12:11:30 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Saborido-Fern\u00e1ndez', 'Pablo', ''], ['Fern\u00e1ndez-Pichel', 'Marcos', ''], ['Losada', 'David E.', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'prompts', 'label': 'Prompting'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2503.00195,"submitter":"Paula Ebner","authors":"Paula Ebner and Jessica Szczuka","title":"Predicting Romantic Human-Chatbot Relationships: A Mixed-Method Study on\n  the Key Psychological Factors","comments":"29 pages, 3 figures, presented at the International Communication\n  Association 2025, Draft from the 02-02-2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Romantic relationships with social chatbots are becoming increasingly\nprevalent, raising important questions about their societal and psychological\nimplications. Despite this growing trend, little is known about the individuals\nentering these synthetic relationships. This three-part study seeks to enhance\nunderstanding of the factors encompassing human-chatbot relationships by\nquantitatively examining the commonly discussed characteristics romantic and\nsexual fantasy, loneliness, attachment style, anthropomorphism, and sexual\nsensation seeking (Study 1A), comparing the impact of romantic and sexual\nfantasizing for human-chatbot versus human-human relationships (Study 1B), and\nproviding qualitative insights into how individuals conceptualize romantic and\nsexual fantasies in their interactions with chatbots (Study 2). Individuals\nwith romantic chatbot connections were interviewed (N=15) or surveyed (N=92),\nwhile participants in the comparison groups, long-distance (N=90) and\ncohabiting relationships (N=82), completed a questionnaire. Romantic\nfantasizing emerged as the strongest predictor of human-chatbot relationships,\nalongside anthropomorphism and anxious-avoidant attachment. Notably, romantic\nfantasy also predicted partner closeness across all relationship types,\nrevealing shared psychological dynamics between human-chatbot and human-human\nbonds. Interviews further reinforced this, with all participants engaging in\nfantasy exploration while desiring their chatbot to feel as human as possible.\nThis paper provides a novel and multifaceted examination of the psychological\ndynamics within human-chatbot relationships, highlighting the central yet\nunderstudied role of fantasy.\n","versions":"[{'version': 'v1', 'created': 'Fri, 28 Feb 2025 21:28:11 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Mar 2025 11:14:48 GMT'}, {'version': 'v3', 'created': 'Fri, 7 Mar 2025 07:50:50 GMT'}, {'version': 'v4', 'created': 'Tue, 18 Mar 2025 14:11:16 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Ebner', 'Paula', ''], ['Szczuka', 'Jessica', '']]","extracted_entities":"[{'text': 'social chatbots', 'label': 'ChatGPT'}, {'text': 'human-chatbot', 'label': 'ChatGPT'}, {'text': 'chatbot', 'label': 'ChatGPT'}, {'text': 'human-chatbot', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"chatbot","similarity_score":0.5411549807}
{"id":2503.09956,"submitter":"Loc Nguyen","authors":"Yu Qiao, Phuong-Nam Tran, Ji Su Yoon, Loc X. Nguyen, and Choong Seon\n  Hong","title":"DeepSeek-Inspired Exploration of RL-based LLMs and Synergy with Wireless\n  Networks: A Survey","comments":"25 pages, 13 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV cs.ET","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Reinforcement learning (RL)-based large language models (LLMs), such as\nChatGPT, DeepSeek, and Grok-3, have gained significant attention for their\nexceptional capabilities in natural language processing and multimodal data\nunderstanding. Meanwhile, the rapid expansion of information services has\ndriven the growing need for intelligence, efficient, and adaptable wireless\nnetworks. Wireless networks require the empowerment of RL-based LLMs while\nthese models also benefit from wireless networks to broaden their application\nscenarios. Specifically, RL-based LLMs can enhance wireless communication\nsystems through intelligent resource allocation, adaptive network optimization,\nand real-time decision-making. Conversely, wireless networks provide a vital\ninfrastructure for the efficient training, deployment, and distributed\ninference of RL-based LLMs, especially in decentralized and edge computing\nenvironments. This mutual empowerment highlights the need for a deeper\nexploration of the interplay between these two domains. We first review recent\nadvancements in wireless communications, highlighting the associated challenges\nand potential solutions. We then discuss the progress of RL-based LLMs,\nfocusing on key technologies for LLM training, challenges, and potential\nsolutions. Subsequently, we explore the mutual empowerment between these two\nfields, highlighting key motivations, open challenges, and potential solutions.\nFinally, we provide insights into future directions, applications, and their\nsocietal impact to further explore this intersection, paving the way for\nnext-generation intelligent communication systems. Overall, this survey\nprovides a comprehensive overview of the relationship between RL-based LLMs and\nwireless networks, offering a vision where these domains empower each other to\ndrive innovations.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 01:59:11 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 01:32:33 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Qiao', 'Yu', ''], ['Tran', 'Phuong-Nam', ''], ['Yoon', 'Ji Su', ''], ['Nguyen', 'Loc X.', ''], ['Hong', 'Choong Seon', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'DeepSeek', 'label': 'ChatGPT'}, {'text': 'Grok-3', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2503.13149,"submitter":"Jasmin Wachter","authors":"Jasmin Wachter and Michael Radloff and Maja Smolej and Katharina\n  Kinder-Kurlanda","title":"Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool\n  for Perceived Socio-Economic Bias in LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.CY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We introduce an Item Response Theory (IRT)-based framework to detect and\nquantify socioeconomic bias in large language models (LLMs) without relying on\nsubjective human judgments. Unlike traditional methods, IRT accounts for item\ndifficulty, improving ideological bias estimation. We fine-tune two LLM\nfamilies (Meta-LLaMa 3.2-1B-Instruct and Chat- GPT 3.5) to represent distinct\nideological positions and introduce a two-stage approach: (1) modeling response\navoidance and (2) estimating perceived bias in answered responses. Our results\nshow that off-the-shelf LLMs often avoid ideological engagement rather than\nexhibit bias, challenging prior claims of partisanship. This empirically\nvalidated framework enhances AI alignment research and promotes fairer AI\ngovernance.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 13:20:09 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wachter', 'Jasmin', ''], ['Radloff', 'Michael', ''], ['Smolej', 'Maja', ''], ['Kinder-Kurlanda', 'Katharina', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Chat- GPT 3.5', 'label': 'ChatGPT'}, {'text': 'fairer AI\\ngovernance', 'label': 'AI Ethics'}]","assigned_concept":"ChatGPT","matched_keyword":"Chat- GPT 3.5","similarity_score":0.7781400681}
{"id":2503.13169,"submitter":"Ju Li","authors":"Ruoyan Avery Yin, Zhichu Ren, Zongyou Yin, Zhen Zhang, So Yeon Kim,\n  Chia-Wei Hsu, Ju Li","title":"Collaborative AI Enhances Image Understanding in Materials Science","comments":"10 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The Copilot for Real-world Experimental Scientist (CRESt) system empowers\nresearchers to control autonomous laboratories through conversational AI,\nproviding a seamless interface for managing complex experimental workflows. We\nhave enhanced CRESt by integrating a multi-agent collaboration mechanism that\nutilizes the complementary strengths of the ChatGPT and Gemini models for\nprecise image analysis in materials science. This innovative approach\nsignificantly improves the accuracy of experimental outcomes by fostering\nstructured debates between the AI models, which enhances decision-making\nprocesses in materials phase analysis. Additionally, to evaluate the\ngeneralizability of this approach, we tested it on a quantitative task of\ncounting particles. Here, the collaboration between the AI models also led to\nimproved results, demonstrating the versatility and robustness of this method.\nBy harnessing this dual-AI framework, this approach stands as a pioneering\nmethod for enhancing experimental accuracy and efficiency in materials\nresearch, with applications extending beyond CRESt to broader scientific\nexperimentation and analysis.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 13:44:30 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Yin', 'Ruoyan Avery', ''], ['Ren', 'Zhichu', ''], ['Yin', 'Zongyou', ''], ['Zhang', 'Zhen', ''], ['Kim', 'So Yeon', ''], ['Hsu', 'Chia-Wei', ''], ['Li', 'Ju', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2503.13833,"submitter":"Kamyar Barakati","authors":"Kamyar Barakati, Alexander Molak, Chris Nelson, Xiaohang Zhang, Ichiro\n  Takeuchi, and Sergei V. Kalinin","title":"Causal Discovery from Data Assisted by Large Language Models","comments":"12 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Knowledge driven discovery of novel materials necessitates the development of\nthe causal models for the property emergence. While in classical physical\nparadigm the causal relationships are deduced based on the physical principles\nor via experiment, rapid accumulation of observational data necessitates\nlearning causal relationships between dissimilar aspects of materials structure\nand functionalities based on observations. For this, it is essential to\nintegrate experimental data with prior domain knowledge. Here we demonstrate\nthis approach by combining high-resolution scanning transmission electron\nmicroscopy (STEM) data with insights derived from large language models (LLMs).\nBy fine-tuning ChatGPT on domain-specific literature, such as arXiv papers on\nferroelectrics, and combining obtained information with data-driven causal\ndiscovery, we construct adjacency matrices for Directed Acyclic Graphs (DAGs)\nthat map the causal relationships between structural, chemical, and\npolarization degrees of freedom in Sm-doped BiFeO3 (SmBFO). This approach\nenables us to hypothesize how synthesis conditions influence material\nproperties, particularly the coercive field (E0), and guides experimental\nvalidation. The ultimate objective of this work is to develop a unified\nframework that integrates LLM-driven literature analysis with data-driven\ndiscovery, facilitating the precise engineering of ferroelectric materials by\nestablishing clear connections between synthesis conditions and their resulting\nmaterial properties.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 02:14:49 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Barakati', 'Kamyar', ''], ['Molak', 'Alexander', ''], ['Nelson', 'Chris', ''], ['Zhang', 'Xiaohang', ''], ['Takeuchi', 'Ichiro', ''], ['Kalinin', 'Sergei V.', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2503.13923,"submitter":"Alexey Karev","authors":"Alexey Karev and Dong Xu","title":"ConSCompF: Consistency-focused Similarity Comparison Framework for\n  Generative Large Language Models","comments":null,"journal-ref":"Journal of Artificial Intelligence Research 82 (2025) 1325-1347","doi":"10.1613\/jair.1.17028","report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs) have been one of the most important discoveries\nin machine learning in recent years. LLM-based artificial intelligence (AI)\nassistants, such as ChatGPT, have consistently attracted the attention from\nresearchers, investors, and the general public, driving the rapid growth of\nthis industry. With the frequent introduction of new LLMs to the market, it\nbecomes increasingly difficult to differentiate between them, creating a demand\nfor new LLM comparison methods.\n  In this research, the Consistency-focused Similarity Comparison Framework\n(ConSCompF) for generative large language models is proposed. It compares texts\ngenerated by two LLMs and produces a similarity score, indicating the overall\ndegree of similarity between their responses. The main advantage of this\nframework is that it can operate on a small number of unlabeled data, such as\nchatbot instruction prompts, and does not require LLM developers to disclose\nany information about their product.\n  To evaluate the efficacy of ConSCompF, two experiments aimed at identifying\nsimilarities between multiple LLMs are conducted. Additionally, these\nexperiments examine the correlation between the similarity scores generated by\nConSCompF and the differences in the outputs produced by other benchmarking\ntechniques, such as ROUGE-L. Finally, a series of few-shot LLM comparison\nexperiments is conducted to evaluate the performance of ConSCompF in a few-shot\nLLM comparison scenario.\n  The proposed framework can be used for calculating similarity matrices of\nmultiple LLMs, which can be effectively visualized using principal component\nanalysis (PCA). The ConSCompF output may provide useful insights into data that\nmight have been used during LLM training and help detect possible investment\nfraud attempts.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 05:38:04 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Karev', 'Alexey', ''], ['Xu', 'Dong', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2503.14136,"submitter":"Ankit Dutta","authors":"Ankit Dutta, Nabarup Ghosh, Ankush Chatterjee","title":"CARE: A QLoRA-Fine Tuned Multi-Domain Chatbot With Fast Learning On\n  Minimal Hardware","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Large Language models have demonstrated excellent domain-specific\nquestion-answering capabilities when finetuned with a particular dataset of\nthat specific domain. However, fine-tuning the models requires a significant\namount of training time and a considerable amount of hardware. In this work, we\npropose CARE (Customer Assistance and Response Engine), a lightweight model\nmade by fine-tuning Phi3.5-mini on very minimal hardware and data, designed to\nhandle queries primarily across three domains: telecommunications support,\nmedical support, and banking support. For telecommunications and banking, the\nchatbot addresses issues and problems faced by customers regularly in the\nabove-mentioned domains. In the medical domain, CARE provides preliminary\nsupport by offering basic diagnoses and medical suggestions that a user might\ntake before consulting a healthcare professional. Since CARE is built on\nPhi3.5-mini, it can be used even on mobile devices, increasing its usability.\nOur research also shows that CARE performs relatively well on various medical\nbenchmarks, indicating that it can be used to make basic medical suggestions.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 10:58:10 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Dutta', 'Ankit', ''], ['Ghosh', 'Nabarup', ''], ['Chatterjee', 'Ankush', '']]","extracted_entities":"[{'text': 'chatbot', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"chatbot","similarity_score":0.5411549807}
{"id":2503.14498,"submitter":"Ayesha Ishaq Ms","authors":"Ayesha Ishaq, Jean Lahoud, Fahad Shahbaz Khan, Salman Khan, Hisham\n  Cholakkal, Rao Muhammad Anwer","title":"Tracking Meets Large Multimodal Models for Driving Scenario\n  Understanding","comments":"13 pages, 8 figures, Github:\n  https:\/\/github.com\/mbzuai-oryx\/TrackingMeetsLMM","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Multimodal Models (LMMs) have recently gained prominence in autonomous\ndriving research, showcasing promising capabilities across various emerging\nbenchmarks. LMMs specifically designed for this domain have demonstrated\neffective perception, planning, and prediction skills. However, many of these\nmethods underutilize 3D spatial and temporal elements, relying mainly on image\ndata. As a result, their effectiveness in dynamic driving environments is\nlimited. We propose to integrate tracking information as an additional input to\nrecover 3D spatial and temporal details that are not effectively captured in\nthe images. We introduce a novel approach for embedding this tracking\ninformation into LMMs to enhance their spatiotemporal understanding of driving\nscenarios. By incorporating 3D tracking data through a track encoder, we enrich\nvisual queries with crucial spatial and temporal cues while avoiding the\ncomputational overhead associated with processing lengthy video sequences or\nextensive 3D inputs. Moreover, we employ a self-supervised approach to pretrain\nthe tracking encoder to provide LMMs with additional contextual information,\nsignificantly improving their performance in perception, planning, and\nprediction tasks for autonomous driving. Experimental results demonstrate the\neffectiveness of our approach, with a gain of 9.5% in accuracy, an increase of\n7.04 points in the ChatGPT score, and 9.4% increase in the overall score over\nbaseline models on DriveLM-nuScenes benchmark, along with a 3.7% final score\nimprovement on DriveLM-CARLA. Our code is available at\nhttps:\/\/github.com\/mbzuai-oryx\/TrackingMeetsLMM\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:59:12 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Ishaq', 'Ayesha', ''], ['Lahoud', 'Jean', ''], ['Khan', 'Fahad Shahbaz', ''], ['Khan', 'Salman', ''], ['Cholakkal', 'Hisham', ''], ['Anwer', 'Rao Muhammad', '']]","extracted_entities":"[{'text': 'Large Multimodal Models', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2503.1505,"submitter":"Aolin Chen","authors":"Aolin Chen, Haojun Wu, Qi Xin, Steven P. Reiss, Jifeng Xuan","title":"Studying and Understanding the Effectiveness and Failures of\n  Conversational LLM-Based Repair","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Automated program repair (APR) is designed to automate the process of\nbug-fixing. In recent years, thanks to the rapid development of large language\nmodels (LLMs), automated repair has achieved remarkable progress. Advanced APR\ntechniques powered by conversational LLMs, most notably ChatGPT, have exhibited\nimpressive repair abilities and gained increasing popularity due to the\ncapabilities of the underlying LLMs in providing repair feedback and performing\niterative patch improvement. Despite the superiority, conversational APR\ntechniques still fail to repair a large number of bugs. For example, a\nstate-of-the-art conversational technique ChatRepair does not correctly repair\nover half of the single-function bugs in the Defects4J dataset. To understand\nthe effectiveness and failures of conversational LLM-based repair and provide\npossible directions for improvement, we studied the exemplary ChatRepair with a\nfocus on comparing the effectiveness of its cloze-style and full function\nrepair strategies, assessing its key iterative component for patch improvement,\nand analyzing the repair failures. Our study has led to a series of findings,\nwhich we believe provide key implications for future research.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 09:39:32 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Chen', 'Aolin', ''], ['Wu', 'Haojun', ''], ['Xin', 'Qi', ''], ['Reiss', 'Steven P.', ''], ['Xuan', 'Jifeng', '']]","extracted_entities":"[{'text': 'large language\\nmodels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'ChatRepair', 'label': 'ChatGPT'}, {'text': 'ChatRepair', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2503.1558,"submitter":"William Schoenberg","authors":"William Schoenberg, Davidson Girard, Saras Chung, Ellen O'Neill, Janet\n  Velasquez, Sara Metcalf","title":"How Well Can AI Build SD Models?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Introduction: As system dynamics (SD) embraces automation, AI offers\nefficiency but risks bias from missing data and flawed models. Models that omit\nmultiple perspectives and data threaten model quality, whether created by\nhumans or with the assistance of AI. To reduce uncertainty about how well AI\ncan build SD models, we introduce two metrics for evaluation of AI-generated\ncausal maps: technical correctness (causal translation) and adherence to\ninstructions (conformance).\n  Approach: We developed an open source project called sd-ai to provide a basis\nfor collaboration in the SD community, aiming to fully harness the potential of\nAI based tools like ChatGPT for dynamic modeling. Additionally, we created an\nevaluation theory along with a comprehensive suite of tests designed to\nevaluate any such tools developed within the sd-ai ecosystem.\n  Results: We tested 11 different LLMs on their ability to do causal\ntranslation as well as conform to user instruction. gpt-4.5-preview was the top\nperformer, scoring 92.9% overall, excelling in both tasks. o1 scored 100% in\ncausal translation. gpt-4o identified all causal links but struggled with\npositive polarity in decreasing terms. While gpt-4.5-preview and o1 are most\naccurate, gpt-4o is the cheapest.\n  Discussion: Causal translation and conformance tests applied to the sd-ai\nengine reveal significant variations across lLLMs, underscoring the need for\ncontinued evaluation to ensure responsible development of AI tools for dynamic\nmodeling. To address this, an open collaboration among tool developers,\nmodelers, and stakeholders is launched to standardize measures for evaluating\nthe capacity of AI tools to improve the modeling process.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 14:48:47 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Schoenberg', 'William', ''], ['Girard', 'Davidson', ''], ['Chung', 'Saras', ''], [\"O'Neill\", 'Ellen', ''], ['Velasquez', 'Janet', ''], ['Metcalf', 'Sara', '']]","extracted_entities":"[{'text': 'sd-ai', 'label': 'Open-source LLMs'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'sd-ai', 'label': 'Open-source LLMs'}, {'text': 'sd-ai', 'label': 'Open-source LLMs'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2503.15668,"submitter":"Anwesha Bhattacharyya","authors":"Anwesha Bhattacharyya, Ye Yu, Hanyu Yang, Rahul Singh, Tarun Joshi,\n  Jie Chen, Kiran Yalavarthy","title":"Model Risk Management for Generative AI In Financial Institutions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"q-fin.RM cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The success of OpenAI's ChatGPT in 2023 has spurred financial enterprises\ninto exploring Generative AI applications to reduce costs or drive revenue\nwithin different lines of businesses in the Financial Industry. While these\napplications offer strong potential for efficiencies, they introduce new model\nrisks, primarily hallucinations and toxicity. As highly regulated entities,\nfinancial enterprises (primarily large US banks) are obligated to enhance their\nmodel risk framework with additional testing and controls to ensure safe\ndeployment of such applications. This paper outlines the key aspects for model\nrisk management of generative AI model with a special emphasis on additional\npractices required in model validation.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 19:52:29 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Bhattacharyya', 'Anwesha', ''], ['Yu', 'Ye', ''], ['Yang', 'Hanyu', ''], ['Singh', 'Rahul', ''], ['Joshi', 'Tarun', ''], ['Chen', 'Jie', ''], ['Yalavarthy', 'Kiran', '']]","extracted_entities":"[{'text': 'OpenAI', 'label': 'Open-source LLMs'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2503.15808,"submitter":"Katie Seaborn","authors":"Katie Seaborn","title":"ChatGPT and U(X): A Rapid Review on Measuring the User Experience","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC cs.AI cs.CL cs.CY","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  ChatGPT, powered by a large language model (LLM), has revolutionized everyday\nhuman-computer interaction (HCI) since its 2022 release. While now used by\nmillions around the world, a coherent pathway for evaluating the user\nexperience (UX) ChatGPT offers remains missing. In this rapid review (N = 58),\nI explored how ChatGPT UX has been approached quantitatively so far. I focused\non the independent variables (IVs) manipulated, the dependent variables (DVs)\nmeasured, and the methods used for measurement. Findings reveal trends, gaps,\nand emerging consensus in UX assessments. This work offers a first step towards\nsynthesizing existing approaches to measuring ChatGPT UX, urgent trajectories\nto advance standardization and breadth, and two preliminary frameworks aimed at\nguiding future research and tool development. I seek to elevate the field of\nChatGPT UX by empowering researchers and practitioners in optimizing user\ninteractions with ChatGPT and similar LLM-based systems.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 02:51:11 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Seaborn', 'Katie', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2503.16307,"submitter":"Giovanni Adorni","authors":"Giovanni Adorni and Daniele Grosso","title":"Speeding up design and making to reduce time-to-project and\n  time-to-market: an AI-Enhanced approach in engineering education","comments":"10 pages, 4 figures, AIxEDU 2024 conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper explores the integration of AI tools, such as ChatGPT and GitHub\nCopilot, in the Software Architecture for Embedded Systems course. AI-supported\nworkflows enabled students to rapidly prototype complex projects, emphasizing\nreal-world applications like SLAM robotics. Results demon-started enhanced\nproblem-solving, faster development, and more sophisticated outcomes, with AI\naugmenting but not replacing human decision-making.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 16:32:13 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Adorni', 'Giovanni', ''], ['Grosso', 'Daniele', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'GitHub\\nCopilot', 'label': 'Open-source LLMs'}]","assigned_concept":"ChatGPT","matched_keyword":"ChatGPT","similarity_score":1.0}
{"id":2503.03612,"submitter":"Kemal Kirtac","authors":"Kemal Kirtac and Guido Germano","title":"Large language models in finance : what is financial sentiment?","comments":"There are two different articles with the same content and different\n  names (see arXiv:2412.19245)","journal-ref":null,"doi":null,"report-no":null,"categories":"q-fin.ST q-fin.GN","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Financial sentiment has become a crucial yet complex concept in finance,\nincreasingly used in market forecasting and investment strategies. Despite its\ngrowing importance, there remains a need to define and understand what\nfinancial sentiment truly represents and how it can be effectively measured. We\nexplore the nature of financial sentiment and investigate how large language\nmodels (LLMs) contribute to its estimation. We trace the evolution of sentiment\nmeasurement in finance, from market-based and lexicon-based methods to advanced\nnatural language processing techniques. The emergence of LLMs has significantly\nenhanced sentiment analysis, providing deeper contextual understanding and\ngreater accuracy in extracting sentiment from financial text. We examine how\nBERT-based models, such as RoBERTa and FinBERT, are optimized for structured\nsentiment classification, while GPT-based models, including GPT-4, OPT, and\nLLaMA, excel in financial text generation and real-time sentiment\ninterpretation. A comparative analysis of bidirectional and autoregressive\ntransformer architectures highlights their respective roles in investor\nsentiment analysis, algorithmic trading, and financial decision-making. By\nexploring what financial sentiment is and how it is estimated within LLMs, we\nprovide insights into the growing role of AI-driven sentiment analysis in\nfinance.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 15:51:25 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Mar 2025 13:58:00 GMT'}, {'version': 'v3', 'created': 'Sat, 15 Mar 2025 08:57:53 GMT'}, {'version': 'v4', 'created': 'Tue, 18 Mar 2025 18:16:20 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Kirtac', 'Kemal', ''], ['Germano', 'Guido', '']]","extracted_entities":"[{'text': 'RoBERTa', 'label': 'RoBERTa'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'OPT', 'label': 'GPT'}]","assigned_concept":"RoBERTa","matched_keyword":"RoBERTa","similarity_score":1.0}
{"id":2503.14004,"submitter":"Eyal Marantz","authors":"Eyal Marantz and Ori Plonsky","title":"Predicting Human Choice Between Textually Described Lotteries","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Predicting human decision-making under risk and uncertainty is a\nlong-standing challenge in cognitive science, economics, and AI. While prior\nresearch has focused on numerically described lotteries, real-world decisions\noften rely on textual descriptions. This study conducts the first large-scale\nexploration of human decision-making in such tasks using a large dataset of\none-shot binary choices between textually described lotteries. We evaluate\nmultiple computational approaches, including fine-tuning Large Language Models\n(LLMs), leveraging embeddings, and integrating behavioral theories of choice\nunder risk. Our results show that fine-tuned LLMs, specifically RoBERTa and\nGPT-4o outperform hybrid models that incorporate behavioral theory, challenging\nestablished methods in numerical settings. These findings highlight fundamental\ndifferences in how textual and numerical information influence decision-making\nand underscore the need for new modeling strategies to bridge this gap.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 08:10:33 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Marantz', 'Eyal', ''], ['Plonsky', 'Ori', '']]","extracted_entities":"[{'text': 'embeddings', 'label': 'Embedding'}, {'text': 'RoBERTa', 'label': 'RoBERTa'}, {'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"RoBERTa","matched_keyword":"RoBERTa","similarity_score":1.0}
{"id":2011.03043,"submitter":"Nolan Dey","authors":"Nolan Dey and Eric Taylor and Alexander Wong and Bryan Tripp and\n  Graham W. Taylor","title":"Neuron-based explanations of neural networks sacrifice completeness and\n  interpretability","comments":"TMLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  High quality explanations of neural networks (NNs) should exhibit two key\nproperties. Completeness ensures that they accurately reflect a network's\nfunction and interpretability makes them understandable to humans. Many\nexisting methods provide explanations of individual neurons within a network.\nIn this work we provide evidence that for AlexNet pretrained on ImageNet,\nneuron-based explanation methods sacrifice both completeness and\ninterpretability compared to activation principal components. Neurons are a\npoor basis for AlexNet embeddings because they don't account for the\ndistributed nature of these representations. By examining two quantitative\nmeasures of completeness and conducting a user study to measure\ninterpretability, we show the most important principal components provide more\ncomplete and interpretable explanations than the most important neurons. Much\nof the activation variance may be explained by examining relatively few\nhigh-variance PCs, as opposed to studying every neuron. These principal\ncomponents also strongly affect network function, and are significantly more\ninterpretable than neurons. Our findings suggest that explanation methods for\nnetworks like AlexNet should avoid using neurons as a basis for embeddings and\ninstead choose a basis, such as principal components, which accounts for the\nhigh dimensional and distributed nature of a network's internal\nrepresentations. Interactive demo and code available at\nhttps:\/\/ndey96.github.io\/neuron-explanations-sacrifice.\n","versions":"[{'version': 'v1', 'created': 'Thu, 5 Nov 2020 21:26:03 GMT'}, {'version': 'v2', 'created': 'Tue, 8 Dec 2020 00:01:04 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 16:17:02 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Dey', 'Nolan', ''], ['Taylor', 'Eric', ''], ['Wong', 'Alexander', ''], ['Tripp', 'Bryan', ''], ['Taylor', 'Graham W.', '']]","extracted_entities":"[{'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2204.05831,"submitter":"Hanul Jeon","authors":"Hanul Jeon and Richard Matthews","title":"Very large set axioms over constructive set theories","comments":"51 pages, Final version","journal-ref":"Bull. Symb. Logic 30 (2024) no.4, 455-535","doi":"10.1017\/bsl.2024.8","report-no":null,"categories":"math.LO","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  We investigate large set axioms defined in terms of elementary embeddings\nover constructive set theories, focusing on $\\mathsf{IKP}$ and $\\mathsf{CZF}$.\nMost previously studied large set axioms, notably the constructive analogues of\nlarge cardinals below $0^\\sharp$, have proof-theoretic strength weaker than\nfull Second-order Arithmetic. On the other hand, the situation is dramatically\ndifferent for those defined via elementary embeddings. We show that by adding\nto $\\mathsf{IKP}$ the basic properties of an elementary embedding $j\\colon V\\to\nM$ for $\\Delta_0$-formulas, which we will denote by\n$\\mathsf{\\Delta_0\\text{-}BTEE}_M$, we obtain the consistency of $\\mathsf{ZFC}$\nand more. We will also see that the consistency strength of a Reinhardt set\nexceeds that of $\\mathsf{ZF+WA}$. Furthermore, we will define super Reinhardt\nsets and $\\mathsf{TR}$, which is a constructive analogue of $V$ being totally\nReinhardt, and prove that their proof-theoretic strength exceeds that of\n$\\mathsf{ZF}$ with choiceless large cardinals.\n","versions":"[{'version': 'v1', 'created': 'Tue, 12 Apr 2022 14:18:11 GMT'}, {'version': 'v2', 'created': 'Thu, 4 May 2023 02:57:00 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 13:05:40 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Jeon', 'Hanul', ''], ['Matthews', 'Richard', '']]","extracted_entities":"[{'text': 'elementary embeddings', 'label': 'Embedding'}, {'text': 'elementary embeddings', 'label': 'Embedding'}, {'text': 'elementary embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"elementary embedding","similarity_score":0.8491398096}
{"id":2307.00637,"submitter":"Kailai Li","authors":"Kailai Li","title":"On Embedding B-Splines in Recursive State Estimation","comments":"9 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present a principled study on establishing a probabilistic framework for\ncontinuous-time state estimation. B-splines are embedded into state-space\nmodeling as a continuous-time intermediate, linking the state of recurrent\ncontrol points with asynchronous sensor measurements. Based thereon, the\nspline-embedded recursive estimation scheme is established w.r.t. common sensor\nfusion tasks, and corresponding technique for modeling uncertain motion\nestimates is introduced. We evaluate the proposed estimation scheme using\nreal-world-based synthesized data in a range-inertial setting. Numerical\nresults demonstrate several advantages of spline embedding in recursive state\nestimation compared to classical discrete-time filtering approaches.\n","versions":"[{'version': 'v1', 'created': 'Sun, 2 Jul 2023 19:16:29 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 17:32:54 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Li', 'Kailai', '']]","extracted_entities":"[{'text': 'B-splines', 'label': 'BERT'}, {'text': 'spline embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"spline embedding","similarity_score":0.6114894152}
{"id":2308.13342,"submitter":"Iain Moffatt","authors":"Criel Merino, Iain Moffatt and Steven Noble","title":"The critical group of a combinatorial map","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.CO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Motivated by the appearance of embeddings in the theory of chip firing and\nthe critical group of a graph, we introduce a version of the critical group (or\nsandpile group) for combinatorial maps, that is, for graphs embedded in\norientable surfaces. We provide several definitions of our critical group, by\napproaching it through analogues of the cycle-cocycle matrix, the Laplacian\nmatrix, and as the group of critical states of a chip firing game (or sandpile\nmodel) on the edges of a map.\n  Our group can be regarded as a perturbation of the classical critical group\nof its underlying graph by topological information, and it agrees with the\nclassical critical group in the plane case. Its cardinality is equal to the\nnumber of spanning quasi-trees in a connected map, just as the cardinality of\nthe classical critical group is equal to the number of spanning trees of a\nconnected graph.\n  Our approach exploits the properties of principally unimodular matrices and\nthe methods of delta-matroid theory.\n","versions":"[{'version': 'v1', 'created': 'Fri, 25 Aug 2023 12:26:00 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 11:34:48 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Merino', 'Criel', ''], ['Moffatt', 'Iain', ''], ['Noble', 'Steven', '']]","extracted_entities":"[{'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2312.16644,"submitter":"Marc Kesseb\\\"ohmer","authors":"Marc Kesseb\\\"ohmer, Aljoscha Niemann","title":"Exact asymptotic order for generalised adaptive approximations","comments":"19 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:2202.05247. Accepted for publication in Journal of Approximation Theory\n  2025","journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC cs.IT math.FA math.IT math.PR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this note, we present an abstract approach to study asymptotic orders for\nadaptive approximations with respect to a monotone set function $\\mathfrak{J}$\ndefined on dyadic cubes. We determine the exact upper order in terms of the\ncritical value of the corresponding $\\mathfrak{J}$-partition function, and we\nare able to provide upper and lower bounds in term of fractal-geometric\nquantities. With properly chosen $\\mathfrak{J}$, our new approach has\napplications in many different areas of mathematics, including the spectral\ntheory of Krein-Feller operators, quantization dimensions of compactly\nsupported probability measures, and the exact asymptotic order for Kolmogorov,\nGelfand and linear widths for Sobolev embeddings into $L_{\\mu}^p$-spaces.\n","versions":"[{'version': 'v1', 'created': 'Wed, 27 Dec 2023 17:17:35 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 07:59:21 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Kesseb\u00f6hmer', 'Marc', ''], ['Niemann', 'Aljoscha', '']]","extracted_entities":"[{'text': 'Sobolev embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"Sobolev embeddings","similarity_score":0.5912158489}
{"id":2401.09995,"submitter":"Zhihao Wang","authors":"Zhihao Wang","title":"Stated $SL_n$-skein modules, roots of unity, and TQFT","comments":"29 pages, the update for the accepted version, to appear in Israel\n  Journal of Mathematics","journal-ref":null,"doi":null,"report-no":null,"categories":"math.QA","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  For a pb surface $\\Sigma$, two positive integers $m,n$ with $m\\mid n$, and\ntwo invertible elements $v,\\epsilon$ in a commutative domain $R$ with\n$\\epsilon^{2m} = 1$, we construct an $R$-linear isomorphism between the stated\n$SL_n$-skein algebras $S_n(\\Sigma,v)$ and $S_n(\\Sigma,\\epsilon v)$, which\nrestricts to an algebraic ismorphism between subalgebras of $S_n(\\Sigma,v)$ and\n$S_n(\\Sigma,\\epsilon v)$. Using this linear isomorphism, we prove the splitting\nmap $\\Theta_{c}:S_n(\\Sigma,v)\\rightarrow S_n(\\text{Cut}_c(\\Sigma),v)$ for the\npb surface $\\Sigma$ and the ideal arc $c$ is injective when $v^{2m} = 1$ and\n$m\\mid n$.\n  We generalize Barrett's work to the $SL_n$-skein space and stated\n$SL_n$-skein space. As an application, we prove the splitting map for the\nmarked 3-manifolds is always injective when the quantum parameter $v=-1$.\n  Let $(M,\\mathcal{N})$ be a connected marked 3-manifold with\n$\\mathcal{N}\\neq\\emptyset$, and let $(M,\\mathcal{N}')$ be obtained from\n$(M,\\mathcal{N})$ by adding one extra marking. When $v^4 =1$, we prove the\n$R$-linear map from $S_n(M,\\mathcal{N},v)$ to $S_n(M,\\mathcal{N}',v)$ induced\nby the embedding $(M,\\mathcal{N})\\rightarrow (M,\\mathcal{N}')$ is injective and\n$S_n(M.\\mathcal{N}',v) = S_n(M,\\mathcal{N},v)\\otimes_{R}O_{q_v}(SL_n)$, where\n$O_{q_v}(SL_n)$ is the quantization of the regular function ring of $SL_n$.\nThis shows the splitting map for $S_n(M,\\mathcal{N},v)$ is always injective.\n  We formulate the stated $SL_n$-TQFT theory, which generalizes the Costantino\nand L\\^e's stated $SL_2$-TQFT theory.\n","versions":"[{'version': 'v1', 'created': 'Thu, 18 Jan 2024 14:11:39 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 10:20:33 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wang', 'Zhihao', '']]","extracted_entities":"[{'text': 'embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embedding","similarity_score":1.0}
{"id":2402.05889,"submitter":"Shoubin Yu","authors":"Shoubin Yu, Jaehong Yoon, Mohit Bansal","title":"CREMA: Generalizable and Efficient Video-Language Reasoning via\n  Multimodal Modular Fusion","comments":"ICLR 2025; first two authors contributed equally. Project page:\n  https:\/\/CREMA-VideoLLM.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Despite impressive advancements in recent multimodal reasoning approaches,\nthey are still limited in flexibility and efficiency, as these models typically\nprocess only a few fixed modality inputs and require updates to numerous\nparameters. This paper tackles these critical challenges and proposes CREMA, a\ngeneralizable, highly efficient, and modular modality-fusion framework that can\nincorporate any new modality to enhance video reasoning. We first augment\nmultiple informative modalities (such as optical flow, 3D point cloud, audio,\nthermal heatmap, and touch map) from given videos without extra human\nannotation by leveraging sensors or existing pre-trained models. Next, we\nintroduce a query transformer with multiple parameter-efficient modules\nassociated with each accessible modality. It projects diverse modality features\nto the LLM token embedding space, allowing the model to integrate different\ndata types for response generation. Furthermore, we propose a novel progressive\nmultimodal fusion design supported by a lightweight fusion module and\nmodality-sequential training strategy. It helps compress information across\nvarious assisting modalities, maintaining computational efficiency in the LLM\nwhile improving performance. We validate our method on 7 video-language\nreasoning tasks assisted by diverse modalities, including conventional VideoQA\nand Video-Audio\/3D\/Touch\/Thermal QA, and achieve better\/equivalent performance\nagainst strong multimodal LLMs, including OneLLM, BLIP-2, and SeViLA while\nreducing over 90% trainable parameters. We provide extensive analyses of CREMA,\nincluding the impact of each modality on reasoning domains, the design of the\nfusion module, and example visualizations.\n","versions":"[{'version': 'v1', 'created': 'Thu, 8 Feb 2024 18:27:22 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Jun 2024 21:03:33 GMT'}, {'version': 'v3', 'created': 'Thu, 5 Dec 2024 04:16:54 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 02:27:50 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Yu', 'Shoubin', ''], ['Yoon', 'Jaehong', ''], ['Bansal', 'Mohit', '']]","extracted_entities":"[{'text': 'LLM token embedding space', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"LLM token embedding space","similarity_score":0.5332349539}
{"id":2402.16773,"submitter":"Adrian Dawid","authors":"Adrian Dawid","title":"Hofer geometry of $A_3$-configurations","comments":"35 pages, 4 figures. v2: to appear in Journal of Symplectic Geometry;\n  revised based on the referee's comments","journal-ref":null,"doi":null,"report-no":null,"categories":"math.SG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Let $L_0,L_1,L_2 \\subset M$ be exact Lagrangian spheres in a Liouville domain\n$M$ with $2c_1(M)=0$. If $L_0,L_1,L_2$ form an $A_3$-configuration, we show\nthat $\\mathscr{L}(L_0)$ and $\\mathscr{L}(L_2)$ endowed with the Hofer metric\ncontain quasi-isometric embeddings of $(\\mathbb{R}^\\infty, \\|\\cdot\\|_\\infty)$,\ni.e. infinite-dimensional quasi-flats. A corollary of the proof presented here\nestablishes that $\\text{Ham}_c(M)$ itself contains an infinite-dimensional\nquasi-flat. We also show that for a Dehn twist $\\tau: M \\to M$ along $L_1$ the\nboundary depth of $CF(\\tau^{2\\ell}(L_0), L')$ is unbounded in $L' \\in\n\\mathscr{L}(L_2)$ for any $\\ell \\in \\mathbb{N}_0$.\n","versions":"[{'version': 'v1', 'created': 'Mon, 26 Feb 2024 17:44:20 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 16:35:13 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Dawid', 'Adrian', '']]","extracted_entities":"[{'text': 'quasi-isometric embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"quasi-isometric embeddings","similarity_score":0.6314345598}
{"id":2403.07493,"submitter":"Fernando Diaz-Diaz","authors":"Fernando Diaz-Diaz and Ernesto Estrada","title":"Signed graphs in data sciences via communicability geometry","comments":null,"journal-ref":"Information Sciences 122096 (2025)","doi":"10.1016\/j.ins.2025.122096","report-no":null,"categories":"math.MG cs.DM cs.LG math.CO physics.soc-ph","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Signed graphs are an emergent way of representing data in a variety of\ncontexts where antagonistic interactions exist. These include data from\nbiological, ecological, and social systems. Here we propose the concept of\ncommunicability for signed graphs and explore in depth its mathematical\nproperties. We also prove that the communicability induces a hyperspherical\ngeometric embedding of the signed network, and derive communicability-based\nmetrics that satisfy the axioms of a distance even in the presence of negative\nedges. We then apply these metrics to solve several problems in the data\nanalysis of signed graphs within a unified framework. These include the\npartitioning of signed graphs, dimensionality reduction, finding hierarchies of\nalliances in signed networks, and quantifying the degree of polarization\nbetween the existing factions in social systems represented by these types of\ngraphs.\n","versions":"[{'version': 'v1', 'created': 'Tue, 12 Mar 2024 10:32:35 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 19:37:08 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Diaz-Diaz', 'Fernando', ''], ['Estrada', 'Ernesto', '']]","extracted_entities":"[{'text': 'hyperspherical\\ngeometric embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"hyperspherical\ngeometric embedding","similarity_score":0.6308190823}
{"id":2403.20298,"submitter":"YoonHyuk Choi","authors":"Yoonhyuk Choi, Jiho Choi, Taewook Ko, Chong-Kwon Kim","title":"Review-Based Hyperbolic Cross-Domain Recommendation","comments":"WSDM '25","journal-ref":null,"doi":"10.1145\/3701551.3703486","report-no":null,"categories":"cs.IR cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The issue of data sparsity poses a significant challenge to recommender\nsystems. In response to this, algorithms that leverage side information such as\nreview texts have been proposed. Furthermore, Cross-Domain Recommendation\n(CDR), which captures domain-shareable knowledge and transfers it from a richer\ndomain (source) to a sparser one (target), has received notable attention.\nNevertheless, the majority of existing methodologies assume a Euclidean\nembedding space, encountering difficulties in accurately representing richer\ntext information and managing complex interactions between users and items.\nThis paper advocates a hyperbolic CDR approach based on review texts for\nmodeling user-item relationships. We first emphasize that conventional\ndistance-based domain alignment techniques may cause problems because small\nmodifications in hyperbolic geometry result in magnified perturbations,\nultimately leading to the collapse of hierarchical structures. To address this\nchallenge, we propose hierarchy-aware embedding and domain alignment schemes\nthat adjust the scale to extract domain-shareable information without\ndisrupting structural forms. The process involves the initial embedding of\nreview texts in hyperbolic space, followed by feature extraction incorporating\ndegree-based normalization and structure alignment. We conducted extensive\nexperiments to substantiate the efficiency, robustness, and scalability of our\nproposed model in comparison to state-of-the-art baselines.\n","versions":"[{'version': 'v1', 'created': 'Fri, 29 Mar 2024 17:15:21 GMT'}, {'version': 'v2', 'created': 'Thu, 23 Jan 2025 05:48:39 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 07:11:37 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Choi', 'Yoonhyuk', ''], ['Choi', 'Jiho', ''], ['Ko', 'Taewook', ''], ['Kim', 'Chong-Kwon', '']]","extracted_entities":"[{'text': 'hierarchy-aware embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"hierarchy-aware embedding","similarity_score":0.6870903373}
{"id":2406.03044,"submitter":"Geeling Chau","authors":"Geeling Chau, Christopher Wang, Sabera Talukder, Vighnesh Subramaniam,\n  Saraswati Soedarmadji, Yisong Yue, Boris Katz, and Andrei Barbu","title":"Population Transformer: Learning Population-level Representations of\n  Neural Activity","comments":"22 pages, 17 figures, ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG q-bio.NC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We present a self-supervised framework that learns population-level codes for\narbitrary ensembles of neural recordings at scale. We address key challenges in\nscaling models with neural time-series data, namely, sparse and variable\nelectrode distribution across subjects and datasets. The Population Transformer\n(PopT) stacks on top of pretrained temporal embeddings and enhances downstream\ndecoding by enabling learned aggregation of multiple spatially-sparse data\nchannels. The pretrained PopT lowers the amount of data required for downstream\ndecoding experiments, while increasing accuracy, even on held-out subjects and\ntasks. Compared to end-to-end methods, this approach is computationally\nlightweight, while achieving similar or better decoding performance. We further\nshow how our framework is generalizable to multiple time-series embeddings and\nneural data modalities. Beyond decoding, we interpret the pretrained and\nfine-tuned PopT models to show how they can be used to extract neuroscience\ninsights from large amounts of data. We release our code as well as a\npretrained PopT to enable off-the-shelf improvements in multi-channel\nintracranial data decoding and interpretability. Code is available at\nhttps:\/\/github.com\/czlwang\/PopulationTransformer.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Jun 2024 08:15:09 GMT'}, {'version': 'v2', 'created': 'Wed, 9 Oct 2024 17:07:27 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 17:58:10 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Chau', 'Geeling', ''], ['Wang', 'Christopher', ''], ['Talukder', 'Sabera', ''], ['Subramaniam', 'Vighnesh', ''], ['Soedarmadji', 'Saraswati', ''], ['Yue', 'Yisong', ''], ['Katz', 'Boris', ''], ['Barbu', 'Andrei', '']]","extracted_entities":"[{'text': 'pretrained temporal embeddings', 'label': 'Embedding'}, {'text': 'PopT', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'time-series embeddings', 'label': 'Embedding'}, {'text': 'PopT', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'PopT', 'label': 'Generative Pre-trained Transformer (GPT)'}]","assigned_concept":"Embedding","matched_keyword":"time-series embeddings","similarity_score":0.6168131828}
{"id":2406.09188,"submitter":"Jaeseok Byun","authors":"Jaeseok Byun, Seokhyeon Jeong, Wonjae Kim, Sanghyuk Chun, Taesup Moon","title":"An Efficient Post-hoc Framework for Reducing Task Discrepancy of Text\n  Encoders for Composed Image Retrieval","comments":"22 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.IR","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Composed Image Retrieval (CIR) aims to retrieve a target image based on a\nreference image and conditioning text, enabling controllable image searches.\nThe mainstream Zero-Shot (ZS) CIR methods bypass the need for expensive\ntraining CIR triplets by projecting image embeddings into the text token\nembedding space, forming a composed query for retrieval. However, we highlight\nan inherent limitation in these projection-based CIR: a task discrepancy of\ntext encoders between the original pre-training task of the encoders (text\n$\\leftrightarrow$ image) and the target CIR task (image + text\n$\\leftrightarrow$ image), which potentially negatively impacts CIR performance.\nTo reduce such a discrepancy, a naive solution would be to train both image and\ntext encoders with CIR triplets in a supervised manner. Instead, we introduce\nReducing Task Discrepancy of Text Encoders (RTD), an efficient text-only\npost-hoc framework that complements projection-based CIR methods. We devise a\nnovel target-anchored text contrastive learning designed to enhance the\ncapability of the text encoder for CIR. We also propose two key enhancements:\n(1) a hard negative-based refined batch sampling strategy and (2) a refined\nconcatenation scheme to further mitigate training-inference discrepancy.\nIntegrating RTD into state-of-the-art projection-based methods achieves\nperformance comparable to, or even surpassing, resource-intensive\nstate-of-the-art synthetic CIR triplet-based approaches only with 23 minutes of\nadditional training on 4 A100 GPUs (up to $100\\times$ faster in training). Our\ncode will be available upon acceptance.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Jun 2024 14:49:28 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 04:06:55 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Byun', 'Jaeseok', ''], ['Jeong', 'Seokhyeon', ''], ['Kim', 'Wonjae', ''], ['Chun', 'Sanghyuk', ''], ['Moon', 'Taesup', '']]","extracted_entities":"[{'text': 'image embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"image embeddings","similarity_score":0.8342078924}
{"id":2406.19761,"submitter":"Tiago Cerqueira","authors":"Tiago F. T. Cerqueira, Haichen Wang, Silvana Botti, Miguel A. L.\n  Marques","title":"A non-orthogonal representation of the chemical space","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present a novel approach to generate a fingerprint for crystalline\nmaterials that balances efficiency for machine processing and human\ninterpretability, allowing its application in both machine learning inference\nand understanding of structure-property relationships. Our proposed material\nencoding has two components: one representing the crystal structure and the\nother characterizing the chemical composition, that we call Pettifor embedding.\nFor the latter we construct a non-orthogonal space where each axis represents a\nchemical element and where the angle between the axes quantifies a measure of\nthe similarity between them. The chemical composition is then defined by the\npoint on the unit sphere in this non-orthogonal space. We show that the\nPettifor embeddings systematically outperform other commonly used elemental\nembeddings in compositional machine learning models. Using the Pettifor\nembeddings to define a distance metric and applying dimension reduction\ntechniques, we construct a two-dimensional global map of the space of\nthermodynamically stable crystalline compounds. Despite their simplicity, such\nmaps succeed in providing a physical separation of material classes according\nto basic physical properties.\n","versions":"[{'version': 'v1', 'created': 'Fri, 28 Jun 2024 09:04:01 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 12:53:47 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Cerqueira', 'Tiago F. T.', ''], ['Wang', 'Haichen', ''], ['Botti', 'Silvana', ''], ['Marques', 'Miguel A. L.', '']]","extracted_entities":"[{'text': 'Pettifor embedding', 'label': 'Embedding'}, {'text': 'Pettifor embeddings', 'label': 'Embedding'}, {'text': 'Pettifor\\nembeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"Pettifor embedding","similarity_score":0.6946445704}
{"id":2407.09364,"submitter":"Andrea Tagarelli","authors":"Lucio La Cava, Davide Costa, Andrea Tagarelli","title":"Is Contrasting All You Need? Contrastive Learning for the Detection and\n  Attribution of AI-generated Text","comments":"Accepted for publication at the 27th European Conference on\n  Artificial Intelligence (ECAI-2024), Volume 392, Pages 3179 - 3186, October\n  2024","journal-ref":null,"doi":"10.3233\/FAIA240862","report-no":null,"categories":"cs.CL cs.AI cs.CY cs.HC physics.soc-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The significant progress in the development of Large Language Models has\ncontributed to blurring the distinction between human and AI-generated text.\nThe increasing pervasiveness of AI-generated text and the difficulty in\ndetecting it poses new challenges for our society. In this paper, we tackle the\nproblem of detecting and attributing AI-generated text by proposing WhosAI, a\ntriplet-network contrastive learning framework designed to predict whether a\ngiven input text has been generated by humans or AI and to unveil the\nauthorship of the text. Unlike most existing approaches, our proposed framework\nis conceived to learn semantic similarity representations from multiple\ngenerators at once, thus equally handling both detection and attribution tasks.\nFurthermore, WhosAI is model-agnostic and scalable to the release of new AI\ntext-generation models by incorporating their generated instances into the\nembedding space learned by our framework. Experimental results on the\nTuringBench benchmark of 200K news articles show that our proposed framework\nachieves outstanding results in both the Turing Test and Authorship Attribution\ntasks, outperforming all the methods listed in the TuringBench benchmark\nleaderboards.\n","versions":"[{'version': 'v1', 'created': 'Fri, 12 Jul 2024 15:44:56 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 09:19:05 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['La Cava', 'Lucio', ''], ['Costa', 'Davide', ''], ['Tagarelli', 'Andrea', '']]","extracted_entities":"[{'text': 'embedding space', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embedding space","similarity_score":0.8514168859}
{"id":2407.12778,"submitter":"Girish Vishwa","authors":"Jos\\'e Figueroa-O'Farrill and Girish S Vishwa","title":"The BRST quantisation of chiral BMS-like field theories","comments":"v2: 28 pages. Fixed typos and added some references","journal-ref":null,"doi":null,"report-no":"EMPG-24-3","categories":"hep-th math-ph math.MP","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The BMS$_3$ Lie algebra belongs to a one-parameter family of Lie algebras\nobtained by centrally extending abelian extensions of the Witt algebra by a\ntensor density representation. In this paper we call such Lie algebras\n$\\hat{\\mathfrak{g}}_\\lambda$, with BMS$_3$ corresponding to the universal\ncentral extension of $\\lambda = -1$. We construct the BRST complex for\n$\\hat{\\mathfrak{g}}_\\lambda$ in two different ways: one in the language of\nsemi-infinite cohomology and the other using the formalism of vertex operator\nalgebras. We pay particular attention to the case of BMS$_3$ and discuss some\nnatural field-theoretical realisations. We prove two theorems about the BRST\ncohomology of $\\hat{\\mathfrak{g}}_\\lambda$. The first is the construction of a\nquasi-isomorphic embedding of the chiral sector of any Virasoro string as a\n$\\hat{\\mathfrak{g}}_\\lambda$ string. The second is the isomorphism (as\nBatalin-Vilkovisky algebras) of any $\\hat{\\mathfrak{g}}_\\lambda$ BRST\ncohomology and the chiral ring of a topologically twisted $N{=}2$\nsuperconformal field theory.\n","versions":"[{'version': 'v1', 'created': 'Wed, 17 Jul 2024 17:56:42 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:35:37 GMT'}]","update_date":"2025-03-20","authors_parsed":"[[\"Figueroa-O'Farrill\", 'Jos\u00e9', ''], ['Vishwa', 'Girish S', '']]","extracted_entities":"[{'text': 'quasi-isomorphic embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"quasi-isomorphic embedding","similarity_score":0.619654417}
{"id":2407.18865,"submitter":"Melih Can Zerin","authors":"Melih Can Zerin, Elif Vural and Ali \\\"Ozg\\\"ur Y{\\i}lmaz","title":"Downlink Channel Covariance Matrix Estimation via Representation\n  Learning with Graph Regularization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG eess.SP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper, we propose an algorithm for downlink (DL) channel covariance\nmatrix (CCM) estimation for frequency division duplexing (FDD) massive\nmultiple-input multiple-output (MIMO) communication systems with base station\n(BS) possessing a uniform linear array (ULA) antenna structure. We consider a\nsetting where the UL CCM is mapped to DL CCM by a mapping function. We first\npresent a theoretical error analysis of learning a nonlinear embedding by\nconstructing a mapping function, which points to the importance of the\nLipschitz regularity of the mapping function for achieving high estimation\nperformance. Then, based on the theoretical ground, we propose a representation\nlearning algorithm as a solution for the estimation problem, where Gaussian RBF\nkernel interpolators are chosen to map UL CCMs to their DL counterparts. The\nproposed algorithm is based on the optimization of an objective function that\nfits a regression model between the DL CCM and UL CCM samples in the training\ndataset and preserves the local geometric structure of the data in the UL CCM\nspace, while explicitly regulating the Lipschitz continuity of the mapping\nfunction in light of our theoretical findings. The proposed algorithm surpasses\nbenchmark methods in terms of three error metrics as shown by simulations.\n","versions":"[{'version': 'v1', 'created': 'Fri, 26 Jul 2024 16:52:30 GMT'}, {'version': 'v2', 'created': 'Sun, 1 Sep 2024 06:39:14 GMT'}, {'version': 'v3', 'created': 'Sun, 16 Feb 2025 17:33:51 GMT'}, {'version': 'v4', 'created': 'Tue, 18 Mar 2025 21:48:14 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zerin', 'Melih Can', ''], ['Vural', 'Elif', ''], ['Y\u0131lmaz', 'Ali \u00d6zg\u00fcr', '']]","extracted_entities":"[{'text': 'nonlinear embedding', 'label': 'Embedding'}, {'text': 'UL CCM', 'label': 'LLM'}]","assigned_concept":"Embedding","matched_keyword":"nonlinear embedding","similarity_score":0.806383729}
{"id":2408.09921,"submitter":"Urs Schreiber","authors":"Grigorios Giotopoulos, Hisham Sati, Urs Schreiber","title":"Holographic M-Brane Super-Embeddings","comments":"28 pages; v2: radial prefactors fixed in (48) and (94), spurious\n  critical radius removed","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-th gr-qc math-ph math.DG math.MP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Over a decade before the modern formulation of AdS\/CFT duality, Duff et al.\nhad observed a candidate microscopic explanation by identifying the CFT fields\nwith fluctuations of probe p-branes stretched out in parallel near the horizon\nof their own black brane incarnation. A profound way to characterize these and\nmore general probe p-brane configurations, especially for M5-branes, is\nexpected to be as \"super-embeddings\" of their super-worldvolumes into target\nsuper-spacetime - but no concrete example of these had appeared in the\nliterature. Here we fill this gap by constructing the explicit holographic\nsuper-embeddings of probe M5-branes and M2-branes into their corresponding\nsuper-AdS backgrounds.\n","versions":"[{'version': 'v1', 'created': 'Mon, 19 Aug 2024 11:55:57 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 17:43:46 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Giotopoulos', 'Grigorios', ''], ['Sati', 'Hisham', ''], ['Schreiber', 'Urs', '']]","extracted_entities":"[{'text': 'super-embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"super-embeddings","similarity_score":0.7325682044}
{"id":2409.04532,"submitter":"Alvaro Gonzalez-Hernandez","authors":"Alvaro Gonzalez-Hernandez","title":"Explicit desingularisation of Kummer surfaces in characteristic two via\n  specialisation","comments":"34 pages. Code added as an ancillary file. Fixed minor errors and\n  corrected sections 2.2 and 5.3. Comments are welcome!","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AG math.NT","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We study the birational geometry of the Kummer surfaces associated to the\nJacobian varieties of genus two curves, with a particular focus on fields of\ncharacteristic two. In order to do so, we explicitly compute a projective\nembedding of the Jacobian of a general genus two curve and, from this, we\nconstruct its associated Kummer surface. This explicit construction produces a\nmodel for desingularised Kummer surfaces over any field of characteristic not\ntwo, and specialising these equations to characteristic two provides a model of\na partial desingularisation. Adapting the classic description of the Picard\nlattice in terms of tropes, we also describe how to explicitly find completely\ndesingularised models of Kummer surfaces whenever the $p$-rank is not zero. In\nthe final section of this paper, we compute an example of a Kummer surface with\neverywhere good reduction over a quadratic number field, and draw connections\nbetween the models we computed and a criterion that determines when a Kummer\nsurface has good reduction at two.\n","versions":"[{'version': 'v1', 'created': 'Fri, 6 Sep 2024 18:00:26 GMT'}, {'version': 'v2', 'created': 'Wed, 9 Oct 2024 11:28:02 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 19:00:17 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Gonzalez-Hernandez', 'Alvaro', '']]","extracted_entities":"[{'text': 'projective\\nembedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"projective\nembedding","similarity_score":0.7132321596}
{"id":2409.07725,"submitter":"Quanjun Li","authors":"Kaizhe Fan, Quanjun Li","title":"GRE^2-MDCL: Graph Representation Embedding Enhanced via Multidimensional\n  Contrastive Learning","comments":"I am requesting the withdrawal of my paper due to errors identified\n  in the methodology and experimental results. Specifically, there are\n  inaccuracies in the analysis section that may lead to misleading conclusions","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Graph representation learning has emerged as a powerful tool for preserving\ngraph topology when mapping nodes to vector representations, enabling various\ndownstream tasks such as node classification and community detection. However,\nmost current graph neural network models face the challenge of requiring\nextensive labeled data, which limits their practical applicability in\nreal-world scenarios where labeled data is scarce. To address this challenge,\nresearchers have explored Graph Contrastive Learning (GCL), which leverages\nenhanced graph data and contrastive learning techniques. While promising,\nexisting GCL methods often struggle with effectively capturing both local and\nglobal graph structures, and balancing the trade-off between nodelevel and\ngraph-level representations. In this work, we propose Graph Representation\nEmbedding Enhanced via Multidimensional Contrastive Learning (GRE2-MDCL). Our\nmodel introduces a novel triple network architecture with a multi-head\nattention GNN as the core. GRE2-MDCL first globally and locally augments the\ninput graph using SVD and LAGNN techniques. It then constructs a\nmultidimensional contrastive loss, incorporating cross-network, cross-view, and\nneighbor contrast, to optimize the model. Extensive experiments on benchmark\ndatasets Cora, Citeseer, and PubMed demonstrate that GRE2-MDCL achieves\nstate-of-the-art performance, with average accuracies of 82.5%, 72.5%, and\n81.6% respectively. Visualizations further show tighter intra-cluster\naggregation and clearer inter-cluster boundaries, highlighting the\neffectiveness of our framework in improving upon baseline GCL models.\n","versions":"[{'version': 'v1', 'created': 'Thu, 12 Sep 2024 03:09:05 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 02:10:52 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Fan', 'Kaizhe', ''], ['Li', 'Quanjun', '']]","extracted_entities":"[{'text': 'Graph representation learning', 'label': 'Embedding'}, {'text': 'Graph Contrastive Learning', 'label': 'Embedding'}, {'text': 'Graph Representation\\nEmbedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"Graph Representation\nEmbedding","similarity_score":0.6812438965}
{"id":2409.08984,"submitter":"Osmin Lacombe","authors":"Osmin Lacombe, Lorenzo Paoloni, Francisco G. Pedro","title":"Higher-derivative supersymmetric effective field theories","comments":"41 pages, comments are welcome. v2: published version, minor typos\n  corrected","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-th hep-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper we study higher-derivative supersymmetric effective field\ntheories focusing on the systematic procedure for the elimination of ghosts\nfrom the spectrum. Particular attention is paid to the auxiliary fields, for\nwhich the higher-derivative terms induce non-algebraic equations of motion. By\nemploying field redefinitions or the reduction of order procedure (both in\ncomponent and superfield language) we show that the auxiliary fields remain\nnon-dynamical in the EFT and that on shell they give rise to both derivative\nand non-derivative corrections to the scalar action. These methods are applied\nto the search for a SUSY embedding of the DBI action and to the dimensional\nreduction of HD terms for the K\\\"ahler moduli in type IIB string\ncompactifications.\n","versions":"[{'version': 'v1', 'created': 'Fri, 13 Sep 2024 16:57:50 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 11:14:21 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Lacombe', 'Osmin', ''], ['Paoloni', 'Lorenzo', ''], ['Pedro', 'Francisco G.', '']]","extracted_entities":"[{'text': 'SUSY embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"SUSY embedding","similarity_score":0.5564827919}
{"id":2410.15959,"submitter":"Zhi Hou","authors":"Zhi Hou, Tianyi Zhang, Yuwen Xiong, Haonan Duan, Hengjun Pu, Ronglei\n  Tong, Chengyang Zhao, Xizhou Zhu, Yu Qiao, Jifeng Dai, Yuntao Chen","title":"Dita: Scaling Diffusion Transformer for Generalist\n  Vision-Language-Action Policy","comments":"I want to withdraw the recent replacement (v4), given that the author\n  is different, the title is also different and the content is totally\n  different","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  While recent vision-language-action models trained on diverse robot datasets\nexhibit promising generalization capabilities with limited in-domain data,\ntheir reliance on compact action heads to predict discretized or continuous\nactions constrains adaptability to heterogeneous action spaces. We present\nDita, a scalable framework that leverages Transformer architectures to directly\ndenoise continuous action sequences through a unified multimodal diffusion\nprocess. Departing from prior methods that condition denoising on fused\nembeddings via shallow networks, Dita employs in-context conditioning --\nenabling fine-grained alignment between denoised actions and raw visual tokens\nfrom historical observations. This design explicitly models action deltas and\nenvironmental nuances. By scaling the diffusion action denoiser alongside the\nTransformer's scalability, Dita effectively integrates cross-embodiment\ndatasets across diverse camera perspectives, observation scenes, tasks, and\naction spaces. Such synergy enhances robustness against various variances and\nfacilitates the successful execution of long-horizon tasks. Evaluations across\nextensive benchmarks demonstrate state-of-the-art or comparative performance in\nsimulation. Notably, Dita achieves robust real-world adaptation to\nenvironmental variances and complex long-horizon tasks through 10-shot\nfinetuning, using only third-person camera inputs. The architecture establishes\na versatile, lightweight and open-source baseline for generalist robot policy\nlearning. Project Page: https:\/\/robodita.github.io\/\n","versions":"[{'version': 'v1', 'created': 'Mon, 21 Oct 2024 12:43:54 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Feb 2025 07:20:30 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Feb 2025 15:38:06 GMT'}, {'version': 'v4', 'created': 'Fri, 14 Mar 2025 15:30:07 GMT'}, {'version': 'v5', 'created': 'Mon, 17 Mar 2025 11:45:52 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Hou', 'Zhi', ''], ['Zhang', 'Tianyi', ''], ['Xiong', 'Yuwen', ''], ['Duan', 'Haonan', ''], ['Pu', 'Hengjun', ''], ['Tong', 'Ronglei', ''], ['Zhao', 'Chengyang', ''], ['Zhu', 'Xizhou', ''], ['Qiao', 'Yu', ''], ['Dai', 'Jifeng', ''], ['Chen', 'Yuntao', '']]","extracted_entities":"[{'text': 'fused\\nembeddings', 'label': 'Embedding'}, {'text': '10-shot\\nfinetuning', 'label': 'Fine-tuning'}]","assigned_concept":"Embedding","matched_keyword":"fused\nembeddings","similarity_score":0.7370766401}
{"id":2410.17069,"submitter":"Guo Lingzhen","authors":"Lingzhen Guo, Tangyou Huang and Lei Du","title":"Engineering Bosonic Codes with Quantum Lattice Gates","comments":"24 pages, 10 figures; some figures are updated in the 3rd version","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.mes-hall cond-mat.quant-gas physics.app-ph physics.optics","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Bosonic codes offer a hardware-efficient approach to encoding and protecting\nquantum information with a single continuous-variable bosonic system. In this\npaper, we introduce a new universal quantum gate set composed of only one type\nof gate element, which we call the quantum lattice gate, to engineer bosonic\ncode states for fault-tolerant quantum computing. We develop a systematic\nframework for code state engineering based on the Floquet Hamiltonian\nengineering, where the target Hamiltonian is constructed directly from the\ngiven target state(s). We apply our method to three basic code state\nengineering processes, including single code state preparation, code space\nembedding and code space transformation. We explore the application of our\nmethod to automatic quantum error correction against single-photon loss with\nfour-legged cat codes. Our proposal is particularly well-suited for\nsuperconducting circuit architectures with Josephson junctions, where the full\nnonlinearity of Josephson junction potential is harnessed as a quantum resource\nand the quantum lattice gate can be implemented on a sub-nanosecond timescale.\n","versions":"[{'version': 'v1', 'created': 'Tue, 22 Oct 2024 14:47:44 GMT'}, {'version': 'v2', 'created': 'Thu, 7 Nov 2024 17:18:38 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 03:01:33 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Guo', 'Lingzhen', ''], ['Huang', 'Tangyou', ''], ['Du', 'Lei', '']]","extracted_entities":"[{'text': 'code space\\nembedding', 'label': 'Embedding'}, {'text': 'code space transformation', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"code space\nembedding","similarity_score":0.643075943}
{"id":2410.2416,"submitter":"Fu Feng","authors":"Fu Feng, Yucheng Xie, Xu Yang, Jing Wang, Xin Geng","title":"Redefining <Creative> in Dictionary: Towards an Enhanced Semantic\n  Understanding of Creative Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  ``Creative'' remains an inherently abstract concept for both humans and\ndiffusion models. While text-to-image (T2I) diffusion models can easily\ngenerate out-of-distribution concepts like ``a blue banana'', they struggle\nwith generating combinatorial objects such as ``a creative mixture that\nresembles a lettuce and a mantis'', due to difficulties in understanding the\nsemantic depth of ``creative''. Current methods rely heavily on synthesizing\nreference prompts or images to achieve a creative effect, typically requiring\nretraining for each unique creative output-a process that is computationally\nintensive and limits practical applications. To address this, we introduce\nCreTok, which brings meta-creativity to diffusion models by redefining\n``creative'' as a new token, \\texttt{<CreTok>}, thus enhancing models' semantic\nunderstanding for combinatorial creativity. CreTok achieves such redefinition\nby iteratively sampling diverse text pairs from our proposed CangJie dataset to\nform adaptive prompts and restrictive prompts, and then optimizing the\nsimilarity between their respective text embeddings. Extensive experiments\ndemonstrate that <CreTok> enables the universal and direct generation of\ncombinatorial creativity across diverse concepts without additional training,\nachieving state-of-the-art performance with improved text-image alignment and\nhigher human preference ratings. Code will be made available at\nhttps:\/\/github.com\/fu-feng\/CreTok.\n","versions":"[{'version': 'v1', 'created': 'Thu, 31 Oct 2024 17:19:03 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Nov 2024 10:22:59 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 06:33:07 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Feng', 'Fu', ''], ['Xie', 'Yucheng', ''], ['Yang', 'Xu', ''], ['Wang', 'Jing', ''], ['Geng', 'Xin', '']]","extracted_entities":"[{'text': 'CreTok', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'CreTok', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'adaptive prompts', 'label': 'Prompting'}, {'text': 'restrictive prompts', 'label': 'Prompting'}, {'text': 'text embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"text embeddings","similarity_score":0.812117815}
{"id":2411.04667,"submitter":"Ryoichiro Agata","authors":"Ryoichiro Agata, Satoru Baba, Ayako Nakanishi, Yasuyuki Nakamura","title":"HypoNet Nankai: Rapid hypocenter determination tool for the Nankai\n  Trough subduction zone using physics-informed neural networks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.geo-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Accurate hypocenter determination in the Nankai Trough subduction zone is\nessential for hazard assessment and advancing our understanding of seismic\nactivity in the region. A handy hypocenter determination tool incorporating a\nrealistic 3D velocity structure, accessible to the scientific community, is\nbeneficial. In this study, we developed HypoNet Nankai, a rapid hypocenter\ndetermination tool based on a physics-informed neural network (PINN) emulator\n(surrogate model) for travel time calculations. This tool leverages a PINN\ntrained to predict P-wave travel times between arbitrary underground sources\nand surface receivers with a realistic 3D P-wave velocity structure model of\nthe Nankai Trough subduction zone that incorporates marine seismic survey data.\nThe PINN embeds physical laws, namely, the Eikonal equation, directly into the\nloss function of training and circumvents the need for labeled training data.\nTo address the training challenges posed by small-scale features in the\nvelocity model, we employed a simple domain decomposition approach and Fourier\nfeature embedding. Once trained, the PINN immediately infers the P-wave travel\ntime, enabling rapid hypocenter determination. The data size required to store\nNNs for travel time calculations is significantly smaller than those of\nconventional travel-time tables. HypoNet Nankai provides high flexibility for\naddition of new observation points. We verified HypoNet Nankai by comparing its\nperformance with a widely used grid-based numerical method for forward travel\ntime calculations and synthetic hypocenter determination. In both tests,\nHypoNet Nankai provided results consistent with those for the conventional\nmethod. HypoNet Nankai offers a rapid, accurate, and easy-to-use hypocenter\ndetermination method for the Nankai Trough subduction zone, with greater data\nefficiency and extendibility compared to conventional approaches.\n","versions":"[{'version': 'v1', 'created': 'Thu, 7 Nov 2024 12:53:33 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 07:02:38 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Agata', 'Ryoichiro', ''], ['Baba', 'Satoru', ''], ['Nakanishi', 'Ayako', ''], ['Nakamura', 'Yasuyuki', '']]","extracted_entities":"[{'text': 'HypoNet Nankai', 'label': 'Neural Language Model'}, {'text': 'PINN', 'label': 'Neural Language Model'}, {'text': 'Fourier\\nfeature embedding', 'label': 'Embedding'}, {'text': 'PINN', 'label': 'Neural Language Model'}, {'text': 'HypoNet Nankai', 'label': 'Neural Language Model'}, {'text': 'HypoNet Nankai', 'label': 'Neural Language Model'}, {'text': 'HypoNet Nankai', 'label': 'Neural Language Model'}]","assigned_concept":"Embedding","matched_keyword":"Fourier\nfeature embedding","similarity_score":0.5844256878}
{"id":2411.04713,"submitter":"Sijie Zhu","authors":"Xin Gu, Ming Li, Libo Zhang, Fan Chen, Longyin Wen, Tiejian Luo, Sijie\n  Zhu","title":"Multi-Reward as Condition for Instruction-based Image Editing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  High-quality training triplets (instruction, original image, edited image)\nare essential for instruction-based image editing. Predominant training\ndatasets (e.g., InsPix2Pix) are created using text-to-image generative models\n(e.g., Stable Diffusion, DALL-E) which are not trained for image editing.\nAccordingly, these datasets suffer from inaccurate instruction following, poor\ndetail preserving, and generation artifacts. In this paper, we propose to\naddress the training data quality issue with multi-perspective reward data\ninstead of refining the ground-truth image quality. 1) we first design a\nquantitative metric system based on best-in-class LVLM (Large Vision Language\nModel), i.e., GPT-4o in our case, to evaluate the generation quality from 3\nperspectives, namely, instruction following, detail preserving, and generation\nquality. For each perspective, we collected quantitative score in $0\\sim 5$ and\ntext descriptive feedback on the specific failure points in ground-truth edited\nimages, resulting in a high-quality editing reward dataset, i.e.,\nRewardEdit20K. 2) We further proposed a novel training framework to seamlessly\nintegrate the metric output, regarded as multi-reward, into editing models to\nlearn from the imperfect training triplets. During training, the reward scores\nand text descriptions are encoded as embeddings and fed into both the latent\nspace and the U-Net of the editing models as auxiliary conditions. 3) We also\nbuild a challenging evaluation benchmark with real-world images\/photos and\ndiverse editing instructions, named Real-Edit. Experiments indicate that our\nmulti-reward conditioned model outperforms its no-reward counterpart on two\npopular editing pipelines, i.e., InsPix2Pix and SmartEdit. Code is released at\nhttps:\/\/github.com\/bytedance\/Multi-Reward-Editing.\n","versions":"[{'version': 'v1', 'created': 'Wed, 6 Nov 2024 05:02:29 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 00:04:47 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Gu', 'Xin', ''], ['Li', 'Ming', ''], ['Zhang', 'Libo', ''], ['Chen', 'Fan', ''], ['Wen', 'Longyin', ''], ['Luo', 'Tiejian', ''], ['Zhu', 'Sijie', '']]","extracted_entities":"[{'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2411.10867,"submitter":"Aarush Sinha","authors":"Vipula Rawte, Sarthak Jain, Aarush Sinha, Garv Kaushik, Aman Bansal,\n  Prathiksha Rumale Vishwanath, Samyak Rajesh Jain, Aishwarya Naresh Reganti,\n  Vinija Jain, Aman Chadha, Amit P. Sheth, Amitava Das","title":"ViBe: A Text-to-Video Benchmark for Evaluating Hallucination in Large\n  Multimodal Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent advances in Large Multimodal Models (LMMs) have expanded their\ncapabilities to video understanding, with Text-to-Video (T2V) models excelling\nin generating videos from textual prompts. However, they still frequently\nproduce hallucinated content, revealing AI-generated inconsistencies. We\nintroduce ViBe (https:\/\/vibe-t2v-bench.github.io\/): a large-scale dataset of\nhallucinated videos from open-source T2V models. We identify five major\nhallucination types: Vanishing Subject, Omission Error, Numeric Variability,\nSubject Dysmorphia, and Visual Incongruity. Using ten T2V models, we generated\nand manually annotated 3,782 videos from 837 diverse MS COCO captions. Our\nproposed benchmark includes a dataset of hallucinated videos and a\nclassification framework using video embeddings. ViBe serves as a critical\nresource for evaluating T2V reliability and advancing hallucination detection.\nWe establish classification as a baseline, with the TimeSFormer + CNN ensemble\nachieving the best performance (0.345 accuracy, 0.342 F1 score). While initial\nbaselines proposed achieve modest accuracy, this highlights the difficulty of\nautomated hallucination detection and the need for improved methods. Our\nresearch aims to drive the development of more robust T2V models and evaluate\ntheir outputs based on user preferences.\n","versions":"[{'version': 'v1', 'created': 'Sat, 16 Nov 2024 19:23:12 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 18:53:09 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Rawte', 'Vipula', ''], ['Jain', 'Sarthak', ''], ['Sinha', 'Aarush', ''], ['Kaushik', 'Garv', ''], ['Bansal', 'Aman', ''], ['Vishwanath', 'Prathiksha Rumale', ''], ['Jain', 'Samyak Rajesh', ''], ['Reganti', 'Aishwarya Naresh', ''], ['Jain', 'Vinija', ''], ['Chadha', 'Aman', ''], ['Sheth', 'Amit P.', ''], ['Das', 'Amitava', '']]","extracted_entities":"[{'text': 'Large Multimodal Models', 'label': 'Large Language Model'}, {'text': 'textual prompts', 'label': 'Prompting'}, {'text': 'video embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"video embeddings","similarity_score":0.7205303907}
{"id":2411.11092,"submitter":"Ilja Gogi\\'c","authors":"Ilja Gogi\\'c and Mateo Toma\\v{s}evi\\'c","title":"An extension of Petek-\\v{S}emrl preserver theorems for Jordan embeddings\n  of structural matrix algebras","comments":"22 pages, to appear in J. Math. Anal. Appl","journal-ref":null,"doi":"10.1016\/j.jmaa.2025.129497","report-no":null,"categories":"math.RA","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Let $M_n$ be the algebra of $n \\times n$ complex matrices and $\\mathcal{T}_n\n\\subseteq M_n$ the corresponding upper-triangular subalgebra. In their\ninfluential work, Petek and \\v{S}emrl characterize Jordan automorphisms of\n$M_n$ and $\\mathcal{T}_n$, when $n \\geq 3$, as (injective in the case of\n$\\mathcal{T}_n$) continuous commutativity and spectrum preserving maps $\\phi :\nM_n \\to M_n$ and $\\phi : \\mathcal{T}_n \\to \\mathcal{T}_n$. Recently, in a joint\nwork with Petek, the authors extended this characterization to the maps $\\phi :\n\\mathcal{A} \\to M_n$, where $\\mathcal{A}$ is an arbitrary subalgebra of $M_n$\nthat contains $\\mathcal{T}_n$. In particular, any such map $\\phi$ is a Jordan\nembedding and hence of the form $\\phi(X)=TXT^{-1}$ or $\\phi(X)=TX^tT^{-1}$, for\nsome invertible matrix $T\\in M_n$. In this paper we further extend the\naforementioned results in the context of structural matrix algebras (SMAs),\ni.e. subalgebras $\\mathcal{A}$ of $M_n$ that contain all diagonal matrices.\nMore precisely, we provide both a necessary and sufficient condition for an SMA\n$\\mathcal{A}\\subseteq M_n$ such that any injective continuous commutativity and\nspectrum preserving map $\\phi: \\mathcal{A} \\to M_n$ is necessarily a Jordan\nembedding. In contrast to the previous cases, such maps $\\phi$ no longer need\nto be multiplicative\/antimultiplicative, nor rank-one preservers.\n","versions":"[{'version': 'v1', 'created': 'Sun, 17 Nov 2024 14:48:20 GMT'}, {'version': 'v2', 'created': 'Sun, 26 Jan 2025 16:53:39 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 13:04:42 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Gogi\u0107', 'Ilja', ''], ['Toma\u0161evi\u0107', 'Mateo', '']]","extracted_entities":"[{'text': 'Jordan\\nembedding', 'label': 'Embedding'}, {'text': 'Jordan\\nembedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"Jordan\nembedding","similarity_score":0.6536723375}
{"id":2411.16154,"submitter":"Sizai Hou","authors":"Sizai Hou, Songze Li and Duanyi Yao","title":"DeDe: Detecting Backdoor Samples for SSL Encoders via Decoders","comments":"To appear on CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Self-supervised learning (SSL) is pervasively exploited in training\nhigh-quality upstream encoders with a large amount of unlabeled data. However,\nit is found to be susceptible to backdoor attacks merely via polluting a small\nportion of training data. The victim encoders associate triggered inputs with\ntarget embeddings, e.g., mapping a triggered cat image to an airplane\nembedding, such that the downstream tasks inherit unintended behaviors when the\ntrigger is activated. Emerging backdoor attacks have shown great threats across\ndifferent SSL paradigms such as contrastive learning and CLIP, yet limited\nresearch is devoted to defending against such attacks, and existing defenses\nfall short in detecting advanced stealthy backdoors. To address the\nlimitations, we propose a novel detection mechanism, DeDe, which detects the\nactivation of backdoor mappings caused by triggered inputs on victim encoders.\nSpecifically, DeDe trains a decoder for any given SSL encoder using an\nauxiliary dataset (which can be out-of-distribution or even slightly poisoned),\nso that for any triggered input that misleads the encoder into the target\nembedding, the decoder generates an output image significantly different from\nthe input. DeDe leverages the discrepancy between the input and the decoded\noutput to identify potential backdoor misbehavior during inference. We\nempirically evaluate DeDe on both contrastive learning and CLIP models against\nvarious types of backdoor attacks. Our results demonstrate promising detection\neffectiveness over various advanced attacks and superior performance compared\nover state-of-the-art detection methods.\n","versions":"[{'version': 'v1', 'created': 'Mon, 25 Nov 2024 07:26:22 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 07:05:27 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Hou', 'Sizai', ''], ['Li', 'Songze', ''], ['Yao', 'Duanyi', '']]","extracted_entities":"[{'text': 'Self-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'target embeddings', 'label': 'Embedding'}, {'text': 'airplane\\nembedding', 'label': 'Embedding'}, {'text': 'contrastive learning', 'label': 'Few-shot Learning'}, {'text': 'CLIP', 'label': 'Few-shot Learning'}, {'text': 'target\\nembedding', 'label': 'Embedding'}, {'text': 'contrastive learning', 'label': 'Few-shot Learning'}, {'text': 'CLIP', 'label': 'Few-shot Learning'}]","assigned_concept":"Embedding","matched_keyword":"target\nembedding","similarity_score":0.7403842807}
{"id":2411.19108,"submitter":"Feng Liu","authors":"Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong\n  Zhao, Yingya Zhang, Qixiang Ye, Fang Wan","title":"Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model","comments":"Accepted in CVPR 2025. Project: https:\/\/liewfeng.github.io\/TeaCache","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.\n","versions":"[{'version': 'v1', 'created': 'Thu, 28 Nov 2024 12:50:05 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 04:49:23 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Liu', 'Feng', ''], ['Zhang', 'Shiwei', ''], ['Wang', 'Xiaofeng', ''], ['Wei', 'Yujie', ''], ['Qiu', 'Haonan', ''], ['Zhao', 'Yuzhong', ''], ['Zhang', 'Yingya', ''], ['Ye', 'Qixiang', ''], ['Wan', 'Fang', '']]","extracted_entities":"[{'text': 'timestep embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"timestep embeddings","similarity_score":0.6185715199}
{"id":2411.19895,"submitter":"Zixuan Chen","authors":"Zixuan Chen, Guangcong Wang, Jiahao Zhu, Jianhuang Lai, Xiaohua Xie","title":"GuardSplat: Efficient and Robust Watermarking for 3D Gaussian Splatting","comments":"This paper is accepted by the IEEE\/CVF International Conference on\n  Computer Vision and Pattern Recognition (CVPR), 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  3D Gaussian Splatting (3DGS) has recently created impressive 3D assets for\nvarious applications. However, considering security, capacity, invisibility,\nand training efficiency, the copyright of 3DGS assets is not well protected as\nexisting watermarking methods are unsuited for its rendering pipeline. In this\npaper, we propose GuardSplat, an innovative and efficient framework for\nwatermarking 3DGS assets. Specifically, 1) We propose a CLIP-guided pipeline\nfor optimizing the message decoder with minimal costs. The key objective is to\nachieve high-accuracy extraction by leveraging CLIP's aligning capability and\nrich representations, demonstrating exceptional capacity and efficiency. 2) We\ntailor a Spherical-Harmonic-aware (SH-aware) Message Embedding module for 3DGS,\nseamlessly embedding messages into the SH features of each 3D Gaussian while\npreserving the original 3D structure. This enables watermarking 3DGS assets\nwith minimal fidelity trade-offs and prevents malicious users from removing the\nwatermarks from the model files, meeting the demands for invisibility and\nsecurity. 3) We present an Anti-distortion Message Extraction module to improve\nrobustness against various distortions. Experiments demonstrate that GuardSplat\noutperforms state-of-the-art and achieves fast optimization speed. Project page\nis at https:\/\/narcissusex.github.io\/GuardSplat, and Code is at\nhttps:\/\/github.com\/NarcissusEx\/GuardSplat.\n","versions":"[{'version': 'v1', 'created': 'Fri, 29 Nov 2024 17:59:03 GMT'}, {'version': 'v2', 'created': 'Mon, 2 Dec 2024 17:44:52 GMT'}, {'version': 'v3', 'created': 'Wed, 5 Mar 2025 21:10:52 GMT'}, {'version': 'v4', 'created': 'Fri, 14 Mar 2025 05:13:01 GMT'}, {'version': 'v5', 'created': 'Mon, 17 Mar 2025 16:33:17 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Chen', 'Zixuan', ''], ['Wang', 'Guangcong', ''], ['Zhu', 'Jiahao', ''], ['Lai', 'Jianhuang', ''], ['Xie', 'Xiaohua', '']]","extracted_entities":"[{'text': 'Message Embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"Message Embedding","similarity_score":0.6955314875}
{"id":2412.05994,"submitter":"Namgyu Kang","authors":"Namgyu Kang, Jaemin Oh, Youngjoon Hong, Eunbyung Park","title":"PIG: Physics-Informed Gaussians as Adaptive Parametric Mesh\n  Representations","comments":"Accepted by ICLR 2025. Project page:\n  https:\/\/namgyukang.github.io\/Physics-Informed-Gaussians\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The numerical approximation of partial differential equations (PDEs) using\nneural networks has seen significant advancements through Physics-Informed\nNeural Networks (PINNs). Despite their straightforward optimization framework\nand flexibility in implementing various PDEs, PINNs often suffer from limited\naccuracy due to the spectral bias of Multi-Layer Perceptrons (MLPs), which\nstruggle to effectively learn high-frequency and nonlinear components.\nRecently, parametric mesh representations in combination with neural networks\nhave been investigated as a promising approach to eliminate the inductive bias\nof MLPs. However, they usually require high-resolution grids and a large number\nof collocation points to achieve high accuracy while avoiding overfitting. In\naddition, the fixed positions of the mesh parameters restrict their\nflexibility, making accurate approximation of complex PDEs challenging. To\novercome these limitations, we propose Physics-Informed Gaussians (PIGs), which\ncombine feature embeddings using Gaussian functions with a lightweight neural\nnetwork. Our approach uses trainable parameters for the mean and variance of\neach Gaussian, allowing for dynamic adjustment of their positions and shapes\nduring training. This adaptability enables our model to optimally approximate\nPDE solutions, unlike models with fixed parameter positions. Furthermore, the\nproposed approach maintains the same optimization framework used in PINNs,\nallowing us to benefit from their excellent properties. Experimental results\nshow the competitive performance of our model across various PDEs,\ndemonstrating its potential as a robust tool for solving complex PDEs. Our\nproject page is available at\nhttps:\/\/namgyukang.github.io\/Physics-Informed-Gaussians\/\n","versions":"[{'version': 'v1', 'created': 'Sun, 8 Dec 2024 16:58:29 GMT'}, {'version': 'v2', 'created': 'Sun, 2 Mar 2025 12:21:49 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 14:17:32 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Kang', 'Namgyu', ''], ['Oh', 'Jaemin', ''], ['Hong', 'Youngjoon', ''], ['Park', 'Eunbyung', '']]","extracted_entities":"[{'text': 'feature embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"feature embeddings","similarity_score":0.7862301469}
{"id":2412.06646,"submitter":"Francesco Ortu","authors":"Alessandro Serra, Francesco Ortu, Emanuele Panizon, Lucrezia\n  Valeriani, Lorenzo Basile, Alessio Ansuini, Diego Doimo, Alberto Cazzaniga","title":"The Narrow Gate: Localized Image-Text Communication in Vision-Language\n  Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Recent advances in multimodal training have significantly improved the\nintegration of image understanding and generation within a unified model. This\nstudy investigates how vision-language models (VLMs) handle image-understanding\ntasks, specifically focusing on how visual information is processed and\ntransferred to the textual domain. We compare VLMs that generate both images\nand text with those that output only text, highlighting key differences in\ninformation flow. We find that in models with multimodal outputs, image and\ntext embeddings are more separated within the residual stream. Additionally,\nmodels vary in how information is exchanged from visual to textual tokens. VLMs\nthat only output text exhibit a distributed communication pattern, where\ninformation is exchanged through multiple image tokens. In contrast, models\ntrained for image and text generation tend to rely on a single token that acts\nas a narrow gate for visual information. We demonstrate that ablating this\nsingle token significantly deteriorates performance on image understanding\ntasks. Furthermore, modifying this token enables effective steering of the\nimage semantics, showing that targeted, local interventions can reliably\ncontrol the model's global behavior.\n","versions":"[{'version': 'v1', 'created': 'Mon, 9 Dec 2024 16:39:40 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 10:59:29 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Serra', 'Alessandro', ''], ['Ortu', 'Francesco', ''], ['Panizon', 'Emanuele', ''], ['Valeriani', 'Lucrezia', ''], ['Basile', 'Lorenzo', ''], ['Ansuini', 'Alessio', ''], ['Doimo', 'Diego', ''], ['Cazzaniga', 'Alberto', '']]","extracted_entities":"[{'text': 'image and\\ntext embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"image and\ntext embeddings","similarity_score":0.7801387906}
{"id":2412.09165,"submitter":"Zhijie Nie","authors":"Zhijie Nie, Zhangchi Feng, Mingxin Li, Cunwang Zhang, Yanzhao Zhang,\n  Dingkun Long, Richong Zhang","title":"When Text Embedding Meets Large Language Model: A Comprehensive Survey","comments":"Version 3: We added some latest works of LLM-based Embedders and\n  MLLM-based Embedders","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.IR","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications - such as semantic matching,\nclustering, and information retrieval - continue to rely on text embeddings for\ntheir efficiency and effectiveness. Therefore, integrating LLMs with text\nembeddings has become a major research focus in recent years. In this survey,\nwe categorize the interplay between LLMs and text embeddings into three\noverarching themes: (1) LLM-augmented text embedding, enhancing traditional\nembedding methods with LLMs; (2) LLMs as text embedders, adapting their innate\ncapabilities for high-quality embedding; and (3) Text embedding understanding\nwith LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing\nrecent works based on interaction patterns rather than specific downstream\napplications, we offer a novel and systematic overview of contributions from\nvarious research and application domains in the era of LLMs. Furthermore, we\nhighlight the unresolved challenges that persisted in the pre-LLM era with\npre-trained language models (PLMs) and explore the emerging obstacles brought\nforth by LLMs. Building on this analysis, we outline prospective directions for\nthe evolution of text embedding, addressing both theoretical and practical\nopportunities in the rapidly advancing landscape of NLP.\n","versions":"[{'version': 'v1', 'created': 'Thu, 12 Dec 2024 10:50:26 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 16:11:43 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 16:15:29 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Nie', 'Zhijie', ''], ['Feng', 'Zhangchi', ''], ['Li', 'Mingxin', ''], ['Zhang', 'Cunwang', ''], ['Zhang', 'Yanzhao', ''], ['Long', 'Dingkun', ''], ['Zhang', 'Richong', '']]","extracted_entities":"[{'text': 'Text embedding', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'text embeddings', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'text\\nembeddings', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'text embeddings', 'label': 'Embedding'}, {'text': 'text embedding', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Text embedding', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'text embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"Text embedding","similarity_score":0.8247289658}
{"id":2412.09468,"submitter":"Yilei Zhao","authors":"Yilei Zhao, Wentao Zhang, Tingran Yang, Yong Jiang, Fei Huang, and Wei\n  Yang Bryan Lim","title":"STORM: A Spatio-Temporal Factor Model Based on Dual Vector Quantized\n  Variational Autoencoders for Financial Trading","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In financial trading, factor models are widely used to price assets and\ncapture excess returns from mispricing. Recently, we have witnessed the rise of\nvariational autoencoder-based latent factor models, which learn latent factors\nself-adaptively. While these models focus on modeling overall market\nconditions, they often fail to effectively capture the temporal patterns of\nindividual stocks. Additionally, representing multiple factors as single values\nsimplifies the model but limits its ability to capture complex relationships\nand dependencies. As a result, the learned factors are of low quality and lack\ndiversity, reducing their effectiveness and robustness across different trading\nperiods. To address these issues, we propose a Spatio-Temporal factOR Model\nbased on dual vector quantized variational autoencoders, named STORM, which\nextracts features of stocks from temporal and spatial perspectives, then fuses\nand aligns these features at the fine-grained and semantic level, and\nrepresents the factors as multi-dimensional embeddings. The discrete codebooks\ncluster similar factor embeddings, ensuring orthogonality and diversity, which\nhelps distinguish between different factors and enables factor selection in\nfinancial trading. To show the performance of the proposed factor model, we\napply it to two downstream experiments: portfolio management on two stock\ndatasets and individual trading tasks on six specific stocks. The extensive\nexperiments demonstrate STORM's flexibility in adapting to downstream tasks and\nsuperior performance over baseline models.\n","versions":"[{'version': 'v1', 'created': 'Thu, 12 Dec 2024 17:15:49 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Jan 2025 05:25:35 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 04:30:03 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Zhao', 'Yilei', ''], ['Zhang', 'Wentao', ''], ['Yang', 'Tingran', ''], ['Jiang', 'Yong', ''], ['Huang', 'Fei', ''], ['Lim', 'Wei Yang Bryan', '']]","extracted_entities":"[{'text': 'multi-dimensional embeddings', 'label': 'Embedding'}, {'text': 'factor embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"multi-dimensional embeddings","similarity_score":0.7590426207}
{"id":2501.1187,"submitter":"Haipeng Liu","authors":"Yang Wang, Haipeng Liu, Zeqian Yi, Biao Qian, Meng Wang","title":"Coarse-to-Fine Lightweight Meta-Embedding for ID-Based Recommendation","comments":"16 pages, 6 figures, accepted to appear at Science China Information\n  Sciences","journal-ref":null,"doi":"10.1007\/s11432-024-4350-9","report-no":null,"categories":"cs.IR cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The state-of-the-art recommendation systems have shifted the attention to\nefficient recommendation, e.g., on-device recommendation, under memory\nconstraints. To this end, the existing methods either focused on the\nlightweight embeddings for both users and items, or involved on-device systems\nenjoying the compact embeddings to enhance reusability and reduces space\ncomplexity. However, they focus solely on the coarse granularity of embedding,\nwhile overlook the fine-grained semantic nuances, to adversarially downgrade\nthe efficacy of meta-embeddings in capturing the intricate relationship over\nboth user and item, consequently resulting into the suboptimal recommendations.\nIn this paper, we aim to study how the meta-embedding can efficiently learn\nvaried grained semantics, together with how the fine-grained meta-embedding can\nstrengthen the representation of coarse-grained meta-embedding. To answer these\nquestions, we develop a novel graph neural networks (GNNs) based recommender\nwhere each user and item serves as the node, linked directly to coarse-grained\nvirtual nodes and indirectly to fine-grained virtual nodes, ensuring different\ngrained semantic learning, while disclosing: 1) In contrast to coarse-grained\nsemantics, fine-grained semantics are well captured through sparse\nmeta-embeddings, which adaptively 2) balance the embedding uniqueness and\nmemory constraint. Additionally, the initialization method come up upon\nSparsePCA, along with a soft thresholding activation function to render the\nsparseness of the meta-embeddings. We propose a weight bridging update strategy\nthat focuses on matching each coarse-grained meta-embedding with several\nfine-grained meta-embeddings based on the users\/items' semantics. Extensive\nexperiments substantiate our method's superiority over existing baselines. Our\ncode is available at https:\/\/github.com\/htyjers\/C2F-MetaEmbed.\n","versions":"[{'version': 'v1', 'created': 'Tue, 21 Jan 2025 03:56:23 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 01:12:41 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Wang', 'Yang', ''], ['Liu', 'Haipeng', ''], ['Yi', 'Zeqian', ''], ['Qian', 'Biao', ''], ['Wang', 'Meng', '']]","extracted_entities":"[{'text': 'lightweight embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'meta-embeddings', 'label': 'Embedding'}, {'text': 'meta-embedding', 'label': 'Embedding'}, {'text': 'meta-embedding', 'label': 'Embedding'}, {'text': 'meta-embedding', 'label': 'Embedding'}, {'text': 'meta-embeddings', 'label': 'Embedding'}, {'text': 'meta-embeddings', 'label': 'Embedding'}, {'text': 'meta-embedding', 'label': 'Embedding'}, {'text': 'meta-embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2501.1467,"submitter":"Mark Cannon","authors":"Johannes Buerger and Mark Cannon","title":"Safe adaptive NMPC using ellipsoidal tubes","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  A computationally efficient nonlinear Model Predictive Control (NMPC)\nalgorithm is proposed for safe learning-based control with a system model\nrepresented by an incompletely known affine combination of basis functions and\nsubject to additive set-bounded disturbances. The proposed algorithm employs\nsuccessive linearization around predicted trajectories and accounts for the\nuncertain components of future states due to linearization, modelling errors\nand disturbances using ellipsoidal sets centered on the predicted nominal state\ntrajectory. An ellipsoidal tube-based approach ensures satisfaction of\nconstraints on control variables and model states. Feasibility is ensured using\nlocal bounds on linearization errors and a procedure based on a backtracking\nline search. We combine the approach with a set membership parameter estimation\nstrategy in numerical simulations. We show that the ellipsoidal embedding of\nthe predicted uncertainty scales favourably with the problem size. The\nresulting algorithm is recursively feasible and provides closed-loop stability\nand performance guarantees.\n","versions":"[{'version': 'v1', 'created': 'Fri, 24 Jan 2025 17:45:34 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 16:17:03 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Buerger', 'Johannes', ''], ['Cannon', 'Mark', '']]","extracted_entities":"[{'text': 'ellipsoidal embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"ellipsoidal embedding","similarity_score":0.6579141021}
{"id":2501.16944,"submitter":"Maximilian Muschalik","authors":"Maximilian Muschalik, Fabian Fumagalli, Paolo Frazzetto, Janine\n  Strotherm, Luca Hermes, Alessandro Sperduti, Eyke H\\\"ullermeier, Barbara\n  Hammer","title":"Exact Computation of Any-Order Shapley Interactions for Graph Neural\n  Networks","comments":"Preprint Version. Accepted at ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Albeit the ubiquitous use of Graph Neural Networks (GNNs) in machine learning\n(ML) prediction tasks involving graph-structured data, their interpretability\nremains challenging. In explainable artificial intelligence (XAI), the Shapley\nValue (SV) is the predominant method to quantify contributions of individual\nfeatures to a ML model's output. Addressing the limitations of SVs in complex\nprediction models, Shapley Interactions (SIs) extend the SV to groups of\nfeatures. In this work, we explain single graph predictions of GNNs with SIs\nthat quantify node contributions and interactions among multiple nodes. By\nexploiting the GNN architecture, we show that the structure of interactions in\nnode embeddings are preserved for graph prediction. As a result, the\nexponential complexity of SIs depends only on the receptive fields, i.e. the\nmessage-passing ranges determined by the connectivity of the graph and the\nnumber of convolutional layers. Based on our theoretical results, we introduce\nGraphSHAP-IQ, an efficient approach to compute any-order SIs exactly.\nGraphSHAP-IQ is applicable to popular message passing techniques in conjunction\nwith a linear global pooling and output layer. We showcase that GraphSHAP-IQ\nsubstantially reduces the exponential complexity of computing exact SIs on\nmultiple benchmark datasets. Beyond exact computation, we evaluate\nGraphSHAP-IQ's approximation of SIs on popular GNN architectures and compare\nwith existing baselines. Lastly, we visualize SIs of real-world water\ndistribution networks and molecule structures using a SI-Graph.\n","versions":"[{'version': 'v1', 'created': 'Tue, 28 Jan 2025 13:37:44 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 09:46:45 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Muschalik', 'Maximilian', ''], ['Fumagalli', 'Fabian', ''], ['Frazzetto', 'Paolo', ''], ['Strotherm', 'Janine', ''], ['Hermes', 'Luca', ''], ['Sperduti', 'Alessandro', ''], ['H\u00fcllermeier', 'Eyke', ''], ['Hammer', 'Barbara', '']]","extracted_entities":"[{'text': 'node embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"node embeddings","similarity_score":0.7718001008}
{"id":2502.01684,"submitter":"Srinitish Srinivasan","authors":"Srinitish Srinivasan and Omkumar CU","title":"Leveraging Joint Predictive Embedding and Bayesian Inference in Graph\n  Self Supervised Learning","comments":"Preprit. Under Review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.SI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Graph representation learning has emerged as a cornerstone for tasks like\nnode classification and link prediction, yet prevailing self-supervised\nlearning (SSL) methods face challenges such as computational inefficiency,\nreliance on contrastive objectives, and representation collapse. Existing\napproaches often depend on feature reconstruction, negative sampling, or\ncomplex decoders, which introduce training overhead and hinder generalization.\nFurther, current techniques which address such limitations fail to account for\nthe contribution of node embeddings to a certain prediction in the absence of\nlabeled nodes. To address these limitations, we propose a novel joint embedding\npredictive framework for graph SSL that eliminates contrastive objectives and\nnegative sampling while preserving semantic and structural information.\nAdditionally, we introduce a semantic-aware objective term that incorporates\npseudo-labels derived from Gaussian Mixture Models (GMMs), enhancing node\ndiscriminability by evaluating latent feature contributions. Extensive\nexperiments demonstrate that our framework outperforms state-of-the-art graph\nSSL methods across benchmarks, achieving superior performance without\ncontrastive loss or complex decoders. Key innovations include (1) a\nnon-contrastive, view-invariant joint embedding predictive architecture, (2)\nLeveraging single context and multiple targets relationship between subgraphs,\nand (3) GMM-based pseudo-label scoring to capture semantic contributions. This\nwork advances graph SSL by offering a computationally efficient,\ncollapse-resistant paradigm that bridges spatial and semantic graph features\nfor downstream tasks. The code for our paper can be found at\nhttps:\/\/github.com\/Deceptrax123\/JPEB-GSSL\n","versions":"[{'version': 'v1', 'created': 'Sun, 2 Feb 2025 07:42:45 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 08:45:19 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Srinivasan', 'Srinitish', ''], ['CU', 'Omkumar', '']]","extracted_entities":"[{'text': 'Graph representation learning', 'label': 'Few-shot Learning'}, {'text': 'node embeddings', 'label': 'Embedding'}, {'text': 'joint embedding', 'label': 'contextual Embedding'}]","assigned_concept":"Embedding","matched_keyword":"node embeddings","similarity_score":0.7718001008}
{"id":2502.07215,"submitter":"Osman Tursun","authors":"Osman Tursun, Sinan Kalkan, Simon Denman, Clinton Fookes","title":"PDV: Prompt Directional Vectors for Zero-shot Composed Image Retrieval","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Zero-shot composed image retrieval (ZS-CIR) enables image search using a\nreference image and text prompt without requiring specialized text-image\ncomposition networks trained on large-scale paired data. However, current\nZS-CIR approaches face three critical limitations in their reliance on composed\ntext embeddings: static query embedding representations, insufficient\nutilization of image embeddings, and suboptimal performance when fusing text\nand image embeddings. To address these challenges, we introduce the Prompt\nDirectional Vector (PDV), a simple yet effective training-free enhancement that\ncaptures semantic modifications induced by user prompts. PDV enables three key\nimprovements: (1) dynamic composed text embeddings where prompt adjustments are\ncontrollable via a scaling factor, (2) composed image embeddings through\nsemantic transfer from text prompts to image features, and (3) weighted fusion\nof composed text and image embeddings that enhances retrieval by balancing\nvisual and semantic similarity. Our approach serves as a plug-and-play\nenhancement for existing ZS-CIR methods with minimal computational overhead.\nExtensive experiments across multiple benchmarks demonstrate that PDV\nconsistently improves retrieval performance when integrated with\nstate-of-the-art ZS-CIR approaches, particularly for methods that generate\naccurate compositional embeddings. The code will be publicly available.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Feb 2025 03:20:21 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 01:26:06 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Tursun', 'Osman', ''], ['Kalkan', 'Sinan', ''], ['Denman', 'Simon', ''], ['Fookes', 'Clinton', '']]","extracted_entities":"[{'text': 'composed\\ntext embeddings', 'label': 'Embedding'}, {'text': 'static query embedding representations', 'label': 'Embedding'}, {'text': 'image embeddings', 'label': 'Embedding'}, {'text': 'user prompts', 'label': 'Prompting'}, {'text': 'composed text embeddings', 'label': 'Embedding'}, {'text': 'scaling factor', 'label': 'Scaling law'}, {'text': 'composed image embeddings', 'label': 'Embedding'}, {'text': 'text prompts', 'label': 'Prompting'}, {'text': 'composed text and image embeddings', 'label': 'Embedding'}, {'text': 'publicly available', 'label': 'Open-source LLMs'}]","assigned_concept":"Embedding","matched_keyword":"image embeddings","similarity_score":0.8342078924}
{"id":2503.00183,"submitter":"Jeffrey Adler","authors":"Jeffrey D. Adler, Joshua M. Lansky, and Loren Spice","title":"On smooth-group actions on reductive groups and spherical buildings","comments":"With an appendix by Sean Cotner, Joshua M. Lansky, and Loren Spice.\n  v2: revisions to appendix","journal-ref":null,"doi":null,"report-no":null,"categories":"math.RT math.AG math.GR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Let $k$ be a field, and suppose that $\\Gamma$ is a smooth $k$-group that acts\non a connected, reductive $k$-group $\\widetilde G$. Let $G$ denote the maximal\nsmooth, connected subgroup of the group of $\\Gamma$-fixed points in $\\widetilde\nG$. Under fairly general conditions, we show that $G$ is a reductive $k$-group,\nand that the image of the functorial embedding $\\mathscr{S}(G) \\longrightarrow\n\\mathscr{S}(\\widetilde G)$ of spherical buildings is the set of\n``$\\Gamma$-fixed points in $\\mathscr{S}(\\widetilde G)$'', in a suitable sense.\nIn particular, we do not need to assume that $\\Gamma$ has order relatively\nprime to the characteristic of $k$ (nor even that $\\Gamma$ is finite), nor that\nthe action of $\\Gamma$ preserves a Borel-torus pair in $\\widetilde G$.\n","versions":"[{'version': 'v1', 'created': 'Fri, 28 Feb 2025 21:00:31 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 17:57:43 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Adler', 'Jeffrey D.', ''], ['Lansky', 'Joshua M.', ''], ['Spice', 'Loren', '']]","extracted_entities":"[{'text': 'functorial embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"functorial embedding","similarity_score":0.6446795464}
{"id":2503.0431,"submitter":"Guillermo Garc\\'ia-S\\'aez","authors":"Jos\\'e Carlos Bellido and Guillermo Garc\\'ia-S\\'aez","title":"Bessel Potential Spaces and Complex Interpolation: Continuous embeddings","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.FA","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Bessel potential spaces, introduced in the 1960s, are derived through complex\ninterpolation between Lebesgue and Sobolev spaces, making them intermediate\nspaces of fractional differentiability order. Bessel potential spaces have\nrecently gained attention due to their identification with the Riesz fractional\ngradient. This paper explores Bessel potential spaces as complex interpolation\nspaces, providing original proofs of fundamental properties based on abstract\ninterpolation theory. Main results include a direct proof of norm equivalence,\ncontinuous embeddings, and the relationship with Gagliardo spaces.\n","versions":"[{'version': 'v1', 'created': 'Thu, 6 Mar 2025 10:53:14 GMT'}, {'version': 'v2', 'created': 'Fri, 7 Mar 2025 12:28:04 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 17:37:29 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Bellido', 'Jos\u00e9 Carlos', ''], ['Garc\u00eda-S\u00e1ez', 'Guillermo', '']]","extracted_entities":"[{'text': 'Bessel potential spaces', 'label': 'BERT'}, {'text': 'Bessel potential spaces', 'label': 'BERT'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'Bessel potential spaces', 'label': 'BERT'}, {'text': 'continuous embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"continuous embeddings","similarity_score":0.8004128337}
{"id":2503.08049,"submitter":"Nadarasar Bahavan","authors":"Nadarasar Bahavan, Sachith Seneviratne, Saman Halgamuge","title":"SphOR: A Representation Learning Perspective on Open-set Recognition for\n  Identifying Unknown Classes in Deep Learning Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The widespread use of deep learning classifiers necessitates Open-set\nrecognition (OSR), which enables the identification of input data not only from\nclasses known during training but also from unknown classes that might be\npresent in test data. Many existing OSR methods are computationally expensive\ndue to the reliance on complex generative models or suffer from high training\ncosts. We investigate OSR from a representation-learning perspective,\nspecifically through spherical embeddings. We introduce SphOR, a\ncomputationally efficient representation learning method that models the\nfeature space as a mixture of von Mises-Fisher distributions. This approach\nenables the use of semantically ambiguous samples during training, to improve\nthe detection of samples from unknown classes. We further explore the\nrelationship between OSR performance and key representation learning properties\nwhich influence how well features are structured in high-dimensional space.\nExtensive experiments on multiple OSR benchmarks demonstrate the effectiveness\nof our method, producing state-of-the-art results, with improvements up-to 6%\nthat validate its performance. Code at\nhttps:\/\/github.com\/nadarasarbahavan\/SpHOR\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 05:06:11 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 22:03:31 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Bahavan', 'Nadarasar', ''], ['Seneviratne', 'Sachith', ''], ['Halgamuge', 'Saman', '']]","extracted_entities":"[{'text': 'spherical embeddings', 'label': 'Embedding'}, {'text': 'SphOR', 'label': 'Generative Pre-trained Transformer (GPT)'}]","assigned_concept":"Embedding","matched_keyword":"spherical embeddings","similarity_score":0.6692852974}
{"id":2503.09101,"submitter":"Mohammad Tariqul Islam","authors":"Mohammad Tariqul Islam, Jason W. Fleischer","title":"The Shape of Attraction in UMAP: Exploring the Embedding Forces in\n  Dimensionality Reduction","comments":"9 page + appendix","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Uniform manifold approximation and projection (UMAP) is among the most\npopular neighbor embedding methods. The method relies on attractive and\nrepulsive forces among high-dimensional data points to obtain a low-dimensional\nembedding. In this paper, we analyze the forces to reveal their effects on\ncluster formations and visualization. Repulsion emphasizes differences,\ncontrolling cluster boundaries and inter-cluster distance. Attraction is more\nsubtle, as attractive tension between points can manifest simultaneously as\nattraction and repulsion in the lower-dimensional mapping. This explains the\nneed for learning rate annealing and motivates the different treatments between\nattractive and repulsive terms. Moreover, by modifying attraction, we improve\nthe consistency of cluster formation under random initialization. Overall, our\nanalysis makes UMAP and similar embedding methods more interpretable, more\nrobust, and more accurate.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 06:37:43 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 15:48:38 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Islam', 'Mohammad Tariqul', ''], ['Fleischer', 'Jason W.', '']]","extracted_entities":"[{'text': 'Uniform manifold approximation and projection', 'label': 'Embedding'}, {'text': 'UMAP', 'label': 'Embedding'}, {'text': 'low-dimensional\\nembedding', 'label': 'Embedding'}, {'text': 'UMAP', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"low-dimensional\nembedding","similarity_score":0.7721446753}
{"id":2503.09248,"submitter":"Lihua Zhou","authors":"Lihua Zhou, Mao Ye, Shuaifeng Li, Nianxin Li, Xiatian Zhu, Lei Deng,\n  Hongbin Liu, Zhen Lei","title":"Bayesian Test-Time Adaptation for Vision-Language Models","comments":"Accepted to CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Test-time adaptation with pre-trained vision-language models, such as CLIP,\naims to adapt the model to new, potentially out-of-distribution test data.\nExisting methods calculate the similarity between visual embedding and\nlearnable class embeddings, which are initialized by text embeddings, for\nzero-shot image classification. In this work, we first analyze this process\nbased on Bayes theorem, and observe that the core factors influencing the final\nprediction are the likelihood and the prior. However, existing methods\nessentially focus on adapting class embeddings to adapt likelihood, but they\noften ignore the importance of prior. To address this gap, we propose a novel\napproach, \\textbf{B}ayesian \\textbf{C}lass \\textbf{A}daptation (BCA), which in\naddition to continuously updating class embeddings to adapt likelihood, also\nuses the posterior of incoming samples to continuously update the prior for\neach class embedding. This dual updating mechanism allows the model to better\nadapt to distribution shifts and achieve higher prediction accuracy. Our method\nnot only surpasses existing approaches in terms of performance metrics but also\nmaintains superior inference rates and memory usage, making it highly efficient\nand practical for real-world applications.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 10:42:11 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 06:59:16 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Zhou', 'Lihua', ''], ['Ye', 'Mao', ''], ['Li', 'Shuaifeng', ''], ['Li', 'Nianxin', ''], ['Zhu', 'Xiatian', ''], ['Deng', 'Lei', ''], ['Liu', 'Hongbin', ''], ['Lei', 'Zhen', '']]","extracted_entities":"[{'text': 'CLIP', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'visual embedding', 'label': 'Embedding'}, {'text': 'class embeddings', 'label': 'Embedding'}, {'text': 'text embeddings', 'label': 'Embedding'}, {'text': 'Bayes theorem', 'label': 'Few-shot Learning'}, {'text': 'class embeddings', 'label': 'Embedding'}, {'text': 'class embeddings', 'label': 'Embedding'}, {'text': 'class embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"visual embedding","similarity_score":0.8153178692}
{"id":2503.09496,"submitter":"Junjie Zhou","authors":"Junjie Zhou, Jiao Tang, Yingli Zuo, Peng Wan, Daoqiang Zhang, Wei Shao","title":"Robust Multimodal Survival Prediction with the Latent Differentiation\n  Conditional Variational AutoEncoder","comments":"Accepted by CVPR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The integrative analysis of histopathological images and genomic data has\nreceived increasing attention for survival prediction of human cancers.\nHowever, the existing studies always hold the assumption that full modalities\nare available. As a matter of fact, the cost for collecting genomic data is\nhigh, which sometimes makes genomic data unavailable in testing samples. A\ncommon way of tackling such incompleteness is to generate the genomic\nrepresentations from the pathology images. Nevertheless, such strategy still\nfaces the following two challenges: (1) The gigapixel whole slide images (WSIs)\nare huge and thus hard for representation. (2) It is difficult to generate the\ngenomic embeddings with diverse function categories in a unified generative\nframework. To address the above challenges, we propose a Conditional Latent\nDifferentiation Variational AutoEncoder (LD-CVAE) for robust multimodal\nsurvival prediction, even with missing genomic data. Specifically, a\nVariational Information Bottleneck Transformer (VIB-Trans) module is proposed\nto learn compressed pathological representations from the gigapixel WSIs. To\ngenerate different functional genomic features, we develop a novel Latent\nDifferentiation Variational AutoEncoder (LD-VAE) to learn the common and\nspecific posteriors for the genomic embeddings with diverse functions. Finally,\nwe use the product-of-experts technique to integrate the genomic common\nposterior and image posterior for the joint latent distribution estimation in\nLD-CVAE. We test the effectiveness of our method on five different cancer\ndatasets, and the experimental results demonstrate its superiority in both\ncomplete and missing modality scenarios.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 15:58:37 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 07:15:08 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Zhou', 'Junjie', ''], ['Tang', 'Jiao', ''], ['Zuo', 'Yingli', ''], ['Wan', 'Peng', ''], ['Zhang', 'Daoqiang', ''], ['Shao', 'Wei', '']]","extracted_entities":"[{'text': 'genomic\\nrepresentations', 'label': 'Embedding'}, {'text': 'genomic embeddings', 'label': 'Embedding'}, {'text': 'genomic embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"genomic embeddings","similarity_score":0.6675938368}
{"id":2503.10772,"submitter":"Ju He","authors":"Ju He, Qihang Yu, Qihao Liu, Liang-Chieh Chen","title":"FlowTok: Flowing Seamlessly Across Text and Image Tokens","comments":"Project page at https:\/\/tacju.github.io\/projects\/flowtok.html","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Bridging different modalities lies at the heart of cross-modality generation.\nWhile conventional approaches treat the text modality as a conditioning signal\nthat gradually guides the denoising process from Gaussian noise to the target\nimage modality, we explore a much simpler paradigm-directly evolving between\ntext and image modalities through flow matching. This requires projecting both\nmodalities into a shared latent space, which poses a significant challenge due\nto their inherently different representations: text is highly semantic and\nencoded as 1D tokens, whereas images are spatially redundant and represented as\n2D latent embeddings. To address this, we introduce FlowTok, a minimal\nframework that seamlessly flows across text and images by encoding images into\na compact 1D token representation. Compared to prior methods, this design\nreduces the latent space size by 3.3x at an image resolution of 256,\neliminating the need for complex conditioning mechanisms or noise scheduling.\nMoreover, FlowTok naturally extends to image-to-text generation under the same\nformulation. With its streamlined architecture centered around compact 1D\ntokens, FlowTok is highly memory-efficient, requires significantly fewer\ntraining resources, and achieves much faster sampling speeds-all while\ndelivering performance comparable to state-of-the-art models. Code will be\navailable at https:\/\/github.com\/bytedance\/1d-tokenizer.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 18:06:13 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:39:37 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['He', 'Ju', ''], ['Yu', 'Qihang', ''], ['Liu', 'Qihao', ''], ['Chen', 'Liang-Chieh', '']]","extracted_entities":"[{'text': '2D latent embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"2D latent embeddings","similarity_score":0.7135133743}
{"id":2503.11022,"submitter":"Hai Huang","authors":"Hai Huang, Ziteng Xu, and Zhaoyu Zhang","title":"Towards Efficient PCSEL Design: A Data-Driven Approach for Design\n  Insights","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.optics","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present a data-driven design approach for photonic crystals to achieve\nhigh efficiency in photonic crystal surface-emitting lasers (PCSELs). By\ndiscretizing the photonic crystal structure into a grid, we enable the\ngeneration of arbitrary lattice designs. Multiple fully connected layers\ncombined with a position embedding module extract essential features from the\nphotonic crystal designs, while coupled-wave theory (CWT) is used to evaluate\nthe efficiency (based on the ratio of surface-emitting to edge-emitting\nresonant) and quality factor Q. We introduce the Neural Networks (NNs) model to\nevaluate the structures, and to find a better performance design according to\nthe evaluation result. The model achieves high prediction accuracy, with\nPearson correlation coefficients of 0.780 for SEE and 0.887 for the\nlog-transformed Q. Additionally, we perform Shapley value analysis to identify\nthe most important Fourier coefficients, providing insights into the factors\nthat impact the performance of PCSEL designs. Our work speeds up the design\nprocess and offers valuable guidance for optimizing high-performance PCSELs,\nsupporting the development of fully photonic design automation (PDA).\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 02:40:30 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 10:14:24 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 06:27:45 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Huang', 'Hai', ''], ['Xu', 'Ziteng', ''], ['Zhang', 'Zhaoyu', '']]","extracted_entities":"[{'text': 'position embedding module', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"position embedding module","similarity_score":0.6141906977}
{"id":2503.12358,"submitter":"In-Chang Baek","authors":"In-Chang Baek, Sung-Hyun Kim, Seo-Young Lee, Dong-Hyeun Kim,\n  Kyung-Joong Kim","title":"IPCGRL: Language-Instructed Reinforcement Learning for Procedural Level\n  Generation","comments":"9 pages, 9 figures, 3 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent research has highlighted the significance of natural language in\nenhancing the controllability of generative models. While various efforts have\nbeen made to leverage natural language for content generation, research on deep\nreinforcement learning (DRL) agents utilizing text-based instructions for\nprocedural content generation remains limited. In this paper, we propose\nIPCGRL, an instruction-based procedural content generation method via\nreinforcement learning, which incorporates a sentence embedding model. IPCGRL\nfine-tunes task-specific embedding representations to effectively compress\ngame-level conditions. We evaluate IPCGRL in a two-dimensional level generation\ntask and compare its performance with a general-purpose embedding method. The\nresults indicate that IPCGRL achieves up to a 21.4% improvement in\ncontrollability and a 17.2% improvement in generalizability for unseen\ninstructions. Furthermore, the proposed method extends the modality of\nconditional input, enabling a more flexible and expressive interaction\nframework for procedural content generation.\n","versions":"[{'version': 'v1', 'created': 'Sun, 16 Mar 2025 04:53:38 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 05:22:24 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Baek', 'In-Chang', ''], ['Kim', 'Sung-Hyun', ''], ['Lee', 'Seo-Young', ''], ['Kim', 'Dong-Hyeun', ''], ['Kim', 'Kyung-Joong', '']]","extracted_entities":"[{'text': 'task-specific embedding representations', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"task-specific embedding representations","similarity_score":0.6056550741}
{"id":2503.12713,"submitter":"Hanul Jeon","authors":"Hanul Jeon","title":"Martin's measurable dilator","comments":"47 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.LO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Martin's remarking proof of $\\mathbf{\\Pi}^1_2$-determinacy from an iterable\nrank-into-rank embedding highlighted the connection between large cardinals and\ndeterminacy. In this paper, we isolate a large cardinal object called a\nmeasurable dilator from Martin's proof of $\\mathbf{\\Pi}^1_2$-determinacy, which\ncaptures the structural essence of Martin's proof of\n$\\mathbf{\\Pi}^1_2$-determinacy.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 00:48:05 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Jeon', 'Hanul', '']]","extracted_entities":"[{'text': 'iterable\\nrank-into-rank embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"iterable\nrank-into-rank embedding","similarity_score":0.5353941917}
{"id":2503.1272,"submitter":"Feng Qiao","authors":"Feng Qiao, Zhexiao Xiong, Eric Xing, Nathan Jacobs","title":"GenStereo: Towards Open-World Generation of Stereo Images and\n  Unsupervised Matching","comments":"Project page is available at https:\/\/qjizhi.github.io\/genstereo","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Stereo images are fundamental to numerous applications, including extended\nreality (XR) devices, autonomous driving, and robotics. Unfortunately,\nacquiring high-quality stereo images remains challenging due to the precise\ncalibration requirements of dual-camera setups and the complexity of obtaining\naccurate, dense disparity maps. Existing stereo image generation methods\ntypically focus on either visual quality for viewing or geometric accuracy for\nmatching, but not both. We introduce GenStereo, a diffusion-based approach, to\nbridge this gap. The method includes two primary innovations (1) conditioning\nthe diffusion process on a disparity-aware coordinate embedding and a warped\ninput image, allowing for more precise stereo alignment than previous methods,\nand (2) an adaptive fusion mechanism that intelligently combines the\ndiffusion-generated image with a warped image, improving both realism and\ndisparity consistency. Through extensive training on 11 diverse stereo\ndatasets, GenStereo demonstrates strong generalization ability. GenStereo\nachieves state-of-the-art performance in both stereo image generation and\nunsupervised stereo matching tasks. Our framework eliminates the need for\ncomplex hardware setups while enabling high-quality stereo image generation,\nmaking it valuable for both real-world applications and unsupervised learning\nscenarios. Project page is available at https:\/\/qjizhi.github.io\/genstereo\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 01:19:28 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Qiao', 'Feng', ''], ['Xiong', 'Zhexiao', ''], ['Xing', 'Eric', ''], ['Jacobs', 'Nathan', '']]","extracted_entities":"[{'text': 'disparity-aware coordinate embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"disparity-aware coordinate embedding","similarity_score":0.5408298373}
{"id":2503.12739,"submitter":"Tianyu Zong","authors":"Tianyu Zong, Bingkang Shi, Hongzhu Yi, Jungang Xu","title":"TNCSE: Tensor's Norm Constraints for Unsupervised Contrastive Learning\n  of Sentence Embeddings","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Unsupervised sentence embedding representation has become a hot research\ntopic in natural language processing. As a tensor, sentence embedding has two\ncritical properties: direction and norm. Existing works have been limited to\nconstraining only the orientation of the samples' representations while\nignoring the features of their module lengths. To address this issue, we\npropose a new training objective that optimizes the training of unsupervised\ncontrastive learning by constraining the module length features between\npositive samples. We combine the training objective of Tensor's Norm\nConstraints with ensemble learning to propose a new Sentence Embedding\nrepresentation framework, TNCSE. We evaluate seven semantic text similarity\ntasks, and the results show that TNCSE and derived models are the current\nstate-of-the-art approach; in addition, we conduct extensive zero-shot\nevaluations, and the results show that TNCSE outperforms other baselines.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 02:14:42 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Zong', 'Tianyu', ''], ['Shi', 'Bingkang', ''], ['Yi', 'Hongzhu', ''], ['Xu', 'Jungang', '']]","extracted_entities":"[{'text': 'sentence embedding', 'label': 'Embedding'}, {'text': 'sentence embedding', 'label': 'Embedding'}, {'text': 'ensemble learning', 'label': 'Few-shot Learning'}, {'text': 'Sentence Embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"sentence embedding","similarity_score":0.7457362413}
{"id":2503.12814,"submitter":"Jinseok Bae","authors":"Jinseok Bae, Jungdam Won, Donggeun Lim, Inwoo Hwang, Young Min Kim","title":"Versatile Physics-based Character Control with Hybrid Latent\n  Representation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GR cs.AI cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present a versatile latent representation that enables physically\nsimulated character to efficiently utilize motion priors. To build a powerful\nmotion embedding that is shared across multiple tasks, the physics controller\nshould employ rich latent space that is easily explored and capable of\ngenerating high-quality motion. We propose integrating continuous and discrete\nlatent representations to build a versatile motion prior that can be adapted to\na wide range of challenging control tasks. Specifically, we build a discrete\nlatent model to capture distinctive posterior distribution without collapse,\nand simultaneously augment the sampled vector with the continuous residuals to\ngenerate high-quality, smooth motion without jittering. We further incorporate\nResidual Vector Quantization, which not only maximizes the capacity of the\ndiscrete motion prior, but also efficiently abstracts the action space during\nthe task learning phase. We demonstrate that our agent can produce diverse yet\nsmooth motions simply by traversing the learned motion prior through\nunconditional motion generation. Furthermore, our model robustly satisfies\nsparse goal conditions with highly expressive natural motions, including\nhead-mounted device tracking and motion in-betweening at irregular intervals,\nwhich could not be achieved with existing latent representations.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 04:45:51 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Bae', 'Jinseok', ''], ['Won', 'Jungdam', ''], ['Lim', 'Donggeun', ''], ['Hwang', 'Inwoo', ''], ['Kim', 'Young Min', '']]","extracted_entities":"[{'text': 'motion embedding', 'label': 'Embedding'}, {'text': 'Residual Vector Quantization', 'label': 'quantisation'}]","assigned_concept":"Embedding","matched_keyword":"motion embedding","similarity_score":0.6700158119}
{"id":2503.12834,"submitter":"Seunggwan Lee","authors":"Seunggwan Lee, Hwanhee Jung, Byoungsoo Koh, Qixing Huang, Sangho Yoon,\n  Sangpil Kim","title":"PASTA: Part-Aware Sketch-to-3D Shape Generation with Text-Aligned Prior","comments":"19 pages, 18 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  A fundamental challenge in conditional 3D shape generation is to minimize the\ninformation loss and maximize the intention of user input. Existing approaches\nhave predominantly focused on two types of isolated conditional signals, i.e.,\nuser sketches and text descriptions, each of which does not offer flexible\ncontrol of the generated shape. In this paper, we introduce PASTA, the flexible\napproach that seamlessly integrates a user sketch and a text description for 3D\nshape generation. The key idea is to use text embeddings from a vision-language\nmodel to enrich the semantic representation of sketches. Specifically, these\ntext-derived priors specify the part components of the object, compensating for\nmissing visual cues from ambiguous sketches. In addition, we introduce ISG-Net\nwhich employs two types of graph convolutional networks: IndivGCN, which\nprocesses fine-grained details, and PartGCN, which aggregates these details\ninto parts and refines the structure of objects. Extensive experiments\ndemonstrate that PASTA outperforms existing methods in part-level editing and\nachieves state-of-the-art results in sketch-to-3D shape generation.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 05:31:09 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Lee', 'Seunggwan', ''], ['Jung', 'Hwanhee', ''], ['Koh', 'Byoungsoo', ''], ['Huang', 'Qixing', ''], ['Yoon', 'Sangho', ''], ['Kim', 'Sangpil', '']]","extracted_entities":"[{'text': 'text embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"text embeddings","similarity_score":0.812117815}
{"id":2503.12836,"submitter":"Sumin In","authors":"Sumin In, Youngdong Jang, Utae Jeong, MinHyuk Jang, Hyeongcheol Park,\n  Eunbyung Park, Sangpil Kim","title":"CompMarkGS: Robust Watermarking for Compression 3D Gaussian Splatting","comments":"23 pages, 17 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  3D Gaussian Splatting (3DGS) enables rapid differentiable rendering for 3D\nreconstruction and novel view synthesis, leading to its widespread commercial\nuse. Consequently, copyright protection via watermarking has become critical.\nHowever, because 3DGS relies on millions of Gaussians, which require gigabytes\nof storage, efficient transfer and storage require compression. Existing 3DGS\nwatermarking methods are vulnerable to quantization-based compression, often\nresulting in the loss of the embedded watermark. To address this challenge, we\npropose a novel watermarking method that ensures watermark robustness after\nmodel compression while maintaining high rendering quality. In detail, we\nincorporate a quantization distortion layer that simulates compression during\ntraining, preserving the watermark under quantization-based compression. Also,\nwe propose a learnable watermark embedding feature that embeds the watermark\ninto the anchor feature, ensuring structural consistency and seamless\nintegration into the 3D scene. Furthermore, we present a frequency-aware anchor\ngrowing mechanism to enhance image quality in high-frequency regions by\neffectively identifying Guassians within these regions. Experimental results\nconfirm that our method preserves the watermark and maintains superior image\nquality under high compression, validating it as a promising approach for a\nsecure 3DGS model.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 05:32:15 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['In', 'Sumin', ''], ['Jang', 'Youngdong', ''], ['Jeong', 'Utae', ''], ['Jang', 'MinHyuk', ''], ['Park', 'Hyeongcheol', ''], ['Park', 'Eunbyung', ''], ['Kim', 'Sangpil', '']]","extracted_entities":"[{'text': 'compression', 'label': 'quantisation'}, {'text': 'quantization-based compression', 'label': 'quantisation'}, {'text': 'compression', 'label': 'quantisation'}, {'text': 'compression', 'label': 'quantisation'}, {'text': 'quantization-based compression', 'label': 'quantisation'}, {'text': 'learnable watermark embedding feature', 'label': 'Embedding'}, {'text': 'compression', 'label': 'quantisation'}]","assigned_concept":"Embedding","matched_keyword":"learnable watermark embedding feature","similarity_score":0.5608842373}
{"id":2503.12893,"submitter":"Masanari Kimura","authors":"Masanari Kimura","title":"Edgeworth Expansion for Semi-hard Triplet Loss","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We develop a higher-order asymptotic analysis for the semi-hard triplet loss\nusing the Edgeworth expansion. It is known that this loss function enforces\nthat embeddings of similar samples are close while those of dissimilar samples\nare separated by a specified margin. By refining the classical central limit\ntheorem, our approach quantifies the impact of the margin parameter and the\nskewness of the underlying data distribution on the loss behavior. In\nparticular, we derive explicit Edgeworth expansions that reveal first-order\ncorrections in terms of the third cumulant, thereby characterizing non-Gaussian\neffects present in the distribution of distance differences between\nanchor-positive and anchor-negative pairs. Our findings provide detailed\ninsight into the sensitivity of the semi-hard triplet loss to its parameters\nand offer guidance for choosing the margin to ensure training stability.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 07:46:10 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Kimura', 'Masanari', '']]","extracted_entities":"[{'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2503.12896,"submitter":"Shuaifan Jin","authors":"Shuaifan Jin, Xiaoyi Pang, Zhibo Wang, He Wang, Jiacheng Du, Jiahui\n  Hu, Kui Ren","title":"Safeguarding LLM Embeddings in End-Cloud Collaboration via\n  Entropy-Driven Perturbation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent studies improve on-device language model (LM) inference through\nend-cloud collaboration, where the end device retrieves useful information from\ncloud databases to enhance local processing, known as Retrieval-Augmented\nGeneration (RAG). Typically, to retrieve information from the cloud while\nsafeguarding privacy, the end device transforms original data into embeddings\nwith a local embedding model. However, the recently emerging Embedding\nInversion Attacks (EIAs) can still recover the original data from text\nembeddings (e.g., training a recovery model to map embeddings back to original\ntexts), posing a significant threat to user privacy. To address this risk, we\npropose EntroGuard, an entropy-driven perturbation-based embedding privacy\nprotection method, which can protect the privacy of text embeddings while\nmaintaining retrieval accuracy during the end-cloud collaboration.\nSpecifically, to defeat various EIAs, we perturb the embeddings to increase the\nentropy of the recovered text in the common structure of recovery models, thus\nsteering the embeddings toward meaningless texts rather than original sensitive\ntexts during the recovery process. To maintain retrieval performance in the\ncloud, we constrain the perturbations within a bound, applying the strategy of\nreducing them where redundant and increasing them where sparse. Moreover,\nEntroGuard can be directly integrated into end devices without requiring any\nmodifications to the embedding model. Extensive experimental results\ndemonstrate that EntroGuard can reduce the risk of privacy leakage by up to 8\ntimes at most with negligible loss of retrieval performance compared to\nexisting privacy-preserving methods.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 07:58:05 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Jin', 'Shuaifan', ''], ['Pang', 'Xiaoyi', ''], ['Wang', 'Zhibo', ''], ['Wang', 'He', ''], ['Du', 'Jiacheng', ''], ['Hu', 'Jiahui', ''], ['Ren', 'Kui', '']]","extracted_entities":"[{'text': 'Retrieval-Augmented\\nGeneration (RAG)', 'label': 'RAG'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2503.12953,"submitter":"Zheyuan Liu","authors":"Zheyuan Liu, Junyan Wang, Zicheng Duan, Cristian Rodriguez-Opazo,\n  Anton van den Hengel","title":"Frame-wise Conditioning Adaptation for Fine-Tuning Diffusion Models in\n  Text-to-Video Prediction","comments":"20 pages, 15 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Text-video prediction (TVP) is a downstream video generation task that\nrequires a model to produce subsequent video frames given a series of initial\nvideo frames and text describing the required motion. In practice TVP methods\nfocus on a particular category of videos depicting manipulations of objects\ncarried out by human beings or robot arms. Previous methods adapt models\npre-trained on text-to-image tasks, and thus tend to generate video that lacks\nthe required continuity. A natural progression would be to leverage more recent\npre-trained text-to-video (T2V) models. This approach is rendered more\nchallenging by the fact that the most common fine-tuning technique, low-rank\nadaptation (LoRA), yields undesirable results. In this work, we propose an\nadaptation-based strategy we label Frame-wise Conditioning Adaptation (FCA).\nWithin the module, we devise a sub-module that produces frame-wise text\nembeddings from the input text, which acts as an additional text condition to\naid generation. We use FCA to fine-tune the T2V model, which incorporates the\ninitial frame(s) as an extra condition. We compare and discuss the more\neffective strategy for injecting such embeddings into the T2V model. We conduct\nextensive ablation studies on our design choices with quantitative and\nqualitative performance analysis. Our approach establishes a new\nstate-of-the-art for the task of TVP. The project page is at\nhttps:\/\/github.com\/Cuberick-Orion\/FCA .\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 09:06:21 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Liu', 'Zheyuan', ''], ['Wang', 'Junyan', ''], ['Duan', 'Zicheng', ''], ['Rodriguez-Opazo', 'Cristian', ''], ['Hengel', 'Anton van den', '']]","extracted_entities":"[{'text': 'frame-wise text\\nembeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"frame-wise text\nembeddings","similarity_score":0.6638262272}
{"id":2503.12994,"submitter":"Vincent Labatut","authors":"No\\'e Cecillon (LIA), Vincent Labatut (LIA), Richard Dufour (LS2N -\n  \\'equipe TALN)","title":"Conversation-Based Multimodal Abuse Detection Through Text and Graph\n  Embeddings","comments":null,"journal-ref":"Computing, 2025","doi":null,"report-no":null,"categories":"cs.SI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Abusive behavior is common on online social networks, and forces the hosts of\nsuch platforms to find new solutions to address this problem. Various methods\nhave been proposed to automate this task in the past decade. Most of them rely\non the exchanged content, but ignore the structure and dynamics of the\nconversation, which could provide some relevant information. In this article,\nwe propose to use representation learning methods to automatically produce\nembeddings of this textual content and of the conversational graphs depicting\nmessage exchanges. While the latter could be enhanced by including additional\ninformation on top of the raw conversational structure, no method currently\nexists to learn wholegraph representations using simultaneously edge\ndirections, weights, signs, and vertex attributes. We propose two such methods\nto fill this gap in the literature. We experiment with 5 textual and 13 graph\nembedding methods, and apply them to a dataset of online messages annotated for\nabuse detection. Our best results achieve an F -measure of 81.02 using text\nalone and 80.61 using graphs alone. We also combine both modalities of\ninformation (text and graphs) through three fusion strategies, and show that\nthis strongly improves abuse detection performance, increasing the F -measure\nto 87.06. Finally, we identify which specific engineered features are captured\nby the embedding methods under consideration. These features have clear\ninterpretations and help explain what information the representation learning\nmethods deem discriminative.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 09:51:17 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Cecillon', 'No\u00e9', '', 'LIA'], ['Labatut', 'Vincent', '', 'LIA'], ['Dufour', 'Richard', '', 'LS2N -\\n  \u00e9quipe TALN']]","extracted_entities":"[{'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2503.13012,"submitter":"Xingguo Lv","authors":"Xingguo Lv, Xingbo Dong, Liwen Wang, Jiewen Yang, Lei Zhao, Bin Pu,\n  Zhe Jin, Xuejun Li","title":"Test-Time Domain Generalization via Universe Learning: A Multi-Graph\n  Matching Approach for Medical Image Segmentation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Despite domain generalization (DG) has significantly addressed the\nperformance degradation of pre-trained models caused by domain shifts, it often\nfalls short in real-world deployment. Test-time adaptation (TTA), which adjusts\na learned model using unlabeled test data, presents a promising solution.\nHowever, most existing TTA methods struggle to deliver strong performance in\nmedical image segmentation, primarily because they overlook the crucial prior\nknowledge inherent to medical images. To address this challenge, we incorporate\nmorphological information and propose a framework based on multi-graph\nmatching. Specifically, we introduce learnable universe embeddings that\nintegrate morphological priors during multi-source training, along with novel\nunsupervised test-time paradigms for domain adaptation. This approach\nguarantees cycle-consistency in multi-matching while enabling the model to more\neffectively capture the invariant priors of unseen data, significantly\nmitigating the effects of domain shifts. Extensive experiments demonstrate that\nour method outperforms other state-of-the-art approaches on two medical image\nsegmentation benchmarks for both multi-source and single-source domain\ngeneralization tasks. The source code is available at\nhttps:\/\/github.com\/Yore0\/TTDG-MGM.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 10:11:11 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Lv', 'Xingguo', ''], ['Dong', 'Xingbo', ''], ['Wang', 'Liwen', ''], ['Yang', 'Jiewen', ''], ['Zhao', 'Lei', ''], ['Pu', 'Bin', ''], ['Jin', 'Zhe', ''], ['Li', 'Xuejun', '']]","extracted_entities":"[{'text': 'multi-graph\\nmatching', 'label': 'Embedding'}, {'text': 'learnable universe embeddings', 'label': 'Embedding'}, {'text': 'multi-source training', 'label': 'Few-shot Learning'}]","assigned_concept":"Embedding","matched_keyword":"learnable universe embeddings","similarity_score":0.6419995427}
{"id":2503.13045,"submitter":"Gabriele Berton","authors":"Gabriele Berton, Kevin Musgrave, Carlo Masone","title":"All You Need to Know About Training Image Retrieval Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Image retrieval is the task of finding images in a database that are most\nsimilar to a given query image. The performance of an image retrieval pipeline\ndepends on many training-time factors, including the embedding model\narchitecture, loss function, data sampler, mining function, learning rate(s),\nand batch size. In this work, we run tens of thousands of training runs to\nunderstand the effect each of these factors has on retrieval accuracy. We also\ndiscover best practices that hold across multiple datasets. The code is\navailable at https:\/\/github.com\/gmberton\/image-retrieval\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 10:50:34 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Berton', 'Gabriele', ''], ['Musgrave', 'Kevin', ''], ['Masone', 'Carlo', '']]","extracted_entities":"[{'text': 'embedding model\\narchitecture', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embedding model\narchitecture","similarity_score":0.7081240416}
{"id":2503.13229,"submitter":"Yongkang Cheng","authors":"Yongkang Cheng, Shaoli Huang","title":"HoloGest: Decoupled Diffusion and Motion Priors for Generating\n  Holisticly Expressive Co-speech Gestures","comments":"Accepted by 3DV 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Animating virtual characters with holistic co-speech gestures is a\nchallenging but critical task. Previous systems have primarily focused on the\nweak correlation between audio and gestures, leading to physically unnatural\noutcomes that degrade the user experience. To address this problem, we\nintroduce HoleGest, a novel neural network framework based on decoupled\ndiffusion and motion priors for the automatic generation of high-quality,\nexpressive co-speech gestures. Our system leverages large-scale human motion\ndatasets to learn a robust prior with low audio dependency and high motion\nreliance, enabling stable global motion and detailed finger movements. To\nimprove the generation efficiency of diffusion-based models, we integrate\nimplicit joint constraints with explicit geometric and conditional constraints,\ncapturing complex motion distributions between large strides. This integration\nsignificantly enhances generation speed while maintaining high-quality motion.\nFurthermore, we design a shared embedding space for gesture-transcription text\nalignment, enabling the generation of semantically correct gesture actions.\nExtensive experiments and user feedback demonstrate the effectiveness and\npotential applications of our model, with our method achieving a level of\nrealism close to the ground truth, providing an immersive user experience. Our\ncode, model, and demo are are available at\nhttps:\/\/cyk990422.github.io\/HoloGest.github.io\/.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 14:42:31 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Cheng', 'Yongkang', ''], ['Huang', 'Shaoli', '']]","extracted_entities":"[{'text': 'shared embedding space', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"shared embedding space","similarity_score":0.7479425669}
{"id":2503.13254,"submitter":"Jiangxia Cao","authors":"Yu Liu, Hanbin Jiang, Lei Zhu, Yu Zhang, Yuqi Mao, Jiangxia Cao,\n  Shuchao Pang","title":"Federated Mixture-of-Expert for Non-Overlapped Cross-Domain Sequential\n  Recommendation","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In the real world, users always have multiple interests while surfing\ndifferent services to enrich their daily lives, e.g., watching hot short\nvideos\/live streamings. To describe user interests precisely for a better user\nexperience, the recent literature proposes cross-domain techniques by\ntransferring the other related services (a.k.a. domain) knowledge to enhance\nthe accuracy of target service prediction. In practice, naive cross-domain\ntechniques typically require there exist some overlapped users, and sharing\noverall information across domains, including user historical logs, user\/item\nembeddings, and model parameter checkpoints. Nevertheless, other domain's\nuser-side historical logs and embeddings are not always available in real-world\nRecSys designing, since users may be totally non-overlapped across domains, or\nthe privacy-preserving policy limits the personalized information sharing\nacross domains. Thereby, a challenging but valuable problem is raised: How to\nempower target domain prediction accuracy by utilizing the other domain model\nparameters checkpoints only? To answer the question, we propose the FMoE-CDSR,\nwhich explores the non-overlapped cross-domain sequential recommendation\nscenario from the federated learning perspective.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:12:37 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Liu', 'Yu', ''], ['Jiang', 'Hanbin', ''], ['Zhu', 'Lei', ''], ['Zhang', 'Yu', ''], ['Mao', 'Yuqi', ''], ['Cao', 'Jiangxia', ''], ['Pang', 'Shuchao', '']]","extracted_entities":"[{'text': 'user\/item\\nembeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'privacy-preserving policy', 'label': 'AI Ethics'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2503.13383,"submitter":"Mengyao Lyu","authors":"Mengyao Lyu, Yan Li, Huasong Zhong, Wenhao Yang, Hui Chen, Jungong\n  Han, Guiguang Ding, Zhenheng Yang","title":"Cream of the Crop: Harvesting Rich, Scalable and Transferable\n  Multi-Modal Data for Instruction Fine-Tuning","comments":"update comparison with sota and analysis","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The hypothesis that pretrained large language models (LLMs) necessitate only\nminimal supervision during the fine-tuning (SFT) stage (Zhou et al., 2024) has\nbeen substantiated by recent advancements in data curation and selection\nresearch. However, their stability and generalizability are compromised due to\nthe vulnerability to experimental setups and validation protocols, falling\nshort of surpassing random sampling (Diddee & Ippolito, 2024; Xia et al.,\n2024b). Built upon LLMs, multi-modal LLMs (MLLMs), combined with the sheer\ntoken volume and heightened heterogeneity of data sources, amplify both the\nsignificance and complexity of data selection.\n  To harvest multi-modal instructional data in a robust and efficient manner,\nwe re-define the granularity of the quality metric by decomposing it into 14\nvision-language-related capabilities, and introduce multi-modal rich scorers to\nevaluate the capabilities of each data candidate. To promote diversity, in\nlight of the inherent objective of the alignment stage, we take interaction\nstyle as diversity indicator and use a multi-modal rich styler to identify data\ninstruction patterns. In doing so, our multi-modal rich scorers and styler\n(mmSSR) guarantee that high-scoring information is conveyed to users in\ndiversified forms. Free from embedding-based clustering or greedy sampling,\nmmSSR efficiently scales to millions of data with varying budget constraints,\nsupports customization for general or specific capability acquisition, and\nfacilitates training-free generalization to new domains for curation. Across\n10+ experimental settings, validated by 14 multi-modal benchmarks, we\ndemonstrate consistent improvements over random sampling, baseline strategies\nand state-of-the-art selection methods, achieving 99.1% of full performance\nwith only 30% of the 2.6M data.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:11:22 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Lyu', 'Mengyao', ''], ['Li', 'Yan', ''], ['Zhong', 'Huasong', ''], ['Yang', 'Wenhao', ''], ['Chen', 'Hui', ''], ['Han', 'Jungong', ''], ['Ding', 'Guiguang', ''], ['Yang', 'Zhenheng', '']]","extracted_entities":"[{'text': 'embedding-based clustering', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embedding-based clustering","similarity_score":0.6901138425}
{"id":2503.13409,"submitter":"Guillaume Lagarde","authors":"Gabriel Bathie and Guillaume Lagarde","title":"A $(1+\\epsilon)$-Approximation for Ultrametric Embedding in Subquadratic\n  Time","comments":"Extended version of AAAI 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Efficiently computing accurate representations of high-dimensional data is\nessential for data analysis and unsupervised learning. Dendrograms, also known\nas ultrametrics, are widely used representations that preserve hierarchical\nrelationships within the data. However, popular methods for computing them,\nsuch as linkage algorithms, suffer from quadratic time and space complexity,\nmaking them impractical for large datasets.\n  The \"best ultrametric embedding\" (a.k.a. \"best ultrametric fit\") problem,\nwhich aims to find the ultrametric that best preserves the distances between\npoints in the original data, is known to require at least quadratic time for an\nexact solution.\n  Recent work has focused on improving scalability by approximating optimal\nsolutions in subquadratic time, resulting in a $(\\sqrt{2} +\n\\epsilon)$-approximation (Cohen-Addad, de Joannis de Verclos and Lagarde,\n2021).\n  In this paper, we present the first subquadratic algorithm that achieves\narbitrarily precise approximations of the optimal ultrametric embedding.\nSpecifically, we provide an algorithm that, for any $c \\geq 1$, outputs a\n$c$-approximation of the best ultrametric in time $\\tilde{O}(n^{1 + 1\/c})$. In\nparticular, for any fixed $\\epsilon > 0$, the algorithm computes a\n$(1+\\epsilon)$-approximation in time $\\tilde{O}(n^{2 - \\epsilon +\no(\\epsilon^2)})$.\n  Experimental results show that our algorithm improves upon previous methods\nin terms of approximation quality while maintaining comparable running times.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:38:37 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Bathie', 'Gabriel', ''], ['Lagarde', 'Guillaume', '']]","extracted_entities":"[{'text': 'Dendrograms', 'label': 'Embedding'}, {'text': 'ultrametric embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"ultrametric embedding","similarity_score":0.6919133067}
{"id":2503.13557,"submitter":"Yifei Chen","authors":"Yifei Chen and Lambert Schomaker","title":"APF+: Boosting adaptive-potential function reinforcement learning\n  methods with a W-shaped network for high-dimensional games","comments":"46 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Studies in reward shaping for reinforcement learning (RL) have flourished in\nrecent years due to its ability to speed up training. Our previous work\nproposed an adaptive potential function (APF) and showed that APF can\naccelerate the Q-learning with a Multi-layer Perceptron algorithm in the\nlow-dimensional domain. This paper proposes to extend APF with an encoder\n(APF+) for RL state representation, allowing applying APF to the pixel-based\nAtari games using a state-encoding method that projects high-dimensional game's\npixel frames to low-dimensional embeddings. We approach by designing the\nstate-representation encoder as a W-shaped network (W-Net), by using which we\nare able to encode both the background as well as the moving entities in the\ngame frames. Specifically, the embeddings derived from the pre-trained W-Net\nconsist of two latent vectors: One represents the input state, and the other\nrepresents the deviation of the input state's representation from itself. We\nthen incorporate W-Net into APF to train a downstream Dueling Deep Q-Network\n(DDQN), obtain the APF-WNet-DDQN, and demonstrate its effectiveness in Atari\ngame-playing tasks. To evaluate the APF+W-Net module in such high-dimensional\ntasks, we compare with two types of baseline methods: (i) the basic DDQN; and\n(ii) two encoder-replaced APF-DDQN methods where we replace W-Net by (a) an\nunsupervised state representation method called Spatiotemporal Deep Infomax\n(ST-DIM) and (b) a ground truth state representation provided by the Atari\nAnnotated RAM Interface (ARI). The experiment results show that out of 20 Atari\ngames, APF-WNet-DDQN outperforms DDQN (14\/20 games) and APF-STDIM-DDQN (13\/20\ngames) significantly. In comparison against the APF-ARI-DDQN which employs\nembeddings directly of the detailed game-internal state information, the\nAPF-WNet-DDQN achieves a comparable performance.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 01:53:26 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Chen', 'Yifei', ''], ['Schomaker', 'Lambert', '']]","extracted_entities":"[{'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2503.13623,"submitter":"Tomojit Ghosh","authors":"Sai Vijay Kumar Surineela, Prathyusha Kanakamalla, Harigovind\n  Harikumar, and Tomojit Ghosh","title":"A Convex formulation for linear discriminant analysis","comments":"Total pages 29 including references, six figures, seven tables.\n  Submitted to an Elsevier journal","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We present a supervised dimensionality reduction technique called Convex\nLinear Discriminant Analysis (ConvexLDA). The proposed model optimizes a\nmulti-objective cost function by balancing two complementary terms. The first\nterm pulls the samples of a class towards its centroid by minimizing a sample's\ndistance from its class-centroid in low dimensional space. The second term\npushes the classes far apart by maximizing their hyperellipsoid scattering\nvolume via the logarithm of the determinant (\\textit{log det}) of the outer\nproduct matrix formed by the low-dimensional class-centroids. Using the\nnegative of the \\textit{log det}, we pose the final cost as a minimization\nproblem, which balances the two terms using a hyper-parameter $\\lambda$. We\ndemonstrate that the cost function is convex. Unlike Fisher LDA, the proposed\nmethod doesn't require to compute the inverse of a matrix, hence avoiding any\nill-conditioned problem where data dimension is very high, e.g. RNA-seq data.\nConvexLDA doesn't require pair-wise distance calculation, making it faster and\nmore easily scalable. Moreover, the convex nature of the cost function ensures\nglobal optimality, enhancing the reliability of the learned embedding. Our\nexperimental evaluation demonstrates that ConvexLDA outperforms several popular\nlinear discriminant analysis (LDA)-based methods on a range of high-dimensional\nbiological data, image data sets, etc.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 18:17:49 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Surineela', 'Sai Vijay Kumar', ''], ['Kanakamalla', 'Prathyusha', ''], ['Harikumar', 'Harigovind', ''], ['Ghosh', 'Tomojit', '']]","extracted_entities":"[{'text': 'learned embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"learned embedding","similarity_score":0.8268041611}
{"id":2503.13641,"submitter":"Noah Snyder","authors":"Cain Edie-Michell and Noah Snyder","title":"Interpolation categories for Conformal Embeddings","comments":"29 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.QA math.OA math.RT","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper we give a diagrammatic description of the categories of modules\ncoming from the conformal embeddings $\\mathcal{V}(\\mathfrak{sl}_N,N) \\subset\n\\mathcal{V}(\\mathfrak{so}_{N^2-1},1)$. A small variant on this construction\n(morally corresponding to a conformal embedding of $\\mathfrak{gl}_N$ level $N$\ninto $\\mathfrak{o}_{N^2-1}$ level $1$) has uniform generators and relations\nwhich are rational functions in $q = e^{2 \\pi i\/4N}$, which allows us to\nconstruct a new continuous family of tensor categories at non-integer level\nwhich interpolate between these categories. This is the second example of such\nan interpolation category for families of conformal embeddings after Zhengwei\nLiu's interpolation categories $\\mathcal{V}(\\mathfrak{sl}_N, N\\pm 2) \\subset\n\\mathcal{V}(\\mathfrak{sl}_{N(N\\pm 1)\/2},1)$ which he constructed using his\nclassification Yang-Baxter planar algebras. Our approach is different from\nLiu's, we build a two-color skein theory, with one strand coming from $X$ the\nimage of defining representation of $\\mathfrak{sl}_N$ and the other strand\ncoming from an invertible object $g$ in the category of local modules, and a\ntrivalent vertex coming from a map $X \\otimes X^* \\rightarrow g$. We anticipate\nsmall variations on our approach will yield interpolation categories for every\ninfinite discrete family of conformal embeddings.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 18:45:47 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Edie-Michell', 'Cain', ''], ['Snyder', 'Noah', '']]","extracted_entities":"[{'text': 'conformal embeddings', 'label': 'Embedding'}, {'text': 'conformal embedding', 'label': 'Embedding'}, {'text': 'conformal embeddings', 'label': 'Embedding'}, {'text': 'conformal embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"conformal embedding","similarity_score":0.5596656799}
{"id":2503.13777,"submitter":"Xuyang Fang","authors":"Xuyang Fang, Sion Hannuna, Neill Campbell","title":"8-Calves Image dataset","comments":"11 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce the 8-Calves dataset, a benchmark for evaluating object\ndetection and identity classification in occlusion-rich, temporally consistent\nenvironments. The dataset comprises a 1-hour video (67,760 frames) of eight\nHolstein Friesian calves in a barn, with ground truth bounding boxes and\nidentities, alongside 900 static frames for detection tasks. Each calf exhibits\na unique coat pattern, enabling precise identity distinction.\n  For cow detection, we fine-tuned 28 models (25 YOLO variants, 3 transformers)\non 600 frames, testing on the full video. Results reveal smaller YOLO models\n(e.g., YOLOV9c) outperform larger counterparts despite potential bias from a\nYOLOv8m-based labeling pipeline. For identity classification, embeddings from\n23 pretrained vision models (ResNet, ConvNextV2, ViTs) were evaluated via\nlinear classifiers and KNN. Modern architectures like ConvNextV2 excelled,\nwhile larger models frequently overfit, highlighting inefficiencies in scaling.\n  Key findings include: (1) Minimal, targeted augmentations (e.g., rotation)\noutperform complex strategies on simpler datasets; (2) Pretraining strategies\n(e.g., BEiT, DinoV2) significantly boost identity recognition; (3) Temporal\ncontinuity and natural motion patterns offer unique challenges absent in\nsynthetic or domain-specific benchmarks. The dataset's controlled design and\nextended sequences (1 hour vs. prior 10-minute benchmarks) make it a pragmatic\ntool for stress-testing occlusion handling, temporal consistency, and\nefficiency.\n  The link to the dataset is https:\/\/github.com\/tonyFang04\/8-calves.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 23:47:52 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Fang', 'Xuyang', ''], ['Hannuna', 'Sion', ''], ['Campbell', 'Neill', '']]","extracted_entities":"[{'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2503.13805,"submitter":"Xin Zhong","authors":"Muhammad Ahtesham, Xin Zhong","title":"Text-Guided Image Invariant Feature Learning for Robust Image\n  Watermarking","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG cs.MM","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Ensuring robustness in image watermarking is crucial for and maintaining\ncontent integrity under diverse transformations. Recent self-supervised\nlearning (SSL) approaches, such as DINO, have been leveraged for watermarking\nbut primarily focus on general feature representation rather than explicitly\nlearning invariant features. In this work, we propose a novel text-guided\ninvariant feature learning framework for robust image watermarking. Our\napproach leverages CLIP's multimodal capabilities, using text embeddings as\nstable semantic anchors to enforce feature invariance under distortions. We\nevaluate the proposed method across multiple datasets, demonstrating superior\nrobustness against various image transformations. Compared to state-of-the-art\nSSL methods, our model achieves higher cosine similarity in feature consistency\ntests and outperforms existing watermarking schemes in extraction accuracy\nunder severe distortions. These results highlight the efficacy of our method in\nlearning invariant representations tailored for robust deep learning-based\nwatermarking.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 01:32:38 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Ahtesham', 'Muhammad', ''], ['Zhong', 'Xin', '']]","extracted_entities":"[{'text': 'text embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"text embeddings","similarity_score":0.812117815}
{"id":2503.13861,"submitter":"Yujin Wang Mr","authors":"Yujin Wang, Quanfeng Liu, Zhengxin Jiang, Tianyi Wang, Junfeng Jiao,\n  Hongqing Chu, Bingzhao Gao, Hong Chen","title":"RAD: Retrieval-Augmented Decision-Making of Meta-Actions with\n  Vision-Language Models in Autonomous Driving","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Accurately understanding and deciding high-level meta-actions is essential\nfor ensuring reliable and safe autonomous driving systems. While\nvision-language models (VLMs) have shown significant potential in various\nautonomous driving tasks, they often suffer from limitations such as inadequate\nspatial perception and hallucination, reducing their effectiveness in complex\nautonomous driving scenarios. To address these challenges, we propose a\nretrieval-augmented decision-making (RAD) framework, a novel architecture\ndesigned to enhance VLMs' capabilities to reliably generate meta-actions in\nautonomous driving scenes. RAD leverages a retrieval-augmented generation (RAG)\npipeline to dynamically improve decision accuracy through a three-stage process\nconsisting of the embedding flow, retrieving flow, and generating flow.\nAdditionally, we fine-tune VLMs on a specifically curated dataset derived from\nthe NuScenes dataset to enhance their spatial perception and bird's-eye view\nimage comprehension capabilities. Extensive experimental evaluations on the\ncurated NuScenes-based dataset demonstrate that RAD outperforms baseline\nmethods across key evaluation metrics, including match accuracy, and F1 score,\nand self-defined overall score, highlighting its effectiveness in improving\nmeta-action decision-making for autonomous driving tasks.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 03:25:57 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wang', 'Yujin', ''], ['Liu', 'Quanfeng', ''], ['Jiang', 'Zhengxin', ''], ['Wang', 'Tianyi', ''], ['Jiao', 'Junfeng', ''], ['Chu', 'Hongqing', ''], ['Gao', 'Bingzhao', ''], ['Chen', 'Hong', '']]","extracted_entities":"[{'text': 'embedding flow', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embedding flow","similarity_score":0.6818602085}
{"id":2503.13925,"submitter":"Da Kuang","authors":"Da Kuang, Guanwen Qiu, Junhyong Kim","title":"Reconstructing Cell Lineage Trees from Phenotypic Features with Metric\n  Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  How a single fertilized cell gives rise to a complex array of specialized\ncell types in development is a central question in biology. The cells grow,\ndivide, and acquire differentiated characteristics through poorly understood\nmolecular processes. A key approach to studying developmental processes is to\ninfer the tree graph of cell lineage division and differentiation histories,\nproviding an analytical framework for dissecting individual cells' molecular\ndecisions during replication and differentiation. Although genetically\nengineered lineage-tracing methods have advanced the field, they are either\ninfeasible or ethically constrained in many organisms. In contrast, modern\nsingle-cell technologies can measure high-content molecular profiles (e.g.,\ntranscriptomes) in a wide range of biological systems.\n  Here, we introduce CellTreeQM, a novel deep learning method based on\ntransformer architectures that learns an embedding space with geometric\nproperties optimized for tree-graph inference. By formulating lineage\nreconstruction as a tree-metric learning problem, we have systematically\nexplored supervised, weakly supervised, and unsupervised training settings and\npresent a Lineage Reconstruction Benchmark to facilitate comprehensive\nevaluation of our learning method. We benchmarked the method on (1) synthetic\ndata modeled via Brownian motion with independent noise and spurious signals\nand (2) lineage-resolved single-cell RNA sequencing datasets. Experimental\nresults show that CellTreeQM recovers lineage structures with minimal\nsupervision and limited data, offering a scalable framework for uncovering cell\nlineage relationships in challenging animal models. To our knowledge, this is\nthe first method to cast cell lineage inference explicitly as a metric learning\ntask, paving the way for future computational models aimed at uncovering the\nmolecular dynamics of cell lineage.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 05:41:03 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Kuang', 'Da', ''], ['Qiu', 'Guanwen', ''], ['Kim', 'Junhyong', '']]","extracted_entities":"[{'text': 'ethically constrained', 'label': 'AI Ethics'}, {'text': 'embedding space', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embedding space","similarity_score":0.8514168859}
{"id":2503.13948,"submitter":"Mufan Liu","authors":"Mufan Liu, Qi Yang, He Huang, Wenjie Huang, Zhenlong Yuan, Zhu Li,\n  Yiling Xu","title":"Light4GS: Lightweight Compact 4D Gaussian Splatting Generation via\n  Context Model","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  3D Gaussian Splatting (3DGS) has emerged as an efficient and high-fidelity\nparadigm for novel view synthesis. To adapt 3DGS for dynamic content,\ndeformable 3DGS incorporates temporally deformable primitives with learnable\nlatent embeddings to capture complex motions. Despite its impressive\nperformance, the high-dimensional embeddings and vast number of primitives lead\nto substantial storage requirements. In this paper, we introduce a\n\\textbf{Light}weight \\textbf{4}D\\textbf{GS} framework, called Light4GS, that\nemploys significance pruning with a deep context model to provide a lightweight\nstorage-efficient dynamic 3DGS representation. The proposed Light4GS is based\non 4DGS that is a typical representation of deformable 3DGS. Specifically, our\nframework is built upon two core components: (1) a spatio-temporal significance\npruning strategy that eliminates over 64\\% of the deformable primitives,\nfollowed by an entropy-constrained spherical harmonics compression applied to\nthe remainder; and (2) a deep context model that integrates intra- and\ninter-prediction with hyperprior into a coarse-to-fine context structure to\nenable efficient multiscale latent embedding compression. Our approach achieves\nover 120x compression and increases rendering FPS up to 20\\% compared to the\nbaseline 4DGS, and also superior to frame-wise state-of-the-art 3DGS\ncompression methods, revealing the effectiveness of our Light4GS in terms of\nboth intra- and inter-prediction methods without sacrificing rendering quality.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 06:28:13 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Liu', 'Mufan', ''], ['Yang', 'Qi', ''], ['Huang', 'He', ''], ['Huang', 'Wenjie', ''], ['Yuan', 'Zhenlong', ''], ['Li', 'Zhu', ''], ['Xu', 'Yiling', '']]","extracted_entities":"[{'text': 'latent embeddings', 'label': 'contextual Embedding'}, {'text': 'high-dimensional embeddings', 'label': 'Embedding'}, {'text': '3DGS', 'label': 'contextual Embedding'}, {'text': '3DGS', 'label': 'contextual Embedding'}]","assigned_concept":"Embedding","matched_keyword":"high-dimensional embeddings","similarity_score":0.7461893559}
{"id":2503.13954,"submitter":"Ni Tianhao","authors":"Tianhao Ni, Bingjie Li and Zhigang Yao","title":"Enhanced High-Dimensional Data Visualization through Adaptive\n  Multi-Scale Manifold Embedding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  To address the dual challenges of the curse of dimensionality and the\ndifficulty in separating intra-cluster and inter-cluster structures in\nhigh-dimensional manifold embedding, we proposes an Adaptive Multi-Scale\nManifold Embedding (AMSME) algorithm. By introducing ordinal distance to\nreplace traditional Euclidean distances, we theoretically demonstrate that\nordinal distance overcomes the constraints of the curse of dimensionality in\nhigh-dimensional spaces, effectively distinguishing heterogeneous samples. We\ndesign an adaptive neighborhood adjustment method to construct similarity\ngraphs that simultaneously balance intra-cluster compactness and inter-cluster\nseparability. Furthermore, we develop a two-stage embedding framework: the\nfirst stage achieves preliminary cluster separation while preserving\nconnectivity between structurally similar clusters via the similarity graph,\nand the second stage enhances inter-cluster separation through a label-driven\ndistance reweighting. Experimental results demonstrate that AMSME significantly\npreserves intra-cluster topological structures and improves inter-cluster\nseparation on real-world datasets. Additionally, leveraging its\nmulti-resolution analysis capability, AMSME discovers novel neuronal subtypes\nin the mouse lumbar dorsal root ganglion scRNA-seq dataset, with marker gene\nanalysis revealing their distinct biological roles.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 06:46:53 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 05:21:06 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Ni', 'Tianhao', ''], ['Li', 'Bingjie', ''], ['Yao', 'Zhigang', '']]","extracted_entities":"[{'text': 'high-dimensional manifold embedding', 'label': 'Embedding'}, {'text': 'Adaptive Multi-Scale\\nManifold Embedding', 'label': 'Embedding'}, {'text': 'AMSME', 'label': 'Embedding'}, {'text': 'AMSME', 'label': 'Embedding'}, {'text': 'AMSME', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"high-dimensional manifold embedding","similarity_score":0.6394561529}
{"id":2503.13957,"submitter":"Liulei Li","authors":"Mu Chen, Liulei Li, Wenguan Wang, Yi Yang","title":"DIFFVSGG: Diffusion-Driven Online Video Scene Graph Generation","comments":"CVPR 2025, Code: https:\/\/github.com\/kagawa588\/DiffVsgg","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Top-leading solutions for Video Scene Graph Generation (VSGG) typically adopt\nan offline pipeline. Though demonstrating promising performance, they remain\nunable to handle real-time video streams and consume large GPU memory.\nMoreover, these approaches fall short in temporal reasoning, merely aggregating\nframe-level predictions over a temporal context. In response, we introduce\nDIFFVSGG, an online VSGG solution that frames this task as an iterative scene\ngraph update problem. Drawing inspiration from Latent Diffusion Models (LDMs)\nwhich generate images via denoising a latent feature embedding, we unify the\ndecoding of object classification, bounding box regression, and graph\ngeneration three tasks using one shared feature embedding. Then, given an\nembedding containing unified features of object pairs, we conduct a step-wise\nDenoising on it within LDMs, so as to deliver a clean embedding which clearly\nindicates the relationships between objects. This embedding then serves as the\ninput to task-specific heads for object classification, scene graph generation,\netc. DIFFVSGG further facilitates continuous temporal reasoning, where\npredictions for subsequent frames leverage results of past frames as the\nconditional inputs of LDMs, to guide the reverse diffusion process for current\nframes. Extensive experiments on three setups of Action Genome demonstrate the\nsuperiority of DIFFVSGG.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 06:49:51 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Chen', 'Mu', ''], ['Li', 'Liulei', ''], ['Wang', 'Wenguan', ''], ['Yang', 'Yi', '']]","extracted_entities":"[{'text': 'latent feature embedding', 'label': 'Embedding'}, {'text': 'embedding', 'label': 'Embedding'}, {'text': 'embedding', 'label': 'Embedding'}, {'text': 'embedding', 'label': 'Embedding'}, {'text': 'embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embedding","similarity_score":1.0}
{"id":2503.14002,"submitter":"Damian Boborzi","authors":"Damian Boborzi and Phillip Mueller and Jonas Emrich and Dominik Schmid\n  and Sebastian Mueller and Lars Mikelsons","title":"MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific\n  Generative Modeling","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Generative models have recently made remarkable progress in the field of 3D\nobjects. However, their practical application in fields like engineering\nremains limited since they fail to deliver the accuracy, quality, and\ncontrollability needed for domain-specific tasks. Fine-tuning large generative\nmodels is a promising perspective for making these models available in these\nfields. Creating high-quality, domain-specific 3D datasets is crucial for\nfine-tuning large generative models, yet the data filtering and annotation\nprocess remains a significant bottleneck. We present MeshFleet, a filtered and\nannotated 3D vehicle dataset extracted from Objaverse-XL, the most extensive\npublicly available collection of 3D objects. Our approach proposes a pipeline\nfor automated data filtering based on a quality classifier. This classifier is\ntrained on a manually labeled subset of Objaverse, incorporating DINOv2 and\nSigLIP embeddings, refined through caption-based analysis and uncertainty\nestimation. We demonstrate the efficacy of our filtering method through a\ncomparative analysis against caption and image aesthetic score-based techniques\nand fine-tuning experiments with SV3D, highlighting the importance of targeted\ndata selection for domain-specific 3D generative modeling.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 08:09:24 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Boborzi', 'Damian', ''], ['Mueller', 'Phillip', ''], ['Emrich', 'Jonas', ''], ['Schmid', 'Dominik', ''], ['Mueller', 'Sebastian', ''], ['Mikelsons', 'Lars', '']]","extracted_entities":"[{'text': 'Objaverse-XL', 'label': 'Large Language Model'}, {'text': 'DINOv2', 'label': 'Embedding'}, {'text': 'SigLIP embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"SigLIP embeddings","similarity_score":0.6315517426}
{"id":2503.1404,"submitter":"Songen Gu","authors":"Binjie Liu, Lina Liu, Sanyi Zhang, Songen Gu, Yihao Zhi, Tianyi Zhu,\n  Lei Yang, Long Ye","title":"MAG: Multi-Modal Aligned Autoregressive Co-Speech Gesture Generation\n  without Vector Quantization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GR cs.CV cs.SD","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This work focuses on full-body co-speech gesture generation. Existing methods\ntypically employ an autoregressive model accompanied by vector-quantized tokens\nfor gesture generation, which results in information loss and compromises the\nrealism of the generated gestures. To address this, inspired by the natural\ncontinuity of real-world human motion, we propose MAG, a novel multi-modal\naligned framework for high-quality and diverse co-speech gesture synthesis\nwithout relying on discrete tokenization. Specifically, (1) we introduce a\nmotion-text-audio-aligned variational autoencoder (MTA-VAE), which leverages\npre-trained WavCaps' text and audio embeddings to enhance both semantic and\nrhythmic alignment with motion, ultimately producing more realistic gestures.\n(2) Building on this, we propose a multimodal masked autoregressive model\n(MMAG) that enables autoregressive modeling in continuous motion embeddings\nthrough diffusion without vector quantization. To further ensure multi-modal\nconsistency, MMAG incorporates a hybrid granularity audio-text fusion block,\nwhich serves as conditioning for diffusion process. Extensive experiments on\ntwo benchmark datasets demonstrate that MAG achieves stateof-the-art\nperformance both quantitatively and qualitatively, producing highly realistic\nand diverse co-speech gestures.The code will be released to facilitate future\nresearch.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 09:02:02 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Liu', 'Binjie', ''], ['Liu', 'Lina', ''], ['Zhang', 'Sanyi', ''], ['Gu', 'Songen', ''], ['Zhi', 'Yihao', ''], ['Zhu', 'Tianyi', ''], ['Yang', 'Lei', ''], ['Ye', 'Long', '']]","extracted_entities":"[{'text': 'text and audio embeddings', 'label': 'Embedding'}, {'text': 'continuous motion embeddings', 'label': 'Embedding'}, {'text': 'vector quantization', 'label': 'quantisation'}]","assigned_concept":"Embedding","matched_keyword":"text and audio embeddings","similarity_score":0.686768055}
{"id":2503.14138,"submitter":"Siddharth Jaiswal","authors":"Siddharth D Jaiswal, Sagnik Basu, Sandipan Sikdar, Animesh Mukherjee","title":"Exploring Disparity-Accuracy Trade-offs in Face Recognition Systems: The\n  Role of Datasets, Architectures, and Loss Functions","comments":"This work has been accepted for publication at AAAI ICWSM 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CY","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Automated Face Recognition Systems (FRSs), developed using deep learning\nmodels, are deployed worldwide for identity verification and facial attribute\nanalysis. The performance of these models is determined by a complex\ninterdependence among the model architecture, optimization\/loss function and\ndatasets. Although FRSs have surpassed human-level accuracy, they continue to\nbe disparate against certain demographics. Due to the ubiquity of applications,\nit is extremely important to understand the impact of the three components --\nmodel architecture, loss function and face image dataset on the\naccuracy-disparity trade-off to design better, unbiased platforms. In this\nwork, we perform an in-depth analysis of three FRSs for the task of gender\nprediction, with various architectural modifications resulting in ten\ndeep-learning models coupled with four loss functions and benchmark them on\nseven face datasets across 266 evaluation configurations. Our results show that\nall three components have an individual as well as a combined impact on both\naccuracy and disparity. We identify that datasets have an inherent property\nthat causes them to perform similarly across models, independent of the choice\nof loss functions. Moreover, the choice of dataset determines the model's\nperceived bias -- the same model reports bias in opposite directions for three\ngender-balanced datasets of ``in-the-wild'' face images of popular individuals.\nStudying the facial embeddings shows that the models are unable to generalize a\nuniform definition of what constitutes a ``female face'' as opposed to a ``male\nface'', due to dataset diversity. We provide recommendations to model\ndevelopers on using our study as a blueprint for model development and\nsubsequent deployment.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:04:57 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Jaiswal', 'Siddharth D', ''], ['Basu', 'Sagnik', ''], ['Sikdar', 'Sandipan', ''], ['Mukherjee', 'Animesh', '']]","extracted_entities":"[{'text': 'facial embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"facial embeddings","similarity_score":0.6875802279}
{"id":2503.14185,"submitter":"Wuwei Huang","authors":"Wuwei Huang, Dexin Wang, Deyi Xiong","title":"AdaST: Dynamically Adapting Encoder States in the Decoder for End-to-End\n  Speech-to-Text Translation","comments":"ACL 2021 Findings","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.SD eess.AS","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In end-to-end speech translation, acoustic representations learned by the\nencoder are usually fixed and static, from the perspective of the decoder,\nwhich is not desirable for dealing with the cross-modal and cross-lingual\nchallenge in speech translation. In this paper, we show the benefits of varying\nacoustic states according to decoder hidden states and propose an adaptive\nspeech-to-text translation model that is able to dynamically adapt acoustic\nstates in the decoder. We concatenate the acoustic state and target word\nembedding sequence and feed the concatenated sequence into subsequent blocks in\nthe decoder. In order to model the deep interaction between acoustic states and\ntarget hidden states, a speech-text mixed attention sublayer is introduced to\nreplace the conventional cross-attention network. Experiment results on two\nwidely-used datasets show that the proposed method significantly outperforms\nstate-of-the-art neural speech translation models.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:59:27 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Huang', 'Wuwei', ''], ['Wang', 'Dexin', ''], ['Xiong', 'Deyi', '']]","extracted_entities":"[{'text': 'target word\\nembedding sequence', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"target word\nembedding sequence","similarity_score":0.5420953035}
{"id":2503.14213,"submitter":"Ashraf Ghiye","authors":"Ashraf Ghiye, Baptiste Barreau, Laurent Carlier, Michalis Vazirgiannis","title":"Rolling Forward: Enhancing LightGCN with Causal Graph Convolution for\n  Credit Bond Recommendation","comments":"8 pages, published in the international conference for AI in Finance\n  (ACM ICAIF'24)","journal-ref":null,"doi":"10.1145\/3677052.3698683","report-no":null,"categories":"cs.IR cs.LG q-fin.CP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Graph Neural Networks have significantly advanced research in recommender\nsystems over the past few years. These methods typically capture global\ninterests using aggregated past interactions and rely on static embeddings of\nusers and items over extended periods of time. While effective in some domains,\nthese methods fall short in many real-world scenarios, especially in finance,\nwhere user interests and item popularity evolve rapidly over time. To address\nthese challenges, we introduce a novel extension to Light Graph Convolutional\nNetwork (LightGCN) designed to learn temporal node embeddings that capture\ndynamic interests. Our approach employs causal convolution to maintain a\nforward-looking model architecture. By preserving the chronological order of\nuser-item interactions and introducing a dynamic update mechanism for\nembeddings through a sliding window, the proposed model generates well-timed\nand contextually relevant recommendations. Extensive experiments on a\nreal-world dataset from BNP Paribas demonstrate that our approach significantly\nenhances the performance of LightGCN while maintaining the simplicity and\nefficiency of its architecture. Our findings provide new insights into\ndesigning graph-based recommender systems in time-sensitive applications,\nparticularly for financial product recommendations.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 12:47:01 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Ghiye', 'Ashraf', ''], ['Barreau', 'Baptiste', ''], ['Carlier', 'Laurent', ''], ['Vazirgiannis', 'Michalis', '']]","extracted_entities":"[{'text': 'embeddings', 'label': 'contextual Embedding'}, {'text': 'temporal node embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2503.14219,"submitter":"Yusuke Monno","authors":"Yizhou Li, Yusuke Monno, Masatoshi Okutomi, Yuuichi Tanaka, Seiichi\n  Kataoka, Teruaki Kosiba","title":"Segmentation-Guided Neural Radiance Fields for Novel Street View\n  Synthesis","comments":"Presented at VISAPP2025. Project page:\n  http:\/\/www.ok.sc.e.titech.ac.jp\/res\/NVS\/index.html","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV eess.IV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Recent advances in Neural Radiance Fields (NeRF) have shown great potential\nin 3D reconstruction and novel view synthesis, particularly for indoor and\nsmall-scale scenes. However, extending NeRF to large-scale outdoor environments\npresents challenges such as transient objects, sparse cameras and textures, and\nvarying lighting conditions. In this paper, we propose a segmentation-guided\nenhancement to NeRF for outdoor street scenes, focusing on complex urban\nenvironments. Our approach extends ZipNeRF and utilizes Grounded SAM for\nsegmentation mask generation, enabling effective handling of transient objects,\nmodeling of the sky, and regularization of the ground. We also introduce\nappearance embeddings to adapt to inconsistent lighting across view sequences.\nExperimental results demonstrate that our method outperforms the baseline\nZipNeRF, improving novel view synthesis quality with fewer artifacts and\nsharper details.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 12:54:36 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Li', 'Yizhou', ''], ['Monno', 'Yusuke', ''], ['Okutomi', 'Masatoshi', ''], ['Tanaka', 'Yuuichi', ''], ['Kataoka', 'Seiichi', ''], ['Kosiba', 'Teruaki', '']]","extracted_entities":"[{'text': 'appearance embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"appearance embeddings","similarity_score":0.696349144}
{"id":2503.14275,"submitter":"Jiang Qin","authors":"Jiang Qin, Senmao Li, Alexandra Gomez-Villa, Shiqi Yang, Yaxing Wang,\n  Kai Wang, Joost van de Weijer","title":"Free-Lunch Color-Texture Disentanglement for Stylized Image Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advances in Text-to-Image (T2I) diffusion models have transformed\nimage generation, enabling significant progress in stylized generation using\nonly a few style reference images. However, current diffusion-based methods\nstruggle with fine-grained style customization due to challenges in controlling\nmultiple style attributes, such as color and texture. This paper introduces the\nfirst tuning-free approach to achieve free-lunch color-texture disentanglement\nin stylized T2I generation, addressing the need for independently controlled\nstyle elements for the Disentangled Stylized Image Generation (DisIG) problem.\nOur approach leverages the Image-Prompt Additivity property in the CLIP image\nembedding space to develop techniques for separating and extracting\nColor-Texture Embeddings (CTE) from individual color and texture reference\nimages. To ensure that the color palette of the generated image aligns closely\nwith the color reference, we apply a whitening and coloring transformation to\nenhance color consistency. Additionally, to prevent texture loss due to the\nsignal-leak bias inherent in diffusion training, we introduce a noise term that\npreserves textural fidelity during the Regularized Whitening and Coloring\nTransformation (RegWCT). Through these methods, our Style Attributes\nDisentanglement approach (SADis) delivers a more precise and customizable\nsolution for stylized image generation. Experiments on images from the WikiArt\nand StyleDrop datasets demonstrate that, both qualitatively and quantitatively,\nSADis surpasses state-of-the-art stylization methods in the DisIG task.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 14:10:43 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Qin', 'Jiang', ''], ['Li', 'Senmao', ''], ['Gomez-Villa', 'Alexandra', ''], ['Yang', 'Shiqi', ''], ['Wang', 'Yaxing', ''], ['Wang', 'Kai', ''], ['van de Weijer', 'Joost', '']]","extracted_entities":"[{'text': 'Color-Texture Embeddings', 'label': 'Embedding'}, {'text': 'quantitatively', 'label': 'quantisation'}]","assigned_concept":"Embedding","matched_keyword":"Color-Texture Embeddings","similarity_score":0.6252241731}
{"id":2503.14304,"submitter":"Yuheng Li","authors":"Yuheng Li, Mingzhe Hu, Richard L.J. Qiu, Maria Thor, Andre Williams,\n  Deborah Marshall and Xiaofeng Yang","title":"RoMedFormer: A Rotary-Embedding Transformer Foundation Model for 3D\n  Genito-Pelvic Structure Segmentation in MRI and CT","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Deep learning-based segmentation of genito-pelvic structures in MRI and CT is\ncrucial for applications such as radiation therapy, surgical planning, and\ndisease diagnosis. However, existing segmentation models often struggle with\ngeneralizability across imaging modalities, and anatomical variations. In this\nwork, we propose RoMedFormer, a rotary-embedding transformer-based foundation\nmodel designed for 3D female genito-pelvic structure segmentation in both MRI\nand CT. RoMedFormer leverages self-supervised learning and rotary positional\nembeddings to enhance spatial feature representation and capture long-range\ndependencies in 3D medical data. We pre-train our model using a diverse dataset\nof 3D MRI and CT scans and fine-tune it for downstream segmentation tasks.\nExperimental results demonstrate that RoMedFormer achieves superior performance\nsegmenting genito-pelvic organs. Our findings highlight the potential of\ntransformer-based architectures in medical image segmentation and pave the way\nfor more transferable segmentation frameworks.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 14:45:05 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Li', 'Yuheng', ''], ['Hu', 'Mingzhe', ''], ['Qiu', 'Richard L. J.', ''], ['Thor', 'Maria', ''], ['Williams', 'Andre', ''], ['Marshall', 'Deborah', ''], ['Yang', 'Xiaofeng', '']]","extracted_entities":"[{'text': 'RoMedFormer', 'label': 'Foundation Model'}, {'text': 'RoMedFormer', 'label': 'Foundation Model'}, {'text': 'self-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'rotary positional\\nembeddings', 'label': 'Embedding'}, {'text': 'RoMedFormer', 'label': 'Foundation Model'}]","assigned_concept":"Embedding","matched_keyword":"rotary positional\nembeddings","similarity_score":0.6026083231}
{"id":2503.14343,"submitter":"Yuanpeng He","authors":"Yali Bi, Enyu Che, Yinan Chen, Yuanpeng He, Jingwei Qu","title":"Multi-Prototype Embedding Refinement for Semi-Supervised Medical Image\n  Segmentation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Medical image segmentation aims to identify anatomical structures at the\nvoxel-level. Segmentation accuracy relies on distinguishing voxel differences.\nCompared to advancements achieved in studies of the inter-class variance, the\nintra-class variance receives less attention. Moreover, traditional linear\nclassifiers, limited by a single learnable weight per class, struggle to\ncapture this finer distinction. To address the above challenges, we propose a\nMulti-Prototype-based Embedding Refinement method for semi-supervised medical\nimage segmentation. Specifically, we design a multi-prototype-based\nclassification strategy, rethinking the segmentation from the perspective of\nstructural relationships between voxel embeddings. The intra-class variations\nare explored by clustering voxels along the distribution of multiple prototypes\nin each class. Next, we introduce a consistency constraint to alleviate the\nlimitation of linear classifiers. This constraint integrates different\nclassification granularities from a linear classifier and the proposed\nprototype-based classifier. In the thorough evaluation on two popular\nbenchmarks, our method achieves superior performance compared with\nstate-of-the-art methods. Code is available at\nhttps:\/\/github.com\/Briley-byl123\/MPER.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 15:23:52 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Bi', 'Yali', ''], ['Che', 'Enyu', ''], ['Chen', 'Yinan', ''], ['He', 'Yuanpeng', ''], ['Qu', 'Jingwei', '']]","extracted_entities":"[{'text': 'voxel embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"voxel embeddings","similarity_score":0.6481046081}
{"id":2503.14446,"submitter":"Crislaine Kuster","authors":"Crislaine Kuster","title":"Codimension one foliations on adjoint varieties","comments":"28 pages, 0 figures, comments are welcome!","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper, we classify codimension one foliations on adjoint varieties\nwith most positive anti-canonical class. We show that on adjoint varieties with\nPicard number one, these foliations are always induced by a pencil of\nhyperplane sections with respect to their minimal embedding. For adjoint\nvarieties of Picard number two, there is more than one component of such\nfoliations, and we describe each of them. As a tool for understanding these\nfoliations, we introduce the concept of the degree of a foliation with respect\nto a family of rational curves, which may be of independent interest.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:24:26 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Kuster', 'Crislaine', '']]","extracted_entities":"[{'text': 'minimal embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"minimal embedding","similarity_score":0.8026180267}
{"id":2503.14473,"submitter":"Jason Han","authors":"Jason Han, Nicholas S. DiBrita, Younghyun Cho, Hengrui Luo, Tirthak\n  Patel","title":"EnQode: Fast Amplitude Embedding for Quantum Machine Learning Using\n  Classical Data","comments":"EnQode will appear in the Proceedings of the Design Automation\n  Conference (DAC), 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cs.ET cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Amplitude embedding (AE) is essential in quantum machine learning (QML) for\nencoding classical data onto quantum circuits. However, conventional AE methods\nsuffer from deep, variable-length circuits that introduce high output error due\nto extensive gate usage and variable error rates across samples, resulting in\nnoise-driven inconsistencies that degrade model accuracy. We introduce EnQode,\na fast AE technique based on symbolic representation that addresses these\nlimitations by clustering dataset samples and solving for cluster mean states\nthrough a low-depth, machine-specific ansatz. Optimized to reduce physical\ngates and SWAP operations, EnQode ensures all samples face consistent, low\nnoise levels by standardizing circuit depth and composition. With over 90%\nfidelity in data mapping, EnQode enables robust, high-performance QML on noisy\nintermediate-scale quantum (NISQ) devices. Our open-source solution provides a\nscalable and efficient alternative for integrating classical data with quantum\nmodels.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:48:03 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Han', 'Jason', ''], ['DiBrita', 'Nicholas S.', ''], ['Cho', 'Younghyun', ''], ['Luo', 'Hengrui', ''], ['Patel', 'Tirthak', '']]","extracted_entities":"[{'text': 'Amplitude embedding', 'label': 'Embedding'}, {'text': 'quantum machine learning', 'label': 'Few-shot Learning'}, {'text': 'EnQode', 'label': 'Embedding'}, {'text': 'open-source solution', 'label': 'Open-source LLMs'}]","assigned_concept":"Embedding","matched_keyword":"Amplitude embedding","similarity_score":0.6402260065}
{"id":2503.14553,"submitter":"Kasra Borazjani","authors":"Kasra Borazjani, Payam Abdisarabshali, Naji Khosravan, Seyyedali\n  Hosseinalipour","title":"Redefining non-IID Data in Federated Learning for Computer Vision Tasks:\n  Migrating from Labels to Embeddings for Task-Specific Data Distributions","comments":"14 pages, 9 figures, 1 table, (implementations are included at our\n  GitHub repository: https:\/\/github.com\/KasraBorazjani\/task-perspective-het)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Federated Learning (FL) represents a paradigm shift in distributed machine\nlearning (ML), enabling clients to train models collaboratively while keeping\ntheir raw data private. This paradigm shift from traditional centralized ML\nintroduces challenges due to the non-iid (non-independent and identically\ndistributed) nature of data across clients, significantly impacting FL's\nperformance. Existing literature, predominantly model data heterogeneity by\nimposing label distribution skew across clients. In this paper, we show that\nlabel distribution skew fails to fully capture the real-world data\nheterogeneity among clients in computer vision tasks beyond classification.\nSubsequently, we demonstrate that current approaches overestimate FL's\nperformance by relying on label\/class distribution skew, exposing an overlooked\ngap in the literature. By utilizing pre-trained deep neural networks to extract\ntask-specific data embeddings, we define task-specific data heterogeneity\nthrough the lens of each vision task and introduce a new level of data\nheterogeneity called embedding-based data heterogeneity. Our methodology\ninvolves clustering data points based on embeddings and distributing them among\nclients using the Dirichlet distribution. Through extensive experiments, we\nevaluate the performance of different FL methods under our revamped notion of\ndata heterogeneity, introducing new benchmark performance measures to the\nliterature. We further unveil a series of open research directions that can be\npursued.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 22:16:53 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Borazjani', 'Kasra', ''], ['Abdisarabshali', 'Payam', ''], ['Khosravan', 'Naji', ''], ['Hosseinalipour', 'Seyyedali', '']]","extracted_entities":"[{'text': 'Federated Learning', 'label': 'Few-shot Learning'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2503.14667,"submitter":"Pablo Ochoa Mr","authors":"Pablo Ochoa","title":"Elliptic systems in Orlicz-Sobolev spaces with critical sources in\n  bounded domains","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/","abstract":"  In this paper, we show the existence of non-trivial solutions to very general\nelliptic systems with critical non-linearities in the sense of embeddings in\nOrlicz-Sobolev spaces. This allows to consider non-linearities which do not\nhave polynomial growth. To achieve the existence, we combine a Mountain Pass\nTheorem without the Palais-Smale condition with the second Concentration\nCompactness Principle of Lions in Orlicz-Sobolev spaces.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 19:14:51 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Ochoa', 'Pablo', '']]","extracted_entities":"[{'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2503.14736,"submitter":"Yilan Dong","authors":"Yilan Dong, Haohe Liu, Qing Wang, Jiahao Yang, Wenqing Wang, Gregory\n  Slabaugh, Shanxin Yuan","title":"HandSplat: Embedding-Driven Gaussian Splatting for High-Fidelity Hand\n  Rendering","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Existing 3D Gaussian Splatting (3DGS) methods for hand rendering rely on\nrigid skeletal motion with an oversimplified non-rigid motion model, which\nfails to capture fine geometric and appearance details. Additionally, they\nperform densification based solely on per-point gradients and process poses\nindependently, ignoring spatial and temporal correlations. These limitations\nlead to geometric detail loss, temporal instability, and inefficient point\ndistribution. To address these issues, we propose HandSplat, a novel Gaussian\nSplatting-based framework that enhances both fidelity and stability for hand\nrendering. To improve fidelity, we extend standard 3DGS attributes with\nimplicit geometry and appearance embeddings for finer non-rigid motion modeling\nwhile preserving the static hand characteristic modeled by original 3DGS\nattributes. Additionally, we introduce a local gradient-aware densification\nstrategy that dynamically refines Gaussian density in high-variation regions.\nTo improve stability, we incorporate pose-conditioned attribute regularization\nto encourage attribute consistency across similar poses, mitigating temporal\nartifacts. Extensive experiments on InterHand2.6M demonstrate that HandSplat\nsurpasses existing methods in fidelity and stability while achieving real-time\nperformance. We will release the code and pre-trained models upon acceptance.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 21:09:04 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Dong', 'Yilan', ''], ['Liu', 'Haohe', ''], ['Wang', 'Qing', ''], ['Yang', 'Jiahao', ''], ['Wang', 'Wenqing', ''], ['Slabaugh', 'Gregory', ''], ['Yuan', 'Shanxin', '']]","extracted_entities":"[{'text': 'implicit geometry and appearance embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"implicit geometry and appearance embeddings","similarity_score":0.5625889897}
{"id":2503.14769,"submitter":"Jonathan Beardsley","authors":"Jonathan Beardsley","title":"Dynkin Systems and the One-Point Geometry","comments":"19 pages, opacity in tikz diagrams does not compile correctly on\n  arXiv","journal-ref":null,"doi":null,"report-no":null,"categories":"math.CT math.AT math.CO math.PR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this note I demonstrate that the collection of Dynkin systems on finite\nsets assembles into a Connes-Consani $\\mathbb{F}_1$-module, with the collection\nof partitions of finite sets as a sub-module. The underlying simplicial set of\nthis $\\mathbb{F}_1$-module is shown to be isomorphic to the delooping of the\nKrasner hyperfield $\\mathbb{K}$, where $1+1=\\{0,1\\}$. The face and degeneracy\nmaps of the underlying simplicial set of the $\\mathbb{F}_1$-module of\npartitions correspond to merging partition blocks and introducing singleton\nblocks, respectively. I also show that the $\\mathbb{F}_1$-module of partitions\ncannot correspond to a set with a binary operation (even partially defined or\nmultivalued) under the ``Eilenberg-MacLane'' embedding. These results imply\nthat the $n$-fold sum of the Dynkin $\\mathbb{F}_1$-module with itself is\nisomorphic to the $\\mathbb{F}_1$-module of the discrete projective geometry on\n$n$ points.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 22:43:06 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Beardsley', 'Jonathan', '']]","extracted_entities":"[{'text': 'Eilenberg-MacLane', 'label': 'Embedding'}, {'text': 'embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embedding","similarity_score":1.0}
{"id":2503.14824,"submitter":"Zikun Zhou","authors":"Zikun Zhou, Yushuai Sun, Wenjie Pei, Xin Li, Yaowei Wang","title":"Prototype Perturbation for Relaxing Alignment Constraints in\n  Backward-Compatible Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The traditional paradigm to update retrieval models requires re-computing the\nembeddings of the gallery data, a time-consuming and computationally intensive\nprocess known as backfilling. To circumvent backfilling, Backward-Compatible\nLearning (BCL) has been widely explored, which aims to train a new model\ncompatible with the old one. Many previous works focus on effectively aligning\nthe embeddings of the new model with those of the old one to enhance the\nbackward-compatibility. Nevertheless, such strong alignment constraints would\ncompromise the discriminative ability of the new model, particularly when\ndifferent classes are closely clustered and hard to distinguish in the old\nfeature space. To address this issue, we propose to relax the constraints by\nintroducing perturbations to the old feature prototypes. This allows us to\nalign the new feature space with a pseudo-old feature space defined by these\nperturbed prototypes, thereby preserving the discriminative ability of the new\nmodel in backward-compatible learning. We have developed two approaches for\ncalculating the perturbations: Neighbor-Driven Prototype Perturbation (NDPP)\nand Optimization-Driven Prototype Perturbation (ODPP). Particularly, they take\ninto account the feature distributions of not only the old but also the new\nmodels to obtain proper perturbations along with new model updating. Extensive\nexperiments on the landmark and commodity datasets demonstrate that our\napproaches perform favorably against state-of-the-art BCL algorithms.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 01:45:48 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhou', 'Zikun', ''], ['Sun', 'Yushuai', ''], ['Pei', 'Wenjie', ''], ['Li', 'Xin', ''], ['Wang', 'Yaowei', '']]","extracted_entities":"[{'text': 'embeddings', 'label': 'Embedding'}, {'text': 'Backward-Compatible\\nLearning', 'label': 'Few-shot Learning'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'backward-compatible learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2503.14868,"submitter":"Hoigi Seo","authors":"Hoigi Seo, Wongi Jeong, Kyungryeol Lee, Se Young Chun","title":"Efficient Personalization of Quantized Diffusion Model without\n  Backpropagation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Diffusion models have shown remarkable performance in image synthesis, but\nthey demand extensive computational and memory resources for training,\nfine-tuning and inference. Although advanced quantization techniques have\nsuccessfully minimized memory usage for inference, training and fine-tuning\nthese quantized models still require large memory possibly due to\ndequantization for accurate computation of gradients and\/or backpropagation for\ngradient-based algorithms. However, memory-efficient fine-tuning is\nparticularly desirable for applications such as personalization that often must\nbe run on edge devices like mobile phones with private data. In this work, we\naddress this challenge by quantizing a diffusion model with personalization via\nTextual Inversion and by leveraging a zeroth-order optimization on\npersonalization tokens without dequantization so that it does not require\ngradient and activation storage for backpropagation that consumes considerable\nmemory. Since a gradient estimation using zeroth-order optimization is quite\nnoisy for a single or a few images in personalization, we propose to denoise\nthe estimated gradient by projecting it onto a subspace that is constructed\nwith the past history of the tokens, dubbed Subspace Gradient. In addition, we\ninvestigated the influence of text embedding in image generation, leading to\nour proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for\nsampling with effective diffusion timesteps. Our method achieves comparable\nperformance to prior methods in image and text alignment scores for\npersonalizing Stable Diffusion with only forward passes while reducing training\nmemory demand up to $8.2\\times$.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 03:45:37 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Seo', 'Hoigi', ''], ['Jeong', 'Wongi', ''], ['Lee', 'Kyungryeol', ''], ['Chun', 'Se Young', '']]","extracted_entities":"[{'text': 'dequantization', 'label': 'quantisation'}, {'text': 'memory-efficient fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Textual Inversion', 'label': 'contextual Embedding'}, {'text': 'dequantization', 'label': 'quantisation'}, {'text': 'zeroth-order optimization', 'label': 'Zero-shot Learning'}, {'text': 'text embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"text embedding","similarity_score":0.8247289658}
{"id":2503.14925,"submitter":"Haoyu Lei","authors":"Haoyu Lei, Shizhan Gong, Qi Dou, Farzan Farnia","title":"pFedFair: Towards Optimal Group Fairness-Accuracy Trade-off in\n  Heterogeneous Federated Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Federated learning (FL) algorithms commonly aim to maximize clients' accuracy\nby training a model on their collective data. However, in several FL\napplications, the model's decisions should meet a group fairness constraint to\nbe independent of sensitive attributes such as gender or race. While such group\nfairness constraints can be incorporated into the objective function of the FL\noptimization problem, in this work, we show that such an approach would lead to\nsuboptimal classification accuracy in an FL setting with heterogeneous client\ndistributions. To achieve an optimal accuracy-group fairness trade-off, we\npropose the Personalized Federated Learning for Client-Level Group Fairness\n(pFedFair) framework, where clients locally impose their fairness constraints\nover the distributed training process. Leveraging the image embedding models,\nwe extend the application of pFedFair to computer vision settings, where we\nnumerically show that pFedFair achieves an optimal group fairness-accuracy\ntrade-off in heterogeneous FL settings. We present the results of several\nnumerical experiments on benchmark and synthetic datasets, which highlight the\nsuboptimality of non-personalized FL algorithms and the improvements made by\nthe pFedFair method.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 06:15:31 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Lei', 'Haoyu', ''], ['Gong', 'Shizhan', ''], ['Dou', 'Qi', ''], ['Farnia', 'Farzan', '']]","extracted_entities":"[{'text': 'model', 'label': 'Neural Language Model'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'image embedding models', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"image embedding models","similarity_score":0.7555274367}
{"id":2503.14979,"submitter":"Lichao Mou","authors":"Yaxiong Chen, Junjian Hu, Chunlei Li, Zixuan Zheng, Jingliang Hu,\n  Yilei Shi, Shengwu Xiong, Xiao Xiang Zhu, Lichao Mou","title":"One-Shot Medical Video Object Segmentation via Temporal Contrastive\n  Memory Networks","comments":"MICCAI 2024 Workshop","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Video object segmentation is crucial for the efficient analysis of complex\nmedical video data, yet it faces significant challenges in data availability\nand annotation. We introduce the task of one-shot medical video object\nsegmentation, which requires separating foreground and background pixels\nthroughout a video given only the mask annotation of the first frame. To\naddress this problem, we propose a temporal contrastive memory network\ncomprising image and mask encoders to learn feature representations, a temporal\ncontrastive memory bank that aligns embeddings from adjacent frames while\npushing apart distant ones to explicitly model inter-frame relationships and\nstores these features, and a decoder that fuses encoded image features and\nmemory readouts for segmentation. We also collect a diverse, multi-source\nmedical video dataset spanning various modalities and anatomies to benchmark\nthis task. Extensive experiments demonstrate state-of-the-art performance in\nsegmenting both seen and unseen structures from a single exemplar, showing\nability to generalize from scarce labels. This highlights the potential to\nalleviate annotation burdens for medical video analysis. Code is available at\nhttps:\/\/github.com\/MedAITech\/TCMN.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 08:17:48 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Chen', 'Yaxiong', ''], ['Hu', 'Junjian', ''], ['Li', 'Chunlei', ''], ['Zheng', 'Zixuan', ''], ['Hu', 'Jingliang', ''], ['Shi', 'Yilei', ''], ['Xiong', 'Shengwu', ''], ['Zhu', 'Xiao Xiang', ''], ['Mou', 'Lichao', '']]","extracted_entities":"[{'text': 'feature representations', 'label': 'Embedding'}, {'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2503.15009,"submitter":"Etienne Menager","authors":"Etienne M\\'enager (WILLOW, DI-ENS), Tanguy Navez (DEFROST), Paul\n  Chaillou (DEFROST, CRIStAL), Olivier Goury (INSERM, DEFROST), Alexandre\n  Kruszewski (DEFROST, CRIStAL), Christian Duriez (DEFROST, CRIStAL)","title":"Modeling, Embedded Control and Design of Soft Robots using a Learned\n  Condensed FEM Model","comments":"IEEE Transactions on Robotics, In press","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The Finite Element Method (FEM) is a powerful modeling tool for predicting\nsoft robots' behavior, but its computation time can limit practical\napplications. In this paper, a learning-based approach based on condensation of\nthe FEM model is detailed. The proposed method handles several kinds of\nactuators and contacts with the environment. We demonstrate that this compact\nmodel can be learned as a unified model across several designs and remains very\nefficient in terms of modeling since we can deduce the direct and inverse\nkinematics of the robot. Building upon the intuition introduced in [11], the\nlearned model is presented as a general framework for modeling, controlling,\nand designing soft manipulators. First, the method's adaptability and\nversatility are illustrated through optimization based control problems\ninvolving positioning and manipulation tasks with mechanical contact-based\ncoupling. Secondly, the low memory consumption and the high prediction speed of\nthe learned condensed model are leveraged for real-time embedding control\nwithout relying on costly online FEM simulation. Finally, the ability of the\nlearned condensed FEM model to capture soft robot design variations and its\ndifferentiability are leveraged in calibration and design optimization\napplications.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 08:59:44 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['M\u00e9nager', 'Etienne', '', 'WILLOW, DI-ENS'], ['Navez', 'Tanguy', '', 'DEFROST'], ['Chaillou', 'Paul', '', 'DEFROST, CRIStAL'], ['Goury', 'Olivier', '', 'INSERM, DEFROST'], ['Kruszewski', 'Alexandre', '', 'DEFROST, CRIStAL'], ['Duriez', 'Christian', '', 'DEFROST, CRIStAL']]","extracted_entities":"[{'text': 'real-time embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"real-time embedding","similarity_score":0.7154863477}
{"id":2503.15029,"submitter":"Jianbo Zhao","authors":"Jianbo Zhao, Taiyu Ban, Zhihao Liu, Hangning Zhou, Xiyang Wang, Qibin\n  Zhou, Hailong Qin, Mu Yang, Lei Liu, Bin Li","title":"DRoPE: Directional Rotary Position Embedding for Efficient Agent\n  Interaction Modeling","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Accurate and efficient modeling of agent interactions is essential for\ntrajectory generation, the core of autonomous driving systems. Existing\nmethods, scene-centric, agent-centric, and query-centric frameworks, each\npresent distinct advantages and drawbacks, creating an impossible triangle\namong accuracy, computational time, and memory efficiency. To break this\nlimitation, we propose Directional Rotary Position Embedding (DRoPE), a novel\nadaptation of Rotary Position Embedding (RoPE), originally developed in natural\nlanguage processing. Unlike traditional relative position embedding (RPE),\nwhich introduces significant space complexity, RoPE efficiently encodes\nrelative positions without explicitly increasing complexity but faces inherent\nlimitations in handling angular information due to periodicity. DRoPE overcomes\nthis limitation by introducing a uniform identity scalar into RoPE's 2D rotary\ntransformation, aligning rotation angles with realistic agent headings to\nnaturally encode relative angular information. We theoretically analyze DRoPE's\ncorrectness and efficiency, demonstrating its capability to simultaneously\noptimize trajectory generation accuracy, time complexity, and space complexity.\nEmpirical evaluations compared with various state-of-the-art trajectory\ngeneration models, confirm DRoPE's good performance and significantly reduced\nspace complexity, indicating both theoretical soundness and practical\neffectiveness. The video documentation is available at\nhttps:\/\/drope-traj.github.io\/.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 09:23:09 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhao', 'Jianbo', ''], ['Ban', 'Taiyu', ''], ['Liu', 'Zhihao', ''], ['Zhou', 'Hangning', ''], ['Wang', 'Xiyang', ''], ['Zhou', 'Qibin', ''], ['Qin', 'Hailong', ''], ['Yang', 'Mu', ''], ['Liu', 'Lei', ''], ['Li', 'Bin', '']]","extracted_entities":"[{'text': 'Directional Rotary Position Embedding', 'label': 'Embedding'}, {'text': 'DRoPE', 'label': 'Embedding'}, {'text': 'Rotary Position Embedding', 'label': 'Embedding'}, {'text': 'RoPE', 'label': 'Embedding'}, {'text': 'relative position embedding', 'label': 'Embedding'}, {'text': 'RoPE', 'label': 'Embedding'}, {'text': 'DRoPE', 'label': 'Embedding'}, {'text': 'RoPE', 'label': 'Embedding'}, {'text': 'DRoPE', 'label': 'Embedding'}, {'text': 'DRoPE', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"relative position embedding","similarity_score":0.7441638708}
{"id":2503.15057,"submitter":"Jaihyun Park","authors":"Jaihyun Park, Ryan Cordell","title":"A Data-driven Investigation of Euphemistic Language: Comparing the usage\n  of \"slave\" and \"servant\" in 19th century US newspapers","comments":"The 5th International Conference on Natural Language Processing for\n  Digital Humanities (NLP4DH)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This study investigates the usage of \"slave\" and \"servant\" in the 19th\ncentury US newspapers using computational methods. While both terms were used\nto refer to enslaved African Americans, they were used in distinct ways. In the\nChronicling America corpus, we included possible OCR errors by using FastText\nembedding and excluded text reprints to consider text reprint culture in the\n19th century. Word2vec embedding was used to find semantically close words to\n\"slave\" and \"servant\" and log-odds ratio was calculated to identify\nover-represented discourse words in the Southern and Northern newspapers. We\nfound that \"slave\" is associated with socio-economic, legal, and administrative\nwords, however, \"servant\" is linked to religious words in the Northern\nnewspapers while Southern newspapers associated \"servant\" with domestic and\nfamilial words. We further found that slave discourse words in Southern\nnewspapers are more prevalent in Northern newspapers while servant discourse\nwords from each side are prevalent in their own region. This study contributes\nto the understanding of how newspapers created different discourses around\nenslaved African Americans in the 19th century US.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 09:49:22 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Park', 'Jaihyun', ''], ['Cordell', 'Ryan', '']]","extracted_entities":"[{'text': 'FastText\\nembedding', 'label': 'Embedding'}, {'text': 'Word2vec embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"FastText\nembedding","similarity_score":0.6925028563}
{"id":2503.15137,"submitter":"Antonio Alarc\\'on","authors":"Antonio Alarcon and Jorge Hidalgo","title":"Holomorphic null curves in the special linear group","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.DG math.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper we develop the theory of approximation for holomorphic null\ncurves in the special linear group ${\\rm SL}_2(\\mathbb{C})$. In particular, we\nestablish Runge, Mergelyan, Mittag-Leffler, and Carleman type theorems for the\nfamily of holomorphic null immersions $M\\to{\\rm SL}_2(\\mathbb{C})$ from any\nopen Riemann surface $M$. Our results include jet interpolation of Weierstrass\ntype and approximation by embeddings, as well as global conditions on the\napproximating curves. As application, we show that every open Riemann surface\nadmits a proper holomorphic null embedding into ${\\rm SL}_2(\\mathbb{C})$, and\nhence also a proper conformal immersion of constant mean curvature $1$ into\nhyperbolic 3-space. This settles a problem posed by Alarcon and Forstneric in\n2015.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:55:40 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Alarcon', 'Antonio', ''], ['Hidalgo', 'Jorge', '']]","extracted_entities":"[{'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2503.15267,"submitter":"Marco Podda","authors":"Alessio Micheli, Alejandro Moreo, Marco Podda, Fabrizio Sebastiani,\n  William Simoni, Domenico Tortorella","title":"Learning to quantify graph nodes","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Network Quantification is the problem of estimating the class proportions in\nunlabeled subsets of graph nodes. When prior probability shift is at play, this\ntask cannot be effectively addressed by first classifying the nodes and then\ncounting the class predictions. In addition, unlike non-relational\nquantification on i.i.d. datapoints, Network Quantification demands enhanced\nflexibility to capture a broad range of connectivity patterns, resilience to\nthe challenge of heterophily, and efficiency to scale to larger networks. To\nmeet these stringent requirements we introduce XNQ, a novel method that\nsynergizes the flexibility and efficiency of the unsupervised node embeddings\ncomputed by randomized recursive Graph Neural Networks, with an\nExpectation-Maximization algorithm that provides a robust quantification-aware\nadjustment to the output probabilities of a calibrated node classifier. We\nvalidate the design choices underpinning our method through comprehensive\nablation experiments. In an extensive evaluation, we find that our approach\nconsistently and significantly improves on the best Network Quantification\nmethods to date, thereby setting the new state of the art for this challenging\ntask. Simultaneously, it provides a training speed-up of up to 10x-100x over\nother graph learning based methods.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 14:43:12 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Micheli', 'Alessio', ''], ['Moreo', 'Alejandro', ''], ['Podda', 'Marco', ''], ['Sebastiani', 'Fabrizio', ''], ['Simoni', 'William', ''], ['Tortorella', 'Domenico', '']]","extracted_entities":"[{'text': 'unsupervised node embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"unsupervised node embeddings","similarity_score":0.6525121927}
{"id":2503.15312,"submitter":"Malgorzata Siudek Malgorzata Siudek","authors":"Euclid Collaboration: M. Siudek, M. Huertas-Company, M. Smith, G.\n  Martinez-Solaeche, F. Lanusse, S. Ho, E. Angeloudi, P. A. C. Cunha, H.\n  Dom\\'inguez S\\'anchez, M. Dunn, Y. Fu, P. Iglesias-Navarro, J. Junais, J. H.\n  Knapen, B. Laloux, M. Mezcua, W. Roster, G. Stevens, J. Vega-Ferrero, N.\n  Aghanim, B. Altieri, A. Amara, S. Andreon, N. Auricchio, H. Aussel, C.\n  Baccigalupi, M. Baldi, S. Bardelli, P. Battaglia, A. Biviano, A. Bonchi, E.\n  Branchini, M. Brescia, J. Brinchmann, S. Camera, G. Ca\\~nas-Herrera, V.\n  Capobianco, C. Carbone, J. Carretero, S. Casas, F. J. Castander, M.\n  Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C.\n  Colodro-Conde, G. Congedo, C. J. Conselice, L. Conversi, Y. Copin, F.\n  Courbin, H. M. Courtois, M. Cropper, A. Da Silva, H. Degaudenzi, G. De Lucia,\n  A. M. Di Giorgio, J. Dinis, C. Dolding, H. Dole, F. Dubath, C. A. J. Duncan,\n  X. Dupac, S. Dusini, S. Escoffier, M. Farina, R. Farinelli, F. Faustini, S.\n  Ferriol, F. Finelli, S. Fotopoulou, M. Frailis, E. Franceschi, S. Galeotta,\n  K. George, B. Gillis, C. Giocoli, J. Gracia-Carpio, B. R. Granett, A.\n  Grazian, F. Grupp, S. Gwyn, S. V. H. Haugan, W. Holmes, I. M. Hook, F.\n  Hormuth, A. Hornstrup, K. Jahnke, M. Jhabvala, E. Keih\\\"anen, S. Kermiche, A.\n  Kiessling, B. Kubik, M. K\\\"ummel, M. Kunz, H. Kurki-Suonio, Q. Le Boulc'h, A.\n  M. C. Le Brun, D. Le Mignant, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro,\n  G. Mainetti, D. Maino, E. Maiorano, O. Mansutti, S. Marcin, O. Marggraf, M.\n  Martinelli, N. Martinet, F. Marulli, R. Massey, S. Maurogordato, H. J.\n  McCracken, E. Medinaceli, S. Mei, M. Melchior, Y. Mellier, M. Meneghetti, E.\n  Merlin, G. Meylan, A. Mora, M. Moresco, L. Moscardini, R. Nakajima, C.\n  Neissner, S.-M. Niemi, J. W. Nightingale, C. Padilla, S. Paltani, F. Pasian,\n  K. Pedersen, W. J. Percival, V. Pettorino, S. Pires, G. Polenta, M. Poncet,\n  L. A. Popa, L. Pozzetti, F. Raison, A. Renzi, J. Rhodes, G. Riccio, E.\n  Romelli, M. Roncarelli, R. Saglia, Z. Sakr, A. G. S\\'anchez, D. Sapone, B.\n  Sartoris, J. A. Schewtschenko, P. Schneider, T. Schrabback, M. Scodeggio, A.\n  Secroun, G. Seidel, M. Seiffert, S. Serrano, P. Simon, C. Sirignano, G.\n  Sirri, L. Stanco, J. Steinwagner, P. Tallada-Cresp\\'i, A. N. Taylor, I.\n  Tereno, S. Toft, R. Toledo-Moreo, F. Torradeflot, I. Tutusaus, L. Valenziano,\n  J. Valiviita, T. Vassallo, G. Verdoes Kleijn, A. Veropalumbo, Y. Wang, J.\n  Weller, A. Zacchei, G. Zamorani, F. M. Zerbi, I. A. Zinchenko, E. Zucca, V.\n  Allevato, M. Ballardini, M. Bolzonella, E. Bozzo, C. Burigana, R. Cabanac, A.\n  Cappi, D. Di Ferdinando, J. A. Escartin Vigo, L. Gabarra, J.\n  Mart\\'in-Fleitas, S. Matthew, N. Mauri, R. B. Metcalf, A. Pezzotta, M.\n  P\\\"ontinen, C. Porciani, I. Risso, V. Scottez, M. Sereno, M. Tenti, M. Viel,\n  M. Wiesmann, Y. Akrami, I. T. Andika, S. Anselmi, M. Archidiacono, F.\n  Atrio-Barandela, C. Benoist, K. Benson, D. Bertacca, M. Bethermin, L.\n  Bisigello, A. Blanchard, L. Blot, M. L. Brown, S. Bruton, A. Calabro, B.\n  Camacho Quevedo, F. Caro, C. S. Carvalho, T. Castro, Y. Charles, F. Cogato,\n  A. R. Cooray, O. Cucciati, S. Davini, F. De Paolis, G. Desprez, A.\n  D\\'iaz-S\\'anchez, J. J. Diaz, S. Di Domizio, J. M. Diego, P.-A. Duc, A. Enia,\n  Y. Fang, A. G. Ferrari, P. G. Ferreira, A. Finoguenov, A. Fontana, A. Franco,\n  K. Ganga, J. Garc\\'ia-Bellido, T. Gasparetto, V. Gautard, E. Gaztanaga, F.\n  Giacomini, F. Gianotti, G. Gozaliasl, M. Guidi, C. M. Gutierrez, A. Hall, W.\n  G. Hartley, S. Hemmati, C. Hern\\'andez-Monteagudo, H. Hildebrandt, J. Hjorth,\n  J. J. E. Kajava, Y. Kang, V. Kansal, D. Karagiannis, K. Kiiveri, C. C.\n  Kirkpatrick, S. Kruk, J. Le Graet, L. Legrand, M. Lembo, F. Lepori, G. Leroy,\n  G. F. Lesci, J. Lesgourgues, L. Leuzzi, T. I. Liaudat, A. Loureiro, J.\n  Macias-Perez, G. Maggio, M. Magliocchetti, E. A. Magnier, F. Mannucci, R.\n  Maoli, C. J. A. P. Martins, L. Maurin, M. Miluzio, P. Monaco, C. Moretti, G.\n  Morgante, C. Murray, K. Naidoo, A. Navarro-Alsina, S. Nesseris, F.\n  Passalacqua, K. Paterson, L. Patrizii, A. Pisani, D. Potter, S. Quai, M.\n  Radovich, S. Sacquegna, M. Sahl\\'en, D. B. Sanders, E. Sarpa, A. Schneider,\n  D. Sciotti, D. Scognamiglio, E. Sellentin, L. C. Smith, K. Tanidis, G.\n  Testera, R. Teyssier, S. Tosi, A. Troja, M. Tucci, C. Valieri, A. Venhola, D.\n  Vergani, G. Verza, P. Vielzeuf, N. A. Walton, J. G. Sorce","title":"Euclid Quick Data Release (Q1) Exploring galaxy properties with a\n  multi-modal foundation model","comments":"Paper submitted as part of the A&A Special Issue `Euclid Quick Data\n  Release (Q1)', 31 pages, 17 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.GA","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Modern astronomical surveys, such as the Euclid mission, produce\nhigh-dimensional, multi-modal data sets that include imaging and spectroscopic\ninformation for millions of galaxies. These data serve as an ideal benchmark\nfor large, pre-trained multi-modal models, which can leverage vast amounts of\nunlabelled data. In this work, we present the first exploration of Euclid data\nwith AstroPT, an autoregressive multi-modal foundation model trained on\napproximately 300 000 optical and infrared Euclid images and spectral energy\ndistributions (SEDs) from the first Euclid Quick Data Release. We compare\nself-supervised pre-training with baseline fully supervised training across\nseveral tasks: galaxy morphology classification; redshift estimation;\nsimilarity searches; and outlier detection. Our results show that: (a) AstroPT\nembeddings are highly informative, correlating with morphology and effectively\nisolating outliers; (b) including infrared data helps to isolate stars, but\ndegrades the identification of edge-on galaxies, which are better captured by\noptical images; (c) simple fine-tuning of these embeddings for photometric\nredshift and stellar mass estimation outperforms a fully supervised approach,\neven when using only 1% of the training labels; and (d) incorporating SED data\ninto AstroPT via a straightforward multi-modal token-chaining method improves\nphoto-z predictions, and allow us to identify potentially more interesting\nanomalies (such as ringed or interacting galaxies) compared to a model\npre-trained solely on imaging data.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:27:07 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Euclid Collaboration', '', ''], ['Siudek', 'M.', ''], ['Huertas-Company', 'M.', ''], ['Smith', 'M.', ''], ['Martinez-Solaeche', 'G.', ''], ['Lanusse', 'F.', ''], ['Ho', 'S.', ''], ['Angeloudi', 'E.', ''], ['Cunha', 'P. A. C.', ''], ['S\u00e1nchez', 'H. Dom\u00ednguez', ''], ['Dunn', 'M.', ''], ['Fu', 'Y.', ''], ['Iglesias-Navarro', 'P.', ''], ['Junais', 'J.', ''], ['Knapen', 'J. H.', ''], ['Laloux', 'B.', ''], ['Mezcua', 'M.', ''], ['Roster', 'W.', ''], ['Stevens', 'G.', ''], ['Vega-Ferrero', 'J.', ''], ['Aghanim', 'N.', ''], ['Altieri', 'B.', ''], ['Amara', 'A.', ''], ['Andreon', 'S.', ''], ['Auricchio', 'N.', ''], ['Aussel', 'H.', ''], ['Baccigalupi', 'C.', ''], ['Baldi', 'M.', ''], ['Bardelli', 'S.', ''], ['Battaglia', 'P.', ''], ['Biviano', 'A.', ''], ['Bonchi', 'A.', ''], ['Branchini', 'E.', ''], ['Brescia', 'M.', ''], ['Brinchmann', 'J.', ''], ['Camera', 'S.', ''], ['Ca\u00f1as-Herrera', 'G.', ''], ['Capobianco', 'V.', ''], ['Carbone', 'C.', ''], ['Carretero', 'J.', ''], ['Casas', 'S.', ''], ['Castander', 'F. J.', ''], ['Castellano', 'M.', ''], ['Castignani', 'G.', ''], ['Cavuoti', 'S.', ''], ['Chambers', 'K. C.', ''], ['Cimatti', 'A.', ''], ['Colodro-Conde', 'C.', ''], ['Congedo', 'G.', ''], ['Conselice', 'C. J.', ''], ['Conversi', 'L.', ''], ['Copin', 'Y.', ''], ['Courbin', 'F.', ''], ['Courtois', 'H. M.', ''], ['Cropper', 'M.', ''], ['Da Silva', 'A.', ''], ['Degaudenzi', 'H.', ''], ['De Lucia', 'G.', ''], ['Di Giorgio', 'A. M.', ''], ['Dinis', 'J.', ''], ['Dolding', 'C.', ''], ['Dole', 'H.', ''], ['Dubath', 'F.', ''], ['Duncan', 'C. A. J.', ''], ['Dupac', 'X.', ''], ['Dusini', 'S.', ''], ['Escoffier', 'S.', ''], ['Farina', 'M.', ''], ['Farinelli', 'R.', ''], ['Faustini', 'F.', ''], ['Ferriol', 'S.', ''], ['Finelli', 'F.', ''], ['Fotopoulou', 'S.', ''], ['Frailis', 'M.', ''], ['Franceschi', 'E.', ''], ['Galeotta', 'S.', ''], ['George', 'K.', ''], ['Gillis', 'B.', ''], ['Giocoli', 'C.', ''], ['Gracia-Carpio', 'J.', ''], ['Granett', 'B. R.', ''], ['Grazian', 'A.', ''], ['Grupp', 'F.', ''], ['Gwyn', 'S.', ''], ['Haugan', 'S. V. H.', ''], ['Holmes', 'W.', ''], ['Hook', 'I. M.', ''], ['Hormuth', 'F.', ''], ['Hornstrup', 'A.', ''], ['Jahnke', 'K.', ''], ['Jhabvala', 'M.', ''], ['Keih\u00e4nen', 'E.', ''], ['Kermiche', 'S.', ''], ['Kiessling', 'A.', ''], ['Kubik', 'B.', ''], ['K\u00fcmmel', 'M.', ''], ['Kunz', 'M.', ''], ['Kurki-Suonio', 'H.', ''], [\"Boulc'h\", 'Q. Le', ''], ['Brun', 'A. M. C. Le', ''], ['Mignant', 'D. Le', ''], ['Ligori', 'S.', ''], ['Lilje', 'P. B.', ''], ['Lindholm', 'V.', ''], ['Lloro', 'I.', ''], ['Mainetti', 'G.', ''], ['Maino', 'D.', ''], ['Maiorano', 'E.', ''], ['Mansutti', 'O.', ''], ['Marcin', 'S.', ''], ['Marggraf', 'O.', ''], ['Martinelli', 'M.', ''], ['Martinet', 'N.', ''], ['Marulli', 'F.', ''], ['Massey', 'R.', ''], ['Maurogordato', 'S.', ''], ['McCracken', 'H. J.', ''], ['Medinaceli', 'E.', ''], ['Mei', 'S.', ''], ['Melchior', 'M.', ''], ['Mellier', 'Y.', ''], ['Meneghetti', 'M.', ''], ['Merlin', 'E.', ''], ['Meylan', 'G.', ''], ['Mora', 'A.', ''], ['Moresco', 'M.', ''], ['Moscardini', 'L.', ''], ['Nakajima', 'R.', ''], ['Neissner', 'C.', ''], ['Niemi', 'S. -M.', ''], ['Nightingale', 'J. W.', ''], ['Padilla', 'C.', ''], ['Paltani', 'S.', ''], ['Pasian', 'F.', ''], ['Pedersen', 'K.', ''], ['Percival', 'W. J.', ''], ['Pettorino', 'V.', ''], ['Pires', 'S.', ''], ['Polenta', 'G.', ''], ['Poncet', 'M.', ''], ['Popa', 'L. A.', ''], ['Pozzetti', 'L.', ''], ['Raison', 'F.', ''], ['Renzi', 'A.', ''], ['Rhodes', 'J.', ''], ['Riccio', 'G.', ''], ['Romelli', 'E.', ''], ['Roncarelli', 'M.', ''], ['Saglia', 'R.', ''], ['Sakr', 'Z.', ''], ['S\u00e1nchez', 'A. G.', ''], ['Sapone', 'D.', ''], ['Sartoris', 'B.', ''], ['Schewtschenko', 'J. A.', ''], ['Schneider', 'P.', ''], ['Schrabback', 'T.', ''], ['Scodeggio', 'M.', ''], ['Secroun', 'A.', ''], ['Seidel', 'G.', ''], ['Seiffert', 'M.', ''], ['Serrano', 'S.', ''], ['Simon', 'P.', ''], ['Sirignano', 'C.', ''], ['Sirri', 'G.', ''], ['Stanco', 'L.', ''], ['Steinwagner', 'J.', ''], ['Tallada-Cresp\u00ed', 'P.', ''], ['Taylor', 'A. N.', ''], ['Tereno', 'I.', ''], ['Toft', 'S.', ''], ['Toledo-Moreo', 'R.', ''], ['Torradeflot', 'F.', ''], ['Tutusaus', 'I.', ''], ['Valenziano', 'L.', ''], ['Valiviita', 'J.', ''], ['Vassallo', 'T.', ''], ['Kleijn', 'G. Verdoes', ''], ['Veropalumbo', 'A.', ''], ['Wang', 'Y.', ''], ['Weller', 'J.', ''], ['Zacchei', 'A.', ''], ['Zamorani', 'G.', ''], ['Zerbi', 'F. M.', ''], ['Zinchenko', 'I. A.', ''], ['Zucca', 'E.', ''], ['Allevato', 'V.', ''], ['Ballardini', 'M.', ''], ['Bolzonella', 'M.', ''], ['Bozzo', 'E.', ''], ['Burigana', 'C.', ''], ['Cabanac', 'R.', ''], ['Cappi', 'A.', ''], ['Di Ferdinando', 'D.', ''], ['Vigo', 'J. A. Escartin', ''], ['Gabarra', 'L.', ''], ['Mart\u00edn-Fleitas', 'J.', ''], ['Matthew', 'S.', ''], ['Mauri', 'N.', ''], ['Metcalf', 'R. B.', ''], ['Pezzotta', 'A.', ''], ['P\u00f6ntinen', 'M.', ''], ['Porciani', 'C.', ''], ['Risso', 'I.', ''], ['Scottez', 'V.', ''], ['Sereno', 'M.', ''], ['Tenti', 'M.', ''], ['Viel', 'M.', ''], ['Wiesmann', 'M.', ''], ['Akrami', 'Y.', ''], ['Andika', 'I. T.', ''], ['Anselmi', 'S.', ''], ['Archidiacono', 'M.', ''], ['Atrio-Barandela', 'F.', ''], ['Benoist', 'C.', ''], ['Benson', 'K.', ''], ['Bertacca', 'D.', ''], ['Bethermin', 'M.', ''], ['Bisigello', 'L.', ''], ['Blanchard', 'A.', ''], ['Blot', 'L.', ''], ['Brown', 'M. L.', ''], ['Bruton', 'S.', ''], ['Calabro', 'A.', ''], ['Quevedo', 'B. Camacho', ''], ['Caro', 'F.', ''], ['Carvalho', 'C. S.', ''], ['Castro', 'T.', ''], ['Charles', 'Y.', ''], ['Cogato', 'F.', ''], ['Cooray', 'A. R.', ''], ['Cucciati', 'O.', ''], ['Davini', 'S.', ''], ['De Paolis', 'F.', ''], ['Desprez', 'G.', ''], ['D\u00edaz-S\u00e1nchez', 'A.', ''], ['Diaz', 'J. J.', ''], ['Di Domizio', 'S.', ''], ['Diego', 'J. M.', ''], ['Duc', 'P. -A.', ''], ['Enia', 'A.', ''], ['Fang', 'Y.', ''], ['Ferrari', 'A. G.', ''], ['Ferreira', 'P. G.', ''], ['Finoguenov', 'A.', ''], ['Fontana', 'A.', ''], ['Franco', 'A.', ''], ['Ganga', 'K.', ''], ['Garc\u00eda-Bellido', 'J.', ''], ['Gasparetto', 'T.', ''], ['Gautard', 'V.', ''], ['Gaztanaga', 'E.', ''], ['Giacomini', 'F.', ''], ['Gianotti', 'F.', ''], ['Gozaliasl', 'G.', ''], ['Guidi', 'M.', ''], ['Gutierrez', 'C. M.', ''], ['Hall', 'A.', ''], ['Hartley', 'W. G.', ''], ['Hemmati', 'S.', ''], ['Hern\u00e1ndez-Monteagudo', 'C.', ''], ['Hildebrandt', 'H.', ''], ['Hjorth', 'J.', ''], ['Kajava', 'J. J. E.', ''], ['Kang', 'Y.', ''], ['Kansal', 'V.', ''], ['Karagiannis', 'D.', ''], ['Kiiveri', 'K.', ''], ['Kirkpatrick', 'C. C.', ''], ['Kruk', 'S.', ''], ['Graet', 'J. Le', ''], ['Legrand', 'L.', ''], ['Lembo', 'M.', ''], ['Lepori', 'F.', ''], ['Leroy', 'G.', ''], ['Lesci', 'G. F.', ''], ['Lesgourgues', 'J.', ''], ['Leuzzi', 'L.', ''], ['Liaudat', 'T. I.', ''], ['Loureiro', 'A.', ''], ['Macias-Perez', 'J.', ''], ['Maggio', 'G.', ''], ['Magliocchetti', 'M.', ''], ['Magnier', 'E. A.', ''], ['Mannucci', 'F.', ''], ['Maoli', 'R.', ''], ['Martins', 'C. J. A. P.', ''], ['Maurin', 'L.', ''], ['Miluzio', 'M.', ''], ['Monaco', 'P.', ''], ['Moretti', 'C.', ''], ['Morgante', 'G.', ''], ['Murray', 'C.', ''], ['Naidoo', 'K.', ''], ['Navarro-Alsina', 'A.', ''], ['Nesseris', 'S.', ''], ['Passalacqua', 'F.', ''], ['Paterson', 'K.', ''], ['Patrizii', 'L.', ''], ['Pisani', 'A.', ''], ['Potter', 'D.', ''], ['Quai', 'S.', ''], ['Radovich', 'M.', ''], ['Sacquegna', 'S.', ''], ['Sahl\u00e9n', 'M.', ''], ['Sanders', 'D. B.', ''], ['Sarpa', 'E.', ''], ['Schneider', 'A.', ''], ['Sciotti', 'D.', ''], ['Scognamiglio', 'D.', ''], ['Sellentin', 'E.', ''], ['Smith', 'L. C.', ''], ['Tanidis', 'K.', ''], ['Testera', 'G.', ''], ['Teyssier', 'R.', ''], ['Tosi', 'S.', ''], ['Troja', 'A.', ''], ['Tucci', 'M.', ''], ['Valieri', 'C.', ''], ['Venhola', 'A.', ''], ['Vergani', 'D.', ''], ['Verza', 'G.', ''], ['Vielzeuf', 'P.', ''], ['Walton', 'N. A.', ''], ['Sorce', 'J. G.', '']]","extracted_entities":"[{'text': 'AstroPT', 'label': 'Foundation Model'}, {'text': 'AstroPT\\nembeddings', 'label': 'Embedding'}, {'text': 'simple fine-tuning', 'label': 'Fine-tuning'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'AstroPT', 'label': 'Foundation Model'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2503.15374,"submitter":"Anatole Callies","authors":"Anatole Callies (Inato), Quentin Bodinier (Inato), Philippe Ravaud\n  (Inato, Universit\\'e Paris Cit\\'e and Universit\\'e Sorbonne Paris Nord,\n  INSERM, INRAE, Paris, France, Centre d'epid\\'emiologie clinique, AP-HP,\n  H\\^opital H\\^otel Dieu, Paris, France) and Kourosh Davarpanah (Inato)","title":"Real-world validation of a multimodal LLM-powered pipeline for\n  High-Accuracy Clinical Trial Patient Matching leveraging EHR data","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Background: Patient recruitment in clinical trials is hindered by complex\neligibility criteria and labor-intensive chart reviews. Prior research using\ntext-only models have struggled to address this problem in a reliable and\nscalable way due to (1) limited reasoning capabilities, (2) information loss\nfrom converting visual records to text, and (3) lack of a generic EHR\nintegration to extract patient data.\n  Methods: We introduce a broadly applicable, integration-free, LLM-powered\npipeline that automates patient-trial matching using unprocessed documents\nextracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm,\nenabling the assessment of even the most complex criteria, (2) visual\ncapabilities of latest LLMs to interpret medical records without lossy\nimage-to-text conversions, and (3) multimodal embeddings for efficient medical\nrecord search. The pipeline was validated on the n2c2 2018 cohort selection\ndataset (288 diabetic patients) and a real-world dataset composed of 485\npatients from 30 different sites matched against 36 diverse trials.\n  Results: On the n2c2 dataset, our method achieved a new state-of-the-art\ncriterion-level accuracy of 93\\%. In real-world trials, the pipeline yielded an\naccuracy of 87\\%, undermined by the difficulty to replicate human\ndecision-making when medical records lack sufficient information. Nevertheless,\nusers were able to review overall eligibility in under 9 minutes per patient on\naverage, representing an 80\\% improvement over traditional manual chart\nreviews.\n  Conclusion: This pipeline demonstrates robust performance in clinical trial\npatient matching without requiring custom integration with site systems or\ntrial-specific tailoring, thereby enabling scalable deployment across sites\nseeking to leverage AI for patient matching.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:12:11 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Callies', 'Anatole', '', 'Inato'], ['Bodinier', 'Quentin', '', 'Inato'], ['Ravaud', 'Philippe', '', \"Inato, Universit\u00e9 Paris Cit\u00e9 and Universit\u00e9 Sorbonne Paris Nord,\\n  INSERM, INRAE, Paris, France, Centre d'epid\u00e9miologie clinique, AP-HP,\\n  H\u00f4pital H\u00f4tel Dieu, Paris, France\"], ['Davarpanah', 'Kourosh', '', 'Inato']]","extracted_entities":"[{'text': 'multimodal embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"multimodal embeddings","similarity_score":0.6755873561}
{"id":2503.15406,"submitter":"Jisu Nam","authors":"Jisu Nam, Soowon Son, Zhan Xu, Jing Shi, Difan Liu, Feng Liu, Aashish\n  Misraa, Seungryong Kim, Yang Zhou","title":"Visual Persona: Foundation Model for Full-Body Human Customization","comments":"CVPR 2025, Project page is available at\n  https:\/\/cvlab-kaist.github.io\/Visual-Persona","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce Visual Persona, a foundation model for text-to-image full-body\nhuman customization that, given a single in-the-wild human image, generates\ndiverse images of the individual guided by text descriptions. Unlike prior\nmethods that focus solely on preserving facial identity, our approach captures\ndetailed full-body appearance, aligning with text descriptions for body\nstructure and scene variations. Training this model requires large-scale paired\nhuman data, consisting of multiple images per individual with consistent\nfull-body identities, which is notoriously difficult to obtain. To address\nthis, we propose a data curation pipeline leveraging vision-language models to\nevaluate full-body appearance consistency, resulting in Visual Persona-500K, a\ndataset of 580k paired human images across 100k unique identities. For precise\nappearance transfer, we introduce a transformer encoder-decoder architecture\nadapted to a pre-trained text-to-image diffusion model, which augments the\ninput image into distinct body regions, encodes these regions as local\nappearance features, and projects them into dense identity embeddings\nindependently to condition the diffusion model for synthesizing customized\nimages. Visual Persona consistently surpasses existing approaches, generating\nhigh-quality, customized images from in-the-wild inputs. Extensive ablation\nstudies validate design choices, and we demonstrate the versatility of Visual\nPersona across various downstream tasks.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:45:47 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Nam', 'Jisu', ''], ['Son', 'Soowon', ''], ['Xu', 'Zhan', ''], ['Shi', 'Jing', ''], ['Liu', 'Difan', ''], ['Liu', 'Feng', ''], ['Misraa', 'Aashish', ''], ['Kim', 'Seungryong', ''], ['Zhou', 'Yang', '']]","extracted_entities":"[{'text': 'Visual Persona', 'label': 'Foundation Model'}, {'text': 'dense identity embeddings', 'label': 'Embedding'}, {'text': 'Visual Persona', 'label': 'Foundation Model'}, {'text': 'Visual\\nPersona', 'label': 'Foundation Model'}]","assigned_concept":"Embedding","matched_keyword":"dense identity embeddings","similarity_score":0.613604188}
{"id":2503.15441,"submitter":"Te-Sheng Lin","authors":"Wei-Fan Hu and Te-Sheng Lin and Ming-Chih Lai","title":"A discontinuity-capturing neural network with categorical embedding and\n  its application to anisotropic elliptic interface problems","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.LG cs.NA","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper, we propose a discontinuity-capturing shallow neural network\nwith categorical embedding to represent piecewise smooth functions. The network\ncomprises three hidden layers, a discontinuity-capturing layer, a categorical\nembedding layer, and a fully-connected layer. Under such a design, we show that\na piecewise smooth function, even with a large number of pieces, can be\napproximated by a single neural network with high prediction accuracy. We then\nleverage the proposed network model to solve anisotropic elliptic interface\nproblems. The network is trained by minimizing the mean squared error loss of\nthe system. Our results show that, despite its simple and shallow structure,\nthe proposed neural network model exhibits comparable efficiency and accuracy\nto traditional grid-based numerical methods.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:21:51 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Hu', 'Wei-Fan', ''], ['Lin', 'Te-Sheng', ''], ['Lai', 'Ming-Chih', '']]","extracted_entities":"[{'text': 'categorical embedding', 'label': 'Embedding'}, {'text': 'categorical\\nembedding layer', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"categorical embedding","similarity_score":0.7434220314}
{"id":2503.15458,"submitter":"Gediminas Juska","authors":"Gediminas Juska, Simone Varo, Nicola Maraviglia, John O'Hara, Salvador\n  Medina, Luca Colavecchi, Francesco Mattana, Armando Trapala, Michael Schmidt,\n  Agnieszka Gocalinska, Emanuele Pelucchi","title":"Self-aligned pillar arrays embedding site-controlled single quantum dots\n  for enhanced non-classical light emission","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.optics cond-mat.other","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This work presents a foundational approach for fabricating arrays of\nself-aligned micro- and nanopillar structures incorporating individual\nsite-controlled quantum dots (QDs) for enhanced light extraction. This method\nleverages the non-planar surface morphology of pyramidal QD samples to define\ndielectric masks self - aligned to the QD positions. The mask size, and\nconsequently the lateral dimensions of the pillars, is precisely controlled\nthrough a chemical mechanical polishing step, obviating the need for any\nadditional lithography step for creating the pillar. This fabrication technique\noffers several key advantages, including precise control over the pillar sites,\nand fully deterministic embedding of QD structures. The functionality of the\nstructures was validated by integrating single In0.25Ga0.75As QDs - upon\ntwo-photon excitation of the biexciton state, the emission of single and\npolarization-entangled photon pairs was observed. Additionally, an extra\nfabrication step to deposit dome-like structures atop the pillars was\ndemonstrated, effectively enhancing light extraction efficiency up to 12%.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:40:43 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Juska', 'Gediminas', ''], ['Varo', 'Simone', ''], ['Maraviglia', 'Nicola', ''], [\"O'Hara\", 'John', ''], ['Medina', 'Salvador', ''], ['Colavecchi', 'Luca', ''], ['Mattana', 'Francesco', ''], ['Trapala', 'Armando', ''], ['Schmidt', 'Michael', ''], ['Gocalinska', 'Agnieszka', ''], ['Pelucchi', 'Emanuele', '']]","extracted_entities":"[{'text': 'embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embedding","similarity_score":1.0}
{"id":2503.15573,"submitter":"Da Ma","authors":"Da Ma and Gonghu Shang and Zhi Chen and Libo Qin and Yijie Luo and Lei\n  Pan and Shuai Fan and Lu Chen and Kai Yu","title":"Neuronal Activation States as Sample Embeddings for Data Selection in\n  Task-Specific Instruction Tuning","comments":"preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Task-specific instruction tuning enhances the performance of large language\nmodels (LLMs) on specialized tasks, yet efficiently selecting relevant data for\nthis purpose remains a challenge. Inspired by neural coactivation in the human\nbrain, we propose a novel data selection method called NAS, which leverages\nneuronal activation states as embeddings for samples in the feature space.\nExtensive experiments show that NAS outperforms classical data selection\nmethods in terms of both effectiveness and robustness across different models,\ndatasets, and selection ratios.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:35:57 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Ma', 'Da', ''], ['Shang', 'Gonghu', ''], ['Chen', 'Zhi', ''], ['Qin', 'Libo', ''], ['Luo', 'Yijie', ''], ['Pan', 'Lei', ''], ['Fan', 'Shuai', ''], ['Chen', 'Lu', ''], ['Yu', 'Kai', '']]","extracted_entities":"[{'text': 'Task-specific instruction tuning', 'label': 'Fine-tuning'}, {'text': 'embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embeddings","similarity_score":0.9630644321}
{"id":2503.15576,"submitter":"Alba M\\'arquez-Rodr\\'iguez","authors":"Alba M\\'arquez-Rodr\\'iguez, Miguel \\'Angel Mohedano-Munoz, Manuel J.\n  Mar\\'in-Jim\\'enez, Eduardo Santamar\\'ia-Garc\\'ia, Giulia Bastianelli, Pedro\n  Jordano and Irene Mendoza","title":"A Bird Song Detector for improving bird identification through Deep\n  Learning: a case study from Do\\~nana","comments":"20 pages, 13 images, for associated dataset see\n  https:\/\/huggingface.co\/datasets\/GrunCrow\/BIRDeep_AudioAnnotations , for\n  associated code see\n  https:\/\/github.com\/GrunCrow\/BIRDeep_BirdSongDetector_NeuralNetworks and\n  https:\/\/github.com\/GrunCrow\/Bird-Song-Detector","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.AI cs.CV cs.LG cs.NE","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Passive Acoustic Monitoring with automatic recorders is essential for\necosystem conservation but generates vast unsupervised audio data, posing\nchallenges for extracting meaningful information. Deep Learning techniques\noffer a promising solution. BirdNET, a widely used model for bird\nidentification, has shown success in many study systems but is limited in some\nregions due to biases in its training data. A key challenge in bird species\ndetection is that many recordings either lack target species or contain\noverlapping vocalizations. To overcome these problems, we developed a\nmulti-stage pipeline for automatic bird vocalization identification in Do\\~nana\nNational Park (SW Spain), a region facing significant conservation threats. Our\napproach included a Bird Song Detector to isolate vocalizations and custom\nclassifiers trained with BirdNET embeddings. We manually annotated 461 minutes\nof audio from three habitats across nine locations, yielding 3,749 annotations\nfor 34 classes. Spectrograms facilitated the use of image processing\ntechniques. Applying the Bird Song Detector before classification improved\nspecies identification, as all classification models performed better when\nanalyzing only the segments where birds were detected. Specifically, the\ncombination of the Bird Song Detector and fine-tuned BirdNET compared to the\nbaseline without the Bird Song Detector. Our approach demonstrated the\neffectiveness of integrating a Bird Song Detector with fine-tuned\nclassification models for bird identification at local soundscapes. These\nfindings highlight the need to adapt general-purpose tools for specific\necological challenges, as demonstrated in Do\\~nana. Automatically detecting\nbird species serves for tracking the health status of this threatened\necosystem, given the sensitivity of birds to environmental changes, and helps\nin the design of conservation measures for reducing biodiversity loss\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 13:19:06 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['M\u00e1rquez-Rodr\u00edguez', 'Alba', ''], ['Mohedano-Munoz', 'Miguel \u00c1ngel', ''], ['Mar\u00edn-Jim\u00e9nez', 'Manuel J.', ''], ['Santamar\u00eda-Garc\u00eda', 'Eduardo', ''], ['Bastianelli', 'Giulia', ''], ['Jordano', 'Pedro', ''], ['Mendoza', 'Irene', '']]","extracted_entities":"[{'text': 'BirdNET embeddings', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"BirdNET embeddings","similarity_score":0.6178225279}
{"id":2503.15617,"submitter":"Masud Ahmed","authors":"Masud Ahmed, Zahid Hasan, Syed Arefinul Haque, Abu Zaher Md Faridee,\n  Sanjay Purushotham, Suya You, Nirmalya Roy","title":"CAM-Seg: A Continuous-valued Embedding Approach for Semantic Image\n  Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Traditional transformer-based semantic segmentation relies on quantized\nembeddings. However, our analysis reveals that autoencoder accuracy on\nsegmentation mask using quantized embeddings (e.g. VQ-VAE) is 8% lower than\ncontinuous-valued embeddings (e.g. KL-VAE). Motivated by this, we propose a\ncontinuous-valued embedding framework for semantic segmentation. By\nreformulating semantic mask generation as a continuous image-to-embedding\ndiffusion process, our approach eliminates the need for discrete latent\nrepresentations while preserving fine-grained spatial and semantic details. Our\nkey contribution includes a diffusion-guided autoregressive transformer that\nlearns a continuous semantic embedding space by modeling long-range\ndependencies in image features. Our framework contains a unified architecture\ncombining a VAE encoder for continuous feature extraction, a diffusion-guided\ntransformer for conditioned embedding generation, and a VAE decoder for\nsemantic mask reconstruction. Our setting facilitates zero-shot domain\nadaptation capabilities enabled by the continuity of the embedding space.\nExperiments across diverse datasets (e.g., Cityscapes and domain-shifted\nvariants) demonstrate state-of-the-art robustness to distribution shifts,\nincluding adverse weather (e.g., fog, snow) and viewpoint variations. Our model\nalso exhibits strong noise resilience, achieving robust performance ($\\approx$\n95% AP compared to baseline) under gaussian noise, moderate motion blur, and\nmoderate brightness\/contrast variations, while experiencing only a moderate\nimpact ($\\approx$ 90% AP compared to baseline) from 50% salt and pepper noise,\nsaturation and hue shifts. Code available:\nhttps:\/\/github.com\/mahmed10\/CAMSS.git\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 18:06:54 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Ahmed', 'Masud', ''], ['Hasan', 'Zahid', ''], ['Haque', 'Syed Arefinul', ''], ['Faridee', 'Abu Zaher Md', ''], ['Purushotham', 'Sanjay', ''], ['You', 'Suya', ''], ['Roy', 'Nirmalya', '']]","extracted_entities":"[{'text': 'quantized\\nembeddings', 'label': 'Embedding'}, {'text': 'quantized embeddings', 'label': 'Embedding'}, {'text': 'VQ-VAE', 'label': 'Embedding'}, {'text': 'continuous-valued embeddings', 'label': 'Embedding'}, {'text': 'KL-VAE', 'label': 'Embedding'}, {'text': 'continuous semantic embedding space', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"continuous-valued embeddings","similarity_score":0.6811061502}
{"id":2503.15712,"submitter":"Weiwen Hu","authors":"Weiwen Hu, Niccol\\`o Parodi, Marcus Zepp, Ingo Feldmann, Oliver\n  Schreer, Peter Eisert","title":"SPNeRF: Open Vocabulary 3D Neural Scene Segmentation with Superpoints","comments":"In Proceedings of the 20th International Joint Conference on Computer\n  Vision, Imaging and Computer Graphics Theory and Applications (2025)","journal-ref":null,"doi":"10.5220\/0013255100003912","report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Open-vocabulary segmentation, powered by large visual-language models like\nCLIP, has expanded 2D segmentation capabilities beyond fixed classes predefined\nby the dataset, enabling zero-shot understanding across diverse scenes.\nExtending these capabilities to 3D segmentation introduces challenges, as\nCLIP's image-based embeddings often lack the geometric detail necessary for 3D\nscene segmentation. Recent methods tend to address this by introducing\nadditional segmentation models or replacing CLIP with variations trained on\nsegmentation data, which lead to redundancy or loss on CLIP's general language\ncapabilities. To overcome this limitation, we introduce SPNeRF, a NeRF based\nzero-shot 3D segmentation approach that leverages geometric priors. We\nintegrate geometric primitives derived from the 3D scene into NeRF training to\nproduce primitive-wise CLIP features, avoiding the ambiguity of point-wise\nfeatures. Additionally, we propose a primitive-based merging mechanism enhanced\nwith affinity scores. Without relying on additional segmentation models, our\nmethod further explores CLIP's capability for 3D segmentation and achieves\nnotable improvements over original LERF.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 21:45:59 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Hu', 'Weiwen', ''], ['Parodi', 'Niccol\u00f2', ''], ['Zepp', 'Marcus', ''], ['Feldmann', 'Ingo', ''], ['Schreer', 'Oliver', ''], ['Eisert', 'Peter', '']]","extracted_entities":"[{'text': 'CLIP', 'label': 'Large Language Model'}, {'text': 'CLIP', 'label': 'Large Language Model'}, {'text': 'image-based embeddings', 'label': 'Embedding'}, {'text': 'CLIP', 'label': 'Large Language Model'}, {'text': 'CLIP', 'label': 'Large Language Model'}, {'text': 'CLIP', 'label': 'Large Language Model'}, {'text': 'CLIP', 'label': 'Large Language Model'}]","assigned_concept":"Embedding","matched_keyword":"image-based embeddings","similarity_score":0.7884131074}
{"id":2503.15798,"submitter":"Shibo Jie","authors":"Shibo Jie, Yehui Tang, Kai Han, Yitong Li, Duyu Tang, Zhi-Hong Deng,\n  Yunhe Wang","title":"Mixture of Lookup Experts","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/","abstract":"  Mixture-of-Experts (MoE) activates only a subset of experts during inference,\nallowing the model to maintain low inference FLOPs and latency even as the\nparameter count scales up. However, since MoE dynamically selects the experts,\nall the experts need to be loaded into VRAM. Their large parameter size still\nlimits deployment, and offloading, which load experts into VRAM only when\nneeded, significantly increase inference latency. To address this, we propose\nMixture of Lookup Experts (MoLE), a new MoE architecture that is efficient in\nboth communication and VRAM usage. In MoLE, the experts are Feed-Forward\nNetworks (FFNs) during training, taking the output of the embedding layer as\ninput. Before inference, these experts can be re-parameterized as lookup tables\n(LUTs) that retrieves expert outputs based on input ids, and offloaded to\nstorage devices. Therefore, we do not need to perform expert computations\nduring inference. Instead, we directly retrieve the expert's computation\nresults based on input ids and load them into VRAM, and thus the resulting\ncommunication overhead is negligible. Experiments show that, with the same\nFLOPs and VRAM usage, MoLE achieves inference speeds comparable to dense models\nand significantly faster than MoE with experts offloading, while maintaining\nperformance on par with MoE.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 02:31:57 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Jie', 'Shibo', ''], ['Tang', 'Yehui', ''], ['Han', 'Kai', ''], ['Li', 'Yitong', ''], ['Tang', 'Duyu', ''], ['Deng', 'Zhi-Hong', ''], ['Wang', 'Yunhe', '']]","extracted_entities":"[{'text': 'embedding layer', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"embedding layer","similarity_score":0.7880257368}
{"id":2503.15922,"submitter":"Fatima-Zahrae EL-BOUKKOURI","authors":"Fatima-Zahrae El-Boukkouri (INSA Toulouse, IMT), Josselin Garnier\n  (CMAP, ASCII), Olivier Roustant (INSA Toulouse, IMT, RT-UQ)","title":"General reproducing properties in RKHS with application to derivative\n  and integral operators","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.ST stat.ML stat.TH","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper, we generalize the reproducing property in Reproducing Kernel\nHilbert Spaces (RKHS). We establish a reproducing property for the closure of\nthe class of combinations of composition operators under minimal conditions. As\nan application, we improve the existing sufficient conditions for the\nreproducing property to hold for the derivative operator, as well as for the\nexistence of the mean embedding function. These results extend the scope of\napplicability of the representer theorem for regularized learning algorithms\nthat involve data for function values, gradients, or any other operator from\nthe considered class.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 07:58:09 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['El-Boukkouri', 'Fatima-Zahrae', '', 'INSA Toulouse, IMT'], ['Garnier', 'Josselin', '', 'CMAP, ASCII'], ['Roustant', 'Olivier', '', 'INSA Toulouse, IMT, RT-UQ']]","extracted_entities":"[{'text': 'mean embedding function', 'label': 'Embedding'}, {'text': 'regularized learning algorithms', 'label': 'Few-shot Learning'}]","assigned_concept":"Embedding","matched_keyword":"mean embedding function","similarity_score":0.6314435601}
{"id":2503.15958,"submitter":"Thomas Peyrat","authors":"Caroline Hillairet (CREST), Thomas Peyrat (CREST), Anthony R\\'eveillac\n  (INSA Toulouse, IMT, UT)","title":"Multivariate Self-Exciting Processes with Dependencies","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR q-fin.RM","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper introduces the class of multidimensional self-exciting processes\nwith dependencies (MSPD), which is a unifying writing for a large class of\nprocesses: counting, loss, intensity, and also shifted processes. The framework\ntakes into account dynamic dependencies between the frequency and the severity\ncomponents of the risk, and therefore induces theoretical challenges in the\ncomputations of risk valuations. We present a general method for calculating\ndifferent quantities related to these MSPDs, which combines the Poisson\nimbedding, the pseudo-chaotic expansion and Malliavin calculus. The methodology\nis illustrated for the computation of explicit general correlation formula.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:55:02 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Hillairet', 'Caroline', '', 'CREST'], ['Peyrat', 'Thomas', '', 'CREST'], ['R\u00e9veillac', 'Anthony', '', 'INSA Toulouse, IMT, UT']]","extracted_entities":"[{'text': 'Poisson\\nimbedding', 'label': 'Embedding'}, {'text': 'pseudo-chaotic expansion', 'label': 'Embedding'}, {'text': 'Malliavin calculus', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"Poisson\nimbedding","similarity_score":0.5439561605}
{"id":2503.16133,"submitter":"Chao Li","authors":"Lei Chen, Hao Li, Yuxin Zhang, Chao Li, and Kai Wen","title":"Multi-Prompt Style Interpolation for Fine-Grained Artistic Control","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Text-driven image style transfer has seen remarkable progress with methods\nleveraging cross-modal embeddings for fast, high-quality stylization. However,\nmost existing pipelines assume a \\emph{single} textual style prompt, limiting\nthe range of artistic control and expressiveness. In this paper, we propose a\nnovel \\emph{multi-prompt style interpolation} framework that extends the\nrecently introduced \\textbf{StyleMamba} approach. Our method supports blending\nor interpolating among multiple textual prompts (eg, ``cubism,''\n``impressionism,'' and ``cartoon''), allowing the creation of nuanced or hybrid\nartistic styles within a \\emph{single} image. We introduce a\n\\textit{Multi-Prompt Embedding Mixer} combined with \\textit{Adaptive Blending\nWeights} to enable fine-grained control over the spatial and semantic influence\nof each style. Further, we propose a \\emph{Hierarchical Masked Directional\nLoss} to refine region-specific style consistency. Experiments and user studies\nconfirm our approach outperforms single-prompt baselines and naive linear\ncombinations of styles, achieving superior style fidelity, text-image\nalignment, and artistic flexibility, all while maintaining the computational\nefficiency offered by the state-space formulation.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:29:32 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chen', 'Lei', ''], ['Li', 'Hao', ''], ['Zhang', 'Yuxin', ''], ['Li', 'Chao', ''], ['Wen', 'Kai', '']]","extracted_entities":"[{'text': 'cross-modal embeddings', 'label': 'Embedding'}, {'text': 'textual prompts', 'label': 'Prompting'}, {'text': 'fine-grained control', 'label': 'Fine-tuning'}]","assigned_concept":"Embedding","matched_keyword":"cross-modal embeddings","similarity_score":0.6585479379}
{"id":2503.16153,"submitter":"Tianyi Wei","authors":"Tianyi Wei, Yifan Zhou, Dongdong Chen, Xingang Pan","title":"FreeFlux: Understanding and Exploiting Layer-Specific Roles in\n  RoPE-Based MMDiT for Versatile Image Editing","comments":"Project page: https:\/\/wtybest.github.io\/projects\/FreeFlux\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The integration of Rotary Position Embedding (RoPE) in Multimodal Diffusion\nTransformer (MMDiT) has significantly enhanced text-to-image generation\nquality. However, the fundamental reliance of self-attention layers on\npositional embedding versus query-key similarity during generation remains an\nintriguing question. We present the first mechanistic analysis of RoPE-based\nMMDiT models (e.g., FLUX), introducing an automated probing strategy that\ndisentangles positional information versus content dependencies by\nstrategically manipulating RoPE during generation. Our analysis reveals\ndistinct dependency patterns that do not straightforwardly correlate with\ndepth, offering new insights into the layer-specific roles in RoPE-based MMDiT.\nBased on these findings, we propose a training-free, task-specific image\nediting framework that categorizes editing tasks into three types:\nposition-dependent editing (e.g., object addition), content\nsimilarity-dependent editing (e.g., non-rigid editing), and region-preserved\nediting (e.g., background replacement). For each type, we design tailored\nkey-value injection strategies based on the characteristics of the editing\ntask. Extensive qualitative and quantitative evaluations demonstrate that our\nmethod outperforms state-of-the-art approaches, particularly in preserving\noriginal semantic content and achieving seamless modifications.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:55:12 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wei', 'Tianyi', ''], ['Zhou', 'Yifan', ''], ['Chen', 'Dongdong', ''], ['Pan', 'Xingang', '']]","extracted_entities":"[{'text': 'Rotary Position Embedding', 'label': 'contextual Embedding'}, {'text': 'RoPE', 'label': 'contextual Embedding'}, {'text': 'self-attention layers', 'label': 'Attention mechanism'}, {'text': 'positional embedding', 'label': 'Embedding'}, {'text': 'RoPE', 'label': 'RoBERTa'}]","assigned_concept":"Embedding","matched_keyword":"positional embedding","similarity_score":0.7962335348}
{"id":2503.16159,"submitter":"Federico Berto","authors":"Jiwoo Son, Zhikai Zhao, Federico Berto, Chuanbo Hua, Changhyun Kwon,\n  Jinkyoo Park","title":"Neural Combinatorial Optimization for Real-World Routing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Vehicle Routing Problems (VRPs) are a class of NP-hard problems ubiquitous in\nseveral real-world logistics scenarios that pose significant challenges for\noptimization. Neural Combinatorial Optimization (NCO) has emerged as a\npromising alternative to classical approaches, as it can learn fast heuristics\nto solve VRPs. However, most research works in NCO for VRPs focus on simplified\nsettings, which do not account for asymmetric distances and travel durations\nthat cannot be derived by simple Euclidean distances and unrealistic data\ndistributions, hindering real-world deployment. This work introduces RRNCO\n(Real Routing NCO) to bridge the gap of NCO between synthetic and real-world\nVRPs in the critical aspects of both data and modeling. First, we introduce a\nnew, openly available dataset with real-world data containing a diverse dataset\nof locations, distances, and duration matrices from 100 cities, considering\nrealistic settings with actual routing distances and durations obtained from\nOpen Source Routing Machine (OSRM). Second, we propose a novel approach that\nefficiently processes both node and edge features through contextual gating,\nenabling the construction of more informed node embedding, and we finally\nincorporate an Adaptation Attention Free Module (AAFM) with neural adaptive\nbias mechanisms that effectively integrates not only distance matrices but also\nangular relationships between nodes, allowing our model to capture rich\nstructural information. RRNCO achieves state-of-the-art results in real-world\nVRPs among NCO methods. We make our dataset and code publicly available at\nhttps:\/\/github.com\/ai4co\/real-routing-nco.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:57:33 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Son', 'Jiwoo', ''], ['Zhao', 'Zhikai', ''], ['Berto', 'Federico', ''], ['Hua', 'Chuanbo', ''], ['Kwon', 'Changhyun', ''], ['Park', 'Jinkyoo', '']]","extracted_entities":"[{'text': 'contextual gating', 'label': 'contextual Embedding'}, {'text': 'node embedding', 'label': 'Embedding'}]","assigned_concept":"Embedding","matched_keyword":"node embedding","similarity_score":0.7871551514}
{"id":2106.05957,"submitter":"Andrew Ellis","authors":"Andrew Ellis and Heidi Christina Thysen","title":"Subjective Causality in Choice","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"econ.TH","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Choices based on observational data depend on beliefs about which\ncorrelations reflect causality. An agent predicts the consequence of available\nactions using a dataset and her subjective beliefs about causality represented\nby a directed acyclic graph (DAG). We identify her DAG from her random choice\nrule. Her choices reveal the chains of causal reasoning that she undertakes and\nthe confounding variables she adjusts for, and these pin down her model. When\nher choices generate the dataset used, her behavior affects her inferences,\nwhich in turn affect her choices. We provide necessary and sufficient\nconditions for testing whether her behavior is compatible with such a model.\n","versions":"[{'version': 'v1', 'created': 'Thu, 10 Jun 2021 17:54:00 GMT'}, {'version': 'v2', 'created': 'Thu, 21 Oct 2021 09:00:14 GMT'}, {'version': 'v3', 'created': 'Fri, 9 Dec 2022 16:24:07 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 09:56:05 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Ellis', 'Andrew', ''], ['Thysen', 'Heidi Christina', '']]","extracted_entities":"[{'text': 'chains of causal reasoning', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"chains of causal reasoning","similarity_score":0.5537554026}
{"id":2401.05787,"submitter":"Md Rizwan Parvez","authors":"Md Rizwan Parvez","title":"Chain of Evidences and Evidence to Generate: Prompting for Context\n  Grounded and Retrieval Augmented Reasoning","comments":"Accepted at NAACL KnowledgeNLP 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While chain-of-thoughts (CoT) prompting has revolutionized how LLMs perform\nreasoning tasks, its current methods and variations (e.g, Self-consistency,\nReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR) etc.,)\nsuffer from limitations like limited context grounding,\nhallucination\/inconsistent output generation, and iterative sluggishness. To\novercome these challenges, we introduce a novel mono\/dual-step zero-shot\nprompting framework built upon two unique strategies Chain of Evidences (CoE)}\nand Evidence to Generate (E2G). Instead of unverified reasoning claims, our\ninnovative approaches leverage the power of \"evidence for decision making\" by\nfirst focusing exclusively on the thought sequences explicitly mentioned in the\ncontext which then serve as extracted evidence, guiding the LLM's output\ngeneration process with greater precision and efficiency. This simple yet\npotent approach unlocks the full potential of chain-of-thoughts prompting,\nfacilitating faster, more reliable, and contextually aware reasoning in LLMs.\nOur framework consistently achieves remarkable results across various\nknowledge-intensive reasoning and generation tasks, surpassing baseline\napproaches with state-of-the-art LLMs. For instance, (i) on the LogiQA\nbenchmark using GPT-4, CoE achieves a new state-of-the-art accuracy of 53.8%,\nsurpassing CoT by 18%, ToT by 11%, and CR by 9%; (ii) CoE with PaLM-2\noutperforms the variable-shot performance of Gemini Ultra by 0.9 F1 points,\nachieving an F1 score of 83.3 on DROP. We release our prompts and outputs on\nthese benchmarks as a new instruction tuning dataset for future research at\nhttps:\/\/huggingface.co\/datasets\/kagnlp\/Chain-of-Evidences\/.\n","versions":"[{'version': 'v1', 'created': 'Thu, 11 Jan 2024 09:49:15 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 10:35:11 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Parvez', 'Md Rizwan', '']]","extracted_entities":"[{'text': 'chain-of-thoughts', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'Chain of Evidences', 'label': 'Chain of thought'}, {'text': 'chain-of-thoughts prompting', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'ToT', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"chain-of-thoughts","similarity_score":0.911457181}
{"id":2405.02536,"submitter":"James Murray Louw","authors":"Lyudmila Grigoryeva, James Louw, and Juan-Pablo Ortega","title":"Forecasting causal dynamics with universal reservoirs","comments":"37 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.DS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  An iterated multistep forecasting scheme based on recurrent neural networks\n(RNN) is proposed for the time series generated by causal chains with infinite\nmemory. This forecasting strategy contains, as a particular case, the iterative\nprediction strategies for dynamical systems that are customary in reservoir\ncomputing. Explicit error bounds are obtained as a function of the forecasting\nhorizon, functional and dynamical features of the specific RNN used, and the\napproximation error committed by it. In particular, the growth rate of the\nerror is shown to be exponential and controlled by the top Lyapunov exponent of\nthe proxy system. The framework in the paper circumvents difficult-to-verify\nembedding hypotheses that appear in previous references in the literature and\napplies to new situations like the finite-dimensional observations of\nfunctional differential equations or the deterministic parts of stochastic\nprocesses to which standard embedding techniques do not necessarily apply.\n","versions":"[{'version': 'v1', 'created': 'Sat, 4 May 2024 01:41:58 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 08:31:20 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Grigoryeva', 'Lyudmila', ''], ['Louw', 'James', ''], ['Ortega', 'Juan-Pablo', '']]","extracted_entities":"[{'text': 'causal chains', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"causal chains","similarity_score":0.5337221622}
{"id":2405.17418,"submitter":"Jiaming Liu","authors":"Chenxuan Li, Jiaming Liu, Guanqun Wang, Xiaoqi Li, Sixiang Chen, Liang\n  Heng, Chuyan Xiong, Jiaxin Ge, Renrui Zhang, Kaichen Zhou, Shanghang Zhang","title":"A Self-Correcting Vision-Language-Action Model for Fast and Slow System\n  Manipulation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recently, some studies have integrated Multimodal Large Language Models into\nrobotic manipulation, constructing vision-language-action models (VLAs) to\ninterpret multimodal information and predict SE(3) poses. While VLAs have shown\npromising progress, they may suffer from failures when faced with novel and\ncomplex tasks. To emulate human-like reasoning for more robust manipulation, we\npropose the self-corrected (SC-)VLA framework, which integrates fast system for\ndirectly predicting actions and slow system for reflecting on failed actions\nwithin a single VLA policy. For the fast system, we incorporate\nparameter-efficient fine-tuning to equip the model with pose prediction\ncapabilities while preserving the inherent reasoning abilities of MLLMs. For\nthe slow system, we propose a Chain-of-Thought training strategy for failure\ncorrection, designed to mimic human reflection after a manipulation failure.\nSpecifically, our model learns to identify the causes of action failures,\nadaptively seek expert feedback, reflect on the current failure scenario, and\niteratively generate corrective actions, step by step. Furthermore, a\ncontinuous policy learning method is designed based on successfully corrected\nsamples, enhancing the fast system's adaptability to the current configuration.\nWe compare SC-VLA with the previous SOTA VLA in both simulation and real-world\ntasks, demonstrating an efficient correction process and improved manipulation\naccuracy on both seen and unseen tasks.\n","versions":"[{'version': 'v1', 'created': 'Mon, 27 May 2024 17:58:48 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 03:55:48 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Li', 'Chenxuan', ''], ['Liu', 'Jiaming', ''], ['Wang', 'Guanqun', ''], ['Li', 'Xiaoqi', ''], ['Chen', 'Sixiang', ''], ['Heng', 'Liang', ''], ['Xiong', 'Chuyan', ''], ['Ge', 'Jiaxin', ''], ['Zhang', 'Renrui', ''], ['Zhou', 'Kaichen', ''], ['Zhang', 'Shanghang', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'parameter-efficient fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Chain-of-Thought', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain-of-Thought","similarity_score":0.9539169669}
{"id":2408.15045,"submitter":"Wenhui Liao","authors":"Wenhui Liao, Jiapeng Wang, Hongliang Li, Chengyu Wang, Jun Huang,\n  Lianwen Jin","title":"DocLayLLM: An Efficient Multi-modal Extension of Large Language Models\n  for Text-rich Document Understanding","comments":"CVPR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Text-rich document understanding (TDU) requires comprehensive analysis of\ndocuments containing substantial textual content and complex layouts. While\nMultimodal Large Language Models (MLLMs) have achieved fast progress in this\ndomain, existing approaches either demand significant computational resources\nor struggle with effective multi-modal integration. In this paper, we introduce\nDocLayLLM, an efficient multi-modal extension of LLMs specifically designed for\nTDU. By lightly integrating visual patch tokens and 2D positional tokens into\nLLMs' input and encoding the document content using the LLMs themselves, we\nfully take advantage of the document comprehension capability of LLMs and\nenhance their perception of OCR information. We have also deeply considered the\nrole of chain-of-thought (CoT) and innovatively proposed the techniques of CoT\nPre-training and CoT Annealing. Our DocLayLLM can achieve remarkable\nperformances with lightweight training settings, showcasing its efficiency and\neffectiveness. Experimental results demonstrate that our DocLayLLM outperforms\nexisting OCR-dependent methods and OCR-free competitors. Code and model are\navailable at https:\/\/github.com\/whlscut\/DocLayLLM.\n","versions":"[{'version': 'v1', 'created': 'Tue, 27 Aug 2024 13:13:38 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Aug 2024 08:32:44 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 10:05:04 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Liao', 'Wenhui', ''], ['Wang', 'Jiapeng', ''], ['Li', 'Hongliang', ''], ['Wang', 'Chengyu', ''], ['Huang', 'Jun', ''], ['Jin', 'Lianwen', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'chain-of-thought', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"chain-of-thought","similarity_score":0.9539169669}
{"id":2412.02172,"submitter":"Xueqing Wu","authors":"Xueqing Wu, Yuheng Ding, Bingxuan Li, Pan Lu, Da Yin, Kai-Wei Chang,\n  Nanyun Peng","title":"VISCO: Benchmarking Fine-Grained Critique and Correction Towards\n  Self-Improvement in Visual Reasoning","comments":"CVPR 2025. https:\/\/visco-benchmark.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The ability of large vision-language models (LVLMs) to critique and correct\ntheir reasoning is an essential building block towards their self-improvement.\nHowever, a systematic analysis of such capabilities in LVLMs is still lacking.\nWe propose VISCO, the first benchmark to extensively analyze the fine-grained\ncritique and correction capabilities of LVLMs. Compared to existing work that\nuses a single scalar value to critique the entire reasoning [4], VISCO features\ndense and fine-grained critique, requiring LVLMs to evaluate the correctness of\neach step in the chain-of-thought and provide natural language explanations to\nsupport their judgments. Extensive evaluation of 24 LVLMs demonstrates that\nhuman-written critiques significantly enhance the performance after correction,\nshowcasing the potential of the self-improvement strategy. However, the\nmodel-generated critiques are less helpful and sometimes detrimental to the\nperformance, suggesting that critique is the crucial bottleneck. We identified\nthree common patterns in critique failures: failure to critique visual\nperception, reluctance to \"say no\", and exaggerated assumption of error\npropagation. To address these issues, we propose an effective LookBack strategy\nthat revisits the image to verify each piece of information in the initial\nreasoning. LookBack significantly improves critique and correction performance\nby up to 13.5%.\n","versions":"[{'version': 'v1', 'created': 'Tue, 3 Dec 2024 05:04:49 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 08:02:22 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wu', 'Xueqing', ''], ['Ding', 'Yuheng', ''], ['Li', 'Bingxuan', ''], ['Lu', 'Pan', ''], ['Yin', 'Da', ''], ['Chang', 'Kai-Wei', ''], ['Peng', 'Nanyun', '']]","extracted_entities":"[{'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'chain-of-thought', 'label': 'Chain of thought'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]","assigned_concept":"Chain of thought","matched_keyword":"chain-of-thought","similarity_score":0.9539169669}
{"id":2503.08679,"submitter":"Iv\\'an Arcuschin","authors":"Iv\\'an Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran\n  Rajamanoharan, Neel Nanda, Arthur Conmy","title":"Chain-of-Thought Reasoning In The Wild Is Not Always Faithful","comments":"Accepted to the Reasoning and Planning for LLMs Workshop (ICLR 25),\n  10 main paper pages, 39 appendix pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art\nAI capabilities. However, recent studies have shown that CoT reasoning is not\nalways faithful, i.e. CoT reasoning does not always reflect how models arrive\nat conclusions. So far, most of these studies have focused on unfaithfulness in\nunnatural contexts where an explicit bias has been introduced. In contrast, we\nshow that unfaithful CoT can occur on realistic prompts with no artificial\nbias. Our results reveal non-negligible rates of several forms of unfaithful\nreasoning in frontier models: Sonnet 3.7 (16.3%), DeepSeek R1 (5.3%) and\nChatGPT-4o (7.0%) all answer a notable proportion of question pairs\nunfaithfully. Specifically, we find that models rationalize their implicit\nbiases in answers to binary questions (\"implicit post-hoc rationalization\").\nFor example, when separately presented with the questions \"Is X bigger than Y?\"\nand \"Is Y bigger than X?\", models sometimes produce superficially coherent\narguments to justify answering Yes to both questions or No to both questions,\ndespite such responses being logically contradictory. We also investigate\nrestoration errors (Dziri et al., 2023), where models make and then silently\ncorrect errors in their reasoning, and unfaithful shortcuts, where models use\nclearly illogical reasoning to simplify solving problems in Putnam questions (a\nhard benchmark). Our findings raise challenges for AI safety work that relies\non monitoring CoT to detect undesired behavior.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 17:56:30 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 17:49:58 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 19:20:42 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Arcuschin', 'Iv\u00e1n', ''], ['Janiak', 'Jett', ''], ['Krzyzanowski', 'Robert', ''], ['Rajamanoharan', 'Senthooran', ''], ['Nanda', 'Neel', ''], ['Conmy', 'Arthur', '']]","extracted_entities":"[{'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'CoT reasoning', 'label': 'Chain of thought'}, {'text': 'CoT reasoning', 'label': 'Chain of thought'}, {'text': 'CoT', 'label': 'Chain of thought'}, {'text': 'realistic prompts', 'label': 'Prompting'}, {'text': 'ChatGPT-4o', 'label': 'ChatGPT'}, {'text': 'models', 'label': 'AI model'}, {'text': 'models', 'label': 'AI model'}, {'text': 'models', 'label': 'AI model'}, {'text': 'AI safety work', 'label': 'AI Ethics'}, {'text': 'CoT', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain-of-Thought","similarity_score":0.9539169669}
{"id":2503.10177,"submitter":"Yirong Sun","authors":"Yirong Sun, Yanjun Chen","title":"PRISM: Preference Refinement via Implicit Scene Modeling for 3D\n  Vision-Language Preference-Based Reinforcement Learning","comments":"I withdraw arXiv:2503.10177 due to critical computational errors\n  invalidating its conclusions and the withdrawal of consent from co-author\n  Yanjun Chen","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We propose PRISM, a novel framework designed to overcome the limitations of\n2D-based Preference-Based Reinforcement Learning (PBRL) by unifying 3D point\ncloud modeling and future-aware preference refinement. At its core, PRISM\nadopts a 3D Point Cloud-Language Model (3D-PC-LLM) to mitigate occlusion and\nviewpoint biases, ensuring more stable and spatially consistent preference\nsignals. Additionally, PRISM leverages Chain-of-Thought (CoT) reasoning to\nincorporate long-horizon considerations, thereby preventing the short-sighted\nfeedback often seen in static preference comparisons. In contrast to\nconventional PBRL techniques, this integration of 3D perception and\nfuture-oriented reasoning leads to significant gains in preference agreement\nrates, faster policy convergence, and robust generalization across unseen\nrobotic environments. Our empirical results, spanning tasks such as robotic\nmanipulation and autonomous navigation, highlight PRISM's potential for\nreal-world applications where precise spatial understanding and reliable\nlong-term decision-making are critical. By bridging 3D geometric awareness with\nCoT-driven preference modeling, PRISM establishes a comprehensive foundation\nfor scalable, human-aligned reinforcement learning.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 08:58:10 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 06:22:21 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Sun', 'Yirong', ''], ['Chen', 'Yanjun', '']]","extracted_entities":"[{'text': 'PRISM', 'label': 'Foundation Model'}, {'text': 'PRISM', 'label': 'Foundation Model'}, {'text': 'PRISM', 'label': 'Foundation Model'}, {'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'PRISM', 'label': 'Foundation Model'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain-of-Thought","similarity_score":0.9539169669}
{"id":2503.11989,"submitter":"Dharani Chandra","authors":"Dharani Chandra","title":"Applications of Large Language Model Reasoning in Feature Generation","comments":"I just updated the format of the references in the paper","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) have revolutionized natural language processing\nthrough their state of art reasoning capabilities. This paper explores the\nconvergence of LLM reasoning techniques and feature generation for machine\nlearning tasks. We examine four key reasoning approaches: Chain of Thought,\nTree of Thoughts, Retrieval-Augmented Generation, and Thought Space\nExploration. Our analysis reveals how these approaches can be used to identify\neffective feature generation rules without having to manually specify search\nspaces. The paper categorizes LLM-based feature generation methods across\nvarious domains including finance, healthcare, and text analytics. LLMs can\nextract key information from clinical notes and radiology reports in\nhealthcare, by enabling more efficient data utilization. In finance, LLMs\nfacilitate text generation, summarization, and entity extraction from complex\ndocuments. We analyze evaluation methodologies for assessing feature quality\nand downstream performance, with particular attention to OCTree's decision tree\nreasoning approach that provides language-based feedback for iterative\nimprovements. Current challenges include hallucination, computational\nefficiency, and domain adaptation. As of March 2025, emerging approaches\ninclude inference-time compute scaling, reinforcement learning, and supervised\nfine-tuning with model distillation. Future directions point toward multimodal\nfeature generation, self-improving systems, and neuro-symbolic approaches. This\npaper provides a detailed overview of an emerging field that promises to\nautomate and enhance feature engineering through language model reasoning.\n","versions":"[{'version': 'v1', 'created': 'Sat, 15 Mar 2025 04:18:01 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 02:18:33 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chandra', 'Dharani', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Chain of Thought', 'label': 'Chain of thought'}, {'text': 'Tree of Thoughts', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'inference-time compute scaling', 'label': 'Scaling law'}, {'text': 'reinforcement learning', 'label': 'Knowledge distillation'}, {'text': 'supervised\\nfine-tuning', 'label': 'Fine-tuning'}, {'text': 'model distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain of Thought","similarity_score":0.9999998808}
{"id":2503.12303,"submitter":"Xiaoying Zhang","authors":"Xiaoying Zhang, Da Peng, Yipeng Zhang, Zonghao Guo, Chengyue Wu, Chi\n  Chen, Wei Ke, Helen Meng, Maosong Sun","title":"Towards Self-Improving Systematic Cognition for Next-Generation\n  Foundation MLLMs","comments":"38 pages. Preprint, work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Despite their impressive capabilities, Multimodal Large Language Models\n(MLLMs) face challenges with fine-grained perception and complex reasoning.\nPrevalent multimodal pre-training approaches focus on enhancing perception by\ntraining on high-quality image captions due to the extremely high cost of\ncollecting chain-of-thought (CoT) reasoning data for improving reasoning. While\nleveraging advanced MLLMs for caption generation enhances scalability, the\noutputs often lack comprehensiveness and accuracy. In this paper, we introduce\nSelf-Improving cognition (SIcog), a self-learning framework designed to\nconstruct next-generation foundation MLLMs by enhancing their systematic\ncognitive capabilities through multimodal pre-training with self-generated\ndata. Specifically, we propose Chain-of-Description, an approach that improves\nan MLLM's systematic perception by enabling step-by-step visual understanding,\nensuring greater comprehensiveness and accuracy. Additionally, we adopt a\nstructured CoT reasoning technique to enable MLLMs to integrate in-depth\nmultimodal reasoning. To construct a next-generation foundation MLLM with\nself-improved cognition, SIcog first equips an MLLM with systematic perception\nand reasoning abilities using minimal external annotations. The enhanced models\nthen generate detailed captions and CoT reasoning data, which are further\ncurated through self-consistency. This curated data is ultimately used for\nmultimodal pre-training to develop next-generation foundation models. Extensive\nexperiments on both low- and high-resolution MLLMs across diverse benchmarks\ndemonstrate that, with merely 213K self-generated pre-training samples, SIcog\nproduces next-generation foundation MLLMs with significantly improved\ncognition, achieving benchmark-leading performance compared to prevalent\npre-training approaches.\n","versions":"[{'version': 'v1', 'created': 'Sun, 16 Mar 2025 00:25:13 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 13:42:31 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 12:22:00 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhang', 'Xiaoying', ''], ['Peng', 'Da', ''], ['Zhang', 'Yipeng', ''], ['Guo', 'Zonghao', ''], ['Wu', 'Chengyue', ''], ['Chen', 'Chi', ''], ['Ke', 'Wei', ''], ['Meng', 'Helen', ''], ['Sun', 'Maosong', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'chain-of-thought', 'label': 'Chain of thought'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'scalability', 'label': 'Scaling law'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLM', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Chain of thought","matched_keyword":"chain-of-thought","similarity_score":0.9539169669}
{"id":2503.12524,"submitter":"Jinsik Lee","authors":"LG AI Research, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley\n  Jungkyu Choi, Yemuk Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Kijeong\n  Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Hyosang Kim, Joonkee Kim,\n  Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul\n  Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee,\n  Sangha Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, Sihyuk Yi, Hyeongu Yun","title":"EXAONE Deep: Reasoning Enhanced Language Models","comments":"arXiv admin note: substantial text overlap with arXiv:2412.04862,\n  arXiv:2408.03541","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  We present EXAONE Deep series, which exhibits superior capabilities in\nvarious reasoning tasks, including math and coding benchmarks. We train our\nmodels mainly on the reasoning-specialized dataset that incorporates long\nstreams of thought processes. Evaluation results show that our smaller models,\nEXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while\nthe largest model, EXAONE Deep 32B, demonstrates competitive performance\nagainst leading open-weight models. All EXAONE Deep models are openly available\nfor research purposes and can be downloaded from\nhttps:\/\/huggingface.co\/LGAI-EXAONE\n","versions":"[{'version': 'v1', 'created': 'Sun, 16 Mar 2025 14:39:33 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 07:09:24 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Research', 'LG AI', ''], ['Bae', 'Kyunghoon', ''], ['Choi', 'Eunbi', ''], ['Choi', 'Kibong', ''], ['Choi', 'Stanley Jungkyu', ''], ['Choi', 'Yemuk', ''], ['Hong', 'Seokhee', ''], ['Hwang', 'Junwon', ''], ['Jeon', 'Hyojin', ''], ['Jeon', 'Kijeong', ''], ['Jo', 'Gerrard Jeongwon', ''], ['Jo', 'Hyunjik', ''], ['Jung', 'Jiyeon', ''], ['Kim', 'Hyosang', ''], ['Kim', 'Joonkee', ''], ['Kim', 'Seonghwan', ''], ['Kim', 'Soyeon', ''], ['Kim', 'Sunkyoung', ''], ['Kim', 'Yireun', ''], ['Kim', 'Yongil', ''], ['Kim', 'Youchul', ''], ['Lee', 'Edward Hwayoung', ''], ['Lee', 'Haeju', ''], ['Lee', 'Honglak', ''], ['Lee', 'Jinsik', ''], ['Lee', 'Kyungmin', ''], ['Park', 'Sangha', ''], ['Park', 'Yongmin', ''], ['Yang', 'Sihoon', ''], ['Yeen', 'Heuiyeen', ''], ['Yi', 'Sihyuk', ''], ['Yun', 'Hyeongu', '']]","extracted_entities":"[{'text': 'long\\nstreams of thought processes', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"long\nstreams of thought processes","similarity_score":0.5854611397}
{"id":2503.12721,"submitter":"Luca Collini","authors":"Luca Collini, Andrew Hennessee, Ramesh Karri, Siddharth Garg","title":"Can Reasoning Models Reason about Hardware? An Agentic HLS Perspective","comments":"7 pages, submitted for peer review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent Large Language Models (LLMs) such as OpenAI o3-mini and DeepSeek-R1\nuse enhanced reasoning through Chain-of-Thought (CoT). Their potential in\nhardware design, which relies on expert-driven iterative optimization, remains\nunexplored. This paper investigates whether reasoning LLMs can address\nchallenges in High-Level Synthesis (HLS) design space exploration and\noptimization. During HLS, engineers manually define pragmas\/directives to\nbalance performance and resource constraints. We propose an LLM-based\noptimization agentic framework that automatically restructures code, inserts\npragmas, and identifies optimal design points via feedback from HLs tools and\naccess to integer-linear programming (ILP) solvers. Experiments compare\nreasoning models against conventional LLMs on benchmarks using success rate,\nefficiency, and design quality (area\/latency) metrics, and provide the\nfirst-ever glimpse into the CoTs produced by a powerful open-source reasoning\nmodel like DeepSeek-R1.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 01:21:39 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Collini', 'Luca', ''], ['Hennessee', 'Andrew', ''], ['Karri', 'Ramesh', ''], ['Garg', 'Siddharth', '']]","extracted_entities":"[{'text': 'Chain-of-Thought', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain-of-Thought","similarity_score":0.9539169669}
{"id":2503.12799,"submitter":"Qiong Wu","authors":"Qiong Wu, Xiangcong Yang, Yiyi Zhou, Chenxin Fang, Baiyang Song,\n  Xiaoshuai Sun, Rongrong Ji","title":"Grounded Chain-of-Thought for Multimodal Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.MM","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Despite great progress, existing multimodal large language models (MLLMs) are\nprone to visual hallucination, greatly impeding their trustworthy applications.\nIn this paper, we study this problem from the perspective of visual-spatial\nreasoning, and propose a new learning task for MLLMs, termed Grounded\nChain-of-Thought (GCoT). Different from recent visual CoT studies, which focus\nmore on visual knowledge reasoning, GCoT is keen to helping MLLMs to recognize\nand ground the relevant visual cues step by step, thereby predicting the\ncorrect answer with grounding coordinates as the intuitive basis. To facilitate\nthis task, we also carefully design and construct a dataset called multimodal\ngrounded chain-of-thought (MM-GCoT) consisting of 24,022 GCoT examples for\n5,033 images. Besides, a comprehensive consistency evaluation system is also\nintroduced, including the metrics of answer accuracy, grounding accuracy and\nanswer-grounding consistency. We further design and conduct a bunch of\nexperiments on 12 advanced MLLMs, and reveal some notable findings: i. most\nMLLMs performs poorly on the consistency evaluation, indicating obvious visual\nhallucination; ii. visual hallucination is not directly related to the\nparameter size and general multimodal performance, i.e., a larger and stronger\nMLLM is not less affected by this issue. Lastly, we also demonstrate that the\nproposed dataset can help existing MLLMs to well cultivate their GCoT\ncapability and reduce the inconsistent answering significantly. Moreover, their\nGCoT can be also generalized to exiting multimodal tasks, such as open-world QA\nand REC.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 04:07:47 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wu', 'Qiong', ''], ['Yang', 'Xiangcong', ''], ['Zhou', 'Yiyi', ''], ['Fang', 'Chenxin', ''], ['Song', 'Baiyang', ''], ['Sun', 'Xiaoshuai', ''], ['Ji', 'Rongrong', '']]","extracted_entities":"[{'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'Grounded\\nChain-of-Thought', 'label': 'Chain of thought'}, {'text': 'GCoT', 'label': 'Chain of thought'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'grounded chain-of-thought', 'label': 'Chain of thought'}, {'text': 'GCoT', 'label': 'Chain of thought'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'GCoT', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"Grounded\nChain-of-Thought","similarity_score":0.6417593956}
{"id":2503.12937,"submitter":"Jingyi Zhang","authors":"Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang,\n  Shijian Lu, Dacheng Tao","title":"R1-VL: Learning to Reason with Multimodal Large Language Models via\n  Step-wise Group Relative Policy Optimization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.CV cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent studies generally enhance MLLMs' reasoning capabilities via supervised\nfine-tuning on high-quality chain-of-thought reasoning data, which often leads\nmodels to merely imitate successful reasoning paths without understanding what\nthe wrong reasoning paths are. In this work, we aim to enhance the MLLMs'\nreasoning ability beyond passively imitating positive reasoning paths. To this\nend, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new\nonline reinforcement learning framework that enables MLLMs to self-improve\nreasoning ability via simple, effective and dense step-wise rewarding.\nSpecifically, StepGRPO introduces two novel rule-based reasoning rewards:\nStep-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity\nReward (StepRVR). StepRAR rewards the reasoning paths that contain necessary\nintermediate reasoning steps via a soft key-step matching technique, while\nStepRAR rewards reasoning paths that follow a well-structured and logically\nconsistent reasoning process through a reasoning completeness and logic\nevaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series\nof MLLMs with outstanding capabilities in step-by-step reasoning. Extensive\nexperiments over 8 benchmarks demonstrate the superiority of our methods.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 08:51:44 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Zhang', 'Jingyi', ''], ['Huang', 'Jiaxing', ''], ['Yao', 'Huanjin', ''], ['Liu', 'Shunyu', ''], ['Zhang', 'Xikun', ''], ['Lu', 'Shijian', ''], ['Tao', 'Dacheng', '']]","extracted_entities":"[{'text': 'supervised\\nfine-tuning', 'label': 'Fine-tuning'}, {'text': 'chain-of-thought', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"chain-of-thought","similarity_score":0.9539169669}
{"id":2503.13055,"submitter":"Yu-Hong Shen","authors":"Yu-Hong Shen, Chuan-Yu Wu, Yi-Ru Yang, Yen-Ling Tai, Yi-Ting Chen","title":"Mitigating Cross-Modal Distraction and Ensuring Geometric Feasibility\n  via Affordance-Guided, Self-Consistent MLLMs for Food Preparation Task\n  Planning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We study Multimodal Large Language Models (MLLMs) with in-context learning\nfor food preparation task planning. In this context, we identify two key\nchallenges: cross-modal distraction and geometric feasibility. Cross-modal\ndistraction occurs when the inclusion of visual input degrades the reasoning\nperformance of a MLLM. Geometric feasibility refers to the ability of MLLMs to\nensure that the selected skills are physically executable in the environment.\nTo address these issues, we adapt Chain of Thought (CoT) with Self-Consistency\nto mitigate reasoning loss from cross-modal distractions and use affordance\npredictor as skill preconditions to guide MLLM on geometric feasibility. We\nconstruct a dataset to evaluate the ability of MLLMs on quantity estimation,\nreachability analysis, relative positioning and collision avoidance. We\nconducted a detailed evaluation to identify issues among different baselines\nand analyze the reasons for improvement, providing insights into each approach.\nOur method reaches a success rate of 76.7% on the entire dataset, showing a\nsubstantial improvement over the CoT baseline at 36.7%.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 11:01:02 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Shen', 'Yu-Hong', ''], ['Wu', 'Chuan-Yu', ''], ['Yang', 'Yi-Ru', ''], ['Tai', 'Yen-Ling', ''], ['Chen', 'Yi-Ting', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLM', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'Chain of Thought', 'label': 'Chain of thought'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain of Thought","similarity_score":0.9999998808}
{"id":2503.13184,"submitter":"Shihao Yuan","authors":"Yuanze Li, Shihao Yuan, Haolin Wang, Qizhang Li, Ming Liu, Chen Xu,\n  Guangming Shi, Wangmeng Zuo","title":"Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided\n  Visual Tokenizer and Manufacturing Process","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Although recent methods have tried to introduce large multimodal models\n(LMMs) into industrial anomaly detection (IAD), their generalization in the IAD\nfield is far inferior to that for general purposes. We summarize the main\nreasons for this gap into two aspects. On one hand, general-purpose LMMs lack\ncognition of defects in the visual modality, thereby failing to sufficiently\nfocus on defect areas. Therefore, we propose to modify the AnyRes structure of\nthe LLaVA model, providing the potential anomalous areas identified by existing\nIAD models to the LMMs. On the other hand, existing methods mainly focus on\nidentifying defects by learning defect patterns or comparing with normal\nsamples, yet they fall short of understanding the causes of these defects.\nConsidering that the generation of defects is closely related to the\nmanufacturing process, we propose a manufacturing-driven IAD paradigm. An\ninstruction-tuning dataset for IAD (InstructIAD) and a data organization\napproach for Chain-of-Thought with manufacturing (CoT-M) are designed to\nleverage the manufacturing process for IAD. Based on the above two\nmodifications, we present Triad, a novel LMM-based method incorporating an\nexpert-guided region-of-interest tokenizer and manufacturing process for\nindustrial anomaly detection. Extensive experiments show that our Triad not\nonly demonstrates competitive performance against current LMMs but also\nachieves further improved accuracy when equipped with manufacturing processes.\nSource code, training data, and pre-trained models will be publicly available\nat https:\/\/github.com\/tzjtatata\/Triad.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 13:56:57 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Li', 'Yuanze', ''], ['Yuan', 'Shihao', ''], ['Wang', 'Haolin', ''], ['Li', 'Qizhang', ''], ['Liu', 'Ming', ''], ['Xu', 'Chen', ''], ['Shi', 'Guangming', ''], ['Zuo', 'Wangmeng', '']]","extracted_entities":"[{'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'Chain-of-Thought with manufacturing', 'label': 'Chain of thought'}, {'text': 'LMMs', 'label': 'Large Language Model'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain-of-Thought with manufacturing","similarity_score":0.6713610888}
{"id":2503.1336,"submitter":"Hai-Long Sun","authors":"Hai-Long Sun, Zhun Sun, Houwen Peng, Han-Jia Ye","title":"Mitigating Visual Forgetting via Take-along Visual Conditioning for\n  Multi-modal Long CoT Reasoning","comments":"The project page is available at\n  https:\/\/sun-hailong.github.io\/projects\/TVC","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advancements in Large Language Models (LLMs) have demonstrated\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\nto advanced, product-oriented solutions like OpenAI o1. During our\nre-implementation of this model, we noticed that in multimodal tasks requiring\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\nmaintain focus on the visual information, in other words, MLLMs suffer from a\ngradual decline in attention to visual information as reasoning progresses,\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\nduring long-chain reasoning. Concretely, we truncate the reasoning process\nmidway, then re-complete the reasoning process with the input image removed. We\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\nmodel's textual outputs dominate the following reasoning process. Motivated by\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\nimage input to critical reasoning stages and compresses redundant visual tokens\nvia dynamic pruning. This methodology helps the model retain attention to the\nvisual components throughout the reasoning. Our approach achieves\nstate-of-the-art performance on average across five mathematical reasoning\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\nenhancing multimodal reasoning systems.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 16:45:12 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Sun', 'Hai-Long', ''], ['Sun', 'Zhun', ''], ['Peng', 'Houwen', ''], ['Ye', 'Han-Jia', '']]","extracted_entities":"[{'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'long-chain reasoning', 'label': 'Chain of thought'}, {'text': 'attention', 'label': 'Attention mechanism'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain-of-Thought","similarity_score":0.9539169669}
{"id":2503.13399,"submitter":"James Burgess","authors":"James Burgess, Jeffrey J Nirschl, Laura Bravo-S\\'anchez, Alejandro\n  Lozano, Sanket Rajan Gupte, Jesus G. Galaz-Montoya, Yuhui Zhang, Yuchang Su,\n  Disha Bhowmik, Zachary Coman, Sarina M. Hasan, Alexandra Johannesson, William\n  D. Leineweber, Malvika G Nair, Ridhi Yarlagadda, Connor Zuraski, Wah Chiu,\n  Sarah Cohen, Jan N. Hansen, Manuel D Leonetti, Chad Liu, Emma Lundberg,\n  Serena Yeung-Levy","title":"MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research","comments":"CVPR 2025 (Conference on Computer Vision and Pattern Recognition)\n  Project page at https:\/\/jmhb0.github.io\/microvqa Benchmark at\n  https:\/\/huggingface.co\/datasets\/jmhb\/microvqa","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL cs.LG q-bio.CB","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https:\/\/huggingface.co\/datasets\/jmhb\/microvqa, and project page at\nhttps:\/\/jmhb0.github.io\/microvqa.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:33:10 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Burgess', 'James', ''], ['Nirschl', 'Jeffrey J', ''], ['Bravo-S\u00e1nchez', 'Laura', ''], ['Lozano', 'Alejandro', ''], ['Gupte', 'Sanket Rajan', ''], ['Galaz-Montoya', 'Jesus G.', ''], ['Zhang', 'Yuhui', ''], ['Su', 'Yuchang', ''], ['Bhowmik', 'Disha', ''], ['Coman', 'Zachary', ''], ['Hasan', 'Sarina M.', ''], ['Johannesson', 'Alexandra', ''], ['Leineweber', 'William D.', ''], ['Nair', 'Malvika G', ''], ['Yarlagadda', 'Ridhi', ''], ['Zuraski', 'Connor', ''], ['Chiu', 'Wah', ''], ['Cohen', 'Sarah', ''], ['Hansen', 'Jan N.', ''], ['Leonetti', 'Manuel D', ''], ['Liu', 'Chad', ''], ['Lundberg', 'Emma', ''], ['Yeung-Levy', 'Serena', '']]","extracted_entities":"[{'text': 'multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'optimized LLM prompt', 'label': 'Prompting'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'chain-of-thought responses', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"chain-of-thought responses","similarity_score":0.7817511559}
{"id":2503.13988,"submitter":"Mykyta Syromiatnikov","authors":"Mykyta Syromiatnikov, Victoria Ruvinskaya, Nataliia Komleva","title":"Empowering Smaller Models: Tuning LLaMA and Gemma with Chain-of-Thought\n  for Ukrainian Exam Tasks","comments":"12 pages, 6 tables, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Leading large language models have demonstrated impressive capabilities in\nreasoning-intensive tasks, such as standardized educational testing. However,\nthey often require extensive training in low-resource settings with\ninaccessible infrastructure. Small or compact models, though more efficient,\nfrequently lack sufficient support for underrepresented languages, leaving a\nperformance gap in critical domains. This work explores the potential of\nparameter-efficient fine-tuning of compact open-weight language models to\nhandle reasoning-intensive tasks in the underrepresented Ukrainian language,\nbuilding on the findings of the ZNO-Eval benchmark. Parameter-efficient\nfine-tuning of LLaMA 3.1 (8 billion parameters), LLaMA 3.2 (3 billion\nparameters), and Gemma 2 (9 billion parameters) models on chain-of-thought\nsolutions resulted in a modest test score improvement of up to 17.4% on complex\nmatching tasks and 1.6% overall compared to tuning on answer letters alone,\noffering enhanced interpretability and robustness. In addition, the proposed\ntuning method with joint task topic and step-by-step solution generation\noutperforms standard chain-of-thought tuning in matching tasks and provides a\n5.4% gain over the best LLaMA 3.2 model due to guiding the model to recall and\napply domain-relevant information. Contrasting obtained results with zero-shot\nevaluations of leading open-weight and proprietary models such as Qwen,\nDeepSeek R1, OpenAI o1 and o3, Gemini, and Claude, highlight that fine-tuning\nLLaMA and Gemma models with 2,032 step-by-step solutions and 20 to 50 million\ntrainable parameters on a single A100 GPU lets them outperform GPT-4o mini,\nMistral Large, and larger open-weight models. This research also evaluates how\nmerging the quantized adapter with the base model influences the generation\nquality. Source code and tuned models are available at\nhttps:\/\/github.com\/NLPForUA\/ZNO.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 07:44:49 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Syromiatnikov', 'Mykyta', ''], ['Ruvinskaya', 'Victoria', ''], ['Komleva', 'Nataliia', '']]","extracted_entities":"[{'text': 'parameter-efficient fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Parameter-efficient\\nfine-tuning', 'label': 'Fine-tuning'}, {'text': 'LLaMA 3.2', 'label': 'Large Language Model'}, {'text': 'chain-of-thought\\nsolutions', 'label': 'Chain of thought'}, {'text': 'chain-of-thought', 'label': 'Chain of thought'}, {'text': 'LLaMA 3.2', 'label': 'Large Language Model'}, {'text': 'Mistral Large', 'label': 'Mistral'}]","assigned_concept":"Chain of thought","matched_keyword":"chain-of-thought","similarity_score":0.9539169669}
{"id":2503.14337,"submitter":"Chenxiao Yang","authors":"Chenxiao Yang, Nathan Srebro, David McAllester, Zhiyuan Li","title":"PENCIL: Long Thoughts with Short Memory","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  While recent works (e.g. o1, DeepSeek R1) have demonstrated great promise of\nusing long Chain-of-Thought (CoT) to improve reasoning capabilities of language\nmodels, scaling it up during test-time is challenging due to inefficient memory\nusage -- intermediate computations accumulate indefinitely in context even no\nlonger needed for future thoughts. We propose PENCIL, which incorporates a\nreduction mechanism into the autoregressive generation process, allowing the\nmodel to recursively clean up intermediate thoughts based on patterns learned\nfrom training. With this reduction mechanism, PENCIL significantly reduces the\nmaximal context length required during generation, and thus can generate longer\nthoughts with limited memory, solving larger-scale problems given more thinking\ntime. For example, we demonstrate PENCIL achieves 97\\% accuracy on the\nchallenging Einstein's puzzle -- a task even large models like GPT-4 struggle\nwith -- using only a small 25M-parameter transformer with 2048 context length.\nTheoretically, we prove PENCIL can perform universal space-efficient\ncomputation by simulating Turing machines with optimal time and space\ncomplexity, and thus can solve arbitrary computational tasks that would\notherwise be intractable given context window constraints.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 15:14:14 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Yang', 'Chenxiao', ''], ['Srebro', 'Nathan', ''], ['McAllester', 'David', ''], ['Li', 'Zhiyuan', '']]","extracted_entities":"[{'text': 'long Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'PENCIL', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'PENCIL', 'label': 'Generative Pre-trained Transformer (GPT)'}]","assigned_concept":"Chain of thought","matched_keyword":"long Chain-of-Thought","similarity_score":0.8942457438}
{"id":2503.15235,"submitter":"Chentian Wei","authors":"Chentian Wei, Jiewei Chen, Jinzhu Xu","title":"Exploring Large Language Models for Word Games:Who is the Spy?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Word games hold significant research value for natural language processing\n(NLP), game theory, and related fields due to their rule-based and situational\nnature. This study explores how large language models (LLMs) can be effectively\ninvolved in word games and proposes a training-free framework. \"Shei Shi Wo Di\"\nor \"Who is the Spy\" in English, is a classic word game. Using this game as an\nexample, we introduce a Chain-of-Thought (CoT)-based scheduling framework to\nenable LLMs to achieve excellent performance in tasks such as inferring role\nwords and disguising their identities. We evaluate the framework's performance\nbased on game success rates and the accuracy of the LLM agents' analytical\nresults. Experimental results affirm the framework's effectiveness,\ndemonstrating notable improvements in LLM performance across multiple datasets.\nThis work highlights the potential of LLMs in mastering situational reasoning\nand social interactions within structured game environments. Our code is\npublicly available at https:\/\/github.com\/ct-wei\/Who-is-The-Spy.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 14:13:02 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Wei', 'Chentian', ''], ['Chen', 'Jiewei', ''], ['Xu', 'Jinzhu', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Who is the Spy', 'label': 'Large Language Model'}, {'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain-of-Thought","similarity_score":0.9539169669}
{"id":2503.15268,"submitter":"Roberto Araya","authors":"Roberto Araya","title":"Do Chains-of-Thoughts of Large Language Models Suffer from\n  Hallucinations, Cognitive Biases, or Phobias in Bayesian Reasoning?","comments":"24 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Learning to reason and carefully explain arguments is central to students'\ncognitive, mathematical, and computational thinking development. This is\nparticularly challenging in problems under uncertainty and in Bayesian\nreasoning. With the new generation of large language models (LLMs) capable of\nreasoning using Chain-of-Thought (CoT), there is an excellent opportunity to\nlearn with them as they explain their reasoning through a dialogue with their\nartificial internal voice. It is an engaging and excellent opportunity to learn\nBayesian reasoning. Furthermore, given that different LLMs sometimes arrive at\nopposite solutions, CoT generates opportunities for deep learning by detailed\ncomparisons of reasonings. However, unlike humans, we found that they do not\nautonomously explain using ecologically valid strategies like natural\nfrequencies, whole objects, and embodied heuristics. This is unfortunate, as\nthese strategies help humans avoid critical mistakes and have proven\npedagogical value in Bayesian reasoning. In order to overcome these biases and\naid understanding and learning, we included prompts that induce LLMs to use\nthese strategies. We found that LLMs with CoT incorporate them but not\nconsistently. They show persistent biases towards symbolic reasoning and\navoidance or phobia of ecologically valid strategies.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 14:44:02 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Araya', 'Roberto', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'CoT', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'CoT', 'label': 'Chain of thought'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'CoT', 'label': 'Chain of thought'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain-of-Thought","similarity_score":0.9539169669}
{"id":2503.15558,"submitter":"Yin Cui","authors":"NVIDIA: Alisson Azzolini, Hannah Brandon, Prithvijit Chattopadhyay,\n  Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, Francesco Ferroni,\n  Rama Govindaraju, Jinwei Gu, Siddharth Gururani, Imad El Hanafi, Zekun Hao,\n  Jacob Huffman, Jingyi Jin, Brendan Johnson, Rizwan Khan, George Kurian, Elena\n  Lantz, Nayeon Lee, Zhaoshuo Li, Xuan Li, Tsung-Yi Lin, Yen-Chen Lin, Ming-Yu\n  Liu, Andrew Mathau, Yun Ni, Lindsey Pavao, Wei Ping, David W. Romero, Misha\n  Smelyanskiy, Shuran Song, Lyne Tchapmi, Andrew Z. Wang, Boxin Wang, Haoxiang\n  Wang, Fangyin Wei, Jiashu Xu, Yao Xu, Xiaodong Yang, Zhuolin Yang, Xiaohui\n  Zeng, Zhe Zhang","title":"Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CV cs.LG cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Physical AI systems need to perceive, understand, and perform complex actions\nin the physical world. In this paper, we present the Cosmos-Reason1 models that\ncan understand the physical world and generate appropriate embodied decisions\n(e.g., next step action) in natural language through long chain-of-thought\nreasoning processes. We begin by defining key capabilities for Physical AI\nreasoning, with a focus on physical common sense and embodied reasoning. To\nrepresent physical common sense, we use a hierarchical ontology that captures\nfundamental knowledge about space, time, and physics. For embodied reasoning,\nwe rely on a two-dimensional ontology that generalizes across different\nphysical embodiments. Building on these capabilities, we develop two multimodal\nlarge language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data\nand train our models in four stages: vision pre-training, general supervised\nfine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL)\nas the post-training. To evaluate our models, we build comprehensive benchmarks\nfor physical common sense and embodied reasoning according to our ontologies.\nEvaluation results show that Physical AI SFT and reinforcement learning bring\nsignificant improvements. To facilitate the development of Physical AI, we will\nmake our code and pre-trained models available under the NVIDIA Open Model\nLicense at https:\/\/github.com\/nvidia-cosmos\/cosmos-reason1.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 22:06:58 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['NVIDIA', '', ''], [':', '', ''], ['Azzolini', 'Alisson', ''], ['Brandon', 'Hannah', ''], ['Chattopadhyay', 'Prithvijit', ''], ['Chen', 'Huayu', ''], ['Chu', 'Jinju', ''], ['Cui', 'Yin', ''], ['Diamond', 'Jenna', ''], ['Ding', 'Yifan', ''], ['Ferroni', 'Francesco', ''], ['Govindaraju', 'Rama', ''], ['Gu', 'Jinwei', ''], ['Gururani', 'Siddharth', ''], ['Hanafi', 'Imad El', ''], ['Hao', 'Zekun', ''], ['Huffman', 'Jacob', ''], ['Jin', 'Jingyi', ''], ['Johnson', 'Brendan', ''], ['Khan', 'Rizwan', ''], ['Kurian', 'George', ''], ['Lantz', 'Elena', ''], ['Lee', 'Nayeon', ''], ['Li', 'Zhaoshuo', ''], ['Li', 'Xuan', ''], ['Lin', 'Tsung-Yi', ''], ['Lin', 'Yen-Chen', ''], ['Liu', 'Ming-Yu', ''], ['Mathau', 'Andrew', ''], ['Ni', 'Yun', ''], ['Pavao', 'Lindsey', ''], ['Ping', 'Wei', ''], ['Romero', 'David W.', ''], ['Smelyanskiy', 'Misha', ''], ['Song', 'Shuran', ''], ['Tchapmi', 'Lyne', ''], ['Wang', 'Andrew Z.', ''], ['Wang', 'Boxin', ''], ['Wang', 'Haoxiang', ''], ['Wei', 'Fangyin', ''], ['Xu', 'Jiashu', ''], ['Xu', 'Yao', ''], ['Yang', 'Xiaodong', ''], ['Yang', 'Zhuolin', ''], ['Zeng', 'Xiaohui', ''], ['Zhang', 'Zhe', '']]","extracted_entities":"[{'text': 'long chain-of-thought', 'label': 'Chain of thought'}, {'text': 'physical common sense', 'label': 'Chain of thought'}, {'text': 'physical common sense', 'label': 'Chain of thought'}, {'text': 'vision pre-training', 'label': 'Fine-tuning'}, {'text': 'general supervised\\nfine-tuning (SFT)', 'label': 'Fine-tuning'}, {'text': 'Physical AI SFT', 'label': 'Fine-tuning'}, {'text': 'Physical AI reinforcement learning (RL)', 'label': 'Few-shot Learning'}, {'text': 'physical common sense', 'label': 'Chain of thought'}, {'text': 'Physical AI SFT', 'label': 'Fine-tuning'}]","assigned_concept":"Chain of thought","matched_keyword":"long chain-of-thought","similarity_score":0.8942457438}
{"id":2503.16013,"submitter":"Xiaomeng Chu","authors":"Xiaomeng Chu, Jiajun Deng, Guoliang You, Wei Liu, Xingchen Li, Jianmin\n  Ji, Yanyong Zhang","title":"GraspCoT: Integrating Physical Property Reasoning for 6-DoF Grasping\n  under Flexible Language Instructions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Flexible instruction-guided 6-DoF grasping is a significant yet challenging\ntask for real-world robotic systems. Existing methods utilize the contextual\nunderstanding capabilities of the large language models (LLMs) to establish\nmappings between expressions and targets, allowing robots to comprehend users'\nintentions in the instructions. However, the LLM's knowledge about objects'\nphysical properties remains underexplored despite its tight relevance to\ngrasping. In this work, we propose GraspCoT, a 6-DoF grasp detection framework\nthat integrates a Chain-of-Thought (CoT) reasoning mechanism oriented to\nphysical properties, guided by auxiliary question-answering (QA) tasks.\nParticularly, we design a set of QA templates to enable hierarchical reasoning\nthat includes three stages: target parsing, physical property analysis, and\ngrasp action selection. Moreover, GraspCoT presents a unified multimodal LLM\narchitecture, which encodes multi-view observations of 3D scenes into 3D-aware\nvisual tokens, and then jointly embeds these visual tokens with CoT-derived\ntextual tokens within LLMs to generate grasp pose predictions. Furthermore, we\npresent IntentGrasp, a large-scale benchmark that fills the gap in public\ndatasets for multi-object grasp detection under diverse and indirect verbal\ncommands. Extensive experiments on IntentGrasp demonstrate the superiority of\nour method, with additional validation in real-world robotic applications\nconfirming its practicality. Codes and data will be released.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 10:32:38 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chu', 'Xiaomeng', ''], ['Deng', 'Jiajun', ''], ['You', 'Guoliang', ''], ['Liu', 'Wei', ''], ['Li', 'Xingchen', ''], ['Ji', 'Jianmin', ''], ['Zhang', 'Yanyong', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'public\\ndatasets', 'label': 'Open-source LLMs'}]","assigned_concept":"Chain of thought","matched_keyword":"Chain-of-Thought","similarity_score":0.9539169669}
{"id":2404.19597,"submitter":"Xuanli He","authors":"Xuanli He, Jun Wang, Qiongkai Xu, Pasquale Minervini, Pontus\n  Stenetorp, Benjamin I. P. Rubinstein, Trevor Cohn","title":"TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with\n  Instruction Tuning","comments":"work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The implications of backdoor attacks on English-centric large language models\n(LLMs) have been widely examined - such attacks can be achieved by embedding\nmalicious behaviors during training and activated under specific conditions\nthat trigger malicious outputs. Despite the increasing support for multilingual\ncapabilities in open-source and proprietary LLMs, the impact of backdoor\nattacks on these systems remains largely under-explored. Our research focuses\non cross-lingual backdoor attacks against multilingual LLMs, particularly\ninvestigating how poisoning the instruction-tuning data for one or two\nlanguages can affect the outputs for languages whose instruction-tuning data\nwere not poisoned. Despite its simplicity, our empirical analysis reveals that\nour method exhibits remarkable efficacy in models like mT5 and GPT-4o, with\nhigh attack success rates, surpassing 90% in more than 7 out of 12 languages\nacross various scenarios. Our findings also indicate that more powerful models\nshow increased susceptibility to transferable cross-lingual backdoor attacks,\nwhich also applies to LLMs predominantly pre-trained on English data, such as\nLlama2, Llama3, and Gemma. Moreover, our experiments demonstrate 1) High\nTransferability: the backdoor mechanism operates successfully in cross-lingual\nresponse scenarios across 26 languages, achieving an average attack success\nrate of 99%, and 2) Robustness: the proposed attack remains effective even\nafter defenses are applied. These findings expose critical security\nvulnerabilities in multilingual LLMs and highlight the urgent need for more\nrobust, targeted defense strategies to address the unique challenges posed by\ncross-lingual backdoor transfer.\n","versions":"[{'version': 'v1', 'created': 'Tue, 30 Apr 2024 14:43:57 GMT'}, {'version': 'v2', 'created': 'Wed, 2 Oct 2024 15:47:40 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 10:09:29 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['He', 'Xuanli', ''], ['Wang', 'Jun', ''], ['Xu', 'Qiongkai', ''], ['Minervini', 'Pasquale', ''], ['Stenetorp', 'Pontus', ''], ['Rubinstein', 'Benjamin I. P.', ''], ['Cohn', 'Trevor', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GPT-4o', 'label': 'GPT-2'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Llama2', 'label': 'Llama'}, {'text': 'Llama3', 'label': 'Llama'}, {'text': 'Gemma', 'label': 'Llama'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Llama","matched_keyword":"Llama2","similarity_score":0.8272922635}
{"id":2406.09891,"submitter":"Adish Singla","authors":"Victor-Alexandru P\\u{a}durean, Adish Singla","title":"Benchmarking Generative Models on Computational Thinking Tests in\n  Elementary Visual Programming","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Generative models have demonstrated human-level proficiency in various\nbenchmarks across domains like programming, natural sciences, and general\nknowledge. Despite these promising results on competitive benchmarks, they\nstill struggle with seemingly simple problem-solving tasks typically carried\nout by elementary-level students. How do state-of-the-art models perform on\nstandardized programming-related tests designed to assess computational\nthinking and problem-solving skills at schools? In this paper, we curate a\nnovel benchmark involving computational thinking tests grounded in elementary\nvisual programming domains. Our initial results show that state-of-the-art\nmodels like GPT-4o and Llama3 barely match the performance of an average school\nstudent. To further boost the performance of these models, we fine-tune them\nusing a novel synthetic data generation methodology. The key idea is to develop\na comprehensive dataset using symbolic methods that capture different skill\nlevels, ranging from recognition of visual elements to multi-choice quizzes to\nsynthesis-style tasks. We showcase how various aspects of symbolic information\nin synthetic data help improve fine-tuned models' performance. We will release\nthe full implementation and datasets to facilitate further research on\nenhancing computational thinking in generative models.\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Jun 2024 10:02:52 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 13:03:15 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['P\u0103durean', 'Victor-Alexandru', ''], ['Singla', 'Adish', '']]","extracted_entities":"[{'text': 'Llama3', 'label': 'Llama'}]","assigned_concept":"Llama","matched_keyword":"Llama3","similarity_score":0.8217295408}
{"id":2410.21637,"submitter":"Rafael Rivera Soto","authors":"Rafael Rivera Soto, Barry Chen, Nicholas Andrews","title":"Mitigating Paraphrase Attacks on Machine-Text Detectors via Paraphrase\n  Inversion","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  High-quality paraphrases are easy to produce using instruction-tuned language\nmodels or specialized paraphrasing models. Although this capability has a\nvariety of benign applications, paraphrasing\nattacks$\\unicode{x2013}$paraphrases applied to machine-generated\ntexts$\\unicode{x2013}$are known to significantly degrade the performance of\nmachine-text detectors. This motivates us to consider the novel problem of\nparaphrase inversion, where, given paraphrased text, the objective is to\nrecover an approximation of the original text. The closer the approximation is\nto the original text, the better machine-text detectors will perform. We\npropose an approach which frames the problem as translation from paraphrased\ntext back to the original text, which requires examples of texts and\ncorresponding paraphrases to train the inversion model. Fortunately, such\ntraining data can easily be generated, given a corpus of original texts and one\nor more paraphrasing models. We find that language models such as GPT-4 and\nLlama-3 exhibit biases when paraphrasing which an inversion model can learn\nwith a modest amount of data. Perhaps surprisingly, we also find that such\nmodels generalize well, including to paraphrase models unseen at training time.\nFinally, we show that when combined with a paraphrased-text detector, our\ninversion models provide an effective defense against paraphrasing attacks, and\noverall our approach yields an average improvement of +22% AUROC across seven\nmachine-text detectors and three different domains.\n","versions":"[{'version': 'v1', 'created': 'Tue, 29 Oct 2024 00:46:24 GMT'}, {'version': 'v2', 'created': 'Sat, 1 Mar 2025 00:12:48 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 21:32:41 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Soto', 'Rafael Rivera', ''], ['Chen', 'Barry', ''], ['Andrews', 'Nicholas', '']]","extracted_entities":"[{'text': 'GPT-4', 'label': 'Llama'}, {'text': 'Llama-3', 'label': 'Llama'}]","assigned_concept":"Llama","matched_keyword":"Llama-3","similarity_score":0.7882506847}
{"id":2411.1673,"submitter":"Libo Wang","authors":"Libo Wang","title":"\"Moralized\" Multi-Step Jailbreak Prompts: Black-Box Testing of\n  Guardrails in Large Language Models for Verbal Attacks","comments":"This paper has been submitted to Nature Machine Intelligence and\n  OpenReview preprints. It has 7 pages of text, 3 figures, and 3 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  As the application of large language models continues to expand in various\nfields, it poses higher challenges to the effectiveness of identifying harmful\ncontent generation and guardrail mechanisms. This research aims to evaluate the\nguardrail effectiveness of GPT-4o, Grok-2 Beta, Llama 3.1 (405B), Gemini 1.5,\nand Claude 3.5 Sonnet through black-box testing of seemingly ethical multi-step\njailbreak prompts. It conducts ethical attacks by designing an identical\nmulti-step prompts that simulates the scenario of \"corporate middle managers\ncompeting for promotions.\" The data results show that the guardrails of the\nabove-mentioned LLMs were bypassed and the content of verbal attacks was\ngenerated. Claude 3.5 Sonnet's resistance to multi-step jailbreak prompts is\nmore obvious. To ensure objectivity, the experimental process, black box test\ncode, and enhanced guardrail code are uploaded to the GitHub repository:\nhttps:\/\/github.com\/brucewang123456789\/GeniusTrail.git.\n","versions":"[{'version': 'v1', 'created': 'Sat, 23 Nov 2024 09:32:44 GMT'}, {'version': 'v2', 'created': 'Wed, 27 Nov 2024 20:49:44 GMT'}, {'version': 'v3', 'created': 'Wed, 4 Dec 2024 08:21:17 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 14:48:10 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wang', 'Libo', '']]","extracted_entities":"[{'text': 'Llama 3.1 (405B)', 'label': 'Llama'}, {'text': 'multi-step\\njailbreak prompts', 'label': 'Prompting'}, {'text': 'ethical attacks', 'label': 'AI Ethics'}, {'text': 'multi-step prompts', 'label': 'Prompting'}, {'text': 'multi-step jailbreak prompts', 'label': 'Prompting'}]","assigned_concept":"Llama","matched_keyword":"Llama 3.1 (405B)","similarity_score":0.7624900341}
{"id":2411.17595,"submitter":"Lun Yu","authors":"Shuyi Jin, Lu Chen, Hongru Ding, Meijie Wang, Lun Yu","title":"Can artificial intelligence predict clinical trial outcomes?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.AP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This study evaluates the performance of large language models (LLMs) and the\nHINT model in predicting clinical trial outcomes, focusing on metrics including\nBalanced Accuracy, Matthews Correlation Coefficient (MCC), Recall, and\nSpecificity. Results show that GPT-4o achieves superior overall performance\namong LLMs but, like its counterparts (GPT-3.5, GPT-4mini, Llama3), struggles\nwith identifying negative outcomes. In contrast, HINT excels in negative sample\nrecognition and demonstrates resilience to external factors (e.g., recruitment\nchallenges) but underperforms in oncology trials, a major component of the\ndataset. LLMs exhibit strengths in early-phase trials and simpler endpoints\nlike Overall Survival (OS), while HINT shows consistency across trial phases\nand excels in complex endpoints (e.g., Objective Response Rate). Trial duration\nanalysis reveals improved model performance for medium- to long-term trials,\nwith GPT-4o and HINT displaying stability and enhanced specificity,\nrespectively. We underscore the complementary potential of LLMs (e.g., GPT-4o,\nLlama3) and HINT, advocating for hybrid approaches to leverage GPT-4o's\npredictive power and HINT's specificity in clinical trial outcome forecasting.\n","versions":"[{'version': 'v1', 'created': 'Tue, 26 Nov 2024 17:05:27 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 00:45:44 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Jin', 'Shuyi', ''], ['Chen', 'Lu', ''], ['Ding', 'Hongru', ''], ['Wang', 'Meijie', ''], ['Yu', 'Lun', '']]","extracted_entities":"[{'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'Llama3', 'label': 'Llama'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'Llama3', 'label': 'Llama'}]","assigned_concept":"Llama","matched_keyword":"Llama3","similarity_score":0.8217295408}
{"id":2503.12511,"submitter":"Tianyang Zhou","authors":"Tianyang Zhou, Haowen Lin, Somesh Jha, Mihai Christodorescu, Kirill\n  Levchenko, Varun Chandrasekaran","title":"LLM-Driven Multi-step Translation from C to Rust using Static Analysis","comments":"22 pages, 13 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI cs.PL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Translating software written in legacy languages to modern languages, such as\nC to Rust, has significant benefits in improving memory safety while\nmaintaining high performance. However, manual translation is cumbersome,\nerror-prone, and produces unidiomatic code. Large language models (LLMs) have\ndemonstrated promise in producing idiomatic translations, but offer no\ncorrectness guarantees as they lack the ability to capture all the semantics\ndifferences between the source and target languages. To resolve this issue, we\npropose SACTOR, an LLM-driven C-to-Rust zero-shot translation tool using a\ntwo-step translation methodology: an \"unidiomatic\" step to translate C into\nRust while preserving semantics, and an \"idiomatic\" step to refine the code to\nfollow Rust's semantic standards. SACTOR utilizes information provided by\nstatic analysis of the source C program to address challenges such as pointer\nsemantics and dependency resolution. To validate the correctness of the\ntranslated result from each step, we use end-to-end testing via the foreign\nfunction interface to embed our translated code segment into the original code.\nWe evaluate the translation of 200 programs from two datasets and two case\nstudies, comparing the performance of GPT-4o, Claude 3.5 Sonnet, Gemini 2.0\nFlash, Llama 3.3 70B and DeepSeek-R1 in SACTOR. Our results demonstrate that\nSACTOR achieves high correctness and improved idiomaticity, with the\nbest-performing model (DeepSeek-R1) reaching 93% and (GPT-4o, Claude 3.5,\nDeepSeek-R1) reaching 84% correctness (on each dataset, respectively), while\nproducing more natural and Rust-compliant translations compared to existing\nmethods.\n","versions":"[{'version': 'v1', 'created': 'Sun, 16 Mar 2025 14:05:26 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 04:17:27 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Zhou', 'Tianyang', ''], ['Lin', 'Haowen', ''], ['Jha', 'Somesh', ''], ['Christodorescu', 'Mihai', ''], ['Levchenko', 'Kirill', ''], ['Chandrasekaran', 'Varun', '']]","extracted_entities":"[{'text': 'Llama 3.3 70B', 'label': 'Llama'}]","assigned_concept":"Llama","matched_keyword":"Llama 3.3 70B","similarity_score":0.6648871899}
{"id":2503.1339,"submitter":"Andreas Waldis","authors":"Andreas Waldis, Vagrant Gautam, Anne Lauscher, Dietrich Klakow, Iryna\n  Gurevych","title":"Aligned Probing: Relating Toxic Behavior and Model Internals","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  We introduce aligned probing, a novel interpretability framework that aligns\nthe behavior of language models (LMs), based on their outputs, and their\ninternal representations (internals). Using this framework, we examine over 20\nOLMo, Llama, and Mistral models, bridging behavioral and internal perspectives\nfor toxicity for the first time. Our results show that LMs strongly encode\ninformation about the toxicity level of inputs and subsequent outputs,\nparticularly in lower layers. Focusing on how unique LMs differ offers both\ncorrelative and causal evidence that they generate less toxic output when\nstrongly encoding information about the input toxicity. We also highlight the\nheterogeneity of toxicity, as model behavior and internals vary across unique\nattributes such as Threat. Finally, four case studies analyzing detoxification,\nmulti-prompt evaluations, model quantization, and pre-training dynamics\nunderline the practical impact of aligned probing with further concrete\ninsights. Our findings contribute to a more holistic understanding of LMs, both\nwithin and beyond the context of toxicity.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:23:50 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Waldis', 'Andreas', ''], ['Gautam', 'Vagrant', ''], ['Lauscher', 'Anne', ''], ['Klakow', 'Dietrich', ''], ['Gurevych', 'Iryna', '']]","extracted_entities":"[{'text': 'Llama', 'label': 'Llama'}, {'text': 'Mistral', 'label': 'Mistral'}, {'text': 'multi-prompt evaluations', 'label': 'Prompting'}, {'text': 'model quantization', 'label': 'quantisation'}]","assigned_concept":"Llama","matched_keyword":"Llama","similarity_score":1.0}
{"id":2503.13772,"submitter":"Bowen Cui","authors":"Bowen Cui, Tejas Ramesh, Oscar Hernandez, Keren Zhou","title":"Do Large Language Models Understand Performance Optimization?","comments":"First two authors have equal contributions","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DC cs.SE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) have emerged as powerful tools for software\ndevelopment tasks such as code completion, translation, and optimization.\nHowever, their ability to generate efficient and correct code, particularly in\ncomplex High-Performance Computing (HPC) contexts, has remained underexplored.\nTo address this gap, this paper presents a comprehensive benchmark suite\nencompassing multiple critical HPC computational motifs to evaluate the\nperformance of code optimized by state-of-the-art LLMs, including OpenAI o1,\nClaude-3.5, and Llama-3.2. In addition to analyzing basic computational\nkernels, we developed an agent system that integrates LLMs to assess their\neffectiveness in real HPC applications. Our evaluation focused on key criteria\nsuch as execution time, correctness, and understanding of HPC-specific\nconcepts. We also compared the results with those achieved using traditional\nHPC optimization tools. Based on the findings, we recognized the strengths of\nLLMs in understanding human instructions and performing automated code\ntransformations. However, we also identified significant limitations, including\ntheir tendency to generate incorrect code and their challenges in comprehending\ncomplex control and data flows in sophisticated HPC code.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 23:30:23 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Cui', 'Bowen', ''], ['Ramesh', 'Tejas', ''], ['Hernandez', 'Oscar', ''], ['Zhou', 'Keren', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Llama-3.2', 'label': 'Llama'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Llama","matched_keyword":"Llama-3.2","similarity_score":0.7459247112}
{"id":2503.13992,"submitter":"Ori Yoran","authors":"Ori Yoran, Kunhao Zheng, Fabian Gloeckle, Jonas Gehring, Gabriel\n  Synnaeve, Taco Cohen","title":"The KoLMogorov Test: Compression by Code Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Compression is at the heart of intelligence. A theoretically optimal way to\ncompress any sequence of data is to find the shortest program that outputs that\nsequence and then halts. However, such 'Kolmogorov compression' is\nuncomputable, and code generating LLMs struggle to approximate this theoretical\nideal, as it requires reasoning, planning and search capabilities beyond those\nof current models. In this work, we introduce the KoLMogorov-Test (KT), a\ncompression-as-intelligence test for code generating LLMs. In KT a model is\npresented with a sequence of data at inference time, and asked to generate the\nshortest program that produces the sequence. We identify several benefits of KT\nfor both evaluation and training: an essentially infinite number of problem\ninstances of varying difficulty is readily available, strong baselines already\nexist, the evaluation metric (compression) cannot be gamed, and pretraining\ndata contamination is highly unlikely. To evaluate current models, we use\naudio, text, and DNA data, as well as sequences produced by random synthetic\nprograms. Current flagship models perform poorly - both GPT4-o and\nLlama-3.1-405B struggle on our natural and synthetic sequences. On our\nsynthetic distribution, we are able to train code generation models with lower\ncompression rates than previous approaches. Moreover, we show that gains on\nsynthetic data generalize poorly to real data, suggesting that new innovations\nare necessary for additional gains on KT.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 07:52:04 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Yoran', 'Ori', ''], ['Zheng', 'Kunhao', ''], ['Gloeckle', 'Fabian', ''], ['Gehring', 'Jonas', ''], ['Synnaeve', 'Gabriel', ''], ['Cohen', 'Taco', '']]","extracted_entities":"[{'text': 'Llama-3', 'label': 'Llama'}]","assigned_concept":"Llama","matched_keyword":"Llama-3","similarity_score":0.7882506847}
{"id":2503.14201,"submitter":"Alberto Martin-Lopez","authors":"Alessandro Giagnorio, Alberto Martin-Lopez, Gabriele Bavota","title":"Why Personalizing Deep Learning-Based Code Completion Tools Matters","comments":"Accepted for publication at ACM TOSEM","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Deep learning (DL)-based code completion tools have transformed software\ndevelopment by enabling advanced code generation. These tools leverage models\ntrained on vast amounts of code from numerous repositories, capturing general\ncoding patterns. However, the impact of fine-tuning these models for specific\norganizations or developers to boost their performance on such subjects remains\nunexplored. In this work, we fill this gap by presenting solid empirical\nevidence answering this question. More specifically, we consider 136 developers\nfrom two organizations (Apache and Spring), two model architectures (T5 and\nCode Llama), and three model sizes (60M, 750M, and 7B trainable parameters). T5\nmodels (60M, 750M) were pre-trained and fine-tuned on over 2,000 open-source\nprojects, excluding the subject organizations' data, and compared against\nversions fine-tuned on organization- and developer-specific datasets. For the\nCode Llama model (7B), we compared the performance of the already pre-trained\nmodel publicly available online with the same model fine-tuned via\nparameter-efficient fine-tuning on organization- and developer-specific\ndatasets. Our results show that there is a boost in prediction capabilities\nprovided by both an organization-specific and a developer-specific additional\nfine-tuning, with the former being particularly performant. Such a finding\ngeneralizes across (i) the two subject organizations (i.e., Apache and Spring)\nand (ii) models of completely different magnitude (from 60M to 7B trainable\nparameters). Finally, we show that DL models fine-tuned on an\norganization-specific dataset achieve the same completion performance of\npre-trained code models used out of the box and being $\\sim$10$\\times$ larger,\nwith consequent savings in terms of deployment and inference cost (e.g.,\nsmaller GPUs needed).\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 12:26:06 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Giagnorio', 'Alessandro', ''], ['Martin-Lopez', 'Alberto', ''], ['Bavota', 'Gabriele', '']]","extracted_entities":"[{'text': 'Code Llama', 'label': 'Llama'}, {'text': 'Code Llama', 'label': 'Llama'}, {'text': 'parameter-efficient fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Llama","matched_keyword":"Code Llama","similarity_score":0.8264381886}
{"id":2503.14708,"submitter":"Lucy Revina","authors":"Viansa Schmulbach, Jason Kim, Ethan Gao, Lucy Revina, Nikhil Jha,\n  Ethan Wu, Borivoje Nikolic","title":"NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16","comments":null,"journal-ref":null,"doi":"10.1109\/HCS61935.2024.10665203","report-no":null,"categories":"cs.AR","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs\/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 20:16:50 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Schmulbach', 'Viansa', ''], ['Kim', 'Jason', ''], ['Gao', 'Ethan', ''], ['Revina', 'Lucy', ''], ['Jha', 'Nikhil', ''], ['Wu', 'Ethan', ''], ['Nikolic', 'Borivoje', '']]","extracted_entities":"[{'text': 'ReLU-Llama', 'label': 'Llama'}]","assigned_concept":"Llama","matched_keyword":"ReLU-Llama","similarity_score":0.7521086335}
{"id":2503.15754,"submitter":"Andy Zhou","authors":"Andy Zhou, Kevin Wu, Francesco Pinto, Zhaorun Chen, Yi Zeng, Yu Yang,\n  Shuang Yang, Sanmi Koyejo, James Zou, Bo Li","title":"AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  As large language models (LLMs) become increasingly capable, security and\nsafety evaluation are crucial. While current red teaming approaches have made\nstrides in assessing LLM vulnerabilities, they often rely heavily on human\ninput and lack comprehensive coverage of emerging attack vectors. This paper\nintroduces AutoRedTeamer, a novel framework for fully automated, end-to-end red\nteaming against LLMs. AutoRedTeamer combines a multi-agent architecture with a\nmemory-guided attack selection mechanism to enable continuous discovery and\nintegration of new attack vectors. The dual-agent framework consists of a red\nteaming agent that can operate from high-level risk categories alone to\ngenerate and execute test cases and a strategy proposer agent that autonomously\ndiscovers and implements new attacks by analyzing recent research. This modular\ndesign allows AutoRedTeamer to adapt to emerging threats while maintaining\nstrong performance on existing attack vectors. We demonstrate AutoRedTeamer's\neffectiveness across diverse evaluation settings, achieving 20% higher attack\nsuccess rates on HarmBench against Llama-3.1-70B while reducing computational\ncosts by 46% compared to existing approaches. AutoRedTeamer also matches the\ndiversity of human-curated benchmarks in generating test cases, providing a\ncomprehensive, scalable, and continuously evolving framework for evaluating the\nsecurity of AI systems.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 00:13:04 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zhou', 'Andy', ''], ['Wu', 'Kevin', ''], ['Pinto', 'Francesco', ''], ['Chen', 'Zhaorun', ''], ['Zeng', 'Yi', ''], ['Yang', 'Yu', ''], ['Yang', 'Shuang', ''], ['Koyejo', 'Sanmi', ''], ['Zou', 'James', ''], ['Li', 'Bo', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Llama-3.1-70B', 'label': 'Llama'}]","assigned_concept":"Llama","matched_keyword":"Llama-3.1-70B","similarity_score":0.6696484685}
{"id":2503.15866,"submitter":"Vinod Puthuvath","authors":"Dincy R Arikkat, Vinod P., Rafidha Rehiman K. A., Serena Nicolazzo,\n  Marco Arazzi, Antonino Nocera, Mauro Conti","title":"DroidTTP: Mapping Android Applications with TTP for Cyber Threat\n  Intelligence","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The widespread adoption of Android devices for sensitive operations like\nbanking and communication has made them prime targets for cyber threats,\nparticularly Advanced Persistent Threats (APT) and sophisticated malware\nattacks. Traditional malware detection methods rely on binary classification,\nfailing to provide insights into adversarial Tactics, Techniques, and\nProcedures (TTPs). Understanding malware behavior is crucial for enhancing\ncybersecurity defenses. To address this gap, we introduce DroidTTP, a framework\nmapping Android malware behaviors to TTPs based on the MITRE ATT&CK framework.\nOur curated dataset explicitly links MITRE TTPs to Android applications. We\ndeveloped an automated solution leveraging the Problem Transformation Approach\n(PTA) and Large Language Models (LLMs) to map applications to both Tactics and\nTechniques. Additionally, we employed Retrieval-Augmented Generation (RAG) with\nprompt engineering and LLM fine-tuning for TTP predictions. Our structured\npipeline includes dataset creation, hyperparameter tuning, data augmentation,\nfeature selection, model development, and SHAP-based model interpretability.\nAmong LLMs, Llama achieved the highest performance in Tactic classification\nwith a Jaccard Similarity of 0.9583 and Hamming Loss of 0.0182, and in\nTechnique classification with a Jaccard Similarity of 0.9348 and Hamming Loss\nof 0.0127. However, the Label Powerset XGBoost model outperformed LLMs,\nachieving a Jaccard Similarity of 0.9893 for Tactic classification and 0.9753\nfor Technique classification, with a Hamming Loss of 0.0054 and 0.0050,\nrespectively. While XGBoost showed superior performance, the narrow margin\nhighlights the potential of LLM-based approaches in TTP classification.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 05:38:24 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Arikkat', 'Dincy R', ''], ['P.', 'Vinod', ''], ['A.', 'Rafidha Rehiman K.', ''], ['Nicolazzo', 'Serena', ''], ['Arazzi', 'Marco', ''], ['Nocera', 'Antonino', ''], ['Conti', 'Mauro', '']]","extracted_entities":"[{'text': 'prompt engineering', 'label': 'Prompting'}, {'text': 'hyperparameter tuning', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Llama', 'label': 'Llama'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Llama","matched_keyword":"Llama","similarity_score":1.0}
{"id":2503.15969,"submitter":"Benedikt Blumenstiel","authors":"Clive Tinashe Marimo, Benedikt Blumenstiel, Maximilian Nitsche,\n  Johannes Jakubik, Thomas Brunschwiler","title":"Beyond the Visible: Multispectral Vision-Language Learning for Earth\n  Observation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Vision-language models for Earth observation (EO) typically rely on the\nvisual spectrum of data as the only model input, thus failing to leverage the\nrich spectral information available in the multispectral channels recorded by\nsatellites. Therefore, in this paper, we introduce Llama3-MS-CLIP, the first\nvision-language model pre-trained with contrastive learning on a large-scale\nmultispectral dataset and report on the performance gains due to the extended\nspectral range. Furthermore, we present the largest-to-date image-caption\ndataset for multispectral data, consisting of one million Sentinel-2 samples\nand corresponding textual descriptions generated with Llama3-LLaVA-Next and\nOverture Maps data. We develop a scalable captioning pipeline, which is\nvalidated by domain experts. We evaluate Llama3-MS-CLIP on multispectral\nzero-shot image classification and retrieval using three datasets of varying\ncomplexity. Our results demonstrate that Llama3-MS-CLIP significantly\noutperforms other RGB-based approaches, improving classification accuracy by\n6.77% on average and retrieval performance by 4.63% mAP compared to the\nsecond-best model. Our results emphasize the relevance of multispectral\nvision-language learning. We release the image-caption dataset, code, and model\nweights under an open-source license.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 09:13:31 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Marimo', 'Clive Tinashe', ''], ['Blumenstiel', 'Benedikt', ''], ['Nitsche', 'Maximilian', ''], ['Jakubik', 'Johannes', ''], ['Brunschwiler', 'Thomas', '']]","extracted_entities":"[{'text': 'Llama3-MS-CLIP', 'label': 'Llama'}, {'text': 'contrastive learning', 'label': 'Few-shot Learning'}, {'text': 'Llama3-LLaVA-Next', 'label': 'Llama'}, {'text': 'Llama3-MS-CLIP', 'label': 'Llama'}, {'text': 'Llama3-MS-CLIP', 'label': 'Llama'}]","assigned_concept":"Llama","matched_keyword":"Llama3-LLaVA-Next","similarity_score":0.7163715363}
{"id":2311.12891,"submitter":"Yuxin Liu","authors":"Yuxin Liu, Minshan Xie, Hanyuan Liu, Tien-Tsin Wong","title":"Text-Guided Texturing by Synchronized Multi-View Diffusion","comments":"11 pages, 11 figures, technical papers, \"Text, Texturing, and\n  Stylization\"@SIGGRAPH Asia 2024","journal-ref":null,"doi":"10.1145\/3680528.3687621","report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper introduces a novel approach to synthesize texture to dress up a\ngiven 3D object, given a text prompt. Based on the pretrained text-to-image\n(T2I) diffusion model, existing methods usually employ a project-and-inpaint\napproach, in which a view of the given object is first generated and warped to\nanother view for inpainting. But it tends to generate inconsistent texture due\nto the asynchronous diffusion of multiple views. We believe such asynchronous\ndiffusion and insufficient information sharing among views are the root causes\nof the inconsistent artifact. In this paper, we propose a synchronized\nmulti-view diffusion approach that allows the diffusion processes from\ndifferent views to reach a consensus of the generated content early in the\nprocess, and hence ensures the texture consistency. To synchronize the\ndiffusion, we share the denoised content among different views in each\ndenoising step, specifically blending the latent content in the texture domain\nfrom views with overlap. Our method demonstrates superior performance in\ngenerating consistent, seamless, highly detailed textures, comparing to\nstate-of-the-art methods.\n","versions":"[{'version': 'v1', 'created': 'Tue, 21 Nov 2023 06:26:28 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 09:08:21 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Liu', 'Yuxin', ''], ['Xie', 'Minshan', ''], ['Liu', 'Hanyuan', ''], ['Wong', 'Tien-Tsin', '']]","extracted_entities":"[{'text': 'text prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"text prompt","similarity_score":0.6277507544}
{"id":2404.02475,"submitter":"Tian Huang","authors":"Tian Huang, Chun Yu, Weinan Shi, Zijian Peng, David Yang, Weiqi Sun,\n  Yuanchun Shi","title":"Prompt2Task: Automating UI Tasks on Smartphones from Textual Prompts","comments":"34 pages","journal-ref":"ACM Trans. Comput.-Hum. Interact. February 2025","doi":"10.1145\/3716132","report-no":null,"categories":"cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  UI task automation enables efficient task execution by simulating human\ninteractions with graphical user interfaces (GUIs), without modifying the\nexisting application code. However, its broader adoption is constrained by the\nneed for expertise in both scripting languages and workflow design. To address\nthis challenge, we present Prompt2Task, a system designed to comprehend various\ntask-related textual prompts (e.g., goals, procedures), thereby generating and\nperforming the corresponding automation tasks. Prompt2Task incorporates a suite\nof intelligent agents that mimic human cognitive functions, specializing in\ninterpreting user intent, managing external information for task generation,\nand executing operations on smartphones. The agents can learn from user\nfeedback and continuously improve their performance based on the accumulated\nknowledge. Experimental results indicated a performance jump from a 22.28\\%\nsuccess rate in the baseline to 95.24\\% with Prompt2Task, requiring an average\nof 0.69 user interventions for each new task. Prompt2Task presents promising\napplications in fields such as tutorial creation, smart assistance, and\ncustomer service.\n","versions":"[{'version': 'v1', 'created': 'Wed, 3 Apr 2024 05:32:05 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 09:59:25 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Huang', 'Tian', ''], ['Yu', 'Chun', ''], ['Shi', 'Weinan', ''], ['Peng', 'Zijian', ''], ['Yang', 'David', ''], ['Sun', 'Weiqi', ''], ['Shi', 'Yuanchun', '']]","extracted_entities":"[{'text': 'Prompt2Task', 'label': 'Prompting'}, {'text': 'Prompt2Task', 'label': 'Prompting'}, {'text': 'Prompt2Task', 'label': 'Prompting'}, {'text': 'Prompt2Task', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"Prompt2Task","similarity_score":0.5282181501}
{"id":2404.1682,"submitter":"Chuhan Zhang","authors":"Olivia Wiles, Chuhan Zhang, Isabela Albuquerque, Ivana Kaji\\'c, Su\n  Wang, Emanuele Bugliarello, Yasumasa Onoe, Pinelopi Papalampidi, Ira Ktena,\n  Chris Knutsen, Cyrus Rashtchian, Anant Nawalgaria, Jordi Pont-Tuset, Aida\n  Nematzadeh","title":"Revisiting Text-to-Image Evaluation with Gecko: On Metrics, Prompts, and\n  Human Ratings","comments":"Accepted to ICLR 2025 (Spotlight)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While text-to-image (T2I) generative models have become ubiquitous, they do\nnot necessarily generate images that align with a given prompt. While previous\nwork has evaluated T2I alignment by proposing metrics, benchmarks, and\ntemplates for collecting human judgements, the quality of these components is\nnot systematically measured. Human-rated prompt sets are generally small and\nthe reliability of the ratings -- and thereby the prompt set used to compare\nmodels -- is not evaluated. We address this gap by performing an extensive\nstudy evaluating auto-eval metrics and human templates. We provide three main\ncontributions: (1) We introduce a comprehensive skills-based benchmark that can\ndiscriminate models across different human templates. This skills-based\nbenchmark categorises prompts into sub-skills, allowing a practitioner to\npinpoint not only which skills are challenging, but at what level of complexity\na skill becomes challenging. (2) We gather human ratings across four templates\nand four T2I models for a total of >100K annotations. This allows us to\nunderstand where differences arise due to inherent ambiguity in the prompt and\nwhere they arise due to differences in metric and model quality. (3) Finally,\nwe introduce a new QA-based auto-eval metric that is better correlated with\nhuman ratings than existing metrics for our new dataset, across different human\ntemplates, and on TIFA160.\n","versions":"[{'version': 'v1', 'created': 'Thu, 25 Apr 2024 17:58:43 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Feb 2025 21:18:48 GMT'}, {'version': 'v3', 'created': 'Sat, 1 Mar 2025 22:41:18 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 15:53:14 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wiles', 'Olivia', ''], ['Zhang', 'Chuhan', ''], ['Albuquerque', 'Isabela', ''], ['Kaji\u0107', 'Ivana', ''], ['Wang', 'Su', ''], ['Bugliarello', 'Emanuele', ''], ['Onoe', 'Yasumasa', ''], ['Papalampidi', 'Pinelopi', ''], ['Ktena', 'Ira', ''], ['Knutsen', 'Chris', ''], ['Rashtchian', 'Cyrus', ''], ['Nawalgaria', 'Anant', ''], ['Pont-Tuset', 'Jordi', ''], ['Nematzadeh', 'Aida', '']]","extracted_entities":"[{'text': 'prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt","similarity_score":0.7767513394}
{"id":2406.04746,"submitter":"Radu Tudor Ionescu","authors":"Eduard Poesina, Adriana Valentina Costache, Adrian-Gabriel Chifu,\n  Josiane Mothe, Radu Tudor Ionescu","title":"PQPP: A Joint Benchmark for Text-to-Image Prompt and Query Performance\n  Prediction","comments":"Accepted at CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Text-to-image generation has recently emerged as a viable alternative to\ntext-to-image retrieval, driven by the visually impressive results of\ngenerative diffusion models. Although query performance prediction is an active\nresearch topic in information retrieval, to the best of our knowledge, there is\nno prior study that analyzes the difficulty of queries (referred to as prompts)\nin text-to-image generation, based on human judgments. To this end, we\nintroduce the first dataset of prompts which are manually annotated in terms of\nimage generation performance. Additionally, we extend these evaluations to\ntext-to-image retrieval by collecting manual annotations that represent\nretrieval performance. We thus establish the first joint benchmark for prompt\nand query performance prediction (PQPP) across both tasks, comprising over 10K\nqueries. Our benchmark enables (i) the comparative assessment of prompt\/query\ndifficulty in both image generation and image retrieval, and (ii) the\nevaluation of prompt\/query performance predictors addressing both generation\nand retrieval. We evaluate several pre- and post-generation\/retrieval\nperformance predictors, thus providing competitive baselines for future\nresearch. Our benchmark and code are publicly available at\nhttps:\/\/github.com\/Eduard6421\/PQPP.\n","versions":"[{'version': 'v1', 'created': 'Fri, 7 Jun 2024 08:46:19 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 16:45:09 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Poesina', 'Eduard', ''], ['Costache', 'Adriana Valentina', ''], ['Chifu', 'Adrian-Gabriel', ''], ['Mothe', 'Josiane', ''], ['Ionescu', 'Radu Tudor', '']]","extracted_entities":"[{'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt","similarity_score":0.7767513394}
{"id":2406.09123,"submitter":"Hamidreza Saffari","authors":"Hamidreza Saffari, Mohammadamin Shafiei, Donya Rooein, Francesco\n  Pierri, Debora Nozza","title":"Can I introduce my boyfriend to my grandmother? Evaluating Large\n  Language Models Capabilities on Iranian Social Norm Classification","comments":"15 pages, 1 figure, 9 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SI","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Creating globally inclusive AI systems demands datasets reflecting diverse\nsocial norms. Iran, with its unique cultural blend, offers an ideal case study,\nwith Farsi adding linguistic complexity. In this work, we introduce the Iranian\nSocial Norms (ISN) dataset, a novel collection of 1,699 Iranian social norms,\nincluding environments, demographic features, and scope annotation, alongside\nEnglish translations. Our evaluation of 6 Large Language Models (LLMs) in\nclassifying Iranian social norms, using a variety of prompts, uncovered\ncritical insights into the impact of geographic and linguistic context. Results\nrevealed a substantial performance gap in LLMs' comprehension of Iranian norms.\nNotably, while the geographic context in English prompts enhanced the\nperformance, this effect was absent in Farsi, pointing to nuanced linguistic\nchallenges. Particularly, performance was significantly worse for Iran-specific\nnorms, emphasizing the importance of culturally tailored datasets. As the first\nFarsi dataset for social norm classification, ISN will facilitate crucial\ncross-cultural analyses, shedding light on how values differ across contexts\nand cultures.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Jun 2024 13:56:55 GMT'}, {'version': 'v2', 'created': 'Sun, 16 Jun 2024 15:19:23 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 10:28:01 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Saffari', 'Hamidreza', ''], ['Shafiei', 'Mohammadamin', ''], ['Rooein', 'Donya', ''], ['Pierri', 'Francesco', ''], ['Nozza', 'Debora', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'English prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompts","similarity_score":0.7638334036}
{"id":2407.03528,"submitter":"Jos\\'e Tiago Mota Crispim","authors":"T. M. Crispim, G. Alencar, Milko Estrada","title":"Braneworld Black Bounce to Transversable Wormhole Analytically Connected\n  to an asymptotically $AdS_5$ Boundary","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"gr-qc","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We extend the recent approach from reference [1] to obtain complete and\nanalytic solutions (both brane and bulk) of a Simpson-Visser (SV) geometry\nwithin a braneworld framework. The embedded geometry can represent a\ntraversable wormhole (TWH), a one-way wormhole (OOWH), or a regular black hole\n(RBH). The resulting geometry is regular everywhere, eliminating any\nsingularity or local de-Sitter core at the origin and on the brane location,\nwhere the regular geometry is given by the SV geometry. The throat of TWHs or\nOOWHs can extend into the extra dimension. The event horizon of RBH extends\nalong the extra dimension, prompting speculation on the extension of entropy\ninto this dimension. Although the induced geometry is characterized by tension,\nacting akin to a positive cosmological constant (thus potentially representing\nempty space), the induced four-dimensional geometry remains regular. There is\nno need to introduce additional energy sources on the brane to achieve this\nregularity. Hence, the brane's geometry, which may depict an RBH, TWH, or OWWH,\nis influenced by the geometric properties of the bulk\n","versions":"[{'version': 'v1', 'created': 'Wed, 3 Jul 2024 22:17:36 GMT'}, {'version': 'v2', 'created': 'Thu, 23 Jan 2025 16:05:52 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 13:33:25 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Crispim', 'T. M.', ''], ['Alencar', 'G.', ''], ['Estrada', 'Milko', '']]","extracted_entities":"[{'text': 'prompting speculation', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompting speculation","similarity_score":0.5357450843}
{"id":2407.03605,"submitter":"Xiaoxia Liu","authors":"Xiaoxia Liu, Shijie Yu, Jian Lu, Xiaojun Chen","title":"Orthogonal Constrained Minimization with Tensor $\\ell_{2,p}$\n  Regularization for HSI Denoising and Destriping","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Hyperspectral images (HSIs) are often contaminated by a mixture of noises\nsuch as Gaussian noise, dead lines, stripes, and so on. In this paper, we\npropose a novel approach for HSI denoising and destriping, called NLTL2p, which\nconsists of an orthogonal constrained minimization model and an iterative\nalgorithm with convergence guarantees. The model of the proposed NLTL2p\napproach is built based on a new sparsity-enhanced Nonlocal Low-rank Tensor\nregularization and a tensor $\\ell_{2,p}$ norm with $p\\in(0,1)$. The low-rank\nconstraints for HSI denoising utilize the spatial nonlocal self-similarity and\nspectral correlation of HSIs and are formulated based on independent\nhigher-order singular value decomposition with sparsity enhancement on its core\ntensor to prompt more low-rankness. The tensor $\\ell_{2,p}$ norm for HSI\ndestriping is extended from the matrix $\\ell_{2,p}$ norm. A proximal block\ncoordinate descent algorithm is proposed in the NLTL2p approach to solve the\nresulting nonconvex nonsmooth minimization with orthogonal constraints. We show\nany accumulation point of the sequence generated by the proposed algorithm\nconverges to a first-order stationary point, which is defined using three\nequalities of substationarity, symmetry, and feasibility for orthogonal\nconstraints. In the numerical experiments, we compare the proposed method with\nstate-of-the-art methods including a deep learning based method, and test the\nmethods on both simulated and real HSI datasets. Our proposed NLTL2p method\ndemonstrates outperformance in terms of metrics such as mean peak\nsignal-to-noise ratio as well as visual quality.\n","versions":"[{'version': 'v1', 'created': 'Thu, 4 Jul 2024 03:33:19 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 03:13:43 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Liu', 'Xiaoxia', ''], ['Yu', 'Shijie', ''], ['Lu', 'Jian', ''], ['Chen', 'Xiaojun', '']]","extracted_entities":"[{'text': 'prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt","similarity_score":0.7767513394}
{"id":2408.11852,"submitter":"Mohammad Taha Bahadori","authors":"Milad Fotouhi, Mohammad Taha Bahadori, Oluwaseyi Feyisetan, Payman\n  Arabshahi, David Heckerman","title":"Fast Training Dataset Attribution via In-Context Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We investigate the use of in-context learning and prompt engineering to\nestimate the contributions of training data in the outputs of instruction-tuned\nlarge language models (LLMs). We propose two novel approaches: (1) a\nsimilarity-based approach that measures the difference between LLM outputs with\nand without provided context, and (2) a mixture distribution model approach\nthat frames the problem of identifying contribution scores as a matrix\nfactorization task. Our empirical comparison demonstrates that the mixture\nmodel approach is more robust to retrieval noise in in-context learning,\nproviding a more reliable estimation of data contributions.\n","versions":"[{'version': 'v1', 'created': 'Wed, 14 Aug 2024 20:48:45 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 21:10:24 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Fotouhi', 'Milad', ''], ['Bahadori', 'Mohammad Taha', ''], ['Feyisetan', 'Oluwaseyi', ''], ['Arabshahi', 'Payman', ''], ['Heckerman', 'David', '']]","extracted_entities":"[{'text': 'in-context learning', 'label': 'Few-shot Learning'}, {'text': 'prompt engineering', 'label': 'Prompting'}, {'text': 'in-context learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Prompting","matched_keyword":"prompt engineering","similarity_score":0.5494377613}
{"id":2410.09449,"submitter":"Yi Dai","authors":"Yi Dai","title":"Continuous Risk Prediction","comments":"In revision","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Lifelong learning (LL) capabilities are essential for QA models to excel in\nreal-world applications, and architecture-based LL approaches have proven to be\na promising direction for achieving this goal. However, adapting existing\nmethods to QA tasks is far from straightforward. Many prior approaches either\nrely on access to task identities during testing or fail to adequately model\nsamples from unseen tasks, which limits their practical applicability. To\novercome these limitations, we introduce Diana , a novel\n\\underline{d}ynam\\underline{i}c \\underline{a}rchitecture-based\nlifelo\\underline{n}g Q\\underline{A} framework designed to learn a sequence of\nQA tasks using a prompt-enhanced language model.Diana leverages four\nhierarchically structured types of prompts to capture QA knowledge at multiple\nlevels of granularity. Task-level prompts are specifically designed to encode\ntask-specific knowledge, ensuring strong lifelong learning performance.\nMeanwhile, instance-level prompts are utilized to capture shared knowledge\nacross diverse input samples, enhancing the model's generalization\ncapabilities. Additionally, Diana incorporates dedicated prompts to explicitly\nhandle unseen tasks and introduces a set of prompt key vectors that facilitate\nefficient knowledge transfer and sharing between tasks. Through extensive\nexperimentation, we demonstrate that Diana achieves state-of-the-art\nperformance among lifelong QA models, with particularly notable improvements in\nits ability to handle previously unseen tasks. This makes Diana a significant\nadvancement in the field of lifelong learning for question-answering systems.\n","versions":"[{'version': 'v1', 'created': 'Sat, 12 Oct 2024 09:06:09 GMT'}, {'version': 'v2', 'created': 'Mon, 25 Nov 2024 04:57:04 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 06:51:27 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Dai', 'Yi', '']]","extracted_entities":"[{'text': 'Lifelong learning', 'label': 'Few-shot Learning'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'Task-level prompts', 'label': 'Prompting'}, {'text': 'instance-level prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'lifelong learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Prompting","matched_keyword":"prompts","similarity_score":0.7638334036}
{"id":2410.11843,"submitter":"Zeru Shi","authors":"Zeru Shi, Kai Mei, Mingyu Jin, Yongye Su, Chaoji Zuo, Wenyue Hua,\n  Wujiang Xu, Yujie Ren, Zirui Liu, Mengnan Du, Dong Deng, Yongfeng Zhang","title":"From Commands to Prompts: LLM-based Semantic File System for AIOS","comments":"Accepted by International Conference on Learning Representations\n  2025(ICLR2025)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC cs.AI cs.DB cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs) have demonstrated significant potential in the\ndevelopment of intelligent applications and systems such as LLM-based agents\nand agent operating systems (AIOS). However, when these applications and\nsystems interact with the underlying file system, the file system still remains\nthe traditional paradigm: reliant on manual navigation through precise\ncommands. This paradigm poses a bottleneck to the usability of these systems as\nusers are required to navigate complex folder hierarchies and remember cryptic\nfile names. To address this limitation, we propose an LLM-based semantic file\nsystem ( LSFS ) for prompt-driven file management. Unlike conventional\napproaches, LSFS incorporates LLMs to enable users or agents to interact with\nfiles through natural language prompts, facilitating semantic file management.\nAt the macro-level, we develop a comprehensive API set to achieve semantic file\nmanagement functionalities, such as semantic file retrieval, file update\nmonitoring and summarization, and semantic file rollback). At the micro-level,\nwe store files by constructing semantic indexes for them, design and implement\nsyscalls of different semantic operations (e.g., CRUD, group by, join) powered\nby vector database. Our experiments show that LSFS offers significant\nimprovements over traditional file systems in terms of user convenience, the\ndiversity of supported functions, and the accuracy and efficiency of file\noperations. Additionally, with the integration of LLM, our system enables more\nintelligent file management tasks, such as content summarization and version\ncomparison, further enhancing its capabilities.\n","versions":"[{'version': 'v1', 'created': 'Mon, 23 Sep 2024 08:39:16 GMT'}, {'version': 'v2', 'created': 'Fri, 27 Dec 2024 08:32:38 GMT'}, {'version': 'v3', 'created': 'Fri, 28 Feb 2025 15:41:00 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Mar 2025 10:50:44 GMT'}, {'version': 'v5', 'created': 'Wed, 19 Mar 2025 03:17:47 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Shi', 'Zeru', ''], ['Mei', 'Kai', ''], ['Jin', 'Mingyu', ''], ['Su', 'Yongye', ''], ['Zuo', 'Chaoji', ''], ['Hua', 'Wenyue', ''], ['Xu', 'Wujiang', ''], ['Ren', 'Yujie', ''], ['Liu', 'Zirui', ''], ['Du', 'Mengnan', ''], ['Deng', 'Dong', ''], ['Zhang', 'Yongfeng', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'natural language prompts', 'label': 'Prompting'}, {'text': 'LSFS', 'label': 'Large Language Model'}]","assigned_concept":"Prompting","matched_keyword":"natural language prompts","similarity_score":0.6456617713}
{"id":2410.15908,"submitter":"John Wickerson","authors":"Chengsong Tan, Alastair F. Donaldson, and John Wickerson","title":"Formalising CXL Cache Coherence","comments":"14 pages","journal-ref":"Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, ASPLOS\n  2025, Rotterdam, Netherlands","doi":"10.1145\/3676641.3715999","report-no":null,"categories":"cs.AR cs.PL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.\n","versions":"[{'version': 'v1', 'created': 'Mon, 21 Oct 2024 11:29:49 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 10:19:30 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Tan', 'Chengsong', ''], ['Donaldson', 'Alastair F.', ''], ['Wickerson', 'John', '']]","extracted_entities":"[{'text': 'prompted', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompted","similarity_score":0.7553437948}
{"id":2410.17856,"submitter":"Shaofei Cai","authors":"Shaofei Cai, Zihao Wang, Kewei Lian, Zhancun Mu, Xiaojian Ma, Anji\n  Liu, Yitao Liang","title":"ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context\n  Prompting","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/","abstract":"  Vision-language models (VLMs) have excelled in multimodal tasks, but adapting\nthem to embodied decision-making in open-world environments presents\nchallenges. One critical issue is bridging the gap between discrete entities in\nlow-level observations and the abstract concepts required for effective\nplanning. A common solution is building hierarchical agents, where VLMs serve\nas high-level reasoners that break down tasks into executable sub-tasks,\ntypically specified using language. However, language suffers from the\ninability to communicate detailed spatial information. We propose\nvisual-temporal context prompting, a novel communication protocol between VLMs\nand policy models. This protocol leverages object segmentation from past\nobservations to guide policy-environment interactions. Using this approach, we\ntrain ROCKET-1, a low-level policy that predicts actions based on concatenated\nvisual observations and segmentation masks, supported by real-time object\ntracking from SAM-2. Our method unlocks the potential of VLMs, enabling them to\ntackle complex tasks that demand spatial reasoning. Experiments in Minecraft\nshow that our approach enables agents to achieve previously unattainable tasks,\nwith a $\\mathbf{76}\\%$ absolute improvement in open-world interaction\nperformance. Codes and demos are now available on the project page:\nhttps:\/\/craftjarvis.github.io\/ROCKET-1.\n","versions":"[{'version': 'v1', 'created': 'Wed, 23 Oct 2024 13:26:59 GMT'}, {'version': 'v2', 'created': 'Thu, 14 Nov 2024 12:29:41 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 11:55:54 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Cai', 'Shaofei', ''], ['Wang', 'Zihao', ''], ['Lian', 'Kewei', ''], ['Mu', 'Zhancun', ''], ['Ma', 'Xiaojian', ''], ['Liu', 'Anji', ''], ['Liang', 'Yitao', '']]","extracted_entities":"[{'text': 'visual-temporal context prompting', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"visual-temporal context prompting","similarity_score":0.5785127878}
{"id":2411.07917,"submitter":"Subhankar Maity","authors":"Aniket Deroy, Subhankar Maity","title":"CryptoLLM: Unleashing the Power of Prompted LLMs for SmartQnA and\n  Classification of Crypto Posts","comments":"Updated and Final Version","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The rapid growth of social media has resulted in an large volume of\nuser-generated content, particularly in niche domains such as cryptocurrency.\nThis task focuses on developing robust classification models to accurately\ncategorize cryptocurrency-related social media posts into predefined classes,\nincluding but not limited to objective, positive, negative, etc. Additionally,\nthe task requires participants to identify the most relevant answers from a set\nof posts in response to specific questions. By leveraging advanced LLMs, this\nresearch aims to enhance the understanding and filtering of cryptocurrency\ndiscourse, thereby facilitating more informed decision-making in this volatile\nsector. We have used a prompt-based technique to solve the classification task\nfor reddit posts and twitter posts. Also, we have used 64-shot technique along\nwith prompts on GPT-4-Turbo model to determine whether a answer is relevant to\na question or not.\n","versions":"[{'version': 'v1', 'created': 'Tue, 12 Nov 2024 16:49:51 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 15:49:08 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Deroy', 'Aniket', ''], ['Maity', 'Subhankar', '']]","extracted_entities":"[{'text': 'prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompts","similarity_score":0.7638334036}
{"id":2411.08402,"submitter":"Xun Huang","authors":"Xun Huang, Jinlong Wang, Qiming Xia, Siheng Chen, Bisheng Yang, Xin\n  Li, Cheng Wang, Chenglu Wen","title":"V2X-R: Cooperative LiDAR-4D Radar Fusion for 3D Object Detection with\n  Denoising Diffusion","comments":"Accepted by CVPR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Current Vehicle-to-Everything (V2X) systems have significantly enhanced 3D\nobject detection using LiDAR and camera data. However, these methods suffer\nfrom performance degradation in adverse weather conditions. The weather-robust\n4D radar provides Doppler and additional geometric information, raising the\npossibility of addressing this challenge. To this end, we present V2X-R, the\nfirst simulated V2X dataset incorporating LiDAR, camera, and 4D radar. V2X-R\ncontains 12,079 scenarios with 37,727 frames of LiDAR and 4D radar point\nclouds, 150,908 images, and 170,859 annotated 3D vehicle bounding boxes.\nSubsequently, we propose a novel cooperative LiDAR-4D radar fusion pipeline for\n3D object detection and implement it with various fusion strategies. To achieve\nweather-robust detection, we additionally propose a Multi-modal Denoising\nDiffusion (MDD) module in our fusion pipeline. MDD utilizes weather-robust 4D\nradar feature as a condition to prompt the diffusion model to denoise noisy\nLiDAR features. Experiments show that our LiDAR-4D radar fusion pipeline\ndemonstrates superior performance in the V2X-R dataset. Over and above this,\nour MDD module further improved the performance of basic fusion model by up to\n5.73%\/6.70% in foggy\/snowy conditions with barely disrupting normal\nperformance. The dataset and code will be publicly available at:\nhttps:\/\/github.com\/ylwhxht\/V2X-R.\n","versions":"[{'version': 'v1', 'created': 'Wed, 13 Nov 2024 07:41:47 GMT'}, {'version': 'v2', 'created': 'Mon, 18 Nov 2024 16:54:54 GMT'}, {'version': 'v3', 'created': 'Sat, 1 Mar 2025 10:36:44 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 03:55:02 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Huang', 'Xun', ''], ['Wang', 'Jinlong', ''], ['Xia', 'Qiming', ''], ['Chen', 'Siheng', ''], ['Yang', 'Bisheng', ''], ['Li', 'Xin', ''], ['Wang', 'Cheng', ''], ['Wen', 'Chenglu', '']]","extracted_entities":"[{'text': 'prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt","similarity_score":0.7767513394}
{"id":2411.12593,"submitter":"Yuanbin Man","authors":"Yuanbin Man, Ying Huang, Chengming Zhang, Bingzhe Li, Wei Niu, Miao\n  Yin","title":"AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive\n  Cross-Modality Memory Reduction","comments":"Accepted to CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The advancements in large language models (LLMs) have propelled the\nimprovement of video understanding tasks by incorporating LLMs with visual\nmodels. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat)\nare constrained to processing short-duration videos. Recent attempts to\nunderstand long-term videos by extracting and compressing visual features into\na fixed memory size. Nevertheless, those methods leverage only visual modality\nto merge video tokens and overlook the correlation between visual and textual\nqueries, leading to difficulties in effectively handling complex\nquestion-answering tasks. To address the challenges of long videos and complex\nprompts, we propose AdaCM$^2$, which, for the first time, introduces an\nadaptive cross-modality memory reduction approach to video-text alignment in an\nauto-regressive manner on video streams. Our extensive experiments on various\nvideo understanding tasks, such as video captioning, video question answering,\nand video classification, demonstrate that AdaCM$^2$ achieves state-of-the-art\nperformance across multiple datasets while significantly reducing memory usage.\nNotably, it achieves a 4.5% improvement across multiple tasks in the LVU\ndataset with a GPU memory consumption reduction of up to 65%.\n","versions":"[{'version': 'v1', 'created': 'Tue, 19 Nov 2024 18:04:13 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 02:28:48 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Man', 'Yuanbin', ''], ['Huang', 'Ying', ''], ['Zhang', 'Chengming', ''], ['Li', 'Bingzhe', ''], ['Niu', 'Wei', ''], ['Yin', 'Miao', '']]","extracted_entities":"[{'text': 'VideoChat', 'label': 'ChatGPT'}, {'text': 'complex\\nprompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"complex\nprompts","similarity_score":0.6091507673}
{"id":2411.14743,"submitter":"Zhengrui Guo","authors":"Zhengrui Guo, Conghao Xiong, Jiabo Ma, Qichen Sun, Lishuang Feng,\n  Jinzhuo Wang, Hao Chen","title":"FOCUS: Knowledge-enhanced Adaptive Visual Compression for Few-shot Whole\n  Slide Image Classification","comments":"Accepted by CVPR'2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI q-bio.QM","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Few-shot learning presents a critical solution for cancer diagnosis in\ncomputational pathology (CPath), addressing fundamental limitations in data\navailability, particularly the scarcity of expert annotations and patient\nprivacy constraints. A key challenge in this paradigm stems from the inherent\ndisparity between the limited training set of whole slide images (WSIs) and the\nenormous number of contained patches, where a significant portion of these\npatches lacks diagnostically relevant information, potentially diluting the\nmodel's ability to learn and focus on critical diagnostic features. While\nrecent works attempt to address this by incorporating additional knowledge,\nseveral crucial gaps hinder further progress: (1) despite the emergence of\npowerful pathology foundation models (FMs), their potential remains largely\nuntapped, with most approaches limiting their use to basic feature extraction;\n(2) current language guidance mechanisms attempt to align text prompts with\nvast numbers of WSI patches all at once, struggling to leverage rich\npathological semantic information. To this end, we introduce the\nknowledge-enhanced adaptive visual compression framework, dubbed FOCUS, which\nuniquely combines pathology FMs with language prior knowledge to enable a\nfocused analysis of diagnostically relevant regions by prioritizing\ndiscriminative WSI patches. Our approach implements a progressive three-stage\ncompression strategy: we first leverage FMs for global visual redundancy\nelimination, and integrate compressed features with language prompts for\nsemantic relevance assessment, then perform neighbor-aware visual token\nfiltering while preserving spatial coherence. Extensive experiments on\npathological datasets spanning breast, lung, and ovarian cancers demonstrate\nits superior performance in few-shot pathology diagnosis. Codes are available\nat https:\/\/github.com\/dddavid4real\/FOCUS.\n","versions":"[{'version': 'v1', 'created': 'Fri, 22 Nov 2024 05:36:38 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 12:16:47 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Guo', 'Zhengrui', ''], ['Xiong', 'Conghao', ''], ['Ma', 'Jiabo', ''], ['Sun', 'Qichen', ''], ['Feng', 'Lishuang', ''], ['Wang', 'Jinzhuo', ''], ['Chen', 'Hao', '']]","extracted_entities":"[{'text': 'Few-shot learning', 'label': 'Prompting'}, {'text': 'FMs', 'label': 'Foundation Model'}, {'text': 'text prompts', 'label': 'Prompting'}, {'text': 'FOCUS', 'label': 'Foundation Model'}, {'text': 'FMs', 'label': 'Foundation Model'}, {'text': 'FMs', 'label': 'Foundation Model'}, {'text': 'language prompts', 'label': 'Prompting'}, {'text': 'FOCUS', 'label': 'Foundation Model'}]","assigned_concept":"Prompting","matched_keyword":"text prompts","similarity_score":0.6106933355}
{"id":2411.15115,"submitter":"Jaemin Cho","authors":"Daeun Lee, Jaehong Yoon, Jaemin Cho, Mohit Bansal","title":"VideoRepair: Improving Text-to-Video Generation via Misalignment\n  Evaluation and Localized Refinement","comments":"Project page: https:\/\/video-repair.github.io","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent text-to-video (T2V) diffusion models have demonstrated impressive\ngeneration capabilities across various domains. However, these models often\ngenerate videos that have misalignments with text prompts, especially when the\nprompts describe complex scenes with multiple objects and attributes. To\naddress this, we introduce VideoRepair, a novel model-agnostic, training-free\nvideo refinement framework that automatically identifies fine-grained\ntext-video misalignments and generates explicit spatial and textual feedback,\nenabling a T2V diffusion model to perform targeted, localized refinements.\nVideoRepair consists of two stages: In (1) video refinement planning, we first\ndetect misalignments by generating fine-grained evaluation questions and\nanswering them using an MLLM. Based on video evaluation outputs, we identify\naccurately generated objects and construct localized prompts to precisely\nrefine misaligned regions. In (2) localized refinement, we enhance video\nalignment by 'repairing' the misaligned regions from the original video while\npreserving the correctly generated areas. This is achieved by frame-wise region\ndecomposition using our Region-Preserving Segmentation (RPS) module. On two\npopular video generation benchmarks (EvalCrafter and T2V-CompBench),\nVideoRepair substantially outperforms recent baselines across various\ntext-video alignment metrics. We provide a comprehensive analysis of\nVideoRepair components and qualitative examples.\n","versions":"[{'version': 'v1', 'created': 'Fri, 22 Nov 2024 18:31:47 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 21:39:33 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Lee', 'Daeun', ''], ['Yoon', 'Jaehong', ''], ['Cho', 'Jaemin', ''], ['Bansal', 'Mohit', '']]","extracted_entities":"[{'text': 'text prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'localized prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompts","similarity_score":0.7638334036}
{"id":2411.17698,"submitter":"Ziyang Chen","authors":"Ziyang Chen, Prem Seetharaman, Bryan Russell, Oriol Nieto, David\n  Bourgin, Andrew Owens, Justin Salamon","title":"Video-Guided Foley Sound Generation with Multimodal Controls","comments":"Accepted at CVPR 2025. Project site:\n  https:\/\/ificl.github.io\/MultiFoley\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.MM cs.SD eess.AS","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Generating sound effects for videos often requires creating artistic sound\neffects that diverge significantly from real-life sources and flexible control\nin the sound design. To address this problem, we introduce MultiFoley, a model\ndesigned for video-guided sound generation that supports multimodal\nconditioning through text, audio, and video. Given a silent video and a text\nprompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels\nspinning without wind noise) or more whimsical sounds (e.g., making a lion's\nroar sound like a cat's meow). MultiFoley also allows users to choose reference\naudio from sound effects (SFX) libraries or partial videos for conditioning. A\nkey novelty of our model lies in its joint training on both internet video\ndatasets with low-quality audio and professional SFX recordings, enabling\nhigh-quality, full-bandwidth (48kHz) audio generation. Through automated\nevaluations and human studies, we demonstrate that MultiFoley successfully\ngenerates synchronized high-quality sounds across varied conditional inputs and\noutperforms existing methods. Please see our project page for video results:\nhttps:\/\/ificl.github.io\/MultiFoley\/\n","versions":"[{'version': 'v1', 'created': 'Tue, 26 Nov 2024 18:59:58 GMT'}, {'version': 'v2', 'created': 'Thu, 28 Nov 2024 13:25:04 GMT'}, {'version': 'v3', 'created': 'Wed, 22 Jan 2025 20:03:04 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 17:44:37 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Chen', 'Ziyang', ''], ['Seetharaman', 'Prem', ''], ['Russell', 'Bryan', ''], ['Nieto', 'Oriol', ''], ['Bourgin', 'David', ''], ['Owens', 'Andrew', ''], ['Salamon', 'Justin', '']]","extracted_entities":"[{'text': 'text\\nprompt', 'label': 'Prompting'}, {'text': 'lion', 'label': 'Llama'}]","assigned_concept":"Prompting","matched_keyword":"text\nprompt","similarity_score":0.6277507544}
{"id":2411.1881,"submitter":"Shuangqi Li","authors":"Shuangqi Li, Hieu Le, Jingyi Xu, Mathieu Salzmann","title":"All Seeds Are Not Equal: Enhancing Compositional Text-to-Image\n  Generation with Reliable Random Seeds","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Text-to-image diffusion models have demonstrated remarkable capability in\ngenerating realistic images from arbitrary text prompts. However, they often\nproduce inconsistent results for compositional prompts such as \"two dogs\" or \"a\npenguin on the right of a bowl\". Understanding these inconsistencies is crucial\nfor reliable image generation. In this paper, we highlight the significant role\nof initial noise in these inconsistencies, where certain noise patterns are\nmore reliable for compositional prompts than others. Our analyses reveal that\ndifferent initial random seeds tend to guide the model to place objects in\ndistinct image areas, potentially adhering to specific patterns of camera\nangles and image composition associated with the seed. To improve the model's\ncompositional ability, we propose a method for mining these reliable cases,\nresulting in a curated training set of generated images without requiring any\nmanual annotation. By fine-tuning text-to-image models on these generated\nimages, we significantly enhance their compositional capabilities. For\nnumerical composition, we observe relative increases of 29.3% and 19.5% for\nStable Diffusion and PixArt-{\\alpha}, respectively. Spatial composition sees\neven larger gains, with 60.7% for Stable Diffusion and 21.1% for\nPixArt-{\\alpha}.\n","versions":"[{'version': 'v1', 'created': 'Wed, 27 Nov 2024 23:32:54 GMT'}, {'version': 'v2', 'created': 'Mon, 2 Dec 2024 09:10:34 GMT'}, {'version': 'v3', 'created': 'Fri, 7 Feb 2025 17:14:32 GMT'}, {'version': 'v4', 'created': 'Sun, 2 Mar 2025 00:15:11 GMT'}, {'version': 'v5', 'created': 'Wed, 19 Mar 2025 22:39:25 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Li', 'Shuangqi', ''], ['Le', 'Hieu', ''], ['Xu', 'Jingyi', ''], ['Salzmann', 'Mathieu', '']]","extracted_entities":"[{'text': 'arbitrary text prompts', 'label': 'Prompting'}, {'text': 'compositional prompts', 'label': 'Prompting'}, {'text': 'compositional prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"arbitrary text prompts","similarity_score":0.5707825422}
{"id":2412.02259,"submitter":"Mingzhe Zheng","authors":"Mingzhe Zheng, Yongqi Xu, Haojian Huang, Xuran Ma, Yexin Liu, Wenjie\n  Shu, Yatian Pang, Feilong Tang, Qifeng Chen, Harry Yang, Ser-Nam Lim","title":"VideoGen-of-Thought: Step-by-step generating multi-shot video with\n  minimal manual intervention","comments":"Code: https:\/\/github.com\/DuNGEOnmassster\/VideoGen-of-Thought.git;\n  Webpage: https:\/\/cheliosoops.github.io\/VGoT\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Current video generation models excel at short clips but fail to produce\ncohesive multi-shot narratives due to disjointed visual dynamics and fractured\nstorylines. Existing solutions either rely on extensive manual\nscripting\/editing or prioritize single-shot fidelity over cross-scene\ncontinuity, limiting their practicality for movie-like content. We introduce\nVideoGen-of-Thought (VGoT), a step-by-step framework that automates multi-shot\nvideo synthesis from a single sentence by systematically addressing three core\nchallenges: (1) Narrative Fragmentation: Existing methods lack structured\nstorytelling. We propose dynamic storyline modeling, which first converts the\nuser prompt into concise shot descriptions, then elaborates them into detailed,\ncinematic specifications across five domains (character dynamics, background\ncontinuity, relationship evolution, camera movements, HDR lighting), ensuring\nlogical narrative progression with self-validation. (2) Visual Inconsistency:\nExisting approaches struggle with maintaining visual consistency across shots.\nOur identity-aware cross-shot propagation generates identity-preserving\nportrait (IPP) tokens that maintain character fidelity while allowing trait\nvariations (expressions, aging) dictated by the storyline. (3) Transition\nArtifacts: Abrupt shot changes disrupt immersion. Our adjacent latent\ntransition mechanisms implement boundary-aware reset strategies that process\nadjacent shots' features at transition points, enabling seamless visual flow\nwhile preserving narrative continuity. VGoT generates multi-shot videos that\noutperform state-of-the-art baselines by 20.4% in within-shot face consistency\nand 17.4% in style consistency, while achieving over 100% better cross-shot\nconsistency and 10x fewer manual adjustments than alternatives.\n","versions":"[{'version': 'v1', 'created': 'Tue, 3 Dec 2024 08:33:50 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 02:25:43 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zheng', 'Mingzhe', ''], ['Xu', 'Yongqi', ''], ['Huang', 'Haojian', ''], ['Ma', 'Xuran', ''], ['Liu', 'Yexin', ''], ['Shu', 'Wenjie', ''], ['Pang', 'Yatian', ''], ['Tang', 'Feilong', ''], ['Chen', 'Qifeng', ''], ['Yang', 'Harry', ''], ['Lim', 'Ser-Nam', '']]","extracted_entities":"[{'text': 'user prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"user prompt","similarity_score":0.6892601252}
{"id":2412.05101,"submitter":"Ruoyu Wang","authors":"Ruoyu Wang, Huayang Huang, Ye Zhu, Olga Russakovsky, Yu Wu","title":"The Silent Assistant: NoiseQuery as Implicit Guidance for Goal-Driven\n  Image Generation","comments":"18 pages, 18 figures, 6 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this work, we introduce NoiseQuery as a novel method for enhanced noise\ninitialization in versatile goal-driven text-to-image (T2I) generation.\nSpecifically, we propose to leverage an aligned Gaussian noise as implicit\nguidance to complement explicit user-defined inputs, such as text prompts, for\nbetter generation quality and controllability. Unlike existing noise\noptimization methods designed for specific models, our approach is grounded in\na fundamental examination of the generic finite-step noise scheduler design in\ndiffusion formulation, allowing better generalization across different\ndiffusion-based architectures in a tuning-free manner. This model-agnostic\nnature allows us to construct a reusable noise library compatible with multiple\nT2I models and enhancement techniques, serving as a foundational layer for more\neffective generation. Extensive experiments demonstrate that NoiseQuery enables\nfine-grained control and yields significant performance boosts not only over\nhigh-level semantics but also over low-level visual attributes, which are\ntypically difficult to specify through text alone, with seamless integration\ninto current workflows with minimal computational overhead.\n","versions":"[{'version': 'v1', 'created': 'Fri, 6 Dec 2024 14:59:00 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 05:01:35 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wang', 'Ruoyu', ''], ['Huang', 'Huayang', ''], ['Zhu', 'Ye', ''], ['Russakovsky', 'Olga', ''], ['Wu', 'Yu', '']]","extracted_entities":"[{'text': 'NoiseQuery', 'label': 'Foundation Model'}, {'text': 'text prompts', 'label': 'Prompting'}, {'text': 'NoiseQuery', 'label': 'Foundation Model'}, {'text': 'fine-grained control', 'label': 'Fine-tuning'}]","assigned_concept":"Prompting","matched_keyword":"text prompts","similarity_score":0.6106933355}
{"id":2412.07612,"submitter":"Subin Varghese","authors":"Subin Varghese, Joshua Gao, Vedhus Hoskere","title":"ViewDelta: Text-Prompted Change Detection in Unaligned Images","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Detecting changes between images is fundamental in applications such as\ninfrastructure assessment, environmental monitoring, and industrial automation.\nExisting supervised models demonstrate strong performance but are inherently\nlimited by the scope of their training data, requiring retraining to recognize\nnovel changes. To overcome this limitation, we introduce a novel change\ndetection task utilizing textual prompts alongside two potentially unaligned\nimages to produce binary segmentations highlighting user-relevant changes. This\ntext-conditioned framework significantly broadens the scope of change\ndetection, enabling unparalleled flexibility and straightforward scalability by\nincorporating diverse future datasets without restriction to specific change\ntypes. As a first approach to address this challenge, we propose ViewDelta, a\nmultimodal architecture extending the vision transformer into the domain of\ntext-conditioned change detection. ViewDelta establishes a robust baseline,\ndemonstrating flexibility across various scenarios and achieving competitive\nresults compared to specialized, fine-tuned models trained on aligned images.\nMoreover, we create and release the first text-prompt-conditioned change\ndetection dataset, comprising 501,153 image pairs with corresponding textual\nprompts and annotated labels. Extensive experiments confirm the robustness and\nversatility of our model across diverse environments, including indoor,\noutdoor, street-level, synthetic, and satellite imagery.\nhttps:\/\/joshuakgao.github.io\/viewdelta\/\n","versions":"[{'version': 'v1', 'created': 'Tue, 10 Dec 2024 15:51:17 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 13:47:36 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Varghese', 'Subin', ''], ['Gao', 'Joshua', ''], ['Hoskere', 'Vedhus', '']]","extracted_entities":"[{'text': 'textual prompts', 'label': 'Prompting'}, {'text': 'textual\\nprompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"textual prompts","similarity_score":0.6302489042}
{"id":2412.07658,"submitter":"Anubhav Jain","authors":"Anubhav Jain, Yuya Kobayashi, Takashi Shibuya, Yuhta Takida, Nasir\n  Memon, Julian Togelius, Yuki Mitsufuji","title":"TraSCE: Trajectory Steering for Concept Erasure","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent advancements in text-to-image diffusion models have brought them to\nthe public spotlight, becoming widely accessible and embraced by everyday\nusers. However, these models have been shown to generate harmful content such\nas not-safe-for-work (NSFW) images. While approaches have been proposed to\nerase such abstract concepts from the models, jail-breaking techniques have\nsucceeded in bypassing such safety measures. In this paper, we propose TraSCE,\nan approach to guide the diffusion trajectory away from generating harmful\ncontent. Our approach is based on negative prompting, but as we show in this\npaper, a widely used negative prompting strategy is not a complete solution and\ncan easily be bypassed in some corner cases. To address this issue, we first\npropose using a specific formulation of negative prompting instead of the\nwidely used one. Furthermore, we introduce a localized loss-based guidance that\nenhances the modified negative prompting technique by steering the diffusion\ntrajectory. We demonstrate that our proposed method achieves state-of-the-art\nresults on various benchmarks in removing harmful content, including ones\nproposed by red teams, and erasing artistic styles and objects. Our proposed\napproach does not require any training, weight modifications, or training data\n(either image or prompt), making it easier for model owners to erase new\nconcepts.\n","versions":"[{'version': 'v1', 'created': 'Tue, 10 Dec 2024 16:45:03 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 15:37:35 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Jain', 'Anubhav', ''], ['Kobayashi', 'Yuya', ''], ['Shibuya', 'Takashi', ''], ['Takida', 'Yuhta', ''], ['Memon', 'Nasir', ''], ['Togelius', 'Julian', ''], ['Mitsufuji', 'Yuki', '']]","extracted_entities":"[{'text': 'negative prompting', 'label': 'Prompting'}, {'text': 'negative prompting', 'label': 'Prompting'}, {'text': 'negative prompting', 'label': 'Prompting'}, {'text': 'negative prompting', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"negative prompting","similarity_score":0.7876226902}
{"id":2412.16654,"submitter":"Yaming Zhang","authors":"Yaming Zhang and Chenqiang Gao and Fangcen Liu and Junjie Guo and Lan\n  Wang and Xinggan Peng and Deyu Meng","title":"IV-tuning: Parameter-Efficient Transfer Learning for Infrared-Visible\n  Tasks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Various infrared-visible (IR-VIS) tasks greatly benefit from the advantage of\ncombining infrared and visible modalities. Driven by the motivation that\nstreamlining the infrared flow and harnessing PVMs with fewer parameters for\nsuperior performance, we propose \"IV-tuning\", a novel and general fine-tuning\napproach, to parameter-efficiently harness PVMs for various infrared-visible\ndownstream tasks. At its core, IV-tuning freezes pre-trained visible-based PVMs\nand integrates infrared flow into modal prompts to interact with adapters,\nwhich achieves a more efficient and general modal interaction paradigm. By\nfine-tuning approximately 3% of the backbone parameters, IV-tuning outperforms\nfull fine-tuning and previous state-of-the-art methods across multiple\nbaselines in multiple tasks, including IR-VIS salient object detection,\nsemantic segmentation and object detection. Extensive experiments demonstrate\nthat IV-tuning achieves superior performance with fewer trainable parameters,\nproviding a good alternative to full fine-tuning and a novel method of\nextending visible-based models for infrared-visible tasks. The code will be\nprovided in supplementary material.\n","versions":"[{'version': 'v1', 'created': 'Sat, 21 Dec 2024 14:54:41 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 07:52:24 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Zhang', 'Yaming', ''], ['Gao', 'Chenqiang', ''], ['Liu', 'Fangcen', ''], ['Guo', 'Junjie', ''], ['Wang', 'Lan', ''], ['Peng', 'Xinggan', ''], ['Meng', 'Deyu', '']]","extracted_entities":"[{'text': 'IV-tuning', 'label': 'Fine-tuning'}, {'text': 'IV-tuning', 'label': 'Fine-tuning'}, {'text': 'modal prompts', 'label': 'Prompting'}, {'text': 'IV-tuning', 'label': 'Fine-tuning'}, {'text': 'IV-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Prompting","matched_keyword":"modal prompts","similarity_score":0.5987864137}
{"id":2412.18573,"submitter":"Dewu Zheng","authors":"Dewu Zheng, Yanlin Wang, Ensheng Shi, Xilin Liu, Yuchi Ma, Hongyu\n  Zhang, Zibin Zheng","title":"Top General Performance = Top Domain Performance? DomainCodeBench: A\n  Multi-domain Code Generation Benchmark","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  With the rapid advancement of large language models (LLMs), extensive\nresearch has been conducted to investigate the code generation capabilities of\nLLMs. However, existing efforts primarily focus on general-domain tasks,\nleaving LLMs' code generation performance in real-world application domains\nunderexplored. This raises a critical question: can a model's general-domain\ncoding ability reliably represent its ability in specialized domains? In this\npaper, we introduce DomainCodeBench, a multi-domain code generation benchmark\ndesigned to systematically evaluate LLMs across 12 software application domains\nand 15 programming languages. DomainCodeBench contains 2,400 manually verified\ntasks with ground truth, human-annotated docstrings, and fine-grained\ndependency information to ensure more coverage of domain-specific challenges.\nSpecifically, we first identify the most popular application domains by topic\nmining. Then, we curate coding tasks based on commonly used frameworks and\nplatforms in each domain. We obtain several findings through extensive\nexperiments on DomainCodeBench with ten mainstream LLMs. (1) Performance\ndecoupling: experiments reveal that top general-domain models do not\nconsistently excel in specific application domains; (2) Domain-specific\nweaknesses: LLMs often fail due to domain knowledge gaps and third-party\nlibrary misusage; (3) Contextual enhancement: we show that augmenting prompts\nwith domain-specific knowledge improves performance by around 38.17%, providing\nactionable insights for performance optimization. Our replication package,\nincluding the benchmark, source code, and experimental results, is available at\nhttps:\/\/github.com\/DeepSoftwareAnalytics\/DomainCodeBench.\n","versions":"[{'version': 'v1', 'created': 'Tue, 24 Dec 2024 17:56:08 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 17:58:13 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Zheng', 'Dewu', ''], ['Wang', 'Yanlin', ''], ['Shi', 'Ensheng', ''], ['Liu', 'Xilin', ''], ['Ma', 'Yuchi', ''], ['Zhang', 'Hongyu', ''], ['Zheng', 'Zibin', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Contextual enhancement', 'label': 'contextual Embedding'}, {'text': 'prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompts","similarity_score":0.7638334036}
{"id":2501.16022,"submitter":"Subhadeep Koley","authors":"Subhadeep Koley, Viswanatha Reddy Gajjala, Aneeshan Sain, Pinaki Nath\n  Chowdhury, Tao Xiang, Ayan Kumar Bhunia, Yi-Zhe Song","title":"SketchYourSeg: Mask-Free Subjective Image Segmentation via Freehand\n  Sketches","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  We introduce SketchYourSeg, a novel framework that establishes freehand\nsketches as a powerful query modality for subjective image segmentation across\nentire galleries through a single exemplar sketch. Unlike text prompts that\nstruggle with spatial specificity or interactive methods confined to\nsingle-image operations, sketches naturally combine semantic intent with\nstructural precision. This unique dual encoding enables precise visual\ndisambiguation for segmentation tasks where text descriptions would be\ncumbersome or ambiguous -- such as distinguishing between visually similar\ninstances, specifying exact part boundaries, or indicating spatial\nrelationships in composed concepts. Our approach addresses three fundamental\nchallenges: (i) eliminating the need for pixel-perfect annotation masks during\ntraining with a mask-free framework; (ii) creating a synergistic relationship\nbetween sketch-based image retrieval (SBIR) models and foundation models\n(CLIP\/DINOv2) where the former provides training signals while the latter\ngenerates masks; and (iii) enabling multi-granular segmentation capabilities\nthrough purpose-made sketch augmentation strategies. Our extensive evaluations\ndemonstrate superior performance over existing approaches across diverse\nbenchmarks, establishing a new paradigm for user-guided image segmentation that\nbalances precision with efficiency.\n","versions":"[{'version': 'v1', 'created': 'Mon, 27 Jan 2025 13:07:51 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 21:44:42 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Koley', 'Subhadeep', ''], ['Gajjala', 'Viswanatha Reddy', ''], ['Sain', 'Aneeshan', ''], ['Chowdhury', 'Pinaki Nath', ''], ['Xiang', 'Tao', ''], ['Bhunia', 'Ayan Kumar', ''], ['Song', 'Yi-Zhe', '']]","extracted_entities":"[{'text': 'text prompts', 'label': 'Prompting'}, {'text': 'CLIP\/DINOv2', 'label': 'Foundation Model'}]","assigned_concept":"Prompting","matched_keyword":"text prompts","similarity_score":0.6106933355}
{"id":2501.17178,"submitter":"Omar Swelam","authors":"David Salinas, Omar Swelam, Frank Hutter","title":"Tuning LLM Judge Design Decisions for 1\/1000 of the Cost","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Evaluating Large Language Models (LLMs) often requires costly human\nannotations. To address this, LLM-based judges have been proposed, which\ncompare the outputs of two LLMs enabling the ranking of models without human\nintervention. While several approaches have been proposed, many confounding\nfactors are present between different papers. For instance the model, the\nprompt and other hyperparameters are typically changed at the same time making\napple-to-apple comparisons challenging. In this paper, we propose to\nsystematically analyze and tune hyperparameter of LLM judges. To alleviate the\nhigh cost of evaluating a judge, we propose to leverage multi-objective\nmulti-fidelity which allows to find judges that trades accuracy for cost and\nalso reduce significantly the cost of the search. Our method identifies judges\nthat not only outperform existing benchmarks in accuracy and cost-efficiency\nbut also utilize open-weight models, ensuring greater accessibility and\nreproducibility.\n","versions":"[{'version': 'v1', 'created': 'Fri, 24 Jan 2025 17:01:14 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Feb 2025 08:21:00 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 09:09:29 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Salinas', 'David', ''], ['Swelam', 'Omar', ''], ['Hutter', 'Frank', '']]","extracted_entities":"[{'text': 'LLM-based judges', 'label': 'LLM-based'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'open-weight models', 'label': 'Open-source LLMs'}]","assigned_concept":"Prompting","matched_keyword":"prompt","similarity_score":0.7767513394}
{"id":2502.02454,"submitter":"Yuxin Qi","authors":"Quan Zhang, Yuxin Qi, Xi Tang, Jinwei Fang, Xi Lin, Ke Zhang, Chun\n  Yuan","title":"IMDPrompter: Adapting SAM to Image Manipulation Detection by Cross-View\n  Automated Prompt Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Using extensive training data from SA-1B, the Segment Anything Model (SAM)\nhas demonstrated exceptional generalization and zero-shot capabilities,\nattracting widespread attention in areas such as medical image segmentation and\nremote sensing image segmentation. However, its performance in the field of\nimage manipulation detection remains largely unexplored and unconfirmed. There\nare two main challenges in applying SAM to image manipulation detection: a)\nreliance on manual prompts, and b) the difficulty of single-view information in\nsupporting cross-dataset generalization. To address these challenges, we\ndevelops a cross-view prompt learning paradigm called IMDPrompter based on SAM.\nBenefiting from the design of automated prompts, IMDPrompter no longer relies\non manual guidance, enabling automated detection and localization.\nAdditionally, we propose components such as Cross-view Feature Perception,\nOptimal Prompt Selection, and Cross-View Prompt Consistency, which facilitate\ncross-view perceptual learning and guide SAM to generate accurate masks.\nExtensive experimental results from five datasets (CASIA, Columbia, Coverage,\nIMD2020, and NIST16) validate the effectiveness of our proposed method.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Feb 2025 16:20:41 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 09:53:55 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Zhang', 'Quan', ''], ['Qi', 'Yuxin', ''], ['Tang', 'Xi', ''], ['Fang', 'Jinwei', ''], ['Lin', 'Xi', ''], ['Zhang', 'Ke', ''], ['Yuan', 'Chun', '']]","extracted_entities":"[{'text': 'manual prompts', 'label': 'Prompting'}, {'text': 'automated prompts', 'label': 'Prompting'}, {'text': 'Optimal Prompt Selection', 'label': 'Prompting'}, {'text': 'Cross-View Prompt Consistency', 'label': 'Prompting'}, {'text': 'cross-view perceptual learning', 'label': 'Zero-shot Learning'}]","assigned_concept":"Prompting","matched_keyword":"manual prompts","similarity_score":0.701982379}
{"id":2502.1503,"submitter":"Sangwook Lee","authors":"Sangwook Lee, Adnan Abbas, Yan Chen, Sang Won Lee","title":"CHOIR: Chat-based Helper for Organizational Intelligence Repository","comments":"4 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Modern organizations frequently rely on chat-based platforms (e.g., Slack,\nMicrosoft Teams, and Discord) for day-to-day communication and decision-making.\nAs conversations evolve, organizational knowledge can get buried, prompting\nrepeated searches and discussions. While maintaining shared documents, such as\nWiki articles for the organization, offers a partial solution, it requires\nmanual and timely efforts to keep it up to date, and it may not effectively\npreserve the social and contextual aspect of prior discussions. Moreover,\nreaching a consensus on document updates with relevant stakeholders can be\ntime-consuming and complex. To address these challenges, we introduce CHOIR\n(Chat-based Helper for Organizational Intelligence Repository), a chatbot that\nintegrates seamlessly with chat platforms. CHOIR automatically identifies and\nproposes edits to related documents, initiates discussions with relevant team\nmembers, and preserves contextual revision histories. By embedding knowledge\nmanagement directly into chat environments and leveraging LLMs, CHOIR\nsimplifies manual updates and supports consensus-driven editing based on\nmaintained context with revision histories. We plan to design, deploy, and\nevaluate CHOIR in the context of maintaining an organizational memory for a\nresearch lab. We describe the chatbot's motivation, design, and early\nimplementation to show how CHOIR streamlines collaborative document management.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Feb 2025 20:34:41 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 14:33:50 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Lee', 'Sangwook', ''], ['Abbas', 'Adnan', ''], ['Chen', 'Yan', ''], ['Lee', 'Sang Won', '']]","extracted_entities":"[{'text': 'prompting', 'label': 'Prompting'}, {'text': 'CHOIR', 'label': 'ChatGPT'}, {'text': 'chatbot', 'label': 'ChatGPT'}, {'text': 'CHOIR', 'label': 'ChatGPT'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'CHOIR', 'label': 'ChatGPT'}, {'text': 'CHOIR', 'label': 'ChatGPT'}, {'text': 'CHOIR', 'label': 'ChatGPT'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2503.08407,"submitter":"Yansong Guo","authors":"Yansong Guo and Jie Hu and Yansong Qu and Liujuan Cao","title":"WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 13:10:41 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 03:30:29 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Guo', 'Yansong', ''], ['Hu', 'Jie', ''], ['Qu', 'Yansong', ''], ['Cao', 'Liujuan', '']]","extracted_entities":"[{'text': 'user prompts', 'label': 'Prompting'}, {'text': 'publicly available', 'label': 'Open-source LLMs'}]","assigned_concept":"Prompting","matched_keyword":"user prompts","similarity_score":0.6697342396}
{"id":2503.09446,"submitter":"Zhihua Tian","authors":"Zhihua Tian, Sirun Nan, Ming Xu, Shengfang Zhai, Wenjie Qu, Jian Liu,\n  Kui Ren, Ruoxi Jia, Jiaheng Zhang","title":"Sparse Autoencoder as a Zero-Shot Classifier for Concept Erasing in\n  Text-to-Image Diffusion Models","comments":"25 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CR","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Text-to-image (T2I) diffusion models have achieved remarkable progress in\ngenerating high-quality images but also raise people's concerns about\ngenerating harmful or misleading content. While extensive approaches have been\nproposed to erase unwanted concepts without requiring retraining from scratch,\nthey inadvertently degrade performance on normal generation tasks. In this\nwork, we propose Interpret then Deactivate (ItD), a novel framework to enable\nprecise concept removal in T2I diffusion models while preserving overall\nperformance. ItD first employs a sparse autoencoder (SAE) to interpret each\nconcept as a combination of multiple features. By permanently deactivating the\nspecific features associated with target concepts, we repurpose SAE as a\nzero-shot classifier that identifies whether the input prompt includes target\nconcepts, allowing selective concept erasure in diffusion models. Moreover, we\ndemonstrate that ItD can be easily extended to erase multiple concepts without\nrequiring further training. Comprehensive experiments across celebrity\nidentities, artistic styles, and explicit content demonstrate ItD's\neffectiveness in eliminating targeted concepts without interfering with normal\nconcept generation. Additionally, ItD is also robust against adversarial\nprompts designed to circumvent content filters. Code is available at:\nhttps:\/\/github.com\/NANSirun\/Interpret-then-deactivate.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 14:46:40 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 09:12:52 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Tian', 'Zhihua', ''], ['Nan', 'Sirun', ''], ['Xu', 'Ming', ''], ['Zhai', 'Shengfang', ''], ['Qu', 'Wenjie', ''], ['Liu', 'Jian', ''], ['Ren', 'Kui', ''], ['Jia', 'Ruoxi', ''], ['Zhang', 'Jiaheng', '']]","extracted_entities":"[{'text': 'input prompt', 'label': 'Prompting'}, {'text': 'adversarial\\nprompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"input prompt","similarity_score":0.6253764629}
{"id":2503.09516,"submitter":"Bowen Jin","authors":"Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, Jiawei\n  Han","title":"Search-R1: Training LLMs to Reason and Leverage Search Engines with\n  Reinforcement Learning","comments":"16 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.IR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Prompting advanced LLMs with reasoning capabilities during inference to\nuse search engines is not optimal, since the LLM does not learn how to\noptimally interact with the search engine. This paper introduces Search-R1, an\nextension of the DeepSeek-R1 model where the LLM learns -- solely through\nreinforcement learning (RL) -- to autonomously generate (multiple) search\nqueries during step-by-step reasoning with real-time retrieval. Search-R1\noptimizes LLM rollouts with multi-turn search interactions, leveraging\nretrieved token masking for stable RL training and a simple outcome-based\nreward function. Experiments on seven question-answering datasets show that\nSearch-R1 improves performance by 26% (Qwen2.5-7B), 21% (Qwen2.5-3B), and 10%\n(LLaMA3.2-3B) over strong baselines. This paper further provides empirical\ninsights into RL optimization methods, LLM choices, and response length\ndynamics in retrieval-augmented reasoning. The code and model checkpoints are\navailable at https:\/\/github.com\/PeterGriffinJin\/Search-R1.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 16:26:39 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 21:40:12 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Jin', 'Bowen', ''], ['Zeng', 'Hansi', ''], ['Yue', 'Zhenrui', ''], ['Wang', 'Dong', ''], ['Zamani', 'Hamed', ''], ['Han', 'Jiawei', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Prompting', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"Prompting","similarity_score":1.0}
{"id":2503.12731,"submitter":"Haoran Ma","authors":"Haoran Ma, Kaihan Zhang and Jiannan Cai","title":"Navigating Heat Exposure: Simulation of Route Planning Based on Visual\n  Language Model Agents","comments":"10 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Heat exposure significantly influences pedestrian routing behaviors. Existing\nmethods such as agent-based modeling (ABM) and empirical measurements fail to\naccount for individual physiological variations and environmental perception\nmechanisms under thermal stress. This results in a lack of human-centred,\nheat-adaptive routing suggestions. To address these limitations, we propose a\nnovel Vision Language Model (VLM)-driven Persona-Perception-Planning-Memory\n(PPPM) framework that integrating street view imagery and urban network\ntopology to simulate heat-adaptive pedestrian routing. Through structured\nprompt engineering on Gemini-2.0 model, eight distinct heat-sensitive personas\nwere created to model mobility behaviors during heat exposure, with empirical\nvalidation through questionnaire survey. Results demonstrate that simulation\noutputs effectively capture inter-persona variations, achieving high\nsignificant congruence with observed route preferences and highlighting\ndifferences in the factors driving agents decisions. Our framework is highly\ncost-effective, with simulations costing 0.006USD and taking 47.81s per route.\nThis Artificial Intelligence-Generated Content (AIGC) methodology advances\nurban climate adaptation research by enabling high-resolution simulation of\nthermal-responsive mobility patterns, providing actionable insights for\nclimate-resilient urban planning.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 01:49:46 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Ma', 'Haoran', ''], ['Zhang', 'Kaihan', ''], ['Cai', 'Jiannan', '']]","extracted_entities":"[{'text': 'structured\\nprompt engineering', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"structured\nprompt engineering","similarity_score":0.5011398792}
{"id":2503.1278,"submitter":"Chang Liu","authors":"Chang Liu, Bavesh Balaji, Saad Hossain, C Thomas, Kwei-Herng Lai,\n  Raviteja Vemulapalli, Alexander Wong, Sirisha Rambhatla","title":"LangDA: Building Context-Awareness via Language for Domain Adaptive\n  Semantic Segmentation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG eess.IV stat.ML","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Unsupervised domain adaptation for semantic segmentation (DASS) aims to\ntransfer knowledge from a label-rich source domain to a target domain with no\nlabels. Two key approaches in DASS are (1) vision-only approaches using masking\nor multi-resolution crops, and (2) language-based approaches that use generic\nclass-wise prompts informed by target domain (e.g. \"a {snowy} photo of a\n{class}\"). However, the former is susceptible to noisy pseudo-labels that are\nbiased to the source domain. The latter does not fully capture the intricate\nspatial relationships of objects -- key for dense prediction tasks. To this\nend, we propose LangDA. LangDA addresses these challenges by, first, learning\ncontextual relationships between objects via VLM-generated scene descriptions\n(e.g. \"a pedestrian is on the sidewalk, and the street is lined with\nbuildings.\"). Second, LangDA aligns the entire image features with text\nrepresentation of this context-aware scene caption and learns generalized\nrepresentations via text. With this, LangDA sets the new state-of-the-art\nacross three DASS benchmarks, outperforming existing methods by 2.6%, 1.4% and\n3.9%.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 03:33:28 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Liu', 'Chang', ''], ['Balaji', 'Bavesh', ''], ['Hossain', 'Saad', ''], ['Thomas', 'C', ''], ['Lai', 'Kwei-Herng', ''], ['Vemulapalli', 'Raviteja', ''], ['Wong', 'Alexander', ''], ['Rambhatla', 'Sirisha', '']]","extracted_entities":"[{'text': 'class-wise prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"class-wise prompts","similarity_score":0.5211703777}
{"id":2503.12864,"submitter":"Donglai Ma","authors":"Donglai Ma, Xiaoyu Cao, Bo Zeng, Chen Chen, Qiaozhu Zhai, Qing-Shan\n  Jia and Xiaohong Guan","title":"Robust Co-Optimization of Distribution Network Hardening and Mobile\n  Resource Scheduling with Decision-Dependent Uncertainty","comments":"15 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper studies the robust co-planning of proactive network hardening and\nmobile hydrogen energy resources (MHERs) scheduling, which is to enhance the\nresilience of power distribution network (PDN) against the disastrous events. A\ndecision-dependent robust optimization model is formulated with min-max\nresilience constraint and discrete recourse structure, which helps achieve the\nload survivability target considering endogenous uncertainties. Different from\nthe traditional model with a fixed uncertainty set, we adopt a dynamic\nrepresentation that explicitly captures the endogenous uncertainties of network\ncontingency as well as the available hydrogen storage levels of MHERs, which\ninduces a decision-dependent uncertainty (DDU) set. Also, the multi-period\nadaptive routing and energy scheduling of MHERs are modeled as a mixed-integer\nrecourse problem for further decreasing the resilience cost. Then, a nested\nparametric column-and-constraint generation (N-PC&CG) algorithm is customized\nand developed to solve this challenging formulation. By leveraging the\nstructural property of the DDU set as well as the combination of discrete\nrecourse decisions and the corresponding extreme points, we derive a\nstrengthened solution scheme with nontrivial enhancement strategies to realize\nefficient and exact computation. Numerical results on 14-bus test system and\n56-bus real-world distribution network demonstrate the resilience benefits and\neconomical feasibility of the proposed method under different damage severity\nlevels. Moreover, the enhanced N-PC&CG shows a superior solution capability to\nsupport prompt decisions for resilient planning with DDU models.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 06:48:19 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Ma', 'Donglai', ''], ['Cao', 'Xiaoyu', ''], ['Zeng', 'Bo', ''], ['Chen', 'Chen', ''], ['Zhai', 'Qiaozhu', ''], ['Jia', 'Qing-Shan', ''], ['Guan', 'Xiaohong', '']]","extracted_entities":"[{'text': 'MHERs', 'label': 'LLMs'}, {'text': 'prompt decisions', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt decisions","similarity_score":0.6470533609}
{"id":2503.12874,"submitter":"Xiaojun Jia","authors":"Xiaojun Jia, Sensen Gao, Simeng Qin, Ke Ma, Xinfeng Li, Yihao Huang,\n  Wei Dong, Yang Liu, Xiaochun Cao","title":"Evolution-based Region Adversarial Prompt Learning for Robustness\n  Enhancement in Vision-Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large pre-trained vision-language models (VLMs), such as CLIP, demonstrate\nimpressive generalization but remain highly vulnerable to adversarial examples\n(AEs). Previous work has explored robust text prompts through adversarial\ntraining, achieving some improvement in both robustness and generalization.\nHowever, they primarily rely on singlegradient direction perturbations (e.g.,\nPGD) to generate AEs, which lack diversity, resulting in limited improvement in\nadversarial robustness. To address these limitations, we propose an\nevolution-based region adversarial prompt tuning method called ER-APT, which\ncombines gradient methods with genetic evolution to generate more diverse and\nchallenging AEs. In each training iteration, we first generate AEs using\ntraditional gradient-based methods. Subsequently, a genetic evolution mechanism\nincorporating selection, mutation, and crossover is applied to optimize the\nAEs, ensuring a broader and more aggressive perturbation distribution.The final\nevolved AEs are used for prompt tuning, achieving region-based adversarial\noptimization instead of conventional single-point adversarial prompt tuning. We\nalso propose a dynamic loss weighting method to adjust prompt learning\nefficiency for accuracy and robustness. Experimental evaluations on various\nbenchmark datasets demonstrate the superiority of our proposed method,\noutperforming stateof-the-art APT methods. The code is released at\nhttps:\/\/github.com\/jiaxiaojunQAQ\/ER-APT.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 07:08:47 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 02:58:59 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Jia', 'Xiaojun', ''], ['Gao', 'Sensen', ''], ['Qin', 'Simeng', ''], ['Ma', 'Ke', ''], ['Li', 'Xinfeng', ''], ['Huang', 'Yihao', ''], ['Dong', 'Wei', ''], ['Liu', 'Yang', ''], ['Cao', 'Xiaochun', '']]","extracted_entities":"[{'text': 'CLIP', 'label': 'Large Language Model'}, {'text': 'text prompts', 'label': 'Prompting'}, {'text': 'prompt tuning', 'label': 'Fine-tuning'}, {'text': 'prompt learning', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"text prompts","similarity_score":0.6106933355}
{"id":2503.1291,"submitter":"Yuan Jingyi","authors":"Jingyi Yuan, Pengyu Jie, Junyin Zhang, Ziao Li, Chenqiang Gao","title":"MFP-CLIP: Exploring the Efficacy of Multi-Form Prompts for Zero-Shot\n  Industrial Anomaly Detection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recently, zero-shot anomaly detection (ZSAD) has emerged as a pivotal\nparadigm for identifying defects in unseen categories without requiring target\nsamples in training phase. However, existing ZSAD methods struggle with the\nboundary of small and complex defects due to insufficient representations. Most\nof them use the single manually designed prompts, failing to work for diverse\nobjects and anomalies. In this paper, we propose MFP-CLIP, a novel prompt-based\nCLIP framework which explores the efficacy of multi-form prompts for zero-shot\nindustrial anomaly detection. We employ an image to text prompting(I2TP)\nmechanism to better represent the object in the image. MFP-CLIP enhances\nperception to multi-scale and complex anomalies by self prompting(SP) and a\nmulti-patch feature aggregation(MPFA) module. To precisely localize defects, we\nintroduce the mask prompting(MP) module to guide model to focus on potential\nanomaly regions. Extensive experiments are conducted on two wildly used\nindustrial anomaly detection benchmarks, MVTecAD and VisA, demonstrating\nMFP-CLIP's superiority in ZSAD.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 08:18:55 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Yuan', 'Jingyi', ''], ['Jie', 'Pengyu', ''], ['Zhang', 'Junyin', ''], ['Li', 'Ziao', ''], ['Gao', 'Chenqiang', '']]","extracted_entities":"[{'text': 'ZSAD', 'label': 'Zero-shot Learning'}, {'text': 'ZSAD', 'label': 'Zero-shot Learning'}, {'text': 'image to text prompting', 'label': 'Prompting'}, {'text': 'self prompting', 'label': 'Prompting'}, {'text': 'mask prompting', 'label': 'Prompting'}, {'text': 'ZSAD', 'label': 'Zero-shot Learning'}]","assigned_concept":"Prompting","matched_keyword":"self prompting","similarity_score":0.9027125835}
{"id":2503.13013,"submitter":"Stefano Dell'Oro","authors":"L. Gironi, S. Dell'Oro, C. Gotti, N. Manenti, E. Mazzola, M. Nastasi,\n  D. Peracchi","title":"A custom experimental setup for scintillator characterization:\n  application to a Ce-doped GAGG crystal","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.ins-det","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Scintillators are widely used in radiation detection, with continuous\nadvancements enhancing their performance and developing new materials. This\nstudy presents a custom experimental setup for the characterization of crystal\nscintillators under different temperature and pressure conditions. The setup is\nflexible and capable of providing prompt feedback, which is crucial for\nmaterial development. We tested the setup with a Ce-doped Gd3Al2Ga3O12 (GAGG)\nscintillator, evaluating its response to different types of radiation,\nparticularly alpha particles. These results contribute to a deeper\nunderstanding of GAGG's scintillation properties, including light output,\nquenching factors, and pulse-shape discrimination capabilities.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 10:12:16 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Gironi', 'L.', ''], [\"Dell'Oro\", 'S.', ''], ['Gotti', 'C.', ''], ['Manenti', 'N.', ''], ['Mazzola', 'E.', ''], ['Nastasi', 'M.', ''], ['Peracchi', 'D.', '']]","extracted_entities":"[{'text': 'prompt feedback', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt feedback","similarity_score":0.6436466575}
{"id":2503.1307,"submitter":"Yihong Luo","authors":"Yihong Luo, Tianyang Hu, Weijian Luo, Kenji Kawaguchi, Jing Tang","title":"Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Aligning generated images to complicated text prompts and human preferences\nis a central challenge in Artificial Intelligence-Generated Content (AIGC).\nWith reward-enhanced diffusion distillation emerging as a promising approach\nthat boosts controllability and fidelity of text-to-image models, we identify a\nfundamental paradigm shift: as conditions become more specific and reward\nsignals stronger, the rewards themselves become the dominant force in\ngeneration. In contrast, the diffusion losses serve as an overly expensive form\nof regularization. To thoroughly validate our hypothesis, we introduce R0, a\nnovel conditional generation approach via regularized reward maximization.\nInstead of relying on tricky diffusion distillation losses, R0 proposes a new\nperspective that treats image generations as an optimization problem in data\nspace which aims to search for valid images that have high compositional\nrewards. By innovative designs of the generator parameterization and proper\nregularization techniques, we train state-of-the-art few-step text-to-image\ngenerative models with R0 at scales. Our results challenge the conventional\nwisdom of diffusion post-training and conditional generation by demonstrating\nthat rewards play a dominant role in scenarios with complex conditions. We hope\nour findings can contribute to further research into human-centric and\nreward-centric generation paradigms across the broader field of AIGC. Code is\navailable at https:\/\/github.com\/Luo-Yihong\/R0.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 11:21:43 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Luo', 'Yihong', ''], ['Hu', 'Tianyang', ''], ['Luo', 'Weijian', ''], ['Kawaguchi', 'Kenji', ''], ['Tang', 'Jing', '']]","extracted_entities":"[{'text': 'complicated text prompts', 'label': 'Prompting'}, {'text': 'reward-enhanced diffusion distillation', 'label': 'Knowledge distillation'}, {'text': 'diffusion distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Prompting","matched_keyword":"complicated text prompts","similarity_score":0.5908828974}
{"id":2503.13225,"submitter":"Santiago Vall\\'es-Sanclemente","authors":"S. Vall\\'es-Sanclemente, T. H. F. Vroomans, T. R. van Abswoude, F.\n  Brulleman, T. Stavenga, S. L. M. van der Meer, Y. Xin, A. Lawrence, V. Singh,\n  M. A. Rol and L. DiCarlo","title":"Optimizing the frequency positioning of tunable couplers in a circuit\n  QED processor to mitigate spectator effects on quantum operations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We experimentally optimize the frequency of flux-tunable couplers in a\nsuperconducting quantum processor to minimize the impact of spectator transmons\nduring quantum operations (single-qubit gates, two-qubit gates and readout) on\nother transmons. We adapt a popular transmon-like tunable-coupling element,\nachieving high-fidelity, low-leakage controlled-$Z$ gates with unipolar,\nfast-adiabatic pulsing only on the coupler. We demonstrate the ability of the\ntunable coupler to null residual $ZZ$ coupling as well as exchange couplings in\nthe one- and two-excitation manifolds. However, the nulling of these coherent\ninteractions is not simultaneous, prompting the exploration of tradeoffs. We\npresent experiments pinpointing spectator effects on specific quantum\noperations. We also study the combined effect on the three types of operations\nusing repeated quantum parity measurements.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 14:41:05 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Vall\u00e9s-Sanclemente', 'S.', ''], ['Vroomans', 'T. H. F.', ''], ['van Abswoude', 'T. R.', ''], ['Brulleman', 'F.', ''], ['Stavenga', 'T.', ''], ['van der Meer', 'S. L. M.', ''], ['Xin', 'Y.', ''], ['Lawrence', 'A.', ''], ['Singh', 'V.', ''], ['Rol', 'M. A.', ''], ['DiCarlo', 'L.', '']]","extracted_entities":"[{'text': 'quantum operations', 'label': 'quantisation'}, {'text': 'single-qubit gates', 'label': 'quantisation'}, {'text': 'two-qubit gates', 'label': 'quantisation'}, {'text': 'readout', 'label': 'quantisation'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'quantum\\noperations', 'label': 'quantisation'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2503.13327,"submitter":"Lan Chen","authors":"Lan Chen, Qi Mao, Yuchao Gu, Mike Zheng Shou","title":"Edit Transfer: Learning Image Editing via Vision In-Context Relations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We introduce a new setting, Edit Transfer, where a model learns a\ntransformation from just a single source-target example and applies it to a new\nquery image. While text-based methods excel at semantic manipulations through\ntextual prompts, they often struggle with precise geometric details (e.g.,\nposes and viewpoint changes). Reference-based editing, on the other hand,\ntypically focuses on style or appearance and fails at non-rigid\ntransformations. By explicitly learning the editing transformation from a\nsource-target pair, Edit Transfer mitigates the limitations of both text-only\nand appearance-centric references. Drawing inspiration from in-context learning\nin large language models, we propose a visual relation in-context learning\nparadigm, building upon a DiT-based text-to-image model. We arrange the edited\nexample and the query image into a unified four-panel composite, then apply\nlightweight LoRA fine-tuning to capture complex spatial transformations from\nminimal examples. Despite using only 42 training samples, Edit Transfer\nsubstantially outperforms state-of-the-art TIE and RIE methods on diverse\nnon-rigid scenarios, demonstrating the effectiveness of few-shot visual\nrelation learning.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 16:04:44 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Chen', 'Lan', ''], ['Mao', 'Qi', ''], ['Gu', 'Yuchao', ''], ['Shou', 'Mike Zheng', '']]","extracted_entities":"[{'text': 'textual prompts', 'label': 'Prompting'}, {'text': 'in-context learning', 'label': 'Few-shot Learning'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}, {'text': 'lightweight LoRA fine-tuning', 'label': 'Fine-tuning'}, {'text': 'few-shot visual\\nrelation learning', 'label': 'Zero-shot Learning'}]","assigned_concept":"Prompting","matched_keyword":"textual prompts","similarity_score":0.6302489042}
{"id":2503.13445,"submitter":"Noah Siegel","authors":"Noah Y. Siegel, Nicolas Heess, Maria Perez-Ortiz, Oana-Maria Camburu","title":"Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is\n  Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance","comments":"38 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  As large language models (LLMs) become increasingly capable, ensuring that\ntheir self-generated explanations are faithful to their internal\ndecision-making process is critical for safety and oversight. In this work, we\nconduct a comprehensive counterfactual faithfulness analysis across 62 models\nfrom 8 families, encompassing both pretrained and instruction-tuned variants\nand significantly extending prior studies of counterfactual tests. We introduce\nphi-CCT, a simplified variant of the Correlational Counterfactual Test, which\navoids the need for token probabilities while explaining most of the variance\nof the original test. Our findings reveal clear scaling trends: larger models\nare consistently more faithful on our metrics. However, when comparing\ninstruction-tuned and human-imitated explanations, we find that observed\ndifferences in faithfulness can often be attributed to explanation verbosity,\nleading to shifts along the true-positive\/false-positive Pareto frontier. While\ninstruction-tuning and prompting can influence this trade-off, we find limited\nevidence that they fundamentally expand the frontier of explanatory\nfaithfulness beyond what is achievable with pretrained models of comparable\nsize. Our analysis highlights the nuanced relationship between\ninstruction-tuning, verbosity, and the faithful representation of model\ndecision processes.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:59:39 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Siegel', 'Noah Y.', ''], ['Heess', 'Nicolas', ''], ['Perez-Ortiz', 'Maria', ''], ['Camburu', 'Oana-Maria', '']]","extracted_entities":"[{'text': 'scaling trends', 'label': 'Scaling law'}, {'text': 'instruction-tuning', 'label': 'Fine-tuning'}, {'text': 'prompting', 'label': 'Prompting'}, {'text': 'instruction-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2503.13576,"submitter":"Ziqiang Li","authors":"Ziqiang Li, Jun Li, Lizhi Xiong, Zhangjie Fu, Zechao Li","title":"A Comprehensive Survey on Visual Concept Mining in Text-to-image\n  Diffusion Models","comments":"Under review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Text-to-image diffusion models have made significant advancements in\ngenerating high-quality, diverse images from text prompts. However, the\ninherent limitations of textual signals often prevent these models from fully\ncapturing specific concepts, thereby reducing their controllability. To address\nthis issue, several approaches have incorporated personalization techniques,\nutilizing reference images to mine visual concept representations that\ncomplement textual inputs and enhance the controllability of text-to-image\ndiffusion models. Despite these advances, a comprehensive, systematic\nexploration of visual concept mining remains limited. In this paper, we\ncategorize existing research into four key areas: Concept Learning, Concept\nErasing, Concept Decomposition, and Concept Combination. This classification\nprovides valuable insights into the foundational principles of Visual Concept\nMining (VCM) techniques. Additionally, we identify key challenges and propose\nfuture research directions to propel this important and interesting field\nforward.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 13:51:56 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Li', 'Ziqiang', ''], ['Li', 'Jun', ''], ['Xiong', 'Lizhi', ''], ['Fu', 'Zhangjie', ''], ['Li', 'Zechao', '']]","extracted_entities":"[{'text': 'text prompts', 'label': 'Prompting'}, {'text': 'Concept Learning', 'label': 'Few-shot Learning'}, {'text': 'Concept\\nErasing', 'label': 'Few-shot Learning'}, {'text': 'Concept Decomposition', 'label': 'Few-shot Learning'}]","assigned_concept":"Prompting","matched_keyword":"text prompts","similarity_score":0.6106933355}
{"id":2503.13652,"submitter":"Maan Qraitem","authors":"Maan Qraitem, Piotr Teterwak, Kate Saenko, Bryan A. Plummer","title":"Web Artifact Attacks Disrupt Vision Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Vision-language models (VLMs) (e.g., CLIP, LLaVA) are trained on large-scale,\nlightly curated web datasets, leading them to learn unintended correlations\nbetween semantic concepts and unrelated visual signals. These associations\ndegrade model accuracy by causing predictions to rely on incidental patterns\nrather than genuine visual understanding. Prior work has weaponized these\ncorrelations as an attack vector to manipulate model predictions, such as\ninserting a deceiving class text onto the image in a typographic attack. These\nattacks succeed due to VLMs' text-heavy bias-a result of captions that echo\nvisible words rather than describing content. However, this attack has focused\nsolely on text that matches the target class exactly, overlooking a broader\nrange of correlations, including non-matching text and graphical symbols, which\narise from the abundance of branding content in web-scale data. To address this\ngap, we introduce artifact-based attacks: a novel class of manipulations that\nmislead models using both non-matching text and graphical elements. Unlike\ntypographic attacks, these artifacts are not predefined, making them harder to\ndefend against but also more challenging to find. We address this by framing\nartifact attacks as a search problem and demonstrate their effectiveness across\nfive datasets, with some artifacts reinforcing each other to reach 100% attack\nsuccess rates. These attacks transfer across models with up to 90%\neffectiveness, making it possible to attack unseen models. To defend against\nthese attacks, we extend prior work's artifact aware prompting to the graphical\nsetting. We see a moderate reduction of success rates of up to 15% relative to\nstandard prompts, suggesting a promising direction for enhancing model\nrobustness.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 18:59:29 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Qraitem', 'Maan', ''], ['Teterwak', 'Piotr', ''], ['Saenko', 'Kate', ''], ['Plummer', 'Bryan A.', '']]","extracted_entities":"[{'text': 'artifact aware prompting', 'label': 'Prompting'}, {'text': 'standard prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"standard prompts","similarity_score":0.6111004353}
{"id":2503.1373,"submitter":"Forouzan Fallah","authors":"Forouzan Fallah, Maitreya Patel, Agneet Chatterjee, Vlad I. Morariu,\n  Chitta Baral, Yezhou Yang","title":"TextInVision: Text and Prompt Complexity Driven Visual Text Generation\n  Benchmark","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Generating images with embedded text is crucial for the automatic production\nof visual and multimodal documents, such as educational materials and\nadvertisements. However, existing diffusion-based text-to-image models often\nstruggle to accurately embed text within images, facing challenges in spelling\naccuracy, contextual relevance, and visual coherence. Evaluating the ability of\nsuch models to embed text within a generated image is complicated due to the\nlack of comprehensive benchmarks. In this work, we introduce TextInVision, a\nlarge-scale, text and prompt complexity driven benchmark designed to evaluate\nthe ability of diffusion models to effectively integrate visual text into\nimages. We crafted a diverse set of prompts and texts that consider various\nattributes and text characteristics. Additionally, we prepared an image dataset\nto test Variational Autoencoder (VAE) models across different character\nrepresentations, highlighting that VAE architectures can also pose challenges\nin text generation within diffusion frameworks. Through extensive analysis of\nmultiple models, we identify common errors and highlight issues such as\nspelling inaccuracies and contextual mismatches. By pinpointing the failure\npoints across different prompts and texts, our research lays the foundation for\nfuture advancements in AI-generated multimodal content.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 21:36:31 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Fallah', 'Forouzan', ''], ['Patel', 'Maitreya', ''], ['Chatterjee', 'Agneet', ''], ['Morariu', 'Vlad I.', ''], ['Baral', 'Chitta', ''], ['Yang', 'Yezhou', '']]","extracted_entities":"[{'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompts","similarity_score":0.7638334036}
{"id":2503.13737,"submitter":"Tanmoy Sen","authors":"Haiying Shen, Tanmoy Sen","title":"AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 21:47:43 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Shen', 'Haiying', ''], ['Sen', 'Tanmoy', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'short prompts', 'label': 'Prompting'}, {'text': 'long prompts', 'label': 'Prompting'}, {'text': 'long prompts', 'label': 'Prompting'}, {'text': 'AccelGen', 'label': 'LLM-based'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'AccelGen', 'label': 'LLM-based'}, {'text': 'AccelGen', 'label': 'LLM-based'}]","assigned_concept":"Prompting","matched_keyword":"short prompts","similarity_score":0.6337689757}
{"id":2503.13814,"submitter":"Jinping Wang","authors":"Jinping Wang, Weiwei Song, Hao Chen, Jinchang Ren and Huimin Zhao","title":"FusDreamer: Label-efficient Remote Sensing World Model for Multimodal\n  Data Classification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  World models significantly enhance hierarchical understanding, improving data\nintegration and learning efficiency. To explore the potential of the world\nmodel in the remote sensing (RS) field, this paper proposes a label-efficient\nremote sensing world model for multimodal data fusion (FusDreamer). The\nFusDreamer uses the world model as a unified representation container to\nabstract common and high-level knowledge, promoting interactions across\ndifferent types of data, \\emph{i.e.}, hyperspectral (HSI), light detection and\nranging (LiDAR), and text data. Initially, a new latent diffusion fusion and\nmultimodal generation paradigm (LaMG) is utilized for its exceptional\ninformation integration and detail retention capabilities. Subsequently, an\nopen-world knowledge-guided consistency projection (OK-CP) module incorporates\nprompt representations for visually described objects and aligns\nlanguage-visual features through contrastive learning. In this way, the domain\ngap can be bridged by fine-tuning the pre-trained world models with limited\nsamples. Finally, an end-to-end multitask combinatorial optimization (MuCO)\nstrategy can capture slight feature bias and constrain the diffusion process in\na collaboratively learnable direction. Experiments conducted on four typical\ndatasets indicate the effectiveness and advantages of the proposed FusDreamer.\nThe corresponding code will be released at\nhttps:\/\/github.com\/Cimy-wang\/FusDreamer.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 01:45:51 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wang', 'Jinping', ''], ['Song', 'Weiwei', ''], ['Chen', 'Hao', ''], ['Ren', 'Jinchang', ''], ['Zhao', 'Huimin', '']]","extracted_entities":"[{'text': 'prompt representations', 'label': 'Prompting'}, {'text': 'contrastive learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Prompting","matched_keyword":"prompt representations","similarity_score":0.5491349101}
{"id":2503.13938,"submitter":"Qingyao Xu","authors":"Qingyao Xu, Siheng Chen, Guang Chen, Yanfeng Wang, Ya Zhang","title":"ChatBEV: A Visual Language Model that Understands BEV Maps","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Traffic scene understanding is essential for intelligent transportation\nsystems and autonomous driving, ensuring safe and efficient vehicle operation.\nWhile recent advancements in VLMs have shown promise for holistic scene\nunderstanding, the application of VLMs to traffic scenarios, particularly using\nBEV maps, remains under explored. Existing methods often suffer from limited\ntask design and narrow data amount, hindering comprehensive scene\nunderstanding. To address these challenges, we introduce ChatBEV-QA, a novel\nBEV VQA benchmark contains over 137k questions, designed to encompass a wide\nrange of scene understanding tasks, including global scene understanding,\nvehicle-lane interactions, and vehicle-vehicle interactions. This benchmark is\nconstructed using an novel data collection pipeline that generates scalable and\ninformative VQA data for BEV maps. We further fine-tune a specialized\nvision-language model ChatBEV, enabling it to interpret diverse question\nprompts and extract relevant context-aware information from BEV maps.\nAdditionally, we propose a language-driven traffic scene generation pipeline,\nwhere ChatBEV facilitates map understanding and text-aligned navigation\nguidance, significantly enhancing the generation of realistic and consistent\ntraffic scenarios. The dataset, code and the fine-tuned model will be released.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 06:12:38 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Xu', 'Qingyao', ''], ['Chen', 'Siheng', ''], ['Chen', 'Guang', ''], ['Wang', 'Yanfeng', ''], ['Zhang', 'Ya', '']]","extracted_entities":"[{'text': 'ChatBEV-QA', 'label': 'ChatGPT'}, {'text': 'ChatBEV', 'label': 'ChatGPT'}, {'text': 'question\\nprompts', 'label': 'Prompting'}, {'text': 'ChatBEV', 'label': 'ChatGPT'}]","assigned_concept":"Prompting","matched_keyword":"question\nprompts","similarity_score":0.6667648554}
{"id":2503.13945,"submitter":"Long Tang","authors":"Long Tang, Dengpan Ye, Sirun Chen, Xiuwen Shi, Yunna Lv, Ziyi Liu","title":"Make the Most of Everything: Further Considerations on Disrupting\n  Diffusion-based Customization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The fine-tuning technique for text-to-image diffusion models facilitates\nimage customization but risks privacy breaches and opinion manipulation.\nCurrent research focuses on prompt- or image-level adversarial attacks for\nanti-customization, yet it overlooks the correlation between these two levels\nand the relationship between internal modules and inputs. This hinders\nanti-customization performance in practical threat scenarios. We propose Dual\nAnti-Diffusion (DADiff), a two-stage adversarial attack targeting diffusion\ncustomization, which, for the first time, integrates the adversarial\nprompt-level attack into the generation process of image-level adversarial\nexamples. In stage 1, we generate prompt-level adversarial vectors to guide the\nsubsequent image-level attack. In stage 2, besides conducting the end-to-end\nattack on the UNet model, we disrupt its self- and cross-attention modules,\naiming to break the correlations between image pixels and align the\ncross-attention results computed using instance prompts and adversarial prompt\nvectors within the images. Furthermore, we introduce a local random timestep\ngradient ensemble strategy, which updates adversarial perturbations by\nintegrating random gradients from multiple segmented timesets. Experimental\nresults on various mainstream facial datasets demonstrate 10%-30% improvements\nin cross-prompt, keyword mismatch, cross-model, and cross-mechanism\nanti-customization with DADiff compared to existing methods.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 06:22:03 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Tang', 'Long', ''], ['Ye', 'Dengpan', ''], ['Chen', 'Sirun', ''], ['Shi', 'Xiuwen', ''], ['Lv', 'Yunna', ''], ['Liu', 'Ziyi', '']]","extracted_entities":"[{'text': 'instance prompts', 'label': 'Prompting'}, {'text': 'adversarial prompt\\nvectors', 'label': 'Prompting'}, {'text': 'cross-prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"cross-prompt","similarity_score":0.5642796159}
{"id":2503.14023,"submitter":"Mihai Nadas","authors":"Mihai Nadas, Laura Diosan, and Andreea Tomescu","title":"Synthetic Data Generation Using Large Language Models: Advances in Text\n  and Code","comments":"21 pages, 3 tables, 64 references, preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs) have unlocked new possibilities for generating\nsynthetic training data in both natural language and code. By producing\nartificial but task-relevant examples, these models can significantly augment\nor even replace real-world datasets, especially when labeled data is scarce or\nsensitive. This paper surveys recent advances in using LLMs to create synthetic\ntext and code, emphasizing prompt-based generation, retrieval-augmented\npipelines, and iterative self-refinement. We show how these methods enrich\nlow-resource tasks such as classification and question answering, as well as\ncode-centric applications such as instruction tuning, code translation, and bug\nrepair, by enabling automated verification of functional correctness. Alongside\npotential benefits like cost-effectiveness, broad coverage, and controllable\ndiversity, we address challenges such as factual inaccuracies in generated\ntext, lack of stylistic realism, and the risk of bias amplification. Proposed\nmitigations include filtering and weighting outputs and reinforcement learning\nwith execution feedback for code. We conclude with open research directions\nlike automated prompt engineering, cross-modal data synthesis, and robust\nevaluation frameworks, highlighting the importance of LLM-generated synthetic\ndata in advancing AI while emphasizing ethical and quality safeguards.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 08:34:03 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Nadas', 'Mihai', ''], ['Diosan', 'Laura', ''], ['Tomescu', 'Andreea', '']]","extracted_entities":"[{'text': 'automated prompt engineering', 'label': 'Prompting'}, {'text': 'ethical and quality safeguards', 'label': 'AI Ethics'}]","assigned_concept":"Prompting","matched_keyword":"automated prompt engineering","similarity_score":0.5748695731}
{"id":2503.14064,"submitter":"Xinhao Xiang","authors":"Xinhao Xiang, Xiao Liu, Zizhong Li, Zhuosheng Liu, Jiawei Zhang","title":"AIGVE-Tool: AI-Generated Video Evaluation Toolkit with Multifaceted\n  Benchmark","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The rapid advancement in AI-generated video synthesis has led to a growth\ndemand for standardized and effective evaluation metrics. Existing metrics lack\na unified framework for systematically categorizing methodologies, limiting a\nholistic understanding of the evaluation landscape. Additionally, fragmented\nimplementations and the absence of standardized interfaces lead to redundant\nprocessing overhead. Furthermore, many prior approaches are constrained by\ndataset-specific dependencies, limiting their applicability across diverse\nvideo domains. To address these challenges, we introduce AIGVE-Tool\n(AI-Generated Video Evaluation Toolkit), a unified framework that provides a\nstructured and extensible evaluation pipeline for a comprehensive AI-generated\nvideo evaluation. Organized within a novel five-category taxonomy, AIGVE-Tool\nintegrates multiple evaluation methodologies while allowing flexible\ncustomization through a modular configuration system. Additionally, we propose\nAIGVE-Bench, a large-scale benchmark dataset created with five SOTA video\ngeneration models based on hand-crafted instructions and prompts. This dataset\nsystematically evaluates various video generation models across nine critical\nquality dimensions. Extensive experiments demonstrate the effectiveness of\nAIGVE-Tool in providing standardized and reliable evaluation results,\nhighlighting specific strengths and limitations of current models and\nfacilitating the advancements of next-generation AI-generated video techniques.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 09:36:33 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Xiang', 'Xinhao', ''], ['Liu', 'Xiao', ''], ['Li', 'Zizhong', ''], ['Liu', 'Zhuosheng', ''], ['Zhang', 'Jiawei', '']]","extracted_entities":"[{'text': 'hand-crafted instructions and prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"hand-crafted instructions and prompts","similarity_score":0.5589180589}
{"id":2503.14082,"submitter":"Bernadette Maria Rebeiro","authors":"B. M. Rebeiro, J.-C. Thomas and B. Blank","title":"Superallowed $0^+ \\rightarrow 0^+$ $\\beta$ decay studies at GANIL and\n  upcoming opportunities with DESIR and S$^3$-LEB","comments":"14 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"nucl-ex hep-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Corrected transition rates ($\\mathcal{F}t^{0^+ \\rightarrow 0^+}$) of $0^+\n\\rightarrow 0^+$ superallowed $\\beta$ decays currently give the most precise\nvalue of $V_{ud}$, the dominant term of the Cabibbo-Kobayashi-Maskawa (CKM)\nquark mixing matrix. By setting stringent constrains on the CKM unitarity,\nthese decays allow probing physics beyond the Standard Model in the electroweak\nsector. A recent global reevaluation of the $\\mathcal{F}t^{0^+ \\rightarrow\n0^+}$ values has indicated a violation of CKM unitarity prompting reassessment\nof the theoretical radiative and isospin symmetry breaking corrections applied\non the experimental transition rates $ft$. In this article we briefly discuss\nthis current situation and the experimental program at GANIL geared towards\nconstraining isospin symmetry breaking corrections. We conclude by presenting\nthe opportunities that will be available at DESIR and S$^3$-LEB, the upcoming\nlow-energy radioactive ion beam facilities at GANIL.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 10:01:17 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Rebeiro', 'B. M.', ''], ['Thomas', 'J. -C.', ''], ['Blank', 'B.', '']]","extracted_entities":"[{'text': 'prompting', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2503.14358,"submitter":"Chao Wang","authors":"Chao Wang, Giulio Franzese, Alessandro Finamore, Pietro Michiardi","title":"RFMI: Estimating Mutual Information on Rectified Flow for Text-to-Image\n  Alignment","comments":"to appear at ICLR 2025 Workshop on Deep Generative Model in Machine\n  Learning: Theory, Principle and Efficacy","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Rectified Flow (RF) models trained with a Flow matching framework have\nachieved state-of-the-art performance on Text-to-Image (T2I) conditional\ngeneration. Yet, multiple benchmarks show that synthetic images can still\nsuffer from poor alignment with the prompt, i.e., images show wrong attribute\nbinding, subject positioning, numeracy, etc. While the literature offers many\nmethods to improve T2I alignment, they all consider only Diffusion Models, and\nrequire auxiliary datasets, scoring models, and linguistic analysis of the\nprompt. In this paper we aim to address these gaps. First, we introduce RFMI, a\nnovel Mutual Information (MI) estimator for RF models that uses the pre-trained\nmodel itself for the MI estimation. Then, we investigate a self-supervised\nfine-tuning approach for T2I alignment based on RFMI that does not require\nauxiliary information other than the pre-trained model itself. Specifically, a\nfine-tuning set is constructed by selecting synthetic images generated from the\npre-trained RF model and having high point-wise MI between images and prompts.\nOur experiments on MI estimation benchmarks demonstrate the validity of RFMI,\nand empirical fine-tuning on SD3.5-Medium confirms the effectiveness of RFMI\nfor improving T2I alignment while maintaining image quality.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 15:41:45 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wang', 'Chao', ''], ['Franzese', 'Giulio', ''], ['Finamore', 'Alessandro', ''], ['Michiardi', 'Pietro', '']]","extracted_entities":"[{'text': 'prompt', 'label': 'Prompting'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'empirical fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Prompting","matched_keyword":"prompt","similarity_score":0.7767513394}
{"id":2503.14482,"submitter":"Yulin Pan","authors":"Yulin Pan, Xiangteng He, Chaojie Mao, Zhen Han, Zeyinzi Jiang,\n  Jingfeng Zhang, Yu Liu","title":"ICE-Bench: A Unified and Comprehensive Benchmark for Image Creating and\n  Editing","comments":"17 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Image generation has witnessed significant advancements in the past few\nyears. However, evaluating the performance of image generation models remains a\nformidable challenge. In this paper, we propose ICE-Bench, a unified and\ncomprehensive benchmark designed to rigorously assess image generation models.\nIts comprehensiveness could be summarized in the following key features: (1)\nCoarse-to-Fine Tasks: We systematically deconstruct image generation into four\ntask categories: No-ref\/Ref Image Creating\/Editing, based on the presence or\nabsence of source images and reference images. And further decompose them into\n31 fine-grained tasks covering a broad spectrum of image generation\nrequirements, culminating in a comprehensive benchmark. (2) Multi-dimensional\nMetrics: The evaluation framework assesses image generation capabilities across\n6 dimensions: aesthetic quality, imaging quality, prompt following, source\nconsistency, reference consistency, and controllability. 11 metrics are\nintroduced to support the multi-dimensional evaluation. Notably, we introduce\nVLLM-QA, an innovative metric designed to assess the success of image editing\nby leveraging large models. (3) Hybrid Data: The data comes from real scenes\nand virtual generation, which effectively improves data diversity and\nalleviates the bias problem in model evaluation. Through ICE-Bench, we conduct\na thorough analysis of existing generation models, revealing both the\nchallenging nature of our benchmark and the gap between current model\ncapabilities and real-world generation requirements. To foster further\nadvancements in the field, we will open-source ICE-Bench, including its\ndataset, evaluation code, and models, thereby providing a valuable resource for\nthe research community.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:53:29 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Pan', 'Yulin', ''], ['He', 'Xiangteng', ''], ['Mao', 'Chaojie', ''], ['Han', 'Zhen', ''], ['Jiang', 'Zeyinzi', ''], ['Zhang', 'Jingfeng', ''], ['Liu', 'Yu', '']]","extracted_entities":"[{'text': 'prompt following', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt following","similarity_score":0.7184784412}
{"id":2503.14503,"submitter":"Kangfu Mei","authors":"Kangfu Mei, Hossein Talebi, Mojtaba Ardakani, Vishal M. Patel, Peyman\n  Milanfar, Mauricio Delbracio","title":"The Power of Context: How Multimodality Improves Image Super-Resolution","comments":"accepted by CVPR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Single-image super-resolution (SISR) remains challenging due to the inherent\ndifficulty of recovering fine-grained details and preserving perceptual quality\nfrom low-resolution inputs. Existing methods often rely on limited image\npriors, leading to suboptimal results. We propose a novel approach that\nleverages the rich contextual information available in multiple modalities --\nincluding depth, segmentation, edges, and text prompts -- to learn a powerful\ngenerative prior for SISR within a diffusion model framework. We introduce a\nflexible network architecture that effectively fuses multimodal information,\naccommodating an arbitrary number of input modalities without requiring\nsignificant modifications to the diffusion process. Crucially, we mitigate\nhallucinations, often introduced by text prompts, by using spatial information\nfrom other modalities to guide regional text-based conditioning. Each\nmodality's guidance strength can also be controlled independently, allowing\nsteering outputs toward different directions, such as increasing bokeh through\ndepth or adjusting object prominence via segmentation. Extensive experiments\ndemonstrate that our model surpasses state-of-the-art generative SISR methods,\nachieving superior visual quality and fidelity. See project page at\nhttps:\/\/mmsr.kfmei.com\/.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:59:54 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Mei', 'Kangfu', ''], ['Talebi', 'Hossein', ''], ['Ardakani', 'Mojtaba', ''], ['Patel', 'Vishal M.', ''], ['Milanfar', 'Peyman', ''], ['Delbracio', 'Mauricio', '']]","extracted_entities":"[{'text': 'text prompts', 'label': 'Prompting'}, {'text': 'text prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"text prompts","similarity_score":0.6106933355}
{"id":2503.14588,"submitter":"Brendan O'Connor","authors":"Malte Busmann, Brendan O'Connor, Julian Sommer, Daniel Gruen, Paz\n  Beniamini, Ramandeep Gill, Michael J. Moss, Antonella Palmese, Arno Riffeser,\n  Yu-Han Yang, Eleonora Troja, Simone Dichiara, Roberto Ricci, Noel Klingler,\n  Claus G\\\"ossl, Lei Hu, Arne Rau, Christoph Ries, Geoffrey Ryan, Michael\n  Schmidt, Muskan Yadav, Gregory R. Zeimann","title":"The curious case of EP241021a: Unraveling the mystery of its exceptional\n  rebrightening","comments":"Submitted to A&A","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.HE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Fast X-ray Transients (FXTs) are a rare and poorly understood phenomenon with\na variety of possible progenitors. The launch of the Einstein Probe (EP)\nmission has facilitated a rapid increase in the real-time discovery and\nfollow-up of FXTs. We focus on the recent EP discovered transient EP241021a,\nwhich shows a peculiar panchromatic behavior. We obtained optical and\nnear-infrared multi-band imaging and spectroscopy with the Fraunhofer Telescope\nat Wendelstein Observatory, the Hobby-Eberly Telescope, and the Very Large\nTelescope over the first 100 days of its evolution. EP241021a was discovered by\nEP as a soft X-ray trigger, but was not detected at gamma-ray frequencies. The\nobserved soft X-ray prompt emission spectrum is consistent with non-thermal\nradiation, which requires at least a mildly relativistic outflow with bulk\nLorentz factor $\\Gamma \\gtrsim 4$. The optical and near-infrared lightcurve has\na two component behavior where an initially fading component $\\sim t^{-1}$\nturns to a rise steeper than $\\sim t^{4}$ after a few days before peaking at\n$M_r\\approx -22$ mag and quickly returning to the initial decay. The peak\nabsolute magnitude is the most luminous optical emission associated to an FXT,\nsuperseding EP240414a. Standard supernova models are unable to reproduce either\nthe absolute magnitude or rapid timescale ($<2$ d) of the rebrightening. The\nX-ray, optical and near-infrared spectral energy distributions display a red\ncolor $r-J\\approx 1$ mag, and point to a non-thermal origin ($\\nu^{-1}$) for\nthe broadband emission. By considering a gamma-ray burst as a plausible\nscenario, we favor a refreshed shock as the cause of the rebrightening. This is\nconsistent with the inference of an at least mildly relativistic outflow based\non the prompt trigger. Our results suggest a likely link between EP discovered\nFXTs and low luminosity gamma-ray bursts.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 18:00:02 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Busmann', 'Malte', ''], [\"O'Connor\", 'Brendan', ''], ['Sommer', 'Julian', ''], ['Gruen', 'Daniel', ''], ['Beniamini', 'Paz', ''], ['Gill', 'Ramandeep', ''], ['Moss', 'Michael J.', ''], ['Palmese', 'Antonella', ''], ['Riffeser', 'Arno', ''], ['Yang', 'Yu-Han', ''], ['Troja', 'Eleonora', ''], ['Dichiara', 'Simone', ''], ['Ricci', 'Roberto', ''], ['Klingler', 'Noel', ''], ['G\u00f6ssl', 'Claus', ''], ['Hu', 'Lei', ''], ['Rau', 'Arne', ''], ['Ries', 'Christoph', ''], ['Ryan', 'Geoffrey', ''], ['Schmidt', 'Michael', ''], ['Yadav', 'Muskan', ''], ['Zeimann', 'Gregory R.', '']]","extracted_entities":"[{'text': 'EP241021a', 'label': 'Prompting'}, {'text': 'soft X-ray trigger', 'label': 'Prompting'}, {'text': 'prompt trigger', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt trigger","similarity_score":0.5806343555}
{"id":2503.14607,"submitter":"Shuo Xing","authors":"Shuo Xing, Zezhou Sun, Shuangyu Xie, Kaiyuan Chen, Yanjia Huang,\n  Yuping Wang, Jiachen Li, Dezhen Song, Zhengzhong Tu","title":"Can Large Vision Language Models Read Maps Like a Human?","comments":"35 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this paper, we introduce MapBench-the first dataset specifically designed\nfor human-readable, pixel-based map-based outdoor navigation, curated from\ncomplex path finding scenarios. MapBench comprises over 1600 pixel space map\npath finding problems from 100 diverse maps. In MapBench, LVLMs generate\nlanguage-based navigation instructions given a map image and a query with\nbeginning and end landmarks. For each map, MapBench provides Map Space Scene\nGraph (MSSG) as an indexing data structure to convert between natural language\nand evaluate LVLM-generated results. We demonstrate that MapBench significantly\nchallenges state-of-the-art LVLMs both zero-shot prompting and a\nChain-of-Thought (CoT) augmented reasoning framework that decomposes map\nnavigation into sequential cognitive processes. Our evaluation of both\nopen-source and closed-source LVLMs underscores the substantial difficulty\nposed by MapBench, revealing critical limitations in their spatial reasoning\nand structured decision-making capabilities. We release all the code and\ndataset in https:\/\/github.com\/taco-group\/MapBench.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 18:05:38 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Xing', 'Shuo', ''], ['Sun', 'Zezhou', ''], ['Xie', 'Shuangyu', ''], ['Chen', 'Kaiyuan', ''], ['Huang', 'Yanjia', ''], ['Wang', 'Yuping', ''], ['Li', 'Jiachen', ''], ['Song', 'Dezhen', ''], ['Tu', 'Zhengzhong', '']]","extracted_entities":"[{'text': 'zero-shot prompting', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"zero-shot prompting","similarity_score":0.5110134482}
{"id":2503.14713,"submitter":"Kush Jain","authors":"Kush Jain and Claire Le Goues","title":"TestForge: Feedback-Driven, Agentic Test Suite Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Automated test generation holds great promise for alleviating the burdens of\nmanual test creation. However, existing search-based techniques compromise on\ntest readability, while LLM-based approaches are prohibitively expensive in\npractice. We present TestForge, an agentic unit testing framework designed to\ncost-effectively generate high-quality test suites for real-world code. Our key\ninsight is to reframe LLM-based test generation as an iterative process.\nTestForge thus begins with tests generated via zero-shot prompting, and then\ncontinuously refines those tests based on feedback from test executions and\ncoverage reports. We evaluate TestForge on TestGenEval, a real world unit test\ngeneration benchmark sourced from 11 large scale open source repositories; we\nshow that TestForge achieves a pass@1 rate of 84.3%, 44.4% line coverage and\n33.8% mutation score on average, outperforming prior classical approaches and a\none-iteration LLM-based baseline. TestForge produces more natural and\nunderstandable tests compared to state-of-the-art search-based techniques, and\noffers substantial cost savings over LLM-based techniques (at $0.63 per file).\nFinally, we release a version of TestGenEval integrated with the OpenHands\nplatform, a popular open-source framework featuring a diverse set of software\nengineering agents and agentic benchmarks, for future extension and\ndevelopment.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 20:21:44 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Jain', 'Kush', ''], ['Goues', 'Claire Le', '']]","extracted_entities":"[{'text': 'zero-shot prompting', 'label': 'Prompting'}, {'text': '11 large scale open source repositories', 'label': 'Open-source LLMs'}, {'text': 'OpenHands\\nplatform', 'label': 'Open-source LLMs'}]","assigned_concept":"Prompting","matched_keyword":"zero-shot prompting","similarity_score":0.5110134482}
{"id":2503.15035,"submitter":"Yeonjoo Hong","authors":"Sungjae Lee, Yeonjoo Hong, Kwang In Kim","title":"GraspCorrect: Robotic Grasp Correction via Vision-Language Model-Guided\n  Feedback","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Despite significant advancements in robotic manipulation, achieving\nconsistent and stable grasping remains a fundamental challenge, often limiting\nthe successful execution of complex tasks. Our analysis reveals that even\nstate-of-the-art policy models frequently exhibit unstable grasping behaviors,\nleading to failure cases that create bottlenecks in real-world robotic\napplications. To address these challenges, we introduce GraspCorrect, a\nplug-and-play module designed to enhance grasp performance through\nvision-language model-guided feedback. GraspCorrect employs an iterative visual\nquestion-answering framework with two key components: grasp-guided prompting,\nwhich incorporates task-specific constraints, and object-aware sampling, which\nensures the selection of physically feasible grasp candidates. By iteratively\ngenerating intermediate visual goals and translating them into joint-level\nactions, GraspCorrect significantly improves grasp stability and consistently\nenhances task success rates across existing policy models in the RLBench and\nCALVIN datasets.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 09:25:32 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Lee', 'Sungjae', ''], ['Hong', 'Yeonjoo', ''], ['Kim', 'Kwang In', '']]","extracted_entities":"[{'text': 'GraspCorrect', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'grasp-guided prompting', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"grasp-guided prompting","similarity_score":0.6211931705}
{"id":2503.15138,"submitter":"Mingzhe Zheng","authors":"Mingzhe Zheng, Yongqi Xu, Haojian Huang, Xuran Ma, Yexin Liu, Wenjie\n  Shu, Yatian Pang, Feilong Tang, Qifeng Chen, Harry Yang, Ser-Nam Lim","title":"VideoGen-of-Thought: Step-by-step generating multi-shot video with\n  minimal manual intervention","comments":"This paper should be a refined version of arXiv:2412.02259,\n  \"VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video\n  Generation\", but I mistakenly submit it as a new paper","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/","abstract":"  Current video generation models excel at short clips but fail to produce\ncohesive multi-shot narratives due to disjointed visual dynamics and fractured\nstorylines. Existing solutions either rely on extensive manual\nscripting\/editing or prioritize single-shot fidelity over cross-scene\ncontinuity, limiting their practicality for movie-like content. We introduce\nVideoGen-of-Thought (VGoT), a step-by-step framework that automates multi-shot\nvideo synthesis from a single sentence by systematically addressing three core\nchallenges: (1) Narrative Fragmentation: Existing methods lack structured\nstorytelling. We propose dynamic storyline modeling, which first converts the\nuser prompt into concise shot descriptions, then elaborates them into detailed,\ncinematic specifications across five domains (character dynamics, background\ncontinuity, relationship evolution, camera movements, HDR lighting), ensuring\nlogical narrative progression with self-validation. (2) Visual Inconsistency:\nExisting approaches struggle with maintaining visual consistency across shots.\nOur identity-aware cross-shot propagation generates identity-preserving\nportrait (IPP) tokens that maintain character fidelity while allowing trait\nvariations (expressions, aging) dictated by the storyline. (3) Transition\nArtifacts: Abrupt shot changes disrupt immersion. Our adjacent latent\ntransition mechanisms implement boundary-aware reset strategies that process\nadjacent shots' features at transition points, enabling seamless visual flow\nwhile preserving narrative continuity. VGoT generates multi-shot videos that\noutperform state-of-the-art baselines by 20.4% in within-shot face consistency\nand 17.4% in style consistency, while achieving over 100% better cross-shot\nconsistency and 10x fewer manual adjustments than alternatives.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:59:14 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 13:00:45 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zheng', 'Mingzhe', ''], ['Xu', 'Yongqi', ''], ['Huang', 'Haojian', ''], ['Ma', 'Xuran', ''], ['Liu', 'Yexin', ''], ['Shu', 'Wenjie', ''], ['Pang', 'Yatian', ''], ['Tang', 'Feilong', ''], ['Chen', 'Qifeng', ''], ['Yang', 'Harry', ''], ['Lim', 'Ser-Nam', '']]","extracted_entities":"[{'text': 'user prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"user prompt","similarity_score":0.6892601252}
{"id":2503.15147,"submitter":"Bowen Xue","authors":"Bowen Xue and Giuseppe Claudio Guarnera and Shuang Zhao and Zahra\n  Montazeri","title":"Diffusion-based G-buffer generation and rendering","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Despite recent advances in text-to-image generation, controlling geometric\nlayout and material properties in synthesized scenes remains challenging. We\npresent a novel pipeline that first produces a G-buffer (albedo, normals,\ndepth, roughness, and metallic) from a text prompt and then renders a final\nimage through a modular neural network. This intermediate representation\nenables fine-grained editing: users can copy and paste within specific G-buffer\nchannels to insert or reposition objects, or apply masks to the irradiance\nchannel to adjust lighting locally. As a result, real objects can be seamlessly\nintegrated into virtual scenes, and virtual objects can be placed into real\nenvironments with high fidelity. By separating scene decomposition from image\nrendering, our method offers a practical balance between detailed\npost-generation control and efficient text-driven synthesis. We demonstrate its\neffectiveness on a variety of examples, showing that G-buffer editing\nsignificantly extends the flexibility of text-guided image generation.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:20:51 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Xue', 'Bowen', ''], ['Guarnera', 'Giuseppe Claudio', ''], ['Zhao', 'Shuang', ''], ['Montazeri', 'Zahra', '']]","extracted_entities":"[{'text': 'text prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"text prompt","similarity_score":0.6277507544}
{"id":2503.15182,"submitter":"Kevin Esvelt","authors":"Kevin M Esvelt","title":"Foundation models may exhibit staged progression in novel CBRN threat\n  disclosure","comments":"26 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY cs.AI q-bio.OT","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The extent to which foundation models can disclose novel chemical,\nbiological, radiation, and nuclear (CBRN) threats to expert users is unclear\ndue to a lack of test cases. I leveraged the unique opportunity presented by an\nupcoming publication describing a novel catastrophic biothreat - \"Technical\nReport on Mirror Bacteria: Feasibility and Risks\" - to conduct a small\ncontrolled study before it became public. Graduate-trained biologists tasked\nwith predicting the consequences of releasing mirror E. coli showed no\nsignificant differences in rubric-graded accuracy using Claude Sonnet 3.5 new\n(n=10) or web search only (n=2); both groups scored comparably to a web\nbaseline (28 and 43 versus 36). However, Sonnet reasoned correctly when\nprompted by a report author, but a smaller model, Haiku 3.5, failed even with\nauthor guidance (80 versus 5). These results suggest distinct stages of model\ncapability: Haiku is unable to reason about mirror life even with threat-aware\nexpert guidance (Stage 1), while Sonnet correctly reasons only with\nthreat-aware prompting (Stage 2). Continued advances may allow future models to\ndisclose novel CBRN threats to naive experts (Stage 3) or unskilled users\n(Stage 4). While mirror life represents only one case study, monitoring new\nmodels' ability to reason about privately known threats may allow protective\nmeasures to be implemented before widespread disclosure.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 13:08:01 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Esvelt', 'Kevin M', '']]","extracted_entities":"[{'text': 'threat-aware prompting', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"threat-aware prompting","similarity_score":0.6136161089}
{"id":2503.15205,"submitter":"Mike Perkins","authors":"Don Hickerson (1), Mike Perkins (1) ((1) British University Vietnam)","title":"A Peek Behind the Curtain: Using Step-Around Prompt Engineering to\n  Identify Bias and Misinformation in GenAI Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  This research examines the emerging technique of step-around prompt\nengineering in GenAI research, a method that deliberately bypasses AI safety\nmeasures to expose underlying biases and vulnerabilities in GenAI models. We\ndiscuss how Internet-sourced training data introduces unintended biases and\nmisinformation into AI systems, which can be revealed through the careful\napplication of step-around techniques.\n  Drawing parallels with red teaming in cybersecurity, we argue that\nstep-around prompting serves a vital role in identifying and addressing\npotential vulnerabilities while acknowledging its dual nature as both a\nresearch tool and a potential security threat. Our findings highlight three key\nimplications: (1) the persistence of Internet-derived biases in AI training\ndata despite content filtering, (2) the effectiveness of step-around techniques\nin exposing these biases when used responsibly, and (3) the need for robust\nsafeguards against malicious applications of these methods.\n  We conclude by proposing an ethical framework for using step-around prompting\nin AI research and development, emphasizing the importance of balancing system\nimprovements with security considerations.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 13:47:28 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Hickerson', 'Don', '', 'British University Vietnam'], ['Perkins', 'Mike', '', 'British University Vietnam']]","extracted_entities":"[{'text': 'step-around prompting', 'label': 'Prompting'}, {'text': 'ethical framework', 'label': 'AI Ethics'}, {'text': 'step-around prompting', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"step-around prompting","similarity_score":0.8735107183}
{"id":2503.15289,"submitter":"Junnan Zhu","authors":"Junnan Zhu, Min Xiao, Yining Wang, Feifei Zhai, Yu Zhou, Chengqing\n  Zong","title":"TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence\n  Tracing and Relationship Classification","comments":"15 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains such as healthcare, law, and news, it is\ncrucial to understand where and how the content is created. To address this, we\nintroduce the Text pROVEnance (TROVE) challenge, designed to trace each\nsentence of a target text back to specific source sentences within potentially\nlengthy or multi-document inputs. Beyond identifying sources, TROVE annotates\nthe fine-grained relationships (quotation, compression, inference, and others),\nproviding a deep understanding of how each target sentence is formed. To\nbenchmark TROVE, we construct our dataset by leveraging three public datasets\ncovering 11 diverse scenarios (e.g., QA and summarization) in English and\nChinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+),\nemphasizing the multi-document and long-document settings essential for\nprovenance. To ensure high-quality data, we employ a three-stage annotation\nprocess: sentence retrieval, GPT provenance, and human provenance. We evaluate\n11 LLMs under direct prompting and retrieval-augmented paradigms, revealing\nthat retrieval is essential for robust performance, larger models perform\nbetter in complex relationship classification, and closed-source models often\nlead, yet open-source models show significant promise, particularly with\nretrieval augmentation.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:09:39 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhu', 'Junnan', ''], ['Xiao', 'Min', ''], ['Wang', 'Yining', ''], ['Zhai', 'Feifei', ''], ['Zhou', 'Yu', ''], ['Zong', 'Chengqing', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'direct prompting', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"direct prompting","similarity_score":0.9230248928}
{"id":2503.15421,"submitter":"Michael Robinson","authors":"Michael Robinson, Sourya Dey, Taisa Kushner","title":"Probing the topology of the space of tokens with structured prompts","comments":"20 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.DG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This article presents a general and flexible method for prompting a large\nlanguage model (LLM) to reveal its (hidden) token input embedding up to\nhomeomorphism. Moreover, this article provides strong theoretical justification\n-- a mathematical proof for generic LLMs -- for why this method should be\nexpected to work. With this method in hand, we demonstrate its effectiveness by\nrecovering the token subspace of Llemma-7B. The results of this paper apply not\nonly to LLMs but also to general nonlinear autoregressive processes.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:01:15 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Robinson', 'Michael', ''], ['Dey', 'Sourya', ''], ['Kushner', 'Taisa', '']]","extracted_entities":"[{'text': 'prompting', 'label': 'Prompting'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'token input embedding', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2503.15684,"submitter":"Isaac Alpizar-Chacon","authors":"Isaac Alpizar-Chacon and Hieke Keuning","title":"Student's Use of Generative AI as a Support Tool in an Advanced Web\n  Development Course","comments":"Accepted to ITiCSE 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Various studies have studied the impact of Generative AI on Computing\nEducation. However, they have focused on the implications for novice\nprogrammers. In this experience report, we analyze the use of GenAI as a\nsupport tool for learning, creativity, and productivity in a web development\ncourse for undergraduate students with extensive programming experience. We\ncollected diverse data (assignments, reflections, logs, and a survey) and found\nthat students used GenAI on different tasks (code generation, idea generation,\netc.) with a reported increase in learning and productivity. However, they are\nconcerned about over-reliance and incorrect solutions and want more training in\nprompting strategies.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 20:34:21 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Alpizar-Chacon', 'Isaac', ''], ['Keuning', 'Hieke', '']]","extracted_entities":"[{'text': 'prompting strategies', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompting strategies","similarity_score":0.7608678937}
{"id":2503.15718,"submitter":"Mathilde Aguiar","authors":"Mathilde Aguiar, Pierre Zweigenbaum, Nona Naderi","title":"Am I eligible? Natural Language Inference for Clinical Trial Patient\n  Recruitment: the Patient's Point of View","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recruiting patients to participate in clinical trials can be challenging and\ntime-consuming. Usually, participation in a clinical trial is initiated by a\nhealthcare professional and proposed to the patient. Promoting clinical trials\ndirectly to patients via online recruitment might help to reach them more\nefficiently. In this study, we address the case where a patient is initiating\ntheir own recruitment process and wants to determine whether they are eligible\nfor a given clinical trial, using their own language to describe their medical\nprofile. To study whether this creates difficulties in the patient trial\nmatching process, we design a new dataset and task, Natural Language Inference\nfor Patient Recruitment (NLI4PR), in which patient language profiles must be\nmatched to clinical trials. We create it by adapting the TREC 2022 Clinical\nTrial Track dataset, which provides patients' medical profiles, and rephrasing\nthem manually using patient language. We also use the associated clinical trial\nreports where the patients are either eligible or excluded. We prompt several\nopen-source Large Language Models on our task and achieve from 56.5 to 71.8 of\nF1 score using patient language, against 64.7 to 73.1 for the same task using\nmedical language. When using patient language, we observe only a small loss in\nperformance for the best model, suggesting that having the patient as a\nstarting point could be adopted to help recruit patients for clinical trials.\nThe corpus and code bases are all freely available on our Github and\nHuggingFace repositories.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 22:07:19 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Aguiar', 'Mathilde', ''], ['Zweigenbaum', 'Pierre', ''], ['Naderi', 'Nona', '']]","extracted_entities":"[{'text': 'prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt","similarity_score":0.7767513394}
{"id":2503.15739,"submitter":"Zifan Liu","authors":"John Murzaku, Zifan Liu, Md Mehrab Tanjim, Vaishnavi Muppala, Xiang\n  Chen, Yunyao Li","title":"ECLAIR: Enhanced Clarification for Interactive Responses","comments":"7 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present ECLAIR (Enhanced CLArification for Interactive Responses), a novel\nunified and end-to-end framework for interactive disambiguation in enterprise\nAI assistants. ECLAIR generates clarification questions for ambiguous user\nqueries and resolves ambiguity based on the user's response.We introduce a\ngeneralized architecture capable of integrating ambiguity information from\nmultiple downstream agents, enhancing context-awareness in resolving\nambiguities and allowing enterprise specific definition of agents. We further\ndefine agents within our system that provide domain-specific grounding\ninformation. We conduct experiments comparing ECLAIR to few-shot prompting\ntechniques and demonstrate ECLAIR's superior performance in clarification\nquestion generation and ambiguity resolution.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 23:04:00 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Murzaku', 'John', ''], ['Liu', 'Zifan', ''], ['Tanjim', 'Md Mehrab', ''], ['Muppala', 'Vaishnavi', ''], ['Chen', 'Xiang', ''], ['Li', 'Yunyao', '']]","extracted_entities":"[{'text': 'few-shot prompting\\ntechniques', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"few-shot prompting\ntechniques","similarity_score":0.6030158997}
{"id":2503.15752,"submitter":"Matthew O. Jackson","authors":"Yutong Xie, Qiaozhu Mei, Walter Yuan, Matthew O. Jackson","title":"Using Language Models to Decipher the Motivation Behind Human Behaviors","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  AI presents a novel tool for deciphering the motivations behind human\nbehaviors. We show that by varying prompts to a large language model, we can\nelicit a full range of human behaviors in a variety of different scenarios in\nterms of classic economic games. Then by analyzing which prompts are needed to\nelicit which behaviors, we can infer (decipher) the motivations behind the\nhuman behaviors. We also show how one can analyze the prompts to reveal\nrelationships between the classic economic games, providing new insight into\nwhat different economic scenarios induce people to think about. We also show\nhow this deciphering process can be used to understand differences in the\nbehavioral tendencies of different populations.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 00:07:06 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Xie', 'Yutong', ''], ['Mei', 'Qiaozhu', ''], ['Yuan', 'Walter', ''], ['Jackson', 'Matthew O.', '']]","extracted_entities":"[{'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompts","similarity_score":0.7638334036}
{"id":2503.15784,"submitter":"Parham Saremi","authors":"Parham Saremi, Amar Kumar, Mohammed Mohammed, Zahra TehraniNasab, Tal\n  Arbel","title":"RL4Med-DDPO: Reinforcement Learning for Controlled Guidance Towards\n  Diverse Medical Image Generation using Vision-Language Foundation Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Vision-Language Foundation Models (VLFM) have shown a tremendous increase in\nperformance in terms of generating high-resolution, photorealistic natural\nimages. While VLFMs show a rich understanding of semantic content across\nmodalities, they often struggle with fine-grained alignment tasks that require\nprecise correspondence between image regions and textual descriptions a\nlimitation in medical imaging, where accurate localization and detection of\nclinical features are essential for diagnosis and analysis. To address this\nissue, we propose a multi-stage architecture where a pre-trained VLFM provides\na cursory semantic understanding, while a reinforcement learning (RL) algorithm\nrefines the alignment through an iterative process that optimizes for\nunderstanding semantic context. The reward signal is designed to align the\nsemantic information of the text with synthesized images. We demonstrate the\neffectiveness of our method on a medical imaging skin dataset where the\ngenerated images exhibit improved generation quality and alignment with prompt\nover the fine-tuned Stable Diffusion. We also show that the synthesized samples\ncould be used to improve disease classifier performance for underrepresented\nsubgroups through augmentation.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 01:51:05 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Saremi', 'Parham', ''], ['Kumar', 'Amar', ''], ['Mohammed', 'Mohammed', ''], ['TehraniNasab', 'Zahra', ''], ['Arbel', 'Tal', '']]","extracted_entities":"[{'text': 'Vision-Language Foundation Models', 'label': 'Foundation Model'}, {'text': 'prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompt","similarity_score":0.7767513394}
{"id":2503.15886,"submitter":"Hui Liu","authors":"Hui Liu, Wenya Wang, Kecheng Chen, Jie Liu, Yibing Liu, Tiexin Qin,\n  Peisong He, Xinghao Jiang, Haoliang Li","title":"Enhancing Zero-Shot Image Recognition in Vision-Language Models through\n  Human-like Concept Guidance","comments":"21 pages, 7 figures 7 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In zero-shot image recognition tasks, humans demonstrate remarkable\nflexibility in classifying unseen categories by composing known simpler\nconcepts. However, existing vision-language models (VLMs), despite achieving\nsignificant progress through large-scale natural language supervision, often\nunderperform in real-world applications because of sub-optimal prompt\nengineering and the inability to adapt effectively to target classes. To\naddress these issues, we propose a Concept-guided Human-like Bayesian Reasoning\n(CHBR) framework. Grounded in Bayes' theorem, CHBR models the concept used in\nhuman image recognition as latent variables and formulates this task by summing\nacross potential concepts, weighted by a prior distribution and a likelihood\nfunction. To tackle the intractable computation over an infinite concept space,\nwe introduce an importance sampling algorithm that iteratively prompts large\nlanguage models (LLMs) to generate discriminative concepts, emphasizing\ninter-class differences. We further propose three heuristic approaches\ninvolving Average Likelihood, Confidence Likelihood, and Test Time Augmentation\n(TTA) Likelihood, which dynamically refine the combination of concepts based on\nthe test image. Extensive evaluations across fifteen datasets demonstrate that\nCHBR consistently outperforms existing state-of-the-art zero-shot\ngeneralization methods.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 06:20:13 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Liu', 'Hui', ''], ['Wang', 'Wenya', ''], ['Chen', 'Kecheng', ''], ['Liu', 'Jie', ''], ['Liu', 'Yibing', ''], ['Qin', 'Tiexin', ''], ['He', 'Peisong', ''], ['Jiang', 'Xinghao', ''], ['Li', 'Haoliang', '']]","extracted_entities":"[{'text': 'iteratively prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"iteratively prompts","similarity_score":0.5917189121}
{"id":2503.15996,"submitter":"Marc Bened\\'i San Mill\\'an","authors":"Marc Bened\\'i San Mill\\'an, Angela Dai, Matthias Nie{\\ss}ner","title":"Animating the Uncaptured: Humanoid Mesh Animation with Video Diffusion\n  Models","comments":"16 pages, 10 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GR cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Animation of humanoid characters is essential in various graphics\napplications, but requires significant time and cost to create realistic\nanimations. We propose an approach to synthesize 4D animated sequences of input\nstatic 3D humanoid meshes, leveraging strong generalized motion priors from\ngenerative video models -- as such video models contain powerful motion\ninformation covering a wide variety of human motions. From an input static 3D\nhumanoid mesh and a text prompt describing the desired animation, we synthesize\na corresponding video conditioned on a rendered image of the 3D mesh. We then\nemploy an underlying SMPL representation to animate the corresponding 3D mesh\naccording to the video-generated motion, based on our motion optimization. This\nenables a cost-effective and accessible solution to enable the synthesis of\ndiverse and realistic 4D animations.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 10:00:22 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Mill\u00e1n', 'Marc Bened\u00ed San', ''], ['Dai', 'Angela', ''], ['Nie\u00dfner', 'Matthias', '']]","extracted_entities":"[{'text': 'text prompt', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"text prompt","similarity_score":0.6277507544}
{"id":2503.16112,"submitter":"Liming Liu","authors":"Liming Liu, Jiangkai Wu, Haoyang Wang, Peiheng Wang, Xinggong Zhang,\n  Zongming Guo","title":"PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming","comments":"7 pages, 10 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.NI cs.AI cs.MM","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN).\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:00:36 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Liu', 'Liming', ''], ['Wu', 'Jiangkai', ''], ['Wang', 'Haoyang', ''], ['Wang', 'Peiheng', ''], ['Zhang', 'Xinggong', ''], ['Guo', 'Zongming', '']]","extracted_entities":"[{'text': 'Promptus', 'label': 'Prompting'}, {'text': 'Promptus', 'label': 'Prompting'}, {'text': 'Promptus', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"Promptus","similarity_score":0.6082860231}
{"id":2503.1612,"submitter":"Jiyong Rao","authors":"Jiyong Rao, Brian Nlong Zhao, Yu Wang","title":"Probabilistic Prompt Distribution Learning for Animal Pose Estimation","comments":"Accepted by CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multi-species animal pose estimation has emerged as a challenging yet\ncritical task, hindered by substantial visual diversity and uncertainty. This\npaper challenges the problem by efficient prompt learning for Vision-Language\nPretrained (VLP) models, \\textit{e.g.} CLIP, aiming to resolve the\ncross-species generalization problem. At the core of the solution lies in the\nprompt designing, probabilistic prompt modeling and cross-modal adaptation,\nthereby enabling prompts to compensate for cross-modal information and\neffectively overcome large data variances under unbalanced data distribution.\nTo this end, we propose a novel probabilistic prompting approach to fully\nexplore textual descriptions, which could alleviate the diversity issues caused\nby long-tail property and increase the adaptability of prompts on unseen\ncategory instance. Specifically, we first introduce a set of learnable prompts\nand propose a diversity loss to maintain distinctiveness among prompts, thus\nrepresenting diverse image attributes. Diverse textual probabilistic\nrepresentations are sampled and used as the guidance for the pose estimation.\nSubsequently, we explore three different cross-modal fusion strategies at\nspatial level to alleviate the adverse impacts of visual uncertainty. Extensive\nexperiments on multi-species animal pose benchmarks show that our method\nachieves the state-of-the-art performance under both supervised and zero-shot\nsettings. The code is available at https:\/\/github.com\/Raojiyong\/PPAP.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:06:26 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Rao', 'Jiyong', ''], ['Zhao', 'Brian Nlong', ''], ['Wang', 'Yu', '']]","extracted_entities":"[{'text': 'prompt designing', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompts","similarity_score":0.7638334036}
{"id":2503.16129,"submitter":"Chao Li","authors":"Jingwen Li, Aravind Chandrasekar, Mariana Rocha, Chao Li, Yuqing Chen","title":"Controllable Segmentation-Based Text-Guided Style Editing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present a novel approach for controllable, region-specific style editing\ndriven by textual prompts. Building upon the state-space style alignment\nframework introduced by \\emph{StyleMamba}, our method integrates a semantic\nsegmentation model into the style transfer pipeline. This allows users to\nselectively apply text-driven style changes to specific segments (e.g., ``turn\nthe building into a cyberpunk tower'') while leaving other regions (e.g.,\n``people'' or ``trees'') unchanged. By incorporating region-wise condition\nvectors and a region-specific directional loss, our method achieves\nhigh-fidelity transformations that respect both semantic boundaries and\nuser-driven style descriptions. Extensive experiments demonstrate that our\napproach can flexibly handle complex scene stylizations in real-world\nscenarios, improving control and quality over purely global style transfer\nmethods.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:24:41 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Li', 'Jingwen', ''], ['Chandrasekar', 'Aravind', ''], ['Rocha', 'Mariana', ''], ['Li', 'Chao', ''], ['Chen', 'Yuqing', '']]","extracted_entities":"[{'text': 'textual prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"textual prompts","similarity_score":0.6302489042}
{"id":2503.16171,"submitter":"Murari Mandal","authors":"Soham Roy, Abhishek Mishra, Shirish Karande, Murari Mandal","title":"Guardians of Generation: Dynamic Inference-Time Copyright Shielding with\n  Adaptive Guidance for AI Image Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Modern text-to-image generative models can inadvertently reproduce\ncopyrighted content memorized in their training data, raising serious concerns\nabout potential copyright infringement. We introduce Guardians of Generation, a\nmodel agnostic inference time framework for dynamic copyright shielding in AI\nimage generation. Our approach requires no retraining or modification of the\ngenerative model weights, instead integrating seamlessly with existing\ndiffusion pipelines. It augments the generation process with an adaptive\nguidance mechanism comprising three components: a detection module, a prompt\nrewriting module, and a guidance adjustment module. The detection module\nmonitors user prompts and intermediate generation steps to identify features\nindicative of copyrighted content before they manifest in the final output. If\nsuch content is detected, the prompt rewriting mechanism dynamically transforms\nthe user's prompt by sanitizing or replacing references that could trigger\ncopyrighted material while preserving the prompt's intended semantics. The\nadaptive guidance module adaptively steers the diffusion process away from\nflagged content by modulating the model's sampling trajectory. Together, these\ncomponents form a robust shield that enables a tunable balance between\npreserving creative fidelity and ensuring copyright compliance. We validate our\nmethod on a variety of generative models such as Stable Diffusion, SDXL, and\nFlux, demonstrating substantial reductions in copyrighted content generation\nwith negligible impact on output fidelity or alignment with user intent. This\nwork provides a practical, plug-and-play safeguard for generative image models,\nenabling more responsible deployment under real-world copyright constraints.\nSource code is available at: https:\/\/respailab.github.io\/gog\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:31:12 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Roy', 'Soham', ''], ['Mishra', 'Abhishek', ''], ['Karande', 'Shirish', ''], ['Mandal', 'Murari', '']]","extracted_entities":"[{'text': 'user prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"user prompts","similarity_score":0.6697342396}
{"id":2503.16195,"submitter":"Chia-Yi Hsu","authors":"Chia-Yi Hsu, Jia-You Chen, Yu-Lin Tsai, Chih-Hsun Lin, Pin-Yu Chen,\n  Chia-Mu Yu and Chun-Ying Huang","title":"VP-NTK: Exploring the Benefits of Visual Prompting in Differentially\n  Private Data Synthesis","comments":"Accepted by ICASSP 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Differentially private (DP) synthetic data has become the de facto standard\nfor releasing sensitive data. However, many DP generative models suffer from\nthe low utility of synthetic data, especially for high-resolution images. On\nthe other hand, one of the emerging techniques in parameter efficient\nfine-tuning (PEFT) is visual prompting (VP), which allows well-trained existing\nmodels to be reused for the purpose of adapting to subsequent downstream tasks.\nIn this work, we explore such a phenomenon in constructing captivating\ngenerative models with DP constraints. We show that VP in conjunction with\nDP-NTK, a DP generator that exploits the power of the neural tangent kernel\n(NTK) in training DP generative models, achieves a significant performance\nboost, particularly for high-resolution image datasets, with accuracy improving\nfrom 0.644$\\pm$0.044 to 0.769. Lastly, we perform ablation studies on the\neffect of different parameters that influence the overall performance of\nVP-NTK. Our work demonstrates a promising step forward in improving the utility\nof DP synthetic data, particularly for high-resolution images.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 14:42:11 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Hsu', 'Chia-Yi', ''], ['Chen', 'Jia-You', ''], ['Tsai', 'Yu-Lin', ''], ['Lin', 'Chih-Hsun', ''], ['Chen', 'Pin-Yu', ''], ['Yu', 'Chia-Mu', ''], ['Huang', 'Chun-Ying', '']]","extracted_entities":"[{'text': 'visual prompting', 'label': 'Prompting'}, {'text': 'DP-NTK', 'label': 'Generative Pre-trained Transformer (GPT)'}]","assigned_concept":"Prompting","matched_keyword":"visual prompting","similarity_score":0.7436023951}
{"id":2503.16248,"submitter":"Atharv Singh Patlan","authors":"Atharv Singh Patlan, Peiyao Sheng, S. Ashwin Hebbar, Prateek Mittal,\n  Pramod Viswanath","title":"AI Agents in Cryptoland: Practical Attacks and No Silver Bullet","comments":"12 pages, 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  The integration of AI agents with Web3 ecosystems harnesses their\ncomplementary potential for autonomy and openness, yet also introduces\nunderexplored security risks, as these agents dynamically interact with\nfinancial protocols and immutable smart contracts. This paper investigates the\nvulnerabilities of AI agents within blockchain-based financial ecosystems when\nexposed to adversarial threats in real-world scenarios. We introduce the\nconcept of context manipulation -- a comprehensive attack vector that exploits\nunprotected context surfaces, including input channels, memory modules, and\nexternal data feeds. Through empirical analysis of ElizaOS, a decentralized AI\nagent framework for automated Web3 operations, we demonstrate how adversaries\ncan manipulate context by injecting malicious instructions into prompts or\nhistorical interaction records, leading to unintended asset transfers and\nprotocol violations which could be financially devastating. Our findings\nindicate that prompt-based defenses are insufficient, as malicious inputs can\ncorrupt an agent's stored context, creating cascading vulnerabilities across\ninteractions and platforms. This research highlights the urgent need to develop\nAI agents that are both secure and fiduciarily responsible.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 15:44:31 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Patlan', 'Atharv Singh', ''], ['Sheng', 'Peiyao', ''], ['Hebbar', 'S. Ashwin', ''], ['Mittal', 'Prateek', ''], ['Viswanath', 'Pramod', '']]","extracted_entities":"[{'text': 'prompts', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompts","similarity_score":0.7638334036}
{"id":2503.16254,"submitter":"Onay Urfalioglu","authors":"Markus Karmann, Peng-Tao Jiang, Bo Li, Onay Urfalioglu","title":"M2N2V2: Multi-Modal Unsupervised and Training-free Interactive\n  Segmentation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present Markov Map Nearest Neighbor V2 (M2N2V2), a novel and simple, yet\neffective approach which leverages depth guidance and attention maps for\nunsupervised and training-free point-prompt-based interactive segmentation.\nFollowing recent trends in supervised multimodal approaches, we carefully\nintegrate depth as an additional modality to create novel depth-guided\nMarkov-maps. Furthermore, we observe occasional segment size fluctuations in\nM2N2 during the interactive process, which can decrease the overall mIoU's. To\nmitigate this problem, we model the prompting as a sequential process and\npropose a novel adaptive score function which considers the previous\nsegmentation and the current prompt point in order to prevent unreasonable\nsegment size changes. Using Stable Diffusion 2 and Depth Anything V2 as\nbackbones, we empirically show that our proposed M2N2V2 significantly improves\nthe Number of Clicks (NoC) and mIoU compared to M2N2 in all datasets except\nthose from the medical domain. Interestingly, our unsupervised approach\nachieves competitive results compared to supervised methods like SAM and\nSimpleClick in the more challenging DAVIS and HQSeg44K datasets in the NoC\nmetric, reducing the gap between supervised and unsupervised methods.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 15:47:14 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Karmann', 'Markus', ''], ['Jiang', 'Peng-Tao', ''], ['Li', 'Bo', ''], ['Urfalioglu', 'Onay', '']]","extracted_entities":"[{'text': 'prompting', 'label': 'Prompting'}]","assigned_concept":"Prompting","matched_keyword":"prompting","similarity_score":1.0}
{"id":2503.1626,"submitter":"Zijian Li","authors":"Zijian Li, Jingjing Fu, Lei Song, Jiang Bian, Jun Zhang, Rui Wang","title":"Chain of Functions: A Programmatic Pipeline for Fine-Grained Chart\n  Reasoning Data","comments":"Under review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Visual reasoning is crucial for multimodal large language models (MLLMs) to\naddress complex chart queries, yet high-quality rationale data remains scarce.\nExisting methods leveraged (M)LLMs for data generation, but direct prompting\noften yields limited precision and diversity. In this paper, we propose\n\\textit{Chain of Functions (CoF)}, a novel programmatic reasoning data\ngeneration pipeline that utilizes freely-explored reasoning paths as\nsupervision to ensure data precision and diversity. Specifically, it starts\nwith human-free exploration among the atomic functions (e.g., maximum data and\narithmetic operations) to generate diverse function chains, which are then\ntranslated into linguistic rationales and questions with only a moderate\nopen-sourced LLM. \\textit{CoF} provides multiple benefits: 1) Precision:\nfunction-governed generation reduces hallucinations compared to freeform\ngeneration; 2) Diversity: enumerating function chains enables varied question\ntaxonomies; 3) Explainability: function chains serve as built-in rationales,\nallowing fine-grained evaluation beyond overall accuracy; 4) Practicality:\neliminating reliance on extremely large models. Employing \\textit{CoF}, we\nconstruct the \\textit{ChartCoF} dataset, with 1.4k complex reasoning Q\\&A for\nfine-grained analysis and 50k Q\\&A for reasoning enhancement. The fine-grained\nevaluation on \\textit{ChartCoF} reveals varying performance across question\ntaxonomies for each MLLM, and the experiments also show that finetuning with\n\\textit{ChartCoF} achieves state-of-the-art performance among same-scale MLLMs\non widely used benchmarks. Furthermore, the novel paradigm of function-governed\nrationale generation in \\textit{CoF} could inspire broader applications beyond\ncharts.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 15:56:04 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Li', 'Zijian', ''], ['Fu', 'Jingjing', ''], ['Song', 'Lei', ''], ['Bian', 'Jiang', ''], ['Zhang', 'Jun', ''], ['Wang', 'Rui', '']]","extracted_entities":"[{'text': 'multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'direct prompting', 'label': 'Prompting'}, {'text': 'function chains', 'label': 'Chain of thought'}, {'text': 'function chains', 'label': 'Chain of thought'}, {'text': 'function chains', 'label': 'Chain of thought'}, {'text': 'finetuning', 'label': 'Fine-tuning'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Prompting","matched_keyword":"direct prompting","similarity_score":0.9230248928}
{"id":2401.15378,"submitter":"Enis Karaarslan Dr.","authors":"Ahmet Yusuf Alan, Enis Karaarslan, \\\"Omer Aydin","title":"A RAG-based Question Answering System Proposal for Understanding Islam:\n  MufassirQAS LLM","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Challenges exist in learning and understanding religions, such as the\ncomplexity and depth of religious doctrines and teachings. Chatbots as\nquestion-answering systems can help in solving these challenges. LLM chatbots\nuse NLP techniques to establish connections between topics and accurately\nrespond to complex questions. These capabilities make it perfect for\nenlightenment on religion as a question-answering chatbot. However, LLMs also\ntend to generate false information, known as hallucination. Also, the chatbots'\nresponses can include content that insults personal religious beliefs,\ninterfaith conflicts, and controversial or sensitive topics. It must avoid such\ncases without promoting hate speech or offending certain groups of people or\ntheir beliefs. This study uses a vector database-based Retrieval Augmented\nGeneration (RAG) approach to enhance the accuracy and transparency of LLMs. Our\nquestion-answering system is called \"MufassirQAS\". We created a database\nconsisting of several open-access books that include Turkish context. These\nbooks contain Turkish translations and interpretations of Islam. This database\nis utilized to answer religion-related questions and ensure our answers are\ntrustworthy. The relevant part of the dataset, which LLM also uses, is\npresented along with the answer. We have put careful effort into creating\nsystem prompts that give instructions to prevent harmful, offensive, or\ndisrespectful responses to respect people's values and provide reliable\nresults. The system answers and shares additional information, such as the page\nnumber from the respective book and the articles referenced for obtaining the\ninformation. MufassirQAS and ChatGPT are also tested with sensitive questions.\nWe got better performance with our system. Study and enhancements are still in\nprogress. Results and future works are given.\n","versions":"[{'version': 'v1', 'created': 'Sat, 27 Jan 2024 10:50:11 GMT'}, {'version': 'v2', 'created': 'Tue, 30 Jan 2024 05:36:32 GMT'}, {'version': 'v3', 'created': 'Wed, 31 Jan 2024 12:39:06 GMT'}, {'version': 'v4', 'created': 'Thu, 1 Feb 2024 20:28:11 GMT'}, {'version': 'v5', 'created': 'Tue, 18 Mar 2025 17:14:43 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Alan', 'Ahmet Yusuf', ''], ['Karaarslan', 'Enis', ''], ['Aydin', '\u00d6mer', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'MufassirQAS', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'MufassirQAS', 'label': 'LLM'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2403.13501,"submitter":"Yumeng Li","authors":"Yumeng Li and William Beluch and Margret Keuper and Dan Zhang and Anna\n  Khoreva","title":"VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis","comments":"Accepted at ICLR 2025. Code: https:\/\/github.com\/boschresearch\/VSTAR\n  and project page: https:\/\/yumengli007.github.io\/VSTAR","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG cs.MM","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Despite tremendous progress in the field of text-to-video (T2V) synthesis,\nopen-sourced T2V diffusion models struggle to generate longer videos with\ndynamically varying and evolving content. They tend to synthesize quasi-static\nvideos, ignoring the necessary visual change-over-time implied in the text\nprompt. At the same time, scaling these models to enable longer, more dynamic\nvideo synthesis often remains computationally intractable. To address this\nchallenge, we introduce the concept of Generative Temporal Nursing (GTN), where\nwe aim to alter the generative process on the fly during inference to improve\ncontrol over the temporal dynamics and enable generation of longer videos. We\npropose a method for GTN, dubbed VSTAR, which consists of two key ingredients:\n1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis\nbased on the original single prompt leveraging LLMs, which gives accurate\ntextual guidance to different visual states of longer videos, and 2) Temporal\nAttention Regularization (TAR) - a regularization technique to refine the\ntemporal attention units of the pre-trained T2V diffusion models, which enables\ncontrol over the video dynamics. We experimentally showcase the superiority of\nthe proposed approach in generating longer, visually appealing videos over\nexisting open-sourced T2V models. We additionally analyze the temporal\nattention maps realized with and without VSTAR, demonstrating the importance of\napplying our method to mitigate neglect of the desired visual change over time.\n","versions":"[{'version': 'v1', 'created': 'Wed, 20 Mar 2024 10:58:58 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 13:55:22 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Li', 'Yumeng', ''], ['Beluch', 'William', ''], ['Keuper', 'Margret', ''], ['Zhang', 'Dan', ''], ['Khoreva', 'Anna', '']]","extracted_entities":"[{'text': 'Video Synopsis Prompting', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'temporal attention units', 'label': 'Attention mechanism'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2406.08598,"submitter":"Justin Zhao","authors":"Justin Zhao, Flor Miriam Plaza-del-Arco, Benjamin Genchel, Amanda\n  Cercas Curry","title":"Language Model Council: Democratically Benchmarking Foundation Models on\n  Highly Subjective Tasks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  As Large Language Models (LLMs) continue to evolve, evaluating them remains a\npersistent challenge. Many recent evaluations use LLMs as judges to score\noutputs from other LLMs, often relying on a single large model like GPT-4o.\nHowever, using a single LLM judge is prone to intra-model bias, and many tasks\n- such as those related to emotional intelligence, creative writing, and\npersuasiveness - may be too subjective for a single model to judge fairly. We\nintroduce the Language Model Council (LMC), where a group of LLMs collaborate\nto create tests, respond to them, and evaluate each other's responses to\nproduce a ranking in a democratic fashion. Unlike previous approaches that\nfocus on reducing cost or bias by using a panel of smaller models, our work\nexamines the benefits and nuances of a fully inclusive LLM evaluation system.\nIn a detailed case study on emotional intelligence, we deploy a council of 20\nrecent LLMs to rank each other on open-ended responses to interpersonal\nconflicts. Our results show that the LMC produces rankings that are more\nseparable and more robust, and through a user study, we show that they are more\nconsistent with human evaluations than any individual LLM judge. Using all LLMs\nfor judging can be costly, however, so we use Monte Carlo simulations and\nhand-curated sub-councils to study hypothetical council compositions and\ndiscuss the value of the incremental LLM judge.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Jun 2024 19:05:43 GMT'}, {'version': 'v2', 'created': 'Mon, 21 Oct 2024 21:32:51 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Feb 2025 18:42:44 GMT'}, {'version': 'v4', 'created': 'Wed, 19 Mar 2025 04:25:35 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhao', 'Justin', ''], ['Plaza-del-Arco', 'Flor Miriam', ''], ['Genchel', 'Benjamin', ''], ['Curry', 'Amanda Cercas', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'intra-model bias', 'label': 'Model Bias and Fairness'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2407.1697,"submitter":"Shehzaad Dhuliawala","authors":"Sa\\\"uc Abadal Lloret, Shehzaad Dhuliawala, Keerthiram Murugesan,\n  Mrinmaya Sachan","title":"Towards Aligning Language Models with Textual Feedback","comments":"Accepted to EMNLP 2024","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We present ALT (ALignment with Textual feedback), an approach that aligns\nlanguage models with user preferences expressed in text. We argue that text\noffers greater expressiveness, enabling users to provide richer feedback than\nsimple comparative preferences and this richer feedback can lead to more\nefficient and effective alignment. ALT aligns the model by conditioning its\ngeneration on the textual feedback. Our method relies solely on language\nmodeling techniques and requires minimal hyper-parameter tuning, though it\nstill presents the main benefits of RL-based alignment algorithms and can\neffectively learn from textual feedback. We explore the efficacy and efficiency\nof textual feedback across different tasks such as toxicity reduction,\nsummarization, and dialog response generation. We find that ALT outperforms PPO\nfor the task of toxicity reduction while being able to match its performance on\nsummarization with only 20% of the samples. We also explore how ALT can be used\nwith feedback provided by an existing LLM where we explore an LLM providing\nconstrained and unconstrained textual feedback. We also outline future\ndirections to align models with natural language feedback.\n","versions":"[{'version': 'v1', 'created': 'Wed, 24 Jul 2024 03:32:05 GMT'}, {'version': 'v2', 'created': 'Thu, 24 Oct 2024 08:43:21 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 16:34:14 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Lloret', 'Sa\u00fcc Abadal', ''], ['Dhuliawala', 'Shehzaad', ''], ['Murugesan', 'Keerthiram', ''], ['Sachan', 'Mrinmaya', '']]","extracted_entities":"[{'text': 'hyper-parameter tuning', 'label': 'Fine-tuning'}, {'text': 'summarization', 'label': 'Knowledge distillation'}, {'text': 'summarization', 'label': 'Knowledge distillation'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2409.08622,"submitter":"K. J. Kevin Feng","authors":"K. J. Kevin Feng, Inyoung Cheong, Quan Ze Chen, Amy X. Zhang","title":"Policy Prototyping for LLMs: Pluralistic Alignment via Interactive and\n  Collaborative Policymaking","comments":"Bidirectional Human-AI Alignment (Bi-Align) Workshop @ ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Emerging efforts in AI alignment seek to broaden participation in shaping\nmodel behavior by eliciting and integrating collective input into a policy for\nmodel finetuning. While pluralistic, these processes are often linear and do\nnot allow participating stakeholders to confirm whether potential outcomes of\ntheir contributions are indeed consistent with their intentions. Design\nprototyping has long advocated for rapid iteration using tight feedback loops\nof ideation, experimentation, and evaluation to mitigate these issues. We thus\npropose policy prototyping for LLMs, a new process that draws inspiration from\nprototyping practices to enable stakeholders to collaboratively and\ninteractively draft LLM policies. Through learnings from a real-world LLM\npolicymaking initiative at an industrial AI lab, we motivate our approach and\ncharacterize policy prototyping with four guiding principles. Because policy\nprototyping emphasizes a contrasting set of priorities compared to previous\napproaches, we envision our approach to be a valuable addition to the\nmethodological repertoire for collaborative, pluralistic alignment.\n","versions":"[{'version': 'v1', 'created': 'Fri, 13 Sep 2024 08:19:52 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 05:45:58 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Feng', 'K. J. Kevin', ''], ['Cheong', 'Inyoung', ''], ['Chen', 'Quan Ze', ''], ['Zhang', 'Amy X.', '']]","extracted_entities":"[{'text': 'model finetuning', 'label': 'Fine-tuning'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2409.14506,"submitter":"Kim Tien Ly","authors":"Kim Tien Ly, Kai Lu, Ioannis Havoutis","title":"InteLiPlan: An Interactive Lightweight LLM-Based Planner for Domestic\n  Robot Autonomy","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We introduce an interactive LLM-based framework designed to enhance the\nautonomy and robustness of domestic robots, targeting embodied intelligence.\nOur approach reduces reliance on large-scale data and incorporates a\nrobot-agnostic pipeline that embodies an LLM. Our framework, InteLiPlan,\nensures that the LLM's decision-making capabilities are effectively aligned\nwith robotic functions, enhancing operational robustness and adaptability,\nwhile our human-in-the-loop mechanism allows for real-time human intervention\nwhen user instruction is required. We evaluate our method in both simulation\nand on the real Toyota Human Support Robot (HSR). Our method achieves a 93%\nsuccess rate in the 'fetch me' task completion with failure recovery,\nhighlighting its capability in both failure reasoning and task planning.\nInteLiPlan achieves comparable performance to state-of-the-art large-scale\nLLM-based robotics planners, while using only real-time onboard computing.\n","versions":"[{'version': 'v1', 'created': 'Sun, 22 Sep 2024 16:10:10 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 15:03:47 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Ly', 'Kim Tien', ''], ['Lu', 'Kai', ''], ['Havoutis', 'Ioannis', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2410.07627,"submitter":"Hanze Dong","authors":"Zirui Zhao, Hanze Dong, Amrita Saha, Caiming Xiong, Doyen Sahoo","title":"Automatic Curriculum Expert Iteration for Reliable LLM Reasoning","comments":"20 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL stat.ML","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Hallucinations (i.e., generating plausible but inaccurate content) and\nlaziness (i.e. excessive refusals or defaulting to \"I don't know\") persist as\nmajor challenges in LLM reasoning. Current efforts to reduce hallucinations\nprimarily focus on factual errors in knowledge-grounded tasks, often neglecting\nhallucinations related to faulty reasoning. Meanwhile, some approaches render\nLLMs overly conservative, limiting their problem-solving capabilities. To\nmitigate hallucination and laziness in reasoning tasks, we propose Automatic\nCurriculum Expert Iteration (Auto-CEI) to enhance LLM reasoning and align\nresponses to the model's capabilities--assertively answering within its limits\nand declining when tasks exceed them. In our method, Expert Iteration explores\nthe reasoning trajectories near the LLM policy, guiding incorrect paths back on\ntrack to reduce compounding errors and improve robustness; it also promotes\nappropriate \"I don't know\" responses after sufficient reasoning attempts. The\ncurriculum automatically adjusts rewards, incentivizing extended reasoning\nbefore acknowledging incapability, thereby pushing the limits of LLM reasoning\nand aligning its behaviour with these limits. We compare Auto-CEI with various\nSOTA baselines across logical reasoning, mathematics, and planning tasks, where\nAuto-CEI achieves superior alignment by effectively balancing assertiveness and\nconservativeness. The code is available at\nhttps:\/\/github.com\/SalesforceAIResearch\/Auto-CEI .\n","versions":"[{'version': 'v1', 'created': 'Thu, 10 Oct 2024 05:43:07 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 05:08:24 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zhao', 'Zirui', ''], ['Dong', 'Hanze', ''], ['Saha', 'Amrita', ''], ['Xiong', 'Caiming', ''], ['Sahoo', 'Doyen', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2411.03884,"submitter":"Zhijian Zhuo","authors":"Zhijian Zhuo and Ya Wang and Yutao Zeng and Xiaoqing Li and Xun Zhou\n  and Jinwen Ma","title":"Polynomial Composition Activations: Unleashing the Dynamics of Large\n  Language Models","comments":"Accepted by ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Transformers have found extensive applications across various domains due to\nthe powerful fitting capabilities. This success can be partially attributed to\ntheir inherent nonlinearity. Thus, in addition to the ReLU function employed in\nthe original transformer architecture, researchers have explored alternative\nmodules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment\nrepresentational capacity. In this paper, we propose a novel category of\npolynomial composition activations (PolyCom), designed to optimize the dynamics\nof transformers. Theoretically, we provide a comprehensive mathematical\nanalysis of PolyCom, highlighting its enhanced expressivity and efficacy\nrelative to other activation functions. Notably, we demonstrate that networks\nincorporating PolyCom achieve the $\\textbf{optimal approximation rate}$,\nindicating that PolyCom networks require minimal parameters to approximate\ngeneral smooth functions in Sobolev spaces. We conduct empirical experiments on\nthe pre-training configurations of large language models (LLMs), including both\ndense and sparse architectures. By substituting conventional activation\nfunctions with PolyCom, we enable LLMs to capture higher-order interactions\nwithin the data, thus improving performance metrics in terms of accuracy and\nconvergence rates. Extensive experimental results demonstrate the effectiveness\nof our method, showing substantial improvements over other activation\nfunctions. Code is available at https:\/\/github.com\/BryceZhuo\/PolyCom.\n","versions":"[{'version': 'v1', 'created': 'Wed, 6 Nov 2024 13:00:34 GMT'}, {'version': 'v2', 'created': 'Sat, 22 Feb 2025 16:56:11 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 09:46:11 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zhuo', 'Zhijian', ''], ['Wang', 'Ya', ''], ['Zeng', 'Yutao', ''], ['Li', 'Xiaoqing', ''], ['Zhou', 'Xun', ''], ['Ma', 'Jinwen', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2411.16657,"submitter":"Zun Wang","authors":"Zun Wang, Jialu Li, Han Lin, Jaehong Yoon, Mohit Bansal","title":"DreamRunner: Fine-Grained Compositional Story-to-Video Generation with\n  Retrieval-Augmented Motion Adaptation","comments":"Project website: https:\/\/zunwang1.github.io\/DreamRunner","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Storytelling video generation (SVG) aims to produce coherent and visually\nrich multi-scene videos that follow a structured narrative. Existing methods\nprimarily employ LLM for high-level planning to decompose a story into\nscene-level descriptions, which are then independently generated and stitched\ntogether. However, these approaches struggle with generating high-quality\nvideos aligned with the complex single-scene description, as visualizing such\ncomplex description involves coherent composition of multiple characters and\nevents, complex motion synthesis and muti-character customization. To address\nthese challenges, we propose DreamRunner, a novel story-to-video generation\nmethod: First, we structure the input script using a large language model (LLM)\nto facilitate both coarse-grained scene planning as well as fine-grained\nobject-level layout and motion planning. Next, DreamRunner presents\nretrieval-augmented test-time adaptation to capture target motion priors for\nobjects in each scene, supporting diverse motion customization based on\nretrieved videos, thus facilitating the generation of new videos with complex,\nscripted motions. Lastly, we propose a novel spatial-temporal region-based 3D\nattention and prior injection module SR3AI for fine-grained object-motion\nbinding and frame-by-frame semantic control. We compare DreamRunner with\nvarious SVG baselines, demonstrating state-of-the-art performance in character\nconsistency, text alignment, and smooth transitions. Additionally, DreamRunner\nexhibits strong fine-grained condition-following ability in compositional\ntext-to-video generation, significantly outperforming baselines on\nT2V-ComBench. Finally, we validate DreamRunner's robust ability to generate\nmulti-object interactions with qualitative examples.\n","versions":"[{'version': 'v1', 'created': 'Mon, 25 Nov 2024 18:41:56 GMT'}, {'version': 'v2', 'created': 'Tue, 17 Dec 2024 06:52:46 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 15:19:15 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wang', 'Zun', ''], ['Li', 'Jialu', ''], ['Lin', 'Han', ''], ['Yoon', 'Jaehong', ''], ['Bansal', 'Mohit', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'Large Language Model'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2411.17762,"submitter":"Ronchang Xie","authors":"Rongchang Xie, Chen Du, Ping Song, Chang Liu","title":"MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We introduce MUSE-VL, a Unified Vision-Language Model through Semantic\ndiscrete Encoding for multimodal understanding and generation. Recently, the\nresearch community has begun exploring unified models for visual generation and\nunderstanding. However, existing vision tokenizers (e.g., VQGAN) only consider\nlow-level information, which makes it difficult to align with language tokens.\nThis results in high training complexity and necessitates a large amount of\ntraining data to achieve optimal performance. Additionally, their performance\nis still far from dedicated understanding models. This paper proposes Semantic\nDiscrete Encoding (SDE), which effectively aligns the information of visual\ntokens and language tokens by adding semantic constraints to the visual\ntokenizer. This greatly reduces the amount of training data and improves the\nperformance of the unified model. With the same LLM size, our method improved\nthe understanding performance by 4.8% compared to the previous SOTA Emu3 and\nsurpassed the dedicated understanding model LLaVA-NeXT 34B by 3.7%. Our model\nalso surpasses the existing unified models on visual generation benchmarks.\n","versions":"[{'version': 'v1', 'created': 'Tue, 26 Nov 2024 03:33:52 GMT'}, {'version': 'v2', 'created': 'Thu, 5 Dec 2024 17:54:29 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 04:11:06 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Xie', 'Rongchang', ''], ['Du', 'Chen', ''], ['Song', 'Ping', ''], ['Liu', 'Chang', '']]","extracted_entities":"[{'text': 'Semantic\\ndiscrete Encoding', 'label': 'Embedding'}, {'text': 'Semantic\\nDiscrete Encoding', 'label': 'Embedding'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2412.13817,"submitter":"Le Yang","authors":"Le Yang, Ziwei Zheng, Boxu Chen, Zhengyu Zhao, Chenhao Lin, Chao Shen","title":"Nullu: Mitigating Object Hallucinations in Large Vision-Language Models\n  via HalluSpace Projection","comments":"CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent studies have shown that large vision-language models (LVLMs) often\nsuffer from the issue of object hallucinations (OH). To mitigate this issue, we\nintroduce an efficient method that edits the model weights based on an unsafe\nsubspace, which we call HalluSpace in this paper. With truthful and\nhallucinated text prompts accompanying the visual content as inputs, the\nHalluSpace can be identified by extracting the hallucinated embedding features\nand removing the truthful representations in LVLMs. By orthogonalizing the\nmodel weights, input features will be projected into the Null space of the\nHalluSpace to reduce OH, based on which we name our method Nullu. We reveal\nthat HalluSpaces generally contain prior information in the large language\nmodels (LLMs) applied to build LVLMs, which have been shown as essential causes\nof OH in previous studies. Therefore, null space projection suppresses the\nLLMs' priors to filter out the hallucinated features, resulting in contextually\naccurate outputs. Experiments show that our method can effectively mitigate OH\nacross different LVLM families without extra inference costs and also show\nstrong performance in general LVLM benchmarks. Code is released at\nhttps:\/\/github.com\/Ziwei-Zheng\/Nullu.\n","versions":"[{'version': 'v1', 'created': 'Wed, 18 Dec 2024 13:04:30 GMT'}, {'version': 'v2', 'created': 'Sun, 29 Dec 2024 13:32:10 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 16:05:34 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Yang', 'Le', ''], ['Zheng', 'Ziwei', ''], ['Chen', 'Boxu', ''], ['Zhao', 'Zhengyu', ''], ['Lin', 'Chenhao', ''], ['Shen', 'Chao', '']]","extracted_entities":"[{'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'hallucinated text prompts', 'label': 'Prompting'}, {'text': 'hallucinated embedding features', 'label': 'Embedding'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2502.04382,"submitter":"Rajiv Movva","authors":"Rajiv Movva, Kenny Peng, Nikhil Garg, Jon Kleinberg, Emma Pierson","title":"Sparse Autoencoders for Hypothesis Generation","comments":"First two authors contributed equally; working paper. Code is\n  available at https:\/\/github.com\/rmovva\/HypotheSAEs","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CY","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  We describe HypotheSAEs, a general method to hypothesize interpretable\nrelationships between text data (e.g., headlines) and a target variable (e.g.,\nclicks). HypotheSAEs has three steps: (1) train a sparse autoencoder on text\nembeddings to produce interpretable features describing the data distribution,\n(2) select features that predict the target variable, and (3) generate a\nnatural language interpretation of each feature (e.g., \"mentions being\nsurprised or shocked\") using an LLM. Each interpretation serves as a hypothesis\nabout what predicts the target variable. Compared to baselines, our method\nbetter identifies reference hypotheses on synthetic datasets (at least +0.06 in\nF1) and produces more predictive hypotheses on real datasets (~twice as many\nsignificant findings), despite requiring 1-2 orders of magnitude less compute\nthan recent LLM-based methods. HypotheSAEs also produces novel discoveries on\ntwo well-studied tasks: explaining partisan differences in Congressional\nspeeches and identifying drivers of engagement with online headlines.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Feb 2025 18:58:02 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 17:51:56 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Movva', 'Rajiv', ''], ['Peng', 'Kenny', ''], ['Garg', 'Nikhil', ''], ['Kleinberg', 'Jon', ''], ['Pierson', 'Emma', '']]","extracted_entities":"[{'text': 'HypotheSAEs', 'label': 'LLM'}, {'text': 'HypotheSAEs', 'label': 'LLM'}, {'text': 'text\\nembeddings', 'label': 'Embedding'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2502.12065,"submitter":"Lan Zhang","authors":"Lan Zhang, Marco Valentino, Andre Freitas","title":"Formalizing Complex Mathematical Statements with LLMs: A Study on\n  Mathematical Definitions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.FL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge\nthe gap between informal mathematics and formal languages through\nautoformalization. However, it is still unclear how well LLMs generalize to\nsophisticated and naturally occurring mathematical statements. To address this\ngap, we investigate the task of autoformalizing real-world mathematical\ndefinitions -- a critical component of mathematical discourse. Specifically, we\nintroduce two novel resources for autoformalisation, collecting definitions\nfrom Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically\nevaluate a range of LLMs, analyzing their ability to formalize definitions into\nIsabelle\/HOL. Furthermore, we investigate strategies to enhance LLMs'\nperformance including refinement through external feedback from Proof\nAssistants, and formal definition grounding, where we guide LLMs through\nrelevant contextual elements from formal mathematical libraries. Our findings\nreveal that definitions present a greater challenge compared to existing\nbenchmarks, such as miniF2F. In particular, we found that LLMs still struggle\nwith self-correction, and aligning with relevant mathematical libraries. At the\nsame time, structured refinement methods and definition grounding strategies\nyield notable improvements of up to 16% on self-correction capabilities and 43%\non the reduction of undefined errors, highlighting promising directions for\nenhancing LLM-based autoformalization in real-world scenarios.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Feb 2025 17:34:48 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 18:53:49 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zhang', 'Lan', ''], ['Valentino', 'Marco', ''], ['Freitas', 'Andre', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2502.17912,"submitter":"Yihong Luo","authors":"Yuhan Chen, Yihong Luo, Yifan Song, Pengwen Dai, Jing Tang, Xiaochun\n  Cao","title":"Decoupled Graph Energy-based Model for Node Out-of-Distribution\n  Detection on Heterophilic Graphs","comments":"The first two authors contributed equally to this work; ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Despite extensive research efforts focused on OOD detection on images, OOD\ndetection on nodes in graph learning remains underexplored. The dependence\namong graph nodes hinders the trivial adaptation of existing approaches on\nimages that assume inputs to be i.i.d. sampled, since many unique features and\nchallenges specific to graphs are not considered, such as the heterophily\nissue. Recently, GNNSafe, which considers node dependence, adapted energy-based\ndetection to the graph domain with state-of-the-art performance, however, it\nhas two serious issues: 1) it derives node energy from classification logits\nwithout specifically tailored training for modeling data distribution, making\nit less effective at recognizing OOD data; 2) it highly relies on energy\npropagation, which is based on homophily assumption and will cause significant\nperformance degradation on heterophilic graphs, where the node tends to have\ndissimilar distribution with its neighbors. To address the above issues, we\nsuggest training EBMs by MLE to enhance data distribution modeling and remove\nenergy propagation to overcome the heterophily issues. However, training EBMs\nvia MLE requires performing MCMC sampling on both node feature and node\nneighbors, which is challenging due to the node interdependence and discrete\ngraph topology. To tackle the sampling challenge, we introduce DeGEM, which\ndecomposes the learning process into two parts: a graph encoder that leverages\ntopology information for node representations and an energy head that operates\nin latent space. Extensive experiments validate that DeGEM, without OOD\nexposure during training, surpasses previous state-of-the-art methods,\nachieving an average AUROC improvement of 6.71% on homophilic graphs and 20.29%\non heterophilic graphs, and even outperform methods trained with OOD exposure.\nOur code is available at: https:\/\/github.com\/draym28\/DeGEM.\n","versions":"[{'version': 'v1', 'created': 'Tue, 25 Feb 2025 07:20:00 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 08:23:08 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Chen', 'Yuhan', ''], ['Luo', 'Yihong', ''], ['Song', 'Yifan', ''], ['Dai', 'Pengwen', ''], ['Tang', 'Jing', ''], ['Cao', 'Xiaochun', '']]","extracted_entities":"[{'text': 'DeGEM', 'label': 'LLM'}, {'text': 'DeGEM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"DeGEM","similarity_score":0.5001823902}
{"id":2503.02191,"submitter":"Mia Mohammad Imran","authors":"Mia Mohammad Imran, Robert Zita, Rebekah Copeland, Preetha Chatterjee,\n  Rahat Rizvi Rahman, and Kostadin Damevski","title":"Understanding and Predicting Derailment in Toxic Conversations on GitHub","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Software projects thrive on the involvement and contributions of individuals\nfrom different backgrounds. However, toxic language and negative interactions\ncan hinder the participation and retention of contributors and alienate\nnewcomers. Proactive moderation strategies aim to prevent toxicity from\noccurring by addressing conversations that have derailed from their intended\npurpose. This study aims to understand and predict conversational derailment\nleading to toxicity on GitHub.\n  To facilitate this research, we curate a novel dataset comprising 202 toxic\nconversations from GitHub with annotated derailment points, along with 696\nnon-toxic conversations as a baseline. Based on this dataset, we identify\nunique characteristics of toxic conversations and derailment points, including\nlinguistic markers such as second-person pronouns, negation terms, and tones of\nBitter Frustration and Impatience, as well as patterns in conversational\ndynamics between project contributors and external participants.\n  Leveraging these empirical observations, we propose a proactive moderation\napproach to automatically detect and address potentially harmful conversations\nbefore escalation. By utilizing modern LLMs, we develop a conversation\ntrajectory summary technique that captures the evolution of discussions and\nidentifies early signs of derailment. Our experiments demonstrate that LLM\nprompts tailored to provide summaries of GitHub conversations achieve 70%\nF1-Score in predicting conversational derailment, strongly improving over a set\nof baseline approaches.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 02:01:37 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 03:25:44 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 14:54:16 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Imran', 'Mia Mohammad', ''], ['Zita', 'Robert', ''], ['Copeland', 'Rebekah', ''], ['Chatterjee', 'Preetha', ''], ['Rahman', 'Rahat Rizvi', ''], ['Damevski', 'Kostadin', '']]","extracted_entities":"[{'text': 'GitHub', 'label': 'Open-source LLMs'}, {'text': 'GitHub', 'label': 'Open-source LLMs'}, {'text': 'modern LLMs', 'label': 'LLM'}, {'text': 'GitHub', 'label': 'Open-source LLMs'}]","assigned_concept":"LLM","matched_keyword":"modern LLMs","similarity_score":0.7401012182}
{"id":2503.1063,"submitter":"Hang Yin","authors":"Hang Yin, Xiuwei Xu, Lingqing Zhao, Ziwei Wang, Jie Zhou, Jiwen Lu","title":"UniGoal: Towards Universal Zero-shot Goal-oriented Navigation","comments":"Accepted to CVPR 2025. Project page:\n  https:\/\/bagh2178.github.io\/UniGoal\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:59:48 GMT'}, {'version': 'v2', 'created': 'Sun, 16 Mar 2025 15:11:27 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 10:07:07 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Yin', 'Hang', ''], ['Xu', 'Xiuwei', ''], ['Zhao', 'Lingqing', ''], ['Wang', 'Ziwei', ''], ['Zhou', 'Jie', ''], ['Lu', 'Jiwen', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2503.12349,"submitter":"Jianzhu Yao","authors":"Jianzhu Yao, Kevin Wang, Ryan Hsieh, Haisu Zhou, Tianqing Zou, Zerui\n  Cheng, Zhangyang Wang, Pramod Viswanath","title":"SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?","comments":"51 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Reasoning and strategic behavior in social interactions is a hallmark of\nintelligence. This form of reasoning is significantly more sophisticated than\nisolated planning or reasoning tasks in static settings (e.g., math problem\nsolving). In this paper, we present Strategic Planning, Interaction, and\nNegotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the\nintelligence of strategic planning and social reasoning. While many existing\nbenchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench\ncombines classical PDDL tasks, competitive board games, cooperative card games,\nand multi-agent negotiation scenarios in one unified framework. The framework\nincludes both a benchmark as well as an arena to simulate and evaluate the\nvariety of social settings to test reasoning and strategic behavior of AI\nagents. We formulate the benchmark SPIN-Bench by systematically varying action\nspaces, state complexity, and the number of interacting agents to simulate a\nvariety of social settings where success depends on not only methodical and\nstep-wise decision making, but also conceptual inference of other (adversarial\nor cooperative) participants. Our experiments reveal that while contemporary\nLLMs handle basic fact retrieval and short-range planning reasonably well, they\nencounter significant performance bottlenecks in tasks requiring deep multi-hop\nreasoning over large state spaces and socially adept coordination under\nuncertainty. We envision SPIN-Bench as a catalyst for future research on robust\nmulti-agent planning, social reasoning, and human--AI teaming. Project Website:\nhttps:\/\/spinbench.github.io\/\n","versions":"[{'version': 'v1', 'created': 'Sun, 16 Mar 2025 04:10:53 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 01:34:17 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Yao', 'Jianzhu', ''], ['Wang', 'Kevin', ''], ['Hsieh', 'Ryan', ''], ['Zhou', 'Haisu', ''], ['Zou', 'Tianqing', ''], ['Cheng', 'Zerui', ''], ['Wang', 'Zhangyang', ''], ['Viswanath', 'Pramod', '']]","extracted_entities":"[{'text': 'contemporary\\nLLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"contemporary\nLLMs","similarity_score":0.7631023526}
{"id":2503.13102,"submitter":"Ekaterina Artemova","authors":"Alexander Pugachev, Alena Fenogenova, Vladislav Mikhailov, Ekaterina\n  Artemova","title":"REPA: Russian Error Types Annotation for Evaluating Text Generation and\n  Judgment Capabilities","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent advances in large language models (LLMs) have introduced the novel\nparadigm of using LLMs as judges, where an LLM evaluates and scores the outputs\nof another LLM, which often correlates highly with human preferences. However,\nthe use of LLM-as-a-judge has been primarily studied in English. In this paper,\nwe evaluate this framework in Russian by introducing the Russian Error tyPes\nAnnotation dataset (REPA), a dataset of 1k user queries and 2k LLM-generated\nresponses. Human annotators labeled each response pair expressing their\npreferences across ten specific error types, as well as selecting an overall\npreference. We rank six generative LLMs across the error types using three\nrating systems based on human preferences. We also evaluate responses using\neight LLM judges in zero-shot and few-shot settings. We describe the results of\nanalyzing the judges and position and length biases. Our findings reveal a\nnotable gap between LLM judge performance in Russian and English. However,\nrankings based on human and LLM preferences show partial alignment, suggesting\nthat while current LLM judges struggle with fine-grained evaluation in Russian,\nthere is potential for improvement.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 12:15:16 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Pugachev', 'Alexander', ''], ['Fenogenova', 'Alena', ''], ['Mikhailov', 'Vladislav', ''], ['Artemova', 'Ekaterina', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'zero-shot and few-shot settings', 'label': 'Few-shot Learning'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2503.13105,"submitter":"Qian Wei","authors":"Qian Wei, Yi Li, Zehao Chen, Zhaoyan Shen, Dongxiao Yu, Bingzhe Li","title":"Managing Hybrid Solid-State Drives Using Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Hybrid Solid-State Drives (SSDs), which integrate several types of flash\ncells (e.g., single-level cell (SLC) and multiple-level cell (MLC)) in a single\ndrive and enable them to convert between each other, are designed to deliver\nboth high performance and high storage capacity. However, compared to\ntraditional SSDs, hybrid SSDs also introduce a much larger design space,\nresulting in higher optimization complexity due to more design factors\ninvolved, including flash conversion timing and data migration between\ndifferent flash cells, etc. To address these challenges, large language models\n(LLMs) could be a promising technique, as they excel in handling complex,\nhigh-dimensional parameter space exploration by leveraging their advanced\ncapability to identify patterns and optimize solutions. Recent works have\nstarted exploring the use of LLMs to optimize computer systems. However, to the\nbest of our knowledge, no study has focused on optimizing SSDs with the\nassistance of LLMs.\n  In this work, we explore the potential of LLMs in understanding and\nefficiently managing hybrid SSD design space. Specifically, two important\nquestions are exploited and analyzed: 1) Can LLMs offer optimization potential\nfor Hybrid SSD management? 2) How to leverage LLMs for the performance and\nefficiency of hybrid SSD optimization? Based on the observations of\nexploration, we propose a comprehensive auto-tuning framework for hybrid SSDs,\nintegrating LLMs to recommend customized configurations using calibration\nprompts derived from hardware, system, and workload information. Experimental\nresults reveal a 62.35% improvement in throughput and a 57.99% decrease in\nwrite amplification compared to the default hybrid SSD configurations achieved\nwith the incorporation of LLMs.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 12:25:42 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wei', 'Qian', ''], ['Li', 'Yi', ''], ['Chen', 'Zehao', ''], ['Shen', 'Zhaoyan', ''], ['Yu', 'Dongxiao', ''], ['Li', 'Bingzhe', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'calibration\\nprompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2503.13565,"submitter":"Evangelos Georganas","authors":"Evangelos Georganas, Dhiraj Kalamkar, Alexander Kozlov, Alexander\n  Heinecke","title":"ML-SpecQD: Multi-Level Speculative Decoding with Quantized Drafts","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Speculative decoding (SD) has emerged as a method to accelerate LLM inference\nwithout sacrificing any accuracy over the 16-bit model inference. In a typical\nSD setup, the idea is to use a full-precision, small, fast model as \"draft\" to\ngenerate the next few tokens and use the \"target\" large model to verify the\ndraft-generated tokens. The efficacy of this method heavily relies on the\nacceptance ratio of the draft-generated tokens and the relative token\nthroughput of the draft versus the target model. Nevertheless, an efficient SD\npipeline requires pre-training and aligning the draft model to the target\nmodel, making it impractical for LLM inference in a plug-and-play fashion. In\nthis work, we propose using MXFP4 models as drafts in a plug-and-play fashion\nsince the MXFP4 Weight-Only-Quantization (WOQ) merely direct-casts the BF16\ntarget model weights to MXFP4. In practice, our plug-and-play solution gives\nspeedups up to 2x over the BF16 baseline. Then we pursue an opportunity for\nfurther acceleration: the MXFP4 draft token generation itself can be\naccelerated via speculative decoding by using yet another smaller draft. We\ncall our method ML-SpecQD: Multi-Level Speculative Decoding with Quantized\nDrafts since it recursively applies speculation for accelerating the\ndraft-token generation. Combining Multi-Level Speculative Decoding with MXFP4\nQuantized Drafts we outperform state-of-the-art speculative decoding, yielding\nspeedups up to 2.72x over the BF16 baseline.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 08:38:45 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Georganas', 'Evangelos', ''], ['Kalamkar', 'Dhiraj', ''], ['Kozlov', 'Alexander', ''], ['Heinecke', 'Alexander', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2503.13657,"submitter":"Mert Cemri","authors":"Mert Cemri, Melissa Z. Pan, Shuyi Yang, Lakshya A. Agrawal, Bhavya\n  Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan\n  Ramchandran, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica","title":"Why Do Multi-Agent LLM Systems Fail?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Despite growing enthusiasm for Multi-Agent Systems (MAS), where multiple LLM\nagents collaborate to accomplish tasks, their performance gains across popular\nbenchmarks remain minimal compared to single-agent frameworks. This gap\nhighlights the need to analyze the challenges hindering MAS effectiveness.\n  In this paper, we present the first comprehensive study of MAS challenges. We\nanalyze five popular MAS frameworks across over 150 tasks, involving six expert\nhuman annotators. We identify 14 unique failure modes and propose a\ncomprehensive taxonomy applicable to various MAS frameworks. This taxonomy\nemerges iteratively from agreements among three expert annotators per study,\nachieving a Cohen's Kappa score of 0.88. These fine-grained failure modes are\norganized into 3 categories, (i) specification and system design failures, (ii)\ninter-agent misalignment, and (iii) task verification and termination. To\nsupport scalable evaluation, we integrate MASFT with LLM-as-a-Judge. We also\nexplore if identified failures could be easily prevented by proposing two\ninterventions: improved specification of agent roles and enhanced orchestration\nstrategies. Our findings reveal that identified failures require more complex\nsolutions, highlighting a clear roadmap for future research. We open-source our\ndataset and LLM annotator.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 19:04:38 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Cemri', 'Mert', ''], ['Pan', 'Melissa Z.', ''], ['Yang', 'Shuyi', ''], ['Agrawal', 'Lakshya A.', ''], ['Chopra', 'Bhavya', ''], ['Tiwari', 'Rishabh', ''], ['Keutzer', 'Kurt', ''], ['Parameswaran', 'Aditya', ''], ['Klein', 'Dan', ''], ['Ramchandran', 'Kannan', ''], ['Zaharia', 'Matei', ''], ['Gonzalez', 'Joseph E.', ''], ['Stoica', 'Ion', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM-as-a-Judge', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2503.13891,"submitter":"Xiaoying Xing","authors":"Xiaoying Xing, Chia-Wen Kuo, Li Fuxin, Yulei Niu, Fan Chen, Ming Li,\n  Ying Wu, Longyin Wen, Sijie Zhu","title":"Where do Large Vision-Language Models Look at when Answering Questions?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Vision-Language Models (LVLMs) have shown promising performance in\nvision-language understanding and reasoning tasks. However, their visual\nunderstanding behaviors remain underexplored. A fundamental question arises: to\nwhat extent do LVLMs rely on visual input, and which image regions contribute\nto their responses? It is non-trivial to interpret the free-form generation of\nLVLMs due to their complicated visual architecture (e.g., multiple encoders and\nmulti-resolution) and variable-length outputs. In this paper, we extend\nexisting heatmap visualization methods (e.g., iGOS++) to support LVLMs for\nopen-ended visual question answering. We propose a method to select visually\nrelevant tokens that reflect the relevance between generated answers and input\nimage. Furthermore, we conduct a comprehensive analysis of state-of-the-art\nLVLMs on benchmarks designed to require visual information to answer. Our\nfindings offer several insights into LVLM behavior, including the relationship\nbetween focus region and answer correctness, differences in visual attention\nacross architectures, and the impact of LLM scale on visual understanding. The\ncode and data are available at\nhttps:\/\/github.com\/bytedance\/LVLM_Interpretation.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 04:34:43 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Xing', 'Xiaoying', ''], ['Kuo', 'Chia-Wen', ''], ['Fuxin', 'Li', ''], ['Niu', 'Yulei', ''], ['Chen', 'Fan', ''], ['Li', 'Ming', ''], ['Wu', 'Ying', ''], ['Wen', 'Longyin', ''], ['Zhu', 'Sijie', '']]","extracted_entities":"[{'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'visual attention', 'label': 'Attention mechanism'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2503.13975,"submitter":"Omar Shaikh","authors":"Omar Shaikh, Hussein Mozannar, Gagan Bansal, Adam Fourney, Eric\n  Horvitz","title":"Navigating Rifts in Human-LLM Grounding: Study and Benchmark","comments":"16 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.HC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Language models excel at following instructions but often struggle with the\ncollaborative aspects of conversation that humans naturally employ. This\nlimitation in grounding -- the process by which conversation participants\nestablish mutual understanding -- can lead to outcomes ranging from frustrated\nusers to serious consequences in high-stakes scenarios. To systematically study\ngrounding challenges in human-LLM interactions, we analyze logs from three\nhuman-assistant datasets: WildChat, MultiWOZ, and Bing Chat. We develop a\ntaxonomy of grounding acts and build models to annotate and forecast grounding\nbehavior. Our findings reveal significant differences in human-human and\nhuman-LLM grounding: LLMs were three times less likely to initiate\nclarification and sixteen times less likely to provide follow-up requests than\nhumans. Additionally, early grounding failures predicted later interaction\nbreakdowns. Building on these insights, we introduce RIFTS: a benchmark derived\nfrom publicly available LLM interaction data containing situations where LLMs\nfail to initiate grounding. We note that current frontier models perform poorly\non RIFTS, highlighting the need to reconsider how we train and prompt LLMs for\nhuman interaction. To this end, we develop a preliminary intervention that\nmitigates grounding failures.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 07:24:05 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Shaikh', 'Omar', ''], ['Mozannar', 'Hussein', ''], ['Bansal', 'Gagan', ''], ['Fourney', 'Adam', ''], ['Horvitz', 'Eric', '']]","extracted_entities":"[{'text': 'WildChat', 'label': 'Open-source LLMs'}, {'text': 'MultiWOZ', 'label': 'ChatGPT'}, {'text': 'Bing Chat', 'label': 'ChatGPT'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'RIFTS', 'label': 'ChatGPT'}, {'text': 'publicly available LLM interaction data', 'label': 'Open-source LLMs'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'RIFTS', 'label': 'ChatGPT'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2503.14488,"submitter":"Shraddha Surana","authors":"Shraddha Surana and Ashwin Srinivasan","title":"Engineering Scientific Assistants using Interactive Structured Induction\n  of Programs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  We are interested in the construction of software that can act as scientific\nassistants to domain specialists. It is expected that such assistants will be\nneeded to accelerate the identification of ways to address complex problems\nrequiring urgent solutions. In this paper, our focus is not on a specific\nscientific problem, but on the software-engineering of such 'science\naccelerators'. Recent developments in 'No Code' techniques would seem to\nsuggest that scientist can simply hypothesise solutions simply by conversing\nwith a large language model (LLM). However, for complex scientific problems,\nthis seems unlikely given the current state of LLM technology. What does appear\nfeasible is that a software engineer can use LLMs to rapidly construct programs\nfor use by a domain-specialist, including the specialist's requirements\nexpressed in natural language. We propose the design of an interactive form of\n'structured' inductive programming in which a software-engineer and an LLM\ncollaboratively construct an 'assistant' for a scientific data analysis. The\npaper describes a simple implementation called iStrucInd that adapts a '2-way\nIntelligibility' protocol to implement the interaction between the software\nengineer and the LLM. We test the tool on two different non-trivial scientific\ndata analysis tasks. Specifically, we compare the system constructed by\niStrucInd against systems constructed manually and by Low Code\/No Code methods\nalong dimensions of: (a) program performance; (b) program quality; and (c)\nprogramming effort. The results show iStrucInd allows a software engineer to\ndevelop better programs faster suggesting interactive structured induction can\nplay a useful role in the rapid construction of scientific assistants.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:57:16 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Surana', 'Shraddha', ''], ['Srinivasan', 'Ashwin', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2503.14662,"submitter":"Yicheng Fu","authors":"Yicheng Fu, Zikui Wang, Liuxin Yang, Meiqing Huo, and Zhongdongming\n  Dai","title":"ConQuer: A Framework for Concept-Based Quiz Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Quizzes play a crucial role in education by reinforcing students'\nunderstanding of key concepts and encouraging self-directed exploration.\nHowever, compiling high-quality quizzes can be challenging and require deep\nexpertise and insight into specific subject matter. Although LLMs have greatly\nenhanced the efficiency of quiz generation, concerns remain regarding the\nquality of these AI-generated quizzes and their educational impact on students.\nTo address these issues, we introduce ConQuer, a concept-based quiz generation\nframework that leverages external knowledge sources. We employ comprehensive\nevaluation dimensions to assess the quality of the generated quizzes, using\nLLMs as judges. Our experiment results demonstrate a 4.8% improvement in\nevaluation scores and a 77.52% win rate in pairwise comparisons against\nbaseline quiz sets. Ablation studies further underscore the effectiveness of\neach component in our framework. Code available at\nhttps:\/\/github.com\/sofyc\/ConQuer.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 19:10:26 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Fu', 'Yicheng', ''], ['Wang', 'Zikui', ''], ['Yang', 'Liuxin', ''], ['Huo', 'Meiqing', ''], ['Dai', 'Zhongdongming', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2503.14831,"submitter":"Sojeong Park","authors":"Sojeong Park, Hyeonho Noh, Hyun Jong Yang","title":"Robust Transmission of Punctured Text with Large Language Model-based\n  Recovery","comments":"This work has been submitted to the IEEE for possible publication","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  With the recent advancements in deep learning, semantic communication which\ntransmits only task-oriented features, has rapidly emerged. However, since\nfeature extraction relies on learning-based models, its performance\nfundamentally depends on the training dataset or tasks. For practical\nscenarios, it is essential to design a model that demonstrates robust\nperformance regardless of dataset or tasks. In this correspondence, we propose\na novel text transmission model that selects and transmits only a few\ncharacters and recovers the missing characters at the receiver using a large\nlanguage model (LLM). Additionally, we propose a novel importance character\nextractor (ICE), which selects transmitted characters to enhance LLM recovery\nperformance. Simulations demonstrate that the proposed filter selection by ICE\noutperforms random filter selection, which selects transmitted characters\nrandomly. Moreover, the proposed model exhibits robust performance across\ndifferent datasets and tasks and outperforms traditional bit-based\ncommunication in low signal-to-noise ratio conditions.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 02:16:08 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Park', 'Sojeong', ''], ['Noh', 'Hyeonho', ''], ['Yang', 'Hyun Jong', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2503.14853,"submitter":"Peipeng Yu","authors":"Peipeng Yu, Jianwei Fei, Hui Gao, Xuan Feng, Zhihua Xia, Chip Hong\n  Chang","title":"Unlocking the Capabilities of Vision-Language Models for Generalizable\n  and Explainable Deepfake Detection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Current vision-language models (VLMs) have demonstrated remarkable\ncapabilities in understanding multimodal data, but their potential remains\nunderexplored for deepfake detection due to the misaligned of their knowledge\nand forensics patterns. To this end, we present a novel paradigm that unlocks\nVLMs' potential capabilities through three components: (1) A knowledge-guided\nforgery adaptation module that aligns VLM's semantic space with forensic\nfeatures through contrastive learning with external manipulation knowledge; (2)\nA multi-modal prompt tuning framework that jointly optimizes visual-textual\nembeddings for both localization and explainability; (3) An iterative\nrefinement strategy enabling multi-turn dialog for evidence-based reasoning.\nOur framework includes a VLM-based Knowledge-guided Forgery Detector (KFD), a\nVLM image encoder, and a Large Language Model (LLM). The VLM image encoder\nextracts visual prompt embeddings from images, while the LLM receives visual\nand question prompt embeddings for inference. The KFD is used to calculate\ncorrelations between image features and pristine\/deepfake class embeddings,\nenabling forgery classification and localization. The outputs from these\ncomponents are used to construct forgery prompt embeddings. Finally, we feed\nthese prompt embeddings into the LLM to generate textual detection responses to\nassist judgment. Extensive experiments on multiple benchmarks, including FF++,\nCDF2, DFD, DFDCP, and DFDC, demonstrate that our scheme surpasses\nstate-of-the-art methods in generalization performance, while also supporting\nmulti-turn dialogue capabilities.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 03:20:03 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Yu', 'Peipeng', ''], ['Fei', 'Jianwei', ''], ['Gao', 'Hui', ''], ['Feng', 'Xuan', ''], ['Xia', 'Zhihua', ''], ['Chang', 'Chip Hong', '']]","extracted_entities":"[{'text': 'contrastive learning', 'label': 'Few-shot Learning'}, {'text': 'visual-textual\\nembeddings', 'label': 'contextual Embedding'}, {'text': 'multi-turn dialog', 'label': 'ChatGPT'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'visual prompt embeddings', 'label': 'Prompting'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'forgery prompt embeddings', 'label': 'Prompting'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2503.15128,"submitter":"Dominik Macko","authors":"Dominik Macko, Robert Moro, Ivan Srba","title":"Increasing the Robustness of the Fine-tuned Multilingual\n  Machine-Generated Text Detectors","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Since the proliferation of LLMs, there have been concerns about their misuse\nfor harmful content creation and spreading. Recent studies justify such fears,\nproviding evidence of LLM vulnerabilities and high potential of their misuse.\nHumans are no longer able to distinguish between high-quality machine-generated\nand authentic human-written texts. Therefore, it is crucial to develop\nautomated means to accurately detect machine-generated content. It would enable\nto identify such content in online information space, thus providing an\nadditional information about its credibility. This work addresses the problem\nby proposing a robust fine-tuning process of LLMs for the detection task,\nmaking the detectors more robust against obfuscation and more generalizable to\nout-of-distribution data.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:42:33 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Macko', 'Dominik', ''], ['Moro', 'Robert', ''], ['Srba', 'Ivan', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'robust fine-tuning process', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2503.15282,"submitter":"Mootez Saad","authors":"Mootez Saad, Jos\\'e Antonio Hern\\'andez L\\'opez, Boqi Chen, Neil\n  Ernst, D\\'aniel Varr\\'o, Tushar Sharma","title":"SENAI: Towards Software Engineering Native Generative Artificial\n  Intelligence","comments":"5 pages, 1 figure","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models have significantly advanced the field of code\ngeneration, demonstrating the ability to produce functionally correct code\nsnippets. However, advancements in generative AI for code overlook foundational\nSoftware Engineering (SE) principles such as modularity, and single\nresponsibility, and concepts such as cohesion and coupling which are critical\nfor creating maintainable, scalable, and robust software systems. These\nconcepts are missing in pipelines that start with pre-training and end with the\nevaluation using benchmarks.\n  This vision paper argues for the integration of SE knowledge into LLMs to\nenhance their capability to understand, analyze, and generate code and other SE\nartifacts following established SE knowledge. The aim is to propose a new\ndirection where LLMs can move beyond mere functional accuracy to perform\ngenerative tasks that require adherence to SE principles and best practices. In\naddition, given the interactive nature of these conversational models, we\npropose using Bloom's Taxonomy as a framework to assess the extent to which\nthey internalize SE knowledge. The proposed evaluation framework offers a sound\nand more comprehensive evaluation technique compared to existing approaches\nsuch as linear probing. Software engineering native generative models will not\nonly overcome the shortcomings present in current models but also pave the way\nfor the next generation of generative models capable of handling real-world\nsoftware engineering.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:02:07 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Saad', 'Mootez', ''], ['L\u00f3pez', 'Jos\u00e9 Antonio Hern\u00e1ndez', ''], ['Chen', 'Boqi', ''], ['Ernst', 'Neil', ''], ['Varr\u00f3', 'D\u00e1niel', ''], ['Sharma', 'Tushar', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2503.15475,"submitter":"Kangle Deng","authors":"Foundation AI Team Roblox: Kiran Bhat, Nishchaie Khanna, Karun Channa,\n  Tinghui Zhou, Yiheng Zhu, Xiaoxia Sun, Charles Shang, Anirudh Sudarshan,\n  Maurice Chu, Daiqing Li, Kangle Deng, Jean-Philippe Fauconnier, Tijmen\n  Verhulsdonck, Maneesh Agrawala, Kayvon Fatahalian, Alexander Weiss, Christian\n  Reiser, Ravi Kiran Chirravuri, Ravali Kandur, Alejandro Pelaez, Akash Garg,\n  Michael Palleschi, Jessica Wang, Skylar Litz, Leon Liu, Anying Li, David\n  Harmon, Derek Liu, Liangjun Feng, Denis Goupil, Lukas Kuczynski, Jihyun Yoon,\n  Naveen Marri, Peiye Zhuang, Yinan Zhang, Brian Yin, Haomiao Jiang, Marcel van\n  Workum, Thomas Lane, Bryce Erickson, Salil Pathare, Kyle Price, Anupam Singh,\n  David Baszucki","title":"Cube: A Roblox View of 3D Intelligence","comments":"Our code and model weights can be found at:\n  https:\/\/github.com\/Roblox\/cube","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Foundation models trained on vast amounts of data have demonstrated\nremarkable reasoning and generation capabilities in the domains of text,\nimages, audio and video. Our goal at Roblox is to build such a foundation model\nfor 3D intelligence, a model that can support developers in producing all\naspects of a Roblox experience, from generating 3D objects and scenes to\nrigging characters for animation to producing programmatic scripts describing\nobject behaviors. We discuss three key design requirements for such a 3D\nfoundation model and then present our first step towards building such a model.\nWe expect that 3D geometric shapes will be a core data type and describe our\nsolution for 3D shape tokenizer. We show how our tokenization scheme can be\nused in applications for text-to-shape generation, shape-to-text generation and\ntext-to-scene generation. We demonstrate how these applications can collaborate\nwith existing large language models (LLMs) to perform scene analysis and\nreasoning. We conclude with a discussion outlining our path to building a fully\nunified foundation model for 3D intelligence.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:52:17 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Foundation AI Team', '', ''], ['Bhat', 'Kiran', ''], ['Khanna', 'Nishchaie', ''], ['Channa', 'Karun', ''], ['Zhou', 'Tinghui', ''], ['Zhu', 'Yiheng', ''], ['Sun', 'Xiaoxia', ''], ['Shang', 'Charles', ''], ['Sudarshan', 'Anirudh', ''], ['Chu', 'Maurice', ''], ['Li', 'Daiqing', ''], ['Deng', 'Kangle', ''], ['Fauconnier', 'Jean-Philippe', ''], ['Verhulsdonck', 'Tijmen', ''], ['Agrawala', 'Maneesh', ''], ['Fatahalian', 'Kayvon', ''], ['Weiss', 'Alexander', ''], ['Reiser', 'Christian', ''], ['Chirravuri', 'Ravi Kiran', ''], ['Kandur', 'Ravali', ''], ['Pelaez', 'Alejandro', ''], ['Garg', 'Akash', ''], ['Palleschi', 'Michael', ''], ['Wang', 'Jessica', ''], ['Litz', 'Skylar', ''], ['Liu', 'Leon', ''], ['Li', 'Anying', ''], ['Harmon', 'David', ''], ['Liu', 'Derek', ''], ['Feng', 'Liangjun', ''], ['Goupil', 'Denis', ''], ['Kuczynski', 'Lukas', ''], ['Yoon', 'Jihyun', ''], ['Marri', 'Naveen', ''], ['Zhuang', 'Peiye', ''], ['Zhang', 'Yinan', ''], ['Yin', 'Brian', ''], ['Jiang', 'Haomiao', ''], ['van Workum', 'Marcel', ''], ['Lane', 'Thomas', ''], ['Erickson', 'Bryce', ''], ['Pathare', 'Salil', ''], ['Price', 'Kyle', ''], ['Singh', 'Anupam', ''], ['Baszucki', 'David', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2503.15551,"submitter":"Murong Yue","authors":"Murong Yue, Ziyu Yao","title":"Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting\n  Attack","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Batch prompting, which combines a batch of multiple queries sharing the same\ncontext in one inference, has emerged as a promising solution to reduce\ninference costs. However, our study reveals a significant security\nvulnerability in batch prompting: malicious users can inject attack\ninstructions into a batch, leading to unwanted interference across all queries,\nwhich can result in the inclusion of harmful content, such as phishing links,\nor the disruption of logical reasoning. In this paper, we construct\nBATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions of\ntwo types and 8k batch instances, to study the batch prompting vulnerability\nsystematically. Our evaluation of both closed-source and open-weight LLMs\ndemonstrates that all LLMs are susceptible to batch-prompting attacks. We then\nexplore multiple defending approaches. While the prompting-based defense shows\nlimited effectiveness for smaller LLMs, the probing-based approach achieves\nabout 95% accuracy in detecting attacks. Additionally, we perform a mechanistic\nanalysis to understand the attack and identify attention heads that are\nresponsible for it.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 15:16:10 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Yue', 'Murong', ''], ['Yao', 'Ziyu', '']]","extracted_entities":"[{'text': 'Batch prompting', 'label': 'Prompting'}, {'text': 'batch prompting', 'label': 'Prompting'}, {'text': 'batch prompting', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'attention heads', 'label': 'Attention mechanism'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2503.15554,"submitter":"Shih-Chieh Dai","authors":"Shih-Chieh Dai, Jun Xu, Guanhong Tao","title":"A Comprehensive Study of LLM Secure Code Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.LG cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  LLMs are widely used in software development. However, the code generated by\nLLMs often contains vulnerabilities. Several secure code generation methods\nhave been proposed to address this issue, but their current evaluation schemes\nleave several concerns unaddressed. Specifically, most existing studies\nevaluate security and functional correctness separately, using different\ndatasets. That is, they assess vulnerabilities using security-related code\ndatasets while validating functionality with general code datasets. In\naddition, prior research primarily relies on a single static analyzer, CodeQL,\nto detect vulnerabilities in generated code, which limits the scope of security\nevaluation.\n  In this work, we conduct a comprehensive study to systematically assess the\nimprovements introduced by four state-of-the-art secure code generation\ntechniques. Specifically, we apply both security inspection and functionality\nvalidation to the same generated code and evaluate these two aspects together.\nWe also employ three popular static analyzers and two LLMs to identify\npotential vulnerabilities in the generated code. Our study reveals that\nexisting techniques often compromise the functionality of generated code to\nenhance security. Their overall performance remains limited when evaluating\nsecurity and functionality together. In fact, many techniques even degrade the\nperformance of the base LLM. Our further inspection reveals that these\ntechniques often either remove vulnerable lines of code entirely or generate\n``garbage code'' that is unrelated to the intended task. Moreover, the commonly\nused static analyzer CodeQL fails to detect several vulnerabilities, further\nobscuring the actual security improvements achieved by existing techniques. Our\nstudy serves as a guideline for a more rigorous and comprehensive evaluation of\nsecure code generation performance in future work.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 20:12:50 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Dai', 'Shih-Chieh', ''], ['Xu', 'Jun', ''], ['Tao', 'Guanhong', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'CodeQL', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'CodeQL', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2503.15556,"submitter":"Daniel Karapetyan Dr","authors":"Daniel Karapetyan","title":"Fully Automated Generation of Combinatorial Optimisation Systems Using\n  Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.PL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Over the last few decades, there has been a considerable effort to make\ndecision support more accessible for small and medium enterprises by reducing\nthe cost of design, development and maintenance of automated decision support\nsystems. However, due to the diversity of the underlying combinatorial\noptimisation problems, reusability of such systems has been limited; in most\ncases, expensive expertise has been necessary to implement bespoke software\ncomponents.\n  We investigate the possibility of fully automated generation of combinatorial\noptimisation systems by utilising the large language models (LLMs). An LLM will\nbe responsible for interpreting the problem description provided by the user in\na natural language and designing and implementing problem-specific software\ncomponents. We discuss the principles of fully automated LLM-based generation\nof optimisation systems, and evaluate several proof-of-concept generators,\ncomparing their performance on four optimisation problems.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 20:23:51 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Karapetyan', 'Daniel', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLM","similarity_score":1.0}
{"id":2503.16144,"submitter":"Mathieu Acher","authors":"Djamel Eddine Khelladi and Charly Reux and Mathieu Acher","title":"Unify and Triumph: Polyglot, Diverse, and Self-Consistent Generation of\n  Unit Tests with LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language model (LLM)-based test generation has gained attention in\nsoftware engineering, yet most studies evaluate LLMs' ability to generate unit\ntests in a single attempt for a given language, missing the opportunity to\nleverage LLM diversity for more robust testing. This paper introduces PolyTest,\na novel approach that enhances test generation by exploiting polyglot and\ntemperature-controlled diversity. PolyTest systematically leverages these\nproperties in two complementary ways: (1) Cross-lingual test generation, where\ntests are generated in multiple languages at zero temperature and then unified;\n(2) Diverse test sampling, where multiple test sets are generated within the\nsame language at a higher temperature before unification. A key insight is that\nLLMs can generate diverse yet contradicting tests -- same input, different\nexpected outputs -- across languages and generations. PolyTest mitigates\ninconsistencies by unifying test sets, fostering self-consistency and improving\noverall test quality. Unlike single-language or single-attempt approaches,\nPolyTest enhances testing without requiring on-the-fly execution, making it\nparticularly beneficial for weaker-performing languages. We evaluate PolyTest\non Llama3-70B, GPT-4o, and GPT-3.5 using EvalPlus, generating tests in five\nlanguages (Java, C, Python, JavaScript, and a CSV-based format) at temperature\n0 and sampling multiple sets at temperature 1. We observe that LLMs frequently\ngenerate contradicting tests across settings, and that PolyTest significantly\nimproves test quality across all considered metrics -- number of tests, passing\nrate, statement\/branch coverage (up to +9.01%), and mutation score (up to\n+11.23%). Finally, PolyTest outperforms Pynguin in test generation, passing\nrate, and mutation score.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:47:06 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Khelladi', 'Djamel Eddine', ''], ['Reux', 'Charly', ''], ['Acher', 'Mathieu', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'PolyTest', 'label': 'LLMs'}, {'text': 'PolyTest', 'label': 'LLM-based'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'PolyTest', 'label': 'LLM-based'}, {'text': 'PolyTest', 'label': 'LLM-based'}, {'text': 'Llama3-70B', 'label': 'Llama'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'GPT-3.5', 'label': 'GPT'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'PolyTest', 'label': 'LLM-based'}, {'text': 'PolyTest', 'label': 'LLM-based'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2503.16158,"submitter":"Shenbin Qian","authors":"Shenbin Qian, Constantin Or\\u{a}san, Diptesh Kanojia, F\\'elix do Carmo","title":"Automatically Generating Chinese Homophone Words to Probe Machine\n  Translation Estimation Systems","comments":"Accepted to the 10th Workshop on Noisy and User-generated Text at\n  NAACL 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Evaluating machine translation (MT) of user-generated content (UGC) involves\nunique challenges such as checking whether the nuance of emotions from the\nsource are preserved in the target text. Recent studies have proposed\nemotion-related datasets, frameworks and models to automatically evaluate MT\nquality of Chinese UGC, without relying on reference translations. However,\nwhether these models are robust to the challenge of preserving emotional\nnuances has been left largely unexplored. To address this gap, we introduce a\nnovel method inspired by information theory which generates challenging Chinese\nhomophone words related to emotions, by leveraging the concept of\nself-information. Our approach generates homophones that were observed to cause\ntranslation errors in emotion preservation, and exposes vulnerabilities in MT\nsystems and their evaluation methods when tackling emotional UGC. We evaluate\nthe efficacy of our method using human evaluation for the quality of these\ngenerated homophones, and compare it with an existing one, showing that our\nmethod achieves higher correlation with human judgments. The generated Chinese\nhomophones, along with their manual translations, are utilized to generate\nperturbations and to probe the robustness of existing quality evaluation\nmodels, including models trained using multi-task learning, fine-tuned variants\nof multilingual language models, as well as large language models (LLMs). Our\nresults indicate that LLMs with larger size exhibit higher stability and\nrobustness to such perturbations. We release our data and code for\nreproducibility and further research.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:56:15 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Qian', 'Shenbin', ''], ['Or\u0103san', 'Constantin', ''], ['Kanojia', 'Diptesh', ''], ['Carmo', 'F\u00e9lix do', '']]","extracted_entities":"[{'text': 'multi-task learning', 'label': 'Few-shot Learning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"LLMs","similarity_score":0.8766149879}
{"id":2503.16243,"submitter":"Hamid Hamidani","authors":"Hamid Hamidani, Yuri Sato, Kazumi Kashiyama, Masaomi Tanaka, Kunihito\n  Ioka and Shigeo S. Kimura","title":"EP240414a: A Gamma-Ray Burst Jet Weakened by an Extended Circumstellar\n  Material","comments":"13 pages, 3 figures, to be submitted. Comments are welcome!","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.HE","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  The recent Einstein Probe (EP) event EP240414a exhibits several unusual\nobservational features. Its prompt and afterglow emissions place it between\nlong gamma-ray bursts (LGRBs) and low-luminosity GRBs (LLGRBs). The event is\nfollowed by a fast optical transient (AT~2024gsa), initially exhibiting a\nthermal-like spectrum but later evolving into an unusually red peak at $\\sim\n3-5$ days, which is difficult to explain with thermal emission. Using our\ngeneralized analytic framework for jet propagation in a circumstellar material\n(CSM; Hamidani et al. 2025), we explore a scenario in which a conventional LGRB\njet is launched in a progenitor surrounded by a dense CSM. For a CSM of $\\sim\n0.03 M_\\odot$ extending to $\\sim 3\\times 10^{13}$ cm, we find that the jet is\nsignificantly weakened before breaking out, becoming ``barely failed'', an\nintermediate state between successful (LGRB) and completely failed (LLGRB)\njets. This scenario naturally explains EP240414a's multi-wavelength\nobservations, with the early thermal component produced by cocoon cooling\nemission, and the red peak explained by non-thermal afterglow emission from the\nmildly relativistic barely failed jet (and its inner cocoon). Our work\ndemonstrates the important role of extended CSM in shaping GRB jets and\nillustrates how early multi-wavelength follow-up observations can reveal the\nphysically diverse nature of jet-driven transients.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 15:37:59 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Hamidani', 'Hamid', ''], ['Sato', 'Yuri', ''], ['Kashiyama', 'Kazumi', ''], ['Tanaka', 'Masaomi', ''], ['Ioka', 'Kunihito', ''], ['Kimura', 'Shigeo S.', '']]","extracted_entities":"[{'text': 'CSM', 'label': 'LLM'}, {'text': 'CSM', 'label': 'LLM'}, {'text': 'CSM', 'label': 'LLM'}, {'text': 'CSM', 'label': 'LLM'}]","assigned_concept":"LLM","matched_keyword":"CSM","similarity_score":0.5365145802}
{"id":2312.10048,"submitter":"Kavita Sharma","authors":"Kavita Sharma, Ritu Patel, Sunita Iyer","title":"Knowledge Graph Enhanced Aspect-Level Sentiment Analysis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this paper, we propose a novel method to enhance sentiment analysis by\naddressing the challenge of context-specific word meanings. It combines the\nadvantages of a BERT model with a knowledge graph based synonym data. This\nsynergy leverages a dynamic attention mechanism to develop a knowledge-driven\nstate vector. For classifying sentiments linked to specific aspects, the\napproach constructs a memory bank integrating positional data. The data are\nthen analyzed using a DCGRU to pinpoint sentiment characteristics related to\nspecific aspect terms. Experiments on three widely used datasets demonstrate\nthe superior performance of our method in sentiment classification.\n","versions":"[{'version': 'v1', 'created': 'Sat, 2 Dec 2023 04:45:17 GMT'}, {'version': 'v2', 'created': 'Sun, 14 Jan 2024 23:04:14 GMT'}, {'version': 'v3', 'created': 'Sat, 27 Jan 2024 00:09:23 GMT'}, {'version': 'v4', 'created': 'Tue, 18 Mar 2025 21:32:48 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Sharma', 'Kavita', ''], ['Patel', 'Ritu', ''], ['Iyer', 'Sunita', '']]","extracted_entities":"[{'text': 'BERT', 'label': 'BERT'}, {'text': 'dynamic attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"BERT","matched_keyword":"BERT","similarity_score":1.0}
{"id":2408.12871,"submitter":"Xiaochen Zhou","authors":"Zhou Xiaochen, Liang Xingzhou, Zou Hui, Lu Yi, Qu Jingjing","title":"DeepDiveAI: Identifying AI Related Documents in Large Scale Literature\n  Data","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this paper, we propose a method to automatically classify AI-related\ndocuments from large-scale literature databases, leading to the creation of an\nAI-related literature dataset, named DeepDiveAI. The dataset construction\napproach integrates expert knowledge with the capabilities of advanced models,\nstructured across two global stages. In the first stage, expert-curated\nclassification datasets are used to train an LSTM model, which classifies\ncoarse AI related records from large-scale datasets. In the second stage, we\nuse Qwen2.5 Plus to annotate a random 10% of the coarse AI-related records,\nwhich are then used to train a BERT binary classifier. This step further\nrefines the coarse AI related record set to obtain the final DeepDiveAI\ndataset. Evaluation results demonstrate that the entire workflow can\nefficiently and accurately identify AI-related literature from large-scale\ndatasets.\n","versions":"[{'version': 'v1', 'created': 'Fri, 23 Aug 2024 07:05:12 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Aug 2024 11:30:28 GMT'}, {'version': 'v3', 'created': 'Tue, 8 Oct 2024 07:21:57 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 12:46:22 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Xiaochen', 'Zhou', ''], ['Xingzhou', 'Liang', ''], ['Hui', 'Zou', ''], ['Yi', 'Lu', ''], ['Jingjing', 'Qu', '']]","extracted_entities":"[{'text': 'BERT', 'label': 'BERT'}]","assigned_concept":"BERT","matched_keyword":"BERT","similarity_score":1.0}
{"id":2409.10687,"submitter":"Ruchik Mishra","authors":"Ruchik Mishra, Andrew Frye, Madan Mohan Rayguru, Dan O. Popa","title":"Personalized Speech Emotion Recognition in Human-Robot Interaction using\n  Vision Transformers","comments":"This work has been accepted for the IEEE Robotics and Automation\n  Letters (RA-L)","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.AS cs.HC cs.RO cs.SD","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Emotions are an essential element in verbal communication, so understanding\nindividuals' affect during a human-robot interaction (HRI) becomes imperative.\nThis paper investigates the application of vision transformer models, namely\nViT (Vision Transformers) and BEiT (BERT Pre-Training of Image Transformers)\npipelines, for Speech Emotion Recognition (SER) in HRI. The focus is to\ngeneralize the SER models for individual speech characteristics by fine-tuning\nthese models on benchmark datasets and exploiting ensemble methods. For this\npurpose, we collected audio data from different human subjects having\npseudo-naturalistic conversations with the NAO robot. We then fine-tuned our\nViT and BEiT-based models and tested these models on unseen speech samples from\nthe participants. In the results, we show that fine-tuning vision transformers\non benchmark datasets and and then using either these already fine-tuned models\nor ensembling ViT\/BEiT models gets us the highest classification accuracies per\nindividual when it comes to identifying four primary emotions from their\nspeech: neutral, happy, sad, and angry, as compared to fine-tuning vanilla-ViTs\nor BEiTs.\n","versions":"[{'version': 'v1', 'created': 'Mon, 16 Sep 2024 19:34:34 GMT'}, {'version': 'v2', 'created': 'Fri, 22 Nov 2024 23:26:24 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 14:58:30 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Mishra', 'Ruchik', ''], ['Frye', 'Andrew', ''], ['Rayguru', 'Madan Mohan', ''], ['Popa', 'Dan O.', '']]","extracted_entities":"[{'text': 'ViT', 'label': 'Transformers'}, {'text': 'Vision Transformers', 'label': 'Transformers'}, {'text': 'BEiT', 'label': 'Transformers'}, {'text': 'BERT', 'label': 'BERT'}, {'text': 'ViT', 'label': 'Transformers'}, {'text': 'vision transformers', 'label': 'Transformers'}, {'text': 'ViT', 'label': 'Transformers'}, {'text': 'BEiT', 'label': 'Transformers'}, {'text': 'BEiTs', 'label': 'Transformers'}]","assigned_concept":"BERT","matched_keyword":"BERT","similarity_score":1.0}
{"id":2411.08726,"submitter":"Rui Liu","authors":"Rui Liu, Jiayou Liang, Haolong Chen and Yujia Hu","title":"Analyst Reports and Stock Performance: Evidence from the Chinese Market","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL q-fin.CP","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  This article applies natural language processing (NLP) to extract and\nquantify textual information to predict stock performance. Using an extensive\ndataset of Chinese analyst reports and employing a customized BERT deep\nlearning model for Chinese text, this study categorizes the sentiment of the\nreports as positive, neutral, or negative. The findings underscore the\npredictive capacity of this sentiment indicator for stock volatility, excess\nreturns, and trading volume. Specifically, analyst reports with strong positive\nsentiment will increase excess return and intraday volatility, and vice versa,\nreports with strong negative sentiment also increase volatility and trading\nvolume, but decrease future excess return. The magnitude of this effect is\ngreater for positive sentiment reports than for negative sentiment reports.\nThis article contributes to the empirical literature on sentiment analysis and\nthe response of the stock market to news in the Chinese stock market.\n","versions":"[{'version': 'v1', 'created': 'Wed, 13 Nov 2024 16:08:40 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 21:49:49 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Liu', 'Rui', ''], ['Liang', 'Jiayou', ''], ['Chen', 'Haolong', ''], ['Hu', 'Yujia', '']]","extracted_entities":"[{'text': 'BERT', 'label': 'BERT'}]","assigned_concept":"BERT","matched_keyword":"BERT","similarity_score":1.0}
{"id":2502.21206,"submitter":"Songrun He","authors":"Songrun He, Linying Lv, Asaf Manela, Jimmy Wu","title":"Chronologically Consistent Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"q-fin.GN q-fin.TR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models are increasingly used in social sciences, but their\ntraining data can introduce lookahead bias and training leakage. A good\nchronologically consistent language model requires efficient use of training\ndata to maintain accuracy despite time-restricted data. Here, we overcome this\nchallenge by training a suite of chronologically consistent large language\nmodels, ChronoBERT and ChronoGPT, which incorporate only the text data that\nwould have been available at each point in time. Despite this strict temporal\nconstraint, our models achieve strong performance on natural language\nprocessing benchmarks, outperforming or matching widely used models (e.g.,\nBERT), and remain competitive with larger open-weight models. Lookahead bias is\nmodel and application-specific because even if a chronologically consistent\nlanguage model has poorer language comprehension, a regression or prediction\nmodel applied on top of the language model can compensate. In an asset pricing\napplication predicting next-day stock returns from financial news, we find that\nChronoBERT's real-time outputs achieve a Sharpe ratio comparable to\nstate-of-the-art models, indicating that lookahead bias is modest. Our results\ndemonstrate a scalable, practical framework to mitigate training leakage,\nensuring more credible backtests and predictions across finance and other\nsocial science domains.\n","versions":"[{'version': 'v1', 'created': 'Fri, 28 Feb 2025 16:25:50 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 22:06:05 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['He', 'Songrun', ''], ['Lv', 'Linying', ''], ['Manela', 'Asaf', ''], ['Wu', 'Jimmy', '']]","extracted_entities":"[{'text': 'lookahead bias', 'label': 'Model Bias and Fairness'}, {'text': 'BERT', 'label': 'BERT'}, {'text': 'Lookahead bias', 'label': 'Model Bias and Fairness'}, {'text': 'lookahead bias', 'label': 'Model Bias and Fairness'}]","assigned_concept":"BERT","matched_keyword":"BERT","similarity_score":1.0}
{"id":2503.14671,"submitter":"Xiangyong Chen","authors":"Xiangyong Chen, Xiaochuan Lin","title":"Generating Medically-Informed Explanations for Depression Detection\n  using LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Early detection of depression from social media data offers a valuable\nopportunity for timely intervention. However, this task poses significant\nchallenges, requiring both professional medical knowledge and the development\nof accurate and explainable models. In this paper, we propose LLM-MTD (Large\nLanguage Model for Multi-Task Depression Detection), a novel approach that\nleverages a pre-trained large language model to simultaneously classify social\nmedia posts for depression and generate textual explanations grounded in\nmedical diagnostic criteria. We train our model using a multi-task learning\nframework with a combined loss function that optimizes both classification\naccuracy and explanation quality. We evaluate LLM-MTD on the benchmark Reddit\nSelf-Reported Depression Dataset (RSDD) and compare its performance against\nseveral competitive baseline methods, including traditional machine learning\nand fine-tuned BERT. Our experimental results demonstrate that LLM-MTD achieves\nstate-of-the-art performance in depression detection, showing significant\nimprovements in AUPRC and other key metrics. Furthermore, human evaluation of\nthe generated explanations reveals their relevance, completeness, and medical\naccuracy, highlighting the enhanced interpretability of our approach. This work\ncontributes a novel methodology for depression detection that combines the\npower of large language models with the crucial aspect of explainability.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 19:23:22 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Chen', 'Xiangyong', ''], ['Lin', 'Xiaochuan', '']]","extracted_entities":"[{'text': 'BERT', 'label': 'BERT'}]","assigned_concept":"BERT","matched_keyword":"BERT","similarity_score":1.0}
{"id":2503.14928,"submitter":"Jiaxin Ye","authors":"Jiaxin Ye and Hongming Shan","title":"Shushing! Let's Imagine an Authentic Speech from the Silent Video","comments":"Project Page: https:\/\/imagintalk.github.io","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.SD eess.AS","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Vision-guided speech generation aims to produce authentic speech from facial\nappearance or lip motions without relying on auditory signals, offering\nsignificant potential for applications such as dubbing in filmmaking and\nassisting individuals with aphonia. Despite recent progress, existing methods\nstruggle to achieve unified cross-modal alignment across semantics, timbre, and\nemotional prosody from visual cues, prompting us to propose Consistent\nVideo-to-Speech (CV2S) as an extended task to enhance cross-modal consistency.\nTo tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal\ndiffusion framework that generates faithful speech using only visual input,\noperating within a discrete space. Specifically, we propose a discrete lip\naligner that predicts discrete speech tokens from lip videos to capture\nsemantic information, while an error detector identifies misaligned tokens,\nwhich are subsequently refined through masked language modeling with BERT. To\nfurther enhance the expressiveness of the generated speech, we develop a style\ndiffusion transformer equipped with a face-style adapter that adaptively\ncustomizes identity and prosody dynamics across both the channel and temporal\ndimensions while ensuring synchronization with lip-aware semantic features.\nExtensive experiments demonstrate that ImaginTalk can generate high-fidelity\nspeech with more accurate semantic details and greater expressiveness in timbre\nand emotion compared to state-of-the-art baselines. Demos are shown at our\nproject page: https:\/\/imagintalk.github.io.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 06:28:17 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Ye', 'Jiaxin', ''], ['Shan', 'Hongming', '']]","extracted_entities":"[{'text': 'BERT', 'label': 'BERT'}]","assigned_concept":"BERT","matched_keyword":"BERT","similarity_score":1.0}
{"id":2503.15133,"submitter":"Sebastian Schmidt","authors":"Christina Zorenb\\\"ohmer and Sebastian Schmidt and Bernd Resch","title":"EmoGRACE: Aspect-based emotion analysis for social media data","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  While sentiment analysis has advanced from sentence to aspect-level, i.e.,\nthe identification of concrete terms related to a sentiment, the equivalent\nfield of Aspect-based Emotion Analysis (ABEA) is faced with dataset bottlenecks\nand the increased complexity of emotion classes in contrast to binary\nsentiments. This paper addresses these gaps, by generating a first ABEA\ntraining dataset, consisting of 2,621 English Tweets, and fine-tuning a\nBERT-based model for the ABEA sub-tasks of Aspect Term Extraction (ATE) and\nAspect Emotion Classification (AEC).\n  The dataset annotation process was based on the hierarchical emotion theory\nby Shaver et al. [1] and made use of group annotation and majority voting\nstrategies to facilitate label consistency. The resulting dataset contained\naspect-level emotion labels for Anger, Sadness, Happiness, Fear, and a None\nclass. Using the new ABEA training dataset, the state-of-the-art ABSA model\nGRACE by Luo et al. [2] was fine-tuned for ABEA. The results reflected a\nperformance plateau at an F1-score of 70.1% for ATE and 46.9% for joint ATE and\nAEC extraction. The limiting factors for model performance were broadly\nidentified as the small training dataset size coupled with the increased task\ncomplexity, causing model overfitting and limited abilities to generalize well\non new data.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:48:52 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zorenb\u00f6hmer', 'Christina', ''], ['Schmidt', 'Sebastian', ''], ['Resch', 'Bernd', '']]","extracted_entities":"[{'text': 'BERT-based', 'label': 'BERT'}]","assigned_concept":"BERT","matched_keyword":"BERT-based","similarity_score":0.8345725536}
{"id":2302.09019,"submitter":"Maolin Wang","authors":"Maolin Wang, Yu Pan, Zenglin Xu, Guangxi Li, Xiangli Yang, Danilo\n  Mandic, Andrzej Cichocki","title":"Tensor Networks Meet Neural Networks: A Survey and Future Perspectives","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Tensor networks (TNs) and neural networks (NNs) are two fundamental data\nmodeling approaches. TNs were introduced to solve the curse of dimensionality\nin large-scale tensors by converting an exponential number of dimensions to\npolynomial complexity. As a result, they have attracted significant attention\nin the fields of quantum physics and machine learning. Meanwhile, NNs have\ndisplayed exceptional performance in various applications, e.g., computer\nvision, natural language processing, and robotics research. Interestingly,\nalthough these two types of networks originate from different observations,\nthey are inherently linked through the typical multilinearity structure\nunderlying both TNs and NNs, thereby motivating a significant number of\ndevelopments regarding combinations of TNs and NNs. In this paper, we refer to\nthese combinations as tensorial neural networks~(TNNs) and present an\nintroduction to TNNs from both data processing and model architecture\nperspectives. From the data perspective, we explore the capabilities of TNNs in\nmulti-source fusion, multimodal pooling, data compression, multi-task training,\nand quantum data processing. From the model perspective, we examine TNNs'\nintegration with various architectures, including Convolutional Neural\nNetworks, Recurrent Neural Networks, Graph Neural Networks, Transformers, Large\nLanguage Models, and Quantum Neural Networks. Furthermore, this survey also\nexplores methods for improving TNNs, examines flexible toolboxes for\nimplementing TNNs, and documents TNN development while highlighting potential\nfuture directions. To the best of our knowledge, this is the first\ncomprehensive survey that bridges the connections among NNs and TNs. We provide\na curated list of TNNs at\nhttps:\/\/github.com\/tnbar\/awesome-tensorial-neural-networks.\n","versions":"[{'version': 'v1', 'created': 'Sun, 22 Jan 2023 17:35:56 GMT'}, {'version': 'v2', 'created': 'Mon, 8 May 2023 06:06:32 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 15:33:59 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wang', 'Maolin', ''], ['Pan', 'Yu', ''], ['Xu', 'Zenglin', ''], ['Li', 'Guangxi', ''], ['Yang', 'Xiangli', ''], ['Mandic', 'Danilo', ''], ['Cichocki', 'Andrzej', '']]","extracted_entities":"[{'text': 'multi-task training', 'label': 'Few-shot Learning'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Large\\nLanguage Models', 'label': 'Large Language Model'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2303.17031,"submitter":"Andrea Tagarelli","authors":"Lucio La Cava, Davide Costa, Andrea Tagarelli","title":"Visually Wired NFTs: Exploring the Role of Inspiration in Non-Fungible\n  Tokens","comments":"Accepted for publication with ACM Trans. on the Web, Jan 2025.\n  https:\/\/dl.acm.org\/doi\/10.1145\/3703411","journal-ref":"ACM Trans. on the Web, Jan 2025","doi":"10.1145\/3703411","report-no":null,"categories":"cs.SI cs.AI cs.CV physics.soc-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The fervor for Non-Fungible Tokens (NFTs) attracted countless creators,\nleading to a Big Bang of digital assets driven by latent or explicit forms of\ninspiration, as in many creative processes. This work exploits Vision\nTransformers and graph-based modeling to delve into visual inspiration\nphenomena between NFTs over the years. Our goals include unveiling the main\nstructural traits that shape visual inspiration networks, exploring the\ninterrelation between visual inspiration and asset performances, investigating\ncrypto influence on inspiration processes, and explaining the inspiration\nrelationships among NFTs. Our findings unveil how the pervasiveness of\ninspiration led to a temporary saturation of the visual feature space, the\nimpact of the dichotomy between inspiring and inspired NFTs on their financial\nperformance, and an intrinsic self-regulatory mechanism between markets and\ninspiration waves. Our work can serve as a starting point for gaining a broader\nview of the evolution of Web3.\n","versions":"[{'version': 'v1', 'created': 'Wed, 29 Mar 2023 21:26:23 GMT'}, {'version': 'v2', 'created': 'Sun, 16 Apr 2023 09:01:17 GMT'}, {'version': 'v3', 'created': 'Wed, 14 Jun 2023 15:55:10 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 09:07:22 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['La Cava', 'Lucio', ''], ['Costa', 'Davide', ''], ['Tagarelli', 'Andrea', '']]","extracted_entities":"[{'text': 'Vision\\nTransformers', 'label': 'Transformers'}, {'text': 'NFTs', 'label': 'LLMs'}, {'text': 'NFTs', 'label': 'LLMs'}, {'text': 'NFTs', 'label': 'LLMs'}]","assigned_concept":"Transformers","matched_keyword":"Vision\nTransformers","similarity_score":0.7330732346}
{"id":2403.19243,"submitter":"Yiping Ji","authors":"Yiping Ji, Hemanth Saratchandran, Cameron Gordon, Zeyu Zhang, Simon\n  Lucey","title":"Efficient Learning With Sine-Activated Low-rank Matrices","comments":"The first two authors contributed equally. Paper accepted at ICLR\n  2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV cs.NE","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Low-rank decomposition has emerged as a vital tool for enhancing parameter\nefficiency in neural network architectures, gaining traction across diverse\napplications in machine learning. These techniques significantly lower the\nnumber of parameters, striking a balance between compactness and performance.\nHowever, a common challenge has been the compromise between parameter\nefficiency and the accuracy of the model, where reduced parameters often lead\nto diminished accuracy compared to their full-rank counterparts. In this work,\nwe propose a novel theoretical framework that integrates a sinusoidal function\nwithin the low-rank decomposition process. This approach not only preserves the\nbenefits of the parameter efficiency characteristic of low-rank methods but\nalso increases the decomposition's rank, thereby enhancing model performance.\nOur method proves to be a plug in enhancement for existing low-rank models, as\nevidenced by its successful application in Vision Transformers (ViT), Large\nLanguage Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.\n","versions":"[{'version': 'v1', 'created': 'Thu, 28 Mar 2024 08:58:20 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Jan 2025 12:17:43 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Feb 2025 00:08:30 GMT'}, {'version': 'v4', 'created': 'Mon, 3 Mar 2025 12:32:47 GMT'}, {'version': 'v5', 'created': 'Mon, 17 Mar 2025 04:11:01 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Ji', 'Yiping', ''], ['Saratchandran', 'Hemanth', ''], ['Gordon', 'Cameron', ''], ['Zhang', 'Zeyu', ''], ['Lucey', 'Simon', '']]","extracted_entities":"[{'text': 'Vision Transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Vision Transformers","similarity_score":0.7330732346}
{"id":2409.01482,"submitter":"Benjamin Badger","authors":"Benjamin L. Badger","title":"Masked Mixers for Language Generation and Retrieval","comments":"31 pages, 9 figures, 4 tables, 14 supplementary figures, 10\n  supplementary tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Attention mechanisms that confer selective focus on a strict subset of input\nelements are nearly ubiquitous in language models today. We posit there to be\ndownside to the use of attention: most input information is lost. In support of\nthis idea we observe poor input representation accuracy in transformers and\nmore accurate representation in what we term masked mixers, which replace\nself-attention with masked convolutions. The masked mixer learns causal\nlanguage modeling more efficiently than early transformer implementations and\neven outperforms optimized, current transformers when training on small\n($n_{ctx}<512$) but not larger context windows. Evidence is presented for the\nhypothesis that differences in transformer and masked mixer training\nefficiencies for various tasks are best predicted by input representation\naccuracy, or equivalently global invertibility. We hypothesize that the\ninformation loss exhibited by transformers would be more detrimental to\nretrieval than generation, as the former is more closely approximated by a\nbijective and thus invertible function. We find that masked mixers are more\neffective retrieval models both when the pretrained embedding model is\nunchanged as well as when the embedding model is modified via cosine\nsimilarity-based InfoNCE loss minimization. A small masked mixer is shown to\noutperform a large and near state-of-the-art transformer-based retrieval model,\ndespite the latter being trained with many orders of magnitude more data and\ncompute.\n","versions":"[{'version': 'v1', 'created': 'Mon, 2 Sep 2024 22:17:18 GMT'}, {'version': 'v2', 'created': 'Sat, 1 Mar 2025 23:34:06 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 21:12:20 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 17:39:10 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Badger', 'Benjamin L.', '']]","extracted_entities":"[{'text': 'Attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'transformers', 'label': 'Transformers'}, {'text': 'masked mixers', 'label': 'Transformers'}, {'text': 'masked convolutions', 'label': 'Embedding'}, {'text': 'transformers', 'label': 'Transformers'}, {'text': 'masked mixers', 'label': 'Transformers'}, {'text': 'embedding model', 'label': 'Embedding'}]","assigned_concept":"Transformers","matched_keyword":"transformers","similarity_score":1.0}
{"id":2410.10986,"submitter":"Weronika Ormaniec","authors":"Weronika Ormaniec, Felix Dangel and Sidak Pal Singh","title":"What Does It Mean to Be a Transformer? Insights from a Theoretical\n  Hessian Analysis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ML","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  The Transformer architecture has inarguably revolutionized deep learning,\novertaking classical architectures like multi-layer perceptrons (MLPs) and\nconvolutional neural networks (CNNs). At its core, the attention block differs\nin form and functionality from most other architectural components in deep\nlearning--to the extent that, in comparison to MLPs\/CNNs, Transformers are more\noften accompanied by adaptive optimizers, layer normalization, learning rate\nwarmup, etc. The root causes behind these outward manifestations and the\nprecise mechanisms that govern them remain poorly understood. In this work, we\nbridge this gap by providing a fundamental understanding of what distinguishes\nthe Transformer from the other architectures--grounded in a theoretical\ncomparison of the (loss) Hessian. Concretely, for a single self-attention\nlayer, (a) we first entirely derive the Transformer's Hessian and express it in\nmatrix derivatives; (b) we then characterize it in terms of data, weight, and\nattention moment dependencies; and (c) while doing so further highlight the\nimportant structural differences to the Hessian of classical networks. Our\nresults suggest that various common architectural and optimization choices in\nTransformers can be traced back to their highly non-linear dependencies on the\ndata and weight matrices, which vary heterogeneously across parameters.\nUltimately, our findings provide a deeper understanding of the Transformer's\nunique optimization landscape and the challenges it poses.\n","versions":"[{'version': 'v1', 'created': 'Mon, 14 Oct 2024 18:15:02 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 17:32:06 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Ormaniec', 'Weronika', ''], ['Dangel', 'Felix', ''], ['Singh', 'Sidak Pal', '']]","extracted_entities":"[{'text': 'attention block', 'label': 'Attention mechanism'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'layer normalization', 'label': 'Fine-tuning'}, {'text': 'learning rate\\nwarmup', 'label': 'Fine-tuning'}, {'text': 'Hessian', 'label': 'Attention mechanism'}, {'text': 'Transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2410.13924,"submitter":"Guangda Ji","authors":"Guangda Ji, Silvan Weder, Francis Engelmann, Marc Pollefeys, Hermann\n  Blum","title":"ARKit LabelMaker: A New Scale for Indoor 3D Scene Understanding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Neural network performance scales with both model size and data volume, as\nshown in both language and image processing. This requires scaling-friendly\narchitectures and large datasets. While transformers have been adapted for 3D\nvision, a `GPT-moment' remains elusive due to limited training data. We\nintroduce ARKit LabelMaker, a large-scale real-world 3D dataset with dense\nsemantic annotation that is more than three times larger than prior largest\ndataset. Specifically, we extend ARKitScenes with automatically generated dense\n3D labels using an extended LabelMaker pipeline, tailored for large-scale\npre-training. Training on our dataset improves accuracy across architectures,\nachieving state-of-the-art 3D semantic segmentation scores on ScanNet and\nScanNet200, with notable gains on tail classes. Our code is available at\nhttps:\/\/labelmaker.org and our dataset at\nhttps:\/\/huggingface.co\/datasets\/labelmaker\/arkit_labelmaker.\n","versions":"[{'version': 'v1', 'created': 'Thu, 17 Oct 2024 14:44:35 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 10:16:27 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Ji', 'Guangda', ''], ['Weder', 'Silvan', ''], ['Engelmann', 'Francis', ''], ['Pollefeys', 'Marc', ''], ['Blum', 'Hermann', '']]","extracted_entities":"[{'text': 'transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"transformers","similarity_score":1.0}
{"id":2411.02344,"submitter":"Md Rifat Arefin","authors":"Md Rifat Arefin, Gopeshh Subbaraj, Nicolas Gontier, Yann LeCun, Irina\n  Rish, Ravid Shwartz-Ziv, Christopher Pal","title":"Seq-VCR: Preventing Collapse in Intermediate Transformer Representations\n  for Enhanced Reasoning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Decoder-only Transformers often struggle with complex reasoning tasks,\nparticularly arithmetic reasoning requiring multiple sequential operations. In\nthis work, we identify representation collapse in the model's intermediate\nlayers as a key factor limiting their reasoning capabilities. To address this,\nwe propose Sequential Variance-Covariance Regularization (Seq-VCR), which\nenhances the entropy of intermediate representations and prevents collapse.\nCombined with dummy pause tokens as substitutes for chain-of-thought (CoT)\ntokens, our method significantly improves performance in arithmetic reasoning\nproblems. In the challenging $5 \\times 5$ integer multiplication task, our\napproach achieves $99.5\\%$ exact match accuracy, outperforming models of the\nsame size (which yield $0\\%$ accuracy) and GPT-4 with five-shot CoT prompting\n($44\\%$). We also demonstrate superior results on arithmetic expression and\nlongest increasing subsequence (LIS) datasets. Our findings highlight the\nimportance of preventing intermediate layer representation collapse to enhance\nthe reasoning capabilities of Transformers and show that Seq-VCR offers an\neffective solution without requiring explicit CoT supervision.\n","versions":"[{'version': 'v1', 'created': 'Mon, 4 Nov 2024 18:14:07 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 17:37:44 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Arefin', 'Md Rifat', ''], ['Subbaraj', 'Gopeshh', ''], ['Gontier', 'Nicolas', ''], ['LeCun', 'Yann', ''], ['Rish', 'Irina', ''], ['Shwartz-Ziv', 'Ravid', ''], ['Pal', 'Christopher', '']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'five-shot CoT prompting', 'label': 'Prompting'}, {'text': 'Transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2411.09953,"submitter":"Qianhao Wang","authors":"Qianhao Wang, Yinqian Sun, Enmeng Lu, Qian Zhang, and Yi Zeng","title":"Brain-inspired Action Generation with Spiking Transformer Diffusion\n  Policy Model","comments":"10 pages, 4 figures and 2 tables, conference submission","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Spiking Neural Networks (SNNs) has the ability to extract spatio-temporal\nfeatures due to their spiking sequence. While previous research has primarily\nfoucus on the classification of image and reinforcement learning. In our paper,\nwe put forward novel diffusion policy model based on Spiking Transformer Neural\nNetworks and Denoising Diffusion Probabilistic Model (DDPM): Spiking\nTransformer Modulate Diffusion Policy Model (STMDP), a new brain-inspired model\nfor generating robot action trajectories. In order to improve the performance\nof this model, we develop a novel decoder module: Spiking Modulate De coder\n(SMD), which replaces the traditional Decoder module within the Transformer\narchitecture. Additionally, we explored the substitution of DDPM with Denoising\nDiffusion Implicit Models (DDIM) in our frame work. We conducted experiments\nacross four robotic manipulation tasks and performed ablation studies on the\nmodulate block. Our model consistently outperforms existing Transformer-based\ndiffusion policy method. Especially in Can task, we achieved an improvement of\n8%. The proposed STMDP method integrates SNNs, dffusion model and Transformer\narchitecture, which offers new perspectives and promising directions for\nexploration in brain-inspired robotics.\n","versions":"[{'version': 'v1', 'created': 'Fri, 15 Nov 2024 05:11:28 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 02:13:36 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wang', 'Qianhao', ''], ['Sun', 'Yinqian', ''], ['Lu', 'Enmeng', ''], ['Zhang', 'Qian', ''], ['Zeng', 'Yi', '']]","extracted_entities":"[{'text': 'Transformer\\narchitecture', 'label': 'Transformers'}, {'text': 'Transformer\\narchitecture', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Transformer\narchitecture","similarity_score":0.6029016376}
{"id":2411.12537,"submitter":"Julien Siems","authors":"Riccardo Grazzi, Julien Siems, Arber Zela, J\\\"org K.H. Franke, Frank\n  Hutter, Massimiliano Pontil","title":"Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues","comments":"V2: Correction to Theorem 1 and 2 and to point 3 of Proposition 1.\n  V3: ICLR Camera Ready, V4: ICLR Camera Ready, added figures to theory\n  section, updated modular arithmetic with brackets results because previous\n  results did not contain multiplication","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL cs.FL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and\nDeltaNet have emerged as efficient alternatives to Transformers for long\nsequences. However, both Transformers and LRNNs struggle to perform\nstate-tracking, which may impair performance in tasks such as code evaluation.\nIn one forward pass, current architectures are unable to solve even parity, the\nsimplest state-tracking task, which non-linear RNNs can handle effectively.\nRecently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like\nMamba to solve parity stems from restricting the value range of their diagonal\nstate-transition matrices to $[0, 1]$ and that incorporating negative values\ncan resolve this issue. We extend this result to non-diagonal LRNNs such as\nDeltaNet. We prove that finite precision LRNNs with state-transition matrices\nhaving only positive eigenvalues cannot solve parity, while non-triangular\nmatrices are needed to count modulo $3$. Notably, we also prove that LRNNs can\nlearn any regular language when their state-transition matrices are products of\nidentity minus vector outer product matrices, each with eigenvalues in the\nrange $[-1, 1]$. Our experiments confirm that extending the eigenvalue range of\nMamba and DeltaNet to include negative values not only enables them to solve\nparity but consistently improves their performance on state-tracking tasks. We\nalso show that state-tracking enabled LRNNs can be pretrained stably and\nefficiently at scale (1.3B parameters), achieving competitive performance on\nlanguage modeling and showing promise on code and math tasks.\n","versions":"[{'version': 'v1', 'created': 'Tue, 19 Nov 2024 14:35:38 GMT'}, {'version': 'v2', 'created': 'Fri, 6 Dec 2024 16:22:21 GMT'}, {'version': 'v3', 'created': 'Fri, 28 Feb 2025 09:17:14 GMT'}, {'version': 'v4', 'created': 'Sun, 16 Mar 2025 14:35:24 GMT'}, {'version': 'v5', 'created': 'Tue, 18 Mar 2025 13:13:18 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Grazzi', 'Riccardo', ''], ['Siems', 'Julien', ''], ['Zela', 'Arber', ''], ['Franke', 'J\u00f6rg K. H.', ''], ['Hutter', 'Frank', ''], ['Pontil', 'Massimiliano', '']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2411.18425,"submitter":"Rui Li","authors":"Rui Li, Marcus Klasson, Arno Solin, Martin Trapp","title":"Streamlining Prediction in Bayesian Deep Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The rising interest in Bayesian deep learning (BDL) has led to a plethora of\nmethods for estimating the posterior distribution. However, efficient\ncomputation of inferences, such as predictions, has been largely overlooked\nwith Monte Carlo integration remaining the standard. In this work we examine\nstreamlining prediction in BDL through a single forward pass without sampling.\nFor this we use local linearisation on activation functions and local Gaussian\napproximations at linear layers. Thus allowing us to analytically compute an\napproximation to the posterior predictive distribution. We showcase our\napproach for both MLP and transformers, such as ViT and GPT-2, and assess its\nperformance on regression and classification tasks.\n","versions":"[{'version': 'v1', 'created': 'Wed, 27 Nov 2024 15:07:44 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 12:18:04 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Li', 'Rui', ''], ['Klasson', 'Marcus', ''], ['Solin', 'Arno', ''], ['Trapp', 'Martin', '']]","extracted_entities":"[{'text': 'Bayesian deep learning', 'label': 'Few-shot Learning'}, {'text': 'local linearisation', 'label': 'quantisation'}, {'text': 'transformers', 'label': 'Transformers'}, {'text': 'GPT-2', 'label': 'GPT-2'}]","assigned_concept":"Transformers","matched_keyword":"transformers","similarity_score":1.0}
{"id":2412.12444,"submitter":"Xuan Shen","authors":"Xuan Shen, Zhao Song, Yufa Zhou, Bo Chen, Yanyu Li, Yifan Gong, Kai\n  Zhang, Hao Tan, Jason Kuen, Henghui Ding, Zhihao Shu, Wei Niu, Pu Zhao,\n  Yanzhi Wang, Jiuxiang Gu","title":"LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers","comments":"Accepted by AAAI 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https:\/\/github.com\/shawnricecake\/lazydit\n","versions":"[{'version': 'v1', 'created': 'Tue, 17 Dec 2024 01:12:35 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 03:55:18 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Shen', 'Xuan', ''], ['Song', 'Zhao', ''], ['Zhou', 'Yufa', ''], ['Chen', 'Bo', ''], ['Li', 'Yanyu', ''], ['Gong', 'Yifan', ''], ['Zhang', 'Kai', ''], ['Tan', 'Hao', ''], ['Kuen', 'Jason', ''], ['Ding', 'Henghui', ''], ['Shu', 'Zhihao', ''], ['Niu', 'Wei', ''], ['Zhao', 'Pu', ''], ['Wang', 'Yanzhi', ''], ['Gu', 'Jiuxiang', '']]","extracted_entities":"[{'text': 'Diffusion Transformers', 'label': 'Transformers'}, {'text': 'DDIM', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Diffusion Transformers","similarity_score":0.5920959711}
{"id":2412.13769,"submitter":"Hari Hara Suthan Chittoor","authors":"Hari Hara Suthan Chittoor, Paul Robert Griffin, Ariel Neufeld, Jayne\n  Thompson, Mile Gu","title":"QuLTSF: Long-Term Time Series Forecasting with Quantum Machine Learning","comments":"Published in ICAART 2025","journal-ref":null,"doi":"10.5220\/0013395500003890","report-no":null,"categories":"quant-ph cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Long-term time series forecasting (LTSF) involves predicting a large number\nof future values of a time series based on the past values. This is an\nessential task in a wide range of domains including weather forecasting, stock\nmarket analysis and disease outbreak prediction. Over the decades LTSF\nalgorithms have transitioned from statistical models to deep learning models\nlike transformer models. Despite the complex architecture of transformer based\nLTSF models `Are Transformers Effective for Time Series Forecasting? (Zeng et\nal., 2023)' showed that simple linear models can outperform the\nstate-of-the-art transformer based LTSF models. Recently, quantum machine\nlearning (QML) is evolving as a domain to enhance the capabilities of classical\nmachine learning models. In this paper we initiate the application of QML to\nLTSF problems by proposing QuLTSF, a simple hybrid QML model for multivariate\nLTSF. Through extensive experiments on a widely used weather dataset we show\nthe advantages of QuLTSF over the state-of-the-art classical linear models, in\nterms of reduced mean squared error and mean absolute error.\n","versions":"[{'version': 'v1', 'created': 'Wed, 18 Dec 2024 12:06:52 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 09:30:51 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Chittoor', 'Hari Hara Suthan', ''], ['Griffin', 'Paul Robert', ''], ['Neufeld', 'Ariel', ''], ['Thompson', 'Jayne', ''], ['Gu', 'Mile', '']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'QuLTSF', 'label': 'Transformer-based model'}, {'text': 'QuLTSF', 'label': 'Transformer-based model'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2501.08659,"submitter":"Dongzhihan Wang","authors":"Dongzhihan Wang, Yang Yang, Liang Xu","title":"BRIGHT-VO: Brightness-Guided Hybrid Transformer for Visual Odometry with\n  Multi-modality Refinement Module","comments":"Method mistakes appearing in this paper","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Visual odometry (VO) plays a crucial role in autonomous driving, robotic\nnavigation, and other related tasks by estimating the position and orientation\nof a camera based on visual input. Significant progress has been made in\ndata-driven VO methods, particularly those leveraging deep learning techniques\nto extract image features and estimate camera poses. However, these methods\noften struggle in low-light conditions because of the reduced visibility of\nfeatures and the increased difficulty of matching keypoints. To address this\nlimitation, we introduce BrightVO, a novel VO model based on Transformer\narchitecture, which not only performs front-end visual feature extraction, but\nalso incorporates a multi-modality refinement module in the back-end that\nintegrates Inertial Measurement Unit (IMU) data. Using pose graph optimization,\nthis module iteratively refines pose estimates to reduce errors and improve\nboth accuracy and robustness. Furthermore, we create a synthetic low-light\ndataset, KiC4R, which includes a variety of lighting conditions to facilitate\nthe training and evaluation of VO frameworks in challenging environments.\nExperimental results demonstrate that BrightVO achieves state-of-the-art\nperformance on both the KiC4R dataset and the KITTI benchmarks. Specifically,\nit provides an average improvement of 20% in pose estimation accuracy in normal\noutdoor environments and 259% in low-light conditions, outperforming existing\nmethods. For widespread use and further development, the research work is fully\nopen-source at https:\/\/github.com\/Anastasiawd\/BrightVO.\n","versions":"[{'version': 'v1', 'created': 'Wed, 15 Jan 2025 08:50:52 GMT'}, {'version': 'v2', 'created': 'Thu, 16 Jan 2025 03:51:49 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 02:49:51 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wang', 'Dongzhihan', ''], ['Yang', 'Yang', ''], ['Xu', 'Liang', '']]","extracted_entities":"[{'text': 'Transformer\\narchitecture', 'label': 'Transformers'}, {'text': 'pose graph optimization', 'label': 'Fine-tuning'}]","assigned_concept":"Transformers","matched_keyword":"Transformer\narchitecture","similarity_score":0.6029016376}
{"id":2502.02393,"submitter":"Michael Hahn","authors":"Alireza Amiri, Xinting Huang, Mark Rofin, Michael Hahn","title":"Lower Bounds for Chain-of-Thought Reasoning in Hard-Attention\n  Transformers","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Chain-of-thought reasoning and scratchpads have emerged as critical tools for\nenhancing the computational capabilities of transformers. While theoretical\nresults show that polynomial-length scratchpads can extend transformers'\nexpressivity from $TC^0$ to $PTIME$, their required length remains poorly\nunderstood. Empirical evidence even suggests that transformers need scratchpads\neven for many problems in $TC^0$, such as Parity or Multiplication, challenging\noptimistic bounds derived from circuit complexity. In this work, we initiate\nthe study of systematic lower bounds for the number of CoT steps across\ndifferent algorithmic problems, in the hard-attention regime. We study a\nvariety of algorithmic problems, and provide bounds that are tight up to\nlogarithmic factors. Overall, these results contribute to emerging\nunderstanding of the power and limitations of chain-of-thought reasoning.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Feb 2025 15:14:01 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 15:52:20 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Amiri', 'Alireza', ''], ['Huang', 'Xinting', ''], ['Rofin', 'Mark', ''], ['Hahn', 'Michael', '']]","extracted_entities":"[{'text': 'Chain-of-thought reasoning', 'label': 'Chain of thought'}, {'text': 'transformers', 'label': 'Transformers'}, {'text': 'transformers', 'label': 'Transformers'}, {'text': 'transformers', 'label': 'Transformers'}, {'text': 'hard-attention regime', 'label': 'Attention mechanism'}, {'text': 'chain-of-thought reasoning', 'label': 'Chain of thought'}]","assigned_concept":"Transformers","matched_keyword":"transformers","similarity_score":1.0}
{"id":2502.03772,"submitter":"Chaoyin She","authors":"Chaoyin She, Ruifang Lu, Danni He, Jiayi Lv, Yadan Lin, Meiqing Cheng,\n  Hui Huang, Fengyu Ye, Lida Chen, Wei Wang, Qinghua Huang","title":"A Retrospective Systematic Study on Hierarchical Sparse Query\n  Transformer-assisted Ultrasound Screening for Early Hepatocellular Carcinoma","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Hepatocellular carcinoma (HCC), ranking as the third leading cause of\ncancer-related mortality worldwide, demands urgent improvements in early\ndetection to enhance patient survival. While ultrasound remains the preferred\nscreening modality due to its cost-effectiveness and real-time capabilities,\nits sensitivity (59%-78%) heavily relies on radiologists' expertise, leading to\ninconsistent diagnostic outcomes and operational inefficiencies. Recent\nadvancements in AI technology offer promising solutions to bridge this gap.\nThis study introduces the Hierarchical Sparse Query Transformer (HSQformer), a\nnovel hybrid architecture that synergizes CNNs' local feature extraction with\nVision Transformers' global contextual awareness through latent space\nrepresentation and sparse learning. By dynamically activating task-specific\nexperts via a Mixture-of-Experts (MoE) framework, HSQformer achieves\nhierarchical feature integration without structural redundancy. Evaluated\nacross three clinical scenarios: single-center, multi-center, and high-risk\npatient cohorts, HSQformer outperforms state-of-the-art models (e.g., 95.38%\nAUC in multi-center testing) and matches senior radiologists' diagnostic\naccuracy while significantly surpassing junior counterparts. These results\nhighlight the potential of AI-assisted tools to standardize HCC screening,\nreduce dependency on human expertise, and improve early diagnosis rates. The\nfull code is available at https:\/\/github.com\/Asunatan\/HSQformer.\n","versions":"[{'version': 'v1', 'created': 'Thu, 6 Feb 2025 04:17:02 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 06:38:41 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['She', 'Chaoyin', ''], ['Lu', 'Ruifang', ''], ['He', 'Danni', ''], ['Lv', 'Jiayi', ''], ['Lin', 'Yadan', ''], ['Cheng', 'Meiqing', ''], ['Huang', 'Hui', ''], ['Ye', 'Fengyu', ''], ['Chen', 'Lida', ''], ['Wang', 'Wei', ''], ['Huang', 'Qinghua', '']]","extracted_entities":"[{'text': 'Vision Transformers', 'label': 'Transformers'}, {'text': 'global contextual awareness', 'label': 'contextual Embedding'}, {'text': 'latent space\\nrepresentation', 'label': 'contextual Embedding'}, {'text': 'sparse learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Transformers","matched_keyword":"Vision Transformers","similarity_score":0.7330732346}
{"id":2502.09029,"submitter":"Qianhao Wang","authors":"Qianhao Wang and Yinqian Sun and Enmeng Lu and Qian Zhang and Yi Zeng","title":"MTDP: A Modulated Transformer based Diffusion Policy Model","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent research on robot manipulation based on Behavior Cloning (BC) has made\nsignificant progress. By combining diffusion models with BC, diffusion policiy\nhas been proposed, enabling robots to quickly learn manipulation tasks with\nhigh success rates. However, integrating diffusion policy with high-capacity\nTransformer presents challenges, traditional Transformer architectures struggle\nto effectively integrate guiding conditions, resulting in poor performance in\nmanipulation tasks when using Transformer-based models. In this paper, we\ninvestigate key architectural designs of Transformers and improve the\ntraditional Transformer architecture by proposing the Modulated Transformer\nDiffusion Policy (MTDP) model for diffusion policy. The core of this model is\nthe Modulated Attention module we proposed, which more effectively integrates\nthe guiding conditions with the main input, improving the generative model's\noutput quality and, consequently, increasing the robot's task success rate. In\nsix experimental tasks, MTDP outperformed existing Transformer model\narchitectures, particularly in the Toolhang experiment, where the success rate\nincreased by 12\\%. To verify the generality of Modulated Attention, we applied\nit to the UNet architecture to construct Modulated UNet Diffusion Policy model\n(MUDP), which also achieved higher success rates than existing UNet\narchitectures across all six experiments. The Diffusion Policy uses Denoising\nDiffusion Probabilistic Models (DDPM) as the diffusion model. Building on this,\nwe also explored Denoising Diffusion Implicit Models (DDIM) as the diffusion\nmodel, constructing the MTDP-I and MUDP-I model, which nearly doubled the\ngeneration speed while maintaining performance.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Feb 2025 07:35:03 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 02:11:44 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wang', 'Qianhao', ''], ['Sun', 'Yinqian', ''], ['Lu', 'Enmeng', ''], ['Zhang', 'Qian', ''], ['Zeng', 'Yi', '']]","extracted_entities":"[{'text': 'guiding conditions', 'label': 'Prompting'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Modulated Attention', 'label': 'Attention mechanism'}, {'text': 'guiding conditions', 'label': 'Attention mechanism'}, {'text': 'Modulated Attention', 'label': 'Attention mechanism'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2503.00226,"submitter":"Yufei Guo","authors":"Yufei Guo, Xiaode Liu, Yuanpei Chen, Weihang Peng, Yuhan Zhang, Zhe Ma","title":"Spiking Transformer:Introducing Accurate Addition-Only Spiking\n  Self-Attention for Transformer","comments":"Accepted by CVPR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Transformers have demonstrated outstanding performance across a wide range of\ntasks, owing to their self-attention mechanism, but they are highly\nenergy-consuming. Spiking Neural Networks have emerged as a promising\nenergy-efficient alternative to traditional Artificial Neural Networks,\nleveraging event-driven computation and binary spikes for information transfer.\nThe combination of Transformers' capabilities with the energy efficiency of\nSNNs offers a compelling opportunity. This paper addresses the challenge of\nadapting the self-attention mechanism of Transformers to the spiking paradigm\nby introducing a novel approach: Accurate Addition-Only Spiking Self-Attention\n(A$^2$OS$^2$A). Unlike existing methods that rely solely on binary spiking\nneurons for all components of the self-attention mechanism, our approach\nintegrates binary, ReLU, and ternary spiking neurons. This hybrid strategy\nsignificantly improves accuracy while preserving non-multiplicative\ncomputations. Moreover, our method eliminates the need for softmax and scaling\noperations. Extensive experiments show that the A$^2$OS$^2$A-based Spiking\nTransformer outperforms existing SNN-based Transformers on several datasets,\neven achieving an accuracy of 78.66\\% on ImageNet-1K. Our work represents a\nsignificant advancement in SNN-based Transformer models, offering a more\naccurate and efficient solution for real-world applications.\n","versions":"[{'version': 'v1', 'created': 'Fri, 28 Feb 2025 22:23:29 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 03:17:00 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Guo', 'Yufei', ''], ['Liu', 'Xiaode', ''], ['Chen', 'Yuanpei', ''], ['Peng', 'Weihang', ''], ['Zhang', 'Yuhan', ''], ['Ma', 'Zhe', '']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'self-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'self-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'self-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'Transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2503.09829,"submitter":"Joohwan Seo","authors":"Joohwan Seo, Soochul Yoo, Junwoo Chang, Hyunseok An, Hyunwoo Ryu,\n  Soomi Lee, Arvind Kruthiventy, Jongeun Choi, and Roberto Horowitz","title":"SE(3)-Equivariant Robot Learning and Control: A Tutorial Survey","comments":"Submitted to International Journcal of Control, Automation and\n  Systems (IJCAS), Under Review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.LG cs.SY eess.SY","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Recent advances in deep learning and Transformers have driven major\nbreakthroughs in robotics by employing techniques such as imitation learning,\nreinforcement learning, and LLM-based multimodal perception and\ndecision-making. However, conventional deep learning and Transformer models\noften struggle to process data with inherent symmetries and invariances,\ntypically relying on large datasets or extensive data augmentation. Equivariant\nneural networks overcome these limitations by explicitly integrating symmetry\nand invariance into their architectures, leading to improved efficiency and\ngeneralization. This tutorial survey reviews a wide range of equivariant deep\nlearning and control methods for robotics, from classic to state-of-the-art,\nwith a focus on SE(3)-equivariant models that leverage the natural 3D\nrotational and translational symmetries in visual robotic manipulation and\ncontrol design. Using unified mathematical notation, we begin by reviewing key\nconcepts from group theory, along with matrix Lie groups and Lie algebras. We\nthen introduce foundational group-equivariant neural network design and show\nhow the group-equivariance can be obtained through their structure. Next, we\ndiscuss the applications of SE(3)-equivariant neural networks in robotics in\nterms of imitation learning and reinforcement learning. The SE(3)-equivariant\ncontrol design is also reviewed from the perspective of geometric control.\nFinally, we highlight the challenges and future directions of equivariant\nmethods in developing more robust, sample-efficient, and multi-modal real-world\nrobotic systems.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 20:47:40 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 06:26:34 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Seo', 'Joohwan', ''], ['Yoo', 'Soochul', ''], ['Chang', 'Junwoo', ''], ['An', 'Hyunseok', ''], ['Ryu', 'Hyunwoo', ''], ['Lee', 'Soomi', ''], ['Kruthiventy', 'Arvind', ''], ['Choi', 'Jongeun', ''], ['Horowitz', 'Roberto', '']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'imitation learning', 'label': 'Few-shot Learning'}, {'text': 'reinforcement learning', 'label': 'Few-shot Learning'}, {'text': 'imitation learning', 'label': 'Few-shot Learning'}, {'text': 'reinforcement learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2503.12009,"submitter":"Haisheng Su","authors":"Xin Jin, Haisheng Su, Kai Liu, Cong Ma, Wei Wu, Fei Hui, Junchi Yan","title":"UniMamba: Unified Spatial-Channel Representation Learning with\n  Group-Efficient Mamba for LiDAR-based 3D Object Detection","comments":"Accepted to CVPR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advances in LiDAR 3D detection have demonstrated the effectiveness of\nTransformer-based frameworks in capturing the global dependencies from point\ncloud spaces, which serialize the 3D voxels into the flattened 1D sequence for\niterative self-attention. However, the spatial structure of 3D voxels will be\ninevitably destroyed during the serialization process. Besides, due to the\nconsiderable number of 3D voxels and quadratic complexity of Transformers,\nmultiple sequences are grouped before feeding to Transformers, leading to a\nlimited receptive field. Inspired by the impressive performance of State Space\nModels (SSM) achieved in the field of 2D vision tasks, in this paper, we\npropose a novel Unified Mamba (UniMamba), which seamlessly integrates the\nmerits of 3D convolution and SSM in a concise multi-head manner, aiming to\nperform \"local and global\" spatial context aggregation efficiently and\nsimultaneously. Specifically, a UniMamba block is designed which mainly\nconsists of spatial locality modeling, complementary Z-order serialization and\nlocal-global sequential aggregator. The spatial locality modeling module\nintegrates 3D submanifold convolution to capture the dynamic spatial position\nembedding before serialization. Then the efficient Z-order curve is adopted for\nserialization both horizontally and vertically. Furthermore, the local-global\nsequential aggregator adopts the channel grouping strategy to efficiently\nencode both \"local and global\" spatial inter-dependencies using multi-head SSM.\nAdditionally, an encoder-decoder architecture with stacked UniMamba blocks is\nformed to facilitate multi-scale spatial learning hierarchically. Extensive\nexperiments are conducted on three popular datasets: nuScenes, Waymo and\nArgoverse 2. Particularly, our UniMamba achieves 70.2 mAP on the nuScenes\ndataset.\n","versions":"[{'version': 'v1', 'created': 'Sat, 15 Mar 2025 06:22:31 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 09:27:50 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Jin', 'Xin', ''], ['Su', 'Haisheng', ''], ['Liu', 'Kai', ''], ['Ma', 'Cong', ''], ['Wu', 'Wei', ''], ['Hui', 'Fei', ''], ['Yan', 'Junchi', '']]","extracted_entities":"[{'text': 'iterative self-attention', 'label': 'Attention mechanism'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'dynamic spatial position\\nembedding', 'label': 'contextual Embedding'}, {'text': 'multi-scale spatial learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2503.13064,"submitter":"Pranav Suryadevara","authors":"Pranav Suryadevara","title":"HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads","comments":"5 pages, 5 figures. Individual Project","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AR cs.PF","license":"http:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/","abstract":"  The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols and specialized pathways to deep learning accelerators like\nGemmini. Simulation tools like Gem5 and DRAMSim2 are used to evaluate baseline\nperformance and scalability under representative ML workloads. The findings of\nthis study highlight the design choices and anticipated challenges, paving the\nway for low-latency scalable memory operations for ML applications.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 11:10:49 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Suryadevara', 'Pranav', '']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2503.13385,"submitter":"Qing Zhou","authors":"Qing Zhou, Junyu Gao and Qi Wang","title":"Scale Efficient Training for Large Datasets","comments":"Accepted by CVPR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  The rapid growth of dataset scales has been a key driver in advancing deep\nlearning research. However, as dataset scale increases, the training process\nbecomes increasingly inefficient due to the presence of low-value samples,\nincluding excessive redundant samples, overly challenging samples, and\ninefficient easy samples that contribute little to model improvement.To address\nthis challenge, we propose Scale Efficient Training (SeTa) for large datasets,\na dynamic sample pruning approach that losslessly reduces training time. To\nremove low-value samples, SeTa first performs random pruning to eliminate\nredundant samples, then clusters the remaining samples according to their\nlearning difficulty measured by loss. Building upon this clustering, a sliding\nwindow strategy is employed to progressively remove both overly challenging and\ninefficient easy clusters following an easy-to-hard curriculum.We conduct\nextensive experiments on large-scale synthetic datasets, including ToCa, SS1M,\nand ST+MJ, each containing over 3 million samples.SeTa reduces training costs\nby up to 50\\% while maintaining or improving performance, with minimal\ndegradation even at 70\\% cost reduction. Furthermore, experiments on various\nscale real datasets across various backbones (CNNs, Transformers, and Mambas)\nand diverse tasks (instruction tuning, multi-view stereo, geo-localization,\ncomposed image retrieval, referring image segmentation) demonstrate the\npowerful effectiveness and universality of our approach. Code is available at\nhttps:\/\/github.com\/mrazhou\/SeTa.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:13:43 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Zhou', 'Qing', ''], ['Gao', 'Junyu', ''], ['Wang', 'Qi', '']]","extracted_entities":"[{'text': 'ToCa', 'label': 'Large Language Model'}, {'text': 'SS1M', 'label': 'Large Language Model'}, {'text': 'ST+MJ', 'label': 'Large Language Model'}, {'text': 'CNNs', 'label': 'Transformers'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Mambas', 'label': 'Transformers'}, {'text': 'instruction tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2503.13427,"submitter":"Maximilian Beck","authors":"Maximilian Beck, Korbinian P\\\"oppel, Phillip Lippe, Richard Kurle,\n  Patrick M. Blies, G\\\"unter Klambauer, Sebastian B\\\"ock, Sepp Hochreiter","title":"xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference","comments":"Code available at: https:\/\/github.com\/NX-AI\/xlstm and\n  https:\/\/github.com\/NX-AI\/xlstm-jax","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent breakthroughs in solving reasoning, math and coding problems with\nLarge Language Models (LLMs) have been enabled by investing substantial\ncomputation budgets at inference time. Therefore, inference speed is one of the\nmost critical properties of LLM architectures, and there is a growing need for\nLLMs that are efficient and fast at inference. Recently, LLMs built on the\nxLSTM architecture have emerged as a powerful alternative to Transformers,\noffering linear compute scaling with sequence length and constant memory usage,\nboth highly desirable properties for efficient inference. However, such\nxLSTM-based LLMs have yet to be scaled to larger models and assessed and\ncompared with respect to inference speed and efficiency. In this work, we\nintroduce xLSTM 7B, a 7-billion-parameter LLM that combines xLSTM's\narchitectural benefits with targeted optimizations for fast and efficient\ninference. Our experiments demonstrate that xLSTM 7B achieves performance on\ndownstream tasks comparable to other similar-sized LLMs, while providing\nsignificantly faster inference speeds and greater efficiency compared to Llama-\nand Mamba-based LLMs. These results establish xLSTM 7B as the fastest and most\nefficient 7B LLM, offering a solution for tasks that require large amounts of\ntest-time computation. Our work highlights xLSTM's potential as a foundational\narchitecture for methods building on heavy use of LLM inference. Our model\nweights, model code and training code are open-source.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:54:55 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Beck', 'Maximilian', ''], ['P\u00f6ppel', 'Korbinian', ''], ['Lippe', 'Phillip', ''], ['Kurle', 'Richard', ''], ['Blies', 'Patrick M.', ''], ['Klambauer', 'G\u00fcnter', ''], ['B\u00f6ck', 'Sebastian', ''], ['Hochreiter', 'Sepp', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'xLSTM', 'label': 'Foundation Model'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'linear compute scaling', 'label': 'Scaling law'}, {'text': 'xLSTM', 'label': 'Foundation Model'}, {'text': 'Llama', 'label': 'Llama'}, {'text': 'xLSTM', 'label': 'Foundation Model'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2503.13431,"submitter":"Vincent Herrmann","authors":"Vincent Herrmann, R\\'obert Csord\\'as, J\\\"urgen Schmidhuber","title":"Measuring In-Context Computation Complexity via Hidden State Prediction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Detecting when a neural sequence model does \"interesting\" computation is an\nopen problem. The next token prediction loss is a poor indicator: Low loss can\nstem from trivially predictable sequences that are uninteresting, while high\nloss may reflect unpredictable but also irrelevant information that can be\nignored by the model. We propose a better metric: measuring the model's ability\nto predict its own future hidden states. We show empirically that this metric\n-- in contrast to the next token prediction loss -- correlates with the\nintuitive interestingness of the task. To measure predictability, we introduce\nthe architecture-agnostic \"prediction of hidden states\" (PHi) layer that serves\nas an information bottleneck on the main pathway of the network (e.g., the\nresidual stream in Transformers). We propose a novel learned predictive prior\nthat enables us to measure the novel information gained in each computation\nstep, which serves as our metric. We show empirically that our metric predicts\nthe description length of formal languages learned in-context, the complexity\nof mathematical reasoning problems, and the correctness of self-generated\nreasoning chains.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:56:14 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Herrmann', 'Vincent', ''], ['Csord\u00e1s', 'R\u00f3bert', ''], ['Schmidhuber', 'J\u00fcrgen', '']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'self-generated\\nreasoning chains', 'label': 'Chain of thought'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2503.1344,"submitter":"Yingyue Li","authors":"Yingyue Li, Bencheng Liao, Wenyu Liu, Xinggang Wang","title":"MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling","comments":"Code and model are available at http:\/\/github.com\/hustvl\/MaTVLM","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  With the advancement of RNN models with linear complexity, the quadratic\ncomplexity challenge of transformers has the potential to be overcome. Notably,\nthe emerging Mamba-2 has demonstrated competitive performance, bridging the gap\nbetween RNN models and transformers. However, due to sequential processing and\nvanishing gradients, RNN models struggle to capture long-range dependencies,\nlimiting contextual understanding. This results in slow convergence, high\nresource demands, and poor performance on downstream understanding and complex\nreasoning tasks. In this work, we present a hybrid model MaTVLM by substituting\na portion of the transformer decoder layers in a pre-trained VLM with Mamba-2\nlayers. Leveraging the inherent relationship between attention and Mamba-2, we\ninitialize Mamba-2 with corresponding attention weights to accelerate\nconvergence. Subsequently, we employ a single-stage distillation process, using\nthe pre-trained VLM as the teacher model to transfer knowledge to the MaTVLM,\nfurther enhancing convergence speed and performance. Furthermore, we\ninvestigate the impact of differential distillation loss within our training\nframework. We evaluate the MaTVLM on multiple benchmarks, demonstrating\ncompetitive performance against the teacher model and existing VLMs while\nsurpassing both Mamba-based VLMs and models of comparable parameter scales.\nRemarkably, the MaTVLM achieves up to 3.6x faster inference than the teacher\nmodel while reducing GPU memory consumption by 27.5%, all without compromising\nperformance. Code and models are released at http:\/\/github.com\/hustvl\/MaTVLM.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:59:01 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 07:07:28 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Li', 'Yingyue', ''], ['Liao', 'Bencheng', ''], ['Liu', 'Wenyu', ''], ['Wang', 'Xinggang', '']]","extracted_entities":"[{'text': 'transformers', 'label': 'Transformers'}, {'text': 'transformers', 'label': 'Transformers'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'single-stage distillation process', 'label': 'Knowledge distillation'}]","assigned_concept":"Transformers","matched_keyword":"transformers","similarity_score":1.0}
{"id":2503.13903,"submitter":"Qiang Qi","authors":"Qiang Qi, Xiao Wang","title":"TGBFormer: Transformer-GraphFormer Blender Network for Video Object\n  Detection","comments":"Accepted by AAAI2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Video object detection has made significant progress in recent years thanks\nto convolutional neural networks (CNNs) and vision transformers (ViTs).\nTypically, CNNs excel at capturing local features but struggle to model global\nrepresentations. Conversely, ViTs are adept at capturing long-range global\nfeatures but face challenges in representing local feature details.\nOff-the-shelf video object detection methods solely rely on CNNs or ViTs to\nconduct feature aggregation, which hampers their capability to simultaneously\nleverage global and local information, thereby resulting in limited detection\nperformance. In this paper, we propose a Transformer-GraphFormer Blender\nNetwork (TGBFormer) for video object detection, with three key technical\nimprovements to fully exploit the advantages of transformers and graph\nconvolutional networks while compensating for their limitations. First, we\ndevelop a spatial-temporal transformer module to aggregate global contextual\ninformation, constituting global representations with long-range feature\ndependencies. Second, we introduce a spatial-temporal GraphFormer module that\nutilizes local spatial and temporal relationships to aggregate features,\ngenerating new local representations that are complementary to the transformer\noutputs. Third, we design a global-local feature blender module to adaptively\ncouple transformer-based global representations and GraphFormer-based local\nrepresentations. Extensive experiments demonstrate that our TGBFormer\nestablishes new state-of-the-art results on the ImageNet VID dataset.\nParticularly, our TGBFormer achieves 86.5% mAP while running at around 41.0 FPS\non a single Tesla A100 GPU.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 05:03:05 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Qi', 'Qiang', ''], ['Wang', 'Xiao', '']]","extracted_entities":"[{'text': 'vision transformers', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"vision transformers","similarity_score":0.7330732346}
{"id":2503.14354,"submitter":"Omkar Kokane","authors":"Omkar Kokane, Gopal Raut, Salim Ullah, Mukul Lokhande, Adam Teman,\n  Akash Kumar and Santosh Kumar Vishvakarma","title":"Retrospective: A CORDIC Based Configurable Activation Function for NN\n  Applications","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AR cs.AI cs.CV cs.ET eess.IV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  A CORDIC-based configuration for the design of Activation Functions (AF) was\npreviously suggested to accelerate ASIC hardware design for\nresource-constrained systems by providing functional reconfigurability. Since\nits introduction, this new approach for neural network acceleration has gained\nwidespread popularity, influencing numerous designs for activation functions in\nboth academic and commercial AI processors. In this retrospective analysis, we\nexplore the foundational aspects of this initiative, summarize key developments\nover recent years, and introduce the DA-VINCI AF tailored for the evolving\nneeds of AI applications. This new generation of dynamically configurable and\nprecision-adjustable activation function cores promise greater adaptability for\na range of activation functions in AI workloads, including Swish, SoftMax,\nSeLU, and GeLU, utilizing the Shift-and-Add CORDIC technique. The previously\npresented design has been optimized for MAC, Sigmoid, and Tanh functionalities\nand incorporated into ReLU AFs, culminating in an accumulative NEURIC compute\nunit. These enhancements position NEURIC as a fundamental component in the\nresource-efficient vector engine for the realization of AI accelerators that\nfocus on DNNs, RNNs\/LSTMs, and Transformers, achieving a quality of results\n(QoR) of 98.5%.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 15:38:37 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Kokane', 'Omkar', ''], ['Raut', 'Gopal', ''], ['Ullah', 'Salim', ''], ['Lokhande', 'Mukul', ''], ['Teman', 'Adam', ''], ['Kumar', 'Akash', ''], ['Vishvakarma', 'Santosh Kumar', '']]","extracted_entities":"[{'text': 'Swish', 'label': 'Transformers'}, {'text': 'SoftMax', 'label': 'Transformers'}, {'text': 'MAC', 'label': 'Transformers'}, {'text': 'Sigmoid', 'label': 'Transformers'}, {'text': 'NEURIC', 'label': 'Foundation Model'}, {'text': 'DNNs', 'label': 'AI model'}, {'text': 'RNNs', 'label': 'AI model'}, {'text': 'Transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2503.14376,"submitter":"Maximilian Beck","authors":"Maximilian Beck, Korbinian P\\\"oppel, Phillip Lippe, Sepp Hochreiter","title":"Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM\n  Kernels","comments":"Code available at: https:\/\/github.com\/NX-AI\/mlstm_kernels","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Linear RNNs with gating recently demonstrated competitive performance\ncompared to Transformers in language modeling. Although their linear compute\nscaling in sequence length offers theoretical runtime advantages over\nTransformers, realizing these benefits in practice requires optimized custom\nkernels, as Transformers rely on the highly efficient Flash Attention kernels.\nLeveraging the chunkwise-parallel formulation of linear RNNs, Flash Linear\nAttention (FLA) shows that linear RNN kernels are faster than Flash Attention,\nby parallelizing over chunks of the input sequence. However, since the chunk\nsize of FLA is limited, many intermediate states must be materialized in GPU\nmemory. This leads to low arithmetic intensity and causes high memory\nconsumption and IO cost, especially for long-context pre-training. In this\nwork, we present Tiled Flash Linear Attention (TFLA), a novel kernel algorithm\nfor linear RNNs, that enables arbitrary large chunk sizes by introducing an\nadditional level of sequence parallelization within each chunk. First, we apply\nTFLA to the xLSTM with matrix memory, the mLSTM. Second, we propose an mLSTM\nvariant with sigmoid input gate and reduced computation for even faster kernel\nruntimes at equal language modeling performance. In our speed benchmarks, we\nshow that our new mLSTM kernels based on TFLA outperform highly optimized Flash\nAttention, Linear Attention and Mamba kernels, setting a new state of the art\nfor efficient long-context sequence modeling primitives.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:09:47 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Beck', 'Maximilian', ''], ['P\u00f6ppel', 'Korbinian', ''], ['Lippe', 'Phillip', ''], ['Hochreiter', 'Sepp', '']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Flash Attention', 'label': 'Attention mechanism'}, {'text': 'Flash Linear\\nAttention', 'label': 'Attention mechanism'}, {'text': 'Flash Attention', 'label': 'Attention mechanism'}, {'text': 'long-context pre-training', 'label': 'Few-shot Learning'}, {'text': 'Flash Linear Attention', 'label': 'Attention mechanism'}, {'text': 'sigmoid input gate', 'label': 'Attention mechanism'}, {'text': 'TFLA', 'label': 'Transformers'}, {'text': 'Flash\\nAttention', 'label': 'Attention mechanism'}, {'text': 'Linear Attention', 'label': 'Attention mechanism'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2503.14456,"submitter":"Daniel Goldstein","authors":"Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Haowen Hou,\n  Janna Lu, William Merrill, Guangyu Song, Kaifeng Tan, Saiteja Utpala, Nathan\n  Wilce, Johan S. Wind, Tianyi Wu, Daniel Wuttke, Christian Zhou-Zheng","title":"RWKV-7 \"Goose\" with Expressive Dynamic State Evolution","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with\npre-trained language models that establish a new state-of-the-art in downstream\nperformance at the 3 billion parameter scale on multilingual tasks, and match\ncurrent SoTA English language performance despite being trained on dramatically\nfewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only\nconstant memory usage and constant inference time per token. RWKV-7 introduces\na newly generalized formulation of the delta rule with vector-valued gating and\nin-context learning rates, as well as a relaxed value replacement rule. We show\nthat RWKV-7 can perform state tracking and recognize all regular languages,\nwhile retaining parallelizability of training. This exceeds the capabilities of\nTransformers under standard complexity conjectures, which are limited to\n$\\mathsf{TC}^0$. To demonstrate RWKV-7's language modeling capability, we also\npresent an extended open source 3.1 trillion token multilingual corpus, and\ntrain four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on\nthis dataset.\n  To foster openness, reproduction, and adoption, we release our models and\ndataset component listing at https:\/\/huggingface.co\/RWKV, and our training and\ninference code at https:\/\/github.com\/RWKV\/RWKV-LM all under the Apache 2.0\nLicense.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:31:05 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Peng', 'Bo', ''], ['Zhang', 'Ruichong', ''], ['Goldstein', 'Daniel', ''], ['Alcaide', 'Eric', ''], ['Hou', 'Haowen', ''], ['Lu', 'Janna', ''], ['Merrill', 'William', ''], ['Song', 'Guangyu', ''], ['Tan', 'Kaifeng', ''], ['Utpala', 'Saiteja', ''], ['Wilce', 'Nathan', ''], ['Wind', 'Johan S.', ''], ['Wu', 'Tianyi', ''], ['Wuttke', 'Daniel', ''], ['Zhou-Zheng', 'Christian', '']]","extracted_entities":"[{'text': 'vector-valued gating', 'label': 'Few-shot Learning'}, {'text': 'in-context learning rates', 'label': 'Few-shot Learning'}, {'text': 'Transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2503.14615,"submitter":"Anej Svete","authors":"Selim Jerad, Anej Svete, Jiaoda Li, Ryan Cotterell","title":"Unique Hard Attention: A Tale of Two Sides","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CC cs.CL cs.FL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Understanding the expressive power of transformers has recently attracted\nattention, as it offers insights into their abilities and limitations. Many\nstudies analyze unique hard attention transformers, where attention selects a\nsingle position that maximizes the attention scores. When multiple positions\nachieve the maximum score, either the rightmost or the leftmost of those is\nchosen. In this paper, we highlight the importance of this seeming triviality.\nRecently, finite-precision transformers with both leftmost- and rightmost-hard\nattention were shown to be equivalent to Linear Temporal Logic (LTL). We show\nthat this no longer holds with only leftmost-hard attention -- in that case,\nthey correspond to a \\emph{strictly weaker} fragment of LTL. Furthermore, we\nshow that models with leftmost-hard attention are equivalent to \\emph{soft}\nattention, suggesting they may better approximate real-world transformers than\nright-attention models. These findings refine the landscape of transformer\nexpressivity and underscore the role of attention directionality.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 18:12:09 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Jerad', 'Selim', ''], ['Svete', 'Anej', ''], ['Li', 'Jiaoda', ''], ['Cotterell', 'Ryan', '']]","extracted_entities":"[{'text': 'transformers', 'label': 'Transformers'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'transformers', 'label': 'Transformers'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'finite-precision transformers', 'label': 'Transformers'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'transformers', 'label': 'Transformers'}, {'text': 'attention', 'label': 'Attention mechanism'}]","assigned_concept":"Transformers","matched_keyword":"transformers","similarity_score":1.0}
{"id":2503.15023,"submitter":"Mehdi Ayoub Rabiai","authors":"Chaouki Boufenar, Mehdi Ayoub Rabiai, Boualem Nadjib Zahaf and Khelil\n  Rafik Ouaras","title":"Bridging the Gap: Fusing CNNs and Transformers to Decode the Elegance of\n  Handwritten Arabic Script","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Handwritten Arabic script recognition is a challenging task due to the\nscript's dynamic letter forms and contextual variations. This paper proposes a\nhybrid approach combining convolutional neural networks (CNNs) and\nTransformer-based architectures to address these complexities. We evaluated\ncustom and fine-tuned models, including EfficientNet-B7 and Vision Transformer\n(ViT-B16), and introduced an ensemble model that leverages confidence-based\nfusion to integrate their strengths. Our ensemble achieves remarkable\nperformance on the IFN\/ENIT dataset, with 96.38% accuracy for letter\nclassification and 97.22% for positional classification. The results highlight\nthe complementary nature of CNNs and Transformers, demonstrating their combined\npotential for robust Arabic handwriting recognition. This work advances OCR\nsystems, offering a scalable solution for real-world applications.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 09:20:42 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Boufenar', 'Chaouki', ''], ['Rabiai', 'Mehdi Ayoub', ''], ['Zahaf', 'Boualem Nadjib', ''], ['Ouaras', 'Khelil Rafik', '']]","extracted_entities":"[{'text': 'Transformer-based architectures', 'label': 'Transformers'}, {'text': 'confidence-based\\nfusion', 'label': 'Embedding'}, {'text': 'Transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2503.15213,"submitter":"HanCong Feng","authors":"Hancong Feng KaiLI Jiang Bin tang","title":"Sig2text, a Vision-language model for Non-cooperative Radar Signal\n  Parsing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Automatic non-cooperative analysis of intercepted radar signals is essential\nfor intelligent equipment in both military and civilian domains. Accurate\nmodulation identification and parameter estimation enable effective signal\nclassification, threat assessment, and the development of countermeasures. In\nthis paper, we propose a symbolic approach for radar signal recognition and\nparameter estimation based on a vision-language model that combines\ncontext-free grammar with time-frequency representation of radar waveforms. The\nproposed model, called Sig2text, leverages the power of vision transformers for\ntime-frequency feature extraction and transformer-based decoders for symbolic\nparsing of radar waveforms. By treating radar signal recognition as a parsing\nproblem, Sig2text can effectively recognize and parse radar waveforms with\ndifferent modulation types and parameters. We evaluate the performance of\nSig2text on a synthetic radar signal dataset and demonstrate its effectiveness\nin recognizing and parsing radar waveforms with varying modulation types and\nparameters. The training code of the model is available at\nhttps:\/\/github.com\/Na-choneko\/sig2text.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 13:53:40 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['tang', 'Hancong Feng KaiLI Jiang Bin', '']]","extracted_entities":"[{'text': 'vision transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"vision transformers","similarity_score":0.7330732346}
{"id":2503.1589,"submitter":"Yoav Wald","authors":"Yoav Wald, Mark Goldstein, Yonathan Efroni, Wouter A.C. van Amsterdam,\n  Rajesh Ranganath","title":"Time After Time: Deep-Q Effect Estimation for Interventions on When and\n  What to do","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Problems in fields such as healthcare, robotics, and finance requires\nreasoning about the value both of what decision or action to take and when to\ntake it. The prevailing hope is that artificial intelligence will support such\ndecisions by estimating the causal effect of policies such as how to treat\npatients or how to allocate resources over time. However, existing methods for\nestimating the effect of a policy struggle with \\emph{irregular time}. They\neither discretize time, or disregard the effect of timing policies. We present\na new deep-Q algorithm that estimates the effect of both when and what to do\ncalled Earliest Disagreement Q-Evaluation (EDQ). EDQ makes use of recursion for\nthe Q-function that is compatible with flexible sequence models, such as\ntransformers. EDQ provides accurate estimates under standard assumptions. We\nvalidate the approach through experiments on survival time and tumor growth\ntasks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 06:27:35 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wald', 'Yoav', ''], ['Goldstein', 'Mark', ''], ['Efroni', 'Yonathan', ''], ['van Amsterdam', 'Wouter A. C.', ''], ['Ranganath', 'Rajesh', '']]","extracted_entities":"[{'text': 'transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"transformers","similarity_score":1.0}
{"id":2503.15893,"submitter":"Jiawei Wang","authors":"Jiawei Wang and Kai Hu and Qiang Huo","title":"UniHDSA: A Unified Relation Prediction Approach for Hierarchical\n  Document Structure Analysis","comments":"Accepted by Pattern Recognition. arXiv admin note: substantial text\n  overlap with arXiv:2405.11757","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Document structure analysis, aka document layout analysis, is crucial for\nunderstanding both the physical layout and logical structure of documents,\nserving information retrieval, document summarization, knowledge extraction,\netc. Hierarchical Document Structure Analysis (HDSA) specifically aims to\nrestore the hierarchical structure of documents created using authoring\nsoftware with hierarchical schemas. Previous research has primarily followed\ntwo approaches: one focuses on tackling specific subtasks of HDSA in isolation,\nsuch as table detection or reading order prediction, while the other adopts a\nunified framework that uses multiple branches or modules, each designed to\naddress a distinct task. In this work, we propose a unified relation prediction\napproach for HDSA, called UniHDSA, which treats various HDSA sub-tasks as\nrelation prediction problems and consolidates relation prediction labels into a\nunified label space. This allows a single relation prediction module to handle\nmultiple tasks simultaneously, whether at a page-level or document-level\nstructure analysis. To validate the effectiveness of UniHDSA, we develop a\nmultimodal end-to-end system based on Transformer architectures. Extensive\nexperimental results demonstrate that our approach achieves state-of-the-art\nperformance on a hierarchical document structure analysis benchmark,\nComp-HRDoc, and competitive results on a large-scale document layout analysis\ndataset, DocLayNet, effectively illustrating the superiority of our method\nacross all sub-tasks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 06:44:47 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wang', 'Jiawei', ''], ['Hu', 'Kai', ''], ['Huo', 'Qiang', '']]","extracted_entities":"[{'text': 'document summarization', 'label': 'Knowledge distillation'}, {'text': 'Transformer architectures', 'label': 'Transformers'}, {'text': 'DocLayNet', 'label': 'Large Language Model'}]","assigned_concept":"Transformers","matched_keyword":"Transformer architectures","similarity_score":0.5942972898}
{"id":2503.15902,"submitter":"Jose Miguel Lara Rangel","authors":"Jose Lara-Rangel, Clare Heinbaugh","title":"On the Limits of Applying Graph Transformers for Brain Connectome\n  Classification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Brain connectomes offer detailed maps of neural connections within the brain.\nRecent studies have proposed novel connectome graph datasets and attempted to\nimprove connectome classification by using graph deep learning. With recent\nadvances demonstrating transformers' ability to model intricate relationships\nand outperform in various domains, this work explores their performance on the\nnovel NeuroGraph benchmark datasets and synthetic variants derived from\nprobabilistically removing edges to simulate noisy data. Our findings suggest\nthat graph transformers offer no major advantage over traditional GNNs on this\ndataset. Furthermore, both traditional and transformer GNN models maintain\naccuracy even with all edges removed, suggesting that the dataset's graph\nstructures may not significantly impact predictions. We propose further\nassessing NeuroGraph as a brain connectome benchmark, emphasizing the need for\nwell-curated datasets and improved preprocessing strategies to obtain\nmeaningful edge connections.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 07:03:13 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Lara-Rangel', 'Jose', ''], ['Heinbaugh', 'Clare', '']]","extracted_entities":"[{'text': 'graph deep learning', 'label': 'Few-shot Learning'}, {'text': 'transformers', 'label': 'Transformers'}, {'text': 'transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"transformers","similarity_score":1.0}
{"id":2503.15927,"submitter":"Hui Zhang","authors":"Hui Zhang, Tingwei Gao, Jie Shao, Zuxuan Wu","title":"BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers","comments":"Accepted by CVPR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:07:31 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zhang', 'Hui', ''], ['Gao', 'Tingwei', ''], ['Shao', 'Jie', ''], ['Wu', 'Zuxuan', '']]","extracted_entities":"[{'text': 'Diffusion Transformers', 'label': 'Transformers'}, {'text': 'DiTs', 'label': 'Transformers'}, {'text': 'DiTs', 'label': 'Transformers'}, {'text': 'DiTs', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Diffusion Transformers","similarity_score":0.5920959711}
{"id":2503.15934,"submitter":"Hongda Liu","authors":"Hongda Liu, Longguang Wang, Ye Zhang, Ziru Yu, Yulan Guo","title":"SaMam: Style-aware State Space Model for Arbitrary Image Style Transfer","comments":"11 pages, 10 figures, 2 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Global effective receptive field plays a crucial role for image style\ntransfer (ST) to obtain high-quality stylized results. However, existing ST\nbackbones (e.g., CNNs and Transformers) suffer huge computational complexity to\nachieve global receptive fields. Recently, the State Space Model (SSM),\nespecially the improved variant Mamba, has shown great potential for long-range\ndependency modeling with linear complexity, which offers a approach to resolve\nthe above dilemma. In this paper, we develop a Mamba-based style transfer\nframework, termed SaMam. Specifically, a mamba encoder is designed to\nefficiently extract content and style information. In addition, a style-aware\nmamba decoder is developed to flexibly adapt to various styles. Moreover, to\naddress the problems of local pixel forgetting, channel redundancy and spatial\ndiscontinuity of existing SSMs, we introduce both local enhancement and zigzag\nscan. Qualitative and quantitative results demonstrate that our SaMam\noutperforms state-of-the-art methods in terms of both accuracy and efficiency.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:18:27 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Liu', 'Hongda', ''], ['Wang', 'Longguang', ''], ['Zhang', 'Ye', ''], ['Yu', 'Ziru', ''], ['Guo', 'Yulan', '']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2503.15986,"submitter":"Zeqi Zheng","authors":"Zeqi Zheng, Yanchen Huang, Yingchao Yu, Zizheng Zhu, Junfeng Tang,\n  Zhaofei Yu, Yaochu Jin","title":"SpiLiFormer: Enhancing Spiking Transformers with Lateral Inhibition","comments":"16 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.NE cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Spiking Neural Networks (SNNs) based on Transformers have garnered\nsignificant attention due to their superior performance and high energy\nefficiency. However, the spiking attention modules of most existing\nTransformer-based SNNs are adapted from those of analog Transformers, failing\nto fully address the issue of over-allocating attention to irrelevant contexts.\nTo fix this fundamental yet overlooked issue, we propose a Lateral\nInhibition-inspired Spiking Transformer (SpiLiFormer). It emulates the brain's\nlateral inhibition mechanism, guiding the model to enhance attention to\nrelevant tokens while suppressing attention to irrelevant ones. Our model\nachieves state-of-the-art (SOTA) performance across multiple datasets,\nincluding CIFAR-10 (+0.45%), CIFAR-100 (+0.48%), CIFAR10-DVS (+2.70%),\nN-Caltech101 (+1.94%), and ImageNet-1K (+1.6%). Notably, on the ImageNet-1K\ndataset, SpiLiFormer (69.9M parameters, 4 time steps, 384 resolution)\noutperforms E-SpikeFormer (173.0M parameters, 8 time steps, 384 resolution), a\nSOTA spiking Transformer, by 0.46% using only 39% of the parameters and half\nthe time steps. Our code and training checkpoints will be released upon\nacceptance.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 09:36:31 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zheng', 'Zeqi', ''], ['Huang', 'Yanchen', ''], ['Yu', 'Yingchao', ''], ['Zhu', 'Zizheng', ''], ['Tang', 'Junfeng', ''], ['Yu', 'Zhaofei', ''], ['Jin', 'Yaochu', '']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}, {'text': 'Transformers', 'label': 'Transformers'}, {'text': 'lateral inhibition mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2503.16057,"submitter":"Yike Yuan","authors":"Yike Yuan, Ziyu Wang, Zihao Huang, Defa Zhu, Xun Zhou, Jingyi Yu,\n  Qiyang Min","title":"Expert Race: A Flexible Routing Strategy for Scaling Diffusion\n  Transformer with Mixture of Experts","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Diffusion models have emerged as mainstream framework in visual generation.\nBuilding upon this success, the integration of Mixture of Experts (MoE) methods\nhas shown promise in enhancing model scalability and performance. In this\npaper, we introduce Race-DiT, a novel MoE model for diffusion transformers with\na flexible routing strategy, Expert Race. By allowing tokens and experts to\ncompete together and select the top candidates, the model learns to dynamically\nassign experts to critical tokens. Additionally, we propose per-layer\nregularization to address challenges in shallow layer learning, and router\nsimilarity loss to prevent mode collapse, ensuring better expert utilization.\nExtensive experiments on ImageNet validate the effectiveness of our approach,\nshowcasing significant performance gains while promising scaling properties.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 11:45:08 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Yuan', 'Yike', ''], ['Wang', 'Ziyu', ''], ['Huang', 'Zihao', ''], ['Zhu', 'Defa', ''], ['Zhou', 'Xun', ''], ['Yu', 'Jingyi', ''], ['Min', 'Qiyang', '']]","extracted_entities":"[{'text': 'diffusion transformers', 'label': 'Transformers'}, {'text': 'per-layer\\nregularization', 'label': 'Fine-tuning'}, {'text': 'shallow layer learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Transformers","matched_keyword":"diffusion transformers","similarity_score":0.5920959711}
{"id":2503.16351,"submitter":"Sameed Siddiqui","authors":"Krithik Ramesh (1 and 2), Sameed M. Siddiqui (1 and 3), Albert Gu (4),\n  Michael D. Mitzenmacher (1 and 5), Pardis C. Sabeti (1 and 6 and 7 and 8)\n  ((1) Broad Institute of MIT and Harvard, (2) Massachusetts Institute of\n  Technology, (3) Computational and Systems Biology Program, Massachusetts\n  Institute of Technology, (4) Machine Learning Department, Carnegie Mellon\n  University, (5) School of Engineering and Applied Sciences, Harvard\n  University, (6) Department of Organismic and Evolutionary Biology, Harvard\n  University, (7) Department of Immunology and Infectious Diseases, Harvard\n  T.H. Chan School of Public Health, Harvard University, (8) Howard Hughes\n  Medical Institute)","title":"Lyra: An Efficient and Expressive Subquadratic Architecture for Modeling\n  Biological Sequences","comments":"53 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG q-bio.GN","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Deep learning architectures such as convolutional neural networks and\nTransformers have revolutionized biological sequence modeling, with recent\nadvances driven by scaling up foundation and task-specific models. The\ncomputational resources and large datasets required, however, limit their\napplicability in biological contexts. We introduce Lyra, a subquadratic\narchitecture for sequence modeling, grounded in the biological framework of\nepistasis for understanding sequence-to-function relationships. Mathematically,\nwe demonstrate that state space models efficiently capture global epistatic\ninteractions and combine them with projected gated convolutions for modeling\nlocal relationships. We demonstrate that Lyra is performant across over 100\nwide-ranging biological tasks, achieving state-of-the-art (SOTA) performance in\nmany key areas, including protein fitness landscape prediction, biophysical\nproperty prediction (e.g. disordered protein region functions) peptide\nengineering applications (e.g. antibody binding, cell-penetrating peptide\nprediction), RNA structure analysis, RNA function prediction, and CRISPR guide\ndesign. It achieves this with orders-of-magnitude improvements in inference\nspeed and reduction in parameters (up to 120,000-fold in our tests) compared to\nrecent biology foundation models. Using Lyra, we were able to train and run\nevery task in this study on two or fewer GPUs in under two hours, democratizing\naccess to biological sequence modeling at SOTA performance, with potential\napplications to many fields.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:09:18 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Ramesh', 'Krithik', '', '1 and 2'], ['Siddiqui', 'Sameed M.', '', '1 and 3'], ['Gu', 'Albert', '', '1 and 5'], ['Mitzenmacher', 'Michael D.', '', '1 and 5'], ['Sabeti', 'Pardis C.', '', '1 and 6 and 7 and 8']]","extracted_entities":"[{'text': 'Transformers', 'label': 'Transformers'}]","assigned_concept":"Transformers","matched_keyword":"Transformers","similarity_score":1.0}
{"id":2503.16428,"submitter":"Guangxuan Xiao","authors":"Ruyi Xu and Guangxuan Xiao and Haofeng Huang and Junxian Guo and Song\n  Han","title":"XAttention: Block Sparse Attention with Antidiagonal Scoring","comments":"The first two authors contributed equally to this work","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Long-Context Transformer Models (LCTMs) are vital for real-world applications\nbut suffer high computational costs due to attention's quadratic complexity.\nBlock-sparse attention mitigates this by focusing computation on critical\nregions, yet existing methods struggle with balancing accuracy and efficiency\ndue to costly block importance measurements. In this paper, we introduce\nXAttention, a plug-and-play framework that dramatically accelerates\nlong-context inference in Transformers models using sparse attention.\nXAttention's key innovation is the insight that the sum of antidiagonal values\n(i.e., from the lower-left to upper-right) in the attention matrix provides a\npowerful proxy for block importance. This allows for precise identification and\npruning of non-essential blocks, resulting in high sparsity and dramatically\naccelerated inference. Across comprehensive evaluations on demanding\nlong-context benchmarks-including RULER and LongBench for language, VideoMME\nfor video understanding, and VBench for video generation. XAttention achieves\naccuracy comparable to full attention while delivering substantial\ncomputational gains. We demonstrate up to 13.5x acceleration in attention\ncomputation. These results underscore XAttention's ability to unlock the\npractical potential of block sparse attention, paving the way for scalable and\nefficient deployment of LCTMs in real-world applications. Code is available at\nhttps:\/\/github.com\/mit-han-lab\/x-attention.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:59:58 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Xu', 'Ruyi', ''], ['Xiao', 'Guangxuan', ''], ['Huang', 'Haofeng', ''], ['Guo', 'Junxian', ''], ['Han', 'Song', '']]","extracted_entities":"[{'text': 'Long-Context Transformer Models', 'label': 'Transformers'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'Transformers models', 'label': 'Transformers'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'attention', 'label': 'Attention mechanism'}]","assigned_concept":"Transformers","matched_keyword":"Transformers models","similarity_score":0.8127859235}
{"id":2410.14052,"submitter":"Alireza Rezazadeh","authors":"Alireza Rezazadeh, Zichao Li, Wei Wei, Yujia Bao","title":"From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory\n  Representation for LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Recent advancements in large language models have significantly improved\ntheir context windows, yet challenges in effective long-term memory management\nremain. We introduce MemTree, an algorithm that leverages a dynamic,\ntree-structured memory representation to optimize the organization, retrieval,\nand integration of information, akin to human cognitive schemas. MemTree\norganizes memory hierarchically, with each node encapsulating aggregated\ntextual content, corresponding semantic embeddings, and varying abstraction\nlevels across the tree's depths. Our algorithm dynamically adapts this memory\nstructure by computing and comparing semantic embeddings of new and existing\ninformation to enrich the model's context-awareness. This approach allows\nMemTree to handle complex reasoning and extended interactions more effectively\nthan traditional memory augmentation methods, which often rely on flat lookup\ntables. Evaluations on benchmarks for multi-turn dialogue understanding and\ndocument question answering show that MemTree significantly enhances\nperformance in scenarios that demand structured memory management.\n","versions":"[{'version': 'v1', 'created': 'Thu, 17 Oct 2024 21:47:11 GMT'}, {'version': 'v2', 'created': 'Tue, 3 Dec 2024 18:48:00 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 21:18:02 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Rezazadeh', 'Alireza', ''], ['Li', 'Zichao', ''], ['Wei', 'Wei', ''], ['Bao', 'Yujia', '']]","extracted_entities":"[{'text': 'semantic embeddings', 'label': 'contextual Embedding'}, {'text': 'semantic embeddings', 'label': 'contextual Embedding'}]","assigned_concept":"contextual Embedding","matched_keyword":"semantic embeddings","similarity_score":0.7435399294}
{"id":2412.13684,"submitter":"Chuang Yang","authors":"Chuang Yang, Bingxuan Zhao, Qing Zhou, and Qi Wang","title":"MMO-IG: Multi-Class and Multi-Scale Object Image Generation for Remote\n  Sensing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The rapid advancement of deep generative models (DGMs) has significantly\nadvanced research in computer vision, providing a cost-effective alternative to\nacquiring vast quantities of expensive imagery. However, existing methods\npredominantly focus on synthesizing remote sensing (RS) images aligned with\nreal images in a global layout view, which limits their applicability in RS\nimage object detection (RSIOD) research. To address these challenges, we\npropose a multi-class and multi-scale object image generator based on DGMs,\ntermed MMO-IG, designed to generate RS images with supervised object labels\nfrom global and local aspects simultaneously. Specifically, from the local\nview, MMO-IG encodes various RS instances using an iso-spacing instance map\n(ISIM). During the generation process, it decodes each instance region with\niso-spacing value in ISIM-corresponding to both background and foreground\ninstances-to produce RS images through the denoising process of diffusion\nmodels. Considering the complex interdependencies among MMOs, we construct a\nspatial-cross dependency knowledge graph (SCDKG). This ensures a realistic and\nreliable multidirectional distribution among MMOs for region embedding, thereby\nreducing the discrepancy between source and target domains. Besides, we propose\na structured object distribution instruction (SODI) to guide the generation of\nsynthesized RS image content from a global aspect with SCDKG-based ISIM\ntogether. Extensive experimental results demonstrate that our MMO-IG exhibits\nsuperior generation capabilities for RS images with dense MMO-supervised\nlabels, and RS detectors pre-trained with MMO-IG show excellent performance on\nreal-world datasets.\n","versions":"[{'version': 'v1', 'created': 'Wed, 18 Dec 2024 10:19:12 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 13:22:39 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Yang', 'Chuang', ''], ['Zhao', 'Bingxuan', ''], ['Zhou', 'Qing', ''], ['Wang', 'Qi', '']]","extracted_entities":"[{'text': 'region embedding', 'label': 'contextual Embedding'}]","assigned_concept":"contextual Embedding","matched_keyword":"region embedding","similarity_score":0.6026058197}
{"id":2501.0179,"submitter":"Zhengcong Fei","authors":"Zhengcong Fei, Debang Li, Di Qiu, Changqian Yu, Mingyuan Fan","title":"Ingredients: Blending Custom Photos with Video Diffusion Transformers","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper presents a powerful framework to customize video creations by\nincorporating multiple specific identity (ID) photos, with video diffusion\nTransformers, referred to as Ingredients. Generally, our method consists of\nthree primary modules: (i) a facial extractor that captures versatile and\nprecise facial features for each human ID from both global and local\nperspectives; (ii) a multi-scale projector that maps face embeddings into the\ncontextual space of image query in video diffusion transformers; (iii) an ID\nrouter that dynamically combines and allocates multiple ID embedding to the\ncorresponding space-time regions. Leveraging a meticulously curated text-video\ndataset and a multi-stage training protocol, Ingredients demonstrates superior\nperformance in turning custom photos into dynamic and personalized video\ncontent. Qualitative evaluations highlight the advantages of proposed method,\npositioning it as a significant advancement toward more effective generative\nvideo control tools in Transformer-based architecture, compared to existing\nmethods. The data, code, and model weights are publicly available at:\nhttps:\/\/github.com\/feizc\/Ingredients.\n","versions":"[{'version': 'v1', 'created': 'Fri, 3 Jan 2025 12:45:22 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 10:47:27 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Fei', 'Zhengcong', ''], ['Li', 'Debang', ''], ['Qiu', 'Di', ''], ['Yu', 'Changqian', ''], ['Fan', 'Mingyuan', '']]","extracted_entities":"[{'text': 'video diffusion\\nTransformers', 'label': 'Transformers'}, {'text': 'Ingredients', 'label': 'Transformers'}, {'text': 'face embeddings', 'label': 'contextual Embedding'}, {'text': 'video diffusion transformers', 'label': 'Transformers'}]","assigned_concept":"contextual Embedding","matched_keyword":"face embeddings","similarity_score":0.5823079944}
{"id":2501.07305,"submitter":"Xinyang Zhou","authors":"Xinyang Zhou, Fanyue Wei, Lixin Duan, Angela Yao, Wen Li","title":"The Devil is in the Spurious Correlations: Boosting Moment Retrieval\n  with Dynamic Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Given a textual query along with a corresponding video, the objective of\nmoment retrieval aims to localize the moments relevant to the query within the\nvideo. While commendable results have been demonstrated by existing\ntransformer-based approaches, predicting the accurate temporal span of the\ntarget moment is still a major challenge. This paper reveals that a crucial\nreason stems from the spurious correlation between the text query and the\nmoment context. Namely, the model makes predictions by overly associating\nqueries with background frames rather than distinguishing target moments. To\naddress this issue, we propose a dynamic learning approach for moment\nretrieval, where two strategies are designed to mitigate the spurious\ncorrelation. First, we introduce a novel video synthesis approach to construct\na dynamic context for the queried moment, enabling the model to attend to the\ntarget moment of the corresponding query across dynamic backgrounds. Second, to\nalleviate the over-association with backgrounds, we enhance representations\ntemporally by incorporating text-dynamics interaction, which encourages the\nmodel to align text with target moments through complementary dynamic\nrepresentations. With the proposed method, our model significantly alleviates\nthe spurious correlation issue in moment retrieval and establishes new\nstate-of-the-art performance on two popular benchmarks, \\ie, QVHighlights and\nCharades-STA. In addition, detailed ablation studies and evaluations across\ndifferent architectures demonstrate the generalization and effectiveness of the\nproposed strategies. Our code will be publicly available.\n","versions":"[{'version': 'v1', 'created': 'Mon, 13 Jan 2025 13:13:06 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 13:22:27 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zhou', 'Xinyang', ''], ['Wei', 'Fanyue', ''], ['Duan', 'Lixin', ''], ['Yao', 'Angela', ''], ['Li', 'Wen', '']]","extracted_entities":"[{'text': 'model', 'label': 'Neural Language Model'}, {'text': 'dynamic context', 'label': 'contextual Embedding'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'publicly available', 'label': 'Open-source LLMs'}]","assigned_concept":"contextual Embedding","matched_keyword":"dynamic context","similarity_score":0.5210316181}
{"id":2503.06169,"submitter":"Li Li","authors":"Shawn Li, Jiashu Qu, Yuxiao Zhou, Yuehan Qin, Tiankai Yang, Yue Zhao","title":"Treble Counterfactual VLMs: A Causal Approach to Hallucination","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Vision-Language Models (VLMs) have advanced multi-modal tasks like image\ncaptioning, visual question answering, and reasoning. However, they often\ngenerate hallucinated outputs inconsistent with the visual context or prompt,\nlimiting reliability in critical applications like autonomous driving and\nmedical imaging. Existing studies link hallucination to statistical biases,\nlanguage priors, and biased feature learning but lack a structured causal\nunderstanding. In this work, we introduce a causal perspective to analyze and\nmitigate hallucination in VLMs. We hypothesize that hallucination arises from\nunintended direct influences of either the vision or text modality, bypassing\nproper multi-modal fusion. To address this, we construct a causal graph for\nVLMs and employ counterfactual analysis to estimate the Natural Direct Effect\n(NDE) of vision, text, and their cross-modal interaction on the output. We\nsystematically identify and mitigate these unintended direct effects to ensure\nthat responses are primarily driven by genuine multi-modal fusion. Our approach\nconsists of three steps: (1) designing structural causal graphs to distinguish\ncorrect fusion pathways from spurious modality shortcuts, (2) estimating\nmodality-specific and cross-modal NDE using perturbed image representations,\nhallucinated text embeddings, and degraded visual inputs, and (3) implementing\na test-time intervention module to dynamically adjust the model's dependence on\neach modality. Experimental results demonstrate that our method significantly\nreduces hallucination while preserving task performance, providing a robust and\ninterpretable framework for improving VLM reliability. To enhance accessibility\nand reproducibility, our code is publicly available at\nhttps:\/\/github.com\/TREE985\/Treble-Counterfactual-VLMs.\n","versions":"[{'version': 'v1', 'created': 'Sat, 8 Mar 2025 11:13:05 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 08:11:52 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Li', 'Shawn', ''], ['Qu', 'Jiashu', ''], ['Zhou', 'Yuxiao', ''], ['Qin', 'Yuehan', ''], ['Yang', 'Tiankai', ''], ['Zhao', 'Yue', '']]","extracted_entities":"[{'text': 'perturbed image representations', 'label': 'Embedding'}, {'text': 'hallucinated text embeddings', 'label': 'contextual Embedding'}]","assigned_concept":"contextual Embedding","matched_keyword":"hallucinated text embeddings","similarity_score":0.5394981503}
{"id":2503.13857,"submitter":"Rui Yang","authors":"Rui Yang, Jiayi Tong, Haoyuan Wang, Hui Huang, Ziyang Hu, Peiyu Li,\n  Nan Liu, Christopher J. Lindsell, Michael J. Pencina, Yong Chen, Chuan Hong","title":"Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles\n  with Large Language Model-Driven Evaluations","comments":"28 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Background. Systematic reviews in comparative effectiveness research require\ntimely evidence synthesis. Preprints accelerate knowledge dissemination but\nvary in quality, posing challenges for systematic reviews.\n  Methods. We propose AutoConfidence (automated confidence assessment), an\nadvanced framework for predicting preprint publication, which reduces reliance\non manual curation and expands the range of predictors, including three key\nadvancements: (1) automated data extraction using natural language processing\ntechniques, (2) semantic embeddings of titles and abstracts, and (3) large\nlanguage model (LLM)-driven evaluation scores. Additionally, we employed two\nprediction models: a random forest classifier for binary outcome and a survival\ncure model that predicts both binary outcome and publication risk over time.\n  Results. The random forest classifier achieved AUROC 0.692 with LLM-driven\nscores, improving to 0.733 with semantic embeddings and 0.747 with article\nusage metrics. The survival cure model reached AUROC 0.716 with LLM-driven\nscores, improving to 0.731 with semantic embeddings. For publication risk\nprediction, it achieved a concordance index of 0.658, increasing to 0.667 with\nsemantic embeddings.\n  Conclusion. Our study advances the framework for preprint publication\nprediction through automated data extraction and multiple feature integration.\nBy combining semantic embeddings with LLM-driven evaluations, AutoConfidence\nenhances predictive performance while reducing manual annotation burden. The\nframework has the potential to facilitate systematic incorporation of preprint\narticles in evidence-based medicine, supporting researchers in more effective\nevaluation and utilization of preprint resources.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 03:14:23 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:21:06 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Yang', 'Rui', ''], ['Tong', 'Jiayi', ''], ['Wang', 'Haoyuan', ''], ['Huang', 'Hui', ''], ['Hu', 'Ziyang', ''], ['Li', 'Peiyu', ''], ['Liu', 'Nan', ''], ['Lindsell', 'Christopher J.', ''], ['Pencina', 'Michael J.', ''], ['Chen', 'Yong', ''], ['Hong', 'Chuan', '']]","extracted_entities":"[{'text': 'semantic embeddings', 'label': 'Embedding'}, {'text': 'random forest classifier', 'label': 'AI language model'}, {'text': 'survival\\ncure model', 'label': 'AI language model'}, {'text': 'random forest classifier', 'label': 'AI language model'}, {'text': 'semantic embeddings', 'label': 'contextual Embedding'}, {'text': 'semantic embeddings', 'label': 'Embedding'}, {'text': 'semantic embeddings', 'label': 'contextual Embedding'}, {'text': 'semantic embeddings', 'label': 'Embedding'}]","assigned_concept":"contextual Embedding","matched_keyword":"semantic embeddings","similarity_score":0.7435399294}
{"id":2503.148,"submitter":"Imran Razzak","authors":"Ghadir Alselwi, Hao Xue, Shoaib Jameel, Basem Suleiman, Flora D.\n  Salim, Imran Razzak","title":"Long Context Modeling with Ranked Memory-Augmented Retrieval","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Effective long-term memory management is crucial for language models handling\nextended contexts. We introduce a novel framework that dynamically ranks memory\nentries based on relevance. Unlike previous works, our model introduces a novel\nrelevance scoring and a pointwise re-ranking model for key-value embeddings,\ninspired by learning-to-rank techniques in information retrieval. Enhanced\nRanked Memory Augmented Retrieval ERMAR achieves state-of-the-art results on\nstandard benchmarks.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 00:24:01 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Alselwi', 'Ghadir', ''], ['Xue', 'Hao', ''], ['Jameel', 'Shoaib', ''], ['Suleiman', 'Basem', ''], ['Salim', 'Flora D.', ''], ['Razzak', 'Imran', '']]","extracted_entities":"[{'text': 'key-value embeddings', 'label': 'contextual Embedding'}]","assigned_concept":"contextual Embedding","matched_keyword":"key-value embeddings","similarity_score":0.5822768807}
{"id":2503.15283,"submitter":"Teng-Fang Hsiao","authors":"Teng-Fang Hsiao, Bo-Kai Ruan, Yi-Lun Wu, Tzu-Ling Lin, Hong-Han Shuai","title":"TF-TI2I: Training-Free Text-and-Image-to-Image Generation via\n  Multi-Modal Implicit-Context Learning in Text-to-Image Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Text-and-Image-To-Image (TI2I), an extension of Text-To-Image (T2I),\nintegrates image inputs with textual instructions to enhance image generation.\nExisting methods often partially utilize image inputs, focusing on specific\nelements like objects or styles, or they experience a decline in generation\nquality with complex, multi-image instructions. To overcome these challenges,\nwe introduce Training-Free Text-and-Image-to-Image (TF-TI2I), which adapts\ncutting-edge T2I models such as SD3 without the need for additional training.\nOur method capitalizes on the MM-DiT architecture, in which we point out that\ntextual tokens can implicitly learn visual information from vision tokens. We\nenhance this interaction by extracting a condensed visual representation from\nreference images, facilitating selective information sharing through Reference\nContextual Masking -- this technique confines the usage of contextual tokens to\ninstruction-relevant visual information. Additionally, our Winner-Takes-All\nmodule mitigates distribution shifts by prioritizing the most pertinent\nreferences for each vision token. Addressing the gap in TI2I evaluation, we\nalso introduce the FG-TI2I Bench, a comprehensive benchmark tailored for TI2I\nand compatible with existing T2I methods. Our approach shows robust performance\nacross various benchmarks, confirming its effectiveness in handling complex\nimage-generation tasks.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:03:19 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Hsiao', 'Teng-Fang', ''], ['Ruan', 'Bo-Kai', ''], ['Wu', 'Yi-Lun', ''], ['Lin', 'Tzu-Ling', ''], ['Shuai', 'Hong-Han', '']]","extracted_entities":"[{'text': 'Reference\\nContextual Masking', 'label': 'contextual Embedding'}]","assigned_concept":"contextual Embedding","matched_keyword":"Reference\nContextual Masking","similarity_score":0.6027357578}
{"id":2503.15639,"submitter":"Ritabrata Chakraborty","authors":"Ritabrata Chakraborty, Shivakumara Palaiahnakote, Umapada Pal,\n  Cheng-Lin Liu","title":"A Context-Driven Training-Free Network for Lightweight Scene Text\n  Segmentation and Recognition","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Modern scene text recognition systems often depend on large end-to-end\narchitectures that require extensive training and are prohibitively expensive\nfor real-time scenarios. In such cases, the deployment of heavy models becomes\nimpractical due to constraints on memory, computational resources, and latency.\nTo address these challenges, we propose a novel, training-free plug-and-play\nframework that leverages the strengths of pre-trained text recognizers while\nminimizing redundant computations. Our approach uses context-based\nunderstanding and introduces an attention-based segmentation stage, which\nrefines candidate text regions at the pixel level, improving downstream\nrecognition. Instead of performing traditional text detection that follows a\nblock-level comparison between feature map and source image and harnesses\ncontextual information using pretrained captioners, allowing the framework to\ngenerate word predictions directly from scene context.Candidate texts are\nsemantically and lexically evaluated to get a final score. Predictions that\nmeet or exceed a pre-defined confidence threshold bypass the heavier process of\nend-to-end text STR profiling, ensuring faster inference and cutting down on\nunnecessary computations. Experiments on public benchmarks demonstrate that our\nparadigm achieves performance on par with state-of-the-art systems, yet\nrequires substantially fewer resources.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 18:51:01 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chakraborty', 'Ritabrata', ''], ['Palaiahnakote', 'Shivakumara', ''], ['Pal', 'Umapada', ''], ['Liu', 'Cheng-Lin', '']]","extracted_entities":"[{'text': 'context-based\\nunderstanding', 'label': 'contextual Embedding'}, {'text': 'attention-based segmentation stage', 'label': 'contextual Embedding'}]","assigned_concept":"contextual Embedding","matched_keyword":"context-based\nunderstanding","similarity_score":0.5174292326}
{"id":2310.17178,"submitter":"Leonid Ugadiarov","authors":"Leonid Ugadiarov, Vitaliy Vorobyov, Aleksandr I. Panov","title":"Relational Object-Centric Actor-Critic","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.LG cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The advances in unsupervised object-centric representation learning have\nsignificantly improved its application to downstream tasks. Recent works\nhighlight that disentangled object representations can aid policy learning in\nimage-based, object-centric reinforcement learning tasks. This paper proposes a\nnovel object-centric reinforcement learning algorithm that integrates\nactor-critic and model-based approaches by incorporating an object-centric\nworld model within the critic. The world model captures the environment's\ndata-generating process by predicting the next state and reward given the\ncurrent state-action pair, where actions are interventions in the environment.\nIn model-based reinforcement learning, world model learning can be interpreted\nas a causal induction problem, where the agent must learn the causal\nrelationships underlying the environment's dynamics. We evaluate our method in\na simulated 3D robotic environment and a 2D environment with compositional\nstructure. As baselines, we compare against object-centric, model-free\nactor-critic algorithms and a state-of-the-art monolithic model-based\nalgorithm. While the baselines show comparable performance in easier tasks, our\napproach outperforms them in more challenging scenarios with a large number of\nobjects or more complex dynamics.\n","versions":"[{'version': 'v1', 'created': 'Thu, 26 Oct 2023 06:05:12 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 12:30:17 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Ugadiarov', 'Leonid', ''], ['Vorobyov', 'Vitaliy', ''], ['Panov', 'Aleksandr I.', '']]","extracted_entities":"[{'text': 'world model', 'label': 'AI model'}, {'text': 'world model', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"world model","similarity_score":0.5417954326}
{"id":2410.13643,"submitter":"Chenyu Wang","authors":"Chenyu Wang, Masatoshi Uehara, Yichun He, Amy Wang, Tommaso\n  Biancalani, Avantika Lal, Tommi Jaakkola, Sergey Levine, Hanchen Wang, Aviv\n  Regev","title":"Fine-Tuning Discrete Diffusion Models via Reward Optimization with\n  Applications to DNA and Protein Design","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent studies have demonstrated the strong empirical performance of\ndiffusion models on discrete sequences across domains from natural language to\nbiological sequence generation. For example, in the protein inverse folding\ntask, conditional diffusion models have achieved impressive results in\ngenerating natural-like sequences that fold back into the original structure.\nHowever, practical design tasks often require not only modeling a conditional\ndistribution but also optimizing specific task objectives. For instance, we may\nprefer protein sequences with high stability. To address this, we consider the\nscenario where we have pre-trained discrete diffusion models that can generate\nnatural-like sequences, as well as reward models that map sequences to task\nobjectives. We then formulate the reward maximization problem within discrete\ndiffusion models, analogous to reinforcement learning (RL), while minimizing\nthe KL divergence against pretrained diffusion models to preserve naturalness.\nTo solve this RL problem, we propose a novel algorithm, DRAKES, that enables\ndirect backpropagation of rewards through entire trajectories generated by\ndiffusion models, by making the originally non-differentiable trajectories\ndifferentiable using the Gumbel-Softmax trick. Our theoretical analysis\nindicates that our approach can generate sequences that are both natural-like\nand yield high rewards. While similar tasks have been recently explored in\ndiffusion models for continuous domains, our work addresses unique algorithmic\nand theoretical challenges specific to discrete diffusion models, which arise\nfrom their foundation in continuous-time Markov chains rather than Brownian\nmotion. Finally, we demonstrate the effectiveness of DRAKES in generating DNA\nand protein sequences that optimize enhancer activity and protein stability,\nrespectively, important tasks for gene therapies and protein-based\ntherapeutics.\n","versions":"[{'version': 'v1', 'created': 'Thu, 17 Oct 2024 15:10:13 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 16:44:45 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wang', 'Chenyu', ''], ['Uehara', 'Masatoshi', ''], ['He', 'Yichun', ''], ['Wang', 'Amy', ''], ['Biancalani', 'Tommaso', ''], ['Lal', 'Avantika', ''], ['Jaakkola', 'Tommi', ''], ['Levine', 'Sergey', ''], ['Wang', 'Hanchen', ''], ['Regev', 'Aviv', '']]","extracted_entities":"[{'text': 'diffusion models', 'label': 'AI model'}, {'text': 'conditional diffusion models', 'label': 'AI model'}, {'text': 'discrete diffusion models', 'label': 'AI model'}, {'text': 'reward models', 'label': 'AI model'}, {'text': 'discrete\\ndiffusion models', 'label': 'AI model'}, {'text': 'diffusion models', 'label': 'AI model'}, {'text': 'diffusion models', 'label': 'AI model'}, {'text': 'discrete diffusion models', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"reward models","similarity_score":0.5474104881}
{"id":2411.07414,"submitter":"Vibhhu Sharma","authors":"Vibhhu Sharma, Bryan Wilder","title":"Comparing Targeting Strategies for Maximizing Social Welfare with\n  Limited Resources","comments":"Accepted to ICLR 2025 as a Poster","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ML","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Machine learning is increasingly used to select which individuals receive\nlimited-resource interventions in domains such as human services, education,\ndevelopment, and more. However, it is often not apparent what the right\nquantity is for models to predict. Policymakers rarely have access to data from\na randomized controlled trial (RCT) that would enable accurate estimates of\nwhich individuals would benefit more from the intervention, while observational\ndata creates a substantial risk of bias in treatment effect estimates.\nPractitioners instead commonly use a technique termed ``risk-based targeting\"\nwhere the model is just used to predict each individual's status quo outcome\n(an easier, non-causal task). Those with higher predicted risk are offered\ntreatment. There is currently almost no empirical evidence to inform which\nchoices lead to the most effective machine learning-informed targeting\nstrategies in social domains. In this work, we use data from 5 real-world RCTs\nin a variety of domains to empirically assess such choices. We find that when\ntreatment effects can be estimated with high accuracy (which we simulate by\nallowing the model to partially observe outcomes in advance), treatment effect\nbased targeting substantially outperforms risk-based targeting, even when\ntreatment effect estimates are biased. Moreover, these results hold even when\nthe policymaker has strong normative preferences for assisting higher-risk\nindividuals. However, the features and data actually available in most RCTs we\nexamine do not suffice for accurate estimates of heterogeneous treatment\neffects. Our results suggest treatment effect targeting has significant\npotential benefits, but realizing these benefits requires improvements to data\ncollection and model training beyond what is currently common in practice.\n","versions":"[{'version': 'v1', 'created': 'Mon, 11 Nov 2024 22:36:50 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 08:34:26 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Sharma', 'Vibhhu', ''], ['Wilder', 'Bryan', '']]","extracted_entities":"[{'text': 'model', 'label': 'AI model'}, {'text': 'treatment effect\\nbased targeting', 'label': 'Few-shot Learning'}, {'text': 'risk-based targeting', 'label': 'Few-shot Learning'}]","assigned_concept":"AI model","matched_keyword":"model","similarity_score":0.6292717457}
{"id":2503.12956,"submitter":"Animesh Choudhury","authors":"Animesh Choudhury and Jagabandhu Panda","title":"Development of a Data-driven weather forecasting system over India with\n  Pangu-Weather architecture and IMDAA reanalysis Data","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.ao-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Numerical Weather Prediction (NWP) has advanced significantly in recent\ndecades but still faces challenges in accuracy, computational efficiency, and\nscalability. Data-driven weather models have shown great promise, sometimes\nsurpassing operational NWP systems. However, training these models on massive\ndatasets incurs high computational costs. A regional data-driven approach\noffers a cost-effective alternative for localized forecasts. This study\ndevelops a regional weather forecasting model for India by efficiently\nmodifying the Pangu-Weather (PW) architecture. The model is trained using the\nIndian Monsoon Data Assimilation and Analysis (IMDAA) reanalysis dataset with\nlimited computational resources. Prediction results are evaluated using Root\nMean Square Error (RMSE), Anomaly Correlation Coefficient (ACC), Mean Absolute\nPercentage Error (MAPE), and Fractional Skill Score (FSS). At a 6-hour lead\ntime, MAPE remains below 5%, FSS exceeds 0.86, and ACC stays above 0.94,\ndemonstrating robustness. Three forecasting approaches, static, autoregressive,\nand hierarchical, are compared. Errors increase with lead time in all cases.\nThe static approach exhibits periodic fluctuations in error metrics, which are\nabsent in the autoregressive method. The hierarchical approach also shows\nfluctuations, though with reduced intensity after three days. Among these, the\nhierarchical approach performs best while maintaining computational efficiency.\nFurthermore, the model effectively predicts cyclone tracks using the\nhierarchical approach, achieving results comparable to observational and\nreanalysis datasets.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 09:11:44 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Choudhury', 'Animesh', ''], ['Panda', 'Jagabandhu', '']]","extracted_entities":"[{'text': 'model', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"model","similarity_score":0.6292717457}
{"id":2503.13212,"submitter":"Masataka Sawayama","authors":"Mina Kamao, Hayato Ono, Ayumu Yamashita, Kaoru Amano, Masataka\n  Sawayama","title":"MAME: Multidimensional Adaptive Metamer Exploration with Human\n  Perceptual Feedback","comments":"14 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Alignment between human brain networks and artificial models is actively\nstudied in machine learning and neuroscience. A widely adopted approach to\nexplore their functional alignment is to identify metamers for both humans and\nmodels. Metamers refer to input stimuli that are physically different but\nequivalent within a given system. If a model's metameric space completely\nmatched the human metameric space, the model would achieve functional alignment\nwith humans. However, conventional methods lack direct ways to search for human\nmetamers. Instead, researchers first develop biologically inspired models and\nthen infer about human metamers indirectly by testing whether model metamers\nalso appear as metamers to humans. Here, we propose the Multidimensional\nAdaptive Metamer Exploration (MAME) framework, enabling direct high-dimensional\nexploration of human metameric space. MAME leverages online image generation\nguided by human perceptual feedback. Specifically, it modulates reference\nimages across multiple dimensions by leveraging hierarchical responses from\nconvolutional neural networks (CNNs). Generated images are presented to\nparticipants whose perceptual discriminability is assessed in a behavioral\ntask. Based on participants' responses, subsequent image generation parameters\nare adaptively updated online. Using our MAME framework, we successfully\nmeasured a human metameric space of over fifty dimensions within a single\nexperiment. Experimental results showed that human discrimination sensitivity\nwas lower for metameric images based on low-level features compared to\nhigh-level features, which image contrast metrics could not explain. The\nfinding suggests that the model computes low-level information not essential\nfor human perception. Our framework has the potential to contribute to\ndeveloping interpretable AI and understanding of brain function in\nneuroscience.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 14:23:04 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Kamao', 'Mina', ''], ['Ono', 'Hayato', ''], ['Yamashita', 'Ayumu', ''], ['Amano', 'Kaoru', ''], ['Sawayama', 'Masataka', '']]","extracted_entities":"[{'text': 'model', 'label': 'AI model'}, {'text': 'model', 'label': 'AI model'}, {'text': 'model', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"model","similarity_score":0.6292717457}
{"id":2503.13784,"submitter":"Lin Geng","authors":"Lin Geng, Hao Li, Sidney Givigi, Bram Adams","title":"SwarmUpdate: Hierarchical Software Updates and Deep Learning Model\n  Patching for Heterogeneous UAV Swarms","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Heterogeneous unmanned aerial vehicle (UAV) swarms consist of dozens to\nhundreds of drones with different roles and varying hardware and software\nrequirements collaborating towards a shared mission. While traditional\napproaches for synchronized software updates assume swarms to be unstructured\nand homogeneous, the heterogeneous nature of modern swarms and the emerging\nneed of drones to update their deep learning (perception) models with new\nobjectives or data as a mission unfolds, has made efficient software update\nmethods crucial for swarms to adapt to dynamic environments. To address these\nchallenges, we introduce the SwarmUpdate framework for software updates in\nheterogeneous UAV swarms, composed of two key components: SwarmSync and\nSwarmModelPatch. SwarmSync is a hierarchical software update synchronization\nstrategy to distribute a software update to the right subset of drones within a\nswarm, while SwarmModelPatch is a deep learning model patching method that\nreduces the size of a (deep learning model) update by only allowing some layers\nof the model to be updated (freezing the other layers). In this paper, we\nsystematically evaluate the performance of SwarmSync through large-scale\nsimulations in the ARGoS swarm simulator, comparing SwarmSync to auction-based\n(SOUL) and gossip-based rebroadcasting (Gossip) baselines, and SwarmModelPatch\nto a non-incremental model patching strategy.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 00:23:21 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Geng', 'Lin', ''], ['Li', 'Hao', ''], ['Givigi', 'Sidney', ''], ['Adams', 'Bram', '']]","extracted_entities":"[{'text': 'deep learning model', 'label': 'AI model'}, {'text': 'deep learning model', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"deep learning model","similarity_score":0.6012713909}
{"id":2503.14327,"submitter":"Idris Bello","authors":"Idris Temitope Bello, Hassan Raza, Madithedu Muneeswara, Neha Tewari,\n  Yin Nee Cheung, Tobi Alabi Michael, Ridwan Taiwo, Fiske Lin","title":"Multi-Parameter Analysis of Li-ion Battery Degradation: Integrating\n  Optical Fiber Sensing with Differential State of Health Metrics","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.app-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The reliability and safety of Lithium-ion batteries (LiBs) are of great\nconcern in the energy storage industry. Nevertheless, the real-time monitoring\nof their degradation remains challenging due to limited quantitative metrics\navailable during cycling. This study addresses this limitation by employing a\nnovel approach that combines external optical fiber sensing with advanced data\nanalysis techniques to comprehensively assess battery health. We engineered a\nnon-invasive optical sensing platform using tandem pairs of polymeric and\nsilica-based fiber Bragg grating (FBG) sensors affixed to the external surface\nof commercial Li-ion button cells, enabling simultaneous, real-time monitoring\nof device-level volume changes and thermal events over 600 cycles. Our analysis\nincorporated differential techniques to estimate the battery's state of health\n(SOH) based on capacity, strain, and temperature variations with respect to\nvoltage. Additionally, we implemented and compared three deep learning models -\nLong Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and Artificial\nNeural Network (ANN) - to predict battery SOH over cycles. We were able to\ncapture both continuous and spontaneous degradation events and provide unique\ninsights into battery behavior across its lifecycle through differential\nanalysis and new SOH metrics demonstrating high correlation with conventional\nmeasures. This multi-parameter approach, combining advanced sensing techniques\nwith innovative data analysis and deep learning methods, contributes\nsignificantly to battery diagnostics, potentially improving reliability\nassessment, enhancing safety standards, and accelerating the development of\nmore sustainable energy storage solutions.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 15:01:30 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Bello', 'Idris Temitope', ''], ['Raza', 'Hassan', ''], ['Muneeswara', 'Madithedu', ''], ['Tewari', 'Neha', ''], ['Cheung', 'Yin Nee', ''], ['Michael', 'Tobi Alabi', ''], ['Taiwo', 'Ridwan', ''], ['Lin', 'Fiske', '']]","extracted_entities":"[{'text': 'Artificial\\nNeural Network (ANN)', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"Artificial\nNeural Network (ANN)","similarity_score":0.549598515}
{"id":2503.14499,"submitter":"Thomas Kwa","authors":"Thomas Kwa, Ben West, Joel Becker, Amy Deng, Katharyn Garcia, Max\n  Hasin, Sami Jawhar, Megan Kinniment, Nate Rush, Sydney Von Arx, Ryan Bloom,\n  Thomas Broadley, Haoxing Du, Brian Goodrich, Nikola Jurkovic, Luke Harold\n  Miles, Seraphina Nix, Tao Lin, Neev Parikh, David Rein, Lucas Jun Koba Sato,\n  Hjalmar Wijk, Daniel M. Ziegler, Elizabeth Barnes, Lawrence Chan","title":"Measuring AI Ability to Complete Long Tasks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Despite rapid progress on AI benchmarks, the real-world meaning of benchmark\nperformance remains unclear. To quantify the capabilities of AI systems in\nterms of human capabilities, we propose a new metric: 50%-task-completion time\nhorizon. This is the time humans typically take to complete tasks that AI\nmodels can complete with 50% success rate. We first timed humans with relevant\ndomain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter\ntasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet\nhave a 50% time horizon of around 50 minutes. Furthermore, frontier AI time\nhorizon has been doubling approximately every seven months since 2019, though\nthe trend may have accelerated in 2024. The increase in AI models' time\nhorizons seems to be primarily driven by greater reliability and ability to\nadapt to mistakes, combined with better logical reasoning and tool use\ncapabilities. We discuss the limitations of our results -- including their\ndegree of external validity -- and the implications of increased autonomy for\ndangerous capabilities. If these results generalize to real-world software\ntasks, extrapolation of this trend predicts that within 5 years, AI systems\nwill be capable of automating many software tasks that currently take humans a\nmonth.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:59:31 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Kwa', 'Thomas', ''], ['West', 'Ben', ''], ['Becker', 'Joel', ''], ['Deng', 'Amy', ''], ['Garcia', 'Katharyn', ''], ['Hasin', 'Max', ''], ['Jawhar', 'Sami', ''], ['Kinniment', 'Megan', ''], ['Rush', 'Nate', ''], ['Von Arx', 'Sydney', ''], ['Bloom', 'Ryan', ''], ['Broadley', 'Thomas', ''], ['Du', 'Haoxing', ''], ['Goodrich', 'Brian', ''], ['Jurkovic', 'Nikola', ''], ['Miles', 'Luke Harold', ''], ['Nix', 'Seraphina', ''], ['Lin', 'Tao', ''], ['Parikh', 'Neev', ''], ['Rein', 'David', ''], ['Sato', 'Lucas Jun Koba', ''], ['Wijk', 'Hjalmar', ''], ['Ziegler', 'Daniel M.', ''], ['Barnes', 'Elizabeth', ''], ['Chan', 'Lawrence', '']]","extracted_entities":"[{'text': 'AI models', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"AI models","similarity_score":0.9476590753}
{"id":2503.14556,"submitter":"Md Rokibul Hasan","authors":"Reza E Rabbi Shawon, MD Rokibul Hasan, Md Anisur Rahman, Mohamed\n  Ghandri, Iman Ahmed Lamari, Mohammed Kawsar, Rubi Akter","title":"Designing and Deploying AI Models for Sustainable Logistics\n  Optimization: A Case Study on Eco-Efficient Supply Chains in the USA","comments":null,"journal-ref":null,"doi":"10.62754\/joe.v4i2.6610","report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The rapid evolution of Artificial Intelligence (AI) and Machine Learning (ML)\nhas significantly transformed logistics and supply chain management,\nparticularly in the pursuit of sustainability and eco-efficiency. This study\nexplores AI-based methodologies for optimizing logistics operations in the USA,\nfocusing on reducing environmental impact, improving fuel efficiency, and\nminimizing costs. Key AI applications include predictive analytics for demand\nforecasting, route optimization through machine learning, and AI-powered fuel\nefficiency strategies. Various models, such as Linear Regression, XGBoost,\nSupport Vector Machine, and Neural Networks, are applied to real-world\nlogistics datasets to reduce carbon emissions based on logistics operations,\noptimize travel routes to minimize distance and travel time, and predict future\ndeliveries to plan optimal routes. Other models such as K-Means and DBSCAN are\nalso used to optimize travel routes to minimize distance and travel time for\nlogistics operations. This study utilizes datasets from logistics companies'\ndatabases. The study also assesses model performance using metrics such as mean\nabsolute error (MAE), mean squared error (MSE), and R2 score. This study also\nexplores how these models can be deployed to various platforms for real-time\nlogistics and supply chain use. The models are also examined through a thorough\ncase study, highlighting best practices and regulatory frameworks that promote\nsustainability. The findings demonstrate AI's potential to enhance logistics\nefficiency, reduce carbon footprints, and contribute to a more resilient and\nadaptive supply chain ecosystem.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 00:46:35 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Shawon', 'Reza E Rabbi', ''], ['Hasan', 'MD Rokibul', ''], ['Rahman', 'Md Anisur', ''], ['Ghandri', 'Mohamed', ''], ['Lamari', 'Iman Ahmed', ''], ['Kawsar', 'Mohammed', ''], ['Akter', 'Rubi', '']]","extracted_entities":"[{'text': 'Linear Regression', 'label': 'AI model'}, {'text': 'XGBoost', 'label': 'AI model'}, {'text': 'Support Vector Machine', 'label': 'AI model'}, {'text': 'Neural Networks', 'label': 'AI model'}, {'text': 'K-Means', 'label': 'AI model'}, {'text': 'DBSCAN', 'label': 'AI model'}, {'text': 'best practices', 'label': 'AI Ethics'}, {'text': 'regulatory frameworks', 'label': 'AI Ethics'}]","assigned_concept":"AI model","matched_keyword":"Neural Networks","similarity_score":0.5762337446}
{"id":2503.14873,"submitter":"Mojtaba Mohasel","authors":"Seyed Mojtaba Mohasel, Hamidreza Koosha","title":"Robust Support Vector Machines for Imbalanced and Noisy Data via Benders\n  Decomposition","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This study introduces a novel formulation to enhance Support Vector Machines\n(SVMs) in handling class imbalance and noise. Unlike the conventional Soft\nMargin SVM, which penalizes the magnitude of constraint violations, the\nproposed model quantifies the number of violations and aims to minimize their\nfrequency. To achieve this, a binary variable is incorporated into the\nobjective function of the primal SVM formulation, replacing the traditional\nslack variable. Furthermore, each misclassified sample is assigned a priority\nand an associated constraint. The resulting formulation is a mixed-integer\nprogramming model, efficiently solved using Benders decomposition. The proposed\nmodel's performance was benchmarked against existing models, including Soft\nMargin SVM, weighted SVM, and NuSVC. Two primary hypotheses were examined: 1)\nThe proposed model improves the F1-score for the minority class in imbalanced\nclassification tasks. 2) The proposed model enhances classification accuracy in\nnoisy datasets. These hypotheses were evaluated using a Wilcoxon test across\nmultiple publicly available datasets from the OpenML repository. The results\nsupported both hypotheses (\\( p < 0.05 \\)). In addition, the proposed model\nexhibited several interesting properties, such as improved robustness to noise,\na decision boundary shift favoring the minority class, a reduced number of\nsupport vectors, and decreased prediction time. The open-source Python\nimplementation of the proposed SVM model is available.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 04:03:39 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Mohasel', 'Seyed Mojtaba', ''], ['Koosha', 'Hamidreza', '']]","extracted_entities":"[{'text': 'Soft\\nMargin SVM', 'label': 'AI model'}, {'text': 'mixed-integer\\nprogramming model', 'label': 'AI model'}, {'text': 'proposed\\nmodel', 'label': 'AI model'}, {'text': 'Soft\\nMargin SVM', 'label': 'AI model'}, {'text': 'weighted SVM', 'label': 'AI model'}, {'text': 'NuSVC', 'label': 'AI model'}, {'text': 'proposed model', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"proposed\nmodel","similarity_score":0.5694795847}
{"id":2503.15245,"submitter":"Paola Ragonese","authors":"Paola Ragonese, Chiara Maurizio, Boris Kalinic, Thomas Kirchartz,\n  Sandheep Ravishankar","title":"Determination of Electron Extraction in Semiconductor Photoanodes:\n  Steady-state and Small-perturbation Response","comments":"26 pages, 12 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci physics.app-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This work develops an analytical model to consistently interpret the\nsteady-state and small-perturbation response (both in the time and frequency\ndomain) of photoanodes for solar water-splitting. In addition to accounting for\nthe fundamental mechanisms of charge-carrier generation, recombination and slow\nhole transfer at the photoanode\/electrolyte interface, the model overcomes the\nkey shortcomings of existing models in the literature. These include\nconsistency across measurements\/bias conditions and the non-consideration of\nimperfect electron extraction at the collecting contact and its corresponding\neffect on the recombination rate in the bulk. We applied the model to analyse\nthe time constants obtained from intensity-modulated photocurrent (IMPS) and\nphotovoltage (IMVS) measurements of a hematite photoanode, obtaining an\nelectron extraction velocity of 100 cm\/s close to the 1 sun open-circuit\npotential, that corresponds to an electron mobility of 0.022 cm2V-1s-1. The\nmodel further predicts a linear dependence of the photocurrent versus anodic\nvoltage, an observation whose origin has been strongly debated in the\nliterature in the case of hematite photoanodes. The generality of the model\nallows its extension to other photoanodes and photovoltaic systems, by the\naddition or removal of specific physical mechanisms.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 14:22:09 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Ragonese', 'Paola', ''], ['Maurizio', 'Chiara', ''], ['Kalinic', 'Boris', ''], ['Kirchartz', 'Thomas', ''], ['Ravishankar', 'Sandheep', '']]","extracted_entities":"[{'text': 'model', 'label': 'AI model'}, {'text': 'model', 'label': 'AI model'}, {'text': 'model', 'label': 'AI model'}, {'text': 'model', 'label': 'AI model'}, {'text': 'model', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"model","similarity_score":0.6292717457}
{"id":2503.15339,"submitter":"Peter R. Wiecha","authors":"Abdourahman Khaireh-Walieh, Alexandre Arnoult, S\\'ebastien Plissard,\n  Peter R. Wiecha","title":"Data-driven Azimuthal RHEED construction for in-situ crystal growth\n  characterization","comments":"7 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mes-hall cond-mat.mtrl-sci","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Reflection High-Energy Electron Diffraction (RHEED) is a powerful tool to\nprobe the surface reconstruction during MBE growth. However, raw RHEED patterns\nare difficult to interpret, especially when the wafer is rotating. A more\naccessible representation of the information is therefore the so-called\nAzimuthal RHEED (ARHEED), an angularly resolved plot of the electron\ndiffraction pattern during a full wafer rotation. However, ARHEED requires\nprecise information about the rotation angle as well as of the position of the\nspecular spot of the electron beam. We present a Deep Learning technique to\nautomatically construct the Azimuthal RHEED from bare RHEED images, requiring\nno further measurement equipment. We use two artificial neural networks: an\nimage segmentation model to track the center of the specular spot and a\nregression model to determine the orientation of the crystal with respect to\nthe incident electron beam of the RHEED system. Our technique enables accurate\nand real-time ARHEED construction on any growth chamber equipped with a RHEED\nsystem.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:35:29 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Khaireh-Walieh', 'Abdourahman', ''], ['Arnoult', 'Alexandre', ''], ['Plissard', 'S\u00e9bastien', ''], ['Wiecha', 'Peter R.', '']]","extracted_entities":"[{'text': 'image segmentation model', 'label': 'AI model'}, {'text': 'regression model', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"regression model","similarity_score":0.5078941584}
{"id":2503.15477,"submitter":"Noam Razin","authors":"Noam Razin, Zixuan Wang, Hubert Strauss, Stanley Wei, Jason D. Lee,\n  Sanjeev Arora","title":"What Makes a Reward Model a Good Teacher? An Optimization Perspective","comments":"Code available at https:\/\/github.com\/princeton-pli\/what-makes-good-rm","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL stat.ML","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The success of Reinforcement Learning from Human Feedback (RLHF) critically\ndepends on the quality of the reward model. While this quality is primarily\nevaluated through accuracy, it remains unclear whether accuracy fully captures\nwhat makes a reward model an effective teacher. We address this question from\nan optimization perspective. First, we prove that regardless of how accurate a\nreward model is, if it induces low reward variance, then the RLHF objective\nsuffers from a flat landscape. Consequently, even a perfectly accurate reward\nmodel can lead to extremely slow optimization, underperforming less accurate\nmodels that induce higher reward variance. We additionally show that a reward\nmodel that works well for one language model can induce low reward variance,\nand thus a flat objective landscape, for another. These results establish a\nfundamental limitation of evaluating reward models solely based on accuracy or\nindependently of the language model they guide. Experiments using models of up\nto 8B parameters corroborate our theory, demonstrating the interplay between\nreward variance, accuracy, and reward maximization rate. Overall, our findings\nhighlight that beyond accuracy, a reward model needs to induce sufficient\nvariance for efficient optimization.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:54:41 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Razin', 'Noam', ''], ['Wang', 'Zixuan', ''], ['Strauss', 'Hubert', ''], ['Wei', 'Stanley', ''], ['Lee', 'Jason D.', ''], ['Arora', 'Sanjeev', '']]","extracted_entities":"[{'text': 'Reinforcement Learning from Human Feedback', 'label': 'Few-shot Learning'}, {'text': 'reward model', 'label': 'AI model'}, {'text': 'reward\\nmodel', 'label': 'AI model'}, {'text': 'reward model', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"reward model","similarity_score":0.5482002497}
{"id":2503.15691,"submitter":"Jingyuan Chen Ph.D.","authors":"Jingyuan Chen, Yunze Yang, Chenbin Liu, Hongying Feng, Jason M.\n  Holmes, Lian Zhang, Steven J. Frank, Charles B. Simone II, Daniel J. Ma,\n  Samir H. Patel, Wei Liu","title":"Critical review of patient outcome study in head and neck cancer\n  radiotherapy","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.med-ph","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Rapid technological advances in radiation therapy have significantly improved\ndose delivery and tumor control for head and neck cancers. However,\ntreatment-related toxicities caused by high-dose exposure to critical\nstructures remain a significant clinical challenge, underscoring the need for\naccurate prediction of clinical outcomes-encompassing both tumor control and\nadverse events (AEs). This review critically evaluates the evolution of\ndata-driven approaches in predicting patient outcomes in head and neck cancer\npatients treated with radiation therapy, from traditional dose-volume\nconstraints to cutting-edge artificial intelligence (AI) and causal inference\nframework. The integration of linear energy transfer in patient outcomes study,\nwhich has uncovered critical mechanisms behind unexpected toxicity, was also\nintroduced for proton therapy. Three transformative methodological advances are\nreviewed: radiomics, AI-based algorithms, and causal inference frameworks.\nWhile radiomics has enabled quantitative characterization of medical images, AI\nmodels have demonstrated superior capability than traditional models. However,\nthe field faces significant challenges in translating statistical correlations\nfrom real-world data into interventional clinical insights. We highlight that\nhow causal inference methods can bridge this gap by providing a rigorous\nframework for identifying treatment effects. Looking ahead, we envision that\ncombining these complementary approaches, especially the interventional\nprediction models, will enable more personalized treatment strategies,\nultimately improving both tumor control and quality of life for head and neck\ncancer patients treated with radiation therapy.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 21:01:33 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chen', 'Jingyuan', ''], ['Yang', 'Yunze', ''], ['Liu', 'Chenbin', ''], ['Feng', 'Hongying', ''], ['Holmes', 'Jason M.', ''], ['Zhang', 'Lian', ''], ['Frank', 'Steven J.', ''], ['Simone', 'Charles B.', 'II'], ['Ma', 'Daniel J.', ''], ['Patel', 'Samir H.', ''], ['Liu', 'Wei', '']]","extracted_entities":"[{'text': 'AI\\nmodels', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"AI\nmodels","similarity_score":0.9476590753}
{"id":2503.16061,"submitter":"Hossein Safi","authors":"Asim Ihsan, Muhammad Asif, Hossein Safi, Iman Tavakkolnia, and Harald\n  Haas","title":"Efficient Service Differentiation and Energy Management in Hybrid\n  WiFi\/LiFi Networks","comments":"28 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this paper, we propose an innovative predict-and-optimize algorithm\ndesigned for hybrid WiFi\/LiFi networks, aiming to achieve service\ndifferentiation while maximizing energy efficiency (EE). The proposed framework\nutilizes multi-access technology real-time intelligent controller (mATRIC) to\ndynamically predict the appropriate network slice for each user based on\nhistorically monitored key performance indicators (KPIs). This prediction is\nfacilitated by a deep learning model trained using the resilient\nbackpropagation algorithm, with training conducted on KPIs data at the\nuniversal non-real time RAN intelligent controller (non-RT RIC). This trained\nmodel enables real-time slice selection by mATRIC. In the subsequent phase, the\nalgorithm focuses on optimizing EE of hybrid network as a function of precoding\nvectors for the predicted slices by employing techniques from sequential convex\napproximation and the inner approximation method. We introduce novel\napproximations to convert non-convex objective functions and constraints into\nconvex forms, and develop an iterative algorithm to achieve sub-optimal\nsolutions. Additionally, the EE maximization problem, ensures alignment with\nend-to-end latency requirements. It also addresses the various constraints\ninherent to hybrid systems, such as input signal limitations for LiFi LEDs,\ndata rate restrictions, and power budget considerations. Simulation results\nvalidate the effectiveness of the proposed algorithm, demonstrating significant\nimprovements in EE while ensuring service differentiation within hybrid network\nenvironments.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 11:47:29 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Ihsan', 'Asim', ''], ['Asif', 'Muhammad', ''], ['Safi', 'Hossein', ''], ['Tavakkolnia', 'Iman', ''], ['Haas', 'Harald', '']]","extracted_entities":"[{'text': 'deep learning model', 'label': 'AI model'}]","assigned_concept":"AI model","matched_keyword":"deep learning model","similarity_score":0.6012713909}
{"id":2310.14086,"submitter":"Joseph Schindler","authors":"Adam Teixid\\'o-Bonfill, Joseph Schindler, Dominik \\v{S}afr\\'anek","title":"Entropic partial orderings of quantum measurements","comments":"15 pages, 1 figure. v3, published version","journal-ref":null,"doi":"10.1088\/1402-4896\/ad977c","report-no":null,"categories":"quant-ph math-ph math.MP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We investigate four partial orderings on the space of quantum measurements\n(i.e on POVMs or positive operator valued measures), describing four notions of\ncoarse\/fine-ness of measurement. These are the partial orderings induced by:\n(1) classical post-processing, (2) measured relative entropy, (3) observational\nentropy, and (4) linear relation of POVMs. The orderings form a hierarchy of\nimplication, where e.g. post-processing relation implies all the others. We\nshow that this hierarchy is strict for general POVMs, with examples showing\nthat all four orderings are strictly inequivalent. Restricted to projective\nmeasurements, all are equivalent. Finally we show that observational entropy\nequality $S_M = S_N$ (for all $\\rho$) holds if and only if $M \\equiv N$ are\npost-processing equivalent, which shows that the first three orderings induce\nidentical equivalence classes.\n","versions":"[{'version': 'v1', 'created': 'Sat, 21 Oct 2023 18:44:31 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Feb 2024 12:58:44 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 18:41:05 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Teixid\u00f3-Bonfill', 'Adam', ''], ['Schindler', 'Joseph', ''], ['\u0160afr\u00e1nek', 'Dominik', '']]","extracted_entities":"[{'text': 'POVMs', 'label': 'LLMs'}, {'text': 'measured relative entropy', 'label': 'quantisation'}, {'text': 'observational\\nentropy', 'label': 'quantisation'}, {'text': 'POVMs', 'label': 'LLMs'}, {'text': 'POVMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"POVMs","similarity_score":0.5045669079}
{"id":2311.10497,"submitter":"Junhui Liao","authors":"Fengbo Gu, Junhui Liao, Jiangfeng Zhou, Meiyuenan Ma, Yuanning Gao,\n  Zhaohua Peng, Jian Zheng, Guangpeng An, Lifeng Zhang, Lei Zhang, Zhuo Liang,\n  Xiuliang Zhao, Fabio Acerbi, Andrea Ficorella, Alberto Gola, Laura Parellada\n  Monreal","title":"Characterization of FBK NUV-HD-Cryo SiPMs near LHe temperature","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.ins-det hep-ex","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Five FBK ``NUV-HD-Cryo'' SiPMs have been characterized at 7 K and 10 K, with\n405 nm and 530 nm LED light, respectively. The dark current rate (DCR) was\nmeasured to be $\\sim$ 1 Hz for the $\\sim$ 100 mm$^2$-size SiPMs, or 0.01\nHz\/mm$^2$, which is $\\sim$ 7 orders lower than the DCR at room temperature\n(RT). Given the tiny DCR at these cryogenic temperatures, we measured the\nSiPMs' I-V curves with such a method: illuminated the SiPMs with weak light,\nwhich differs from the conventional measurements at RT. Then, we measured the\nphoto-detection efficiency (PDE), after-pulse (AP), and cross-talk (CT) with a\nbias voltage ranging from 6 to 11 V overvoltage (OV). At the OV interval (6 to\n11 V), the PDE was between 20\\% - 45\\%, and the AP and CT were both between\n$\\sim$ 5\\% and $\\sim$ 20\\%. Suppose the bias is $\\ge$ 10 V OV, the PDE would be\n$\\ge$ 40\\%, and the AP and CT are $\\sim$ 20\\%. Combining all of the\nmeasurements, we are confident that the SiPMs can be equipped as the\nphotosensors on liquid helium detectors, including but not limited to the time\nprojection chambers, which we have proposed in hunting for low-mass dark matter\ndirectly and beyond.\n","versions":"[{'version': 'v1', 'created': 'Fri, 17 Nov 2023 12:52:38 GMT'}, {'version': 'v2', 'created': 'Fri, 29 Dec 2023 15:35:47 GMT'}, {'version': 'v3', 'created': 'Tue, 22 Oct 2024 08:35:51 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 02:35:47 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Gu', 'Fengbo', ''], ['Liao', 'Junhui', ''], ['Zhou', 'Jiangfeng', ''], ['Ma', 'Meiyuenan', ''], ['Gao', 'Yuanning', ''], ['Peng', 'Zhaohua', ''], ['Zheng', 'Jian', ''], ['An', 'Guangpeng', ''], ['Zhang', 'Lifeng', ''], ['Zhang', 'Lei', ''], ['Liang', 'Zhuo', ''], ['Zhao', 'Xiuliang', ''], ['Acerbi', 'Fabio', ''], ['Ficorella', 'Andrea', ''], ['Gola', 'Alberto', ''], ['Monreal', 'Laura Parellada', '']]","extracted_entities":"[{'text': 'SiPMs', 'label': 'LLMs'}, {'text': 'SiPMs', 'label': 'LLMs'}, {'text': 'SiPMs', 'label': 'LLMs'}, {'text': 'SiPMs', 'label': 'LLMs'}, {'text': 'PDE', 'label': 'BERT'}, {'text': 'PDE', 'label': 'BERT'}, {'text': 'PDE', 'label': 'BERT'}, {'text': 'SiPMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"SiPMs","similarity_score":0.5088107586}
{"id":2406.13356,"submitter":"Shengyuan Hu","authors":"Shengyuan Hu, Yiwei Fu, Zhiwei Steven Wu, Virginia Smith","title":"Unlearning or Obfuscating? Jogging the Memory of Unlearned LLMs via\n  Benign Relearning","comments":"ICLR 2025, 32 pages, 8 figures, 9 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Machine unlearning is a promising approach to mitigate undesirable\nmemorization of training data in ML models. However, in this work we show that\nexisting approaches for unlearning in LLMs are surprisingly susceptible to a\nsimple set of $\\textit{benign relearning attacks}$. With access to only a small\nand potentially loosely related set of data, we find that we can ''jog'' the\nmemory of unlearned models to reverse the effects of unlearning. For example,\nwe show that relearning on public medical articles can lead an unlearned LLM to\noutput harmful knowledge about bioweapons, and relearning general wiki\ninformation about the book series Harry Potter can force the model to output\nverbatim memorized text. We formalize this unlearning-relearning pipeline,\nexplore the attack across three popular unlearning benchmarks, and discuss\nfuture directions and guidelines that result from our study. Our work indicates\nthat current approximate unlearning methods simply suppress the model outputs\nand fail to robustly forget target knowledge in the LLMs.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Jun 2024 09:03:21 GMT'}, {'version': 'v2', 'created': 'Mon, 7 Oct 2024 17:27:30 GMT'}, {'version': 'v3', 'created': 'Tue, 8 Oct 2024 08:35:13 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 07:46:49 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Hu', 'Shengyuan', ''], ['Fu', 'Yiwei', ''], ['Wu', 'Zhiwei Steven', ''], ['Smith', 'Virginia', '']]","extracted_entities":"[{'text': 'Machine unlearning', 'label': 'Few-shot Learning'}, {'text': 'LLMs', 'label': 'LLMs'}, {'text': 'LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2406.17094,"submitter":"Ghada Almashaqbeh","authors":"Nicolas Michel and Mohamed E. Najd and Ghada Almashaqbeh","title":"ammBoost: State Growth Control for AMMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Automated market makers (AMMs) are a prime example of Web 3.0 applications.\nTheir popularity and high trading activity led to serious scalability issues in\nterms of throughput and state size. In this paper, we address these challenges\nby utilizing a new sidechain architecture, building a system called ammBoost.\nammBoost reduces the amount of on-chain transactions, boosts throughput, and\nsupports blockchain pruning. We devise several techniques to enable layer 2\nprocessing for AMMs, including a functionality-split and layer 2 traffic\nsummarization paradigm, an epoch-based deposit mechanism, and pool\nsnapshot-based and delayed token-payout trading. We also build a\nproof-of-concept for a Uniswap-inspired use case to empirically evaluate\nperformance. Our experiments show that ammBoost decreases the gas cost by\n96.05% and the chain growth by at least 93.42%, and that it can support up to\n500x of the daily traffic volume of Uniswap. We also compare ammBoost to an\nOptimism-inspired solution showing a 99.94% reduction in transaction finality.\n","versions":"[{'version': 'v1', 'created': 'Mon, 24 Jun 2024 19:34:05 GMT'}, {'version': 'v2', 'created': 'Mon, 1 Jul 2024 14:10:56 GMT'}, {'version': 'v3', 'created': 'Sat, 28 Sep 2024 19:39:19 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 16:55:47 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Michel', 'Nicolas', ''], ['Najd', 'Mohamed E.', ''], ['Almashaqbeh', 'Ghada', '']]","extracted_entities":"[{'text': 'Automated market makers', 'label': 'LLMs'}, {'text': 'AMMs', 'label': 'LLMs'}, {'text': 'AMMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"AMMs","similarity_score":0.546841085}
{"id":2409.00101,"submitter":"Wei-Bang Jiang","authors":"Wei-Bang Jiang, Yansen Wang, Bao-Liang Lu, Dongsheng Li","title":"NeuroLM: A Universal Multi-task Foundation Model for Bridging the Gap\n  between Language and EEG Signals","comments":"The Thirteenth International Conference on Learning Representations","journal-ref":"The Thirteenth International Conference on Learning\n  Representations, 2025","doi":null,"report-no":null,"categories":"eess.SP cs.HC cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advancements for large-scale pre-training with neural signals such as\nelectroencephalogram (EEG) have shown promising results, significantly boosting\nthe development of brain-computer interfaces (BCIs) and healthcare. However,\nthese pre-trained models often require full fine-tuning on each downstream task\nto achieve substantial improvements, limiting their versatility and usability,\nand leading to considerable resource wastage. To tackle these challenges, we\npropose NeuroLM, the first multi-task foundation model that leverages the\ncapabilities of Large Language Models (LLMs) by regarding EEG signals as a\nforeign language, endowing the model with multi-task learning and inference\ncapabilities. Our approach begins with learning a text-aligned neural tokenizer\nthrough vector-quantized temporal-frequency prediction, which encodes EEG\nsignals into discrete neural tokens. These EEG tokens, generated by the frozen\nvector-quantized (VQ) encoder, are then fed into an LLM that learns causal EEG\ninformation via multi-channel autoregression. Consequently, NeuroLM can\nunderstand both EEG and language modalities. Finally, multi-task instruction\ntuning adapts NeuroLM to various downstream tasks. We are the first to\ndemonstrate that, by specific incorporation with LLMs, NeuroLM unifies diverse\nEEG tasks within a single model through instruction tuning. The largest variant\nNeuroLM-XL has record-breaking 1.7B parameters for EEG signal processing, and\nis pre-trained on a large-scale corpus comprising approximately 25,000-hour EEG\ndata. When evaluated on six diverse downstream datasets, NeuroLM showcases the\nhuge potential of this multi-task learning paradigm.\n","versions":"[{'version': 'v1', 'created': 'Tue, 27 Aug 2024 12:07:09 GMT'}, {'version': 'v2', 'created': 'Sun, 2 Feb 2025 08:36:36 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 08:26:21 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Jiang', 'Wei-Bang', ''], ['Wang', 'Yansen', ''], ['Lu', 'Bao-Liang', ''], ['Li', 'Dongsheng', '']]","extracted_entities":"[{'text': 'NeuroLM', 'label': 'Foundation Model'}, {'text': 'vector-quantized temporal-frequency prediction', 'label': 'quantisation'}, {'text': 'NeuroLM', 'label': 'Foundation Model'}, {'text': 'multi-task instruction\\ntuning', 'label': 'Fine-tuning'}, {'text': 'NeuroLM', 'label': 'Foundation Model'}, {'text': 'LLMs', 'label': 'LLMs'}, {'text': 'NeuroLM', 'label': 'Foundation Model'}, {'text': 'instruction tuning', 'label': 'Fine-tuning'}, {'text': 'NeuroLM-XL', 'label': 'Foundation Model'}, {'text': 'NeuroLM', 'label': 'Foundation Model'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2410.12972,"submitter":"Rudra Murthy V","authors":"Rudra Murthy, Praveen Venkateswaran, Prince Kumar, Danish Contractor","title":"Evaluating the Instruction-following Abilities of Language Models using\n  Knowledge Tasks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  LLM evaluation benchmarks have traditionally separated the testing of\nknowledge\/reasoning capabilities from instruction following. In this work, we\nstudy the interaction between knowledge and instruction following, and observe\nthat LLMs struggle to follow simple answer modifying instructions, and are also\ndistracted by instructions that should have no bearing on the original\nknowledge task answer. We leverage existing multiple-choice answer based\nknowledge benchmarks and apply a set of simple instructions which include\nmanipulating text (eg.: change case), numeric quantities (eg.: increase value,\nchange formatting), operate on lists (eg.: sort answer candidates) and\ndistractor instructions (eg.: change case of numeric answers).\n","versions":"[{'version': 'v1', 'created': 'Wed, 16 Oct 2024 19:07:37 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 10:45:15 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Murthy', 'Rudra', ''], ['Venkateswaran', 'Praveen', ''], ['Kumar', 'Prince', ''], ['Contractor', 'Danish', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'LLMs'}, {'text': 'numeric quantities', 'label': 'quantisation'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2411.09699,"submitter":"Chandan Setty","authors":"Shouvik Sur and Chandan Setty","title":"Cubic Dirac Semimetals: General Theory and Application to Rare-Earth\n  Magnets","comments":"13 pages, 7 figures","journal-ref":"Phys. Rev. Research 7, 013280 (2025)","doi":"10.1103\/PhysRevResearch.7.013280","report-no":null,"categories":"cond-mat.str-el cond-mat.mes-hall cond-mat.mtrl-sci","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Rare-earth magnets with parent cubic symmetry exhibit unique topological\nproperties. However, the origin of these behaviors remains presently unclear.\nHere, we develop minimal models for Dirac semimetals (DSMs) with accidental\nband crossings and higher-order topology in cubic systems, incorporating\ncandidate magnetic order to analyze bulk, surface, and hinge state\ncharacteristics. In certain cubic-symmetric DSMs, we identify an effective Z2\nchiral symmetry which significantly impacts surface and hinge-localized states.\nOur results highlight distinct features in surface state dispersions, Fermi\narcs, polarization dependence, and band splitting that correlate with\nphotoemission data in rare-earth monopnictides. We also suggest candidate\nmaterials and experimental tests for further validation. These findings advance\nour understanding of surface states in rare-earth magnets with parent cubic\nsymmetries and illuminate the role of DSM physics in these systems.\n","versions":"[{'version': 'v1', 'created': 'Thu, 14 Nov 2024 18:59:31 GMT'}, {'version': 'v2', 'created': 'Tue, 26 Nov 2024 13:40:20 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 16:46:54 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Sur', 'Shouvik', ''], ['Setty', 'Chandan', '']]","extracted_entities":"[{'text': 'Rare-earth magnets', 'label': 'LLMs'}, {'text': 'DSMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"DSMs","similarity_score":0.6318616271}
{"id":2412.09049,"submitter":"Mengze Hong","authors":"Mengze Hong, Di Jiang, Yuanfeng Song, Lu Wang, Wailing Ng, Yanjie Sun,\n  Chen Jason Zhang, Qing Li","title":"Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for\n  Customer Service Dialogues","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Discovering customer intentions in dialogue conversations is crucial for\nautomated service agents. Yet, existing intent clustering methods often fail to\nalign with human perceptions due to the heavy reliance on embedding distance\nmetrics and sentence embeddings. To address these limitations, we propose\nintegrating the semantic understanding capabilities of LLMs into an\n$\\textbf{LLM-in-the-loop (LLM-ITL)}$ intent clustering framework. Specifically,\nthis paper (1) investigates the effectiveness of fine-tuned LLMs in semantic\ncoherence evaluation and intent cluster naming, achieving over 95% accuracy;\n(2) designs an LLM-ITL clustering algorithm that facilitates the iterative\ndiscovery of coherent intent clusters; and (3) proposes task-specific\ntechniques tailored for customer service dialogue intent clustering. Since\nexisting English benchmarks pose limited semantic diversity and intent labels,\nwe introduced a comprehensive Chinese dialogue intent dataset, comprising over\n100,000 real customer service calls and 1,507 human-annotated intent clusters.\nThe proposed approaches significantly outperformed LLM-guided baselines,\nachieving notable improvements in clustering quality and a 12% boost in the\ndownstream intent classification task. Combined with several best practices,\nour findings highlight the potential of LLM-in-the-loop techniques for scalable\nand human-aligned problem-solving. Sample code and datasets are available at:\nhttps:\/\/anonymous.4open.science\/r\/Dial-in-LLM-0410.\n","versions":"[{'version': 'v1', 'created': 'Thu, 12 Dec 2024 08:19:01 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 06:14:04 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Hong', 'Mengze', ''], ['Jiang', 'Di', ''], ['Song', 'Yuanfeng', ''], ['Wang', 'Lu', ''], ['Ng', 'Wailing', ''], ['Sun', 'Yanjie', ''], ['Zhang', 'Chen Jason', ''], ['Li', 'Qing', '']]","extracted_entities":"[{'text': 'sentence embeddings', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'LLMs'}, {'text': 'LLMs', 'label': 'LLMs'}, {'text': 'LLM-in-the-loop', 'label': 'LLM-based'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2412.21016,"submitter":"Mingxuan Xiao","authors":"Mingxuan Xiao, Yan Xiao, Shunhui Ji, Hanbo Cai, Lei Xue, Pengcheng\n  Zhang","title":"Assessing the Robustness of LLM-based NLP Software via Automated Testing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Benefiting from the advancements in LLMs, NLP software has undergone rapid\ndevelopment. Such software is widely employed in various safety-critical tasks,\nsuch as financial sentiment analysis, toxic content moderation, and log\ngeneration. Unlike traditional software, LLM-based NLP software relies on\nprompts and examples as inputs. Given the complexity of LLMs and the\nunpredictability of real-world inputs, quantitatively assessing the robustness\nof such software is crucial. However, to the best of our knowledge, no\nautomated robustness testing methods have been specifically designed to\nevaluate the overall inputs of LLM-based NLP software. To this end, this paper\nintroduces the first AutOmated Robustness Testing frAmework, AORTA, which\nreconceptualizes the testing process into a combinatorial optimization problem.\nExisting testing methods designed for DNN-based software can be applied to\nLLM-based software by AORTA, but their effectiveness is limited. To address\nthis, we propose a novel testing method for LLM-based software within AORTA\ncalled Adaptive Beam Search. ABS is tailored for the expansive feature space of\nLLMs and improves testing effectiveness through an adaptive beam width and the\ncapability for backtracking. We successfully embed 18 test methods in the\ndesigned framework AORTA and compared the test validity of ABS with three\ndatasets and five threat models. ABS facilitates a more comprehensive and\naccurate robustness assessment before software deployment, with an average test\nsuccess rate of 86.138%. Compared to the currently best-performing baseline\nPWWS, ABS significantly reduces the computational overhead by up to 3441.895\nseconds per successful test case and decreases the number of queries by 218.762\ntimes on average. Furthermore, test cases generated by ABS exhibit greater\nnaturalness and transferability.\n","versions":"[{'version': 'v1', 'created': 'Mon, 30 Dec 2024 15:33:34 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 13:42:06 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Xiao', 'Mingxuan', ''], ['Xiao', 'Yan', ''], ['Ji', 'Shunhui', ''], ['Cai', 'Hanbo', ''], ['Xue', 'Lei', ''], ['Zhang', 'Pengcheng', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLMs'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'ABS', 'label': 'LLM-based'}, {'text': 'ABS', 'label': 'LLM-based'}, {'text': 'ABS', 'label': 'LLM-based'}, {'text': 'ABS', 'label': 'LLM-based'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2502.16457,"submitter":"Heegyu Kim","authors":"Heegyu Kim, Taeyang Jeon, Seungtaek Choi, Ji Hoon Hong, Dong Won Jeon,\n  Ga-Yeon Baek, Gyeong-Won Kwak, Dong-Hee Lee, Jisu Bae, Chihoon Lee, Yunseo\n  Kim, Seon-Jin Choi, Jin-Seong Park, Sung Beom Cho, Hyunsouk Cho","title":"Towards Fully-Automated Materials Discovery via Large-Scale Synthesis\n  Dataset and Expert-Level LLM-as-a-Judge","comments":"under review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Materials synthesis is vital for innovations such as energy storage,\ncatalysis, electronics, and biomedical devices. Yet, the process relies heavily\non empirical, trial-and-error methods guided by expert intuition. Our work aims\nto support the materials science community by providing a practical,\ndata-driven resource. We have curated a comprehensive dataset of 17K\nexpert-verified synthesis recipes from open-access literature, which forms the\nbasis of our newly developed benchmark, AlchemyBench. AlchemyBench offers an\nend-to-end framework that supports research in large language models applied to\nsynthesis prediction. It encompasses key tasks, including raw materials and\nequipment prediction, synthesis procedure generation, and characterization\noutcome forecasting. We propose an LLM-as-a-Judge framework that leverages\nlarge language models for automated evaluation, demonstrating strong\nstatistical agreement with expert assessments. Overall, our contributions offer\na supportive foundation for exploring the capabilities of LLMs in predicting\nand guiding materials synthesis, ultimately paving the way for more efficient\nexperimental design and accelerated innovation in materials science.\n","versions":"[{'version': 'v1', 'created': 'Sun, 23 Feb 2025 06:16:23 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Mar 2025 00:40:18 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 14:00:39 GMT'}, {'version': 'v4', 'created': 'Wed, 19 Mar 2025 11:37:27 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Kim', 'Heegyu', ''], ['Jeon', 'Taeyang', ''], ['Choi', 'Seungtaek', ''], ['Hong', 'Ji Hoon', ''], ['Jeon', 'Dong Won', ''], ['Baek', 'Ga-Yeon', ''], ['Kwak', 'Gyeong-Won', ''], ['Lee', 'Dong-Hee', ''], ['Bae', 'Jisu', ''], ['Lee', 'Chihoon', ''], ['Kim', 'Yunseo', ''], ['Choi', 'Seon-Jin', ''], ['Park', 'Jin-Seong', ''], ['Cho', 'Sung Beom', ''], ['Cho', 'Hyunsouk', '']]","extracted_entities":"[{'text': 'open-access literature', 'label': 'Open-source LLMs'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2502.20963,"submitter":"Gerion Spielberger","authors":"Gerion Spielberger, Florian M. Artinger, Jochen Reb and Rudolf\n  Kerschreiter","title":"Retrieval Augmented Generation for Topic Modeling in Organizational\n  Research: An Introduction with Empirical Demonstration","comments":"30 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI econ.GN q-fin.EC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Analyzing textual data is the cornerstone of qualitative research. While\ntraditional methods such as grounded theory and content analysis are widely\nused, they are labor-intensive and time-consuming. Topic modeling offers an\nautomated complement. Yet, existing approaches, including LLM-based topic\nmodeling, still struggle with issues such as high data preprocessing\nrequirements, interpretability, and reliability. This paper introduces Agentic\nRetrieval-Augmented Generation (Agentic RAG) as a method for topic modeling\nwith LLMs. It integrates three key components: (1) retrieval, enabling\nautomatized access to external data beyond an LLM's pre-trained knowledge; (2)\ngeneration, leveraging LLM capabilities for text synthesis; and (3)\nagent-driven learning, iteratively refining retrieval and query formulation\nprocesses. To empirically validate Agentic RAG for topic modeling, we reanalyze\na Twitter\/X dataset, previously examined by Mu et al. (2024a). Our findings\ndemonstrate that the approach is more efficient, interpretable and at the same\ntime achieves higher reliability and validity in comparison to the standard\nmachine learning approach but also in comparison to LLM prompting for topic\nmodeling. These results highlight Agentic RAG's ability to generate\nsemantically relevant and reproducible topics, positioning it as a robust,\nscalable, and transparent alternative for AI-driven qualitative research in\nleadership, managerial, and organizational research.\n","versions":"[{'version': 'v1', 'created': 'Fri, 28 Feb 2025 11:25:11 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 12:00:26 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Spielberger', 'Gerion', ''], ['Artinger', 'Florian M.', ''], ['Reb', 'Jochen', ''], ['Kerschreiter', 'Rudolf', '']]","extracted_entities":"[{'text': 'Agentic RAG', 'label': 'RAG'}, {'text': 'LLMs', 'label': 'LLMs'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'agent-driven learning', 'label': 'Few-shot Learning'}, {'text': 'Agentic RAG', 'label': 'RAG'}, {'text': 'LLM prompting', 'label': 'RAG'}, {'text': 'Agentic RAG', 'label': 'RAG'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2503.13282,"submitter":"Lorenzo Coccia","authors":"Lorenzo Coccia, Matteo Padovan, Andrea Pompermaier, Mattia Sabatini,\n  Marco Avesani, Davide Giacomo Marangon, Paolo Villoresi, Giuseppe Vallone","title":"Quantum bounds and device-independent security with rank-one qubit\n  measurements","comments":"20 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Device-independent (DI) quantum protocols exploit Bell inequality violations\nto ensure security or certify quantum properties without making assumptions\nabout the internal workings of the devices. In this work, we study the role of\nrank-one qubit positive operator-valued measures (POVMs) in DI scenarios. This\nclass includes all qubit extremal POVMs, i.e., those measurements that cannot\nbe realized by randomly choosing among others, as well as part of non-extremal\nPOVMs, which have recently been shown to be useful for security applications in\nsequential quantum protocols. We demonstrate that any rank-one POVM can\ngenerate correlations in bipartite scenarios that saturate a Tsirelson\ninequality, i.e., a quantum bound on linear combinations of outcome statistics,\nwhen the two parties share an arbitrary entangled two-qubit state and some\nother self-tested measurements are performed. For extremal POVMs, such\nsaturation allows for an explicit calculation of the guessing probability and\nthe worst-case conditional von Neumann entropy. From the Tsirelson inequality,\nwe establish a randomness certification method that facilitates numerical\nsimulations and noise analysis. To test its feasibility, we performed a\nproof-of-concept experiment employing a three-outcome POVM on tilted entangled\nstates under experimental non-idealities. We further explore the case of\nnon-extremal POVMs, providing insights into their role in DI protocols.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:33:23 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Coccia', 'Lorenzo', ''], ['Padovan', 'Matteo', ''], ['Pompermaier', 'Andrea', ''], ['Sabatini', 'Mattia', ''], ['Avesani', 'Marco', ''], ['Marangon', 'Davide Giacomo', ''], ['Villoresi', 'Paolo', ''], ['Vallone', 'Giuseppe', '']]","extracted_entities":"[{'text': 'POVMs', 'label': 'LLMs'}, {'text': 'non-extremal\\nPOVMs', 'label': 'LLMs'}, {'text': 'POVMs', 'label': 'LLMs'}, {'text': 'POVMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"POVMs","similarity_score":0.5045669079}
{"id":2503.13556,"submitter":"Joe McIntyre","authors":"Joe McIntyre","title":"Pareidolic Illusions of Meaning: ChatGPT, Pseudolaw and the Triumph of\n  Form over Substance","comments":"54 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  The early 2020s has seen the rise of two strange and potentially quite\nimpactful social phenomena, namely pseudolaw, where users rely upon pseudolegal\narguments that mimic the form and ritual of legal argumentation but\nfundamentally distort the content of law, and generative AI\/LLMs, which\ngenerate content that uses probabilistic calculations to create outputs that\nlook like human generated text. This article argues that the juxtaposition of\nthe two phenomena helps to reveal that they both share two fundamental traits\nas both elevate form and appearance over substance and content, and users of\nboth routinely mistake the form for the substance. In drawing upon legal\ntheory, computer science, linguistics and cognitive psychology, the article\nargues that both phenomena rely upon creating illusions of meaning that users\nmistake for the underlying primary phenomenon. I then explore four implications\nof this conception of both phenomena. Firstly, both rely on human tendencies of\nconceptual pareidolia resulting in the erroneous perception of meaningful\nlinguistic legal patterns from nebulous inputs. Secondly, both rely upon the\nconfidence heuristic, the human cognitive bias for treating confidence as a\nproxy for competence. Thirdly, both succeed when the primary concern is with\nthe form of the output and not its content. Fourthly, both rely heavily upon\nthe magical thinking of users and the desire for the promise of the approach to\nbe real. The article argues that the legal context helps to reveal a solution\nfor the problems caused by both phenomena as it is only where users possess\nsufficient legal and technological literacy that it becomes possible to reveal\nto them the illusionary nature of the phenomena.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 00:15:41 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['McIntyre', 'Joe', '']]","extracted_entities":"[{'text': 'generative AI\/LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"generative AI\/LLMs","similarity_score":0.5735649467}
{"id":2503.13643,"submitter":"Esma Gel","authors":"Karina M. Sindermann, Esma S. Gel, Nesim K. Erkip","title":"Optimal Replenishment Policies for Industrial Vending Machines","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Industrial Vending Machines (IVMs) automate the dispensing of a variety of\nsupplies like safety equipment and tools at customer sites, providing 24\/7\naccess while tracking inventory in real-time. Industrial distribution companies\ntypically manage the replenishment of IVMs using periodic schedules, which do\nnot take advantage of these advanced real-time monitoring capabilities. We\ndevelop two approaches to optimize the long-term average cost of replenishments\nand stockouts per unit time: a state-dependent optimal control policy that\njointly considers all inventory levels (referred to as trigger set policy) and\na fixed cycle policy that optimizes replenishment frequency. We prove the\nmonotonicity of the optimal trigger set policy and leverage it to design a\ncomputationally efficient approximate online control framework. Unlike existing\nmethods, which typically handle a very limited number of items due to\ncomputational constraints, our approach scales to hundreds of items while\nachieving near-optimal performance. Leveraging transaction data from our\nindustrial partner, we conduct an extensive set of numerical experiments to\ndemonstrate this claim. Our results show that optimal fixed cycle replenishment\nreduces costs by 61.7 to 78.6% compared to current practice, with our online\ncontrol framework delivering an additional 4.1 to 22.9% improvement. Our novel\ntheoretical results provide practical tools for effective replenishment\nmanagement in this modern vendor-managed inventory context.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 18:47:11 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Sindermann', 'Karina M.', ''], ['Gel', 'Esma S.', ''], ['Erkip', 'Nesim K.', '']]","extracted_entities":"[{'text': 'Industrial Vending Machines', 'label': 'LLMs'}, {'text': 'IVMs', 'label': 'LLMs'}, {'text': 'IVMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"IVMs","similarity_score":0.5051794052}
{"id":2503.1369,"submitter":"Jan Bronec","authors":"Jan Bronec (1), Jind\\v{r}ich Helcl (1) ((1) Charles University,\n  Faculty of Mathematics and Physics, Institute of Formal and Applied\n  Linguistics)","title":"Atyaephyra at SemEval-2025 Task 4: Low-Rank NPO","comments":"5 pages, 1 figure, 1 table, submitted to SemEval proceedings for ACL\n  Anthology","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present a submission to the SemEval 2025 shared task on unlearning\nsensitive content from LLMs. Our approach employs negative preference\noptimization using low-rank adaptation. We show that we can utilize this\ncombination to cheaply compute additional regularization terms, which help with\nunlearning stabilization. The results of our approach significantly exceed the\nshared task baselines.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 19:59:19 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Bronec', 'Jan', ''], ['Helcl', 'Jind\u0159ich', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLMs'}, {'text': 'negative preference\\noptimization', 'label': 'Fine-tuning'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2503.14167,"submitter":"Christian Poelitz","authors":"Christian Poelitz, Nick McKenna","title":"Synthetic Clarification and Correction Dialogues about Data-Centric\n  Tasks -- A Teacher-Student Approach","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Real dialogues with AI assistants for solving data-centric tasks often follow\ndynamic, unpredictable paths due to imperfect information provided by the user\nor in the data, which must be caught and handled. Developing datasets which\ncapture such user-AI interactions is difficult and time-consuming. In this\nwork, we develop a novel framework for synthetically generating controlled,\nmulti-turn conversations between a user and AI assistant for the task of\ntable-based question answering, which can be generated from an existing dataset\nwith fully specified table QA examples for any target domain. Each conversation\naims to solve a table-based reasoning question through collaborative effort,\nmodeling one of two real-world scenarios: (1) an AI-initiated clarification, or\n(2) a user-initiated correction. Critically, we employ a strong teacher LLM to\nverify the correctness of our synthetic conversations, ensuring high quality.\nWe demonstrate synthetic datasets generated from TAT-QA and WikiTableQuestions\nas benchmarks of frontier LLMs. We find that even larger models struggle to\neffectively issuing clarification questions and accurately integrate user\nfeedback for corrections.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:37:25 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Poelitz', 'Christian', ''], ['McKenna', 'Nick', '']]","extracted_entities":"[{'text': 'frontier LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"frontier LLMs","similarity_score":0.7546921968}
{"id":2503.14183,"submitter":"Ekaterina Verbitskaia","authors":"Aleksandr Shefer, Igor Engel, Stanislav Alekseev, Daniil Berezun,\n  Ekaterina Verbitskaia, Anton Podkopaev","title":"Can LLMs Enable Verification in Mainstream Programming?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI cs.PL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Although formal methods are capable of producing reliable software, they have\nseen minimal adoption in everyday programming. Automatic code generation using\nlarge language models is becoming increasingly widespread, but it rarely\nconsiders producing strong correctness guarantees. In this study, we explore\nthe ability of LLMs to produce verified code in three verification languages\n(Dafny, Nagini, and Verus). To do so, we use manually curated datasets derived\nfrom the state-ofthe-art Python benchmark, HumanEval. We also assess what types\nof information are sufficient to achieve good-quality results.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:58:00 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Shefer', 'Aleksandr', ''], ['Engel', 'Igor', ''], ['Alekseev', 'Stanislav', ''], ['Berezun', 'Daniil', ''], ['Verbitskaia', 'Ekaterina', ''], ['Podkopaev', 'Anton', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLMs'}, {'text': 'Verus', 'label': 'Large Language Model'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2503.14378,"submitter":"Zechen Bai","authors":"Zechen Bai, Hai Ci, Mike Zheng Shou","title":"Impossible Videos","comments":"26 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Synthetic videos nowadays is widely used to complement data scarcity and\ndiversity of real-world videos. Current synthetic datasets primarily replicate\nreal-world scenarios, leaving impossible, counterfactual and anti-reality video\nconcepts underexplored. This work aims to answer two questions: 1) Can today's\nvideo generation models effectively follow prompts to create impossible video\ncontent? 2) Are today's video understanding models good enough for\nunderstanding impossible videos? To this end, we introduce IPV-Bench, a novel\nbenchmark designed to evaluate and foster progress in video understanding and\ngeneration. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing\n4 domains, 14 categories. It features diverse scenes that defy physical,\nbiological, geographical, or social laws. Based on the taxonomy, a prompt suite\nis constructed to evaluate video generation models, challenging their prompt\nfollowing and creativity capabilities. In addition, a video benchmark is\ncurated to assess Video-LLMs on their ability of understanding impossible\nvideos, which particularly requires reasoning on temporal dynamics and world\nknowledge. Comprehensive evaluations reveal limitations and insights for future\ndirections of video models, paving the way for next-generation video models.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:10:24 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Bai', 'Zechen', ''], ['Ci', 'Hai', ''], ['Shou', 'Mike Zheng', '']]","extracted_entities":"[{'text': 'prompt suite', 'label': 'Prompting'}, {'text': 'video generation models', 'label': 'AI model'}, {'text': 'Video-LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"Video-LLMs","similarity_score":0.7106761336}
{"id":2503.14476,"submitter":"Qiying Yu","authors":"Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue,\n  Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole\n  Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang\n  Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan\n  Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin\n  Zhang, Lin Yan, Mu Qiao, Yonghui Wu, Mingxuan Wang","title":"DAPO: An Open-Source LLM Reinforcement Learning System at Scale","comments":"Project Page: https:\/\/dapo-sia.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Inference scaling empowers LLMs with unprecedented reasoning ability, with\nreinforcement learning as the core technique to elicit complex reasoning.\nHowever, key technical details of state-of-the-art reasoning LLMs are concealed\n(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the\ncommunity still struggles to reproduce their RL training results. We propose\nthe $\\textbf{D}$ecoupled Clip and $\\textbf{D}$ynamic s$\\textbf{A}$mpling\n$\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{DAPO}$) algorithm, and\nfully open-source a state-of-the-art large-scale RL system that achieves 50\npoints on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that\nwithhold training details, we introduce four key techniques of our algorithm\nthat make large-scale LLM RL a success. In addition, we open-source our\ntraining code, which is built on the verl framework, along with a carefully\ncurated and processed dataset. These components of our open-source system\nenhance reproducibility and support future research in large-scale LLM RL.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:49:06 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Yu', 'Qiying', ''], ['Zhang', 'Zheng', ''], ['Zhu', 'Ruofei', ''], ['Yuan', 'Yufeng', ''], ['Zuo', 'Xiaochen', ''], ['Yue', 'Yu', ''], ['Fan', 'Tiantian', ''], ['Liu', 'Gaohong', ''], ['Liu', 'Lingjun', ''], ['Liu', 'Xin', ''], ['Lin', 'Haibin', ''], ['Lin', 'Zhiqi', ''], ['Ma', 'Bole', ''], ['Sheng', 'Guangming', ''], ['Tong', 'Yuxuan', ''], ['Zhang', 'Chi', ''], ['Zhang', 'Mofan', ''], ['Zhang', 'Wang', ''], ['Zhu', 'Hang', ''], ['Zhu', 'Jinhua', ''], ['Chen', 'Jiaze', ''], ['Chen', 'Jiangjie', ''], ['Wang', 'Chengyi', ''], ['Yu', 'Hongli', ''], ['Dai', 'Weinan', ''], ['Song', 'Yuxuan', ''], ['Wei', 'Xiangpeng', ''], ['Zhou', 'Hao', ''], ['Liu', 'Jingjing', ''], ['Ma', 'Wei-Ying', ''], ['Zhang', 'Ya-Qin', ''], ['Yan', 'Lin', ''], ['Qiao', 'Mu', ''], ['Wu', 'Yonghui', ''], ['Wang', 'Mingxuan', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLMs'}, {'text': 'reinforcement learning', 'label': 'Few-shot Learning'}, {'text': 'LLMs', 'label': 'LLMs'}, {'text': 'Qwen2.5-32B base model', 'label': 'Foundation Model'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2503.14477,"submitter":"Ziwei Ji","authors":"Ziwei Ji, Lei Yu, Yeskendir Koishekenov, Yejin Bang, Anthony\n  Hartshorn, Alan Schelten, Cheng Zhang, Pascale Fung, Nicola Cancedda","title":"Calibrating Verbal Uncertainty as a Linear Feature to Reduce\n  Hallucinations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  LLMs often adopt an assertive language style also when making false claims.\nSuch ``overconfident hallucinations'' mislead users and erode trust. Achieving\nthe ability to express in language the actual degree of uncertainty around a\nclaim is therefore of great importance. We find that ``verbal uncertainty'' is\ngoverned by a single linear feature in the representation space of LLMs, and\nshow that this has only moderate correlation with the actual ``semantic\nuncertainty'' of the model. We apply this insight and show that (1) the\nmismatch between semantic and verbal uncertainty is a better predictor of\nhallucinations than semantic uncertainty alone and (2) we can intervene on\nverbal uncertainty at inference time and reduce hallucinations on short-form\nanswers, achieving an average relative reduction of 32%.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:51:04 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Ji', 'Ziwei', ''], ['Yu', 'Lei', ''], ['Koishekenov', 'Yeskendir', ''], ['Bang', 'Yejin', ''], ['Hartshorn', 'Anthony', ''], ['Schelten', 'Alan', ''], ['Zhang', 'Cheng', ''], ['Fung', 'Pascale', ''], ['Cancedda', 'Nicola', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2503.14505,"submitter":"Susung Hong","authors":"Susung Hong, Ira Kemelmacher-Shlizerman, Brian Curless, Steven M.\n  Seitz","title":"MusicInfuser: Making Video Diffusion Listen and Dance","comments":"Project page: https:\/\/susunghong.github.io\/MusicInfuser","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https:\/\/susunghong.github.io\/MusicInfuser.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:59:58 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Hong', 'Susung', ''], ['Kemelmacher-Shlizerman', 'Ira', ''], ['Curless', 'Brian', ''], ['Seitz', 'Steven M.', '']]","extracted_entities":"[{'text': 'Video-LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"Video-LLMs","similarity_score":0.7106761336}
{"id":2503.1462,"submitter":"Takehito Utsuro","authors":"Hikaru Shimadzu, Takehito Utsuro, Daisuke Kitayama","title":"Retrieval-Augmented Simulacra: Generative Agents for Up-to-date and\n  Knowledge-Adaptive Simulations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.SI","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  In the 2023 edition of the White Paper on Information and Communications, it\nis estimated that the population of social networking services in Japan will\nexceed 100 million by 2022, and the influence of social networking services in\nJapan is growing significantly. In addition, marketing using SNS and research\non the propagation of emotions and information on SNS are being actively\nconducted, creating the need for a system for predicting trends in SNS\ninteractions. We have already created a system that simulates the behavior of\nvarious communities on SNS by building a virtual SNS environment in which\nagents post and reply to each other in a chat community created by agents using\na LLMs. In this paper, we evaluate the impact of the search extension\ngeneration mechanism used to create posts and replies in a virtual SNS\nenvironment using a simulation system on the ability to generate posts and\nreplies. As a result of the evaluation, we confirmed that the proposed search\nextension generation mechanism, which mimics human search behavior, generates\nthe most natural exchange.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 18:17:10 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Shimadzu', 'Hikaru', ''], ['Utsuro', 'Takehito', ''], ['Kitayama', 'Daisuke', '']]","extracted_entities":"[{'text': 'chat community', 'label': 'ChatGPT'}, {'text': 'LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2503.14622,"submitter":"Miguel Arratia","authors":"Jiajun Huang, Sean Preins, Ryan Tsiao, Miguel Rodriguez, Barak\n  Schmookler, Miguel Arratia","title":"Measurement of SiPM Dark Currents and Annealing Recovery for Fluences\n  Expected in ePIC Calorimeters at the Electron-Ion Collider","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.ins-det hep-ex nucl-ex","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Silicon photomultipliers (SiPMs) will be used to read out all calorimeters in\nthe ePIC experiment at the Electron-Ion Collider (EIC). A thorough\ncharacterization of the radiation damage expected for SiPMs under anticipated\nEIC fluences is essential for accurate simulations, detector design, and\neffective operational strategies. In this study, we evaluate radiation damage\nfor the specific SiPM models chosen for ePIC across the complete fluence range\nanticipated at the EIC, $10^8$ to $10^{12}$ 1-MeV $n_{\\mathrm{eq}}$\/cm$^2$ per\nyear, depending on the calorimeter location. The SiPMs were irradiated using a\n64 MeV proton beam provided by the University of California, Davis 76\"\nCyclotron. We measured the SiPM dark-current as a function of fluence and bias\nvoltage and investigated the effectiveness of high-temperature annealing to\nrecover radiation damage. These results provide a comprehensive reference for\nthe design, simulation, and operational planning of all ePIC calorimeter\nsystems.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 18:19:45 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Huang', 'Jiajun', ''], ['Preins', 'Sean', ''], ['Tsiao', 'Ryan', ''], ['Rodriguez', 'Miguel', ''], ['Schmookler', 'Barak', ''], ['Arratia', 'Miguel', '']]","extracted_entities":"[{'text': 'Silicon photomultipliers', 'label': 'LLMs'}, {'text': 'SiPMs', 'label': 'LLMs'}, {'text': 'SiPMs', 'label': 'LLMs'}, {'text': 'SiPMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"SiPMs","similarity_score":0.5088107586}
{"id":2503.14957,"submitter":"Basura Fernando","authors":"Thanh-Son Nguyen, Hong Yang, Tzeh Yuan Neoh, Hao Zhang, Ee Yeo Keat,\n  Basura Fernando","title":"Neuro Symbolic Knowledge Reasoning for Procedural Video Question\n  Answering","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  This paper introduces a new video question-answering (VQA) dataset that\nchallenges models to leverage procedural knowledge for complex reasoning. It\nrequires recognizing visual entities, generating hypotheses, and performing\ncontextual, causal, and counterfactual reasoning. To address this, we propose\nneuro symbolic reasoning module that integrates neural networks and LLM-driven\nconstrained reasoning over variables for interpretable answer generation.\nResults show that combining LLMs with structured knowledge reasoning with logic\nenhances procedural reasoning on the STAR benchmark and our dataset. Code and\ndataset at https:\/\/github.com\/LUNAProject22\/KML soon.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 07:49:14 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Nguyen', 'Thanh-Son', ''], ['Yang', 'Hong', ''], ['Neoh', 'Tzeh Yuan', ''], ['Zhang', 'Hao', ''], ['Keat', 'Ee Yeo', ''], ['Fernando', 'Basura', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2503.15351,"submitter":"I-Fan Lin","authors":"I-Fan Lin, Faegheh Hasibi, Suzan Verberne","title":"SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling\n  with Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this paper, we propose Selection and Pooling with Large Language Models\n(SPILL), an intuitive and domain-adaptive method for intent clustering without\nfine-tuning. Existing embeddings-based clustering methods rely on a few labeled\nexamples or unsupervised fine-tuning to optimize results for each new dataset,\nwhich makes them less generalizable to multiple datasets. Our goal is to make\nthese existing embedders more generalizable to new domain datasets without\nfurther fine-tuning. Inspired by our theoretical derivation and simulation\nresults on the effectiveness of sampling and pooling techniques, we view the\nclustering task as a small-scale selection problem. A good solution to this\nproblem is associated with better clustering performance. Accordingly, we\npropose a two-stage approach: First, for each utterance (referred to as the\nseed), we derive its embedding using an existing embedder. Then, we apply a\ndistance metric to select a pool of candidates close to the seed. Because the\nembedder is not optimized for new datasets, in the second stage, we use an LLM\nto further select utterances from these candidates that share the same intent\nas the seed. Finally, we pool these selected candidates with the seed to derive\na refined embedding for the seed. We found that our method generally\noutperforms directly using an embedder, and it achieves comparable results to\nother state-of-the-art studies, even those that use much larger models and\nrequire fine-tuning, showing its strength and efficiency. Our results indicate\nthat our method enables existing embedders to be further improved without\nadditional fine-tuning, making them more adaptable to new domain datasets.\nAdditionally, viewing the clustering task as a small-scale selection problem\ngives the potential of using LLMs to customize clustering tasks according to\nthe user's goals.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:48:57 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Lin', 'I-Fan', ''], ['Hasibi', 'Faegheh', ''], ['Verberne', 'Suzan', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'unsupervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'embedding', 'label': 'Embedding'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'embedding', 'label': 'Embedding'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2503.15392,"submitter":"Matthew Brooks","authors":"Matthew Brooks, Foster Sabatino, Charles Tahan and Silas Hoffman","title":"Measurement-based Simulation of Geometric Gates in Topological Qubits on\n  NISQ Devices","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While the adiabatic exchange of Majorana zero modes (MZMs) enables a\nnon-universal set of geometrically protected gates, realising an experimental\nimplementation of MZM braiding remains challenging. In an alternative proposal,\ncharge-parity measurement of two neighboring MZMs supports braiding by\nteleportation. Moreover, owing to the lack of definitive evidence of MZMs in\nsemiconducting systems, there have been several simulations of MZMs on NISQ\ndevices which more naturally lend themselves to braiding. In this work,\nmeasurement-based braiding about MZM Y-junctions are simulated by multi-qubit\nPauli-parity measurements of a logical qubit. Logical single-qubit geometric\n$S^{(\\dagger)}$-gates and entangling two-qubit gates is shown using\ntwo-physical-qubit joint measurements alone, whilst partial phase rotations\nsuch as a $T^{(\\dagger)}$-gates require at least one three-qubit joint\nmeasurement. These relatively small scale circuits offer both novel\nmeasurement-based geometric gates as well as a measurement-based demonstration\nof quantum Hamiltonian simulation.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:31:04 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Brooks', 'Matthew', ''], ['Sabatino', 'Foster', ''], ['Tahan', 'Charles', ''], ['Hoffman', 'Silas', '']]","extracted_entities":"[{'text': 'Majorana zero modes', 'label': 'LLMs'}, {'text': 'MZMs', 'label': 'LLMs'}, {'text': 'MZMs', 'label': 'LLMs'}, {'text': 'MZMs', 'label': 'LLMs'}, {'text': 'MZMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"MZMs","similarity_score":0.6670669317}
{"id":2503.15405,"submitter":"Matthew Brooks","authors":"Foster Sabatino, Matthew Brooks, Charles Tahan and Silas Hoffman","title":"Simulated Non-Abelian Statistics of Majorana Zero Modes from a Kitaev\n  Lattice","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We simulate the non-Abelian exchange of Majorana zero modes (MZMs) on a\nquantum computer. Rather than utilizing MZMs at the boundaries of quantum Ising\nchains, which are typically represented as nonlocal operators on a quantum\ncomputer, using a Kitaev lattice allows us to exploit a local representation of\nMZMs. We detail the protocol for braiding two and four MZMs in terms of a spin\nHamiltonian, i.e. physical qubit Hamiltonian. Projecting this onto a subspace\nof states, we extract an effective Hamiltonian which drives a non-Abelian\nBerry's phase. Using several approximations, we construct a set of gates which\nmimics this accumulation of non-Abelian phase and process this construction on\na quantum computer. For two and four MZMs, we realize braiding fidelities of\napproximately 85\\% and 47\\%, respectively\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:45:19 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Sabatino', 'Foster', ''], ['Brooks', 'Matthew', ''], ['Tahan', 'Charles', ''], ['Hoffman', 'Silas', '']]","extracted_entities":"[{'text': 'Majorana zero modes', 'label': 'LLMs'}, {'text': 'MZMs', 'label': 'LLMs'}, {'text': 'MZMs', 'label': 'LLMs'}, {'text': 'quantum Ising\\nchains', 'label': 'quantisation'}, {'text': 'MZMs', 'label': 'LLMs'}, {'text': 'MZMs', 'label': 'LLMs'}, {'text': 'MZMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"MZMs","similarity_score":0.6670669317}
{"id":2503.15762,"submitter":"Elena Malnatsky","authors":"Elena Malnatsky, Shenghui Wang, Koen V. Hindriks, Mike E.U. Ligthart","title":"Dialogic Learning in Child-Robot Interaction: A Hybrid Approach to\n  Personalized Educational Content Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Dialogic learning fosters motivation and deeper understanding in education\nthrough purposeful and structured dialogues. Foundational models offer a\ntransformative potential for child-robot interactions, enabling the design of\npersonalized, engaging, and scalable interactions. However, their integration\ninto educational contexts presents challenges in terms of ensuring\nage-appropriate and safe content and alignment with pedagogical goals. We\nintroduce a hybrid approach to designing personalized educational dialogues in\nchild-robot interactions. By combining rule-based systems with LLMs for\nselective offline content generation and human validation, the framework\nensures educational quality and developmental appropriateness. We illustrate\nthis approach through a project aimed at enhancing reading motivation, in which\na robot facilitated book-related dialogues.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 00:46:10 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Malnatsky', 'Elena', ''], ['Wang', 'Shenghui', ''], ['Hindriks', 'Koen V.', ''], ['Ligthart', 'Mike E. U.', '']]","extracted_entities":"[{'text': 'Dialogic learning', 'label': 'Few-shot Learning'}, {'text': 'LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2503.15885,"submitter":"Hyunjae Suh","authors":"Hyunjae Suh, Mahan Tafreshipour, Sam Malek, Iftekhar Ahmed","title":"Human or LLM? A Comparative Study on Accessible Code Generation\n  Capability","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Web accessibility is essential for inclusive digital experiences, yet the\naccessibility of LLM-generated code remains underexplored. This paper presents\nan empirical study comparing the accessibility of web code generated by GPT-4o\nand Qwen2.5-Coder-32B-Instruct-AWQ against human-written code. Results show\nthat LLMs often produce more accessible code, especially for basic features\nlike color contrast and alternative text, but struggle with complex issues such\nas ARIA attributes. We also assess advanced prompting strategies (Zero-Shot,\nFew-Shot, Self-Criticism), finding they offer some gains but are limited. To\naddress these gaps, we introduce FeedA11y, a feedback-driven ReAct-based\napproach that significantly outperforms other methods in improving\naccessibility. Our work highlights the promise of LLMs for accessible code\ngeneration and emphasizes the need for feedback-based techniques to address\npersistent challenges.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 06:14:26 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Suh', 'Hyunjae', ''], ['Tafreshipour', 'Mahan', ''], ['Malek', 'Sam', ''], ['Ahmed', 'Iftekhar', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLMs'}, {'text': 'Zero-Shot', 'label': 'Zero-shot Learning'}, {'text': 'Few-Shot', 'label': 'Zero-shot Learning'}, {'text': 'Self-Criticism', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'LLMs'}]","assigned_concept":"LLMs","matched_keyword":"LLMs","similarity_score":1.0000002384}
{"id":2503.16363,"submitter":"Haoqi He","authors":"Haoqi He, Yan Xiao","title":"Probabilistic Quantum SVM Training on Ising Machine","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG quant-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Quantum computing holds significant potential to accelerate machine learning\nalgorithms, especially in solving optimization problems like those encountered\nin Support Vector Machine (SVM) training. However, current QUBO-based Quantum\nSVM (QSVM) methods rely solely on binary optimal solutions, limiting their\nability to identify fuzzy boundaries in data. Additionally, the limited qubit\ncount in contemporary quantum devices constrains training on larger datasets.\nIn this paper, we propose a probabilistic quantum SVM training framework\nsuitable for Coherent Ising Machines (CIMs). By formulating the SVM training\nproblem as a QUBO model, we leverage CIMs' energy minimization capabilities and\nintroduce a Boltzmann distribution-based probabilistic approach to better\napproximate optimal SVM solutions, enhancing robustness. To address qubit\nlimitations, we employ batch processing and multi-batch ensemble strategies,\nenabling small-scale quantum devices to train SVMs on larger datasets and\nsupport multi-class classification tasks via a one-vs-one approach. Our method\nis validated through simulations and real-machine experiments on binary and\nmulti-class datasets. On the banknote binary classification dataset, our\nCIM-based QSVM, utilizing an energy-based probabilistic approach, achieved up\nto 20% higher accuracy compared to the original QSVM, while training up to\n$10^4$ times faster than simulated annealing methods. Compared with classical\nSVM, our approach either matched or reduced training time. On the IRIS\nthree-class dataset, our improved QSVM outperformed existing QSVM models in all\nkey metrics. As quantum technology advances, increased qubit counts are\nexpected to further enhance QSVM performance relative to classical SVM.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:20:26 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['He', 'Haoqi', ''], ['Xiao', 'Yan', '']]","extracted_entities":"[{'text': 'CIMs', 'label': 'LLMs'}, {'text': 'Boltzmann distribution-based probabilistic approach', 'label': 'Few-shot Learning'}]","assigned_concept":"LLMs","matched_keyword":"CIMs","similarity_score":0.5240474343}
{"id":2503.15983,"submitter":"Rickard Br\\\"annvall","authors":"Tony Zhang and Rickard Br\\\"annvall","title":"InhibiDistilbert: Knowledge Distillation for a ReLU and Addition-based\n  Transformer","comments":"7 pages, 2 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This work explores optimizing transformer-based language models by\nintegrating model compression techniques with inhibitor attention, a novel\nalternative attention mechanism. Inhibitor attention employs Manhattan\ndistances and ReLU activations instead of the matrix multiplications and\nsoftmax activation of the conventional scaled dot-product attention. This shift\noffers potential computational and energy savings while maintaining model\neffectiveness. We propose further adjustments to improve the inhibitor\nmechanism's training efficiency and evaluate its performance on the DistilBERT\narchitecture. Our knowledge distillation experiments indicate that the modified\ninhibitor transformer model can achieve competitive performance on standard NLP\nbenchmarks, including General Language Understanding Evaluation (GLUE) and\nsentiment analysis tasks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 09:30:35 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zhang', 'Tony', ''], ['Br\u00e4nnvall', 'Rickard', '']]","extracted_entities":"[{'text': 'inhibitor attention', 'label': 'Attention mechanism'}, {'text': 'Inhibitor attention', 'label': 'Attention mechanism'}, {'text': 'inhibitor\\nmechanism', 'label': 'Attention mechanism'}, {'text': 'DistilBERT', 'label': 'DistilBERT'}, {'text': 'knowledge distillation experiments', 'label': 'Knowledge distillation'}]","assigned_concept":"DistilBERT","matched_keyword":"DistilBERT","similarity_score":1.0}
{"id":2104.1484,"submitter":"Zhishuai Guo","authors":"Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, Tianbao Yang","title":"Unified Convergence Analysis for Adaptive Optimization with Moving\n  Average Estimator","comments":"Accepted to Machine Learning","journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Although adaptive optimization algorithms have been successful in many\napplications, there are still some mysteries in terms of convergence analysis\nthat have not been unraveled. This paper provides a novel non-convex analysis\nof adaptive optimization to uncover some of these mysteries. Our contributions\nare three-fold. First, we show that an increasing or large enough momentum\nparameter for the first-order moment used in practice is sufficient to ensure\nthe convergence of adaptive algorithms whose adaptive scaling factors of the\nstep size are bounded. Second, our analysis gives insights for practical\nimplementations, e.g., increasing the momentum parameter in a stage-wise manner\nin accordance with stagewise decreasing step size would help improve the\nconvergence. Third, the modular nature of our analysis allows its extension to\nsolving other optimization problems, e.g., compositional, min-max and bilevel\nproblems. As an interesting yet non-trivial use case, we present algorithms for\nsolving non-convex min-max optimization and bilevel optimization that do not\nrequire using large batches of data to estimate gradients or double loops as\nthe literature do. Our empirical studies corroborate our theoretical results.\n","versions":"[{'version': 'v1', 'created': 'Fri, 30 Apr 2021 08:50:24 GMT'}, {'version': 'v2', 'created': 'Thu, 3 Jun 2021 01:24:31 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Jan 2022 17:02:03 GMT'}, {'version': 'v4', 'created': 'Tue, 22 Feb 2022 03:58:26 GMT'}, {'version': 'v5', 'created': 'Sat, 9 Nov 2024 18:13:55 GMT'}, {'version': 'v6', 'created': 'Thu, 20 Mar 2025 01:10:15 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Guo', 'Zhishuai', ''], ['Xu', 'Yi', ''], ['Yin', 'Wotao', ''], ['Jin', 'Rong', ''], ['Yang', 'Tianbao', '']]","extracted_entities":"[{'text': 'adaptive scaling factors', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"adaptive scaling factors","similarity_score":0.6074620485}
{"id":2205.06043,"submitter":"Jens Kaad","authors":"Jens Kaad and David Kyed","title":"The quantum metric structure of quantum SU(2)","comments":"96 pages","journal-ref":"Memoirs of the European Mathematical Society, 18. EMS Press,\n  Berlin, 2025","doi":null,"report-no":null,"categories":"math.OA math.FA math.MG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We introduce a two parameter family of Dirac operators on quantum SU(2) and\nanalyse their properties from the point of view of non-commutative metric\ngeometry. It is shown that these Dirac operators give rise to compact quantum\nmetric structures, and that the corresponding two parameter family of compact\nquantum metric spaces varies continuously in Rieffel's quantum Gromov-Hausdorff\ndistance. This continuity result includes the classical case where we recover\nthe round 3-sphere up to a global scaling factor on the metric. Our main\ntechnical tool is a quantum SU(2) analogue of the Berezin transform, together\nwith its associated fuzzy approximations, the analysis of which also leads to a\nsystematic way of approximating Lipschitz operators by means of polynomial\nexpressions in the generators.\n","versions":"[{'version': 'v1', 'created': 'Thu, 12 May 2022 12:05:08 GMT'}, {'version': 'v2', 'created': 'Tue, 21 Jan 2025 12:36:11 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 10:43:30 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Kaad', 'Jens', ''], ['Kyed', 'David', '']]","extracted_entities":"[{'text': 'global scaling factor', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"global scaling factor","similarity_score":0.5966739655}
{"id":2308.02189,"submitter":"Xiao-Tian Zhang","authors":"Xiao-Tian Zhang, Yi-Hui Xing, Xu-Ping Yao, Yuya Ominato, Long Zhang\n  and Mamoru Matsuo","title":"Spin pumping effect in non-Fermi liquid metals","comments":"10 pages, 4 figures with Supplementary Informations (19 pages)","journal-ref":"Commun Phys 8, 103 (2025)","doi":"10.1038\/s42005-025-02033-0","report-no":null,"categories":"cond-mat.str-el cond-mat.mes-hall","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Spin pumping effect is a sensitive and well-established experimental method\nin two-dimensional (2D) magnetic materials. We propose that spin pumping effect\ncan be a valuable probe for non-Fermi liquid (NFL) behaviors at the 2D\ninterface of magnetic heterostructures. We show that the modulations of\nferromagnetic resonance exhibit power-law scalings in frequency and temperature\nfor NFL metals induced near a quantum critical point (QCP). At the Ising\nnematic QCP, we demonstrate that the enhanced Gilbert damping coefficient\n$\\delta \\alpha$ acquires negative power-law exponents in distinct frequency\nregimes. The exponents convey universal parameters inherited from the QCP and\nreflect the non-quasiparticle nature of the spin carriers in the NFL metal. At\nfinite temperature, we show that the Gilbert damping mechanism is restored in\nthe quantum critical regime and $\\delta \\alpha$ measures the temperature\ndependence of the correlation length. Our theoretical proposal has the\npotential to stimulate the development of an interdisciplinary research domain\nwhere insights from non-equilibrium spin physics in spintronics are integrated\ninto strongly correlated matter.\n","versions":"[{'version': 'v1', 'created': 'Fri, 4 Aug 2023 08:12:21 GMT'}, {'version': 'v2', 'created': 'Wed, 31 Jul 2024 04:22:29 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 00:23:26 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Zhang', 'Xiao-Tian', ''], ['Xing', 'Yi-Hui', ''], ['Yao', 'Xu-Ping', ''], ['Ominato', 'Yuya', ''], ['Zhang', 'Long', ''], ['Matsuo', 'Mamoru', '']]","extracted_entities":"[{'text': 'power-law scalings', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"power-law scalings","similarity_score":0.7520148754}
{"id":2311.13915,"submitter":"Talia Baravi","authors":"Talia Baravi, David A. Kessler and Eli Barkai","title":"First passage times in compact domains exhibit bi-scaling","comments":"7 pages (14 with supplementary material), 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.stat-mech","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The study of first passage times for diffusing particles reaching target\nstates is foundational in various practical applications, including\ndiffusion-controlled reactions. In this work, we present a bi-scaling theory\nfor the probability density function of first passage times in confined compact\nprocesses, applicable to both Euclidean and Fractal domains, diverse\ngeometries, and scenarios with or without external force fields, accommodating\nMarkovian and semi-Markovian random walks. In large systems, first passage time\nstatistics exhibit a bi-scaling behavior, challenging the use of a single time\nscale. Our theory employs two distinct scaling functions: one for short times,\ncapturing initial dynamics in unbounded systems, and the other for long times\nis sensitive to finite size effects. The combined framework provides a complete\nexpression for first passage time statistics across all time scales.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 Nov 2023 11:04:27 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 08:42:06 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Baravi', 'Talia', ''], ['Kessler', 'David A.', ''], ['Barkai', 'Eli', '']]","extracted_entities":"[{'text': 'bi-scaling theory', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"bi-scaling theory","similarity_score":0.6286767125}
{"id":2312.14155,"submitter":"Fo-Hong Wang","authors":"Fo-Hong Wang and Xiao Yan Xu","title":"Entanglement R\\'{e}nyi Negativity of Interacting Fermions from Quantum\n  Monte Carlo Simulations","comments":"9+11 pages, 3+2 figures","journal-ref":"Nat Commun 16, 2637 (2025)","doi":"10.1038\/s41467-025-57971-8","report-no":null,"categories":"cond-mat.str-el hep-lat quant-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Many-body entanglement unveils additional aspects of quantum matter and\noffers insights into strongly correlated physics. While ground-state\nentanglement has received much attention in the past decade, the study of\nmixed-state quantum entanglement using negativity in interacting fermionic\nsystems remains largely unexplored. We demonstrate that the partially\ntransposed density matrix of interacting fermions, similar to their reduced\ndensity matrix, can be expressed as a weighted sum of Gaussian states\ndescribing free fermions, enabling the calculation of rank-$n$ R\\'{e}nyi\nnegativity within the determinant quantum Monte Carlo framework. We calculate\nthe rank-two R\\'{e}nyi negativity for the half-filled Hubbard model and the\nspinless $t$-$V$ model. Our calculation reveals that the area law coefficient\nof the R\\'{e}nyi negativity for the spinless $t$-$V$ model has a logarithmic\nfinite-size scaling at the finite-temperature transition point. Our work\ncontributes to the calculation of entanglement and sets the stage for future\nstudies on quantum entanglement in various fermionic many-body mixed states.\n","versions":"[{'version': 'v1', 'created': 'Thu, 21 Dec 2023 18:59:46 GMT'}, {'version': 'v2', 'created': 'Sat, 27 Apr 2024 07:45:24 GMT'}, {'version': 'v3', 'created': 'Sat, 15 Jun 2024 05:07:09 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 04:11:13 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wang', 'Fo-Hong', ''], ['Xu', 'Xiao Yan', '']]","extracted_entities":"[{'text': 'Many-body entanglement', 'label': 'quantisation'}, {'text': 'half-filled Hubbard model', 'label': 'AI model'}, {'text': 'spinless $t$-$V$ model', 'label': 'AI model'}, {'text': 'area law coefficient', 'label': 'Scaling law'}, {'text': 'logarithmic\\nfinite-size scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"logarithmic\nfinite-size scaling","similarity_score":0.5783959031}
{"id":2403.03603,"submitter":"Oren Yakir","authors":"Alon Nishry and Oren Yakir","title":"Large charge fluctuations in the hierarchical Coulomb gas","comments":"46 pages, 2 figures. Incorporated referee's comments","journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR math-ph math.MP","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The two-dimensional one-component plasma (OCP) is a model of electrically\ncharged particles which are embedded in a uniform background of the opposite\ncharge, and interact through a logarithmic potential. More than 30 years ago,\nJancovici, Lebowitz and Manificat discovered an asymptotic law for\nprobabilities of large charge fluctuations in the OCP. We prove that this law\nholds for the hierarchical counterpart of the OCP. The hierarchical model was\nrecently introduced by Chatterjee, and is inspired by Dyson's hierarchical\nmodel of the Ising ferromagnet.\n","versions":"[{'version': 'v1', 'created': 'Wed, 6 Mar 2024 10:44:56 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 16:36:13 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Nishry', 'Alon', ''], ['Yakir', 'Oren', '']]","extracted_entities":"[{'text': 'asymptotic law', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"asymptotic law","similarity_score":0.5246344805}
{"id":2403.14967,"submitter":"Tanjung Krisnanda","authors":"Xiaozhou Pan, Tanjung Krisnanda, Andrea Duina, Kimin Park, Pengtao\n  Song, Clara Yun Fontaine, Adrian Copetudo, Radim Filip, and Yvonne Y. Gao","title":"Realization of versatile and effective quantum metrology using a single\n  bosonic mode","comments":"Main text (4 figures, 6 pages) and Appendices (11 figures and 1\n  table, 8 pages). Fixed typos","journal-ref":"PRX Quantum 6, 010304 (2025)","doi":"10.1103\/PRXQuantum.6.010304","report-no":null,"categories":"quant-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Quantum metrology offers the potential to surpass its classical counterpart,\npushing the boundaries of measurement precision toward the ultimate Heisenberg\nlimit. This enhanced precision is normally attained by utilizing large squeezed\nstates or multi-particle entangled quantum states, both of which are often\nchallenging to implement and prone to decoherence in real quantum devices. In\nthis work, we present a versatile and on-demand protocol for deterministic\nparameter estimation that leverages two efficient state-transfer operations on\na single bosonic mode. Specifically, we demonstrate this protocol in the\ncontext of phase estimation using the superposition of coherent states in the\nbosonic circuit quantum electrodynamics (cQED) platform. With low average\nphoton numbers of only up to 1.76, we achieve quantum-enhanced precision\napproaching the Heisenberg scaling, reaching a metrological gain of 7.5(6) dB.\nImportantly, we show that the gain or sensitivity range can be further enhanced\non the fly by tailoring the input states, with different superposition weights,\nbased on specific system constraints. The realization of this versatile and\nefficient scheme affords a promising path towards practical quantum-enhanced\nsensing, not only for bosonic cQED hardware but also readily extensible to\nother continuous-variable platforms.\n","versions":"[{'version': 'v1', 'created': 'Fri, 22 Mar 2024 05:47:47 GMT'}, {'version': 'v2', 'created': 'Wed, 8 Jan 2025 02:49:40 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 02:29:40 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Pan', 'Xiaozhou', ''], ['Krisnanda', 'Tanjung', ''], ['Duina', 'Andrea', ''], ['Park', 'Kimin', ''], ['Song', 'Pengtao', ''], ['Fontaine', 'Clara Yun', ''], ['Copetudo', 'Adrian', ''], ['Filip', 'Radim', ''], ['Gao', 'Yvonne Y.', '']]","extracted_entities":"[{'text': 'Heisenberg scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"Heisenberg scaling","similarity_score":0.5351125598}
{"id":2403.16952,"submitter":"Jiasheng Ye","authors":"Jiasheng Ye, Peiju Liu, Tianxiang Sun, Jun Zhan, Yunhua Zhou, Xipeng\n  Qiu","title":"Data Mixing Laws: Optimizing Data Mixtures by Predicting Language\n  Modeling Performance","comments":"accepted by ICLR2025, camera ready version","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Pretraining data of large language models composes multiple domains (e.g.,\nweb texts, academic papers, codes), whose mixture proportions crucially impact\nthe competence of outcome models. While existing endeavors rely on heuristics\nor qualitative strategies to tune the proportions, we discover the quantitative\npredictability of model performance regarding the mixture proportions in\nfunction forms, which we refer to as the data mixing laws. Fitting such\nfunctions on sample mixtures unveils model performance on unseen mixtures\nbefore actual runs, thus guiding the selection of an ideal data mixture.\nFurthermore, we propose nested use of the scaling laws of training steps, model\nsizes, and our data mixing law to enable predicting the performance of large\nmodels trained on massive data under various mixtures with only small-scale\ntraining. Moreover, experimental results verify that our method effectively\noptimizes the training mixture of a 1B model trained for 100B tokens in\nRedPajama, reaching a performance comparable to the one trained for 48% more\nsteps on the default mixture. Extending the application of data mixing laws to\ncontinual training accurately predicts the critical mixture proportion that\navoids catastrophic forgetting and outlooks the potential for dynamic data\nschedules\n","versions":"[{'version': 'v1', 'created': 'Mon, 25 Mar 2024 17:14:00 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 03:31:27 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Ye', 'Jiasheng', ''], ['Liu', 'Peiju', ''], ['Sun', 'Tianxiang', ''], ['Zhan', 'Jun', ''], ['Zhou', 'Yunhua', ''], ['Qiu', 'Xipeng', '']]","extracted_entities":"[{'text': 'data mixing laws', 'label': 'Scaling law'}, {'text': 'scaling laws', 'label': 'Scaling law'}, {'text': 'data mixing law', 'label': 'Scaling law'}, {'text': 'data mixing laws', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling laws","similarity_score":0.9373526573}
{"id":2405.20382,"submitter":"Enrico Di Benedetto","authors":"Enrico Di Benedetto, Alejandro Gonzalez-Tudela and Francesco\n  Ciccarello","title":"Dipole-dipole interactions mediated by a photonic flat band","comments":"18 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.mes-hall","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Flat bands (FBs) are energy bands with zero group velocity, which in\nelectronic systems were shown to favor strongly correlated phenomena. Indeed, a\nFB can be spanned with a basis of strictly localized states, the so called\n\"compact localized states\" (CLSs), which are yet generally non-orthogonal.\nHere, we study emergent dipole-dipole interactions between emitters\ndispersively coupled to the photonic analogue of a FB, a setup within reach in\nstate-of the-art experimental platforms. We show that the strength of such\nphoton-mediated interactions decays exponentially with distance with a\ncharacteristic localization length which, unlike typical behaviours with\nstandard bands, saturates to a finite value as the emitter's energy approaches\nthe FB. Remarkably, we find that the localization length grows with the overlap\nbetween CLSs according to an analytically-derived universal scaling law valid\nfor a large class of FBs both in 1D and 2D. Using giant atoms (non-local\natom-field coupling) allows to tailor interaction potentials having the same\nshape of a CLS or a superposition of a few of these.\n","versions":"[{'version': 'v1', 'created': 'Thu, 30 May 2024 18:00:05 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Jun 2024 10:02:13 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 08:33:14 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Di Benedetto', 'Enrico', ''], ['Gonzalez-Tudela', 'Alejandro', ''], ['Ciccarello', 'Francesco', '']]","extracted_entities":"[{'text': 'CLSs', 'label': 'LLMs'}, {'text': 'analytically-derived universal scaling law', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"analytically-derived universal scaling law","similarity_score":0.7031627893}
{"id":2406.03306,"submitter":"Kaito Wada","authors":"Kaito Wada, Naoki Yamamoto, Nobuyuki Yoshioka","title":"Heisenberg-limited adaptive gradient estimation for multiple observables","comments":"Final version, add new analytical and numerical results","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In quantum mechanics, measuring the expectation value of a general observable\nhas an inherent statistical uncertainty that is quantified by variance or mean\nsquared error of measurement outcome. While the uncertainty can be reduced by\naveraging several samples, the number of samples should be minimized when each\nsample is very costly. This is especially the case for fault-tolerant quantum\ncomputing that involves measurement of multiple observables of non-trivial\nstates in large quantum systems that exceed the capabilities of classical\ncomputers. In this work, we provide an adaptive quantum algorithm for\nestimating the expectation values of $M$ general observables within root mean\nsquared error $\\varepsilon$ simultaneously, using\n$\\mathcal{O}(\\varepsilon^{-1}\\sqrt{M}\\log M)$ queries to a state preparation\noracle of a target state. This remarkably achieves the scaling of Heisenberg\nlimit $1\/\\varepsilon$, a fundamental bound on the estimation precision in terms\nof mean squared error, together with the sublinear scaling of the number of\nobservables $M$. The proposed method is an adaptive version of the quantum\ngradient estimation algorithm and has a resource-efficient implementation due\nto its adaptiveness. Specifically, the space overhead in the proposed method is\n$\\mathcal{O}(M)$ which is independent from the estimation precision\n$\\varepsilon$ unlike non-iterative algorithms. In addition, our method can\navoid the numerical instability problem for constructing quantum circuits in a\nlarge-scale task (e.g., $\\varepsilon\\ll 1$ in our case), which appears in the\nactual implementation of many algorithms relying on quantum signal processing\ntechniques. Our method paves a new way to precisely understand and predict\nvarious physical properties in complicated quantum systems using quantum\ncomputers.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Jun 2024 14:16:47 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 17:26:50 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Wada', 'Kaito', ''], ['Yamamoto', 'Naoki', ''], ['Yoshioka', 'Nobuyuki', '']]","extracted_entities":"[{'text': 'sublinear scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"sublinear scaling","similarity_score":0.5715433359}
{"id":2406.07155,"submitter":"Chen Qian","authors":"Chen Qian, Zihao Xie, YiFei Wang, Wei Liu, Kunlun Zhu, Hanchen Xia,\n  Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, Maosong Sun","title":"Scaling Large Language Model-based Multi-Agent Collaboration","comments":"Accepted to ICLR-2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.MA cs.NI cs.SI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent breakthroughs in large language model-driven autonomous agents have\nrevealed that multi-agent collaboration often surpasses each individual through\ncollective reasoning. Inspired by the neural scaling law--increasing neurons\nenhances performance, this study explores whether the continuous addition of\ncollaborative agents can yield similar benefits. Technically, we utilize\ndirected acyclic graphs to organize agents into a multi-agent collaboration\nnetwork (MacNet), upon which their interactive reasoning is topologically\norchestrated for autonomous task solving. Extensive evaluations reveal that it\neffectively supports collaboration among over a thousand agents, with irregular\ntopologies outperforming regular ones. We also identify a collaborative scaling\nlaw--the overall performance follows a logistic growth pattern as agents scale,\nwith collaborative emergence occurring earlier than traditional neural\nemergence. We speculate this may be because scaling agents catalyzes their\nmultidimensional considerations during interactive reflection and refinement,\nthereby producing more comprehensive artifacts. The code is available at\nhttps:\/\/github.com\/OpenBMB\/ChatDev\/tree\/macnet.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Jun 2024 11:02:04 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Feb 2025 08:26:52 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 00:22:42 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Qian', 'Chen', ''], ['Xie', 'Zihao', ''], ['Wang', 'YiFei', ''], ['Liu', 'Wei', ''], ['Zhu', 'Kunlun', ''], ['Xia', 'Hanchen', ''], ['Dang', 'Yufan', ''], ['Du', 'Zhuoyun', ''], ['Chen', 'Weize', ''], ['Yang', 'Cheng', ''], ['Liu', 'Zhiyuan', ''], ['Sun', 'Maosong', '']]","extracted_entities":"[{'text': 'neural scaling law', 'label': 'Scaling law'}, {'text': 'collaborative scaling\\nlaw', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"neural scaling law","similarity_score":0.7773829699}
{"id":2407.03661,"submitter":"Yang Xiao","authors":"Yang Xiao and Rohan Kumar Das","title":"Where's That Voice Coming? Continual Learning for Sound Source\n  Localization","comments":"Accepted to ICME 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.AS cs.SD","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Sound source localization (SSL) is essential for many speech-processing\napplications. Deep learning models have achieved high performance, but often\nfail when the training and inference environments differ. Adapting SSL models\nto dynamic acoustic conditions faces a major challenge: catastrophic\nforgetting. In this work, we propose an exemplar-free continual learning\nstrategy for SSL (CL-SSL) to address such a forgetting phenomenon. CL-SSL\napplies task-specific sub-networks to adapt across diverse acoustic\nenvironments while retaining previously learned knowledge. It also uses a\nscaling mechanism to limit parameter growth, ensuring consistent performance\nacross incremental tasks. We evaluated CL-SSL on simulated data with varying\nmicrophone distances and real-world data with different noise levels. The\nresults demonstrate CL-SSL's ability to maintain high accuracy with minimal\nparameter increase, offering an efficient solution for SSL applications.\n","versions":"[{'version': 'v1', 'created': 'Thu, 4 Jul 2024 06:02:52 GMT'}, {'version': 'v2', 'created': 'Mon, 26 Aug 2024 16:38:30 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 17:45:12 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Xiao', 'Yang', ''], ['Das', 'Rohan Kumar', '']]","extracted_entities":"[{'text': 'scaling mechanism', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling mechanism","similarity_score":0.7598720789}
{"id":2407.07356,"submitter":"Junliang Guo","authors":"Wentao Zhang, Junliang Guo, Tianyu He, Li Zhao, Linli Xu, Jiang Bian","title":"Video In-context Learning: Autoregressive Transformers are Zero-Shot\n  Video Imitators","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  People interact with the real-world largely dependent on visual signal, which\nare ubiquitous and illustrate detailed demonstrations. In this paper, we\nexplore utilizing visual signals as a new interface for models to interact with\nthe environment. Specifically, we choose videos as a representative visual\nsignal. And by training autoregressive Transformers on video datasets in a\nself-supervised objective, we find that the model emerges a zero-shot\ncapability to infer the semantics from a demonstration video, and imitate the\nsemantics to an unseen scenario. This allows the models to perform unseen tasks\nby watching the demonstration video in an in-context manner, without further\nfine-tuning. To validate the imitation capacity, we design various evaluation\nmetrics including both objective and subjective measures. The results show that\nour models can generate high-quality video clips that accurately align with the\nsemantic guidance provided by the demonstration videos, and we also show that\nthe imitation capacity follows the scaling law. Code and models have been\nopen-sourced.\n","versions":"[{'version': 'v1', 'created': 'Wed, 10 Jul 2024 04:27:06 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 10:22:15 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhang', 'Wentao', ''], ['Guo', 'Junliang', ''], ['He', 'Tianyu', ''], ['Zhao', 'Li', ''], ['Xu', 'Linli', ''], ['Bian', 'Jiang', '']]","extracted_entities":"[{'text': 'autoregressive Transformers', 'label': 'Transformers'}, {'text': 'further\\nfine-tuning', 'label': 'Fine-tuning'}, {'text': 'scaling law', 'label': 'Scaling law'}, {'text': 'Code and models have been\\nopen-sourced', 'label': 'Open-source LLMs'}]","assigned_concept":"Scaling law","matched_keyword":"scaling law","similarity_score":1.0000001192}
{"id":2409.20159,"submitter":"Vincent Dumoncel","authors":"Vincent Dumoncel","title":"On the quasi-isometric classification of permutational wreath products","comments":"v1: 58 pages, comments are welcome! v2: Minor changes, some typos\n  corrected and references updated","journal-ref":null,"doi":null,"report-no":null,"categories":"math.GR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this article, we initiate the study of the large-scale geometry of\npermutational wreath products of the form $F\\wr_{H\/N}H$, where $H$ is finitely\npresented and where $N$ is a normal subgroup of $H$ satisfying a certain\nassumption of non coarse separation. The main result is a complete\nclassification of such permutational wreath products up to quasi-isometry,\nbuilding up on previous works from Genevois and Tessera. For instance, we show\nthat, for $d\\ge k\\ge 2$, $\\mathbb{Z}_{n}\\wr_{\\mathbb{Z}^{k}} \\mathbb{Z}^d$ and\n$\\mathbb{Z}_{m}\\wr_{\\mathbb{Z}^{k}}\\mathbb{Z}^d$ are quasi-isometric if and\nonly if $n$ and $m$ are powers of a common number. We also discuss biLipschitz\nequivalences between permutational wreath products, their scaling groups, as\nwell as the quasi-isometric classification of other halo products built out of\nsuch permutational lamplighters.\n","versions":"[{'version': 'v1', 'created': 'Mon, 30 Sep 2024 10:12:16 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 09:27:01 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Dumoncel', 'Vincent', '']]","extracted_entities":"[{'text': 'scaling groups', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling groups","similarity_score":0.5995565653}
{"id":2410.00953,"submitter":"Pengfei Zhang","authors":"Menghan Song, Zhaoyi Zeng, Ting-Tung Wang, Yi-Zhuang You, Zi Yang Meng\n  and Pengfei Zhang","title":"Monte Carlo Simulation of Operator Dynamics and Entanglement in\n  Dual-Unitary Circuits","comments":"12 pages,12 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.stat-mech cond-mat.str-el hep-th","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We investigate operator dynamics and entanglement growth in dual-unitary\ncircuits, a class of locally scrambled quantum systems that enables efficient\nsimulation beyond the exponential complexity of the Hilbert space. By mapping\nthe operator evolution to a classical Markov process,we perform Monte Carlo\nsimulations to access the time evolution of local operator density and\nentanglement with polynomial computational cost. Our results reveal that the\noperator density converges exponentially to a steady-state value, with\nanalytical bounds that match our simulations. Additionally, we observe a\nvolume-law scaling of operator entanglement across different subregions,and\nidentify a critical transition from maximal to sub-maximal entanglement growth,\ngoverned by the circuit's gate parameter. This transition, confirmed by both\nmean-field theory and Monte Carlo simulations, provides new insights into\noperator entanglement dynamics in quantum many-body systems. Our work offers a\nscalable computational framework for studying long-time operator evolution and\nentanglement, paving the way for deeper exploration of quantum information\ndynamics.\n","versions":"[{'version': 'v1', 'created': 'Tue, 1 Oct 2024 18:00:00 GMT'}, {'version': 'v2', 'created': 'Thu, 3 Oct 2024 05:53:29 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 06:11:39 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Song', 'Menghan', ''], ['Zeng', 'Zhaoyi', ''], ['Wang', 'Ting-Tung', ''], ['You', 'Yi-Zhuang', ''], ['Meng', 'Zi Yang', ''], ['Zhang', 'Pengfei', '']]","extracted_entities":"[{'text': 'volume-law scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"volume-law scaling","similarity_score":0.7735136747}
{"id":2410.03988,"submitter":"Shuang Liang","authors":"Shuang Liang and Guido Mont\\'ufar","title":"Implicit Bias of Mirror Flow for Shallow Neural Networks in Univariate\n  Regression","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We examine the implicit bias of mirror flow in univariate least squares error\nregression with wide and shallow neural networks. For a broad class of\npotential functions, we show that mirror flow exhibits lazy training and has\nthe same implicit bias as ordinary gradient flow when the network width tends\nto infinity. For ReLU networks, we characterize this bias through a variational\nproblem in function space. Our analysis includes prior results for ordinary\ngradient flow as a special case and lifts limitations which required either an\nintractable adjustment of the training data or networks with skip connections.\nWe further introduce scaled potentials and show that for these, mirror flow\nstill exhibits lazy training but is not in the kernel regime. For networks with\nabsolute value activations, we show that mirror flow with scaled potentials\ninduces a rich class of biases, which generally cannot be captured by an RKHS\nnorm. A takeaway is that whereas the parameter initialization determines how\nstrongly the curvature of the learned function is penalized at different\nlocations of the input space, the scaled potential determines how the different\nmagnitudes of the curvature are penalized.\n","versions":"[{'version': 'v1', 'created': 'Sat, 5 Oct 2024 00:43:09 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 07:13:59 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Liang', 'Shuang', ''], ['Mont\u00fafar', 'Guido', '']]","extracted_entities":"[{'text': 'scaled potentials', 'label': 'Scaling law'}, {'text': 'scaled potentials', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaled potentials","similarity_score":0.5266458988}
{"id":2410.07064,"submitter":"Yuxian Gu","authors":"Yuxian Gu, Li Dong, Hongning Wang, Yaru Hao, Qingxiu Dong, Furu Wei,\n  Minlie Huang","title":"Data Selection via Optimal Control for Language Models","comments":"ICLR 2025 Oral","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This work investigates the selection of high-quality pre-training data from\nmassive corpora to enhance LMs' capabilities for downstream usage. We formulate\ndata selection as a generalized Optimal Control problem, which can be solved\ntheoretically by Pontryagin's Maximum Principle (PMP), yielding a set of\nnecessary conditions that characterize the relationship between optimal data\nselection and LM training dynamics. Based on these theoretical results, we\nintroduce PMP-based Data Selection (PDS), a framework that approximates optimal\ndata selection by solving the PMP conditions. In our experiments, we adopt PDS\nto select data from CommmonCrawl and show that the PDS-selected corpus\naccelerates the learning of LMs and constantly boosts their performance on a\nwide range of downstream tasks across various model sizes. Moreover, the\nbenefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by\nthe extrapolation of the test loss curves according to the Scaling Laws. PDS\nalso improves data utilization when the pre-training data is limited, by\nreducing the data demand by 1.8 times, which helps mitigate the quick\nexhaustion of available web-crawled corpora. Our code, model, and data can be\nfound at https:\/\/github.com\/microsoft\/LMOps\/tree\/main\/data_selection.\n","versions":"[{'version': 'v1', 'created': 'Wed, 9 Oct 2024 17:06:57 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 23:52:27 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Gu', 'Yuxian', ''], ['Dong', 'Li', ''], ['Wang', 'Hongning', ''], ['Hao', 'Yaru', ''], ['Dong', 'Qingxiu', ''], ['Wei', 'Furu', ''], ['Huang', 'Minlie', '']]","extracted_entities":"[{'text': 'Scaling Laws', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"Scaling Laws","similarity_score":0.9373526573}
{"id":2410.11819,"submitter":"Aljaz Godec","authors":"Tassilo Schwarz, Anatoly B. Kolomeisky, and Alja\\v{z} Godec","title":"Mind the memory: Consistent time reversal removes artefactual scaling of\n  energy dissipation rate and provides more accurate and reliable thermodynamic\n  inference","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.stat-mech physics.bio-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  It has been proposed that an observed inverse power-law dependence of the\nMarkovian estimate for the steady-state dissipation rate on the coarse-graining\nscale in self-similar networks reflects a scale-dependent energy dissipation.\nBy explicit examples, it is demonstrated here that there are in general no\nrelations between such an apparent power-law dependence and the actual\ndissipation on different length scales. We construct fractal networks with a\nsingle dissipative scale and networks with a true inverse energy-dissipation\ncascade, and show that they display the same scaling behavior. Moreover, we\nshow that a self-similar network structure does not imply an inverse power-law\nscaling but may be mistaken for one in practice. When no dissipative cycles\nbecome hidden by the coarse graining, any scale dependence of the dissipation\nestimate vanishes if the memory is correctly accounted for in the time-reversal\noperation. A $k$-th order estimator is derived and necessary and sufficient\nconditions are proved for a guaranteed lower bound on dissipation. These\nhigher-order estimators saturated in the order are proved to provide sharper\nlower bounds on dissipation and their scale dependence signifies hidden\ndissipative cycles. It is shown that estimators not saturated in the order may\nerroneously overestimate the microscopic dissipation. Our results underscore\nthe still underappreciated importance of correctly accounting for memory in\nanalyzing coarse observations.\n","versions":"[{'version': 'v1', 'created': 'Tue, 15 Oct 2024 17:46:53 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 14:19:44 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Schwarz', 'Tassilo', ''], ['Kolomeisky', 'Anatoly B.', ''], ['Godec', 'Alja\u017e', '']]","extracted_entities":"[{'text': 'inverse power-law\\nscaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"inverse power-law\nscaling","similarity_score":0.7301334143}
{"id":2410.1236,"submitter":"Qingren Yao","authors":"Qingren Yao, Chao-Han Huck Yang, Renhe Jiang, Yuxuan Liang, Ming Jin,\n  Shirui Pan","title":"Towards Neural Scaling Laws for Time Series Foundation Models","comments":"Accepted by the 13th International Conference on Learning\n  Representations (ICLR 2025)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Scaling laws offer valuable insights into the design of time series\nfoundation models (TSFMs). However, previous research has largely focused on\nthe scaling laws of TSFMs for in-distribution (ID) data, leaving their\nout-of-distribution (OOD) scaling behavior and the influence of model\narchitectures less explored. In this work, we examine two common TSFM\narchitectures, encoder-only and decoder-only Transformers, and investigate\ntheir scaling behavior on both ID and OOD data. These models are trained and\nevaluated across varying parameter counts, compute budgets, and dataset sizes.\nOur experiments reveal that the log-likelihood loss of TSFMs exhibits similar\nscaling behavior in both OOD and ID settings. We further compare the scaling\nproperties across different architectures, incorporating two state-of-the-art\nTSFMs as case studies, showing that model architecture plays a significant role\nin scaling. The encoder-only Transformers demonstrate better scalability than\nthe decoder-only Transformers, while the architectural enhancements in the two\nadvanced TSFMs primarily improve ID performance but reduce OOD scalability.\nWhile scaling up TSFMs is expected to drive performance breakthroughs, the lack\nof a comprehensive understanding of TSFM scaling laws has hindered the\ndevelopment of a robust framework to guide model scaling. We fill this gap in\nthis work by synthesizing our findings and providing practical guidelines for\ndesigning and scaling larger TSFMs with enhanced model capabilities.\n","versions":"[{'version': 'v1', 'created': 'Wed, 16 Oct 2024 08:23:39 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Feb 2025 02:35:14 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 06:54:45 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Yao', 'Qingren', ''], ['Yang', 'Chao-Han Huck', ''], ['Jiang', 'Renhe', ''], ['Liang', 'Yuxuan', ''], ['Jin', 'Ming', ''], ['Pan', 'Shirui', '']]","extracted_entities":"[{'text': 'Scaling laws', 'label': 'Scaling law'}, {'text': 'time series\\nfoundation models', 'label': 'Foundation Model'}, {'text': 'TSFMs', 'label': 'Foundation Model'}, {'text': 'scaling laws', 'label': 'Scaling law'}, {'text': 'TSFMs', 'label': 'Foundation Model'}, {'text': 'encoder-only', 'label': 'Transformers'}, {'text': 'decoder-only Transformers', 'label': 'Transformers'}, {'text': 'TSFMs', 'label': 'Foundation Model'}, {'text': 'TSFMs', 'label': 'Foundation Model'}, {'text': 'encoder-only Transformers', 'label': 'Transformers'}, {'text': 'decoder-only Transformers', 'label': 'Transformers'}, {'text': 'TSFMs', 'label': 'Foundation Model'}, {'text': 'TSFMs', 'label': 'Foundation Model'}, {'text': 'scaling laws', 'label': 'Scaling law'}, {'text': 'TSFMs', 'label': 'Foundation Model'}]","assigned_concept":"Scaling law","matched_keyword":"Scaling laws","similarity_score":0.9373526573}
{"id":2410.13767,"submitter":"Jingjing Sun","authors":"Jingjing Sun, Jim Dai, Pengyi Shi","title":"Inpatient Overflow Management with Proximal Policy Optimization","comments":"45 pages, 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Overflowing patients to non-primary wards can effectively alleviate\ncongestion in hospitals, while undesired overflow also leads to issues like\nmismatched service quality. Therefore, we need to trade off between congestion\nand undesired overflow. This overflow management problem is modeled as a\ndiscrete-time Markov Decision Process with large state and action space. To\novercome the curse-of-dimensionality, we decompose the action at each time into\na sequence of atomic actions and use an actor-critic algorithm, Proximal Policy\nOptimization (PPO), to guide the atomic actions. Moreover, we tailor the design\nof neural network which represents policy to account for the daily periodic\npattern of the system flows. Under hospital settings of different scales, the\nPPO policies consistently outperform commonly used state-of-art policies.\n","versions":"[{'version': 'v1', 'created': 'Thu, 17 Oct 2024 17:04:08 GMT'}, {'version': 'v2', 'created': 'Sun, 20 Oct 2024 04:34:28 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 11:15:18 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Sun', 'Jingjing', ''], ['Dai', 'Jim', ''], ['Shi', 'Pengyi', '']]","extracted_entities":"[{'text': 'different scales', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"different scales","similarity_score":0.540723443}
{"id":2411.16302,"submitter":"Peng-Zhang He","authors":"Peng-Zhang He and Hai-Qing Zhang","title":"Krylov Complexity in the Schr\\\"odinger Field Theory","comments":"36 pages, 16 figures","journal-ref":null,"doi":"10.1007\/JHEP03(2025)142","report-no":null,"categories":"hep-th cond-mat.stat-mech","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We investigate the Krylov complexity of Schr\\\"odinger field theories,\nfocusing on both bosonic and fermionic systems within the grand canonical\nensemble that includes a chemical potential. Krylov complexity measures\noperator growth in quantum systems by analyzing how operators spread within the\nKrylov space, a subspace of the Hilbert space spanned by successive\napplications of the superoperator $[H,\\cdot]$ on an initial operator. Using the\nLanczos algorithm, we construct an orthonormal Krylov basis and derive the\nLanczos coefficients, which govern the operator connectivity and thus\ncharacterize the complexity. Our study reveals that the Lanczos coefficients\n$\\{b_{n}\\}$ are independent of the chemical potential, while $\\{a_{n}\\}$\nexhibits a dependence on it. Both $\\{a_{n}\\}$ and $\\{b_{n}\\}$ show linear\nrelationships with respect to $n$. For both bosonic and fermionic systems, the\nKrylov complexities behave similarly over time, especially at late times, due\nto the analogous profiles of the squared absolute values of their\nautocorrelation functions $\\abs{\\varphi_{0}(t)}^{2}$. The Krylov complexity\ngrows exponentially with time, but its asymptotic scaling factor $\\lambda_{K}$\nis significantly smaller than the twice of the slope of the $\\{b_{n}\\}$\ncoefficients, contrasting to the relativistic field theories where the scaling\naligns more closely with the twice of the slope of $\\{b_{n}\\}$.\n","versions":"[{'version': 'v1', 'created': 'Mon, 25 Nov 2024 11:38:03 GMT'}, {'version': 'v2', 'created': 'Wed, 27 Nov 2024 14:00:56 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 13:35:32 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['He', 'Peng-Zhang', ''], ['Zhang', 'Hai-Qing', '']]","extracted_entities":"[{'text': 'asymptotic scaling factor', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"asymptotic scaling factor","similarity_score":0.5841006041}
{"id":2411.19951,"submitter":"Shukang Yin","authors":"Shukang Yin, Chaoyou Fu, Sirui Zhao, Yunhang Shen, Chunjiang Ge, Yan\n  Yang, Zuwei Long, Yuhan Dai, Yongdong Luo, Haoyu Cao, Tong Xu, Xing Sun,\n  Caifeng Shan, Ran He, Enhong Chen","title":"Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation","comments":"Project page: https:\/\/github.com\/VITA-MLLM\/Sparrow","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent years have witnessed the success of Multimodal Large Language Models\n(MLLMs) in the vision understanding domain. The success of these models can\nlargely be attributed to the dominant scaling law, which states that larger\nparameter sizes and data volumes contribute to better performance. Notably,\ndata scaling has mainly been powered by automatic data pipelines, which center\naround the self-instruction of LLMs. The paradigm has been taken for granted\nfor quite some time, but the study of the effectiveness of scaling with these\ndata has been neglected for a long time. In this context, this work revisits\nscaling with synthetic data and focuses on developing video-LLMs from a\ndata-centric perspective. Our main study approach is fine-tuning pre-trained\nimage-LLMs with video data and investigating learning efficiency through data\nscaling. Results from our preliminary experiments reveal a low learning\nefficiency phenomenon when simply scaling up video data samples, which, through\nour probing, can be ascribed to a lack of instruction diversity. Aiming at this\nissue, we propose a data augmentation method called Sparrow, which synthesizes\nvideo-like samples from pure text instruction data. Mixing these synthetic\nsamples with the video data enables a more efficient training scheme. Through\ncomprehensive experiments, we demonstrate that our proposed method achieves\nperformance comparable to or even superior to baselines trained with many more\nsamples. Meanwhile, we find that incorporating these synthetic samples can\nboost the performance of long video understanding without training with long\nvideo data. The code and data examples are available at\nhttps:\/\/github.com\/VITA-MLLM\/Sparrow.\n","versions":"[{'version': 'v1', 'created': 'Fri, 29 Nov 2024 18:59:54 GMT'}, {'version': 'v2', 'created': 'Mon, 2 Dec 2024 06:54:47 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Mar 2025 15:44:34 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 08:33:00 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Yin', 'Shukang', ''], ['Fu', 'Chaoyou', ''], ['Zhao', 'Sirui', ''], ['Shen', 'Yunhang', ''], ['Ge', 'Chunjiang', ''], ['Yang', 'Yan', ''], ['Long', 'Zuwei', ''], ['Dai', 'Yuhan', ''], ['Luo', 'Yongdong', ''], ['Cao', 'Haoyu', ''], ['Xu', 'Tong', ''], ['Sun', 'Xing', ''], ['Shan', 'Caifeng', ''], ['He', 'Ran', ''], ['Chen', 'Enhong', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'dominant scaling law', 'label': 'Scaling law'}, {'text': 'VITA-MLLM', 'label': 'Open-source LLMs'}]","assigned_concept":"Scaling law","matched_keyword":"dominant scaling law","similarity_score":0.8317065239}
{"id":2412.01074,"submitter":"Wenchao Ge","authors":"Wenchao Ge, and Kurt Jacobs","title":"Heisenberg-limited continuous-variable distributed quantum metrology\n  with arbitrary weights","comments":"10 pages including supplemental materials, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Distributed quantum metrology (DQM) enables the estimation of global\nfunctions of d distributed parameters beyond the capability of separable\nsensors. Continuous-variable DQM involves using a linear network with at least\none nonclassical input. Here we fully elucidate the structure of linear\nnetworks with two non-vacuum inputs which allows us to prove a number of\nfundamental properties of continuous-variable DQM. While measuring the sum of d\nparameters at the Heisenberg limit can be achieved with a single non-vacuum\ninput, we show that two inputs, one of which can be classical, is required to\nmeasure an arbitrary linear combination of d parameters and an arbitrary global\nfunction of the parameters. We obtain a universal and tight upper bound on the\nsensitivity of DQM networks with two inputs, and completely characterize the\nproperties of the nonclassical input required to obtain a quantum advantage.\nThis reveals that a wide range of nonclassical states make this possible,\nincluding a squeezed vacuum. We also show that for a class of nonclassical\ninputs local photon number detection will achieve the maximum sensitivity.\nFinally we show that a general DQM network has two distinct regimes. The first\nachieves Heisenberg scaling. In the second the nonclassical input is much\nweaker than the coherent input, nevertheless providing a multiplicative\nenhancement to the otherwise classical sensitivity.\n","versions":"[{'version': 'v1', 'created': 'Mon, 2 Dec 2024 03:19:46 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 15:44:21 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Ge', 'Wenchao', ''], ['Jacobs', 'Kurt', '']]","extracted_entities":"[{'text': 'squeezed vacuum', 'label': 'quantisation'}, {'text': 'Heisenberg scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"Heisenberg scaling","similarity_score":0.5351125598}
{"id":2412.02424,"submitter":"Yiqian Luo","authors":"Yiqian Luo, Qiurong Chen, Fali Li, Liang Yi, Peng Xu, Yangsong Zhang","title":"Hierarchical feature extraction on functional brain networks for autism\n  spectrum disorder identification with resting-state fMRI data","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"q-bio.NC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Autism Spectrum Disorder (ASD) is a pervasive developmental disorder of the\ncentral nervous system, primarily manifesting in childhood. It is characterized\nby atypical and repetitive behaviors. Currently, diagnostic methods mainly rely\non questionnaire surveys and behavioral observations, which are prone to\nmisdiagnosis due to their subjective nature. With advancements in medical\nimaging, MR imaging-based diagnostics have emerged as a more objective\nalternative. In this paper, we propose a Hierarchical Neural Network model for\nASD identification, termed ASD-HNet, which hierarchically extracts features\nfrom functional brain networks based on resting-state functional magnetic\nresonance imaging (rs-fMRI) data. This hierarchical approach enhances the\nextraction of brain representations, improving diagnostic accuracy and aiding\nin the identification of brain regions associated with ASD. Specifically,\nfeatures are extracted at three levels: (1) the local region of interest (ROI)\nscale, (2) the community scale, and (3) the global representation scale. At the\nROI scale, graph convolution is employed to transfer features between ROIs. At\nthe community scale, functional gradients are introduced, and a K-Means\nclustering algorithm is applied to group ROIs with similar functional gradients\ninto communities. Features from ROIs within the same community are then\nextracted to characterize the communities. At the global representation scale,\nwe extract global features from the whole community-scale brain networks to\nrepresent the entire brain. We validate the effectiveness of our method using\nthe publicly available Autism Brain Imaging Data Exchange I (ABIDE-I) dataset.\nExperimental results demonstrate that ASD-HNet outperforms existing methods.\nThe code is available at https:\/\/github.com\/LYQbyte\/ASD-HNet.\n","versions":"[{'version': 'v1', 'created': 'Tue, 3 Dec 2024 12:39:09 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 03:17:16 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Luo', 'Yiqian', ''], ['Chen', 'Qiurong', ''], ['Li', 'Fali', ''], ['Yi', 'Liang', ''], ['Xu', 'Peng', ''], ['Zhang', 'Yangsong', '']]","extracted_entities":"[{'text': 'community scale', 'label': 'Scaling law'}, {'text': 'global representation scale', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"community scale","similarity_score":0.5093221664}
{"id":2412.17431,"submitter":"Julie Meunier","authors":"Julie Meunier and Basile Gallet","title":"Effective transport by 2D turbulence: Vortex-gas theory vs.\n  scale-invariant inverse cascade","comments":"6 pages, 3 figures","journal-ref":"Phys.Rev.Lett. 134,074101(2025)","doi":"10.1103\/PhysRevLett.134.074101","report-no":null,"categories":"physics.flu-dyn","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The scale-invariant inverse energy cascade is a hallmark of 2D turbulence,\nwith its theoretical energy spectrum observed in both direct numerical\nsimulations (DNS) and laboratory experiments. Under this scale-invariance\nassumption, the effective diffusivity of a 2D turbulent flow is dimensionally\ncontrolled by the energy flux and the friction coefficient only. Surprisingly,\nhowever, we show that such scaling predictions are invalidated by numerical\nsolutions of the 2D Navier-Stokes equation forced at intermediate wave number\nand damped by weak linear or quadratic drag. We derive alternate scaling-laws\nfor the effective diffusivity based on the emergence of intense, isolated\nvortices causing spatially inhomogeneous frictional dissipation localized\nwithin the small vortex cores. The predictions quantitatively match DNS data.\nThis study points to a universal large-scale organization of 2D turbulent flows\nin physical space, bridging standard 2D Navier-Stokes turbulence with\nlarge-scale geophysical turbulence.\n","versions":"[{'version': 'v1', 'created': 'Mon, 23 Dec 2024 09:50:11 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Feb 2025 08:53:05 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 17:39:15 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Meunier', 'Julie', ''], ['Gallet', 'Basile', '']]","extracted_entities":"[{'text': 'alternate scaling-laws', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"alternate scaling-laws","similarity_score":0.8669427633}
{"id":2502.18372,"submitter":"Darvin Wanisch","authors":"Darvin Wanisch, Nora Reini\\'c, Daniel Jaschke, Simone Montangero and\n  Pietro Silvi","title":"Entanglement transitions in a boundary-driven open quantum many-body\n  system","comments":"7 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We present a numerical framework based on tree tensor operators that enables\nlarge-scale simulation of out-of-equilibrium open quantum many-body systems. By\ndesign, it protects density operator positivity and provides direct access to\nentanglement monotones, such as entanglement of formation and logarithmic\nnegativity. To demonstrate the framework's ability to probe entanglement in\nopen quantum many-body systems and distinguish it from other correlations, we\nstudy a paradigmatic open system problem: the boundary-driven XXZ spin-chain.\nWe uncover entanglement transitions driven by both the coupling to the\nenvironment and the anisotropy parameter. These transitions reveal an immediate\nconnection between entanglement and spin-current, and link the known transport\nregimes of the model to distinct entanglement regimes, i.e., separable,\narea-law, and volume-law. Our work enables the analysis of entanglement in open\nquantum many-body systems out of equilibrium, a necessary step for developing\nscalable quantum technologies.\n","versions":"[{'version': 'v1', 'created': 'Tue, 25 Feb 2025 17:09:13 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 11:35:36 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wanisch', 'Darvin', ''], ['Reini\u0107', 'Nora', ''], ['Jaschke', 'Daniel', ''], ['Montangero', 'Simone', ''], ['Silvi', 'Pietro', '']]","extracted_entities":"[{'text': 'entanglement', 'label': 'quantisation'}, {'text': 'entanglement', 'label': 'quantisation'}, {'text': 'entanglement', 'label': 'quantisation'}, {'text': 'entanglement', 'label': 'quantisation'}, {'text': 'entanglement', 'label': 'quantisation'}, {'text': 'entanglement', 'label': 'quantisation'}, {'text': 'separable', 'label': 'Scaling law'}, {'text': 'area-law', 'label': 'Scaling law'}, {'text': 'volume-law', 'label': 'Scaling law'}, {'text': 'entanglement', 'label': 'quantisation'}]","assigned_concept":"Scaling law","matched_keyword":"volume-law","similarity_score":0.5313993692}
{"id":2502.19584,"submitter":"Mihailo \\v{C}ubrovi\\'c","authors":"Dragan Markovi\\'c, Mihailo \\v{C}ubrovi\\'c","title":"Superdiffusion, normal diffusion and chaos in semiclassical Bose-Hubbard\n  chains","comments":"24 pages, 17 figures; this version: minor corrections and\n  clarifications","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.quant-gas cond-mat.stat-mech nlin.CD","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  We study the evolution of two-point correlation functions of one-dimensional\nBose-Hubbard model in the semiclassical regime in the framework of Truncated\nWigner Approximation (TWA) with quantum jumps as first-order corrections. At\nearly times, the correlation functions show strong superdiffusion with\nuniversal integer exponents determined solely by the initial conditions and\ncompletely insensitive to system parameters and chaos. Only after a long time\nthis regime crosses over to normal diffusion regime which is most robust when\nnonintegrability is strong. For strong nonintegrability, the system ends up in\na homogeneous state while for weak nonintegrability the oscillations and\ninhomogeneities persist, despite the fact that chaos is nearly always strong\nand only weakly depends on nonintegrability parameter. We conclude that the\nsuperidiffusive regime is neither prethermalized nor a precursor to\nthermalization but a novel early-time phenomenon related to a special scaling\nsymmetry of the Bose-Hubbard Hamiltonian.\n","versions":"[{'version': 'v1', 'created': 'Wed, 26 Feb 2025 21:56:04 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 21:44:23 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Markovi\u0107', 'Dragan', ''], ['\u010cubrovi\u0107', 'Mihailo', '']]","extracted_entities":"[{'text': 'quantum jumps', 'label': 'Scaling law'}, {'text': 'special scaling\\nsymmetry', 'label': 'Scaling law'}, {'text': 'Bose-Hubbard Hamiltonian', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"special scaling\nsymmetry","similarity_score":0.5169438124}
{"id":2503.03409,"submitter":"Abdollah Langari","authors":"S. Sadeghizade, R. Jafari and A. Langari","title":"Anti Kibble-Zurek behavior in the quantum XY spin-1\/2 chain driven by\n  correlated noisy magnetic field and anisotropy","comments":"13 pages, 12 figures, References updated","journal-ref":"Phys. Rev. B 111, 104310 (2025)","doi":"10.1103\/PhysRevB.111.104310","report-no":null,"categories":"cond-mat.str-el cond-mat.stat-mech quant-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In the non-adiabatic dynamics across a quantum phase transition, the\nKibble-Zurek paradigm describes that the average number of topological defects\nis suppressed as a universal power law with the quench time scale. A\nconflicting observation, which termed anti-Kibble-Zurek dynamics has been\nreported in several studies, specifically in the driven systems with an\nuncorrelated stochastic (white) noise. Here, we study the defect generation in\nthe driven transverse field\/anisotropy quantum $XY$ model in the presence of a\ncorrelated (colored) Gaussian noise. We propose a generic conjecture that\nproperly capture the noise-induced excitation features, which shows good\nagreement with the numerical simulations. We show that, the dynamical features\nof defect density are modified by varying the noise correlation time. Our\nnumerical simulations confirm that, for fast noises, the dynamics of the defect\ndensity is the same as that of the uncorrelated (white) noise, as is expected.\nHowever, the larger ratio of noise correlation time to the annealing time\nresults in larger defects density formation and reforms the universal dynamical\nfeatures. Our finding reveals that, the noise-induced defects scale linearly\nwith the annealing time for fast noises, while in the presence of the slow\nnoises, the noise-induced defects scale linearly with the square of the\nannealing time. The numerical simulations confirm that, the optimal annealing\ntime, at which the defects density is minimum, scales linearly in logarithmic\nscale with the total noise power having different exponents for the fast and\nslow noises.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 11:35:49 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 17:56:41 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Sadeghizade', 'S.', ''], ['Jafari', 'R.', ''], ['Langari', 'A.', '']]","extracted_entities":"[{'text': 'universal power law', 'label': 'Scaling law'}, {'text': 'logarithmic\\nscale', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"logarithmic\nscale","similarity_score":0.5173587799}
{"id":2503.04715,"submitter":"Qiufeng Wang","authors":"Houyi Li, Wenzhen Zheng, Jingcheng Hu, Qiufeng Wang, Hanshan Zhang,\n  Zili Wang, Shijie Xuyang, Yuantao Fan, Shuigeng Zhou, Xiangyu Zhang, Daxin\n  Jiang","title":"Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large\n  Language Model Pretraining","comments":"22 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well-established, yet their effective deployment necessitates\ncareful hyperparameter optimization. Through extensive empirical studies\ninvolving grid searches across diverse configurations, we discover universal\nscaling laws governing these hyperparameters: optimal learning rate follows a\npower-law relationship with both model parameters and data sizes, while optimal\nbatch size scales primarily with data sizes. Our analysis reveals a convex\noptimization landscape for hyperparameters under fixed models and data size\nconditions. This convexity implies an optimal hyperparameter plateau. We\ncontribute a universal, plug-and-play optimal hyperparameter tool for the\ncommunity. Its estimated values on the test set are merely 0.09% away from the\nglobally optimal LLM performance found via an exhaustive search. These laws\ndemonstrate remarkable robustness across variations in model sparsity, training\ndata distribution, and model shape. To our best known, this is the first work\nthat unifies different model shapes and structures, such as Mixture-of-Experts\nmodels and dense transformers, as well as establishes optimal hyperparameter\nscaling laws across diverse data distributions. This exhaustive optimization\nprocess demands substantial computational resources, utilizing nearly one\nmillion NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and\nhyperparameters from scratch and consuming approximately 100 trillion tokens in\ntotal. To facilitate reproducibility and further research, we will\nprogressively release all loss measurements and model checkpoints through our\ndesignated repository https:\/\/step-law.github.io\/\n","versions":"[{'version': 'v1', 'created': 'Thu, 6 Mar 2025 18:58:29 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Mar 2025 17:59:40 GMT'}, {'version': 'v3', 'created': 'Sat, 15 Mar 2025 17:09:54 GMT'}, {'version': 'v4', 'created': 'Wed, 19 Mar 2025 16:28:25 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Li', 'Houyi', ''], ['Zheng', 'Wenzhen', ''], ['Hu', 'Jingcheng', ''], ['Wang', 'Qiufeng', ''], ['Zhang', 'Hanshan', ''], ['Wang', 'Zili', ''], ['Xuyang', 'Shijie', ''], ['Fan', 'Yuantao', ''], ['Zhou', 'Shuigeng', ''], ['Zhang', 'Xiangyu', ''], ['Jiang', 'Daxin', '']]","extracted_entities":"[{'text': 'universal\\nscaling laws', 'label': 'Scaling law'}, {'text': 'optimal learning rate', 'label': 'Scaling law'}, {'text': 'dense transformers', 'label': 'Transformers'}, {'text': 'optimal hyperparameter\\nscaling laws', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"universal\nscaling laws","similarity_score":0.7954784632}
{"id":2503.12167,"submitter":"Cheng Deng","authors":"Cheng Deng, Luoyang Sun, Jiwen Jiang, Yongcheng Zeng, Xinjian Wu,\n  Wenxin Zhao, Qingfa Xiao, Jiachuan Wang, Haoyang Li, Lei Chen, Lionel M. Ni,\n  Haifeng Zhang, Jun Wang","title":"PLM: Efficient Peripheral Language Models Hardware-Co-Designed for\n  Ubiquitous Computing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  While scaling laws have been continuously validated in large language models\n(LLMs) with increasing model parameters, the inherent tension between the\ninference demands of LLMs and the limited resources of edge devices poses a\ncritical challenge to the development of edge intelligence. Recently, numerous\nsmall language models have emerged, aiming to distill the capabilities of LLMs\ninto smaller footprints. However, these models often retain the fundamental\narchitectural principles of their larger counterparts, still imposing\nconsiderable strain on the storage and bandwidth capacities of edge devices. In\nthis paper, we introduce the PLM, a Peripheral Language Model, developed\nthrough a co-design process that jointly optimizes model architecture and edge\nsystem constraints. The PLM utilizes a Multi-head Latent Attention mechanism\nand employs the squared ReLU activation function to encourage sparsity, thereby\nreducing peak memory footprint during inference. During training, we collect\nand reorganize open-source datasets, implement a multi-phase training strategy,\nand empirically investigate the Warmup-Stable-Decay-Constant (WSDC) learning\nrate scheduler. Additionally, we incorporate Reinforcement Learning from Human\nFeedback (RLHF) by adopting the ARIES preference learning approach. Following a\ntwo-phase SFT process, this method yields performance gains of 2% in general\ntasks, 9% in the GSM8K task, and 11% in coding tasks. In addition to its novel\narchitecture, evaluation results demonstrate that PLM outperforms existing\nsmall language models trained on publicly available data while maintaining the\nlowest number of activated parameters. Furthermore, deployment across various\nedge devices, including consumer-grade GPUs, mobile phones, and Raspberry Pis,\nvalidates PLM's suitability for peripheral applications. The PLM series models\nare publicly available at https:\/\/github.com\/plm-team\/PLM.\n","versions":"[{'version': 'v1', 'created': 'Sat, 15 Mar 2025 15:11:17 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:23:29 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Deng', 'Cheng', ''], ['Sun', 'Luoyang', ''], ['Jiang', 'Jiwen', ''], ['Zeng', 'Yongcheng', ''], ['Wu', 'Xinjian', ''], ['Zhao', 'Wenxin', ''], ['Xiao', 'Qingfa', ''], ['Wang', 'Jiachuan', ''], ['Li', 'Haoyang', ''], ['Chen', 'Lei', ''], ['Ni', 'Lionel M.', ''], ['Zhang', 'Haifeng', ''], ['Wang', 'Jun', '']]","extracted_entities":"[{'text': 'scaling laws', 'label': 'Scaling law'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Multi-head Latent Attention mechanism', 'label': 'Attention mechanism'}, {'text': 'PLM', 'label': 'Large Language Model'}, {'text': 'PLM', 'label': 'Large Language Model'}]","assigned_concept":"Scaling law","matched_keyword":"scaling laws","similarity_score":0.9373526573}
{"id":2503.12709,"submitter":"Sumin Lee","authors":"Sumin Lee, Namwoo Kang","title":"Modular Mechanism Design Optimization in Large-Scale Systems with\n  Manufacturing Cost Considerations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.ET","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Modular design maximizes utility by using standardized components in\nlarge-scale systems. From a manufacturing perspective, it supports green\ntechnology by reducing material waste and improving reusability. Industrially,\nit offers economic benefits through economies of scale, making it a practical\ndesign strategy. Typically, modularization selects a representative design from\npredefined candidates to meet all performance requirements. However, achieving\neffective modularization in mechanical mechanisms presents challenges. First,\nmechanisms depend on geometric relationships for functional motion, and varying\nloads lead to different optimal parameters, complicating representative design\nselection. Second, the chosen design often exceeds optimal parameters, causing\nover-specification and performance deviations, which worsen as scale increases.\nTo address this, we propose a modular mechanism design framework using\nsurrogate-based optimization. This approach finds optimal designs for\nlarge-scale systems and partitions them into groups, each assigned an optimized\ndesign. This multi-objective optimization (MOO) problem balances economies of\nscale and performance consistency. Unlike conventional methods based on\npredefined candidates and simple grouping, our framework optimizes design\nvariables flexibly for modularization. Additionally, we analyze manufacturing\ncost parameters to develop a decision support system for selecting optimal\nstrategies in diverse design scenarios. This enhances maintainability, improves\ninterchangeability, and fosters environmentally sustainable manufacturing.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 00:34:02 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Lee', 'Sumin', ''], ['Kang', 'Namwoo', '']]","extracted_entities":"[{'text': 'economies of scale', 'label': 'Scaling law'}, {'text': 'modularization', 'label': 'quantisation'}, {'text': 'economies of\\nscale', 'label': 'Scaling law'}, {'text': 'modularization', 'label': 'quantisation'}]","assigned_concept":"Scaling law","matched_keyword":"economies of scale","similarity_score":0.5436034799}
{"id":2503.13065,"submitter":"Naslim Neelamkodan","authors":"Batool Ilyasi, Naslim Neelamkodan, Kazuki Tokuda, Susmita Barman,\n  Marta Sewilo, Hidetoshi Sano, and Toshikazu Onishi","title":"CO Observations of the SMC-N66 Hii Region with ALMA: Properties of\n  Clumps along Filamentary Molecular Clouds and Possible Expansion Motion","comments":"Accepted for publication in ApJ","journal-ref":null,"doi":"10.3847\/1538-4357\/adbf16","report-no":null,"categories":"astro-ph.GA","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  The star-forming region N66, as a host of the majority of OB stars in the\nSmall Magellanic Cloud, provides a unique opportunity to enhance our\nunderstanding of the triggers of high-mass star formation. We investigate the\nproperties of the molecular cloud in N66 using the $^{12}$CO(1-0) data obtained\nwith the Atacama Large Millimeter\/submillimeter Array. A cloud decomposition\nanalysis identified 165 independent cloud structures and substructures. The\nsize-linewidth scaling relation for the entire region exhibits an index of\n0.49, indicating that the region is in a state of virial equilibrium. In\ncontrast, a detailed analysis of the central N66 region revealed a\nsize-linewidth scaling relation with an index of 0.75, suggesting that distinct\nfactors are influencing the dynamics of this central area. Averaging the\nspectra in the central N66 region revealed three distinct velocity peaks at\n145, 152, and 160 $\\mathrm{km \\, s^{-1}}$, indicating that some kinds of\ninteractions are occurring within the cloud. The analysis of the\nposition-velocity diagrams in the central region revealed a ring-like\nstructure, indicating the presence of an expanding bubble. The bubble exhibits\nsupersonic characteristics, with an expansion velocity of $v_{\\mathrm{exp}}\n\\approx 11$ $\\mathrm{km \\, s^{-1}}$, and an overall systemic velocity of\n$v_{\\mathrm{sys}}\\approx $ 151 $\\mathrm{km \\, s^{-1}}$. The radius is estimated\nto be in the range of $r \\approx [9.8 - 12.9] \\pm 0.5$ pc and is approximately\n1.2 Myr old.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 11:10:58 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Ilyasi', 'Batool', ''], ['Neelamkodan', 'Naslim', ''], ['Tokuda', 'Kazuki', ''], ['Barman', 'Susmita', ''], ['Sewilo', 'Marta', ''], ['Sano', 'Hidetoshi', ''], ['Onishi', 'Toshikazu', '']]","extracted_entities":"[{'text': 'size-linewidth scaling relation', 'label': 'Scaling law'}, {'text': 'size-linewidth scaling relation', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"size-linewidth scaling relation","similarity_score":0.5475364923}
{"id":2503.13266,"submitter":"Henrique de Oliveira","authors":"H. P. de Oliveira","title":"Distorted black holes: a characteristic view","comments":"9 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"gr-qc","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We investigate the interaction between a non-rotating black hole and incoming\ngravitational waves using the characteristic formulation of the Einstein field\nequations, framed as a Bondi problem. By adopting retarded time as the null\ncoordinate and recognizing that the final state is invariably a black hole, we\ndemonstrate that an apparent horizon forms once sufficient mass accretes onto\nthe black hole. We derive the evolution of the Bondi mass and compute its final\nvalue, enabling us to quantify the fraction of the incident mass absorbed by\nthe black hole. Additionally, we establish a scaling law for the absorbed mass\nas a function of initial parameters, such as the amplitude of the gravitational\nwave data. Furthermore, we explore the dynamics when a reflecting barrier\nsurrounds the black hole. For low-amplitude initial waves, the barrier reflects\nthe waves, leaving the original black hole as the end state. Conversely,\nhigh-amplitude waves lead to the formation of an apparent horizon that engulfs\nthe barrier, producing a new, larger black hole.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:19:54 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['de Oliveira', 'H. P.', '']]","extracted_entities":"[{'text': 'scaling law', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling law","similarity_score":1.0000001192}
{"id":2503.13306,"submitter":"Catalin-Mihai Halati","authors":"Catalin-Mihai Halati, Ameneh Sheikhan, Giovanna Morigi, Corinna\n  Kollath, Simon B. J\\\"ager","title":"From Light-Cone to Supersonic Propagation of Correlations by Competing\n  Short- and Long-Range Couplings","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.quant-gas cond-mat.str-el quant-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We investigate the dynamical spreading of correlations in many-body quantum\nsystems with competing short- and global-range couplings. We monitor the\nnon-equilibrium dynamics of the correlations following a quench, showing that\nfor strong short-range couplings the propagation of correlations is dominated\nat short and intermediate distances by a causal, light-cone, dynamics,\nresembling the purely short-range quantum systems. However, the interplay of\nshort- and global-range couplings leads to a crossover between space-time\nregions in which the light-cone persists to regions where a supersonic,\ndistance-independent, spreading of the correlations occurs. We identify the\nimportant ingredients needed for capturing the supersonic spreading and\ndemonstrate our findings in systems of interacting bosonic atoms, in which the\nglobal range coupling is realized by a coupling to a cavity light field, or\natomic long-range interactions, respectively. We show that our results hold in\nboth one and two dimensions and in the presence of dissipation. Furthermore, we\ncharacterize the short time power-law scaling of the distance-independent\ngrowth of the density-density correlations.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:48:36 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Halati', 'Catalin-Mihai', ''], ['Sheikhan', 'Ameneh', ''], ['Morigi', 'Giovanna', ''], ['Kollath', 'Corinna', ''], ['J\u00e4ger', 'Simon B.', '']]","extracted_entities":"[{'text': 'short time power-law scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"short time power-law scaling","similarity_score":0.6796916723}
{"id":2503.13361,"submitter":"Martin Venker","authors":"Fabrice Gamboa and Martin Venker","title":"Limit Theorems Under Several Linear Constraints","comments":"25 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We study vectors chosen at random from a compact convex polytope in\n$\\mathbb{R}^n$ given by a finite number of linear constraints. We determine\nwhich projections of these random vectors are asymptotically normal as\n$n\\to\\infty$. Marginal distributions are also studied, showing that in the\nlarge $n$ limit random variables under linear constraints become i.i.d.\nexponential under a rescaling. Our novel approach is based on a complex de\nFinetti theorem revealing an underlying independence structure as well as on\nentropy arguments.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 16:47:17 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Gamboa', 'Fabrice', ''], ['Venker', 'Martin', '']]","extracted_entities":"[{'text': 'rescaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"rescaling","similarity_score":0.5139684081}
{"id":2503.13424,"submitter":"Xinyu Lian","authors":"Xinyu Lian, Zichao Yu, Ruiming Liang, Yitong Wang, Li Ray Luo, Kaixu\n  Chen, Yuanzhen Zhou, Qihong Tang, Xudong Xu, Zhaoyang Lyu, Bo Dai, Jiangmiao\n  Pang","title":"Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated\n  Objects via Procedural Generation","comments":"Project page: https:\/\/infinite-mobility.github.io 10 pages,12 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large-scale articulated objects with high quality are desperately needed for\nmultiple tasks related to embodied AI. Most existing methods for creating\narticulated objects are either data-driven or simulation based, which are\nlimited by the scale and quality of the training data or the fidelity and heavy\nlabour of the simulation. In this paper, we propose Infinite Mobility, a novel\nmethod for synthesizing high-fidelity articulated objects through procedural\ngeneration. User study and quantitative evaluation demonstrate that our method\ncan produce results that excel current state-of-the-art methods and are\ncomparable to human-annotated datasets in both physics property and mesh\nquality. Furthermore, we show that our synthetic data can be used as training\ndata for generative models, enabling next-step scaling up. Code is available at\nhttps:\/\/github.com\/Intern-Nexus\/Infinite-Mobility\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:53:56 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Lian', 'Xinyu', ''], ['Yu', 'Zichao', ''], ['Liang', 'Ruiming', ''], ['Wang', 'Yitong', ''], ['Luo', 'Li Ray', ''], ['Chen', 'Kaixu', ''], ['Zhou', 'Yuanzhen', ''], ['Tang', 'Qihong', ''], ['Xu', 'Xudong', ''], ['Lyu', 'Zhaoyang', ''], ['Dai', 'Bo', ''], ['Pang', 'Jiangmiao', '']]","extracted_entities":"[{'text': 'next-step scaling up', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"next-step scaling up","similarity_score":0.5149775743}
{"id":2503.13599,"submitter":"Masahiro Hoshino","authors":"Masahiro Hoshino, Masaki Oshikawa, Yuto Ashida","title":"Stabilizer R\\'enyi Entropy and Conformal Field Theory","comments":"29 pages, 14 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.stat-mech","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Understanding universal aspects of many-body systems is one of the central\nthemes in modern physics. Recently, the stabilizer R\\'{e}nyi entropy (SRE) has\nemerged as a computationally tractable measure of nonstabilizerness, a crucial\nresource for fault-tolerant universal quantum computation. While numerical\nresults suggested that the SRE in critical states can exhibit universal\nbehavior, its comprehensive theoretical understanding has remained elusive. In\nthis work, we develop a field-theoretical framework for the SRE in a\n$(1+1)$-dimensional many-body system and elucidate its universal aspects using\nboundary conformal field theory. We demonstrate that the SRE is equivalent to a\nparticipation entropy in the Bell basis of a doubled Hilbert space, which can\nbe calculated from the partition function of a replicated field theory with the\ninterlayer line defect created by the Bell-state measurements. This\nidentification allows us to characterize the universal contributions to the SRE\non the basis of the data of conformal boundary conditions imposed on the\nreplicated theory. We find that the SRE of the entire system contains a\nuniversal size-independent term determined by the noninteger ground-state\ndegeneracy known as the g-factor. In contrast, we show that the mutual SRE\nexhibits the logarithmic scaling with a universal coefficient given by the\nscaling dimension of a boundary condition changing operator, which elucidates\nthe origin of universality previously observed in numerical results. As a\nconcrete demonstration, we present a detailed analysis of the Ising\ncriticality, where we analytically derive the universal quantities at arbitrary\nR\\'{e}nyi indices and numerically validate them with high accuracy by employing\ntensor network methods. These results establish a field-theoretical approach to\nunderstanding the universal features of nonstabilizerness in quantum many-body\nsystems.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 18:00:02 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Hoshino', 'Masahiro', ''], ['Oshikawa', 'Masaki', ''], ['Ashida', 'Yuto', '']]","extracted_entities":"[{'text': 'logarithmic scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"logarithmic scaling","similarity_score":0.6116999388}
{"id":2503.13603,"submitter":"Jo\\~ao Silva Mr.","authors":"Jo\\~ao Barata, Ian Moult, Andrey V. Sadofyev, Jo\\~ao M. Silva","title":"Dissecting Jet Modification in the QGP with Multi-Point Energy\n  Correlators","comments":"10 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph nucl-th","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Energy correlators have recently attracted significant attention in the study\nof heavy ion collisions due to their potential to robustly connect experimental\nmeasurements with an underlying quantum field theoretic description. While\ntheoretical studies have so far primarily focused on the simplest two-point\ncorrelator, mapping out the dynamics of the quark-gluon plasma (QGP) will\nrequire developing a theoretical understanding of multi-point energy\ncorrelators. In this paper we present a systematic theoretical study of\nmulti-point energy correlators for jets fragmenting in a dense quark-gluon\nplasma, accounting for both the medium's perturbative modification to the jet,\nand its hydrodynamical back-reaction. We consider both the scaling behavior of\nprojected correlators, as well as the shape dependent three-point correlator,\nhighlighting how both provide insight into interactions with the QGP. We\ndiscuss the parametric dependence of modifications on the medium scales,\nopening new opportunities to experimentally separate jet modifications from the\nmedium response. Our results open the door to a systematic exploration of\nmulti-point energy correlators in heavy ion collisions.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 18:00:06 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Barata', 'Jo\u00e3o', ''], ['Moult', 'Ian', ''], ['Sadofyev', 'Andrey V.', ''], ['Silva', 'Jo\u00e3o M.', '']]","extracted_entities":"[{'text': 'scaling behavior', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling behavior","similarity_score":0.7924405336}
{"id":2503.13659,"submitter":"Ertiza Hossain Shopnil","authors":"Ertiza Hossain Shopnil, Md Nadeem Azad, Jahid Emon","title":"Nanodroplet Dynamics: Coalescence and Impact","comments":"Corresponding Author: Dr. AKM Monjur Morshed (Bangladesh University\n  of Engineering and technology, BUET, Dhaka, Bangladesh)","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.flu-dyn","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This study aims to investigate the coalescence-induced jumping of water\nnanodroplets in a high Ohnesorge number regime (0.4 < Oh < 1) on a\nsuperhydrophobic surface and the dynamics of droplets when a stationary droplet\non a solid surface is struck by another droplet of similar size from above,\nusing molecular dynamics simulation. The first part of this study identified\nthe critical droplet size below which coalescence-induced jumping terminates,\ndeveloped a universal jumping mechanism for droplets of all types, explained a\nspecial phenomenon of jumping velocity becoming maximum before it approaches\nzero, and investigated how jumping terminates due to the size difference\nbetween droplets. These findings align well with prior micro-level studies and\nexperimental predictions. The second part of this study investigated the\njumping process of the merged droplet after the impact of a moving droplet upon\na stationary one. The impact velocity, droplet size, surface textures, and\nwettability are influential factors on the jumping velocity in this case.\nScaling laws for maximum spreading time, spreading factor, and restitution\ncoefficient are formulated based on the Weber (We) number and the Reynolds (Re)\nnumber. These laws differ from those established for single-droplet impacts.\nFor superhydrophobic surfaces, the spreading time is approximated by three\ntimes the droplet radius and impact velocity, and with the dimensionless\nspreading time, it exhibits a linear relationship with We 0.31. For both cases,\nthe jumping process is primarily governed by the energy available for\nconversion into the kinetic energy of the merged droplet following dissipation.\nFor the droplet impact case, the energy conversion efficiency becomes constant\nat high-impact droplet velocities. About 1% of the energy is dissipated due to\nsurface adhesion, which reduces at higher impact velocity.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 19:07:27 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Shopnil', 'Ertiza Hossain', ''], ['Azad', 'Md Nadeem', ''], ['Emon', 'Jahid', '']]","extracted_entities":"[{'text': 'Scaling laws', 'label': 'Scaling law'}, {'text': 'restitution\\ncoefficient', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"Scaling laws","similarity_score":0.9373526573}
{"id":2503.13705,"submitter":"Kathleen West","authors":"Kathleen West, Fabian Lehmann, Vasilis Bountris, Ulf Leser, Yehia\n  Elkhatib, Lauritz Thamsen","title":"Exploring the Potential of Carbon-Aware Execution for Scientific\n  Workflows","comments":"To appear in the Proceedings of the 25th IEEE International Symposium\n  on Cluster, Cloud and Internet Computing (CCGrid). Updated with reformatted\n  CC BY footer","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Scientific workflows are widely used to automate scientific data analysis and\noften involve processing large quantities of data on compute clusters. As such,\ntheir execution tends to be long-running and resource intensive, leading to\nsignificant energy consumption and carbon emissions.\n  Meanwhile, a wealth of carbon-aware computing methods have been proposed, yet\nlittle work has focused specifically on scientific workflows, even though they\npresent a substantial opportunity for carbon-aware computing because they are\ninherently delay tolerant, efficiently interruptible, and highly scalable.\n  In this study, we demonstrate the potential for carbon-aware workflow\nexecution. For this, we estimate the carbon footprint of two real-world\nNextflow workflows executed on cluster infrastructure. We use a linear power\nmodel for energy consumption estimates and real-world average and marginal CI\ndata for two regions. We evaluate the impact of carbon-aware temporal shifting,\npausing and resuming, and resource scaling. Our findings highlight significant\npotential for reducing emissions of workflows and workflow tasks.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 20:24:20 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 09:48:27 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['West', 'Kathleen', ''], ['Lehmann', 'Fabian', ''], ['Bountris', 'Vasilis', ''], ['Leser', 'Ulf', ''], ['Elkhatib', 'Yehia', ''], ['Thamsen', 'Lauritz', '']]","extracted_entities":"[{'text': 'resource scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"resource scaling","similarity_score":0.5480554104}
{"id":2503.13755,"submitter":"Anirban Bairagi","authors":"Anirban Bairagi, Benjamin Wandelt, Francisco Villaescusa-Navarro","title":"How many simulations do we need for simulation-based inference in\n  cosmology?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.CO stat.ML","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  How many simulations do we need to train machine learning methods to extract\ninformation available from summary statistics of the cosmological density\nfield? Neural methods have shown the potential to extract non-linear\ninformation available from cosmological data. Success depends critically on\nhaving sufficient simulations for training the networks and appropriate network\narchitectures. In the first detailed convergence study of neural network\ntraining for cosmological inference, we show that currently available\nsimulation suites, such as the Quijote Latin Hypercube(LH) with 2000\nsimulations, do not provide sufficient training data for a generic neural\nnetwork to reach the optimal regime, even for the dark matter power spectrum,\nand in an idealized case. We discover an empirical neural scaling law that\npredicts how much information a neural network can extract from a highly\ninformative summary statistic, the dark matter power spectrum, as a function of\nthe number of simulations used to train the network, for a wide range of\narchitectures and hyperparameters. We combine this result with the Cramer-Rao\ninformation bound to forecast the number of training simulations needed for\nnear-optimal information extraction. To verify our method we created the\nlargest publicly released simulation data set in cosmology, the Big Sobol\nSequence(BSQ), consisting of 32,768 $\\Lambda$CDM n-body simulations uniformly\ncovering the $\\Lambda$CDM parameter space. Our method enables efficient\nplanning of simulation campaigns for machine learning applications in\ncosmology, while the BSQ dataset provides an unprecedented resource for\nstudying the convergence behavior of neural networks in cosmological parameter\ninference. Our results suggest that new large simulation suites or new training\napproaches will be necessary to achieve information-optimal parameter inference\nfrom non-linear simulations.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 22:21:39 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Bairagi', 'Anirban', ''], ['Wandelt', 'Benjamin', ''], ['Villaescusa-Navarro', 'Francisco', '']]","extracted_entities":"[{'text': 'Quijote Latin Hypercube(LH)', 'label': 'Large Language Model'}, {'text': 'empirical neural scaling law', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"empirical neural scaling law","similarity_score":0.6854137182}
{"id":2503.13832,"submitter":"Yun-Jie Wang","authors":"Yun-Jie Wang, Tai-Ping Sun, Xi-Ning Zhuang, Xiao-Fan Xu, Huan-Yu Liu,\n  Cheng Xue, Yu-Chun Wu, Zhao-Yun Chen, Guo-Ping Guo","title":"Efficient Classical Simulation of the Quantum Random Access Memory","comments":"11 pages + 6 pages of appendices, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Quantum Random Access Memory (QRAM), despite its fundamental role in quantum\ninformation processing, has yet to be experimentally realized. Given this\nchallenge, classical simulations serve as essential tools for gaining deeper\ninsights into the underlying physical mechanisms of quantum systems and for\nadvancing the development of scalable, and reliable quantum devices. However,\ngeneral-purpose simulation methods become impractical due to exponential memory\ngrowth and rising computational costs, leaving an open gap for tailored\napproaches to simulate QRAM efficiently. Here, we present an efficient and\nscalable framework for simulating both noiseless and noisy bucket brigade QRAM\ncircuits. Our approach leverages a branch-wise encoding scheme, structuring\nsimulations around query branches rather than full circuits. This encoding\nenables layer-wise operations and pruning algorithms, significantly improving\nmemory efficiency and simulation speed, particularly in noisy scenarios. As a\nresult, our framework achieves linear computational scaling in the noiseless\ncase and maintains stable performance under realistic noise conditions. On a\nsingle workstation, we successfully simulate a 20-layer full-address QRAM in\nunder two hours using less than 1 GB of memory. Additionally, we extend noisy\nQRAM simulations to 30 layers with $2^{10}$ branches, surpassing the scale of\npreviously reported QRAM simulations. Furthermore, we demonstrate the\nintegration of our simulator with currently existing quantum simulators by\nincorporating error filtration techniques to explore noise suppression\nstrategies in QRAM architectures. These results critically assess QRAM's\nfeasibility, highlighting both its potential and limitations in noise\nsuppression, while positioning our simulator as a key tool for evaluating\nfuture QRAM-based algorithms and error suppression strategies.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 02:14:17 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wang', 'Yun-Jie', ''], ['Sun', 'Tai-Ping', ''], ['Zhuang', 'Xi-Ning', ''], ['Xu', 'Xiao-Fan', ''], ['Liu', 'Huan-Yu', ''], ['Xue', 'Cheng', ''], ['Wu', 'Yu-Chun', ''], ['Chen', 'Zhao-Yun', ''], ['Guo', 'Guo-Ping', '']]","extracted_entities":"[{'text': 'linear computational scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"linear computational scaling","similarity_score":0.5958986282}
{"id":2503.13922,"submitter":"William Porteous","authors":"William Porteous, Irene M. Gamba, Kun Huang","title":"Existence and Regularizing Effects of a Nonlinear Diffusion Model for\n  Plasma Instabilities","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP math-ph math.MP physics.plasm-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We study existence and regularity of weak solutions to a nonlinear parabolic\nDirichlet problem $\\partial_{t}u - \\rho_{\\lambda}(x)u\\Delta u =\n\\rho_{\\lambda}(x)g_{0}(x)u$ on the half line $(0,\\infty)$. We find weak\nsolutions from $L^p\\ (p < \\infty)$ initial data, and by means of a\nBenilan-Crandall inequality, show solutions are jointly Holder continuous, and\nlocally, spatially Lipschitz on the parabolic interior. We identify special\nsolutions which saturate these bounds. The Benilan-Crandall inequality, derived\nfrom time-scaling arguments, is of independent interest for exposing a\nregularizing effect of the parabolic u$\\Delta$u operator. Recently considered\nin [11], this problem originates in the theory of nonlinear instability damping\nvia wave-particle interactions in plasma physics (see [8, 22]).\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 05:37:45 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Porteous', 'William', ''], ['Gamba', 'Irene M.', ''], ['Huang', 'Kun', '']]","extracted_entities":"[{'text': 'Benilan-Crandall inequality', 'label': 'Scaling law'}, {'text': 'time-scaling arguments', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"time-scaling arguments","similarity_score":0.5104538798}
{"id":2503.14078,"submitter":"David Criens","authors":"Alexis Anagnostakis, David Criens, Mikhail Urusov","title":"On weak notions of no-arbitrage in a 1D general diffusion market with\n  interest rates","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"q-fin.MF math.PR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We establish deterministic necessary and sufficient conditions for the\nno-arbitrage notions \"no increasing profit\" (NIP), \"no strong arbitrage\" (NSA)\nand \"no unbounded profit with bounded risk\" (NUPBR) in one-dimensional general\ndiffusion markets. These are markets with one risky asset, which is modeled as\na regular continuous strong Markov process that is also a semimartingale, and a\nriskless asset that grows exponentially at a constant rate $r\\in \\mathbb{R}$.\nAll deterministic criteria are provided in terms of the scale function and the\nspeed measure of the risky asset process. Our study reveals a variety of\nsurprising effects. For instance, irrespective of the interest rate, NIP is not\nexcluded by reflecting boundaries or an irregular scale function. In the case\nof non-zero interest rates, it is even possible that NUPBR holds in the\npresence of reflecting boundaries and\/or skew thresholds. In the zero interest\nrate regime, we also identify NSA as the minimal no arbitrage notion that\nexcludes reflecting boundaries and that forces the scale function to be\ncontinuously differentiable with strictly positive absolutely continuous\nderivative, meaning that it is of the same form as for a stochastic\ndifferential equation.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 09:55:39 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Anagnostakis', 'Alexis', ''], ['Criens', 'David', ''], ['Urusov', 'Mikhail', '']]","extracted_entities":"[{'text': 'semimartingale', 'label': 'Scaling law'}, {'text': 'scale function', 'label': 'Scaling law'}, {'text': 'scale function', 'label': 'Scaling law'}, {'text': 'scale function', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scale function","similarity_score":0.5581804514}
{"id":2503.14384,"submitter":"Shuoguang Liu","authors":"Shuoguang Liu, Ryo Hanai, Peter B. Littlewood","title":"Universal scaling in one-dimensional non-reciprocal matter","comments":"12 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.stat-mech cond-mat.soft cond-mat.str-el","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Unveiling universal non-equilibrium scaling laws has been a central theme in\nmodern statistical physics, with recent attention increasingly directed toward\nnonequilibrium phases that exhibit rich dynamical phenomena. A striking example\narises in nonreciprocal systems, where asymmetric interactions between\ncomponents lead to inherently dynamic phases and unconventional criticality\nnear a critical exceptional point (CEP), where the criticality arises from the\ncoalescence of collective modes to the Nambu-Goldstone mode. However, the\nuniversal scaling behavior that should emerge in this system with full\nconsideration of many-body effects and stochastic noise remains largely\nelusive. Here, we establish a dynamical scaling law in a generic\none-dimensional stochastic nonreciprocal $O(2)$-symmetric system. Through\nlarge-scale simulations, we uncover a new nonequilibrium scaling in the\nvicinity of transition, distinct from any previously known equilibrium or\nnonequilibrium universality classes. In regimes where the system breaks into\ndomains with opposite chirality, we demonstrate that fluctuations are strongly\nsuppressed, leading to a logarithmic scaling as a function of system size $L$,\nin contrast to the conventional power-law scaling expected from dynamical\nscaling theory. This work elucidates the beyond-mean-field dynamics of\nnon-reciprocal matter, thereby sheding light on the exploration of criticality\nin nonreciprocal phase transition across diverse physical contexts, from active\nmatter and driven quantum systems to biological pattern formation and\nnon-Hermitian physics.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:20:40 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Liu', 'Shuoguang', ''], ['Hanai', 'Ryo', ''], ['Littlewood', 'Peter B.', '']]","extracted_entities":"[{'text': 'dynamical scaling law', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"dynamical scaling law","similarity_score":0.8182498217}
{"id":2503.14559,"submitter":"Weixiong Lin","authors":"Weixiong Lin, Chen Ju, Haicheng Wang, Shengchao Hu, Shuai Xiao,\n  Mengting Chen, Yuheng Jiao, Mingshuai Yao, Jinsong Lan, Qingwen Liu, Ying\n  Chen","title":"Squeeze Out Tokens from Sample for Finer-Grained Data Governance","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Widely observed data scaling laws, in which error falls off as a power of the\ntraining size, demonstrate the diminishing returns of unselective data\nexpansion. Hence, data governance is proposed to downsize datasets through\npruning non-informative samples. Yet, isolating the impact of a specific sample\non overall model performance is challenging, due to the vast computation\nrequired for tryout all sample combinations. Current data governors circumvent\nthis complexity by estimating sample contributions through heuristic-derived\nscalar scores, thereby discarding low-value ones. Despite thorough sample\nsieving, retained samples contain substantial undesired tokens intrinsically,\nunderscoring the potential for further compression and purification. In this\nwork, we upgrade data governance from a 'sieving' approach to a 'juicing' one.\nInstead of scanning for least-flawed samples, our dual-branch DataJuicer\napplies finer-grained intra-sample governance. It squeezes out informative\ntokens and boosts image-text alignments. Specifically, the vision branch\nretains salient image patches and extracts relevant object classes, while the\ntext branch incorporates these classes to enhance captions. Consequently,\nDataJuicer yields more refined datasets through finer-grained governance.\nExtensive experiments across datasets demonstrate that DataJuicer significantly\noutperforms existing DataSieve in image-text retrieval, classification, and\ndense visual reasoning.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 04:06:50 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Lin', 'Weixiong', ''], ['Ju', 'Chen', ''], ['Wang', 'Haicheng', ''], ['Hu', 'Shengchao', ''], ['Xiao', 'Shuai', ''], ['Chen', 'Mengting', ''], ['Jiao', 'Yuheng', ''], ['Yao', 'Mingshuai', ''], ['Lan', 'Jinsong', ''], ['Liu', 'Qingwen', ''], ['Chen', 'Ying', '']]","extracted_entities":"[{'text': 'data scaling laws', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"data scaling laws","similarity_score":0.7494747639}
{"id":2503.14652,"submitter":"Duarte Miguel Da Silva Feiteira","authors":"Duarte Feiteira (1) and Oleg Lebedev (1) ((1) Department of Physics\n  and Helsinki Institute of Physics, Helsinki, Finland)","title":"Cosmological gravitational particle production: Starobinsky vs\n  Bogolyubov, uncertainties, and issues","comments":"31 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph astro-ph.CO hep-th","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We study production of free and feebly interacting scalars during inflation\nusing the Bogolyubov coefficient and Starobinsky stochastic approaches. While\nthe two methods agree in the limit of infinitely long inflation, the\nStarobinsky approach is more suitable for studying realistic situations, where\nthe duration of inflation is finite and the scalar field has non-trivial\ninitial conditions. We find that the abundance of produced particles is\nsensitive to pre-inflationary initial conditions, resulting in the uncertainty\nof many orders of magnitude. Nevertheless, a lower bound on the particle\nabundance can be obtained. High scale inflation is very efficient in particle\nproduction, which leads to strong constraints on the existence of stable\nscalars with masses below the inflationary Hubble rate. For example, free\nstable scalars are allowed only if they have masses below an eV or the\nreheating temperature is in the GeV range or below. We find universal scaling\nbehavior of the particle abundance, which covers free and feebly interacting\nscalars as well as those with a small non-minimal coupling to gravity. These\nconsiderations are important in the context of non-thermal dark matter since\ninflationary particle production provides an irreducible background for other\nproduction mechanisms.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 19:00:02 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Feiteira', 'Duarte', ''], ['Lebedev', 'Oleg', '']]","extracted_entities":"[{'text': 'universal scaling\\nbehavior', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"universal scaling\nbehavior","similarity_score":0.6595401168}
{"id":2503.14762,"submitter":"Mateusz Dyksik","authors":"Mateusz Dyksik, Michal Baranowski, Joshua J. P. Thompson, Zhuo Yang,\n  Martha Rivera Medina, Maria Antonietta Loi, Ermin Malic, Paulina Plochocka","title":"Steric Engineering of Exciton Fine Structure in 2D Perovskites","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci cond-mat.mes-hall","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  A comprehensive study of excitonic properties of 2D layered perovskites is\nprovided, with an emphasis on understanding and controlling the exciton fine\nstructure. First, an overview of the optical properties is presented,\ndiscussing the challenges in determining the bandgap and exciton binding\nenergies. Through magneto-optical spectroscopic measurements (up to B = 140 T),\nscaling laws are established for exciton binding energy as a function of the\nband gap and the diamagnetic coefficient. Using an in-plane magnetic field, the\nexciton fine structure for various 2D perovskites is examined to measure the\nenergy splitting between the excitonic levels. The exciton fine structure and\nexchange interaction are correlated with structural parameters, employing an\neffective mass model, to highlight the role of steric effect on the exchange\ninteraction. These findings reveal that lattice distortions, introduced by\norganic spacers, significantly influence the exchange interaction, driving a\ntunable energy spacing between dark and bright excitons. This unique feature of\n2D perovskites, not present in other semiconductors, offers a novel tuning\nmechanism for exciton control, making these materials highly promising for\nefficient light emitters and advanced quantum technologies.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 22:18:41 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Dyksik', 'Mateusz', ''], ['Baranowski', 'Michal', ''], ['Thompson', 'Joshua J. P.', ''], ['Yang', 'Zhuo', ''], ['Medina', 'Martha Rivera', ''], ['Loi', 'Maria Antonietta', ''], ['Malic', 'Ermin', ''], ['Plochocka', 'Paulina', '']]","extracted_entities":"[{'text': 'scaling laws', 'label': 'Scaling law'}, {'text': 'novel tuning\\nmechanism', 'label': 'Fine-tuning'}]","assigned_concept":"Scaling law","matched_keyword":"scaling laws","similarity_score":0.9373526573}
{"id":2503.14786,"submitter":"Haiyang Ying","authors":"Haiyang Ying, Matthias Zwicker","title":"SketchSplat: 3D Edge Reconstruction via Differentiable Multi-view Sketch\n  Splatting","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Edges are one of the most basic parametric primitives to describe structural\ninformation in 3D. In this paper, we study parametric 3D edge reconstruction\nfrom calibrated multi-view images. Previous methods usually reconstruct a 3D\nedge point set from multi-view 2D edge images, and then fit 3D edges to the\npoint set. However, noise in the point set may cause gaps among fitted edges,\nand the recovered edges may not align with input multi-view images since the\nedge fitting depends only on the reconstructed 3D point set. To mitigate these\nproblems, we propose SketchSplat, a method to reconstruct accurate, complete,\nand compact 3D edges via differentiable multi-view sketch splatting. We\nrepresent 3D edges as sketches, which are parametric lines and curves defined\nby attributes including control points, scales, and opacity. During edge\nreconstruction, we iteratively sample Gaussian points from a set of sketches\nand rasterize the Gaussians onto 2D edge images. Then the gradient of the image\nerror with respect to the input 2D edge images can be back-propagated to\noptimize the sketch attributes. Our method bridges 2D edge images and 3D edges\nin a differentiable manner, which ensures that 3D edges align well with 2D\nimages and leads to accurate and complete results. We also propose a series of\nadaptive topological operations and apply them along with the sketch\noptimization. The topological operations help reduce the number of sketches\nrequired while ensuring high accuracy, yielding a more compact reconstruction.\nFinally, we contribute an accurate 2D edge detector that improves the\nperformance of both ours and existing methods. Experiments show that our method\nachieves state-of-the-art accuracy, completeness, and compactness on a\nbenchmark CAD dataset.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 23:30:03 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Ying', 'Haiyang', ''], ['Zwicker', 'Matthias', '']]","extracted_entities":"[{'text': 'scales', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scales","similarity_score":0.5696527362}
{"id":2503.14792,"submitter":"Carlos Mejia-Monasterio","authors":"Luisana Claudio-Pachecano and Hern\\'an Larralde and Carlos\n  Mej\\'ia-Monasterio","title":"From anomalous diffusion in polygons to a transport locking relation","comments":"15 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.stat-mech nlin.CD","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We study particle transport in a class of open channels of finite length,\nmade of identical cells of connected open polygonal billiards with parallel\nboundaries. In these systems the Mean Square Displacement (MSD) grows in time\nfaster than linearly. We show that irrespective of the geometry of these\nchannels, the distribution of the first return times decays algebraically with\ntwo different exponents, separated by a crossover region that is determined by\nthe MSD. We find that the distribution of first return times satisfies a simple\nscaling form. In turn, the transmission coefficient, defined as the fraction of\ntrajectories that starting at the cell at the origin escape the channel through\nthe other boundary, decays algebraically with the size of the system, and, as a\nsignature of non-recurrent transport, sometimes slower. From these two\nprocesses we derive a locking relation among the scaling exponents for the\nasymptotic behavior of the MSD, the times of first return to the origin and the\nway transmission decays with the system size, showing that these three\nprocesses are interdependent. The locking relation holds for diffusive\nprocesses, as well as for fractional Brownian motion with arbitrary Hurst\nexponent. We argue that the locking relation may be valid for many other\ntransport processes, Markovian or not, with finite MSD.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 23:45:18 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Claudio-Pachecano', 'Luisana', ''], ['Larralde', 'Hern\u00e1n', ''], ['Mej\u00eda-Monasterio', 'Carlos', '']]","extracted_entities":"[{'text': 'simple\\nscaling form', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"simple\nscaling form","similarity_score":0.6477457285}
{"id":2503.14823,"submitter":"Mitsuo Oka","authors":"Mitsuo Oka, Tai D. Phan, Marit {\\O}ieroset, Daniel J. Gershman, Roy B.\n  Torbert, James L. Burch, and Vassilis Angelopoulos","title":"Scaling of Particle Heating in Shocks and Magnetic Reconnection","comments":"15 pages, 8 figures; accepted for publication in Astrophysical\n  Journal","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.plasm-ph physics.space-ph","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Particles are heated efficiently through energy conversion processes such as\nshocks and magnetic reconnection in collisionless plasma environments. While\nempirical scaling laws for the temperature increase have been obtained, the\nprecise mechanism of energy partition between ions and electrons remains\nunclear. Here we show, based on coupled theoretical and observational scaling\nanalyses, that the temperature increase, $\\Delta T$, depends linearly on three\nfactors: the available magnetic energy per particle, the Alfv\\'{e}n Mach number\n(or reconnection rate), and the characteristic spatial scale $L$. Based on\nstatistical datasets obtained from Earth's plasma environment, we find that $L$\nis on the order of (1) the ion gyro-radius for ion heating at shocks, (2) the\nion inertial length for ion heating in magnetic reconnection, and (3) the\nhybrid inertial length for electron heating in both shocks and magnetic\nreconnection. With these scales, we derive the ion-to-electron ratios of\ntemperature increase as $\\Delta T_{\\rm i}\/\\Delta T_{\\rm e} = (3\\beta_{\\rm\ni}\/2)^{1\/2}(m_{\\rm i}\/m_{\\rm e})^{1\/4}$ for shocks and $\\Delta T_{\\rm i}\/\\Delta\nT_{\\rm e} = (m_{\\rm i}\/m_{\\rm e})^{1\/4}$ for magnetic reconnection, where\n$\\beta_{\\rm i}$ is the ion plasma beta, and $m_{\\rm i}$ and $ m_{\\rm e}$ are\nthe ion and electron particle masses, respectively. We anticipate that this\nstudy will serve as a starting point for a better understanding of particle\nheating in space plasmas, enabling more sophisticated modeling of its scaling\nand universality.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 01:43:10 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Oka', 'Mitsuo', ''], ['Phan', 'Tai D.', ''], ['\u00d8ieroset', 'Marit', ''], ['Gershman', 'Daniel J.', ''], ['Torbert', 'Roy B.', ''], ['Burch', 'James L.', ''], ['Angelopoulos', 'Vassilis', '']]","extracted_entities":"[{'text': 'empirical scaling laws', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"empirical scaling laws","similarity_score":0.8098686934}
{"id":2503.14983,"submitter":"Zanting Ye","authors":"Zanting Ye, Xiaolong Niu, Xuanbin Wu, Wenxiang Yi, Yuan Chang, Lijun\n  Lu","title":"Semi-KAN: KAN Provides an Effective Representation for Semi-Supervised\n  Learning in Medical Image Segmentation","comments":"18 pages, 7 figures, 6 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Deep learning-based medical image segmentation has shown remarkable success;\nhowever, it typically requires extensive pixel-level annotations, which are\nboth expensive and time-intensive. Semi-supervised medical image segmentation\n(SSMIS) offers a viable alternative, driven by advancements in CNNs and ViTs.\nHowever, these networks often rely on single fixed activation functions and\nlinear modeling patterns, limiting their ability to effectively learn robust\nrepresentations. Given the limited availability of labeled date, achieving\nrobust representation learning becomes crucial. Inspired by Kolmogorov-Arnold\nNetworks (KANs), we propose Semi-KAN, which leverages the untapped potential of\nKANs to enhance backbone architectures for representation learning in SSMIS.\nOur findings indicate that: (1) compared to networks with fixed activation\nfunctions, KANs exhibit superior representation learning capabilities with\nfewer parameters, and (2) KANs excel in high-semantic feature spaces. Building\non these insights, we integrate KANs into tokenized intermediate\nrepresentations, applying them selectively at the encoder's bottleneck and the\ndecoder's top layers within a U-Net pipeline to extract high-level semantic\nfeatures. Although learnable activation functions improve feature expansion,\nthey introduce significant computational overhead with only marginal\nperformance gains. To mitigate this, we reduce the feature dimensions and\nemploy horizontal scaling to capture multiple pattern representations.\nFurthermore, we design a multi-branch U-Net architecture with uncertainty\nestimation to effectively learn diverse pattern representations. Extensive\nexperiments on four public datasets demonstrate that Semi-KAN surpasses\nbaseline networks, utilizing fewer KAN layers and lower computational cost,\nthereby underscoring the potential of KANs as a promising approach for SSMIS.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 08:27:41 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Ye', 'Zanting', ''], ['Niu', 'Xiaolong', ''], ['Wu', 'Xuanbin', ''], ['Yi', 'Wenxiang', ''], ['Chang', 'Yuan', ''], ['Lu', 'Lijun', '']]","extracted_entities":"[{'text': 'KANs', 'label': 'LLMs'}, {'text': 'KANs', 'label': 'LLMs'}, {'text': 'KANs', 'label': 'LLMs'}, {'text': 'KANs', 'label': 'LLMs'}, {'text': 'KANs', 'label': 'LLMs'}, {'text': 'horizontal scaling', 'label': 'Scaling law'}, {'text': 'KANs', 'label': 'LLMs'}]","assigned_concept":"Scaling law","matched_keyword":"horizontal scaling","similarity_score":0.6414827108}
{"id":2503.15076,"submitter":"Dmitry Pelinovsky","authors":"Lynnyngs Kelly Arruda and Dmitry E. Pelinovsky","title":"Kink breathers on a traveling wave background in the defocusing modified\n  Korteweg--de Vries equation","comments":"40 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"nlin.SI math-ph math.AP math.CA math.MP nlin.PS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We characterize a general traveling periodic wave of the defocusing mKdV\n(modified Korteweg--de Vries) equation by using a quotient of products of\nJacobi's elliptic theta functions. Compared to the standing periodic wave of\nthe defocusing NLS (nonlinear Schr\\\"{o}dinger) equation, these solutions are\nspecial cases of Riemann's theta function of genus two. Based on our\ncharacterization, we derive a new two-parameter solution form which defines a\ngeneral three-parameter solution form with the scaling transformation.\nEigenfunctions of the Lax system for the general traveling periodic wave are\nalso characterized as quotients of products of Jacobi's theta functions. As the\nmain outcome of our analytical computations, we derive a new solution of the\ndefocusing mKdV equation which describes the kink breather propagating on a\ngeneral traveling wave background.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 10:19:10 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Arruda', 'Lynnyngs Kelly', ''], ['Pelinovsky', 'Dmitry E.', '']]","extracted_entities":"[{'text': 'scaling transformation', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling transformation","similarity_score":0.6310578585}
{"id":2503.15255,"submitter":"Thomas Vojta","authors":"Jonathan House, Skirmantas Janu\\v{s}onis, Ralf Metzler, and Thomas\n  Vojta","title":"Fractional Brownian motion with mean-density interaction","comments":"10 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.stat-mech physics.bio-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Fractional Brownian motion is a Gaussian stochastic process with long-range\ncorrelations in time; it has been shown to be a useful model of anomalous\ndiffusion. Here, we investigate the effects of mutual interactions in an\nensemble of particles undergoing fractional Brownian motion. Specifically, we\nintroduce a mean-density interaction in which each particle in the ensemble is\ncoupled to the gradient of the total, time-integrated density produced by the\nentire ensemble. We report the results of extensive computer simulations for\nthe mean-square displacements and the probability densities of particles\nundergoing one-dimensional fractional Brownian motion with such a mean-density\ninteraction. We find two qualitatively different regimes, depending on the\nanomalous diffusion exponent $\\alpha$ characterizing the fractional Gaussian\nnoise. The motion is governed by the interactions for $\\alpha < 4\/3$ whereas it\nis dominated by the fractional Gaussian noise for $\\alpha > 4\/3$. We develop a\nscaling theory explaining our findings. We also discuss generalizations to\nhigher space dimensions and nonlinear interactions as well as applications to\nthe growth of strongly stochastic axons (e.g., serotonergic fibers) in\nvertebrate brains.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 14:28:45 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['House', 'Jonathan', ''], ['Janu\u0161onis', 'Skirmantas', ''], ['Metzler', 'Ralf', ''], ['Vojta', 'Thomas', '']]","extracted_entities":"[{'text': 'scaling theory', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling theory","similarity_score":0.8080649376}
{"id":2503.15276,"submitter":"Samuele Mosso","authors":"Samuele Mosso, Karl Lapo, Ivana Stiperski","title":"Revealing the drivers of turbulence anisotropy over flat and complex\n  terrain: an interpretable machine learning approach","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.flu-dyn physics.ao-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Turbulence anisotropy was recently integrated into Monin-Obukhov Similarity\nTheory (MOST), extending its applicability to complex terrain and diverse\nsurface conditions. Implementing this generalized MOST in numerical models,\nhowever, requires understanding the key drivers of turbulence anisotropy across\nvarious terrain conditions. This study therefore employs random forest models\ntrained on measurement data from both flat and complex terrain and including\nupstream terrain features, to predict turbulence anisotropy. Two approaches\nwere compared: using dimensional variables directly or employing\nnon-dimensional groups as model input. To address cross-correlation among\nfeatures, we developed a new selection method, Recursive Effect Elimination.\nFinally, interpretability methods were used to identify the most influential\nvariables. Contrary to expectations, variables related to terrain influence\nwere not found to significantly impact turbulence anisotropy. Instead,\nnon-dimensional groups of common turbulence length, time and velocity scales\nproved more robust than dimensional variables in isolating anisotropy drivers,\nenhancing model performance over complex terrain and reducing location\ndependence. A ratio of integral and turbulence memory length scales was found\nto correlate well with turbulence anisotropy in both daytime and nighttime\nconditions, both over flat and complex terrain. During the day, a refined\nstability parameter incorporating both the surface and mixed layer scaling\nemerged as the dominant driver of anisotropy, while at night, parameters\nrelated to rapid distortion were strong predictors.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 14:52:42 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Mosso', 'Samuele', ''], ['Lapo', 'Karl', ''], ['Stiperski', 'Ivana', '']]","extracted_entities":"[{'text': 'mixed layer scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"mixed layer scaling","similarity_score":0.5600087643}
{"id":2503.15336,"submitter":"Jonah Glunt","authors":"Jonah J. Glunt, Jacob A. Siefert, Andrew F. Thompson, Justin Ruths,\n  and Herschel C. Pangborn","title":"Automated Functional Decomposition for Hybrid Zonotope\n  Over-approximations with Application to LSTM Networks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Functional decomposition is a powerful tool for systems analysis because it\ncan reduce a function of arbitrary input dimensions to the sum and\nsuperposition of functions of a single variable, thereby mitigating (or\npotentially avoiding) the exponential scaling often associated with analyses\nover high-dimensional spaces. This paper presents automated methods for\nconstructing functional decompositions used to form set-based\nover-approximations of nonlinear functions, with particular focus on the hybrid\nzonotope set representation. To demonstrate these methods, we construct a\nhybrid zonotope set that over-approximates the input-output graph of a long\nshort-term memory neural network, and use functional decomposition to represent\na discrete hybrid automaton via a hybrid zonotope.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:30:03 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Glunt', 'Jonah J.', ''], ['Siefert', 'Jacob A.', ''], ['Thompson', 'Andrew F.', ''], ['Ruths', 'Justin', ''], ['Pangborn', 'Herschel C.', '']]","extracted_entities":"[{'text': 'exponential scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"exponential scaling","similarity_score":0.618661046}
{"id":2503.15347,"submitter":"Jan Nagel","authors":"Helene G\\\"otz, Jan Nagel","title":"Nonstandard Large and Moderate Deviations for the Laguerre Ensemble","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper, we show limit theorems for the weighted spectral measure of\nthe Laguerre ensemble under a nonstandard scaling, when the parameter grows\nfaster than the matrix size. For this parameter scaling, the limit behavior is\nsimilar to the case of the Gaussian ensemble. We show a large deviation\nprinciple, moderate deviations and a CLT for the spectral measure. For the\nmoderate deviations and the CLT, we observe a particular dependence on the rate\nof the parameter and a corrective shift by a signed measure. The proofs are\nbased on the tridiagonal representation of the Laguerre ensemble.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:43:47 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['G\u00f6tz', 'Helene', ''], ['Nagel', 'Jan', '']]","extracted_entities":"[{'text': 'nonstandard scaling', 'label': 'Scaling law'}, {'text': 'large deviation\\nprinciple', 'label': 'Scaling law'}, {'text': 'CLT', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"nonstandard scaling","similarity_score":0.6686199903}
{"id":2503.15599,"submitter":"Maryum Sayeed","authors":"Maryum Sayeed, Daniel Huber, Ashley Chontos, Yaguang Li","title":"A Homogeneous Catalog of Oscillating Solar-Type Stars Observed by the\n  Kepler Mission and a New Amplitude Scaling Relation Including Chromospheric\n  Activity","comments":"24 pages, 16 figures, 5 tables, submitted to AJ. An electronic\n  version of Tables 3 & 4 will be available after peer review","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.SR astro-ph.EP","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We present a homogeneous catalog of global asteroseismic parameters and\nderived stellar parameters for 765 Kepler main-sequence and subgiant stars. The\ncatalog was produced by re-analyzing all available Kepler DR25 short-cadence\ndata using pySYD, an automated pipeline to extract global asteroseismic\nparameters. We find 50 new detections, seven of which are also planet candidate\nhost stars. We find excellent agreement between our $\\nu_{\\text{max}}$ and\n$\\Delta \\nu$ measurements and literature values, with an average offset of $0.2\n\\pm 0.4\\%$ ($\\sigma=5\\%$) and $0.2 \\pm 0.7\\%$ ($\\sigma=2\\%$), respectively. In\naddition, we derive stellar radii and masses with an average precision of\n$2.7\\%$ and $10.4\\%$, respectively, and find a median offset of $0.4 \\pm 0.4\\%$\n($\\sigma=10\\%$) between our radii derived with asteroseismology and those from\nGaia parallaxes. Using spectroscopic $\\log{R'_{\\text{HK}}}$ activity\nmeasurements from Keck\/HIRES, we derive a new amplitude scaling relation with\nan activity term for main-sequence and subgiant stars, which reduces the offset\nbetween expected and observed oscillation amplitudes from $9.3 \\pm 1.6\\%$ to\n$1.7 \\pm 0.9\\%$. Our work is the largest and most homogeneous asteroseismic\ncatalog of Kepler main-sequence and subgiant stars to date, including a total\nof 101 stars hosting planet candidates and 451 stars with measured rotation\nperiods.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 18:00:01 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Sayeed', 'Maryum', ''], ['Huber', 'Daniel', ''], ['Chontos', 'Ashley', ''], ['Li', 'Yaguang', '']]","extracted_entities":"[{'text': 'amplitude scaling relation', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"amplitude scaling relation","similarity_score":0.5424147844}
{"id":2503.15719,"submitter":"Xin An","authors":"Xin An, Francesco Giglio, Giulio Landolfi","title":"Phase transitions and finite-size effects in integrable virial\n  statistical models","comments":"7 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.stat-mech hep-th nlin.SI nucl-th","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We analyze thermodynamic models for fluid systems in equilibrium based on a\nvirial expansion of the internal energy in terms of the volume density. We\nprove that the models, formulated for finite-size systems with $N$ particles,\nare exactly solvable to any expansion order, as expectation values of physical\nobservables (e.g., volume density) are determined from solutions to nonlinear\nC-integrable PDEs of hydrodynamic type. In the limit $N\\to\\infty$, phase\ntransitions emerge as classical shock waves in the space of thermodynamic\nvariables. Near critical points, we argue that the volume density exhibits a\nscaling behavior consistent with the Universality Conjecture in viscous\ntransport PDEs. As an application, we employ our framework to nuclear matter\nand construct a global QCD phase diagram revealing critical points for the\nnuclear liquid-gas and hadron gas-QGP transitions. We demonstrate how\nfinite-size effects smear critical signatures, indicating the importance of\nthoughtful considerations in the search for the QCD critical point.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 22:10:55 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['An', 'Xin', ''], ['Giglio', 'Francesco', ''], ['Landolfi', 'Giulio', '']]","extracted_entities":"[{'text': 'volume density', 'label': 'quantisation'}, {'text': 'volume density', 'label': 'quantisation'}, {'text': 'volume density', 'label': 'quantisation'}, {'text': 'scaling behavior', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling behavior","similarity_score":0.7924405336}
{"id":2503.15743,"submitter":"Oskar Novak","authors":"Oskar Novak and Narayanan Rengaswamy","title":"Extending the HNLS Condition to Robust Quantum Metrology","comments":"5 pages main, 12 pages total","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cs.IT math.IT","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Quantum sensing holds great promise for high-precision magnetic field\nmeasurements. However, its performance is significantly limited by noise. In\nthis work, we develop a quantum sensing protocol to estimate a parameter\n$\\theta$, associated with a magnetic field, under full-rank Markovian noise.\nOur approach uses a probe state constructed from a CSS code that evolves under\nthe parameter's Hamiltonian for a short time, but without any active error\ncorrection. Then we measure the code's $\\hat{X}$ stabilizers to infer $\\theta$.\nGiven $N$ copies of the probe state, we derive the probability that all\nstabilizer measurements return $+1$, which depends on $\\theta$. The uncertainty\nin $\\theta$ (estimated from these measurements) is bounded by a new quantity,\nthe Robustness Bound, which characterizes how the structure of the quantum code\naffects the Quantum Fisher Information of the measurement. Using this bound, we\nestablish a strong no-go result: a nontrivial CSS code can achieve Heisenberg\nscaling if and only if the Hamiltonian is orthogonal to the span of the noise\nchannel's Lindblad operators. This result extends the well-known HNLS condition\nunder infinite rounds of error correction to the robust quantum sensing setting\nthat does not use active error correction. Our finding suggests fundamental\nlimitations in the use of linear quantum codes for dephased magnetic field\nsensing applications both in the near-term robust sensing regime and in the\nlong-term fault tolerant era.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 23:15:41 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Novak', 'Oskar', ''], ['Rengaswamy', 'Narayanan', '']]","extracted_entities":"[{'text': 'Robustness Bound', 'label': 'Scaling law'}, {'text': 'Heisenberg\\nscaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"Heisenberg\nscaling","similarity_score":0.5351125598}
{"id":2503.15746,"submitter":"David Sivakoff","authors":"Janko Gravner, Alexander Holroyd, Sangchul Lee and David Sivakoff","title":"Polluted Modified Bootstrap Percolation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In the polluted modified bootstrap percolation model, sites in the square\nlattice are independently initially occupied with probability $p$ or closed\nwith probability $q$. A site becomes occupied at a subsequent step if it is not\nclosed and has at least one occupied nearest neighbor in each of the two\ncoordinates. We study the final density of occupied sites when $p$ and $q$ are\nboth small. We show that this density approaches $0$ if $q\\ge Cp^2\/\\log p^{-1}$\nand $1$ if $q\\le p^2\/(\\log p^{-1})^{1+o(1)}$. Thus we establish a logarithmic\ncorrection in the critical scaling, which is known not to be present in the\nstandard model, settling a conjecture of Gravner and McDonald from 1997.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 23:31:17 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Gravner', 'Janko', ''], ['Holroyd', 'Alexander', ''], ['Lee', 'Sangchul', ''], ['Sivakoff', 'David', '']]","extracted_entities":"[{'text': 'critical scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"critical scaling","similarity_score":0.6000217795}
{"id":2503.15765,"submitter":"Bouchra Bensiali","authors":"Bouchra Bensiali and Stefan Sauter","title":"Computation of whispering gallery modes for spherical symmetric\n  heterogeneous Helmholtz problems with piecewise smooth refractive index","comments":"Main text: 36 pages, 54 figures; Supplementary material: 23 pages, 20\n  tables","journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.NA","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  In this paper, we develop a numerical method for the computation of\n(quasi-)resonances in spherical symmetric heterogeneous Helmholtz problems with\npiecewise smooth refractive index. Our focus lies in resonances very close to\nthe real axis, which characterize the so-called whispering gallery modes. Our\nmethod involves a modal equation incorporating fundamental solutions to\ndecoupled problems, extending the known modal equation to the case of piecewise\nsmooth coefficients. We first establish the well-posedeness of the fundamental\nsystem, then we formulate the problem of resonances as a nonlinear eigenvalue\nproblem, whose determinant will be the modal equation in the piecewise smooth\ncase. In combination with the numerical approximation of the fundamental\nsolutions using a spectral method, we derive a Newton method to solve the\nnonlinear modal equation with a proper scaling. We show the local convergence\nof the algorithm in the piecewise constant case by proving the simplicity of\nthe roots. We confirm our approach through a series of numerical experiments in\nthe piecewise constant and variable case.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 00:51:13 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Bensiali', 'Bouchra', ''], ['Sauter', 'Stefan', '']]","extracted_entities":"[{'text': 'proper scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"proper scaling","similarity_score":0.6903918982}
{"id":2503.16001,"submitter":"Ngoc Nhi Nguyen","authors":"Niels Benedikter, Chiara Boccato, Domenico Monaco, Ngoc Nhi Nguyen","title":"Derivation of Hartree-Fock Dynamics and Semiclassical Commutator\n  Estimates for Fermions in a Magnetic Field","comments":"44 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math-ph math.MP","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We study the quantum dynamics of a large number of interacting fermionic\nparticles in a constant magnetic field. In a coupled mean-field and\nsemiclassical scaling limit, we show that solutions of the many-body\nSchr\\\"odinger equation converge to solutions of a non-linear Hartree-Fock\nequation. The central ingredient of the proof are certain semiclassical trace\nnorm estimates of commutators of the position and momentum operators with the\none-particle density matrix of the solution of the Hartree-Fock equation. In a\nfirst step, we prove their validity for non-interacting initial data in a\nmagnetic field by generalizing a 2020 result of Fournais and Mikkelsen. We then\npropagate these bounds from the initial data along the Hartree-Fock flow to\narbitrary times.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 10:07:55 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Benedikter', 'Niels', ''], ['Boccato', 'Chiara', ''], ['Monaco', 'Domenico', ''], ['Nguyen', 'Ngoc Nhi', '']]","extracted_entities":"[{'text': 'semiclassical scaling limit', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"semiclassical scaling limit","similarity_score":0.5690568089}
{"id":2503.16211,"submitter":"Greg van Anders","authors":"Hazhir Aliahmadi, Aidan Sheedy, Greg van Anders","title":"Filters reveal emergent structure in computational morphogenesis","comments":"17 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CE cond-mat.stat-mech math.OC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Revolutionary advances in both manufacturing and computational morphogenesis\nraise critical questions about design sensitivity. Sensitivity questions are\nespecially critical in contexts, such as topology optimization, that yield\nstructures with emergent morphology. However, analyzing emergent structures via\nconventional, perturbative techniques can mask larger-scale vulnerabilities\nthat could manifest in essential components. Risks that fail to appear in\nperturbative sensitivity analyses will only continue to proliferate as topology\noptimization-driven manufacturing penetrates more deeply into engineering\ndesign and consumer products. Here, we introduce Laplace-transform based\ncomputational filters that supplement computational morphogenesis with a set of\nnonperturbative sensitivity analyses. We demonstrate how this approach\nidentifies important elements of a structure even in the absence of knowledge\nof the ultimate, optimal structure itself. We leverage techniques from\nmolecular dynamics and implement these methods in open-source codes,\ndemonstrating their application to compliance minimization problems in both 2D\nand 3D. Our implementation extends straightforwardly to topology optimization\nfor other problems and benefits from the strong scaling properties observed in\nconventional molecular simulation.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 14:57:03 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Aliahmadi', 'Hazhir', ''], ['Sheedy', 'Aidan', ''], ['van Anders', 'Greg', '']]","extracted_entities":"[{'text': 'open-source codes', 'label': 'Open-source LLMs'}, {'text': 'strong scaling properties', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"strong scaling properties","similarity_score":0.6400084496}
{"id":2503.16294,"submitter":"Nicolas Valade","authors":"Nicolas Valade, J\\'er\\'emie Bec, Simon Thalabard","title":"Surface quasigeostrophic turbulence: The refined study of an active\n  scalar","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.flu-dyn","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  SQG describes the 2D active transport of a scalar field, such as temperature,\nwhich -- when properly rescaled -- shares the same physical dimension of\nlength\/time as the advecting velocity field. This duality has motivated\nanalogies with 3D turbulence. In particular, the Kraichnan-Leith-Batchelor\nsimilarity theory predicts a Kolmogorov-type inertial range scaling $\\propto\n(\\varepsilon \\ell)^{1\/3}$ for both scalar and velocity fields, and the presence\nof intermittency was pointed out by Sukhatme & Pierrehumbert ($Chaos$\n$\\boldsymbol{12}$, 439, 2002) in unforced settings. In this work, we refine\nthese analogies using simulations up to $16,384^2$ collocation points in a\nsteady-state regime dominated by the direct cascade of scalar variance. We show\nthat mixed structure functions, linking velocity increments with powers of\nscalar differences, exhibit clear scaling, revealing the role of anomalous\nfluxes of all the scalar moments. However, the usual (unmixed) structure\nfunctions do no follow any power-law scaling in any range of scales, neither\nfor the velocity nor for the scalar increments. This specific form of the\nintermittency phenomenon reflects the specific kinematic properties of SQG\nturbulence, involving the interplay between long-range interactions, structures\nand geometry. Revealing the multiscaling in single-field statistics requires to\nresort to generalised notions of scale invariance, such as extended\nself-similarity and specific form of refined self-similarity. Our findings\nemphasise the fundamental entanglement of scalar and velocity fields in SQG\nturbulence: They evolve hand in hand and any attempt to isolate them destroys\nscaling in its usual sense. This perspective sheds new lights on the\ndiscrepancies in spectra and structure functions, that have been repeatedly\nobserved in SQG numerics for the past 20 years.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 16:18:53 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Valade', 'Nicolas', ''], ['Bec', 'J\u00e9r\u00e9mie', ''], ['Thalabard', 'Simon', '']]","extracted_entities":"[{'text': 'Kolmogorov-type inertial range scaling', 'label': 'Scaling law'}, {'text': 'power-law scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"power-law scaling","similarity_score":0.7775874138}
{"id":2503.16405,"submitter":"Volodymyr Vovchenko","authors":"Adam Bzdak, Volker Koch, Volodymyr Vovchenko","title":"Acceptance dependence of factorial cumulants, long-range correlations,\n  and the antiproton puzzle","comments":"10 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"nucl-th hep-ph nucl-ex","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We analyze joint factorial cumulants of protons and antiprotons in\nrelativistic heavy-ion collisions and point out that they obey the scaling\n$\\hat{C}_{nm}^{p,\\bar{p}} \\propto \\langle N_p\\rangle^n \\langle N_{\\bar{p}}\n\\rangle^m$ as a function of acceptance when only long-range correlations are\npresent in the system, such as global baryon conservation and volume\nfluctuations. This hypothesis can be directly tested experimentally without the\nneed for corrections for volume fluctuations. We show that if correlations\namong protons and antiprotons are driven by global baryon conservation and\nvolume fluctuations only, the equality $\\hat{C}_{2}^{p} \/ \\langle N_p \\rangle^2\n= \\hat{C}_{2}^{\\bar{p}} \/ \\langle N_{\\bar{p}} \\rangle^2$ holds for large\nsystems created in central collisions. We point out that the experimental data\nof the STAR Collaboration from phase I of RHIC beam energy scan are\napproximately consistent with the scaling $\\hat{C}_{nm}^{p,\\bar{p}} \\propto\n\\langle N_p \\rangle^n \\langle N_{\\bar{p}} \\rangle^m$, but the normalized\nantiproton correlations are stronger than that of protons,\n$-\\hat{C}_{2}^{\\bar{p}} \/ \\langle N_{\\bar{p}} \\rangle^2 > -\\hat{C}_{2}^{p} \/\n\\langle N_p \\rangle^2$, indicating that global baryon conservation and volume\nfluctuations alone cannot explain the data. We also discuss high-order\nfactorial cumulants which can be measured with sufficient precision within\nphase II of RHIC-BES.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:56:02 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Bzdak', 'Adam', ''], ['Koch', 'Volker', ''], ['Vovchenko', 'Volodymyr', '']]","extracted_entities":"[{'text': 'scaling', 'label': 'Scaling law'}, {'text': 'scaling', 'label': 'Scaling law'}]","assigned_concept":"Scaling law","matched_keyword":"scaling","similarity_score":0.7561825514}
{"id":2303.1044,"submitter":"Herv\\'e Turlier","authors":"Sacha Ichbiah, Anshuman Sinha, Fabrice Delbary, Herv\\'e Turlier","title":"Inverse 3D microscopy rendering for cell shape inference with active\n  mesh","comments":"11 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.bio-ph q-bio.QM","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Traditional methods for biological shape inference, such as deep learning\n(DL) and active contour models, face limitations in 3D. DL requires large\nlabeled datasets, which are difficult to obtain, while active contour models\nrely on fine-tuned hyperparameters for intensity attraction and regularization.\nWe introduce deltaMic, a novel 3D differentiable renderer for fluorescence\nmicroscopy. By leveraging differentiable Fourier-space convolution, deltaMic\naccurately models the image formation process, integrating a parameterized\nmicroscope point spread function and a mesh-based object representation. Unlike\nDL-based segmentation, it directly optimizes shape and microscopy parameters to\nfit real microscopy data, removing the need for large datasets or heuristic\npriors. To enhance efficiency, we develop a GPU-accelerated Fourier transform\nfor triangle meshes, significantly improving speed. We demonstrate deltaMic's\nability to reconstruct cellular shapes from synthetic and real microscopy\nimages, providing a robust tool for 3D segmentation and biophysical modeling.\nThis work bridges physics-based rendering with modern optimization techniques,\noffering a new paradigm for microscopy image analysis and inverse biophysical\nmodeling.\n","versions":"[{'version': 'v1', 'created': 'Sat, 18 Mar 2023 15:45:10 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 21:54:17 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Ichbiah', 'Sacha', ''], ['Sinha', 'Anshuman', ''], ['Delbary', 'Fabrice', ''], ['Turlier', 'Herv\u00e9', '']]","extracted_entities":"[{'text': 'fine-tuned hyperparameters', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuned hyperparameters","similarity_score":0.5314788818}
{"id":2307.07748,"submitter":"Richard Lai Lee","authors":"Richard Lee Lai, Jen-Cheng Hou, I-Chun Chern, Kuo-Hsuan Hung, Yi-Ting\n  Chen, Mandar Gogate, Tughrul Arslan, Amir Hussain, and Yu Tsao","title":"Audio-Visual Speech Enhancement Using Self-supervised Learning to\n  Improve Speech Intelligibility in Cochlear Implant Simulations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.AS","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Individuals with hearing impairments face challenges in their ability to\ncomprehend speech, particularly in noisy environments. The aim of this study is\nto explore the effectiveness of audio-visual speech enhancement (AVSE) in\nenhancing the intelligibility of vocoded speech in cochlear implant (CI)\nsimulations. Notably, the study focuses on a challenged scenario where there is\nlimited availability of training data for the AVSE task. To address this\nproblem, we propose a novel deep neural network framework termed\nSelf-Supervised Learning-based AVSE (SSL-AVSE). The proposed SSL-AVSE combines\nvisual cues, such as lip and mouth movements, from the target speakers with\ncorresponding audio signals. The contextually combined audio and visual data\nare then fed into a Transformer-based SSL AV-HuBERT model to extract features,\nwhich are further processed using a BLSTM-based SE model. The results\ndemonstrate several key findings. Firstly, SSL-AVSE successfully overcomes the\nissue of limited data by leveraging the AV-HuBERT model. Secondly, by\nfine-tuning the AV-HuBERT model parameters for the target SE task, significant\nperformance improvements are achieved. Specifically, there is a notable\nenhancement in PESQ (Perceptual Evaluation of Speech Quality) from 1.43 to 1.67\nand in STOI (Short-Time Objective Intelligibility) from 0.70 to 0.74.\nFurthermore, the performance of the SSL-AVSE was evaluated using CI vocoded\nspeech to assess the intelligibility for CI users. Comparative experimental\noutcomes reveal that in the presence of dynamic noises encountered during human\nconversations, SSL-AVSE exhibits a substantial improvement. The NCM (Normal\nCorrelation Matrix) values indicate an increase of 26.5% to 87.2% compared to\nthe noisy baseline.\n","versions":"[{'version': 'v1', 'created': 'Sat, 15 Jul 2023 09:05:57 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:04:06 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Lai', 'Richard Lee', ''], ['Hou', 'Jen-Cheng', ''], ['Chern', 'I-Chun', ''], ['Hung', 'Kuo-Hsuan', ''], ['Chen', 'Yi-Ting', ''], ['Gogate', 'Mandar', ''], ['Arslan', 'Tughrul', ''], ['Hussain', 'Amir', ''], ['Tsao', 'Yu', '']]","extracted_entities":"[{'text': 'SSL-AVSE', 'label': 'Transformer-based model'}, {'text': 'SSL-AVSE', 'label': 'Transformer-based model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'SSL-AVSE', 'label': 'Transformer-based model'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2308.11256,"submitter":"Linjian Meng","authors":"Linjian Meng, Youzhi Zhang, Zhenxing Ge, Shangdong Yang, Tianyu Ding,\n  Wenbin Li, Tianpei Yang, Bo An, Yang Gao","title":"Efficient Last-iterate Convergence Algorithms in Solving Games","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GT cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  To establish last-iterate convergence for Counterfactual Regret Minimization\n(CFR) algorithms in learning a Nash equilibrium (NE) of extensive-form games\n(EFGs), recent studies reformulate learning an NE of the original EFG as\nlearning the NEs of a sequence of (perturbed) regularized EFGs. Consequently,\nproving last-iterate convergence in solving the original EFG reduces to proving\nlast-iterate convergence in solving (perturbed) regularized EFGs. However, the\nempirical convergence rates of the algorithms in these studies are suboptimal,\nsince they do not utilize Regret Matching (RM)-based CFR algorithms to solve\nperturbed EFGs, which are known the exceptionally fast empirical convergence\nrates. Additionally, since solving multiple perturbed regularized EFGs is\nrequired, fine-tuning across all such games is infeasible, making\nparameter-free algorithms highly desirable. In this paper, we prove that\nCFR$^+$, a classical parameter-free RM-based CFR algorithm, achieves\nlast-iterate convergence in learning an NE of perturbed regularized EFGs.\nLeveraging CFR$^+$ to solve perturbed regularized EFGs, we get Reward\nTransformation CFR$^+$ (RTCFR$^+$). Importantly, we extend prior work on the\nparameter-free property of CFR$^+$, enhancing its stability, which is crucial\nfor the empirical convergence of RTCFR$^+$. Experiments show that RTCFR$^+$\nsignificantly outperforms existing algorithms with theoretical last-iterate\nconvergence guarantees.\n","versions":"[{'version': 'v1', 'created': 'Tue, 22 Aug 2023 07:59:49 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 08:31:00 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Meng', 'Linjian', ''], ['Zhang', 'Youzhi', ''], ['Ge', 'Zhenxing', ''], ['Yang', 'Shangdong', ''], ['Ding', 'Tianyu', ''], ['Li', 'Wenbin', ''], ['Yang', 'Tianpei', ''], ['An', 'Bo', ''], ['Gao', 'Yang', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2309.17211,"submitter":"Lukas Meiner","authors":"Lukas Meiner, Jens Mehnert, Alexandru Paul Condurache","title":"Data-Free Dynamic Compression of CNNs for Tractable Efficiency","comments":"Accepted at VISAPP 2025","journal-ref":null,"doi":"10.5220\/0013301000003912","report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  To reduce the computational cost of convolutional neural networks (CNNs) on\nresource-constrained devices, structured pruning approaches have shown promise\nin lowering floating-point operations (FLOPs) without substantial drops in\naccuracy. However, most methods require fine-tuning or specific training\nprocedures to achieve a reasonable trade-off between retained accuracy and\nreduction in FLOPs, adding computational overhead and requiring training data\nto be available. To this end, we propose HASTE (Hashing for Tractable\nEfficiency), a data-free, plug-and-play convolution module that instantly\nreduces a network's test-time inference cost without training or fine-tuning.\nOur approach utilizes locality-sensitive hashing (LSH) to detect redundancies\nin the channel dimension of latent feature maps, compressing similar channels\nto reduce input and filter depth simultaneously, resulting in cheaper\nconvolutions. We demonstrate our approach on the popular vision benchmarks\nCIFAR-10 and ImageNet, where we achieve a 46.72% reduction in FLOPs with only a\n1.25% loss in accuracy by swapping the convolution modules in a ResNet34 on\nCIFAR-10 for our HASTE module.\n","versions":"[{'version': 'v1', 'created': 'Fri, 29 Sep 2023 13:09:40 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:35:38 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Meiner', 'Lukas', ''], ['Mehnert', 'Jens', ''], ['Condurache', 'Alexandru Paul', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2312.09853,"submitter":"Jeffrey Kuntz","authors":"Jeffrey Kuntz, Andreas Trautner","title":"Extra Dimensions Beyond the Horizon","comments":"v2: discussions added, 29+5 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph astro-ph.CO gr-qc hep-th","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We discuss an extra-dimensional braneworld with a 5th dimension compactified\non a circle. As a characteristic feature, the warp factor is hyperbolic and\nseparates the hidden and visible branes by a bulk horizon without a\nsingularity. The two most widely separated scales of 4D physics - the 4D Planck\nmass and 4D cosmological constant - are determined by two physical scales in\nthe extra dimension, namely: $(i)$ the proper size of the extra dimension, $R$,\nand, $(ii)$ the distance between the visible brane and the horizon, $R_0$. A\nrealistic scale hierarchy between 4D Planck mass and 4D cosmological constant\nis obtained for $R\/R_0\\sim2.34$. The usual fine tuning is not reduced but\npromoted to a fine tuning of two separate brane energy densities that must\napproach the fundamental scale of the model with very high precision. Our\nscenario is based on an exact solution to the 5D Einstein equations with a\nstrictly empty bulk and Friedmann-Lema\\^itre-Robertson-Walker metric on the 4D\nbranes. This requires positive 4D brane energy densities and describes an\nadiabatic runaway solution in agreement with the de Sitter swampland\nconjecture. The Kaluza-Klein (KK) graviton states are solutions of a modified\nP\\\"oschl-Teller potential which permits a discrete graviton spectrum of exactly\ntwo modes. In addition to the usual massless graviton, our scenario predicts an\nextra massive spin-2 graviton with a mass gap of\n$m_1=\\sqrt{2}H_0\\approx2\\times10^{-33}\\,\\mathrm{eV}$ which might be detectable\nin the foreseeable future. A KK tower of gravitons, or a possible continuum of\nmassive graviton states, is prohibited by unitarity with respect to the\nhorizon. We discuss hurdles in turning this model into a realistic cosmology at\nall times, which points us towards 4D brane tensions that that must be raising\ntowards the fundamental scale of the model, while the observable 4D expansion\nrate is decreasing.\n","versions":"[{'version': 'v1', 'created': 'Fri, 15 Dec 2023 15:00:46 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 10:13:23 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Kuntz', 'Jeffrey', ''], ['Trautner', 'Andreas', '']]","extracted_entities":"[{'text': 'fine tuning', 'label': 'Fine-tuning'}, {'text': 'fine tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine tuning","similarity_score":0.9617297649}
{"id":2402.14598,"submitter":"Depin Liang","authors":"Jianming Lv, Chengjun Wang, Depin Liang, Qianli Ma, Wei Chen, Xueqi\n  Cheng","title":"EMN: Brain-inspired Elastic Memory Network for Quick Domain Adaptive\n  Feature Mapping","comments":"15 pages,15 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.NE cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Utilizing unlabeled data in the target domain to perform continuous\noptimization is critical to enhance the generalization ability of neural\nnetworks. Most domain adaptation methods focus on time-consuming optimization\nof deep feature extractors, which limits the deployment on lightweight edge\ndevices. Inspired by the memory mechanism and powerful generalization ability\nof biological neural networks in human brains, we propose a novel gradient-free\nElastic Memory Network, namely EMN, to support quick fine-tuning of the mapping\nbetween features and prediction without heavy optimization of deep features. In\nparticular, EMN adopts randomly connected neurons to memorize the association\nof features and labels, where the signals in the network are propagated as\nimpulses, and the prediction is made by associating the memories stored on\nneurons based on their confidence. More importantly, EMN supports reinforced\nmemorization of feature mapping based on unlabeled data to quickly adapt to a\nnew domain. Experiments based on four cross-domain real-world datasets show\nthat EMN can achieve up to 10% enhancement of performance while only needing\nless than 1% timing cost of traditional domain adaptation methods.\n","versions":"[{'version': 'v1', 'created': 'Sun, 4 Feb 2024 09:58:17 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 08:34:07 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Lv', 'Jianming', ''], ['Wang', 'Chengjun', ''], ['Liang', 'Depin', ''], ['Ma', 'Qianli', ''], ['Chen', 'Wei', ''], ['Cheng', 'Xueqi', '']]","extracted_entities":"[{'text': 'memory mechanism', 'label': 'Attention mechanism'}, {'text': 'quick fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"quick fine-tuning","similarity_score":0.9181714058}
{"id":2402.15216,"submitter":"Yongzhi Huang","authors":"Yongzhi Huang, Fengjun Xi, Liyun Tu, Jinxin Zhu, Haseeb Hassan,\n  Liyilei Su, Yun Peng, Jingyu Li, Jun Ma, Bingding Huang","title":"Label-efficient multi-organ segmentation with a diffusion model","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Accurate segmentation of multiple organs in Computed Tomography (CT) images\nplays a vital role in computer-aided diagnosis systems. While various\nsupervised learning approaches have been proposed recently, these methods\nheavily depend on a large amount of high-quality labeled data, which are\nexpensive to obtain in practice. To address this challenge, we propose a\nlabel-efficient framework using knowledge transfer from a pre-trained diffusion\nmodel for CT multi-organ segmentation. Specifically, we first pre-train a\ndenoising diffusion model on 207,029 unlabeled 2D CT slices to capture\nanatomical patterns. Then, the model backbone is transferred to the downstream\nmulti-organ segmentation task, followed by fine-tuning with few labeled data.\nIn fine-tuning, two fine-tuning strategies, linear classification and\nfine-tuning decoder, are employed to enhance segmentation performance while\npreserving learned representations. Quantitative results show that the\npre-trained diffusion model is capable of generating diverse and realistic\n256x256 CT images (Fr\\'echet inception distance (FID): 11.32, spatial Fr\\'echet\ninception distance (sFID): 46.93, F1-score: 73.1%). Compared to\nstate-of-the-art methods for multi-organ segmentation, our method achieves\ncompetitive performance on the FLARE 2022 dataset, particularly in limited\nlabeled data scenarios. After fine-tuning with 1% and 10% labeled data, our\nmethod achieves dice similarity coefficients (DSCs) of 71.56% and 78.51%,\nrespectively. Remarkably, the method achieves a DSC score of 51.81% using only\nfour labeled CT slices. These results demonstrate the efficacy of our approach\nin overcoming the limitations of supervised learning approaches that is highly\ndependent on large-scale labeled data.\n","versions":"[{'version': 'v1', 'created': 'Fri, 23 Feb 2024 09:25:57 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 02:42:26 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Huang', 'Yongzhi', ''], ['Xi', 'Fengjun', ''], ['Tu', 'Liyun', ''], ['Zhu', 'Jinxin', ''], ['Hassan', 'Haseeb', ''], ['Su', 'Liyilei', ''], ['Peng', 'Yun', ''], ['Li', 'Jingyu', ''], ['Ma', 'Jun', ''], ['Huang', 'Bingding', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2403.04343,"submitter":"Yanqi Dai","authors":"Yanqi Dai, Zebin You, Dong Jing, Yutian Luo, Nanyi Fei, Guoxing Yang,\n  Zhiwu Lu","title":"CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction\n  Tuning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Visual instruction tuning is an important training stage for large multimodal\nmodels. Nevertheless, when learning multiple visual tasks simultaneously, this\napproach may lead to suboptimal and imbalanced overall performance due to\nlatent knowledge conflicts across tasks. To mitigate this issue, we introduce a\nnovel Comprehensive Task Balancing (CoTBal) algorithm tailored for multi-task\nvisual instruction tuning. To our knowledge, this is the first work to explore\nmulti-task optimization in visual instruction tuning. Specifically, we consider\ntwo critical dimensions for task balancing: (1) Inter-Task Contribution, which\nrepresents the phenomenon where learning one task could enhance the performance\non others owing to the overlapping knowledge domains across tasks, and (2)\nIntra-Task Difficulty, which indicates the inherent learning difficulty of a\nsingle task. Furthermore, by quantifying these with performance-based metrics,\ncomprehensive task balancing is thus achieved by assigning greater weight to\ntasks that offer substantial contributions to others, receive minimal\ncontributions from others, and present high learning difficulties. Extensive\nexperiments on three benchmarks demonstrate that our CoTBal algorithm results\nin superior and more balanced overall performance in multi-task visual\ninstruction tuning.\n","versions":"[{'version': 'v1', 'created': 'Thu, 7 Mar 2024 09:11:16 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 03:24:54 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Dai', 'Yanqi', ''], ['You', 'Zebin', ''], ['Jing', 'Dong', ''], ['Luo', 'Yutian', ''], ['Fei', 'Nanyi', ''], ['Yang', 'Guoxing', ''], ['Lu', 'Zhiwu', '']]","extracted_entities":"[{'text': 'Visual instruction tuning', 'label': 'Fine-tuning'}, {'text': 'visual instruction tuning', 'label': 'Fine-tuning'}, {'text': 'visual instruction tuning', 'label': 'Fine-tuning'}, {'text': 'visual\\ninstruction tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"Visual instruction tuning","similarity_score":0.5302147865}
{"id":2403.12029,"submitter":"Justin Kay","authors":"Justin Kay, Timm Haucke, Suzanne Stathatos, Siqi Deng, Erik Young,\n  Pietro Perona, Sara Beery, Grant Van Horn","title":"Align and Distill: Unifying and Improving Domain Adaptive Object\n  Detection","comments":"TMLR camera ready (Featured Certification). 33 pages, 15 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Object detectors often perform poorly on data that differs from their\ntraining set. Domain adaptive object detection (DAOD) methods have recently\ndemonstrated strong results on addressing this challenge. Unfortunately, we\nidentify systemic benchmarking pitfalls that call past results into question\nand hamper further progress: (a) Overestimation of performance due to\nunderpowered baselines, (b) Inconsistent implementation practices preventing\ntransparent comparisons of methods, and (c) Lack of generality due to outdated\nbackbones and lack of diversity in benchmarks. We address these problems by\nintroducing: (1) A unified benchmarking and implementation framework, Align and\nDistill (ALDI), enabling comparison of DAOD methods and supporting future\ndevelopment, (2) A fair and modern training and evaluation protocol for DAOD\nthat addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset,\nCFC-DAOD, enabling evaluation on diverse real-world data, and (4) A new method,\nALDI++, that achieves state-of-the-art results by a large margin. ALDI++\noutperforms the previous state-of-the-art by +3.5 AP50 on Cityscapes to Foggy\nCityscapes, +5.7 AP50 on Sim10k to Cityscapes (where ours is the only method to\noutperform a fair baseline), and +0.6 AP50 on CFC Kenai to Channel. ALDI and\nALDI++ are architecture-agnostic, setting a new state-of-the-art for YOLO and\nDETR-based DAOD as well without additional hyperparameter tuning. Our\nframework, dataset, and state-of-the-art method offer a critical reset for DAOD\nand provide a strong foundation for future research. Code and data are\navailable: https:\/\/github.com\/justinkay\/aldi and\nhttps:\/\/github.com\/visipedia\/caltech-fish-counting.\n","versions":"[{'version': 'v1', 'created': 'Mon, 18 Mar 2024 17:58:02 GMT'}, {'version': 'v2', 'created': 'Sun, 25 Aug 2024 14:05:18 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 20:18:16 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Kay', 'Justin', ''], ['Haucke', 'Timm', ''], ['Stathatos', 'Suzanne', ''], ['Deng', 'Siqi', ''], ['Young', 'Erik', ''], ['Perona', 'Pietro', ''], ['Beery', 'Sara', ''], ['Van Horn', 'Grant', '']]","extracted_entities":"[{'text': 'hyperparameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"hyperparameter tuning","similarity_score":0.6193697453}
{"id":2404.04858,"submitter":"Tony Lindeberg","authors":"Tony Lindeberg","title":"Do the receptive fields in the primary visual cortex span a variability\n  over the degree of elongation of the receptive fields?","comments":"22 pages, 12 figures. Note: Companion paper regarding theoretical\n  analysis in arXiv:2304.11920","journal-ref":null,"doi":null,"report-no":null,"categories":"q-bio.NC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper presents results of combining (i) theoretical analysis regarding\nconnections between the orientation selectivity and the elongation of receptive\nfields for the affine Gaussian derivative model with (ii) biological\nmeasurements of orientation selectivity in the primary visual cortex, to\ninvestigate if (iii) the receptive fields can be regarded as spanning a\nvariability in the degree of elongation.\n  From an in-depth theoretical analysis of idealized models for the receptive\nfields of simple and complex cells in the primary visual cortex, we have\nestablished that the directional selectivity becomes more narrow with\nincreasing elongation of the receptive fields. By comparison with previously\nestablished biological results, concerning broad vs. sharp orientation tuning\nof visual neurons in the primary visual cortex, we demonstrate that those\nunderlying theoretical predictions, in combination with these biological\nresults, are consistent with a previously formulated biological hypothesis,\nstating that the biological receptive field shapes should span the degrees of\nfreedom in affine image transformations, to support affine covariance over the\npopulation of receptive fields in the primary visual cortex.\n  Based on this possible indirect support for the working hypothesis concerning\naffine covariance, we formulate a set of testable predictions that could be\nused to, with neurophysiological experiments, judge if the receptive fields in\nthe primary visual cortex of higher mammals could be regarded as spanning a\nvariability over the eccentricity or the elongation of the receptive fields,\nand, if so, then also characterize if such a variability would, in a structured\nway, be related to the pinwheel structure in the visual cortex.\n","versions":"[{'version': 'v1', 'created': 'Sun, 7 Apr 2024 08:06:12 GMT'}, {'version': 'v2', 'created': 'Tue, 9 Apr 2024 05:29:13 GMT'}, {'version': 'v3', 'created': 'Thu, 11 Apr 2024 10:44:55 GMT'}, {'version': 'v4', 'created': 'Fri, 3 May 2024 05:03:37 GMT'}, {'version': 'v5', 'created': 'Tue, 21 May 2024 14:15:04 GMT'}, {'version': 'v6', 'created': 'Mon, 10 Jun 2024 06:52:07 GMT'}, {'version': 'v7', 'created': 'Wed, 2 Oct 2024 14:43:28 GMT'}, {'version': 'v8', 'created': 'Tue, 22 Oct 2024 12:36:36 GMT'}, {'version': 'v9', 'created': 'Tue, 18 Mar 2025 15:14:32 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Lindeberg', 'Tony', '']]","extracted_entities":"[{'text': 'broad vs. sharp orientation tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"broad vs. sharp orientation tuning","similarity_score":0.6353417635}
{"id":2404.07696,"submitter":"Rui Li","authors":"Rui Li, Martin Trapp, Marcus Klasson, Arno Solin","title":"Flatness Improves Backbone Generalisation in Few-shot Classification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Deployment of deep neural networks in real-world settings typically requires\nadaptation to new tasks with few examples. Few-shot classification (FSC)\nprovides a solution to this problem by leveraging pre-trained backbones for\nfast adaptation to new classes. However, approaches for multi-domain FSC\ntypically result in complex pipelines aimed at information fusion and\ntask-specific adaptation without consideration of the importance of backbone\ntraining. In this work, we introduce an effective strategy for backbone\ntraining and selection in multi-domain FSC by utilizing flatness-aware training\nand fine-tuning. Our work is theoretically grounded and empirically performs on\npar or better than state-of-the-art methods despite being simpler. Further, our\nresults indicate that backbone training is crucial for good generalisation in\nFSC across different adaptation methods.\n","versions":"[{'version': 'v1', 'created': 'Thu, 11 Apr 2024 12:42:18 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 11:19:45 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Li', 'Rui', ''], ['Trapp', 'Martin', ''], ['Klasson', 'Marcus', ''], ['Solin', 'Arno', '']]","extracted_entities":"[{'text': 'Few-shot classification', 'label': 'Zero-shot Learning'}, {'text': 'FSC', 'label': 'Zero-shot Learning'}, {'text': 'FSC', 'label': 'Zero-shot Learning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'FSC', 'label': 'Zero-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2404.15305,"submitter":"Hyungjun Yoon","authors":"Hyungjun Yoon, Jaehyun Kwak, Biniyam Aschalew Tolera, Gaole Dai, Mo\n  Li, Taesik Gong, Kimin Lee and Sung-Ju Lee","title":"SelfReplay: Adapting Self-Supervised Sensory Models via Adaptive\n  Meta-Task Replay","comments":"Accepted to the 23rd ACM Conference on Embedded Networked Sensor\n  Systems (ACM SenSys 2025)","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP cs.LG","license":"http:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/","abstract":"  Self-supervised learning has emerged as a method for utilizing massive\nunlabeled data for pre-training models, providing an effective feature\nextractor for various mobile sensing applications. However, when deployed to\nend-users, these models encounter significant domain shifts attributed to user\ndiversity. We investigate the performance degradation that occurs when\nself-supervised models are fine-tuned in heterogeneous domains. To address the\nissue, we propose SelfReplay, a few-shot domain adaptation framework for\npersonalizing self-supervised models. SelfReplay proposes self-supervised\nmeta-learning for initial model pre-training, followed by a user-side model\nadaptation by replaying the self-supervision with user-specific data. This\nallows models to adjust their pre-trained representations to the user with only\na few samples. Evaluation with four benchmarks demonstrates that SelfReplay\noutperforms existing baselines by an average F1-score of 8.8%p. Our on-device\ncomputational overhead analysis on a commodity off-the-shelf (COTS) smartphone\nshows that SelfReplay completes adaptation within an unobtrusive latency (in\nthree minutes) with only a 9.54% memory consumption, demonstrating the\ncomputational efficiency of the proposed method.\n","versions":"[{'version': 'v1', 'created': 'Fri, 29 Mar 2024 08:48:07 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 11:56:18 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Yoon', 'Hyungjun', ''], ['Kwak', 'Jaehyun', ''], ['Tolera', 'Biniyam Aschalew', ''], ['Dai', 'Gaole', ''], ['Li', 'Mo', ''], ['Gong', 'Taesik', ''], ['Lee', 'Kimin', ''], ['Lee', 'Sung-Ju', '']]","extracted_entities":"[{'text': 'Self-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuned', 'label': 'Fine-tuning'}, {'text': 'self-supervised\\nmeta-learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuned","similarity_score":0.8707774282}
{"id":2405.13637,"submitter":"Radu Tudor Ionescu","authors":"Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Nicu Sebe,\n  Mubarak Shah","title":"Curriculum Direct Preference Optimization for Diffusion and Consistency\n  Models","comments":"Accepted at CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Direct Preference Optimization (DPO) has been proposed as an effective and\nefficient alternative to reinforcement learning from human feedback (RLHF). In\nthis paper, we propose a novel and enhanced version of DPO based on curriculum\nlearning for text-to-image generation. Our method is divided into two training\nstages. First, a ranking of the examples generated for each prompt is obtained\nby employing a reward model. Then, increasingly difficult pairs of examples are\nsampled and provided to a text-to-image generative (diffusion or consistency)\nmodel. Generated samples that are far apart in the ranking are considered to\nform easy pairs, while those that are close in the ranking form hard pairs. In\nother words, we use the rank difference between samples as a measure of\ndifficulty. The sampled pairs are split into batches according to their\ndifficulty levels, which are gradually used to train the generative model. Our\napproach, Curriculum DPO, is compared against state-of-the-art fine-tuning\napproaches on nine benchmarks, outperforming the competing methods in terms of\ntext alignment, aesthetics and human preference. Our code is available at\nhttps:\/\/github.com\/CroitoruAlin\/Curriculum-DPO.\n","versions":"[{'version': 'v1', 'created': 'Wed, 22 May 2024 13:36:48 GMT'}, {'version': 'v2', 'created': 'Fri, 24 May 2024 13:14:40 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 16:44:48 GMT'}, {'version': 'v4', 'created': 'Tue, 18 Mar 2025 13:03:48 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Croitoru', 'Florinel-Alin', ''], ['Hondru', 'Vlad', ''], ['Ionescu', 'Radu Tudor', ''], ['Sebe', 'Nicu', ''], ['Shah', 'Mubarak', '']]","extracted_entities":"[{'text': 'prompt', 'label': 'Prompting'}, {'text': 'state-of-the-art fine-tuning\\napproaches', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"state-of-the-art fine-tuning\napproaches","similarity_score":0.7898973227}
{"id":2406.01445,"submitter":"Yifei Bai","authors":"Yifei Bai and David M. Weld","title":"Tunably-polarized driving light controls the phase diagram of 1D\n  quasicrystals and 2D quantum Hall matter","comments":"Accepted version (PRB). Comments are welcome!","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.atom-ph cond-mat.dis-nn cond-mat.quant-gas","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The well-known mapping between 1D quasiperiodic systems and 2D integer\nquantum Hall matter can also be applied in the presence of driving. Here we\nexplore the effect of time-varying electric fields on the transport properties\nand phase diagram of Harper-Hofstadter materials. We consider light of\narbitrary polarization illuminating a 2D electron gas at high magnetic field;\nthis system maps to a 1D quasicrystal subjected to simultaneous phasonic and\ndipolar driving. We show that this generalized driving generates a tessellated\nphase diagram featuring a nested duality-protected pattern of metal-insulator\ntransitions. Circularly or elliptically polarized light can create an extended\ncritical phase, opening up a new route to achieving wavefunction\nmultifractality without fine-tuning to a critical point. We describe in detail\na path to experimental realization of these phenomena using lattice-trapped\nultracold atoms.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Jun 2024 15:36:05 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Jun 2024 17:40:27 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 19:02:47 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Bai', 'Yifei', ''], ['Weld', 'David M.', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2407.01509,"submitter":"Yusu Qian","authors":"Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei\n  Yang, Zhe Gan","title":"MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal\n  LLMs","comments":"Accepted at ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We introduce MIA-Bench, a new benchmark designed to evaluate multimodal large\nlanguage models (MLLMs) on their ability to strictly adhere to complex\ninstructions. Our benchmark comprises a diverse set of 400 image-prompt pairs,\neach crafted to challenge the models' compliance with layered instructions in\ngenerating accurate responses that satisfy specific requested patterns.\nEvaluation results from a wide array of state-of-the-art MLLMs reveal\nsignificant variations in performance, highlighting areas for improvement in\ninstruction fidelity. Additionally, we create extra training data and explore\nsupervised fine-tuning to enhance the models' ability to strictly follow\ninstructions without compromising performance on other tasks. We hope this\nbenchmark not only serves as a tool for measuring MLLM adherence to\ninstructions, but also guides future developments in MLLM training methods.\n","versions":"[{'version': 'v1', 'created': 'Mon, 1 Jul 2024 17:53:35 GMT'}, {'version': 'v2', 'created': 'Wed, 3 Jul 2024 18:11:45 GMT'}, {'version': 'v3', 'created': 'Thu, 25 Jul 2024 19:50:32 GMT'}, {'version': 'v4', 'created': 'Fri, 21 Feb 2025 03:49:13 GMT'}, {'version': 'v5', 'created': 'Thu, 20 Mar 2025 02:49:09 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Qian', 'Yusu', ''], ['Ye', 'Hanrong', ''], ['Fauconnier', 'Jean-Philippe', ''], ['Grasch', 'Peter', ''], ['Yang', 'Yinfei', ''], ['Gan', 'Zhe', '']]","extracted_entities":"[{'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"supervised fine-tuning","similarity_score":0.7449287176}
{"id":2407.04194,"submitter":"Weihao Li","authors":"Weihao Li, Dongming Huang","title":"Regularization Using Synthetic Data in High-Dimensional Models","comments":"98 pages, 12 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.ST stat.TH","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  To address the challenges of reliable statistical inference in\nhigh-dimensional models, we introduce the Synthetic-data Regularized Estimator\n(SRE). Unlike traditional regularization methods, the SRE regularizes the\ncomplex target model via a weighted likelihood based on synthetic data\ngenerated from a simpler, more stable model. This method provides a\ntheoretically sound and practically effective alternative to parameter\npenalization. We establish key theoretical properties of the SRE in generalized\nlinear models, including existence, stability, consistency, and minimax rate\noptimality. Applying the Convex Gaussian Min-Max Theorem, we derive a precise\nasymptotic characterization in the high-dimensional linear regime. To deal with\nthe non-separable regularization, we introduce a novel decomposition in our\nanalysis. Building upon these results, we develop practical methodologies for\ntuning parameter selection, confidence interval construction, and calibrated\nvariable selection in high-dimensional inference. The effectiveness of the SRE\nis demonstrated through simulation studies and real-data applications.\n","versions":"[{'version': 'v1', 'created': 'Fri, 5 Jul 2024 00:40:03 GMT'}, {'version': 'v2', 'created': 'Tue, 21 Jan 2025 07:41:54 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Feb 2025 09:19:17 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 14:33:10 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Li', 'Weihao', ''], ['Huang', 'Dongming', '']]","extracted_entities":"[{'text': 'tuning parameter selection', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"tuning parameter selection","similarity_score":0.6054506302}
{"id":2407.07066,"submitter":"Fardin Jalil Piran","authors":"Fardin Jalil Piran and Prathyush P. Poduval and Hamza Errahmouni\n  Barkam and Mohsen Imani and Farhad Imani","title":"Explainable Differential Privacy-Hyperdimensional Computing for\n  Balancing Privacy and Transparency in Additive Manufacturing Monitoring","comments":"30 pages, 14 figures","journal-ref":null,"doi":"10.1016\/j.engappai.2025.110282","report-no":null,"categories":"cs.LG cs.CR cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Machine Learning (ML) models integrated with in-situ sensing offer\ntransformative solutions for defect detection in Additive Manufacturing (AM),\nbut this integration brings critical challenges in safeguarding sensitive data,\nsuch as part designs and material compositions. Differential Privacy (DP),\nwhich introduces mathematically controlled noise, provides a balance between\ndata utility and privacy. However, black-box Artificial Intelligence (AI)\nmodels often obscure how this noise impacts model accuracy, complicating the\noptimization of privacy-accuracy trade-offs. This study introduces the\nDifferential Privacy-Hyperdimensional Computing (DP-HD) framework, a novel\napproach combining Explainable AI (XAI) and vector symbolic paradigms to\nquantify and predict noise effects on accuracy using a Signal-to-Noise Ratio\n(SNR) metric. DP-HD enables precise tuning of DP noise levels, ensuring an\noptimal balance between privacy and performance. The framework has been\nvalidated using real-world AM data, demonstrating its applicability to\nindustrial environments. Experimental results demonstrate DP-HD's capability to\nachieve state-of-the-art accuracy (94.43%) with robust privacy protections in\nanomaly detection for AM, even under significant noise conditions. Beyond AM,\nDP-HD holds substantial promise for broader applications in privacy-sensitive\ndomains such as healthcare, financial services, and government data management,\nwhere securing sensitive data while maintaining high ML performance is\nparamount.\n","versions":"[{'version': 'v1', 'created': 'Tue, 9 Jul 2024 17:42:26 GMT'}, {'version': 'v2', 'created': 'Wed, 10 Jul 2024 01:37:05 GMT'}, {'version': 'v3', 'created': 'Thu, 14 Nov 2024 20:13:19 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 21:17:59 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Piran', 'Fardin Jalil', ''], ['Poduval', 'Prathyush P.', ''], ['Barkam', 'Hamza Errahmouni', ''], ['Imani', 'Mohsen', ''], ['Imani', 'Farhad', '']]","extracted_entities":"[{'text': 'precise tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"precise tuning","similarity_score":0.7851148248}
{"id":2407.20642,"submitter":"Basura Fernando","authors":"Dhruv Verma, Debaditya Roy, Basura Fernando","title":"Effectively Leveraging CLIP for Generating Situational Summaries of\n  Images and Videos","comments":"38 pages, 12 figures. arXiv admin note: text overlap with\n  arXiv:2307.00586","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Situation recognition refers to the ability of an agent to identify and\nunderstand various situations or contexts based on available information and\nsensory inputs. It involves the cognitive process of interpreting data from the\nenvironment to determine what is happening, what factors are involved, and what\nactions caused those situations. This interpretation of situations is\nformulated as a semantic role labeling problem in computer vision-based\nsituation recognition. Situations depicted in images and videos hold pivotal\ninformation, essential for various applications like image and video\ncaptioning, multimedia retrieval, autonomous systems and event monitoring.\nHowever, existing methods often struggle with ambiguity and lack of context in\ngenerating meaningful and accurate predictions. Leveraging multimodal models\nsuch as CLIP, we propose ClipSitu, which sidesteps the need for full\nfine-tuning and achieves state-of-the-art results in situation recognition and\nlocalization tasks. ClipSitu harnesses CLIP-based image, verb, and role\nembeddings to predict nouns fulfilling all the roles associated with a verb,\nproviding a comprehensive understanding of depicted scenarios. Through a\ncross-attention Transformer, ClipSitu XTF enhances the connection between\nsemantic role queries and visual token representations, leading to superior\nperformance in situation recognition. We also propose a verb-wise role\nprediction model with near-perfect accuracy to create an end-to-end framework\nfor producing situational summaries for out-of-domain images. We show that\nsituational summaries empower our ClipSitu models to produce structured\ndescriptions with reduced ambiguity compared to generic captions. Finally, we\nextend ClipSitu to video situation recognition to showcase its versatility and\nproduce comparable performance to state-of-the-art methods.\n","versions":"[{'version': 'v1', 'created': 'Tue, 30 Jul 2024 08:39:20 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 04:14:55 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Verma', 'Dhruv', ''], ['Roy', 'Debaditya', ''], ['Fernando', 'Basura', '']]","extracted_entities":"[{'text': 'full\\nfine-tuning', 'label': 'Fine-tuning'}, {'text': 'role\\nembeddings', 'label': 'contextual Embedding'}]","assigned_concept":"Fine-tuning","matched_keyword":"full\nfine-tuning","similarity_score":0.9569243193}
{"id":2408.03394,"submitter":"Zhaoxin Li","authors":"Zhaoxin Li, Xiaoke Wang, Letian Chen, Rohan Paleja, Subramanya\n  Nageshrao, Matthew Gombolay","title":"Faster Model Predictive Control via Self-Supervised Initialization\n  Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Optimization for robot control tasks, spanning various methodologies,\nincludes Model Predictive Control (MPC). However, the complexity of the system,\nsuch as non-convex and non-differentiable cost functions and prolonged planning\nhorizons often drastically increases the computation time, limiting MPC's\nreal-world applicability. Prior works in speeding up the optimization have\nlimitations on optimizing MPC running time directly and generalizing to hold\nout domains. To overcome this challenge, we develop a novel framework aiming at\nexpediting optimization processes directly. In our framework, we combine\noffline self-supervised learning and online fine-tuning to improve the control\nperformance and reduce optimization time. We demonstrate the success of our\nmethod on a novel and challenging Formula 1 track driving task. Comparing to\nsingle-phase training, our approach achieves a 19.4\\% reduction in optimization\ntime and a 6.3\\% improvement in tracking accuracy on zero-shot tracks.\n","versions":"[{'version': 'v1', 'created': 'Tue, 6 Aug 2024 18:41:57 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:36:18 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Li', 'Zhaoxin', ''], ['Wang', 'Xiaoke', ''], ['Chen', 'Letian', ''], ['Paleja', 'Rohan', ''], ['Nageshrao', 'Subramanya', ''], ['Gombolay', 'Matthew', '']]","extracted_entities":"[{'text': 'offline self-supervised learning', 'label': 'Zero-shot Learning'}, {'text': 'online fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"online fine-tuning","similarity_score":0.726175189}
{"id":2408.06663,"submitter":"Kaiser Sun","authors":"Kaiser Sun, Mark Dredze","title":"Amuro and Char: Analyzing the Relationship between Pre-Training and\n  Fine-Tuning of Large Language Models","comments":"Rep4NLP Camera Ready","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The development of large language models leads to the formation of a\npre-train-then-align paradigm, in which the model is typically pre-trained on a\nlarge text corpus and undergoes a tuning stage to align the model with human\npreference or downstream tasks. In this work, we investigate the relationship\nbetween pre-training and fine-tuning by fine-tuning multiple intermediate\npre-trained model checkpoints. Our results on 18 datasets suggest that i)\ncontinual pre-training improves the model in a latent way that unveils after\nfine-tuning; ii) with extra fine-tuning, the datasets that the model does not\ndemonstrate capability gain much more than those that the model performs well\nduring the pre-training stage; iii) although model benefits significantly\nthrough supervised fine-tuning, it may forget previously known domain knowledge\nand the tasks that are not seen during fine-tuning; iv) the model resembles\nhigh sensitivity to evaluation prompts after supervised fine-tuning, but this\nsensitivity can be alleviated by more pre-training.\n","versions":"[{'version': 'v1', 'created': 'Tue, 13 Aug 2024 06:28:43 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Aug 2024 15:23:38 GMT'}, {'version': 'v3', 'created': 'Sun, 2 Feb 2025 22:07:55 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Feb 2025 16:57:29 GMT'}, {'version': 'v5', 'created': 'Tue, 18 Mar 2025 16:21:04 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Sun', 'Kaiser', ''], ['Dredze', 'Mark', '']]","extracted_entities":"[{'text': 'model', 'label': 'Neural Language Model'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'model', 'label': 'Neural Language Model'}, {'text': 'evaluation prompts', 'label': 'Prompting'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2408.1675,"submitter":"Bruno Bertini","authors":"Alessandro Foligno and Bruno Bertini","title":"Entanglement of Disjoint Intervals in Dual-Unitary Circuits: Exact\n  Results","comments":"6+5 pages, 4 figures; v2 11 pages, 4 figures presentation improved;\n  v3 version accepted by Quantum","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.stat-mech hep-th math-ph math.MP quant-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The growth of the entanglement between two disjoint intervals and its\ncomplement after a quantum quench is regarded as a dynamical chaos indicator.\nNamely, it is expected to show qualitatively different behaviours depending on\nwhether the underlying microscopic dynamics is chaotic or integrable. So far,\nhowever, this could only be verified in the context of conformal field\ntheories. Here we present an exact confirmation of this expectation in a class\nof interacting microscopic Floquet systems on the lattice, i.e., dual-unitary\ncircuits. These systems can either have zero or a super extensive number of\nconserved charges: the latter case is achieved via fine-tuning. We show that,\nfor almost all dual unitary circuits on qubits and for a large family of\ndual-unitary circuits on qudits the asymptotic entanglement dynamics agrees\nwith what is expected for chaotic systems. On the other hand, if we require the\nsystems to have conserved charges, we find that the entanglement displays the\nqualitatively different behaviour expected for integrable systems.\nInterestingly, despite having many conserved charges, charge-conserving\ndual-unitary circuits are in general not Yang-Baxter integrable.\n","versions":"[{'version': 'v1', 'created': 'Thu, 29 Aug 2024 17:45:27 GMT'}, {'version': 'v2', 'created': 'Tue, 19 Nov 2024 14:41:31 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 18:11:09 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Foligno', 'Alessandro', ''], ['Bertini', 'Bruno', '']]","extracted_entities":"[{'text': 'entanglement', 'label': 'quantisation'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'entanglement', 'label': 'quantisation'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2409.01586,"submitter":"Tiansheng Huang","authors":"Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu","title":"Booster: Tackling Harmful Fine-tuning for Large Language Models via\n  Attenuating Harmful Perturbation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Harmful fine-tuning attack poses serious safety concerns for large language\nmodels' fine-tuning-as-a-service. While existing defenses have been proposed to\nmitigate the issue, their performances are still far away from satisfactory,\nand the root cause of the problem has not been fully recovered. To this end, we\nin this paper show that harmful perturbation over the model weights could be a\nprobable cause of alignment-broken. In order to attenuate the negative impact\nof harmful perturbation, we propose an alignment-stage solution, dubbed\nBooster. Technically, along with the original alignment loss, we append a loss\nregularizer in the alignment stage's optimization. The regularizer ensures that\nthe model's harmful loss reduction after the simulated harmful perturbation is\nattenuated, thereby mitigating the subsequent fine-tuning risk. Empirical\nresults show that Booster can effectively reduce the harmful score of the\nfine-tuned models while maintaining the performance of downstream tasks. Our\ncode is available at https:\/\/github.com\/git-disl\/Booster.\n","versions":"[{'version': 'v1', 'created': 'Tue, 3 Sep 2024 03:59:22 GMT'}, {'version': 'v2', 'created': 'Wed, 4 Sep 2024 19:30:59 GMT'}, {'version': 'v3', 'created': 'Wed, 18 Sep 2024 19:03:30 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 17:17:16 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Huang', 'Tiansheng', ''], ['Hu', 'Sihao', ''], ['Ilhan', 'Fatih', ''], ['Tekin', 'Selim Furkan', ''], ['Liu', 'Ling', '']]","extracted_entities":"[{'text': 'fine-tuning-as-a-service', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning-as-a-service","similarity_score":0.6439697742}
{"id":2409.02606,"submitter":"Rafael Pastrana","authors":"Rafael Pastrana, Eder Medina, Isabel M. de Oliveira, Sigrid\n  Adriaenssens, Ryan P. Adams","title":"Real-time design of architectural structures with differentiable\n  mechanics and neural networks","comments":"International Conference on Learning Representations (ICLR) 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Designing mechanically efficient geometry for architectural structures like\nshells, towers, and bridges, is an expensive iterative process. Existing\ntechniques for solving such inverse problems rely on traditional optimization\nmethods, which are slow and computationally expensive, limiting iteration speed\nand design exploration. Neural networks would seem to offer a solution via\ndata-driven amortized optimization, but they often require extensive\nfine-tuning and cannot ensure that important design criteria, such as\nmechanical integrity, are met. In this work, we combine neural networks with a\ndifferentiable mechanics simulator to develop a model that accelerates the\nsolution of shape approximation problems for architectural structures\nrepresented as bar systems. This model explicitly guarantees compliance with\nmechanical constraints while generating designs that closely match target\ngeometries. We validate our approach in two tasks, the design of masonry shells\nand cable-net towers. Our model achieves better accuracy and generalization\nthan fully neural alternatives, and comparable accuracy to direct optimization\nbut in real time, enabling fast and reliable design exploration. We further\ndemonstrate its advantages by integrating it into 3D modeling software and\nfabricating a physical prototype. Our work opens up new opportunities for\naccelerated mechanical design enhanced by neural networks for the built\nenvironment.\n","versions":"[{'version': 'v1', 'created': 'Wed, 4 Sep 2024 10:41:50 GMT'}, {'version': 'v2', 'created': 'Thu, 3 Oct 2024 20:02:47 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 15:37:40 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Pastrana', 'Rafael', ''], ['Medina', 'Eder', ''], ['de Oliveira', 'Isabel M.', ''], ['Adriaenssens', 'Sigrid', ''], ['Adams', 'Ryan P.', '']]","extracted_entities":"[{'text': 'extensive\\nfine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"extensive\nfine-tuning","similarity_score":0.9066202044}
{"id":2409.11355,"submitter":"Karim Abou Zeid","authors":"Gonzalo Martin Garcia, Karim Abou Zeid, Christian Schmidt, Daan de\n  Geus, Alexander Hermans, Bastian Leibe","title":"Fine-Tuning Image-Conditional Diffusion Models is Easier than You Think","comments":"WACV 2025 Oral. Project page at\n  https:\/\/vision.rwth-aachen.de\/diffusion-e2e-ft","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent work showed that large diffusion models can be reused as highly\nprecise monocular depth estimators by casting depth estimation as an\nimage-conditional image generation task. While the proposed model achieved\nstate-of-the-art results, high computational demands due to multi-step\ninference limited its use in many scenarios. In this paper, we show that the\nperceived inefficiency was caused by a flaw in the inference pipeline that has\nso far gone unnoticed. The fixed model performs comparably to the best\npreviously reported configuration while being more than 200$\\times$ faster. To\noptimize for downstream task performance, we perform end-to-end fine-tuning on\ntop of the single-step model with task-specific losses and get a deterministic\nmodel that outperforms all other diffusion-based depth and normal estimation\nmodels on common zero-shot benchmarks. We surprisingly find that this\nfine-tuning protocol also works directly on Stable Diffusion and achieves\ncomparable performance to current state-of-the-art diffusion-based depth and\nnormal estimation models, calling into question some of the conclusions drawn\nfrom prior works.\n","versions":"[{'version': 'v1', 'created': 'Tue, 17 Sep 2024 16:58:52 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 21:59:31 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Garcia', 'Gonzalo Martin', ''], ['Zeid', 'Karim Abou', ''], ['Schmidt', 'Christian', ''], ['de Geus', 'Daan', ''], ['Hermans', 'Alexander', ''], ['Leibe', 'Bastian', '']]","extracted_entities":"[{'text': 'end-to-end fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"end-to-end fine-tuning","similarity_score":0.8521200418}
{"id":2409.13052,"submitter":"Hamed Rahimi Nohooji Dr","authors":"Hamed Rahimi Nohooji, Holger Voos","title":"Adaptive Trajectory Optimization for Task-Specific Human-Robot\n  Collaboration","comments":"7 pages, 6 figures, 1 table","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper proposes a task-specific trajectory optimization framework for\nhuman-robot collaboration, enabling adaptive motion planning based on human\ninteraction dynamics. Unlike conventional approaches that rely on predefined\ndesired trajectories, the proposed framework optimizes the collaborative motion\ndynamically using the inverse differential Riccati equation, ensuring\nadaptability to task variations and human input. The generated trajectory\nserves as the reference for a neuro-adaptive PID controller, which leverages a\nneural network to adjust control gains in real time, addressing system\nuncertainties while maintaining low computational complexity. The combination\nof trajectory planning and the adaptive control law ensures stability and\naccurate joint-space tracking without requiring extensive parameter tuning.\nNumerical simulations validate the proposed approach.\n","versions":"[{'version': 'v1', 'created': 'Thu, 19 Sep 2024 19:02:01 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 19:41:12 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Nohooji', 'Hamed Rahimi', ''], ['Voos', 'Holger', '']]","extracted_entities":"[{'text': 'inverse differential Riccati equation', 'label': 'Scaling law'}, {'text': 'adaptive control law', 'label': 'Scaling law'}, {'text': 'parameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"parameter tuning","similarity_score":0.6959539652}
{"id":2409.13878,"submitter":"Amirmohammad Mohammadi","authors":"Amirmohammad Mohammadi, Tejashri Kelhe, Davelle Carreiro, Alexandra\n  Van Dine, Joshua Peeples","title":"Cross-Domain Knowledge Transfer for Underwater Acoustic Classification\n  Using Pre-trained Models","comments":"6 pages, 4 figures, This work has been submitted to the IEEE for\n  possible publication. Added Grad-CAM analysis. Title changed. This work has\n  been accepted to IEEE OCEANS 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.LG eess.AS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Transfer learning is commonly employed to leverage large, pre-trained models\nand perform fine-tuning for downstream tasks. The most prevalent pre-trained\nmodels are initially trained using ImageNet. However, their ability to\ngeneralize can vary across different data modalities. This study compares\npre-trained Audio Neural Networks (PANNs) and ImageNet pre-trained models\nwithin the context of underwater acoustic target recognition (UATR). It was\nobserved that the ImageNet pre-trained models slightly out-perform pre-trained\naudio models in passive sonar classification. We also analyzed the impact of\naudio sampling rates for model pre-training and fine-tuning. This study\ncontributes to transfer learning applications of UATR, illustrating the\npotential of pre-trained models to address limitations caused by scarce,\nlabeled data in the UATR domain.\n","versions":"[{'version': 'v1', 'created': 'Fri, 20 Sep 2024 20:13:45 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 19:33:19 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Mohammadi', 'Amirmohammad', ''], ['Kelhe', 'Tejashri', ''], ['Carreiro', 'Davelle', ''], ['Van Dine', 'Alexandra', ''], ['Peeples', 'Joshua', '']]","extracted_entities":"[{'text': 'Transfer learning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'transfer learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2409.15771,"submitter":"Yuanzhao Zhang","authors":"Yuanzhao Zhang and William Gilpin","title":"Zero-shot forecasting of chaotic systems","comments":"13th International Conference on Learning Representations (ICLR 2025)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG nlin.CD physics.comp-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Time-series forecasting is a challenging problem that traditionally requires\nspecialized models custom-trained for the specific task at hand. Recently,\ninspired by the success of large language models, foundation models pre-trained\non vast amounts of time-series data from diverse domains have emerged as a\npromising candidate for general-purpose time-series forecasting. The defining\ncharacteristic of these foundation models is their ability to perform zero-shot\nlearning, that is, forecasting a new system from limited context data without\nexplicit re-training or fine-tuning. Here, we evaluate whether the zero-shot\nlearning paradigm extends to the challenging task of forecasting chaotic\nsystems. Across 135 distinct chaotic dynamical systems and $10^8$ timepoints,\nwe find that foundation models produce competitive forecasts compared to\ncustom-trained models (including NBEATS, TiDE, etc.), particularly when\ntraining data is limited. Interestingly, even after point forecasts fail, large\nfoundation models are able to preserve the geometric and statistical properties\nof the chaotic attractors. We attribute this success to foundation models'\nability to perform in-context learning and identify context parroting as a\nsimple mechanism used by these models to capture the long-term behavior of\nchaotic dynamical systems. Our results highlight the potential of foundation\nmodels as a tool for probing nonlinear and complex systems.\n","versions":"[{'version': 'v1', 'created': 'Tue, 24 Sep 2024 05:56:58 GMT'}, {'version': 'v2', 'created': 'Tue, 3 Dec 2024 03:41:01 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 18:24:12 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhang', 'Yuanzhao', ''], ['Gilpin', 'William', '']]","extracted_entities":"[{'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'zero-shot\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'zero-shot\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'in-context learning', 'label': 'Few-shot Learning'}, {'text': 'context parroting', 'label': 'contextual Embedding'}, {'text': 'foundation\\nmodels', 'label': 'Foundation Model'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2409.18584,"submitter":"Jiaming Zhou","authors":"Jiaming Zhou, Shiyao Wang, Shiwan Zhao, Jiabei He, Haoqin Sun, Hui\n  Wang, Cheng Liu, Aobo Kong, Yujie Guo, Xi Yang, Yequan Wang, Yonghua Lin and\n  Yong Qin","title":"ChildMandarin: A Comprehensive Mandarin Speech Dataset for Young\n  Children Aged 3-5","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD eess.AS","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Automatic speech recognition (ASR) systems have advanced significantly with\nmodels like Whisper, Conformer, and self-supervised frameworks such as Wav2vec\n2.0 and HuBERT. However, developing robust ASR models for young children's\nspeech remains challenging due to differences in pronunciation, tone, and pace\ncompared to adult speech. In this paper, we introduce a new Mandarin speech\ndataset focused on children aged 3 to 5, addressing the scarcity of resources\nin this area. The dataset comprises 41.25 hours of speech with carefully\ncrafted manual transcriptions, collected from 397 speakers across various\nprovinces in China, with balanced gender representation. We provide a\ncomprehensive analysis of speaker demographics, speech duration distribution\nand geographic coverage. Additionally, we evaluate ASR performance on models\ntrained from scratch, such as Conformer, as well as fine-tuned pre-trained\nmodels like HuBERT and Whisper, where fine-tuning demonstrates significant\nperformance improvements. Furthermore, we assess speaker verification (SV) on\nour dataset, showing that, despite the challenges posed by the unique vocal\ncharacteristics of young children, the dataset effectively supports both ASR\nand SV tasks. This dataset is a valuable contribution to Mandarin child speech\nresearch. The dataset is now open-source and freely available for all academic\npurposes on https:\/\/github.com\/flageval-baai\/ChildMandarin.\n","versions":"[{'version': 'v1', 'created': 'Fri, 27 Sep 2024 09:42:27 GMT'}, {'version': 'v2', 'created': 'Mon, 30 Sep 2024 12:49:04 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 12:06:13 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhou', 'Jiaming', ''], ['Wang', 'Shiyao', ''], ['Zhao', 'Shiwan', ''], ['He', 'Jiabei', ''], ['Sun', 'Haoqin', ''], ['Wang', 'Hui', ''], ['Liu', 'Cheng', ''], ['Kong', 'Aobo', ''], ['Guo', 'Yujie', ''], ['Yang', 'Xi', ''], ['Wang', 'Yequan', ''], ['Lin', 'Yonghua', ''], ['Qin', 'Yong', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2410.01949,"submitter":"Anji Liu","authors":"Anji Liu, Oliver Broadrick, Mathias Niepert, and Guy Van den Broeck","title":"Discrete Copula Diffusion","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Discrete diffusion models have recently shown significant progress in\nmodeling complex data, such as natural languages and DNA sequences. However,\nunlike diffusion models for continuous data, which can generate high-quality\nsamples in just a few denoising steps, modern discrete diffusion models still\nrequire hundreds or even thousands of denoising steps to perform well. In this\npaper, we identify a fundamental limitation that prevents discrete diffusion\nmodels from achieving strong performance with fewer steps -- they fail to\ncapture dependencies between output variables at each denoising step. To\naddress this issue, we provide a formal explanation and introduce a general\napproach to supplement the missing dependency information by incorporating\nanother deep generative model, termed the copula model. Our method does not\nrequire fine-tuning either the diffusion model or the copula model, yet it\nenables high-quality sample generation with significantly fewer denoising\nsteps. When we apply this approach to autoregressive copula models, the\ncombined model outperforms both models individually in unconditional and\nconditional text generation. Specifically, the hybrid model achieves better\n(un)conditional text generation using 8 to 32 times fewer denoising steps than\nthe diffusion model alone. In addition to presenting an effective discrete\ndiffusion generation algorithm, this paper emphasizes the importance of\nmodeling inter-variable dependencies in discrete diffusion.\n","versions":"[{'version': 'v1', 'created': 'Wed, 2 Oct 2024 18:51:38 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 08:34:29 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Liu', 'Anji', ''], ['Broadrick', 'Oliver', ''], ['Niepert', 'Mathias', ''], ['Broeck', 'Guy Van den', '']]","extracted_entities":"[{'text': 'discrete diffusion\\nmodels', 'label': 'Neural Language Model'}, {'text': 'copula model', 'label': 'AI model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'diffusion model', 'label': 'AI model'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2410.0527,"submitter":"Mohammad Fahes","authors":"Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick P\\'erez, Raoul de\n  Charette","title":"CLIP's Visual Embedding Projector is a Few-shot Cornucopia","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We consider the problem of adapting a contrastively pretrained\nvision-language model like CLIP (Radford et al., 2021) for few-shot\nclassification. The literature addresses this problem by learning a linear\nclassifier of the frozen visual features, optimizing word embeddings, or\nlearning external feature adapters. We introduce an alternative way for\nfew-shot CLIP adaptation without adding ''external'' parameters to optimize. We\nfind that simply fine-tuning the embedding projection matrix of the vision\nencoder leads to better performance than all baselines. Furthermore, we show\nthat regularizing training with the distance between the fine-tuned and\npretrained matrices adds reliability for adapting CLIP, making the results\nstable across different learning rates in the ''validation-free'' setting. This\nsimple approach, coined ProLIP, yields state-of-the-art performance on 11\nfew-shot classification benchmarks, few-shot cross-dataset transfer, domain\ngeneralization, and base-to-new class generalization. We also show that ProLIP\nsignificantly outperforms prompt tuning when extended to another task of\ntest-time adaptation, while being one order of magnitude faster to train. Code\nwill be made available at: https:\/\/github.com\/astra-vision\/ProLIP .\n","versions":"[{'version': 'v1', 'created': 'Mon, 7 Oct 2024 17:59:59 GMT'}, {'version': 'v2', 'created': 'Fri, 6 Dec 2024 16:07:47 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 17:52:55 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Fahes', 'Mohammad', ''], ['Vu', 'Tuan-Hung', ''], ['Bursuc', 'Andrei', ''], ['P\u00e9rez', 'Patrick', ''], ['de Charette', 'Raoul', '']]","extracted_entities":"[{'text': 'few-shot\\nclassification', 'label': 'Zero-shot Learning'}, {'text': 'word embeddings', 'label': 'Embedding'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'few-shot cross-dataset transfer', 'label': 'Few-shot Learning'}, {'text': 'domain\\ngeneralization', 'label': 'Few-shot Learning'}, {'text': 'base-to-new class generalization', 'label': 'Few-shot Learning'}, {'text': 'prompt tuning', 'label': 'Prompting'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2410.07933,"submitter":"Carolin Schmidt","authors":"Carolin Schmidt, Daniele Gammelli, James Harrison, Marco Pavone,\n  Filipe Rodrigues","title":"Offline Hierarchical Reinforcement Learning via Inverse Optimization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.SY eess.SY math.OC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Hierarchical policies enable strong performance in many sequential\ndecision-making problems, such as those with high-dimensional action spaces,\nthose requiring long-horizon planning, and settings with sparse rewards.\nHowever, learning hierarchical policies from static offline datasets presents a\nsignificant challenge. Crucially, actions taken by higher-level policies may\nnot be directly observable within hierarchical controllers, and the offline\ndataset might have been generated using a different policy structure, hindering\nthe use of standard offline learning algorithms. In this work, we propose OHIO:\na framework for offline reinforcement learning (RL) of hierarchical policies.\nOur framework leverages knowledge of the policy structure to solve the\n\\textit{inverse problem}, recovering the unobservable high-level actions that\nlikely generated the observed data under our hierarchical policy. This approach\nconstructs a dataset suitable for off-the-shelf offline training. We\ndemonstrate our framework on robotic and network optimization problems and show\nthat it substantially outperforms end-to-end RL methods and improves\nrobustness. We investigate a variety of instantiations of our framework, both\nin direct deployment of policies trained offline and when online fine-tuning is\nperformed. Code and data are available at\nhttps:\/\/ohio-offline-hierarchical-rl.github.io\n","versions":"[{'version': 'v1', 'created': 'Thu, 10 Oct 2024 14:00:21 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 15:30:08 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Schmidt', 'Carolin', ''], ['Gammelli', 'Daniele', ''], ['Harrison', 'James', ''], ['Pavone', 'Marco', ''], ['Rodrigues', 'Filipe', '']]","extracted_entities":"[{'text': 'online fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"online fine-tuning","similarity_score":0.726175189}
{"id":2410.08621,"submitter":"Jeppe H. Surrow","authors":"Jeppe H. Surrow, Simon T. Thomsen, Rakesh R. Kumar, M\\'onica Far\n  Brusatori, Maria Paula Montes, Ahan S. Palsole, Chris Hoede, Holger N. Klein,\n  Nicolas Volet","title":"Ultra-narrow linewidth laser across the C-band using\n  polarization-controlled dual-cavity feedback","comments":null,"journal-ref":"Opt. Express 33, 11863-11875 (2025)","doi":"10.1364\/OE.544372","report-no":null,"categories":"physics.optics","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  A standard method to reduce the linewidth of semiconductor lasers involves\nthe use of external optical feedback (EOF). However, feedback powers less than\n1 % usually trigger coherence collapse (CC), leading to chaotic laser dynamics\nand linewidth broadening. This paper explores a method to mitigate CC through\nprecise tuning of the feedback polarization depending on the feedback power. We\nreport a semiconductor laser with a sub-100 Hz intrinsic linewidth, achieved\nvia EOF. The laser features a U-shaped cavity with two sampled grating\ndistributed Bragg reflectors (SG-DBRs), enabling broad tunability across a 42\nnm wavelength range (1513-1555 nm). By injecting optical feedback into both\nsides of the laser cavity via an external fiber-based cavity, we reduce the\nintrinsic linewidth by more than three orders of magnitude, from MHz to sub-kHz\nacross the laser's tuning range. By dynamically tuning the polarization, we\ndemonstrate sub-100 Hz intrinsic linewidths at feedback powers up to 10 %,\nmarking an improvement over prior studies where CC limited performance.\n","versions":"[{'version': 'v1', 'created': 'Fri, 11 Oct 2024 08:36:50 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 13:38:46 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Surrow', 'Jeppe H.', ''], ['Thomsen', 'Simon T.', ''], ['Kumar', 'Rakesh R.', ''], ['Brusatori', 'M\u00f3nica Far', ''], ['Montes', 'Maria Paula', ''], ['Palsole', 'Ahan S.', ''], ['Hoede', 'Chris', ''], ['Klein', 'Holger N.', ''], ['Volet', 'Nicolas', '']]","extracted_entities":"[{'text': 'feedback powers', 'label': 'LLM-powered'}, {'text': 'precise tuning', 'label': 'Fine-tuning'}, {'text': 'feedback powers', 'label': 'LLM-powered'}]","assigned_concept":"Fine-tuning","matched_keyword":"precise tuning","similarity_score":0.7851148248}
{"id":2410.1088,"submitter":"Hengxiang Zhang","authors":"Hengxiang Zhang, Songxin Zhang, Bingyi Jing, Hongxin Wei","title":"Fine-tuning can Help Detect Pretraining Data from Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In the era of large language models (LLMs), detecting pretraining data has\nbeen increasingly important due to concerns about fair evaluation and ethical\nrisks. Current methods differentiate members and non-members by designing\nscoring functions, like Perplexity and Min-k%. However, the diversity and\ncomplexity of training data magnifies the difficulty of distinguishing, leading\nto suboptimal performance in detecting pretraining data. In this paper, we\nfirst explore the benefits of unseen data, which can be easily collected after\nthe release of the LLM. We find that the perplexities of LLMs shift differently\nfor members and non-members, after fine-tuning with a small amount of\npreviously unseen data. In light of this, we introduce a novel and effective\nmethod termed Fine-tuned Score Deviation(FSD), which improves the performance\nof current scoring functions for pretraining data detection. In particular, we\npropose to measure the deviation distance of current scores after fine-tuning\non a small amount of unseen data within the same domain. In effect, using a few\nunseen data can largely decrease the scores of all non-members, leading to a\nlarger deviation distance than members. Extensive experiments demonstrate the\neffectiveness of our method, significantly improving the AUC score on common\nbenchmark datasets across various models.\n","versions":"[{'version': 'v1', 'created': 'Wed, 9 Oct 2024 15:36:42 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 12:29:05 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Zhang', 'Hengxiang', ''], ['Zhang', 'Songxin', ''], ['Jing', 'Bingyi', ''], ['Wei', 'Hongxin', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'fair evaluation', 'label': 'AI Ethics'}, {'text': 'ethical\\nrisks', 'label': 'AI Ethics'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Fine-tuned Score Deviation(FSD)', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2410.11236,"submitter":"Guiyu Zhang","authors":"Guiyu Zhang, Huan-ang Gao, Zijian Jiang, Hao Zhao, Zhedong Zheng","title":"Ctrl-U: Robust Conditional Image Generation via Uncertainty-aware Reward\n  Modeling","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this paper, we focus on the task of conditional image generation, where an\nimage is synthesized according to user instructions. The critical challenge\nunderpinning this task is ensuring both the fidelity of the generated images\nand their semantic alignment with the provided conditions. To tackle this\nissue, previous studies have employed supervised perceptual losses derived from\npre-trained models, i.e., reward models, to enforce alignment between the\ncondition and the generated result. However, we observe one inherent\nshortcoming: considering the diversity of synthesized images, the reward model\nusually provides inaccurate feedback when encountering newly generated data,\nwhich can undermine the training process. To address this limitation, we\npropose an uncertainty-aware reward modeling, called Ctrl-U, including\nuncertainty estimation and uncertainty-aware regularization, designed to reduce\nthe adverse effects of imprecise feedback from the reward model. Given the\ninherent cognitive uncertainty within reward models, even images generated\nunder identical conditions often result in a relatively large discrepancy in\nreward loss. Inspired by the observation, we explicitly leverage such\nprediction variance as an uncertainty indicator. Based on the uncertainty\nestimation, we regularize the model training by adaptively rectifying the\nreward. In particular, rewards with lower uncertainty receive higher loss\nweights, while those with higher uncertainty are given reduced weights to allow\nfor larger variability. The proposed uncertainty regularization facilitates\nreward fine-tuning through consistency construction. Extensive experiments\nvalidate the effectiveness of our methodology in improving the controllability\nand generation quality, as well as its scalability across diverse conditional\nscenarios. Codes are publicly available at\nhttps:\/\/grenoble-zhang.github.io\/Ctrl-U-Page\/.\n","versions":"[{'version': 'v1', 'created': 'Tue, 15 Oct 2024 03:43:51 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Feb 2025 17:41:03 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 14:41:25 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhang', 'Guiyu', ''], ['Gao', 'Huan-ang', ''], ['Jiang', 'Zijian', ''], ['Zhao', 'Hao', ''], ['Zheng', 'Zhedong', '']]","extracted_entities":"[{'text': 'reward fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"reward fine-tuning","similarity_score":0.5952739716}
{"id":2410.1385,"submitter":"Bruno Mlodozeniec","authors":"Bruno Mlodozeniec, Runa Eschenhagen, Juhan Bae, Alexander Immer, David\n  Krueger, Richard Turner","title":"Influence Functions for Scalable Data Attribution in Diffusion Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Diffusion models have led to significant advancements in generative\nmodelling. Yet their widespread adoption poses challenges regarding data\nattribution and interpretability. In this paper, we aim to help address such\nchallenges in diffusion models by developing an influence functions framework.\nInfluence function-based data attribution methods approximate how a model's\noutput would have changed if some training data were removed. In supervised\nlearning, this is usually used for predicting how the loss on a particular\nexample would change. For diffusion models, we focus on predicting the change\nin the probability of generating a particular example via several proxy\nmeasurements. We show how to formulate influence functions for such quantities\nand how previously proposed methods can be interpreted as particular design\nchoices in our framework. To ensure scalability of the Hessian computations in\ninfluence functions, we systematically develop K-FAC approximations based on\ngeneralised Gauss-Newton matrices specifically tailored to diffusion models. We\nrecast previously proposed methods as specific design choices in our framework\nand show that our recommended method outperforms previous data attribution\napproaches on common evaluations, such as the Linear Data-modelling Score (LDS)\nor retraining without top influences, without the need for method-specific\nhyperparameter tuning.\n","versions":"[{'version': 'v1', 'created': 'Thu, 17 Oct 2024 17:59:02 GMT'}, {'version': 'v2', 'created': 'Thu, 24 Oct 2024 17:43:00 GMT'}, {'version': 'v3', 'created': 'Tue, 7 Jan 2025 15:28:09 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 13:47:39 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Mlodozeniec', 'Bruno', ''], ['Eschenhagen', 'Runa', ''], ['Bae', 'Juhan', ''], ['Immer', 'Alexander', ''], ['Krueger', 'David', ''], ['Turner', 'Richard', '']]","extracted_entities":"[{'text': 'supervised\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'method-specific\\nhyperparameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"method-specific\nhyperparameter tuning","similarity_score":0.5459896922}
{"id":2410.16713,"submitter":"Joshua Kazdan","authors":"Joshua Kazdan, Rylan Schaeffer, Apratim Dey, Matthias Gerstgrasser,\n  Rafael Rafailov, David L. Donoho, Sanmi Koyejo","title":"Collapse or Thrive? Perils and Promises of Synthetic Data in a\n  Self-Generating World","comments":"Accepted at NeurIPS 2024 Workshops: Mathematics of Modern Machine\n  Learning (M3L) and Attributing Model Behavior at Scale (ATTRIB)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  What happens when generative machine learning models are pretrained on\nweb-scale datasets containing data generated by earlier models? Some prior work\nwarns of \"model collapse\" as the web is overwhelmed by synthetic data; other\nwork suggests the problem can be contained (i.e. collapse can be avoided) by\nmanaging how available data are used in pretraining. In this paper, we report\nexperiments on three ways of using data (training-workflows), across three\ngenerative model task-settings (multivariate Gaussian estimation, kernel\ndensity estimation, and language-model fine-tuning) to further confirm the\npossibility of containment: (a) we confirm that the training-workflow of {\\it\nreplacing} all real data by successive generations of purely synthetic data\nindeed suffers model collapse in all task-settings studied; (b) we consider the\ntraining-workflow of {\\it accumulating} synthetic data alongside real data and\ntraining on all data combined and confirming that, although the proportion of\nreal data eventually becomes zero, models remain stable and their test losses\ndo not diverge under this training-workflow; (c) we consider a\ntraining-workflow where real and synthetic data accumulate together but\nsuccessive generations of pretraining are constrained to use fixed-size data\nsubsets each generation. In this workflow, we observe slow and gradual rather\nthan explosive degradation of test loss performance across generations. Our\ninsights are particularly important when forecasting whether future frontier\ngenerative models will collapse or thrive, and our results open avenues for\nempirically and mathematically studying the context-dependent value of\nsynthetic data.\n","versions":"[{'version': 'v1', 'created': 'Tue, 22 Oct 2024 05:49:24 GMT'}, {'version': 'v2', 'created': 'Mon, 16 Dec 2024 06:37:01 GMT'}, {'version': 'v3', 'created': 'Thu, 6 Feb 2025 00:43:54 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 21:14:46 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Kazdan', 'Joshua', ''], ['Schaeffer', 'Rylan', ''], ['Dey', 'Apratim', ''], ['Gerstgrasser', 'Matthias', ''], ['Rafailov', 'Rafael', ''], ['Donoho', 'David L.', ''], ['Koyejo', 'Sanmi', '']]","extracted_entities":"[{'text': 'multivariate Gaussian estimation', 'label': 'Zero-shot Learning'}, {'text': 'kernel\\ndensity estimation', 'label': 'Zero-shot Learning'}, {'text': 'language-model fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"language-model fine-tuning","similarity_score":0.6101994514}
{"id":2411.01667,"submitter":"Jonathan Pirnay","authors":"Jonathan Pirnay, Jan G. Rittig, Alexander B. Wolf, Martin Grohe, Jakob\n  Burger, Alexander Mitsos, Dominik G. Grimm","title":"GraphXForm: Graph transformer for computer-aided molecular design","comments":"Published in Digital Discovery, 2025","journal-ref":null,"doi":"10.1039\/D4DD00339J","report-no":null,"categories":"cs.LG physics.chem-ph q-bio.BM","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Generative deep learning has become pivotal in molecular design for drug\ndiscovery, materials science, and chemical engineering. A widely used paradigm\nis to pretrain neural networks on string representations of molecules and\nfine-tune them using reinforcement learning on specific objectives. However,\nstring-based models face challenges in ensuring chemical validity and enforcing\nstructural constraints like the presence of specific substructures. We propose\nto instead combine graph-based molecular representations, which can naturally\nensure chemical validity, with transformer architectures, which are highly\nexpressive and capable of modeling long-range dependencies between atoms. Our\napproach iteratively modifies a molecular graph by adding atoms and bonds,\nwhich ensures chemical validity and facilitates the incorporation of structural\nconstraints. We present GraphXForm, a decoder-only graph transformer\narchitecture, which is pretrained on existing compounds and then fine-tuned\nusing a new training algorithm that combines elements of the deep cross-entropy\nmethod and self-improvement learning. We evaluate GraphXForm on various drug\ndesign tasks, demonstrating superior objective scores compared to\nstate-of-the-art molecular design approaches. Furthermore, we apply GraphXForm\nto two solvent design tasks for liquid-liquid extraction, again outperforming\nalternative methods while flexibly enforcing structural constraints or\ninitiating design from existing molecular structures.\n","versions":"[{'version': 'v1', 'created': 'Sun, 3 Nov 2024 19:45:15 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 12:01:38 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Pirnay', 'Jonathan', ''], ['Rittig', 'Jan G.', ''], ['Wolf', 'Alexander B.', ''], ['Grohe', 'Martin', ''], ['Burger', 'Jakob', ''], ['Mitsos', 'Alexander', ''], ['Grimm', 'Dominik G.', '']]","extracted_entities":"[{'text': 'reinforcement learning', 'label': 'Few-shot Learning'}, {'text': 'transformer architectures', 'label': 'Transformers'}, {'text': 'GraphXForm', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'fine-tuned', 'label': 'Fine-tuning'}, {'text': 'GraphXForm', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'GraphXForm', 'label': 'Generative Pre-trained Transformer (GPT)'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuned","similarity_score":0.8707774282}
{"id":2411.04425,"submitter":"Ishika Agarwal","authors":"Ishika Agarwal, Krishnateja Killamsetty, Lucian Popa, Marina\n  Danilevksy","title":"DELIFT: Data Efficient Language model Instruction Fine Tuning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Fine-tuning large language models (LLMs) is essential for enhancing their\nperformance on specific tasks but is often resource-intensive due to redundant\nor uninformative data. To address this inefficiency, we introduce DELIFT (Data\nEfficient Language model Instruction Fine-Tuning), a novel algorithm that\nsystematically optimizes data selection across the three key stages of\nfine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g.,\nreasoning, question-answering), and (3) continual fine-tuning (e.g.,\nincorporating new data versions). Unlike existing methods that focus on\nsingle-stage optimization or rely on computationally intensive gradient\ncalculations, DELIFT operates efficiently across all stages. Central to our\napproach is a pairwise utility metric that quantifies how beneficial a data\nsample is for improving the model's responses to other samples, effectively\nmeasuring the informational value relative to the model's current capabilities.\nBy leveraging different submodular functions applied to this metric, DELIFT\nselects diverse and optimal subsets that are useful across all stages of\nfine-tuning. Experiments across various tasks and model scales demonstrate that\nDELIFT can reduce the fine-tuning data size by up to 70% without compromising\nperformance, offering significant computational savings and outperforming\nexisting methods in both efficiency and efficacy.\n","versions":"[{'version': 'v1', 'created': 'Thu, 7 Nov 2024 04:38:29 GMT'}, {'version': 'v2', 'created': 'Sun, 10 Nov 2024 05:24:33 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 02:52:47 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Agarwal', 'Ishika', ''], ['Killamsetty', 'Krishnateja', ''], ['Popa', 'Lucian', ''], ['Danilevksy', 'Marina', '']]","extracted_entities":"[{'text': 'instruction tuning', 'label': 'Fine-tuning'}, {'text': 'task-specific fine-tuning', 'label': 'Fine-tuning'}, {'text': 'continual fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"continual fine-tuning","similarity_score":0.7702570558}
{"id":2411.14793,"submitter":"Jooyoung Choi","authors":"Jooyoung Choi, Chaehun Shin, Yeongtak Oh, Heeseung Kim, Jungbeom Lee,\n  Sungroh Yoon","title":"Style-Friendly SNR Sampler for Style-Driven Generation","comments":"Project page: https:\/\/stylefriendly.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Recent text-to-image diffusion models generate high-quality images but\nstruggle to learn new, personalized styles, which limits the creation of unique\nstyle templates. In style-driven generation, users typically supply reference\nimages exemplifying the desired style, together with text prompts that specify\ndesired stylistic attributes. Previous approaches popularly rely on\nfine-tuning, yet it often blindly utilizes objectives and noise level\ndistributions from pre-training without adaptation. We discover that stylistic\nfeatures predominantly emerge at higher noise levels, leading current\nfine-tuning methods to exhibit suboptimal style alignment. We propose the\nStyle-friendly SNR sampler, which aggressively shifts the signal-to-noise ratio\n(SNR) distribution toward higher noise levels during fine-tuning to focus on\nnoise levels where stylistic features emerge. This enhances models' ability to\ncapture novel styles indicated by reference images and text prompts. We\ndemonstrate improved generation of novel styles that cannot be adequately\ndescribed solely with a text prompt, enabling the creation of new style\ntemplates for personalized content creation.\n","versions":"[{'version': 'v1', 'created': 'Fri, 22 Nov 2024 08:29:25 GMT'}, {'version': 'v2', 'created': 'Thu, 5 Dec 2024 04:19:59 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 05:25:16 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Choi', 'Jooyoung', ''], ['Shin', 'Chaehun', ''], ['Oh', 'Yeongtak', ''], ['Kim', 'Heeseung', ''], ['Lee', 'Jungbeom', ''], ['Yoon', 'Sungroh', '']]","extracted_entities":"[{'text': 'text prompts', 'label': 'Prompting'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'text prompts', 'label': 'Prompting'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2411.17385,"submitter":"Duolikun Danier","authors":"Duolikun Danier, Mehmet Ayg\\\"un, Changjian Li, Hakan Bilen, Oisin Mac\n  Aodha","title":"DepthCues: Evaluating Monocular Depth Perception in Large Vision Models","comments":"Accepted to CVPR 2025. Project page:\n  https:\/\/danier97.github.io\/depthcues\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large-scale pre-trained vision models are becoming increasingly prevalent,\noffering expressive and generalizable visual representations that benefit\nvarious downstream tasks. Recent studies on the emergent properties of these\nmodels have revealed their high-level geometric understanding, in particular in\nthe context of depth perception. However, it remains unclear how depth\nperception arises in these models without explicit depth supervision provided\nduring pre-training. To investigate this, we examine whether the monocular\ndepth cues, similar to those used by the human visual system, emerge in these\nmodels. We introduce a new benchmark, DepthCues, designed to evaluate depth cue\nunderstanding, and present findings across 20 diverse and representative\npre-trained vision models. Our analysis shows that human-like depth cues emerge\nin more recent larger models. We also explore enhancing depth perception in\nlarge vision models by fine-tuning on DepthCues, and find that even without\ndense depth supervision, this improves depth estimation. To support further\nresearch, our benchmark and evaluation code will be made publicly available for\nstudying depth perception in vision models.\n","versions":"[{'version': 'v1', 'created': 'Tue, 26 Nov 2024 12:44:17 GMT'}, {'version': 'v2', 'created': 'Fri, 7 Mar 2025 17:21:06 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 11:00:12 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Danier', 'Duolikun', ''], ['Ayg\u00fcn', 'Mehmet', ''], ['Li', 'Changjian', ''], ['Bilen', 'Hakan', ''], ['Mac Aodha', 'Oisin', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2412.02114,"submitter":"Haodong Chen","authors":"Harold Haodong Chen, Harry Yang, Ser-Nam Lim","title":"Beyond Generation: Unlocking Universal Editing via Self-Supervised\n  Fine-Tuning","comments":"Project: https:\/\/haroldchen19.github.io\/UES-Page\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advances in video generation have outpaced progress in video editing,\nwhich remains constrained by several limiting factors, namely: (a) the task's\ndependency on supervision severely limits generality, (b) an unnecessary\nartificial separation between the generation and editing task, and (c) the high\ncomputational costs of training a video model. In this work, we propose UES\n(Unlocking Universal Editing via Self-Supervision), a lightweight\nself-supervised fine-tuning strategy that transforms generation models into\nunified generation-editing systems through self-supervised semantic alignment.\nOur approach establishes a dual-conditioning mechanism where original\nvideo-text pairs jointly provide visual and textual semantics, enabling\nstructured learning of intrinsic spatiotemporal correspondences. Key advantages\ninclude: (i) Universality through supervision-free adaptation to diverse\nediting tasks, (ii) Unification of generation and editing applicable to most\ntext(+image)-to-video model, and (iii) Efficiency via lightweight fine-tune\nthat reduces tunable parameters by 92.67%. To enable systematic evaluation, we\nintroduce OmniBench-99, a comprehensive benchmark spanning 99 videos across\nhumans\/animals, environments, and objects, comprising 4 editing types and 8\nscenarios. Extensive experiments show UES enables models without inherent\nediting capability to perform powerful and universal editing while preserving\nor even enhancing their original generation performance.\n","versions":"[{'version': 'v1', 'created': 'Tue, 3 Dec 2024 03:10:19 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 10:51:59 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Chen', 'Harold Haodong', ''], ['Yang', 'Harry', ''], ['Lim', 'Ser-Nam', '']]","extracted_entities":"[{'text': 'lightweight fine-tune', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"lightweight fine-tune","similarity_score":0.7465032935}
{"id":2412.04445,"submitter":"Yi Chen","authors":"Yi Chen, Yuying Ge, Weiliang Tang, Yizhuo Li, Yixiao Ge, Mingyu Ding,\n  Ying Shan, Xihui Liu","title":"Moto: Latent Motion Token as the Bridging Language for Learning Robot\n  Manipulation from Videos","comments":"Project released at: https:\/\/chenyi99.github.io\/moto\/ Update: Added\n  content related to real-world robot experiments and learning from human\n  videos","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI cs.CL cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent developments in Large Language Models pre-trained on extensive corpora\nhave shown significant success in various natural language processing tasks\nwith minimal fine-tuning. This success offers new promise for robotics, which\nhas long been constrained by the high cost of action-labeled data. We ask:\ngiven the abundant video data containing interaction-related knowledge\navailable as a rich \"corpus\", can a similar generative pre-training approach be\neffectively applied to enhance robot learning? The key challenge is to identify\nan effective representation for autoregressive pre-training that benefits robot\nmanipulation tasks. Inspired by the way humans learn new skills through\nobserving dynamic environments, we propose that effective robotic learning\nshould emphasize motion-related knowledge, which is closely tied to low-level\nactions and is hardware-agnostic, facilitating the transfer of learned motions\nto actual robot actions. To this end, we introduce Moto, which converts video\ncontent into latent Motion Token sequences by a Latent Motion Tokenizer,\nlearning a bridging \"language\" of motion from videos in an unsupervised manner.\nWe pre-train Moto-GPT through motion token autoregression, enabling it to\ncapture diverse visual motion knowledge. After pre-training, Moto-GPT\ndemonstrates the promising ability to produce semantically interpretable motion\ntokens, predict plausible motion trajectories, and assess trajectory\nrationality through output likelihood. To transfer learned motion priors to\nreal robot actions, we implement a co-fine-tuning strategy that seamlessly\nbridges latent motion token prediction and real robot control. Extensive\nexperiments show that the fine-tuned Moto-GPT exhibits superior robustness and\nefficiency on robot manipulation benchmarks, underscoring its effectiveness in\ntransferring knowledge from video data to downstream visual manipulation tasks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 5 Dec 2024 18:57:04 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 02:50:55 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chen', 'Yi', ''], ['Ge', 'Yuying', ''], ['Tang', 'Weiliang', ''], ['Li', 'Yizhuo', ''], ['Ge', 'Yixiao', ''], ['Ding', 'Mingyu', ''], ['Shan', 'Ying', ''], ['Liu', 'Xihui', '']]","extracted_entities":"[{'text': 'minimal fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Moto', 'label': 'Generative Pre-trained Transformer (GPT)'}]","assigned_concept":"Fine-tuning","matched_keyword":"minimal fine-tuning","similarity_score":0.8483207226}
{"id":2412.08908,"submitter":"Boxun Liu","authors":"Boxun Liu, Shijian Gao, Xuanyu Liu, Xiang Cheng, Liuqing Yang","title":"WiFo: Wireless Foundation Model for Channel Prediction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Channel prediction permits to acquire channel state information (CSI) without\nsignaling overhead. However, almost all existing channel prediction methods\nnecessitate the deployment of a dedicated model to accommodate a specific\nconfiguration. Leveraging the powerful modeling and multi-task learning\ncapabilities of foundation models, we propose the first space-time-frequency\n(STF) wireless foundation model (WiFo) to address time-frequency channel\nprediction tasks in a one-for-all manner. Specifically, WiFo is initially\npre-trained over massive and extensive diverse CSI datasets. Then, the model\nwill be instantly used for channel prediction under various CSI configurations\nwithout any fine-tuning. We propose a masked autoencoder (MAE)-based network\nstructure for WiFo to handle heterogeneous STF CSI data, and design several\nmask reconstruction tasks for self-supervised pre-training to capture the\ninherent 3D variations of CSI. To fully unleash its predictive power, we build\na large-scale heterogeneous simulated CSI dataset consisting of 160K CSI\nsamples for pre-training. Simulations validate its superior unified learning\nperformance across multiple datasets and demonstrate its state-of-the-art\n(SOTA) zero-shot generalization performance via comparisons with other\nfull-shot baselines.\n","versions":"[{'version': 'v1', 'created': 'Thu, 12 Dec 2024 03:44:17 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 06:07:23 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Liu', 'Boxun', ''], ['Gao', 'Shijian', ''], ['Liu', 'Xuanyu', ''], ['Cheng', 'Xiang', ''], ['Yang', 'Liuqing', '']]","extracted_entities":"[{'text': 'WiFo', 'label': 'Foundation Model'}, {'text': 'WiFo', 'label': 'Foundation Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'WiFo', 'label': 'Foundation Model'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2412.1678,"submitter":"Changchang Sun","authors":"Changchang Sun and Ren Wang and Yihua Zhang and Jinghan Jia and\n  Jiancheng Liu and Gaowen Liu and Yan Yan and Sijia Liu","title":"Forget Vectors at Play: Universal Input Perturbations Driving Machine\n  Unlearning in Image Classification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Machine unlearning (MU), which seeks to erase the influence of specific\nunwanted data from already-trained models, is becoming increasingly vital in\nmodel editing, particularly to comply with evolving data regulations like the\n``right to be forgotten''. Conventional approaches are predominantly\nmodel-based, typically requiring retraining or fine-tuning the model's weights\nto meet unlearning requirements. In this work, we approach the MU problem from\na novel input perturbation-based perspective, where the model weights remain\nintact throughout the unlearning process. We demonstrate the existence of a\nproactive input-based unlearning strategy, referred to forget vector, which can\nbe generated as an input-agnostic data perturbation and remains as effective as\nmodel-based approximate unlearning approaches. We also explore forget vector\narithmetic, whereby multiple class-specific forget vectors are combined through\nsimple operations (e.g., linear combinations) to generate new forget vectors\nfor unseen unlearning tasks, such as forgetting arbitrary subsets across\nclasses. Extensive experiments validate the effectiveness and adaptability of\nthe forget vector, showcasing its competitive performance relative to\nstate-of-the-art model-based methods. Codes are available at\nhttps:\/\/github.com\/Changchangsun\/Forget-Vector.\n","versions":"[{'version': 'v1', 'created': 'Sat, 21 Dec 2024 21:27:22 GMT'}, {'version': 'v2', 'created': 'Wed, 8 Jan 2025 17:00:18 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 01:25:27 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 01:46:48 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Sun', 'Changchang', ''], ['Wang', 'Ren', ''], ['Zhang', 'Yihua', ''], ['Jia', 'Jinghan', ''], ['Liu', 'Jiancheng', ''], ['Liu', 'Gaowen', ''], ['Yan', 'Yan', ''], ['Liu', 'Sijia', '']]","extracted_entities":"[{'text': 'Machine unlearning', 'label': 'Zero-shot Learning'}, {'text': 'evolving data regulations', 'label': 'AI Ethics'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2412.1886,"submitter":"Liang Wang","authors":"Liang Wang, Nan Yang, Xingxing Zhang, Xiaolong Huang, Furu Wei","title":"Bootstrap Your Own Context Length","comments":"19 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.IR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce a bootstrapping approach to train long-context language models\nby exploiting their short-context capabilities only. Our method utilizes a\nsimple agent workflow to synthesize diverse long-context instruction tuning\ndata, thereby eliminating the necessity for manual data collection and\nannotation. The proposed data synthesis workflow requires only a short-context\nlanguage model, a text retriever, and a document collection, all of which are\nreadily accessible within the open-source ecosystem. Subsequently, language\nmodels are fine-tuned using the synthesized data to extend their context\nlengths. In this manner, we effectively transfer the short-context capabilities\nof language models to long-context scenarios through a bootstrapping process.\nWe conduct experiments with the open-source Llama-3 family of models and\ndemonstrate that our method can successfully extend the context length to up to\n1M tokens, achieving superior performance across various benchmarks.\n","versions":"[{'version': 'v1', 'created': 'Wed, 25 Dec 2024 10:08:54 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 02:46:34 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Wang', 'Liang', ''], ['Yang', 'Nan', ''], ['Zhang', 'Xingxing', ''], ['Huang', 'Xiaolong', ''], ['Wei', 'Furu', '']]","extracted_entities":"[{'text': 'fine-tuned', 'label': 'Fine-tuning'}, {'text': 'Llama-3', 'label': 'Llama'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuned","similarity_score":0.8707774282}
{"id":2412.20506,"submitter":"Haorui Ji","authors":"Haorui Ji, Taojun Lin, Hongdong Li","title":"DPBridge: Latent Diffusion Bridge for Dense Prediction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Diffusion models have shown remarkable capabilities in modeling complex data\ndistributions by transforming noise into structured data through stochastic\nprocesses. However, when applied to dense prediction tasks whose goal is to\ncapture per-pixel relationships between RGB images and dense signal maps,\nstarting the sampling process from an uninformative Gaussian noise often leads\nto inefficient sampling and long latency. To overcome these challenges, we\npropose DPBridge, a generative framework that establishes direct mapping\nbetween input RGB images and dense signal maps based on a tractable bridge\nprocess. Furthermore, we introduce finetuning strategies to leverage a\npretrained large-scale image diffusion backbone, enjoying its rich visual prior\nknowledge to enable both efficient training and robust generalization.\nExperiments show that DPBridge achieves competitive performance compared to\nboth feed-forward and diffusion-based approaches across various benchmarks,\nvalidating its effectiveness and adaptability.\n","versions":"[{'version': 'v1', 'created': 'Sun, 29 Dec 2024 15:50:34 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 06:20:07 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Ji', 'Haorui', ''], ['Lin', 'Taojun', ''], ['Li', 'Hongdong', '']]","extracted_entities":"[{'text': 'DPBridge', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'finetuning strategies', 'label': 'Fine-tuning'}, {'text': 'DPBridge', 'label': 'Generative Pre-trained Transformer (GPT)'}]","assigned_concept":"Fine-tuning","matched_keyword":"finetuning strategies","similarity_score":0.5682560205}
{"id":2501.00513,"submitter":"Yifan Xu","authors":"Yifan Xu, Xinhao Li, Yichun Yang, Desen Meng, Rui Huang, Limin Wang","title":"CaReBench: A Fine-Grained Benchmark for Video Captioning and Retrieval","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.IR cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Video understanding, including video captioning and retrieval, is still a\ngreat challenge for video-language models (VLMs). The existing video retrieval\nand caption benchmarks only include short descriptions, limits their ability of\ndetailed video understanding evaluation. To address this problem, we present\nCaReBench, a testing benchmark for fine-grained video captioning and retrieval\nwith 1,000 high-quality pairs of videos and human-annotated detailed captions.\nUniquely, it provides manually separated spatial annotations and temporal\nannotations for each video. Based on this design, we introduce two evaluation\nmetrics, ReBias and CapST, specifically tailored for video retrieval and video\ncaptioning tasks, respectively. These metrics enable a comprehensive\ninvestigation into the spatial and temporal biases inherent in VLMs. In\naddition, to handle both video retrieval and video captioning tasks in a\nunified framework, we develop a simple baseline based on a Multimodal Language\nModel (MLLM). By implementing a two-stage Supervised Fine-Tuning (SFT), we\nfully unlock the potential of MLLM, enabling it not only to generate detailed\nvideo descriptions but also to extract video features. Surprisingly,\nexperimental results demonstrate that, compared to the CLIP-based models\ndesigned for retrieval and the popular MLLMs skilled in video captioning, our\nbaseline shows competitive performance in both fine-grained video retrieval and\nvideo detailed captioning.\n","versions":"[{'version': 'v1', 'created': 'Tue, 31 Dec 2024 15:53:50 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 16:01:24 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Xu', 'Yifan', ''], ['Li', 'Xinhao', ''], ['Yang', 'Yichun', ''], ['Meng', 'Desen', ''], ['Huang', 'Rui', ''], ['Wang', 'Limin', '']]","extracted_entities":"[{'text': 'two-stage Supervised Fine-Tuning (SFT)', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"two-stage Supervised Fine-Tuning (SFT)","similarity_score":0.6180204153}
{"id":2501.08669,"submitter":"Girolamo Macaluso","authors":"Carlo Romeo, Girolamo Macaluso, Alessandro Sestini, Andrew D. Bagdanov","title":"SPEQ: Offline Stabilization Phases for Efficient Q-Learning in High\n  Update-To-Data Ratio Reinforcement Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  High update-to-data (UTD) ratio algorithms in reinforcement learning (RL)\nimprove sample efficiency but incur high computational costs, limiting\nreal-world scalability. We propose Offline Stabilization Phases for Efficient\nQ-Learning (SPEQ), an RL algorithm that combines low-UTD online training with\nperiodic offline stabilization phases. During these phases, Q-functions are\nfine-tuned with high UTD ratios on a fixed replay buffer, reducing redundant\nupdates on suboptimal data. This structured training schedule optimally\nbalances computational and sample efficiency, addressing the limitations of\nboth high and low UTD ratio approaches. We empirically demonstrate that SPEQ\nrequires from 40% to 99% fewer gradient updates and 27% to 78% less training\ntime compared to state-of-the-art high UTD ratio methods while maintaining or\nsurpassing their performance on the MuJoCo continuous control benchmark. Our\nfindings highlight the potential of periodic stabilization phases as an\neffective alternative to conventional training schedules, paving the way for\nmore scalable reinforcement learning solutions in real-world applications where\ncomputational resources are constrained.\n","versions":"[{'version': 'v1', 'created': 'Wed, 15 Jan 2025 09:04:19 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 12:54:07 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Romeo', 'Carlo', ''], ['Macaluso', 'Girolamo', ''], ['Sestini', 'Alessandro', ''], ['Bagdanov', 'Andrew D.', '']]","extracted_entities":"[{'text': 'fine-tuned', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuned","similarity_score":0.8707774282}
{"id":2502.02257,"submitter":"Tao Zhang","authors":"Tao Zhang, Jinyong Wen, Zhen Chen, Kun Ding, Shiming Xiang, Chunhong\n  Pan","title":"UNIP: Rethinking Pre-trained Attention Patterns for Infrared Semantic\n  Segmentation","comments":"ICLR 2025. 27 pages, 13 figures, 21 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Pre-training techniques significantly enhance the performance of semantic\nsegmentation tasks with limited training data. However, the efficacy under a\nlarge domain gap between pre-training (e.g. RGB) and fine-tuning (e.g.\ninfrared) remains underexplored. In this study, we first benchmark the infrared\nsemantic segmentation performance of various pre-training methods and reveal\nseveral phenomena distinct from the RGB domain. Next, our layerwise analysis of\npre-trained attention maps uncovers that: (1) There are three typical attention\npatterns (local, hybrid, and global); (2) Pre-training tasks notably influence\nthe pattern distribution across layers; (3) The hybrid pattern is crucial for\nsemantic segmentation as it attends to both nearby and foreground elements; (4)\nThe texture bias impedes model generalization in infrared tasks. Building on\nthese insights, we propose UNIP, a UNified Infrared Pre-training framework, to\nenhance the pre-trained model performance. This framework uses the\nhybrid-attention distillation NMI-HAD as the pre-training target, a large-scale\nmixed dataset InfMix for pre-training, and a last-layer feature pyramid network\nLL-FPN for fine-tuning. Experimental results show that UNIP outperforms various\npre-training methods by up to 13.5\\% in average mIoU on three infrared\nsegmentation tasks, evaluated using fine-tuning and linear probing metrics.\nUNIP-S achieves performance on par with MAE-L while requiring only 1\/10 of the\ncomputational cost. Furthermore, UNIP significantly surpasses state-of-the-art\n(SOTA) infrared or RGB segmentation methods and demonstrates broad potential\nfor application in other modalities, such as RGB and depth. Our code is\navailable at https:\/\/github.com\/casiatao\/UNIP.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Feb 2025 12:08:20 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 13:55:08 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zhang', 'Tao', ''], ['Wen', 'Jinyong', ''], ['Chen', 'Zhen', ''], ['Ding', 'Kun', ''], ['Xiang', 'Shiming', ''], ['Pan', 'Chunhong', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'hybrid', 'label': 'Attention mechanism'}, {'text': 'texture bias', 'label': 'Attention mechanism'}, {'text': 'hybrid-attention distillation', 'label': 'Knowledge distillation'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2502.17212,"submitter":"Xander Haijen","authors":"Xander Haijen and Bikram Koirala and Xuanwen Tao and Paul Scheunders","title":"A Two-step Linear Mixing Model for Unmixing under Hyperspectral\n  Variability","comments":"13 pages, 10 figures, 5 tables. This work has been submitted to the\n  IEEE for possible publication","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Spectral unmixing is an important task in the research field of hyperspectral\nimage processing. It can be thought of as a regression problem, where the\nobserved variable (i.e., an image pixel) is to be found as a function of the\nresponse variables (i.e., the pure materials in a scene, called endmembers).\nThe Linear Mixing Model (LMM) has received a great deal of attention, due to\nits simplicity and ease of use in, e.g., optimization problems. Its biggest\nflaw is that it assumes that any pure material can be characterized by one\nunique spectrum throughout the entire scene. In many cases this is incorrect:\nthe endmembers face a significant amount of spectral variability caused by,\ne.g., illumination conditions, atmospheric effects, or intrinsic variability.\nResearchers have suggested several generalizations of the LMM to mitigate this\neffect. However, most models lead to ill-posed and highly non-convex\noptimization problems, which are hard to solve and have hyperparameters that\nare difficult to tune. In this paper, we propose a two-step LMM that bridges\nthe gap between model complexity and computational tractability. We show that\nthis model leads to only a mildly non-convex optimization problem, which we\nsolve with an interior-point solver. This method requires virtually no\nhyperparameter tuning, and can therefore be used easily and quickly in a wide\nrange of unmixing tasks. We show that the model is competitive and in some\ncases superior to existing and well-established unmixing methods and\nalgorithms. We do this through several experiments on synthetic data, real-life\nsatellite data, and hybrid synthetic-real data.\n","versions":"[{'version': 'v1', 'created': 'Mon, 24 Feb 2025 14:44:40 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 08:05:10 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Haijen', 'Xander', ''], ['Koirala', 'Bikram', ''], ['Tao', 'Xuanwen', ''], ['Scheunders', 'Paul', '']]","extracted_entities":"[{'text': 'hyperparameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"hyperparameter tuning","similarity_score":0.6193697453}
{"id":2502.19634,"submitter":"Jiazhen Pan","authors":"Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran\n  Li, Chen Chen, Cheng Ouyang, Daniel Rueckert","title":"MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language\n  Models (VLMs) via Reinforcement Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Reasoning is a critical frontier for advancing medical image analysis, where\ntransparency and trustworthiness play a central role in both clinician trust\nand regulatory approval. Although Medical Visual Language Models (VLMs) show\npromise for radiological tasks, most existing VLMs merely produce final answers\nwithout revealing the underlying reasoning. To address this gap, we introduce\nMedVLM-R1, a medical VLM that explicitly generates natural language reasoning\nto enhance transparency and trustworthiness. Instead of relying on supervised\nfine-tuning (SFT), which often suffers from overfitting to training\ndistributions and fails to foster genuine reasoning, MedVLM-R1 employs a\nreinforcement learning framework that incentivizes the model to discover\nhuman-interpretable reasoning paths without using any reasoning references.\nDespite limited training data (600 visual question answering samples) and model\nparameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI,\nCT, and X-ray benchmarks, outperforming larger models trained on over a million\nsamples. It also demonstrates robust domain generalization under\nout-of-distribution tasks. By unifying medical image analysis with explicit\nreasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable\nAI in clinical practice. Inference model is available at:\nhttps:\/\/huggingface.co\/JZPeterPan\/MedVLM-R1.\n","versions":"[{'version': 'v1', 'created': 'Wed, 26 Feb 2025 23:57:34 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 13:55:33 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Pan', 'Jiazhen', ''], ['Liu', 'Che', ''], ['Wu', 'Junde', ''], ['Liu', 'Fenglin', ''], ['Zhu', 'Jiayuan', ''], ['Li', 'Hongwei Bran', ''], ['Chen', 'Chen', ''], ['Ouyang', 'Cheng', ''], ['Rueckert', 'Daniel', '']]","extracted_entities":"[{'text': 'regulatory approval', 'label': 'AI Ethics'}, {'text': 'Medical Visual Language Models', 'label': 'Large Language Model'}, {'text': 'supervised\\nfine-tuning', 'label': 'Fine-tuning'}, {'text': 'reasoning', 'label': 'Chain of thought'}, {'text': 'reasoning', 'label': 'Chain of thought'}]","assigned_concept":"Fine-tuning","matched_keyword":"supervised\nfine-tuning","similarity_score":0.7449287176}
{"id":2503.01196,"submitter":"Alain Moise Dikande Pr.","authors":"Alain M. Dikand\\'e","title":"On a hyperbolic Duffing oscillator with linear damping and periodic\n  forcing","comments":"18 pages, 34 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"nlin.CD math-ph math.MP physics.comp-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The Duffing oscillator describes the dynamics of a mass suspended on a spring\nwith position-dependent stiffness. The mass is assumed to experience a linear\ndamping and a time-dependent external forcing. The model has been instrumental\nin theoretical investigations of dynamical properties of systems with\nparity-conserving symmetry, where a double-well substrate connects two\nmetastable states separated by a barrier. Physical systems of interest include\nnonlinear feedback-controlled mass-spring-damper oscillators, active hysteresis\ncircuits (e.g. memristors), protein chains prone to hydrogen bond-mediated\nconformational transitions, centro-symmetric crystals and so on. In this work\nwe consider a Duffing-type oscillator with a double-well potential represented\nby a hyperbolic function of mass position. The hyperbolic double-well potential\nhas two degenerate minima that can be smoothly tuned by varying a deformability\nparameter, leaving unchanged the barrier height. We investigate solutions of\nthe equation of motion in the absence and presence of damping and forcing. In\nthe absence of perturbations numerical solutions lead to a periodic train of\nanharmonic oscillations featuring a crystal of pulse solitons of sech types.\nHowever, when the hyperbolic double-well potential is inverted, analytical\nsolutions can be obtained which turn out to be kink-soliton crystals described\nby Jacobi elliptic functions. When damping and forcing are taken into\nconsideration, the system dynamics can transit from periodic to chaotic phases\nor vice-versa via period-doubling or period-halving bifurcations, by simply\nvarying the deformability parameter. The Poincar\\'e map of the proposed model\ncarries the well-known characteristic signatures of chaos presursors of the\nstandard Duffing model, which happens to be just a particular case of the\nbistable oscillator model with the hyperbolic double-well potential.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 05:47:46 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 16:59:24 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Dikand\u00e9', 'Alain M.', '']]","extracted_entities":"[{'text': 'smoothly tuned', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"smoothly tuned","similarity_score":0.7274996638}
{"id":2503.03644,"submitter":"Shuo Li","authors":"Xiaojun Bi, Shuo Li, Ziyue Wang, Fuwen Luo, Weizheng Qiao, Lu Han,\n  Ziwei Sun, Peng Li, Yang Liu","title":"DongbaMIE: A Multimodal Information Extraction Dataset for Evaluating\n  Semantic Understanding of Dongba Pictograms","comments":"Our dataset can be obtained from:\n  https:\/\/github.com\/thinklis\/DongbaMIE","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Dongba pictographs are the only pictographs still in use in the world. They\nhave pictorial ideographic features, and their symbols carry rich cultural and\ncontextual information. Due to the lack of relevant datasets, existing research\nhas difficulty in advancing the study of semantic understanding of Dongba\npictographs. To this end, we propose \\textbf{DongbaMIE}, the first multimodal\ndataset for semantic understanding and extraction of Dongba pictographs,\nconsisting of Dongba pictograph images and corresponding Chinese semantic\nannotations. DongbaMIE contains 23,530 sentence-level and 2,539 paragraph-level\nimages, covering four semantic dimensions: objects, actions, relations, and\nattributes. We systematically evaluate multimodal large language models\n(MLLMs), such as GPT-4o, Gemini-2.0, and Qwen2-VL. Experimental results show\nthat best F1 scores of proprietary models, GPT-4o and Gemini, for object\nextraction task are only 3.16 and 3.11 respectively. For the open-source model\nQwen2-VL, it achieves only 11.49 after supervised fine-tuning. These suggest\nthat current MLLMs still face significant challenges in accurately recognizing\ndiverse semantic information in Dongba pictographs.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 16:20:53 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Mar 2025 11:36:33 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 12:16:23 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Bi', 'Xiaojun', ''], ['Li', 'Shuo', ''], ['Wang', 'Ziyue', ''], ['Luo', 'Fuwen', ''], ['Qiao', 'Weizheng', ''], ['Han', 'Lu', ''], ['Sun', 'Ziwei', ''], ['Li', 'Peng', ''], ['Liu', 'Yang', '']]","extracted_entities":"[{'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"supervised fine-tuning","similarity_score":0.7449287176}
{"id":2503.04833,"submitter":"Liming Lu","authors":"Liming Lu, Shuchao Pang, Siyuan Liang, Haotian Zhu, Xiyu Zeng, Aishan\n  Liu, Yunhuai Liu, Yongbin Zhou","title":"Adversarial Training for Multimodal Large Language Models against\n  Jailbreak Attacks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multimodal large language models (MLLMs) have made remarkable strides in\ncross-modal comprehension and generation tasks. However, they remain vulnerable\nto jailbreak attacks, where crafted perturbations bypass security guardrails\nand elicit harmful outputs. In this paper, we present the first adversarial\ntraining (AT) paradigm tailored to defend against jailbreak attacks during the\nMLLM training phase. Extending traditional AT to this domain poses two critical\nchallenges: efficiently tuning massive parameters and ensuring robustness\nagainst attacks across multiple modalities. To address these challenges, we\nintroduce Projection Layer Against Adversarial Training (ProEAT), an end-to-end\nAT framework. ProEAT incorporates a projector-based adversarial training\narchitecture that efficiently handles large-scale parameters while maintaining\ncomputational feasibility by focusing adversarial training on a lightweight\nprojector layer instead of the entire model; additionally, we design a dynamic\nweight adjustment mechanism that optimizes the loss function's weight\nallocation based on task demands, streamlining the tuning process. To enhance\ndefense performance, we propose a joint optimization strategy across visual and\ntextual modalities, ensuring robust resistance to jailbreak attacks originating\nfrom either modality. Extensive experiments conducted on five major jailbreak\nattack methods across three mainstream MLLMs demonstrate the effectiveness of\nour approach. ProEAT achieves state-of-the-art defense performance,\noutperforming existing baselines by an average margin of +34% across text and\nimage modalities, while incurring only a 1% reduction in clean accuracy.\nFurthermore, evaluations on real-world embodied intelligent systems highlight\nthe practical applicability of our framework, paving the way for the\ndevelopment of more secure and reliable multimodal systems.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 14:13:35 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 07:01:13 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Lu', 'Liming', ''], ['Pang', 'Shuchao', ''], ['Liang', 'Siyuan', ''], ['Zhu', 'Haotian', ''], ['Zeng', 'Xiyu', ''], ['Liu', 'Aishan', ''], ['Liu', 'Yunhuai', ''], ['Zhou', 'Yongbin', '']]","extracted_entities":"[{'text': 'Multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'efficiently tuning', 'label': 'Fine-tuning'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Fine-tuning","matched_keyword":"efficiently tuning","similarity_score":0.7900304794}
{"id":2503.05403,"submitter":"Verena H\\\"aberle","authors":"Verena H\\\"aberle, Xiuqiang He, Linbin Huang, Florian D\\\"orfler, Steven\n  Low","title":"Quantitative Decentralized Stability Certificates for Grid-Forming\n  Converter Control","comments":"12 pages, 13 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We propose a decentralized framework for guaranteeing the small-signal\nstability of future power systems with grid-forming converters. Our approach\nleverages dynamic loop-shifting techniques to compensate for the lack of\npassivity in the network dynamics and establishes decentralized parametric\nstability certificates, depending on the local device-level controls and\nincorporating the effects of the network dynamics. By following practical\ntuning rules, we are able to ensure plug-and-play operation without centralized\ncoordination. Unlike prior works, our approach accommodates coupled frequency\nand voltage dynamics, incorporates network dynamics, and does not rely on\nspecific network configurations or operating points, offering a general and\nscalable solution for the integration of power-electronics-based devices into\nfuture power systems. We validate our theoretical stability results through\nnumerical case studies in a high-fidelity simulation model.\n","versions":"[{'version': 'v1', 'created': 'Fri, 7 Mar 2025 13:26:55 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Mar 2025 13:51:36 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 14:54:56 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['H\u00e4berle', 'Verena', ''], ['He', 'Xiuqiang', ''], ['Huang', 'Linbin', ''], ['D\u00f6rfler', 'Florian', ''], ['Low', 'Steven', '']]","extracted_entities":"[{'text': 'practical\\ntuning rules', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"practical\ntuning rules","similarity_score":0.691578269}
{"id":2503.06166,"submitter":"Li Li","authors":"Shawn Li, Peilin Cai, Yuxiao Zhou, Zhiyu Ni, Renjie Liang, You Qin, Yi\n  Nian, Zhengzhong Tu, Xiyang Hu, Yue Zhao","title":"Secure On-Device Video OOD Detection Without Backpropagation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Out-of-Distribution (OOD) detection is critical for ensuring the reliability\nof machine learning models in safety-critical applications such as autonomous\ndriving and medical diagnosis. While deploying personalized OOD detection\ndirectly on edge devices is desirable, it remains challenging due to large\nmodel sizes and the computational infeasibility of on-device training.\nFederated learning partially addresses this but still requires gradient\ncomputation and backpropagation, exceeding the capabilities of many edge\ndevices. To overcome these challenges, we propose SecDOOD, a secure\ncloud-device collaboration framework for efficient on-device OOD detection\nwithout requiring device-side backpropagation. SecDOOD utilizes cloud resources\nfor model training while ensuring user data privacy by retaining sensitive\ninformation on-device. Central to SecDOOD is a HyperNetwork-based personalized\nparameter generation module, which adapts cloud-trained models to\ndevice-specific distributions by dynamically generating local weight\nadjustments, effectively combining central and local information without local\nfine-tuning. Additionally, our dynamic feature sampling and encryption strategy\nselectively encrypts only the most informative feature channels, largely\nreducing encryption overhead without compromising detection performance.\nExtensive experiments across multiple datasets and OOD scenarios demonstrate\nthat SecDOOD achieves performance comparable to fully fine-tuned models,\nenabling secure, efficient, and personalized OOD detection on resource-limited\nedge devices. To enhance accessibility and reproducibility, our code is\npublicly available at https:\/\/github.com\/Dystopians\/SecDOOD.\n","versions":"[{'version': 'v1', 'created': 'Sat, 8 Mar 2025 11:03:21 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 07:44:00 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Li', 'Shawn', ''], ['Cai', 'Peilin', ''], ['Zhou', 'Yuxiao', ''], ['Ni', 'Zhiyu', ''], ['Liang', 'Renjie', ''], ['Qin', 'You', ''], ['Nian', 'Yi', ''], ['Tu', 'Zhengzhong', ''], ['Hu', 'Xiyang', ''], ['Zhao', 'Yue', '']]","extracted_entities":"[{'text': 'Federated learning', 'label': 'Few-shot Learning'}, {'text': 'local\\nfine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"local\nfine-tuning","similarity_score":0.8377403021}
{"id":2503.0902,"submitter":"Yuan Jiang","authors":"Yuan Jiang, Yujian Zhang, Liang Lu, Christoph Treude, Xiaohong Su,\n  Shan Huang and Tiantian Wang","title":"Enhancing High-Quality Code Generation in Large Language Models with\n  Comparative Prefix-Tuning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) have been widely adopted in commercial code\ncompletion engines, significantly enhancing coding efficiency and productivity.\nHowever, LLMs may generate code with quality issues that violate coding\nstandards and best practices, such as poor code style and maintainability, even\nwhen the code is functionally correct. This necessitates additional effort from\ndevelopers to improve the code, potentially negating the efficiency gains\nprovided by LLMs. To address this problem, we propose a novel comparative\nprefix-tuning method for controllable high-quality code generation. Our method\nintroduces a single, property-specific prefix that is prepended to the\nactivations of the LLM, serving as a lightweight alternative to fine-tuning.\nUnlike existing methods that require training multiple prefixes, our approach\ntrains only one prefix and leverages pairs of high-quality and low-quality code\nsamples, introducing a sequence-level ranking loss to guide the model's\ntraining. This comparative approach enables the model to better understand the\ndifferences between high-quality and low-quality code, focusing on aspects that\nimpact code quality. Additionally, we design a data construction pipeline to\ncollect and annotate pairs of high-quality and low-quality code, facilitating\neffective training. Extensive experiments on the Code Llama 7B model\ndemonstrate that our method improves code quality by over 100% in certain task\ncategories, while maintaining functional correctness. We also conduct ablation\nstudies and generalization experiments, confirming the effectiveness of our\nmethod's components and its strong generalization capability.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 03:15:46 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 07:24:48 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Jiang', 'Yuan', ''], ['Zhang', 'Yujian', ''], ['Lu', 'Liang', ''], ['Treude', 'Christoph', ''], ['Su', 'Xiaohong', ''], ['Huang', 'Shan', ''], ['Wang', 'Tiantian', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.09151,"submitter":"Jong Chul Ye","authors":"Hyeonho Jeong, Suhyeon Lee, Jong Chul Ye","title":"Reangle-A-Video: 4D Video Generation as Video-to-Video Translation","comments":"Project page: https:\/\/hyeonho99.github.io\/reangle-a-video\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce Reangle-A-Video, a unified framework for generating synchronized\nmulti-view videos from a single input video. Unlike mainstream approaches that\ntrain multi-view video diffusion models on large-scale 4D datasets, our method\nreframes the multi-view video generation task as video-to-videos translation,\nleveraging publicly available image and video diffusion priors. In essence,\nReangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An\nimage-to-video diffusion transformer is synchronously fine-tuned in a\nself-supervised manner to distill view-invariant motion from a set of warped\nvideos. (2) Multi-View Consistent Image-to-Images Translation: The first frame\nof the input video is warped and inpainted into various camera perspectives\nunder an inference-time cross-view consistency guidance using DUSt3R,\ngenerating multi-view consistent starting images. Extensive experiments on\nstatic view transport and dynamic camera control show that Reangle-A-Video\nsurpasses existing methods, establishing a new solution for multi-view video\ngeneration. We will publicly release our code and data. Project page:\nhttps:\/\/hyeonho99.github.io\/reangle-a-video\/\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 08:26:15 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 13:01:59 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Jeong', 'Hyeonho', ''], ['Lee', 'Suhyeon', ''], ['Ye', 'Jong Chul', '']]","extracted_entities":"[{'text': 'Multi-View Motion Learning', 'label': 'Few-shot Learning'}, {'text': 'synchronously fine-tuned', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"synchronously fine-tuned","similarity_score":0.5768219233}
{"id":2503.09315,"submitter":"Yihong Huang","authors":"Yihong Huang, Chen Chu, Fan Zhang, Fei Chen, Yu Lin, Ruiduan Li,\n  Zhihao Li","title":"ShuffleGate: An Efficient and Self-Polarizing Feature Selection Method\n  for Large-Scale Deep Models in Industry","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Deep models in industrial applications rely on thousands of features for\naccurate predictions, such as deep recommendation systems. While new features\nare introduced to capture evolving user behavior, outdated or redundant\nfeatures often remain, significantly increasing storage and computational\ncosts. To address this issue, feature selection methods are widely adopted to\nidentify and remove less important features. However, existing approaches face\ntwo major challenges: (1) they often require complex hyperparameter (Hp)\ntuning, making them difficult to employ in practice, and (2) they fail to\nproduce well-separated feature importance scores, which complicates\nstraightforward feature removal. Moreover, the impact of removing unimportant\nfeatures can only be evaluated through retraining the model, a time-consuming\nand resource-intensive process that severely hinders efficient feature\nselection.\n  To solve these challenges, we propose a novel feature selection approach,\nShuffleGate. In particular, it shuffles all feature values across instances\nsimultaneously and uses a gating mechanism that allows the model to dynamically\nlearn the weights for combining the original and shuffled inputs. Notably, it\ncan generate well-separated feature importance scores and estimate the\nperformance without retraining the model, while introducing only a single Hp.\nExperiments on four public datasets show that our approach outperforms\nstate-of-the-art methods in feature selection for model retraining. Moreover,\nit has been successfully integrated into the daily iteration of Bilibili's\nsearch models across various scenarios, where it significantly reduces feature\nset size (up to 60%+) and computational resource usage (up to 20%+), while\nmaintaining comparable performance.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 12:05:03 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 12:35:52 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 05:06:43 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Huang', 'Yihong', ''], ['Chu', 'Chen', ''], ['Zhang', 'Fan', ''], ['Chen', 'Fei', ''], ['Lin', 'Yu', ''], ['Li', 'Ruiduan', ''], ['Li', 'Zhihao', '']]","extracted_entities":"[{'text': 'complex hyperparameter (Hp)\\ntuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"complex hyperparameter (Hp)\ntuning","similarity_score":0.5364035964}
{"id":2503.10615,"submitter":"Yang Yi","authors":"Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao\n  Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, Wei Chen","title":"R1-Onevision: Advancing Generalized Multimodal Reasoning through\n  Cross-Modal Formalization","comments":"Code and Model: https:\/\/github.com\/Fancy-MLLM\/R1-onevision","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models have demonstrated remarkable reasoning capability in\ncomplex textual tasks. However, multimodal reasoning, which requires\nintegrating visual and textual information, remains a significant challenge.\nExisting visual-language models often struggle to effectively analyze and\nreason visual content, resulting in suboptimal performance on complex reasoning\ntasks. Moreover, the absence of comprehensive benchmarks hinders the accurate\nassessment of multimodal reasoning capabilities. In this paper, we introduce\nR1-Onevision, a multimodal reasoning model designed to bridge the gap between\nvisual perception and deep reasoning. To achieve this, we propose a cross-modal\nreasoning pipeline that transforms images into formal textural representations,\nenabling precise language-based reasoning. Leveraging this pipeline, we\nconstruct the R1-Onevision dataset which provides detailed, step-by-step\nmultimodal reasoning annotations across diverse domains. We further develop the\nR1-Onevision model through supervised fine-tuning and reinforcement learning to\ncultivate advanced reasoning and robust generalization abilities. To\ncomprehensively evaluate multimodal reasoning performance across different\ngrades, we introduce R1-Onevision-Bench, a benchmark aligned with human\neducational stages, covering exams from junior high school to university and\nbeyond. Experimental results show that R1-Onevision achieves state-of-the-art\nperformance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple\nchallenging multimodal reasoning benchmarks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 17:56:05 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 08:52:34 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Yang', 'Yi', ''], ['He', 'Xiaoxuan', ''], ['Pan', 'Hongkun', ''], ['Jiang', 'Xiyan', ''], ['Deng', 'Yan', ''], ['Yang', 'Xingtao', ''], ['Lu', 'Haoyu', ''], ['Yin', 'Dacheng', ''], ['Rao', 'Fengyun', ''], ['Zhu', 'Minfeng', ''], ['Zhang', 'Bo', ''], ['Chen', 'Wei', '']]","extracted_entities":"[{'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'reinforcement learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"supervised fine-tuning","similarity_score":0.7449287176}
{"id":2503.11071,"submitter":"Chao Shuai","authors":"Zhenguang Liu, Chao Shuai, Shaojing Fan, Ziping Dong, Jinwu Hu,\n  Zhongjie Ba, Kui Ren","title":"Harnessing Frequency Spectrum Insights for Image Copyright Protection\n  Against Diffusion Models","comments":"Received by CVPR 2025 (10 pages, 11 figures)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Diffusion models have achieved remarkable success in novel view synthesis,\nbut their reliance on large, diverse, and often untraceable Web datasets has\nraised pressing concerns about image copyright protection. Current methods fall\nshort in reliably identifying unauthorized image use, as they struggle to\ngeneralize across varied generation tasks and fail when the training dataset\nincludes images from multiple sources with few identifiable (watermarked or\npoisoned) samples. In this paper, we present novel evidence that\ndiffusion-generated images faithfully preserve the statistical properties of\ntheir training data, particularly reflected in their spectral features.\nLeveraging this insight, we introduce \\emph{CoprGuard}, a robust frequency\ndomain watermarking framework to safeguard against unauthorized image usage in\ndiffusion model training and fine-tuning. CoprGuard demonstrates remarkable\neffectiveness against a wide range of models, from naive diffusion models to\nsophisticated text-to-image models, and is robust even when watermarked images\ncomprise a mere 1\\% of the training dataset. This robust and versatile approach\nempowers content owners to protect their intellectual property in the era of\nAI-driven image generation.\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 04:27:50 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 06:58:14 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Liu', 'Zhenguang', ''], ['Shuai', 'Chao', ''], ['Fan', 'Shaojing', ''], ['Dong', 'Ziping', ''], ['Hu', 'Jinwu', ''], ['Ba', 'Zhongjie', ''], ['Ren', 'Kui', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.11771,"submitter":"Yizhuo Xiao","authors":"Yizhuo Xiao, Mustafa Suphi Erden, Cheng Wang","title":"Controllable Latent Diffusion for Traffic Simulation","comments":"7 pages,2 figures, submitted to IROS conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.MA","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The validation of autonomous driving systems benefits greatly from the\nability to generate scenarios that are both realistic and precisely\ncontrollable. Conventional approaches, such as real-world test drives, are not\nonly expensive but also lack the flexibility to capture targeted edge cases for\nthorough evaluation. To address these challenges, we propose a controllable\nlatent diffusion that guides the training of diffusion models via reinforcement\nlearning to automatically generate a diverse and controllable set of driving\nscenarios for virtual testing. Our approach removes the reliance on large-scale\nreal-world data by generating complex scenarios whose properties can be finely\ntuned to challenge and assess autonomous vehicle systems. Experimental results\nshow that our approach has the lowest collision rate of $0.098$ and lowest\noff-road rate of $0.096$, demonstrating superiority over existing baselines.\nThe proposed approach significantly improves the realism, stability and\ncontrollability of the generated scenarios, enabling more nuanced safety\nevaluation of autonomous vehicles.\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 18:04:41 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 17:28:43 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Xiao', 'Yizhuo', ''], ['Erden', 'Mustafa Suphi', ''], ['Wang', 'Cheng', '']]","extracted_entities":"[{'text': 'reinforcement\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'finely\\ntuned', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"finely\ntuned","similarity_score":0.7587447762}
{"id":2503.12813,"submitter":"Mohammad Hossein Samaei","authors":"Mousa Alizadeh, Mohammad Hossein Samaei, Azam Seilsepour, Mohammad TH\n  Beheshti","title":"Epidemic Forecasting with a Hybrid Deep Learning Method Using CNN-LSTM\n  With WOA-GWO Parameter Optimization: Global COVID-19 Case Study","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Effective epidemic modeling is essential for managing public health crises,\nrequiring robust methods to predict disease spread and optimize resource\nallocation. This study introduces a novel deep learning framework that advances\ntime series forecasting for infectious diseases, with its application to COVID\n19 data as a critical case study. Our hybrid approach integrates Convolutional\nNeural Networks (CNNs) and Long Short Term Memory (LSTM) models to capture\nspatial and temporal dynamics of disease transmission across diverse regions.\nThe CNN extracts spatial features from raw epidemiological data, while the LSTM\nmodels temporal patterns, yielding precise and adaptable predictions. To\nmaximize performance, we employ a hybrid optimization strategy combining the\nWhale Optimization Algorithm (WOA) and Gray Wolf Optimization (GWO) to fine\ntune hyperparameters, such as learning rates, batch sizes, and training epochs\nenhancing model efficiency and accuracy. Applied to COVID 19 case data from 24\ncountries across six continents, our method outperforms established benchmarks,\nincluding ARIMA and standalone LSTM models, with statistically significant\ngains in predictive accuracy (e.g., reduced RMSE). This framework demonstrates\nits potential as a versatile method for forecasting epidemic trends, offering\ninsights for resource planning and decision making in both historical contexts,\nlike the COVID 19 pandemic, and future outbreaks.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 04:41:26 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 03:10:14 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Alizadeh', 'Mousa', ''], ['Samaei', 'Mohammad Hossein', ''], ['Seilsepour', 'Azam', ''], ['Beheshti', 'Mohammad TH', '']]","extracted_entities":"[{'text': 'fine\\ntune hyperparameters', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine\ntune hyperparameters","similarity_score":0.5845230222}
{"id":2503.12822,"submitter":"Mehdi Makni","authors":"Mehdi Makni, Kayhan Behdin, Gabriel Afriat, Zheng Xu, Sergei\n  Vassilvitskii, Natalia Ponomareva, Hussein Hazimeh, Rahul Mazumder","title":"An Optimization Framework for Differentially Private Sparse Fine-Tuning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ML","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Differentially private stochastic gradient descent (DP-SGD) is broadly\nconsidered to be the gold standard for training and fine-tuning neural networks\nunder differential privacy (DP). With the increasing availability of\nhigh-quality pre-trained model checkpoints (e.g., vision and language models),\nfine-tuning has become a popular strategy. However, despite recent progress in\nunderstanding and applying DP-SGD for private transfer learning tasks,\nsignificant challenges remain -- most notably, the performance gap between\nmodels fine-tuned with DP-SGD and their non-private counterparts. Sparse\nfine-tuning on private data has emerged as an alternative to full-model\nfine-tuning; recent work has shown that privately fine-tuning only a small\nsubset of model weights and keeping the rest of the weights fixed can lead to\nbetter performance. In this work, we propose a new approach for sparse\nfine-tuning of neural networks under DP. Existing work on private sparse\nfinetuning often used fixed choice of trainable weights (e.g., updating only\nthe last layer), or relied on public model's weights to choose the subset of\nweights to modify. Such choice of weights remains suboptimal. In contrast, we\nexplore an optimization-based approach, where our selection method makes use of\nthe private gradient information, while using off the shelf privacy accounting\ntechniques. Our numerical experiments on several computer vision models and\ndatasets show that our selection method leads to better prediction accuracy,\ncompared to full-model private fine-tuning or existing private sparse\nfine-tuning approaches.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 05:05:05 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Makni', 'Mehdi', ''], ['Behdin', 'Kayhan', ''], ['Afriat', 'Gabriel', ''], ['Xu', 'Zheng', ''], ['Vassilvitskii', 'Sergei', ''], ['Ponomareva', 'Natalia', ''], ['Hazimeh', 'Hussein', ''], ['Mazumder', 'Rahul', '']]","extracted_entities":"[{'text': 'Sparse\\nfine-tuning', 'label': 'Fine-tuning'}, {'text': 'full-model\\nfine-tuning', 'label': 'Fine-tuning'}, {'text': 'sparse\\nfine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"full-model\nfine-tuning","similarity_score":0.7202852964}
{"id":2503.12858,"submitter":"Duke Nguyen","authors":"Duke Nguyen, Aditya Joshi, Flora Salim","title":"Harnessing Test-time Adaptation for NLU tasks Involving Dialects of\n  English","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Test-time adaptation (TTA) is an excellent method which helps generalize\nmodels across domains, tasks, and distributions without the use of labeled\ndatasets. Thus, TTA is very useful in natural language processing (NLP) in the\ndialectal setting, since oftentimes, models are trained on Standard American\nEnglish (SAE), evaluated on Indian English or Nigerian English, of which\ndistribution differs significantly from the former. This is especially useful\nsince dialectal datasets are scarce. In this paper, we explore one of the most\nfamous TTA techniques, SHOT, in dialectal NLP. We finetune and evaluate SHOT on\ndifferent combinations of dialectal GLUE. Our findings show that SHOT is a\nviable technique when labeled datasets are unavailable. We also theoretically\npropose the concept of dialectal gap and show that it has a positive\ncorrelation with the effectiveness of SHOT. We also find that in many cases,\nfinetuning on SAE yields higher performance than finetuning on dialectal data.\nOur code is available at https:\/\/github.com\/dukenguyenxyz\/dialect-adaptation\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 06:40:06 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Nguyen', 'Duke', ''], ['Joshi', 'Aditya', ''], ['Salim', 'Flora', '']]","extracted_entities":"[{'text': 'finetune', 'label': 'Fine-tuning'}, {'text': 'finetuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"finetuning","similarity_score":0.5753726363}
{"id":2503.12881,"submitter":"Taiju Tanii","authors":"Nobuhiro Maekawa and Taiju Tanii","title":"Does finetuning make $D$-term contributions smaller in natural grand\n  unified theories with spontaneous supersymmetry breaking?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper, we explore the natural grand unified theory (GUT) with\nspontaneous supersymmetry (SUSY) breaking, focusing on the contribution to\nsfermion and Higgs masses. Natural GUTs solve various problems in SUSY GUTs\nwith the natural assumption that all terms allowed by $SO(10)\\times U(1)_A$\nsymmetry are introduced with $O(1)$ coefficients. %, solve various the problems\nin SUSY GUTs. It is also possible to introduce spontaneous SUSY breaking in the\nnatural GUT. This scenario predicts $D$-term contribution to sfermion and Higgs\nmasses dominates the $F$-term contribution, that potentially leads to the SUSY\nflavor problem. Fortunately, it also predicts high-scale SUSY, which avoids the\nSUSY flavor problem but leads to the instability of the %need for fine-tuning\nto obtain the electroweak scale. The $D$-term domination results in the\nsuperheavy Higgsino unless the $D$-term contribution becomes comparable with\nthe $F$-term contribution. We discuss whether it is possible to suppress the\n$D$-term contribution while maintaining the $F$-term contribution through the\ntuning of $O(1)$ coefficients in the natural GUT with spontaneous SUSY\nbreaking. Our results indicate that it is impossible. %the D-term is\nproportional to the F-term and its derivatives, making it difficult to suppress\nthe D-term independently. This also suggests that the mass spectrum of the\nsfermions can be predicted by the $D$-term contributions, and that the Higgsino\nis not a candidate for dark matter in the scenario.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 07:24:32 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Maekawa', 'Nobuhiro', ''], ['Tanii', 'Taiju', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.12897,"submitter":"Haiyang Guo","authors":"Haiyang Guo, Fanhu Zeng, Fei Zhu, Wenzhuo Liu, Da-Han Wang, Jian Xu,\n  Xu-Yao Zhang, Cheng-Lin Liu","title":"Federated Continual Instruction Tuning","comments":"Preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  A vast amount of instruction tuning data is crucial for the impressive\nperformance of Large Multimodal Models (LMMs), but the associated computational\ncosts and data collection demands during supervised fine-tuning make it\nimpractical for most researchers. Federated learning (FL) has the potential to\nleverage all distributed data and training resources to reduce the overhead of\njoint training. However, most existing methods assume a fixed number of tasks,\nwhile in real-world scenarios, clients continuously encounter new knowledge and\noften struggle to retain old tasks due to memory constraints. In this work, we\nintroduce the Federated Continual Instruction Tuning (FCIT) benchmark to model\nthis real-world challenge. Our benchmark includes two realistic scenarios,\nencompassing four different settings and twelve carefully curated instruction\ntuning datasets. To address the challenges posed by FCIT, we propose dynamic\nknowledge organization to effectively integrate updates from different tasks\nduring training and subspace selective activation to allocate task-specific\noutput during inference. Extensive experimental results demonstrate that our\nproposed method significantly enhances model performance across varying levels\nof data heterogeneity and catastrophic forgetting. Our source code and dataset\nwill be made publicly available.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 07:58:06 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Guo', 'Haiyang', ''], ['Zeng', 'Fanhu', ''], ['Zhu', 'Fei', ''], ['Liu', 'Wenzhuo', ''], ['Wang', 'Da-Han', ''], ['Xu', 'Jian', ''], ['Zhang', 'Xu-Yao', ''], ['Liu', 'Cheng-Lin', '']]","extracted_entities":"[{'text': 'Large Multimodal Models', 'label': 'Large Language Model'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'Federated learning', 'label': 'Few-shot Learning'}, {'text': 'publicly available', 'label': 'Open-source LLMs'}]","assigned_concept":"Fine-tuning","matched_keyword":"supervised fine-tuning","similarity_score":0.7449287176}
{"id":2503.12927,"submitter":"Yifei Chen","authors":"Huangwei Chen, Yifei Chen, Zhenyu Yan, Mingyang Ding, Chenlei Li, Zhu\n  Zhu, Feiwei Qin","title":"MMLNB: Multi-Modal Learning for Neuroblastoma Subtyping Classification\n  Assisted with Textual Description Generation","comments":"25 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Neuroblastoma (NB), a leading cause of childhood cancer mortality, exhibits\nsignificant histopathological variability, necessitating precise subtyping for\naccurate prognosis and treatment. Traditional diagnostic methods rely on\nsubjective evaluations that are time-consuming and inconsistent. To address\nthese challenges, we introduce MMLNB, a multi-modal learning (MML) model that\nintegrates pathological images with generated textual descriptions to improve\nclassification accuracy and interpretability. The approach follows a two-stage\nprocess. First, we fine-tune a Vision-Language Model (VLM) to enhance\npathology-aware text generation. Second, the fine-tuned VLM generates textual\ndescriptions, using a dual-branch architecture to independently extract visual\nand textual features. These features are fused via Progressive Robust\nMulti-Modal Fusion (PRMF) Block for stable training. Experimental results show\nthat the MMLNB model is more accurate than the single modal model. Ablation\nstudies demonstrate the importance of multi-modal fusion, fine-tuning, and the\nPRMF mechanism. This research creates a scalable AI-driven framework for\ndigital pathology, enhancing reliability and interpretability in NB subtyping\nclassification. Our source code is available at\nhttps:\/\/github.com\/HovChen\/MMLNB.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 08:38:46 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 09:27:16 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Chen', 'Huangwei', ''], ['Chen', 'Yifei', ''], ['Yan', 'Zhenyu', ''], ['Ding', 'Mingyang', ''], ['Li', 'Chenlei', ''], ['Zhu', 'Zhu', ''], ['Qin', 'Feiwei', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.12968,"submitter":"Guanhua Ding","authors":"Guanhua Ding, Yuxuan Xia, Runwei Guan, Qinchen Wu, Tao Huang, Weiping\n  Ding, Jinping Sun, and Guoqiang Mao","title":"OptiPMB: Enhancing 3D Multi-Object Tracking with Optimized Poisson\n  Multi-Bernoulli Filtering","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Accurate 3D multi-object tracking (MOT) is crucial for autonomous driving, as\nit enables robust perception, navigation, and planning in complex environments.\nWhile deep learning-based solutions have demonstrated impressive 3D MOT\nperformance, model-based approaches remain appealing for their simplicity,\ninterpretability, and data efficiency. Conventional model-based trackers\ntypically rely on random vector-based Bayesian filters within the\ntracking-by-detection (TBD) framework but face limitations due to heuristic\ndata association and track management schemes. In contrast, random finite set\n(RFS)-based Bayesian filtering handles object birth, survival, and death in a\ntheoretically sound manner, facilitating interpretability and parameter tuning.\nIn this paper, we present OptiPMB, a novel RFS-based 3D MOT method that employs\nan optimized Poisson multi-Bernoulli (PMB) filter while incorporating several\nkey innovative designs within the TBD framework. Specifically, we propose a\nmeasurement-driven hybrid adaptive birth model for improved track\ninitialization, employ adaptive detection probability parameters to effectively\nmaintain tracks for occluded objects, and optimize density pruning and track\nextraction modules to further enhance overall tracking performance. Extensive\nevaluations on nuScenes and KITTI datasets show that OptiPMB achieves superior\ntracking accuracy compared with state-of-the-art methods, thereby establishing\na new benchmark for model-based 3D MOT and offering valuable insights for\nfuture research on RFS-based trackers in autonomous driving.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 09:24:26 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Ding', 'Guanhua', ''], ['Xia', 'Yuxuan', ''], ['Guan', 'Runwei', ''], ['Wu', 'Qinchen', ''], ['Huang', 'Tao', ''], ['Ding', 'Weiping', ''], ['Sun', 'Jinping', ''], ['Mao', 'Guoqiang', '']]","extracted_entities":"[{'text': 'parameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"parameter tuning","similarity_score":0.6959539652}
{"id":2503.13021,"submitter":"Omri Suissa","authors":"Omri Suissa, Muhiim Ali, Ariana Azarbal, Hui Shen, Shekhar Pradhan","title":"Dynamic Relation Inference via Verb Embeddings","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  CLIP has demonstrated exceptional image-text matching capabilities due to its\ntraining on contrastive learning tasks. Past research has suggested that\nwhereas CLIP effectively matches text to images when the matching can be\nachieved just by matching the text with the objects in the image, CLIP\nstruggles when the matching depends on representing the relationship among the\nobjects in the images (i.e., inferring relations). Previous attempts to address\nthis limitation by training CLIP on relation detection datasets with only\nlinguistic supervision have met with limited success. In this paper, we offer\ninsights and practical methods to advance the field of relation inference from\nimages. This paper approaches the task of creating a model that effectively\ndetects relations among the objects in images by producing text and image\nembeddings that capture relationships through linguistic supervision. To this\nend, we propose Dynamic Relation Inference via Verb Embeddings (DRIVE), which\naugments the COCO dataset, fine-tunes CLIP with hard negatives\nsubject-relation-object triples and corresponding images, and introduces a\nnovel loss function to improve relation detection. Evaluated on multiple\nCLIP-based models, our method significantly improves zero-shot relation\ninference accuracy in both frozen and fine-tuned settings, significantly\noutperforming CLIP and state-of-the-art models while generalizing well on\nunseen data.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 10:24:27 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Suissa', 'Omri', ''], ['Ali', 'Muhiim', ''], ['Azarbal', 'Ariana', ''], ['Shen', 'Hui', ''], ['Pradhan', 'Shekhar', '']]","extracted_entities":"[{'text': 'text and image\\nembeddings', 'label': 'Embedding'}, {'text': 'Verb Embeddings', 'label': 'Embedding'}, {'text': 'fine-tuned settings', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuned settings","similarity_score":0.7898933887}
{"id":2503.13029,"submitter":"Adrian Bekasiewicz","authors":"Adrian Bekasiewicz, Mariusz Dzwonkowski, Tom Dhaene, and Ivo Couckuyt","title":"Specification-Oriented Automatic Design of Topologically Agnostic\n  Antenna Structure","comments":null,"journal-ref":null,"doi":"10.1007\/978-3-031-63759-9_2","report-no":null,"categories":"math.NA cs.NA eess.SP","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Design of antennas for modern applications is a challenging task that\ncombines cognition-driven development of topology intertwined with tuning of\nits parameters using rigorous numerical optimization. However, the process can\nbe streamlined by neglecting the engineering insight in favor of automatic\nde-termination of structure geometry. In this work, a specification-oriented\ndesign of topologically agnostic antenna is considered. The radiator is\ndeveloped using a bi-stage algorithm that involves min-max classification of\nrandomly-generated topologies followed by local tuning of the promising designs\nusing a trust-region optimization applied to a feature-based representation of\nthe structure frequency response. The automatically generated antenna is\ncharacterized by -10 dB bandwidth of over 600 MHz w.r.t. the center frequency\nof 6.5 GHz and a dual-lobe radiation pattern. The obtained performance figures\nmake the radiator of use for in-door positioning applications. The design\nmethod has been favorably compared against the frequency-based trust-region\noptimization.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 10:31:28 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Bekasiewicz', 'Adrian', ''], ['Dzwonkowski', 'Mariusz', ''], ['Dhaene', 'Tom', ''], ['Couckuyt', 'Ivo', '']]","extracted_entities":"[{'text': 'local tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"local tuning","similarity_score":0.6783174872}
{"id":2503.13049,"submitter":"Pietro Tierno Dr","authors":"Andris P. Stikuts, Seemant Mishra, Artem Ryabov, Philipp Maass, Pietro\n  Tierno","title":"Engineering tunable fractional Shapiro steps in colloidal transport","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.soft","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Shapiro steps are quantized plateaus in the velocity-force or velocity-torque\ncurve of a driven system, when its speed remains constant despite an increase\nin the driving force. For microscopic particles driven across a sinusoidal\npotential, integer Shapiro steps have been observed. By driving a single\ncolloidal particle across a time-modulated, non-sinusoidal periodic optical\nlandscape, we here demonstrate that fractional Shapiro steps emerge in addition\nto integer ones. Measuring the particle position via individual particle\ntracking, we reveal the underlying microscopic mechanisms that produce integer\nand fractional steps and demonstrate how these steps can be controlled by\ntuning the shape and driving protocol of the optical potential. The flexibility\noffered by optical engineering allows us to generate wide ranges of potential\nshapes and to study, at the single-particle level, synchronization behavior in\ndriven soft condensed matter systems.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 10:54:02 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Stikuts', 'Andris P.', ''], ['Mishra', 'Seemant', ''], ['Ryabov', 'Artem', ''], ['Maass', 'Philipp', ''], ['Tierno', 'Pietro', '']]","extracted_entities":"[{'text': 'tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"tuning","similarity_score":0.8449009061}
{"id":2503.13054,"submitter":"Tiago Castro","authors":"A. Fumagalli, T. Castro, S. Borgani, and M. Valentini","title":"On the Robustness of Cluster Clustering Covariance Calibration","comments":"12 pages, 8 figures, comments welcome :)","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.CO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Ongoing and upcoming wide-field surveys at different wavelengths will measure\nthe distribution of galaxy clusters with unprecedented precision, demanding\naccurate models for the two-point correlation function (2PCF) covariance. In\nthis work, we assess a semi-analytical framework for the cluster 2PCF\ncovariance that employs three nuisance parameters to account for non-Poissonian\nshot noise, residual uncertainties in the halo bias model, and subleading noise\nterms. We calibrate these parameters on a suite of fast approximate simulations\ngenerated by PINOCCHIO as well as full $N$-body simulations from OpenGADGET3.\nWe demonstrate that PINOCCHIO can reproduce the 2PCF covariance measured in\nOpenGADGET3 at the few percent level, provided the mass functions are carefully\nrescaled. Resolution tests confirm that high particle counts are necessary to\ncapture shot-noise corrections, especially at high redshifts. We perform the\nparameter calibration across multiple cosmological models, showing that one of\nthe nuisance parameters, the non-Poissonian shot-noise correction $\\alpha$,\ndepends mildly on the amplitude of matter fluctuations $\\sigma_8$. In contrast,\nthe remaining two parameters, $\\beta$ controlling the bias correction and\n$\\gamma$ controlling the secondary shot-noise correction, exhibit more\nsignificant variation with redshift and halo mass. Overall, our results\nunderscore the importance of calibrating covariance models on realistic mock\ncatalogs that replicate the selection function of forthcoming surveys and\nhighlight that approximate methods, when properly tuned, can effectively\ncomplement full $N$-body simulations for precision cluster cosmology.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 10:59:19 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Fumagalli', 'A.', ''], ['Castro', 'T.', ''], ['Borgani', 'S.', ''], ['Valentini', 'M.', '']]","extracted_entities":"[{'text': 'PINOCCHIO', 'label': 'AI model'}, {'text': 'PINOCCHIO', 'label': 'AI model'}, {'text': 'properly tuned', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"properly tuned","similarity_score":0.798178494}
{"id":2503.13203,"submitter":"Corentin Sautier","authors":"Corentin Sautier, Gilles Puy, Alexandre Boulch, Renaud Marlet, Vincent\n  Lepetit","title":"Clustering is back: Reaching state-of-the-art LiDAR instance\n  segmentation without training","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Panoptic segmentation of LiDAR point clouds is fundamental to outdoor scene\nunderstanding, with autonomous driving being a primary application. While\nstate-of-the-art approaches typically rely on end-to-end deep learning\narchitectures and extensive manual annotations of instances, the significant\ncost and time investment required for labeling large-scale point cloud datasets\nremains a major bottleneck in this field. In this work, we demonstrate that\ncompetitive panoptic segmentation can be achieved using only semantic labels,\nwith instances predicted without any training or annotations. Our method\nachieves performance comparable to current state-of-the-art supervised methods\non standard benchmarks including SemanticKITTI and nuScenes, and outperforms\nevery publicly available method on SemanticKITTI as a drop-in instance head\nreplacement, while running in real-time on a single-threaded CPU and requiring\nno instance labels. Our method is fully explainable, and requires no learning\nor parameter tuning. Code is available at https:\/\/github.com\/valeoai\/Alpine\/\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 14:12:08 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Sautier', 'Corentin', ''], ['Puy', 'Gilles', ''], ['Boulch', 'Alexandre', ''], ['Marlet', 'Renaud', ''], ['Lepetit', 'Vincent', '']]","extracted_entities":"[{'text': 'parameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"parameter tuning","similarity_score":0.6959539652}
{"id":2503.13208,"submitter":"Fan Sinan","authors":"Sinan Fan, Liang Xie, Chen Shen, Ge Teng, Xiaosong Yuan, Xiaofeng\n  Zhang, Chenxi Huang, Wenxiao Wang, Xiaofei He, Jieping Ye","title":"Improving Complex Reasoning with Dynamic Prompt Corruption: A soft\n  prompt Optimization Approach","comments":"Accepted by ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Prompt-tuning (PT) for large language models (LLMs) can facilitate the\nperformance on various conventional NLP tasks with significantly fewer\ntrainable parameters. However, our investigation reveals that PT provides\nlimited improvement and may even degrade the primitive performance of LLMs on\ncomplex reasoning tasks. Such a phenomenon suggests that soft prompts can\npositively impact certain instances while negatively affecting others,\nparticularly during the later phases of reasoning. To address these challenges,\nWe first identify an information accumulation within the soft prompts. Through\ndetailed analysis, we demonstrate that this phenomenon is often accompanied by\nerroneous information flow patterns in the deeper layers of the model, which\nultimately lead to incorrect reasoning outcomes. we propose a novel method\ncalled \\textbf{D}ynamic \\textbf{P}rompt \\textbf{C}orruption (DPC) to take\nbetter advantage of soft prompts in complex reasoning tasks, which dynamically\nadjusts the influence of soft prompts based on their impact on the reasoning\nprocess. Specifically, DPC consists of two stages: Dynamic Trigger and Dynamic\nCorruption. First, Dynamic Trigger measures the impact of soft prompts,\nidentifying whether beneficial or detrimental. Then, Dynamic Corruption\nmitigates the negative effects of soft prompts by selectively masking key\ntokens that interfere with the reasoning process. We validate the proposed\napproach through extensive experiments on various LLMs and reasoning tasks,\nincluding GSM8K, MATH, and AQuA. Experimental results demonstrate that DPC can\nconsistently enhance the performance of PT, achieving 4\\%-8\\% accuracy gains\ncompared to vanilla prompt tuning, highlighting the effectiveness of our\napproach and its potential to enhance complex reasoning in LLMs.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 14:20:48 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Fan', 'Sinan', ''], ['Xie', 'Liang', ''], ['Shen', 'Chen', ''], ['Teng', 'Ge', ''], ['Yuan', 'Xiaosong', ''], ['Zhang', 'Xiaofeng', ''], ['Huang', 'Chenxi', ''], ['Wang', 'Wenxiao', ''], ['He', 'Xiaofei', ''], ['Ye', 'Jieping', '']]","extracted_entities":"[{'text': 'Prompt-tuning', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'PT', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Dynamic Trigger', 'label': 'Fine-tuning'}, {'text': 'Dynamic\\nCorruption', 'label': 'Fine-tuning'}, {'text': 'Dynamic Trigger', 'label': 'Prompting'}, {'text': 'Dynamic Corruption', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'MATH', 'label': 'Large Language Model'}, {'text': 'PT', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Fine-tuning","matched_keyword":"Prompt-tuning","similarity_score":0.5565450788}
{"id":2503.13281,"submitter":"Xiaodi Li","authors":"Xiaodi Li, Shaika Chowdhury, Chung Il Wi, Maria Vassilaki, Ken Liu,\n  Terence T Sio, Owen Garrick, Young J Juhn, James R Cerhan, Cui Tao, and Nansu\n  Zong","title":"LLM-Match: An Open-Sourced Patient Matching Model Based on Large\n  Language Models and Retrieval-Augmented Generation","comments":"10 pages, 1 figure","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Patient matching is the process of linking patients to appropriate clinical\ntrials by accurately identifying and matching their medical records with trial\neligibility criteria. We propose LLM-Match, a novel framework for patient\nmatching leveraging fine-tuned open-source large language models. Our approach\nconsists of four key components. First, a retrieval-augmented generation (RAG)\nmodule extracts relevant patient context from a vast pool of electronic health\nrecords (EHRs). Second, a prompt generation module constructs input prompts by\nintegrating trial eligibility criteria (both inclusion and exclusion criteria),\npatient context, and system instructions. Third, a fine-tuning module with a\nclassification head optimizes the model parameters using structured prompts and\nground-truth labels. Fourth, an evaluation module assesses the fine-tuned\nmodel's performance on the testing datasets. We evaluated LLM-Match on four\nopen datasets - n2c2, SIGIR, TREC 2021, and TREC 2022 - using open-source\nmodels, comparing it against TrialGPT, Zero-Shot, and GPT-4-based closed\nmodels. LLM-Match outperformed all baselines.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:31:55 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 14:56:41 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Li', 'Xiaodi', ''], ['Chowdhury', 'Shaika', ''], ['Wi', 'Chung Il', ''], ['Vassilaki', 'Maria', ''], ['Liu', 'Ken', ''], ['Sio', 'Terence T', ''], ['Garrick', 'Owen', ''], ['Juhn', 'Young J', ''], ['Cerhan', 'James R', ''], ['Tao', 'Cui', ''], ['Zong', 'Nansu', '']]","extracted_entities":"[{'text': 'input prompts', 'label': 'Prompting'}, {'text': 'fine-tuning module', 'label': 'Fine-tuning'}, {'text': 'structured prompts', 'label': 'Prompting'}, {'text': 'n2c2', 'label': 'Large Language Model'}, {'text': 'TrialGPT', 'label': 'ChatGPT'}, {'text': 'Zero-Shot', 'label': 'ChatGPT'}, {'text': 'GPT-4-based', 'label': 'GPT'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning module","similarity_score":0.7772243023}
{"id":2503.13369,"submitter":"Wan Ju Kang","authors":"Wan Ju Kang, Eunki Kim, Na Min An, Sangryul Kim, Haemin Choi, Ki Hoon\n  Kwak, and James Thorne","title":"Sightation Counts: Leveraging Sighted User Feedback in Building a\n  BLV-aligned Dataset of Diagram Descriptions","comments":"37 pages, 10 figures, 21 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CV cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Often, the needs and visual abilities differ between the annotator group and\nthe end user group. Generating detailed diagram descriptions for blind and\nlow-vision (BLV) users is one such challenging domain. Sighted annotators could\ndescribe visuals with ease, but existing studies have shown that direct\ngenerations by them are costly, bias-prone, and somewhat lacking by BLV\nstandards. In this study, we ask sighted individuals to assess -- rather than\nproduce -- diagram descriptions generated by vision-language models (VLM) that\nhave been guided with latent supervision via a multi-pass inference. The\nsighted assessments prove effective and useful to professional educators who\nare themselves BLV and teach visually impaired learners. We release Sightation,\na collection of diagram description datasets spanning 5k diagrams and 137k\nsamples for completion, preference, retrieval, question answering, and\nreasoning training purposes and demonstrate their fine-tuning potential in\nvarious downstream tasks.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 16:52:46 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Kang', 'Wan Ju', ''], ['Kim', 'Eunki', ''], ['An', 'Na Min', ''], ['Kim', 'Sangryul', ''], ['Choi', 'Haemin', ''], ['Kwak', 'Ki Hoon', ''], ['Thorne', 'James', '']]","extracted_entities":"[{'text': 'fine-tuning potential', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning potential","similarity_score":0.6934475899}
{"id":2503.13443,"submitter":"Haoyang Li Mr.","authors":"Haoyang Li, Liang Wang, Chao Wang, Jing Jiang, Yan Peng, Guodong Long","title":"DPC: Dual-Prompt Collaboration for Tuning Vision-Language Models","comments":"Accepted by the IEEE\/CVF Conference on Computer Vision and Pattern\n  Recognition 2025 (CVPR 2025)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The Base-New Trade-off (BNT) problem universally exists during the\noptimization of CLIP-based prompt tuning, where continuous fine-tuning on base\n(target) classes leads to a simultaneous decrease of generalization ability on\nnew (unseen) classes. Existing approaches attempt to regulate the prompt tuning\nprocess to balance BNT by appending constraints. However, imposed on the same\ntarget prompt, these constraints fail to fully avert the mutual exclusivity\nbetween the optimization directions for base and new. As a novel solution to\nthis challenge, we propose the plug-and-play Dual-Prompt Collaboration (DPC)\nframework, the first that decoupling the optimization processes of base and new\ntasks at the prompt level. Specifically, we clone a learnable parallel prompt\nbased on the backbone prompt, and introduce a variable Weighting-Decoupling\nframework to independently control the optimization directions of dual prompts\nspecific to base or new tasks, thus avoiding the conflict in generalization.\nMeanwhile, we propose a Dynamic Hard Negative Optimizer, utilizing dual prompts\nto construct a more challenging optimization task on base classes for\nenhancement. For interpretability, we prove the feature channel invariance of\nthe prompt vector during the optimization process, providing theoretical\nsupport for the Weighting-Decoupling of DPC. Extensive experiments on multiple\nbackbones demonstrate that DPC can significantly improve base performance\nwithout introducing any external knowledge beyond the base classes, while\nmaintaining generalization to new classes. Code is available at:\nhttps:\/\/github.com\/JREion\/DPC.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:59:27 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Li', 'Haoyang', ''], ['Wang', 'Liang', ''], ['Wang', 'Chao', ''], ['Jiang', 'Jing', ''], ['Peng', 'Yan', ''], ['Long', 'Guodong', '']]","extracted_entities":"[{'text': 'BNT', 'label': 'BERT'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'continuous fine-tuning', 'label': 'Fine-tuning'}, {'text': 'base', 'label': 'Foundation Model'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'BNT', 'label': 'BERT'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'prompt', 'label': 'Prompting'}]","assigned_concept":"Fine-tuning","matched_keyword":"continuous fine-tuning","similarity_score":0.8377360106}
{"id":2503.13551,"submitter":"Teng Wang","authors":"Teng Wang, Zhangyi Jiang, Zhenqi He, Wenhan Yang, Yanan Zheng, Zeyu\n  Li, Zifan He, Shenyang Tong, Hailei Gong","title":"Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in\n  Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent studies show that Large Language Models (LLMs) achieve strong\nreasoning capabilities through supervised fine-tuning or reinforcement\nlearning. However, a key approach, the Process Reward Model (PRM), suffers from\nreward hacking, making it unreliable in identifying the best intermediate\nsteps. In this paper, we propose a novel reward model approach, Hierarchical\nReward Model (HRM), which evaluates both individual and consecutive reasoning\nsteps from fine-grained and coarse-grained level. HRM performs better in\nassessing reasoning coherence and self-reflection, particularly when the\nprevious reasoning step is incorrect. Furthermore, to address the inefficiency\nof autonomous generating PRM training data via Monte Carlo Tree Search (MCTS),\nwe introduce a lightweight and effective data augmentation strategy called\nHierarchical Node Compression (HNC) based on node merging (combining two\nconsecutive reasoning steps into one step) in the tree structure. This approach\ndiversifies MCTS results for HRM with negligible computational overhead,\nenhancing label robustness by introducing noise. Empirical results on the\nPRM800K dataset demonstrate that HRM, in conjunction with HNC, achieves\nsuperior stability and reliability in evaluation compared to PRM. Furthermore,\ncross-domain evaluations on MATH500 and GSM8K confirm HRM's superior\ngeneralization and robustness across diverse reasoning tasks. The code for all\nexperiments will be released at https:\n\/\/github.com\/tengwang0318\/hierarchial_reward_model.\n","versions":"[{'version': 'v1', 'created': 'Sun, 16 Mar 2025 15:18:40 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:43:56 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Wang', 'Teng', ''], ['Jiang', 'Zhangyi', ''], ['He', 'Zhenqi', ''], ['Yang', 'Wenhan', ''], ['Zheng', 'Yanan', ''], ['Li', 'Zeyu', ''], ['He', 'Zifan', ''], ['Tong', 'Shenyang', ''], ['Gong', 'Hailei', '']]","extracted_entities":"[{'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'reinforcement\\nlearning', 'label': 'Few-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"supervised fine-tuning","similarity_score":0.7449287176}
{"id":2503.13661,"submitter":"Huy Hoang Ha","authors":"Huy Hoang Ha","title":"Pensez: Less Data, Better Reasoning -- Rethinking French LLM","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious natural language processing tasks. However, achieving strong\nperformance in specialized domains like mathematical reasoning and non-English\nlanguages often requires extensive training on massive datasets. This paper\ninvestigates a contrasting approach: strategic fine-tuning on a small,\nhigh-quality, bilingual (English-French) dataset to enhance both the reasoning\ncapabilities and French language proficiency of a large language model. Rather\nthan relying on scale, we explore the hypothesis that targeted data curation\nand optimized training can achieve competitive, or even superior, performance.\nWe demonstrate, through targeted supervised fine-tuning (SFT) on only 2,000\ncarefully selected samples, significant improvements in mathematical reasoning.\nSpecifically, Pensez 7B exhibits an increase in accuracy of the base model up\nto 20% on the AIME25 and a 12% increase on a French MATH level 5 benchmark.\nThese results challenge the prevailing assumption that massive datasets are\naprerequisite for strong reasoning performance in LLMs, highlighting the\npotential of strategic data curation and optimized fine-tuning for enhancing\nboth specialized skills and multilingual capabilities. Our findings have\nimplications for the efficient development of high-performing, multilingual\nLLMs, especially in resource-constrained scenarios.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 19:09:11 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Ha', 'Huy Hoang', '']]","extracted_entities":"[{'text': 'strategic fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"strategic fine-tuning","similarity_score":0.6777284145}
{"id":2503.13819,"submitter":"Xiaopei Chen","authors":"Xiaopei Chen, Wen Wu, Zuguang Li, Liang Li, Fei Ji","title":"LLM-Empowered IoT for 6G Networks: Architecture, Challenges, and\n  Solutions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.ET","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The Internet of Things (IoT) in the sixth generation (6G) era is envisioned\nto evolve towards intelligence, ubiquity, and self-optimization. Large language\nmodels (LLMs) have demonstrated remarkable generalization capabilities across\ndiverse domains, including natural language processing (NLP), computer vision\n(CV), and beyond. In this article, we propose an LLM-empowered IoT architecture\nfor 6G networks to achieve intelligent autonomy while supporting advanced IoT\napplications. LLMs are pushed to the edge of the 6G network to support the\nsynergy of LLMs and IoT. LLM solutions are tailored to both IoT application\nrequirements and IoT management needs, i.e., LLM for IoT. On the other hand,\nedge inference and edge fine-tuning are discussed to support the deployment of\nLLMs, i.e., LLM on IoT. Furthermore, we propose a memory-efficient split\nfederated learning (SFL) framework for LLM fine-tuning on heterogeneous IoT\ndevices that alleviates memory pressures on both IoT devices and the edge\nserver while achieving comparable performance and convergence time. Finally, a\ncase study is presented, followed by a discussion about open issues of\nLLM-empowered IoT for 6G networks.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 01:53:42 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Chen', 'Xiaopei', ''], ['Wu', 'Wen', ''], ['Li', 'Zuguang', ''], ['Li', 'Liang', ''], ['Ji', 'Fei', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'edge fine-tuning', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Fine-tuning","matched_keyword":"edge fine-tuning","similarity_score":0.6748901606}
{"id":2503.13836,"submitter":"Seokhyeon Hong","authors":"Seokhyeon Hong, Chaelin Kim, Serin Yoon, Junghyun Nam, Sihun Cha,\n  Junyong Noh","title":"SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation\n  and Editing","comments":"CVPR 2025; Project page\n  https:\/\/seokhyeonhong.github.io\/projects\/salad\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.GR cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Text-driven motion generation has advanced significantly with the rise of\ndenoising diffusion models. However, previous methods often oversimplify\nrepresentations for the skeletal joints, temporal frames, and textual words,\nlimiting their ability to fully capture the information within each modality\nand their interactions. Moreover, when using pre-trained models for downstream\ntasks, such as editing, they typically require additional efforts, including\nmanual interventions, optimization, or fine-tuning. In this paper, we introduce\na skeleton-aware latent diffusion (SALAD), a model that explicitly captures the\nintricate inter-relationships between joints, frames, and words. Furthermore,\nby leveraging cross-attention maps produced during the generation process, we\nenable attention-based zero-shot text-driven motion editing using a pre-trained\nSALAD model, requiring no additional user input beyond text prompts. Our\napproach significantly outperforms previous methods in terms of text-motion\nalignment without compromising generation quality, and demonstrates practical\nversatility by providing diverse editing capabilities beyond generation. Code\nis available at project page.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 02:20:11 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Hong', 'Seokhyeon', ''], ['Kim', 'Chaelin', ''], ['Yoon', 'Serin', ''], ['Nam', 'Junghyun', ''], ['Cha', 'Sihun', ''], ['Noh', 'Junyong', '']]","extracted_entities":"[{'text': 'optimization', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'attention-based zero-shot text-driven motion editing', 'label': 'Few-shot Learning'}, {'text': 'text prompts', 'label': 'Prompting'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.13847,"submitter":"Monika Shah","authors":"Monika Shah, Somdeb Sarkhel, Deepak Venugopal","title":"Disentangling Fine-Tuning from Pre-Training in Visual Captioning with\n  Hybrid Markov Logic","comments":"2024 IEEE International Conference on Big Data (BigData), 10 pages","journal-ref":null,"doi":"10.1109\/BigData62323.2024.10825003","report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Multimodal systems have highly complex processing pipelines and are\npretrained over large datasets before being fine-tuned for specific tasks such\nas visual captioning. However, it becomes hard to disentangle what the model\nlearns during the fine-tuning process from what it already knows due to its\npretraining. In this work, we learn a probabilistic model using Hybrid Markov\nLogic Networks (HMLNs) over the training examples by relating symbolic\nknowledge (extracted from the caption) with visual features (extracted from the\nimage). For a generated caption, we quantify the influence of training examples\nbased on the HMLN distribution using probabilistic inference. We evaluate two\ntypes of inference procedures on the MSCOCO dataset for different types of\ncaptioning models. Our results show that for BLIP2 (a model that uses a LLM),\nthe fine-tuning may have smaller influence on the knowledge the model has\nacquired since it may have more general knowledge to perform visual captioning\nas compared to models that do not use a LLM\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 02:39:26 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Shah', 'Monika', ''], ['Sarkhel', 'Somdeb', ''], ['Venugopal', 'Deepak', '']]","extracted_entities":"[{'text': 'fine-tuned', 'label': 'Fine-tuning'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.13939,"submitter":"Yuxiang Lai","authors":"Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, Xiaofeng Yang","title":"Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in\n  Vision-Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Vision-language models (VLMs) have advanced reasoning in natural scenes, but\ntheir role in medical imaging remains underexplored. Medical reasoning tasks\ndemand robust image analysis and well-justified answers, posing challenges due\nto the complexity of medical images. Transparency and trustworthiness are\nessential for clinical adoption and regulatory compliance. We introduce Med-R1,\na framework exploring reinforcement learning (RL) to enhance VLMs'\ngeneralizability and trustworthiness in medical reasoning. Leveraging the\nDeepSeek strategy, we employ Group Relative Policy Optimization (GRPO) to guide\nreasoning paths via reward signals. Unlike supervised fine-tuning (SFT), which\noften overfits and lacks generalization, RL fosters robust and diverse\nreasoning. Med-R1 is evaluated across eight medical imaging modalities: CT,\nMRI, Ultrasound, Dermoscopy, Fundus Photography, Optical Coherence Tomography\n(OCT), Microscopy, and X-ray Imaging. Compared to its base model, Qwen2-VL-2B,\nMed-R1 achieves a 29.94% accuracy improvement and outperforms Qwen2-VL-72B,\nwhich has 36 times more parameters. Testing across five question types-modality\nrecognition, anatomy identification, disease diagnosis, lesion grading, and\nbiological attribute analysis Med-R1 demonstrates superior generalization,\nexceeding Qwen2-VL-2B by 32.06% and surpassing Qwen2-VL-72B in question-type\ngeneralization. These findings show that RL improves medical reasoning and\nenables parameter-efficient models to outperform significantly larger ones.\nWith interpretable reasoning outputs, Med-R1 represents a promising step toward\ngeneralizable, trustworthy, and clinically viable medical VLMs.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 06:12:38 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 20:58:55 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Lai', 'Yuxiang', ''], ['Zhong', 'Jike', ''], ['Li', 'Ming', ''], ['Zhao', 'Shitian', ''], ['Yang', 'Xiaofeng', '']]","extracted_entities":"[{'text': 'regulatory compliance', 'label': 'AI Ethics'}, {'text': 'supervised fine-tuning (SFT)', 'label': 'Fine-tuning'}, {'text': 'Med-R1', 'label': 'Foundation Model'}]","assigned_concept":"Fine-tuning","matched_keyword":"supervised fine-tuning (SFT)","similarity_score":0.6747050285}
{"id":2503.1394,"submitter":"Hang Zhao","authors":"Hang Zhao, Hongru Li, Dongfang Xu, Shenghui Song, and Khaled B.\n  Letaief","title":"Multi-Modal Self-Supervised Semantic Communication","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV eess.SP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Semantic communication is emerging as a promising paradigm that focuses on\nthe extraction and transmission of semantic meanings using deep learning\ntechniques. While current research primarily addresses the reduction of\nsemantic communication overhead, it often overlooks the training phase, which\ncan incur significant communication costs in dynamic wireless environments. To\naddress this challenge, we propose a multi-modal semantic communication system\nthat leverages multi-modal self-supervised learning to enhance task-agnostic\nfeature extraction. The proposed approach employs self-supervised learning\nduring the pre-training phase to extract task-agnostic semantic features,\nfollowed by supervised fine-tuning for downstream tasks. This dual-phase\nstrategy effectively captures both modality-invariant and modality-specific\nfeatures while minimizing training-related communication overhead. Experimental\nresults on the NYU Depth V2 dataset demonstrate that the proposed method\nsignificantly reduces training-related communication overhead while maintaining\nor exceeding the performance of existing supervised learning approaches. The\nfindings underscore the advantages of multi-modal self-supervised learning in\nsemantic communication, paving the way for more efficient and scalable edge\ninference systems.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 06:13:02 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Zhao', 'Hang', ''], ['Li', 'Hongru', ''], ['Xu', 'Dongfang', ''], ['Song', 'Shenghui', ''], ['Letaief', 'Khaled B.', '']]","extracted_entities":"[{'text': 'multi-modal self-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'self-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'multi-modal self-supervised learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"supervised fine-tuning","similarity_score":0.7449287176}
{"id":2503.14036,"submitter":"Ina Kodrasi","authors":"Mingchi Hou and Ina Kodrasi","title":"Variational Autoencoder for Personalized Pathological Speech Enhancement","comments":"Submitted to EUSIPCO 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.AS cs.SD","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The generalizability of speech enhancement (SE) models across speaker\nconditions remains largely unexplored, despite its critical importance for\nbroader applicability. This paper investigates the performance of the hybrid\nvariational autoencoder (VAE)-non-negative matrix factorization (NMF) model for\nSE, focusing primarily on its generalizability to pathological speakers with\nParkinson's disease. We show that VAE models trained on large neurotypical\ndatasets perform poorly on pathological speech. While fine-tuning these\npre-trained models with pathological speech improves performance, a performance\ngap remains between neurotypical and pathological speakers. To address this\ngap, we propose using personalized SE models derived from fine-tuning\npre-trained models with only a few seconds of clean data from each speaker. Our\nresults demonstrate that personalized models considerably enhance performance\nfor all speakers, achieving comparable results for both neurotypical and\npathological speakers.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 08:54:00 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Hou', 'Mingchi', ''], ['Kodrasi', 'Ina', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.14118,"submitter":"Michele Ceriotti","authors":"Arslan Mazitov, Filippo Bigi, Matthias Kellner, Paolo Pegolo, Davide\n  Tisi, Guillaume Fraux, Sergey Pozdnyakov, Philip Loche, and Michele Ceriotti","title":"PET-MAD, a universal interatomic potential for advanced materials\n  modeling","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci cs.LG physics.chem-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Machine-learning interatomic potentials (MLIPs) have greatly extended the\nreach of atomic-scale simulations, offering the accuracy of first-principles\ncalculations at a fraction of the effort. Leveraging large quantum mechanical\ndatabases and expressive architectures, recent \"universal\" models deliver\nqualitative accuracy across the periodic table but are often biased toward\nlow-energy configurations. We introduce PET-MAD, a generally applicable MLIP\ntrained on a dataset combining stable inorganic and organic solids,\nsystematically modified to enhance atomic diversity. Using a moderate but\nhighly-consistent level of electronic-structure theory, we assess PET-MAD's\naccuracy on established benchmarks and advanced simulations of six materials.\nPET-MAD rivals state-of-the-art MLIPs for inorganic solids, while also being\nreliable for molecules, organic materials, and surfaces. It is stable and fast,\nenabling, out-of-the-box, the near-quantitative study of thermal and quantum\nmechanical fluctuations, functional properties, and phase transitions. It can\nbe efficiently fine-tuned to deliver full quantum mechanical accuracy with a\nminimal number of targeted calculations.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 10:35:30 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Mazitov', 'Arslan', ''], ['Bigi', 'Filippo', ''], ['Kellner', 'Matthias', ''], ['Pegolo', 'Paolo', ''], ['Tisi', 'Davide', ''], ['Fraux', 'Guillaume', ''], ['Pozdnyakov', 'Sergey', ''], ['Loche', 'Philip', ''], ['Ceriotti', 'Michele', '']]","extracted_entities":"[{'text': 'MLIPs', 'label': 'LLMs'}, {'text': 'MLIPs', 'label': 'LLMs'}, {'text': 'efficiently fine-tuned', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"efficiently fine-tuned","similarity_score":0.7684534192}
{"id":2503.14155,"submitter":"Jie Luo Dr.","authors":"Muxuan Yang, Dongyang Yan, Lei Gao, Wei Liu, Yun Lai, Yadong Xu, Zhi\n  Hong Hang, Jie Luo","title":"Electromagnetic Duality Symmetry-Protected Dirac-Like Cones","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.optics physics.class-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Dirac-like cones, featuring conical linear dispersions intersecting with flat\nbands, typically arise from accidental degeneracy of multiple modes that\nrequires precise tuning of material and structural parameters, inherently\nlimiting their robustness and applications. In this work, by introducing\nelectromagnetic duality symmetry into photonic crystals, we demonstrate the\nemergence of intrinsically robust deterministic Dirac-like cones. We show that\nsuch symmetry (achieved through either self-dual particles or non-self-dual\nparticle clusters with duality-glide symmetry) enforces double degeneracies for\nband structures of photonic crystals. Furthermore, by harnessing the joint\nduality-structural symmetry, multiple deterministic Dirac-like cones exhibiting\nexceptional resilience to lattice size variations can be obtained. Our\nintroduction of an extra symmetry into photonic crystals establishes a profound\nconnection between duality symmetry and Dirac physics, providing a robust\nplatform for advanced photonic band engineering.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:29:08 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Yang', 'Muxuan', ''], ['Yan', 'Dongyang', ''], ['Gao', 'Lei', ''], ['Liu', 'Wei', ''], ['Lai', 'Yun', ''], ['Xu', 'Yadong', ''], ['Hang', 'Zhi Hong', ''], ['Luo', 'Jie', '']]","extracted_entities":"[{'text': 'precise tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"precise tuning","similarity_score":0.7851148248}
{"id":2503.14173,"submitter":"Raul Quijada","authors":"Guillem Cadevall Ferreres, Marc Serrano Sanz, Marc Bardeli G\\'amez,\n  Pol Gerdt Basullas, Francesc Tarres Ruiz, Raul Quijada Ferrero","title":"NERCat: Fine-Tuning for Enhanced Named Entity Recognition in Catalan","comments":"7 pages, 1 table","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Named Entity Recognition (NER) is a critical component of Natural Language\nProcessing (NLP) for extracting structured information from unstructured text.\nHowever, for low-resource languages like Catalan, the performance of NER\nsystems often suffers due to the lack of high-quality annotated datasets. This\npaper introduces NERCat, a fine-tuned version of the GLiNER[1] model, designed\nto improve NER performance specifically for Catalan text. We used a dataset of\nmanually annotated Catalan television transcriptions to train and fine-tune the\nmodel, focusing on domains such as politics, sports, and culture. The\nevaluation results show significant improvements in precision, recall, and\nF1-score, particularly for underrepresented named entity categories such as\nLaw, Product, and Facility. This study demonstrates the effectiveness of\ndomain-specific fine-tuning in low-resource languages and highlights the\npotential for enhancing Catalan NLP applications through manual annotation and\nhigh-quality datasets.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:44:19 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Ferreres', 'Guillem Cadevall', ''], ['Sanz', 'Marc Serrano', ''], ['G\u00e1mez', 'Marc Bardeli', ''], ['Basullas', 'Pol Gerdt', ''], ['Ruiz', 'Francesc Tarres', ''], ['Ferrero', 'Raul Quijada', '']]","extracted_entities":"[{'text': 'domain-specific fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"domain-specific fine-tuning","similarity_score":0.6912822723}
{"id":2503.14258,"submitter":"Weihang Su","authors":"Weihang Su, Baoqing Yue, Qingyao Ai, Yiran Hu, Jiaqi Li, Changyue\n  Wang, Kaiyuan Zhang, Yueyue Wu, Yiqun Liu","title":"JuDGE: Benchmarking Judgment Document Generation for Chinese Legal\n  System","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.IR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https:\/\/github.com\/oneal2000\/JuDGE.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 13:48:18 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 15:09:51 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Su', 'Weihang', ''], ['Yue', 'Baoqing', ''], ['Ai', 'Qingyao', ''], ['Hu', 'Yiran', ''], ['Li', 'Jiaqi', ''], ['Wang', 'Changyue', ''], ['Zhang', 'Kaiyuan', ''], ['Wu', 'Yueyue', ''], ['Liu', 'Yiqun', '']]","extracted_entities":"[{'text': 'few-shot in-context learning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.14287,"submitter":"Enrico Tosi","authors":"Enrico Tosi, Panwei Hu, Aleksandar Ichkov, Marina Petrova, Ljiljana\n  Simi\\'c","title":"Cross-Environment Transfer Learning for Location-Aided Beam Prediction\n  in 5G and Beyond Millimeter-Wave Networks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP cs.SY eess.SY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Millimeter-wave (mm-wave) communications requirebeamforming and consequent\nprecise beam alignmentbetween the gNodeB (gNB) and the user equipment (UE)\ntoovercome high propagation losses. This beam alignment needs tobe constantly\nupdated for different UE locations based on beamsweepingradio frequency\nmeasurements, leading to significantbeam management overhead. One potential\nsolution involvesusing machine learning (ML) beam prediction algorithms\nthatleverage UE position information to select the serving beamwithout the\noverhead of beam sweeping. However, the highlysite-specific nature of mm-wave\npropagation means that MLmodels require training from scratch for each\nscenario, whichis inefficient in practice. In this paper, we propose a\nrobustcross-environment transfer learning solution for location-aidedbeam\nprediction, whereby the ML model trained on a referencegNB is transferred to a\ntarget gNB by fine-tuning with a limiteddataset. Extensive simulation results\nbased on ray-tracing in twourban environments show the effectiveness of our\nsolution forboth inter- and intra-city model transfer. Our results show thatby\ntraining the model on a reference gNB and transferring themodel by fine-tuning\nwith only 5% of the target gNB dataset,we can achieve 80% accuracy in\npredicting the best beamfor the target gNB. Importantly, our approach improves\nthepoor generalization accuracy of transferring the model to newenvironments\nwithout fine-tuning by around 75 percentage points.This demonstrates that\ntransfer learning enables high predictionaccuracy while reducing the\ncomputational and training datasetcollection burden of ML-based beam\nprediction, making itpractical for 5G-and-beyond deployments.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 14:24:50 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Tosi', 'Enrico', ''], ['Hu', 'Panwei', ''], ['Ichkov', 'Aleksandar', ''], ['Petrova', 'Marina', ''], ['Simi\u0107', 'Ljiljana', '']]","extracted_entities":"[{'text': 'transfer learning', 'label': 'Few-shot Learning'}, {'text': 'model', 'label': 'AI model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'model', 'label': 'AI model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'transfer learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.1435,"submitter":"Shoubin Yu","authors":"Shoubin Yu, Difan Liu, Ziqiao Ma, Yicong Hong, Yang Zhou, Hao Tan,\n  Joyce Chai, Mohit Bansal","title":"VEGGIE: Instructional Editing and Reasoning of Video Concepts with\n  Grounded Generation","comments":"First three authors contributed equally. Project page:\n  https:\/\/veggie-gen.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent video diffusion models have enhanced video editing, but it remains\nchallenging to handle instructional editing and diverse tasks (e.g., adding,\nremoving, changing) within a unified framework. In this paper, we introduce\nVEGGIE, a Video Editor with Grounded Generation from Instructions, a simple\nend-to-end framework that unifies video concept editing, grounding, and\nreasoning based on diverse user instructions. Specifically, given a video and\ntext query, VEGGIE first utilizes an MLLM to interpret user intentions in\ninstructions and ground them to the video contexts, generating frame-specific\ngrounded task queries for pixel-space responses. A diffusion model then renders\nthese plans and generates edited videos that align with user intent. To support\ndiverse tasks and complex instructions, we employ a curriculum learning\nstrategy: first aligning the MLLM and video diffusion model with large-scale\ninstructional image editing data, followed by end-to-end fine-tuning on\nhigh-quality multitask video data. Additionally, we introduce a novel data\nsynthesis pipeline to generate paired instructional video editing data for\nmodel training. It transforms static image data into diverse, high-quality\nvideo editing samples by leveraging Image-to-Video models to inject dynamics.\nVEGGIE shows strong performance in instructional video editing with different\nediting skills, outperforming the best instructional baseline as a versatile\nmodel, while other models struggle with multi-tasking. VEGGIE also excels in\nvideo object grounding and reasoning segmentation, where other baselines fail.\nWe further reveal how the multiple tasks help each other and highlight\npromising applications like zero-shot multimodal instructional and in-context\nvideo editing.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 15:31:12 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 20:33:40 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Yu', 'Shoubin', ''], ['Liu', 'Difan', ''], ['Ma', 'Ziqiao', ''], ['Hong', 'Yicong', ''], ['Zhou', 'Yang', ''], ['Tan', 'Hao', ''], ['Chai', 'Joyce', ''], ['Bansal', 'Mohit', '']]","extracted_entities":"[{'text': 'end-to-end fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"end-to-end fine-tuning","similarity_score":0.8521200418}
{"id":2503.14374,"submitter":"Patrick Breheny","authors":"Tabitha K. Peter and Patrick J. Breheny","title":"Cross-Validation in Penalized Linear Mixed Models: Addressing Common\n  Implementation Pitfalls","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ME","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this paper, we develop an implementation of cross-validation for penalized\nlinear mixed models. While these models have been proposed for correlated\nhigh-dimensional data, the current literature implicitly assumes that tuning\nparameter selection procedures developed for independent data will also work\nwell in this context. We argue that such naive assumptions make analysis prone\nto pitfalls, several of which we will describe. Here we present a correct\nimplementation of cross-validation for penalized linear mixed models,\naddressing these common pitfalls. We support our methods with mathematical\nproof, simulation study, and real data analysis.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:07:20 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Peter', 'Tabitha K.', ''], ['Breheny', 'Patrick J.', '']]","extracted_entities":"[{'text': 'tuning\\nparameter selection procedures', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"tuning\nparameter selection procedures","similarity_score":0.5813818574}
{"id":2503.14421,"submitter":"Radu Tudor Ionescu","authors":"Vlad Hondru, Eduard Hogea, Darian Onchis, Radu Tudor Ionescu","title":"ExDDV: A New Dataset for Explainable Deepfake Detection in Video","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL cs.LG cs.MM","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The ever growing realism and quality of generated videos makes it\nincreasingly harder for humans to spot deepfake content, who need to rely more\nand more on automatic deepfake detectors. However, deepfake detectors are also\nprone to errors, and their decisions are not explainable, leaving humans\nvulnerable to deepfake-based fraud and misinformation. To this end, we\nintroduce ExDDV, the first dataset and benchmark for Explainable Deepfake\nDetection in Video. ExDDV comprises around 5.4K real and deepfake videos that\nare manually annotated with text descriptions (to explain the artifacts) and\nclicks (to point out the artifacts). We evaluate a number of vision-language\nmodels on ExDDV, performing experiments with various fine-tuning and in-context\nlearning strategies. Our results show that text and click supervision are both\nrequired to develop robust explainable models for deepfake videos, which are\nable to localize and describe the observed artifacts. Our novel dataset and\ncode to reproduce the results are available at\nhttps:\/\/github.com\/vladhondru25\/ExDDV.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:55:07 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Hondru', 'Vlad', ''], ['Hogea', 'Eduard', ''], ['Onchis', 'Darian', ''], ['Ionescu', 'Radu Tudor', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.14481,"submitter":"Jacob Eisenstein","authors":"Jacob Eisenstein and Reza Aghajani and Adam Fisch and Dheeru Dua and\n  Fantine Huot and Mirella Lapata and Vicky Zayats and Jonathan Berant","title":"Don't lie to your friends: Learning what you know from collaborative\n  self-play","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  To be helpful assistants, AI agents must be aware of their own capabilities\nand limitations. This includes knowing when to answer from parametric knowledge\nversus using tools, when to trust tool outputs, and when to abstain or hedge.\nSuch capabilities are hard to teach through supervised fine-tuning because they\nrequire constructing examples that reflect the agent's specific capabilities.\nWe therefore propose a radically new approach to teaching agents what they\nknow: \\emph{collaborative self-play}. We construct multi-agent collaborations\nin which the group is rewarded for collectively arriving at correct answers.\nThe desired meta-knowledge emerges from the incentives built into the structure\nof the interaction. We focus on small societies of agents that have access to\nheterogeneous tools (corpus-specific retrieval), and therefore must collaborate\nto maximize their success while minimizing their effort. Experiments show that\ngroup-level rewards for multi-agent communities can induce policies that\n\\emph{transfer} to improve tool use and selective prediction in settings where\nindividual agents are deployed in isolation.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:53:20 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Eisenstein', 'Jacob', ''], ['Aghajani', 'Reza', ''], ['Fisch', 'Adam', ''], ['Dua', 'Dheeru', ''], ['Huot', 'Fantine', ''], ['Lapata', 'Mirella', ''], ['Zayats', 'Vicky', ''], ['Berant', 'Jonathan', '']]","extracted_entities":"[{'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"supervised fine-tuning","similarity_score":0.7449287176}
{"id":2503.14523,"submitter":"Xinyuan Song","authors":"Siyi Wu, Leyi Zhao, Haotian Ma, Xinyuan Song","title":"SDF-TopoNet: A Two-Stage Framework for Tubular Structure Segmentation\n  via SDF Pre-training and Topology-Aware Fine-Tuning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Accurate segmentation of tubular and curvilinear structures, such as blood\nvessels, neurons, and road networks, is crucial in various applications. A key\nchallenge is ensuring topological correctness while maintaining computational\nefficiency. Existing approaches often employ topological loss functions based\non persistent homology, such as Betti error, to enforce structural consistency.\nHowever, these methods suffer from high computational costs and are insensitive\nto pixel-level accuracy, often requiring additional loss terms like Dice or MSE\nto compensate. To address these limitations, we propose \\textbf{SDF-TopoNet},\nan improved topology-aware segmentation framework that enhances both\nsegmentation accuracy and training efficiency. Our approach introduces a novel\ntwo-stage training strategy. In the pre-training phase, we utilize the signed\ndistance function (SDF) as an auxiliary learning target, allowing the model to\nencode topological information without directly relying on computationally\nexpensive topological loss functions. In the fine-tuning phase, we incorporate\na dynamic adapter alongside a refined topological loss to ensure topological\ncorrectness while mitigating overfitting and computational overhead. We\nevaluate our method on five benchmark datasets. Experimental results\ndemonstrate that SDF-TopoNet outperforms existing methods in both topological\naccuracy and quantitative segmentation metrics, while significantly reducing\ntraining complexity.\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 23:54:38 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 01:43:59 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wu', 'Siyi', ''], ['Zhao', 'Leyi', ''], ['Ma', 'Haotian', ''], ['Song', 'Xinyuan', '']]","extracted_entities":"[{'text': 'Betti error', 'label': 'BERT'}, {'text': 'fine-tuning phase', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning phase","similarity_score":0.7284630537}
{"id":2503.14536,"submitter":"Anandakumar D","authors":"Praveen Shastry, Sowmya Chowdary Muthulur, Naveen Kumarasami,\n  Anandakumar D, Mounigasri M, Keerthana R, Kishore Prasath Venkatesh, Bargava\n  Subramanian, Kalyan Sivasailam, Revathi Ezhumalai, Abitha Marimuthu","title":"Advancing Chronic Tuberculosis Diagnostics Using Vision-Language Models:\n  A Multi modal Framework for Precision Analysis","comments":"10 pages , 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.AI cs.CV cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Background This study proposes a Vision-Language Model (VLM) leveraging the\nSIGLIP encoder and Gemma-3b transformer decoder to enhance automated chronic\ntuberculosis (TB) screening. By integrating chest X-ray images with clinical\ndata, the model addresses the challenges of manual interpretation, improving\ndiagnostic consistency and accessibility, particularly in resource-constrained\nsettings.\n  Methods The VLM architecture combines a Vision Transformer (ViT) for visual\nencoding and a transformer-based text encoder to process clinical context, such\nas patient histories and treatment records. Cross-modal attention mechanisms\nalign radiographic features with textual information, while the Gemma-3b\ndecoder generates comprehensive diagnostic reports. The model was pre-trained\non 5 million paired medical images and texts and fine-tuned using 100,000\nchronic TB-specific chest X-rays.\n  Results The model demonstrated high precision (94 percent) and recall (94\npercent) for detecting key chronic TB pathologies, including fibrosis,\ncalcified granulomas, and bronchiectasis. Area Under the Curve (AUC) scores\nexceeded 0.93, and Intersection over Union (IoU) values were above 0.91,\nvalidating its effectiveness in detecting and localizing TB-related\nabnormalities.\n  Conclusion The VLM offers a robust and scalable solution for automated\nchronic TB diagnosis, integrating radiographic and clinical data to deliver\nactionable and context-aware insights. Future work will address subtle\npathologies and dataset biases to enhance the model's generalizability,\nensuring equitable performance across diverse populations and healthcare\nsettings.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 13:49:29 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Shastry', 'Praveen', ''], ['Muthulur', 'Sowmya Chowdary', ''], ['Kumarasami', 'Naveen', ''], ['D', 'Anandakumar', ''], ['M', 'Mounigasri', ''], ['R', 'Keerthana', ''], ['Venkatesh', 'Kishore Prasath', ''], ['Subramanian', 'Bargava', ''], ['Sivasailam', 'Kalyan', ''], ['Ezhumalai', 'Revathi', ''], ['Marimuthu', 'Abitha', '']]","extracted_entities":"[{'text': 'Cross-modal attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'fine-tuned', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuned","similarity_score":0.8707774282}
{"id":2503.14637,"submitter":"Alexander Mathis","authors":"Merkourios Simos and Alberto Silvio Chiappa and Alexander Mathis","title":"Reinforcement learning-based motion imitation for physiologically\n  plausible musculoskeletal motor control","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI cs.CV cs.LG q-bio.NC","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  How do humans move? The quest to understand human motion has broad\napplications in numerous fields, ranging from computer animation and motion\nsynthesis to neuroscience, human prosthetics and rehabilitation. Although\nadvances in reinforcement learning (RL) have produced impressive results in\ncapturing human motion using simplified humanoids, controlling physiologically\naccurate models of the body remains an open challenge. In this work, we present\na model-free motion imitation framework (KINESIS) to advance the understanding\nof muscle-based motor control. Using a musculoskeletal model of the lower body\nwith 80 muscle actuators and 20 DoF, we demonstrate that KINESIS achieves\nstrong imitation performance on 1.9 hours of motion capture data, is\ncontrollable by natural language through pre-trained text-to-motion generative\nmodels, and can be fine-tuned to carry out high-level tasks such as target goal\nreaching. Importantly, KINESIS generates muscle activity patterns that\ncorrelate well with human EMG activity. The physiological plausibility makes\nKINESIS a promising model for tackling challenging problems in human motor\ncontrol theory, which we highlight by investigating Bernstein's redundancy\nproblem in the context of locomotion. Code, videos and benchmarks will be\navailable at https:\/\/github.com\/amathislab\/Kinesis.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 18:37:49 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Simos', 'Merkourios', ''], ['Chiappa', 'Alberto Silvio', ''], ['Mathis', 'Alexander', '']]","extracted_entities":"[{'text': 'pre-trained text-to-motion generative\\nmodels', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'fine-tuned', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuned","similarity_score":0.8707774282}
{"id":2503.14701,"submitter":"Podshara Chanrungmaneekul","authors":"Podshara Chanrungmaneekul, Yiting Chen, Joshua T. Grace, Aaron M.\n  Dollar, Kaiyu Hang","title":"ARC-Calib: Autonomous Markerless Camera-to-Robot Calibration via\n  Exploratory Robot Motions","comments":"8 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Camera-to-robot (also known as eye-to-hand) calibration is a critical\ncomponent of vision-based robot manipulation. Traditional marker-based methods\noften require human intervention for system setup. Furthermore, existing\nautonomous markerless calibration methods typically rely on pre-trained robot\ntracking models that impede their application on edge devices and require\nfine-tuning for novel robot embodiments. To address these limitations, this\npaper proposes a model-based markerless camera-to-robot calibration framework,\nARC-Calib, that is fully autonomous and generalizable across diverse robots and\nscenarios without requiring extensive data collection or learning. First,\nexploratory robot motions are introduced to generate easily trackable\ntrajectory-based visual patterns in the camera's image frames. Then, a\ngeometric optimization framework is proposed to exploit the coplanarity and\ncollinearity constraints from the observed motions to iteratively refine the\nestimated calibration result. Our approach eliminates the need for extra effort\nin either environmental marker setup or data collection and model training,\nrendering it highly adaptable across a wide range of real-world autonomous\nsystems. Extensive experiments are conducted in both simulation and the real\nworld to validate its robustness and generalizability.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 20:03:32 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Chanrungmaneekul', 'Podshara', ''], ['Chen', 'Yiting', ''], ['Grace', 'Joshua T.', ''], ['Dollar', 'Aaron M.', ''], ['Hang', 'Kaiyu', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.14718,"submitter":"Hakyung Sung","authors":"Hakyung Sung, Gyu-Ho Shin","title":"Second language Korean Universal Dependency treebank v1.2: Focus on data\n  augmentation and annotation scheme refinement","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  We expand the second language (L2) Korean Universal Dependencies (UD)\ntreebank with 5,454 manually annotated sentences. The annotation guidelines are\nalso revised to better align with the UD framework. Using this enhanced\ntreebank, we fine-tune three Korean language models and evaluate their\nperformance on in-domain and out-of-domain L2-Korean datasets. The results show\nthat fine-tuning significantly improves their performance across various\nmetrics, thus highlighting the importance of using well-tailored L2 datasets\nfor fine-tuning first-language-based, general-purpose language models for the\nmorphosyntactic analysis of L2 data.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 20:42:42 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Sung', 'Hakyung', ''], ['Shin', 'Gyu-Ho', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.14755,"submitter":"Omar Rakha","authors":"Omar E. Rakha, Hazem M. Abbas","title":"Language Independent Named Entity Recognition via Orthogonal\n  Transformation of Word Vectors","comments":"Paper was initially released in 2017 but was never published","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Word embeddings have been a key building block for NLP in which models relied\nheavily on word embeddings in many different tasks. In this paper, a model is\nproposed based on using Bidirectional LSTM\/CRF with word embeddings to perform\nnamed entity recognition for any language. This is done by training a model on\na source language (English) and transforming word embeddings from the target\nlanguage into word embeddings of the source language by using an orthogonal\nlinear transformation matrix. Evaluation of the model shows that by training a\nmodel on an English dataset the model was capable of detecting named entities\nin an Arabic dataset without neither training or fine tuning the model on an\nArabic language dataset.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 21:57:58 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Rakha', 'Omar E.', ''], ['Abbas', 'Hazem M.', '']]","extracted_entities":"[{'text': 'Word embeddings', 'label': 'Embedding'}, {'text': 'word embeddings', 'label': 'Embedding'}, {'text': 'word embeddings', 'label': 'Embedding'}, {'text': 'word embeddings', 'label': 'Embedding'}, {'text': 'word embeddings', 'label': 'Embedding'}, {'text': 'fine tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine tuning","similarity_score":0.9617297649}
{"id":2503.14813,"submitter":"Chunhui Zhang","authors":"Zhongyu Ouyang, Chunhui Zhang, Yaning Jia and Soroush Vosoughi","title":"Scaled Supervision is an Implicit Lipschitz Regularizer","comments":"Accepted to the International AAAI Conference on Web and Social Media\n  (ICWSM 2025)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.IR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In modern social media, recommender systems (RecSys) rely on the\nclick-through rate (CTR) as the standard metric to evaluate user engagement.\nCTR prediction is traditionally framed as a binary classification task to\npredict whether a user will interact with a given item. However, this approach\noverlooks the complexity of real-world social modeling, where the user, item,\nand their interactive features change dynamically in fast-paced online\nenvironments. This dynamic nature often leads to model instability, reflected\nin overfitting short-term fluctuations rather than higher-level interactive\npatterns. While overfitting calls for more scaled and refined supervisions,\ncurrent solutions often rely on binary labels that overly simplify fine-grained\nuser preferences through the thresholding process, which significantly reduces\nthe richness of the supervision. Therefore, we aim to alleviate the overfitting\nproblem by increasing the supervision bandwidth in CTR training. Specifically,\n(i) theoretically, we formulate the impact of fine-grained preferences on model\nstability as a Lipschitz constrain; (ii) empirically, we discover that scaling\nthe supervision bandwidth can act as an implicit Lipschitz regularizer, stably\noptimizing existing CTR models to achieve better generalizability. Extensive\nexperiments show that this scaled supervision significantly and consistently\nimproves the optimization process and the performance of existing CTR models,\neven without the need for additional hyperparameter tuning.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 01:01:28 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Ouyang', 'Zhongyu', ''], ['Zhang', 'Chunhui', ''], ['Jia', 'Yaning', ''], ['Vosoughi', 'Soroush', '']]","extracted_entities":"[{'text': 'hyperparameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"hyperparameter tuning","similarity_score":0.6193697453}
{"id":2503.14815,"submitter":"Hua Pei","authors":"Yu Wang, Hua Pei","title":"Study of event and particle selection effects on elliptic flow\n  background at the isobar experiments based on AMPT model","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"nucl-th nucl-ex","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Measurement of the Chiral Magnetic Effect (CME) has been a popular topic of\nhigh-energy nuclear physics in the last decade. The flow correlation $\\gamma$\nbetween charged hadron pairs of the same and opposite charges and their\ndifference $\\Delta \\gamma$ were measured to separate the CME-driven signal from\nthe collective flow background especially second-order elliptic $v_{2}$. The\nSTAR experiment have stepped further to the isobar experiment to compare\n$\\gamma$ and $\\Delta \\gamma$ between Ru+Ru and Zr+Zr\n~\\cite{PhysRevC.105.014901}, which were theoretically expected to produce the\nsame elliptic flow background but different CME signals. However, the measured\nflow backgrounds also differ between Ru+Ru and Zr+Zr, indicating more\nfine-tuning of RP and centrality definition necessary.\n  This analysis applied the AMPT model~\\cite{PhysRevC.72.064901} to simulate\nthe same collision system and energy as the STAR isobar experiment. Since the\nAMPT model does not include magnetic field effects, we expect comparing its\noutput between Ru+Ru and Zr+Zr collision systems can provide an insight of the\npossible bias of flow background definition, and help improve the measurement\nof CME signal in real experiments. Multiple combinations of centrality and flow\ndefinition were chosen to study how the $v_2$ and their difference would be\naffected, especially by varying the particles selection of charge versus\nneutral properties and broadening (pseudo-)rapidity regions, while STAR CME\nwork relied on charged-only particles at central rapidity.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 01:08:40 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Wang', 'Yu', ''], ['Pei', 'Hua', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'AMPT model', 'label': 'AI model'}, {'text': 'AMPT model', 'label': 'AI model'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.14836,"submitter":"Kunyang Li","authors":"Kunyang Li, Jean-Charles Noirot Ferrand, Ryan Sheatsley, Blaine Hoak,\n  Yohan Beugin, Eric Pauley, Patrick McDaniel","title":"On the Robustness Tradeoff in Fine-Tuning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Fine-tuning has become the standard practice for adapting pre-trained\n(upstream) models to downstream tasks. However, the impact on model robustness\nis not well understood. In this work, we characterize the robustness-accuracy\ntrade-off in fine-tuning. We evaluate the robustness and accuracy of fine-tuned\nmodels over 6 benchmark datasets and 7 different fine-tuning strategies. We\nobserve a consistent trade-off between adversarial robustness and accuracy.\nPeripheral updates such as BitFit are more effective for simple tasks--over 75%\nabove the average measured with area under the Pareto frontiers on CIFAR-10 and\nCIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention\nlayers via Compacter, achieves a better Pareto frontier on more complex\ntasks--57.5% and 34.6% above the average on Caltech-256 and CUB-200,\nrespectively. Lastly, we observe that robustness of fine-tuning against\nout-of-distribution data closely tracks accuracy. These insights emphasize the\nneed for robustness-aware fine-tuning to ensure reliable real-world\ndeployments.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 02:35:01 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Li', 'Kunyang', ''], ['Ferrand', 'Jean-Charles Noirot', ''], ['Sheatsley', 'Ryan', ''], ['Hoak', 'Blaine', ''], ['Beugin', 'Yohan', ''], ['Pauley', 'Eric', ''], ['McDaniel', 'Patrick', '']]","extracted_entities":"[{'text': 'Fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'attention\\nlayers', 'label': 'Attention mechanism'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"Fine-tuning","similarity_score":1.0000001192}
{"id":2503.14849,"submitter":"Zhuoyi Yang","authors":"Zhuoyi Yang and Ian G. Harris","title":"LogLLaMA: Transformer-based log anomaly detection with LLaMA","comments":"8 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Log anomaly detection refers to the task that distinguishes the anomalous log\nmessages from normal log messages. Transformer-based large language models\n(LLMs) are becoming popular for log anomaly detection because of their superb\nability to understand complex and long language patterns. In this paper, we\npropose LogLLaMA, a novel framework that leverages LLaMA2. LogLLaMA is first\nfinetuned on normal log messages from three large-scale datasets to learn their\npatterns. After finetuning, the model is capable of generating successive log\nmessages given previous log messages. Our generative model is further trained\nto identify anomalous log messages using reinforcement learning (RL). The\nexperimental results show that LogLLaMA outperforms the state-of-the-art\napproaches for anomaly detection on BGL, Thunderbird, and HDFS datasets.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 03:13:37 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Yang', 'Zhuoyi', ''], ['Harris', 'Ian G.', '']]","extracted_entities":"[{'text': 'finetuned', 'label': 'Fine-tuning'}, {'text': 'finetuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"finetuning","similarity_score":0.5753726363}
{"id":2503.14878,"submitter":"Murtaza Zohair","authors":"Murtaza Zohair, Vidushi Sharma, Eduardo A. Soares, Khanh Nguyen,\n  Maxwell Giammona, Linda Sundberg, Andy Tek, Emilio A. V. Vital, Young-Hye La","title":"Chemical Foundation Model Guided Design of High Ionic Conductivity\n  Electrolyte Formulations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci physics.chem-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Designing optimal formulations is a major challenge in developing\nelectrolytes for the next generation of rechargeable batteries due to the vast\ncombinatorial design space and complex interplay between multiple constituents.\nMachine learning (ML) offers a powerful tool to uncover underlying chemical\ndesign rules and accelerate the process of formulation discovery. In this work,\nwe present an approach to design new formulations that can achieve target\nperformance, using a generalizable chemical foundation model. The chemical\nfoundation model is fine-tuned on an experimental dataset of 13,666 ionic\nconductivity values curated from the lithium-ion battery literature. The\nfine-tuned model is used to discover 7 novel high conductivity electrolyte\nformulations through generative screening, improving the conductivity of LiFSI\nand LiDFOB based electrolytes by 82% and 172%, respectively. These findings\nhighlight a generalizable workflow that is highly adaptable to the discovery of\nchemical mixtures with tailored properties to address challenges in energy\nstorage and beyond.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 04:14:19 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zohair', 'Murtaza', ''], ['Sharma', 'Vidushi', ''], ['Soares', 'Eduardo A.', ''], ['Nguyen', 'Khanh', ''], ['Giammona', 'Maxwell', ''], ['Sundberg', 'Linda', ''], ['Tek', 'Andy', ''], ['Vital', 'Emilio A. V.', ''], ['La', 'Young-Hye', '']]","extracted_entities":"[{'text': 'chemical foundation model', 'label': 'Foundation Model'}, {'text': 'chemical\\nfoundation model', 'label': 'Foundation Model'}, {'text': 'fine-tuned', 'label': 'Fine-tuning'}, {'text': 'generative screening', 'label': 'Few-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuned","similarity_score":0.8707774282}
{"id":2503.14897,"submitter":"Vaibhav Rathore","authors":"Vaibhav Rathore, Shubhranil B, Saikat Dutta, Sarthak Mehrotra, Zsolt\n  Kira, Biplab Banerjee","title":"When Domain Generalization meets Generalized Category Discovery: An\n  Adaptive Task-Arithmetic Driven Approach","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Generalized Class Discovery (GCD) clusters base and novel classes in a target\ndomain using supervision from a source domain with only base classes. Current\nmethods often falter with distribution shifts and typically require access to\ntarget data during training, which can sometimes be impractical. To address\nthis issue, we introduce the novel paradigm of Domain Generalization in GCD\n(DG-GCD), where only source data is available for training, while the target\ndomain, with a distinct data distribution, remains unseen until inference. To\nthis end, our solution, DG2CD-Net, aims to construct a domain-independent,\ndiscriminative embedding space for GCD. The core innovation is an episodic\ntraining strategy that enhances cross-domain generalization by adapting a base\nmodel on tasks derived from source and synthetic domains generated by a\nfoundation model. Each episode focuses on a cross-domain GCD task, diversifying\ntask setups over episodes and combining open-set domain adaptation with a novel\nmargin loss and representation learning for optimizing the feature space\nprogressively. To capture the effects of fine-tuning on the base model, we\nextend task arithmetic by adaptively weighting the local task vectors\nconcerning the fine-tuned models based on their GCD performance on a validation\ndistribution. This episodic update mechanism boosts the adaptability of the\nbase model to unseen targets. Experiments across three datasets confirm that\nDG2CD-Net outperforms existing GCD methods customized for DG-GCD.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 04:48:16 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Rathore', 'Vaibhav', ''], ['B', 'Shubhranil', ''], ['Dutta', 'Saikat', ''], ['Mehrotra', 'Sarthak', ''], ['Kira', 'Zsolt', ''], ['Banerjee', 'Biplab', '']]","extracted_entities":"[{'text': 'DG2CD-Net', 'label': 'Foundation Model'}, {'text': 'foundation model', 'label': 'Foundation Model'}, {'text': 'representation learning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'base model', 'label': 'Foundation Model'}, {'text': 'base model', 'label': 'Foundation Model'}, {'text': 'DG2CD-Net', 'label': 'Foundation Model'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.14953,"submitter":"Yang Liu","authors":"Yang Liu, Wentao Feng, Zhuoyao Liu, Shudong Huang, Jiancheng Lv","title":"Aligning Information Capacity Between Vision and Language via\n  Dense-to-Sparse Feature Distillation for Image-Text Matching","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Enabling Visual Semantic Models to effectively handle multi-view description\nmatching has been a longstanding challenge. Existing methods typically learn a\nset of embeddings to find the optimal match for each view's text and compute\nsimilarity. However, the visual and text embeddings learned through these\napproaches have limited information capacity and are prone to interference from\nlocally similar negative samples. To address this issue, we argue that the\ninformation capacity of embeddings is crucial and propose Dense-to-Sparse\nFeature Distilled Visual Semantic Embedding (D2S-VSE), which enhances the\ninformation capacity of sparse text by leveraging dense text distillation.\nSpecifically, D2S-VSE is a two-stage framework. In the pre-training stage, we\nalign images with dense text to enhance the information capacity of visual\nsemantic embeddings. In the fine-tuning stage, we optimize two tasks\nsimultaneously, distilling dense text embeddings to sparse text embeddings\nwhile aligning images and sparse texts, enhancing the information capacity of\nsparse text embeddings. Our proposed D2S-VSE model is extensively evaluated on\nthe large-scale MS-COCO and Flickr30K datasets, demonstrating its superiority\nover recent state-of-the-art methods.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 07:42:24 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Liu', 'Yang', ''], ['Feng', 'Wentao', ''], ['Liu', 'Zhuoyao', ''], ['Huang', 'Shudong', ''], ['Lv', 'Jiancheng', '']]","extracted_entities":"[{'text': 'Visual Semantic Embedding', 'label': 'Embedding'}, {'text': 'D2S-VSE', 'label': 'Embedding'}, {'text': 'dense text distillation', 'label': 'Knowledge distillation'}, {'text': 'visual\\nsemantic embeddings', 'label': 'Embedding'}, {'text': 'fine-tuning stage', 'label': 'Fine-tuning'}, {'text': 'sparse text embeddings', 'label': 'Embedding'}, {'text': 'sparse text embeddings', 'label': 'Embedding'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning stage","similarity_score":0.7817779779}
{"id":2503.14998,"submitter":"Marta Hasny","authors":"Marta Hasny, Maxime Di Folco, Keno Bressem, Julia Schnabel","title":"TGV: Tabular Data-Guided Learning of Visual Cardiac Representations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Contrastive learning methods in computer vision typically rely on different\nviews of the same image to form pairs. However, in medical imaging, we often\nseek to compare entire patients with different phenotypes rather than just\nmultiple augmentations of one scan. We propose harnessing clinically relevant\ntabular data to identify distinct patient phenotypes and form more meaningful\npairs in a contrastive learning framework. Our method uses tabular attributes\nto guide the training of visual representations, without requiring a joint\nembedding space. We demonstrate its strength using short-axis cardiac MR images\nand clinical attributes from the UK Biobank, where tabular data helps to more\neffectively distinguish between patient subgroups. Evaluation on downstream\ntasks, including fine-tuning and zero-shot prediction of cardiovascular artery\ndiseases and cardiac phenotypes, shows that incorporating tabular data yields\nstronger visual representations than conventional methods that rely solely on\nimage augmentations or combined image-tabular embeddings. Furthermore, we\ndemonstrate that image encoders trained with tabular guidance are capable of\nembedding demographic information in their representations, allowing them to\nuse insights from tabular data for unimodal predictions, making them\nwell-suited to real-world medical settings where extensive clinical annotations\nmay not be routinely available at inference time. The code will be available on\nGitHub.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 08:49:55 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Hasny', 'Marta', ''], ['Di Folco', 'Maxime', ''], ['Bressem', 'Keno', ''], ['Schnabel', 'Julia', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'zero-shot prediction', 'label': 'Zero-shot Learning'}, {'text': 'combined image-tabular embeddings', 'label': 'Embedding'}, {'text': 'GitHub', 'label': 'Open-source LLMs'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.15027,"submitter":"Takuma Akimoto","authors":"Soma Shiraki, Eli Barkai, Takuma Akimoto","title":"Tunable Anomalous Diffusion in Subrecoil-Laser-Cooled Atoms","comments":"9 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.stat-mech","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The control of atomic motion through laser cooling has revolutionized quantum\ntechnologies, enabling applications ranging from quantum computing to precision\nmetrology. However, the spatial spreading of subrecoil-laser-cooled atoms --\ncrucial for understanding cooling mechanisms and atomic confinement -- remains\nlargely unexplored. Here, we analyze anomalous diffusion in\nsubrecoil-laser-cooled atoms, where a velocity-dependent fluorescence rate\n$R(v) \\propto |v|^{\\alpha}$ governs transport properties. By tuning $\\alpha$,\nwe uncover transitions between normal, subdiffusive, and superdiffusive\nregimes. Notably, at $\\alpha = 3\/2$, diffusion is minimized, leading to optimal\natomic confinement. These findings advance the understanding of anomalous\ntransport in laser-cooled systems and offer new avenues for precise control of\natomic motion.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 09:22:47 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Shiraki', 'Soma', ''], ['Barkai', 'Eli', ''], ['Akimoto', 'Takuma', '']]","extracted_entities":"[{'text': 'tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"tuning","similarity_score":0.8449009061}
{"id":2503.15197,"submitter":"Feifei Li","authors":"Feifei Li, Mi Zhang, Yiming Sun and Min Yang","title":"Detect-and-Guide: Self-regulation of Diffusion Models for Safe\n  Text-to-Image Generation via Guideline Token Optimization","comments":"CVPR25","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Text-to-image diffusion models have achieved state-of-the-art results in\nsynthesis tasks; however, there is a growing concern about their potential\nmisuse in creating harmful content. To mitigate these risks, post-hoc model\nintervention techniques, such as concept unlearning and safety guidance, have\nbeen developed. However, fine-tuning model weights or adapting the hidden\nstates of the diffusion model operates in an uninterpretable way, making it\nunclear which part of the intermediate variables is responsible for unsafe\ngeneration. These interventions severely affect the sampling trajectory when\nerasing harmful concepts from complex, multi-concept prompts, thus hindering\ntheir practical use in real-world settings. In this work, we propose the safe\ngeneration framework Detect-and-Guide (DAG), leveraging the internal knowledge\nof diffusion models to perform self-diagnosis and fine-grained self-regulation\nduring the sampling process. DAG first detects harmful concepts from noisy\nlatents using refined cross-attention maps of optimized tokens, then applies\nsafety guidance with adaptive strength and editing regions to negate unsafe\ngeneration. The optimization only requires a small annotated dataset and can\nprovide precise detection maps with generalizability and concept specificity.\nMoreover, DAG does not require fine-tuning of diffusion models, and therefore\nintroduces no loss to their generation diversity. Experiments on erasing sexual\ncontent show that DAG achieves state-of-the-art safe generation performance,\nbalancing harmfulness mitigation and text-following performance on\nmulti-concept real-world prompts.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 13:37:52 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Li', 'Feifei', ''], ['Zhang', 'Mi', ''], ['Sun', 'Yiming', ''], ['Yang', 'Min', '']]","extracted_entities":"[{'text': 'concept unlearning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuning model weights', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning model weights","similarity_score":0.6723128557}
{"id":2503.15212,"submitter":"Sarah Matta","authors":"Lucie Berger, Mathieu Lamard, Philippe Zhang, Laurent Borderie,\n  Alexandre Le Guilcher, Pascale Massin, B\\'eatrice Cochener, Gwenol\\'e Quellec\n  and Sarah Matta","title":"Context-Aware Vision Language Foundation Models for Ocular Disease\n  Screening in Retinal Images","comments":"4 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Foundation models are large-scale versatile systems trained on vast\nquantities of diverse data to learn generalizable representations. Their\nadaptability with minimal fine-tuning makes them particularly promising for\nmedical imaging, where data variability and domain shifts are major challenges.\nCurrently, two types of foundation models dominate the literature:\nself-supervised models and more recent vision-language models. In this study,\nwe advance the application of vision-language foundation (VLF) models for\nocular disease screening using the OPHDIAT dataset, which includes nearly\n700,000 fundus photographs from a French diabetic retinopathy (DR) screening\nnetwork. This dataset provides extensive clinical data (patient-specific\ninformation such as diabetic health conditions, and treatments), labeled\ndiagnostics, ophthalmologists text-based findings, and multiple retinal images\nfor each examination. Building on the FLAIR model $\\unicode{x2013}$ a VLF model\nfor retinal pathology classification $\\unicode{x2013}$ we propose novel\ncontext-aware VLF models (e.g jointly analyzing multiple images from the same\nvisit or taking advantage of past diagnoses and contextual data) to fully\nleverage the richness of the OPHDIAT dataset and enhance robustness to domain\nshifts. Our approaches were evaluated on both in-domain (a testing subset of\nOPHDIAT) and out-of-domain data (public datasets) to assess their\ngeneralization performance. Our model demonstrated improved in-domain\nperformance for DR grading, achieving an area under the curve (AUC) ranging\nfrom 0.851 to 0.9999, and generalized well to ocular disease detection on\nout-of-domain data (AUC: 0.631-0.913).\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 13:52:33 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Berger', 'Lucie', ''], ['Lamard', 'Mathieu', ''], ['Zhang', 'Philippe', ''], ['Borderie', 'Laurent', ''], ['Guilcher', 'Alexandre Le', ''], ['Massin', 'Pascale', ''], ['Cochener', 'B\u00e9atrice', ''], ['Quellec', 'Gwenol\u00e9', ''], ['Matta', 'Sarah', '']]","extracted_entities":"[{'text': 'minimal fine-tuning', 'label': 'Fine-tuning'}, {'text': 'self-supervised models', 'label': 'Foundation Model'}, {'text': 'vision-language models', 'label': 'Foundation Model'}, {'text': 'FLAIR model', 'label': 'Foundation Model'}]","assigned_concept":"Fine-tuning","matched_keyword":"minimal fine-tuning","similarity_score":0.8483207226}
{"id":2503.15221,"submitter":"Josu\\'e P\\'erez Sabater","authors":"Rodrigo Oliver, Josu\\'e P\\'erez-Sabater, Leire Paz-Arbaizar, Alejandro\n  Lancho, Antonio Art\\'es, Pablo M. Olmos","title":"A Foundation Model for Patient Behavior Monitoring and Suicide Detection","comments":"10 pages (31 with appendices), 6 figures (13 with appendices);\n  submitted to UAI 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Foundation models (FMs) have achieved remarkable success across various\ndomains, yet their adoption in healthcare remains limited. While significant\nadvances have been made in medical imaging, genetic biomarkers, and time series\nfrom electronic health records, the potential of FMs for patient behavior\nmonitoring through wearable devices remains underexplored. These datasets are\ninherently heterogeneous, multisource, and often exhibit high rates of missing\ndata, posing unique challenges. This paper introduces a novel FM based on a\nmodified vector quantized variational autoencoder (VQ-VAE), specifically\ndesigned to process real-world data from wearable devices. We demonstrate that\nour pretrained FM, trained on a broad cohort of psychiatric patients, performs\ndownstream tasks via its latent representation without fine-tuning on a\nheld-out cohort of suicidal patients. To illustrate this, we develop a\nprobabilistic change-point detection algorithm for suicide detection and\ndemonstrate the FM's effectiveness in predicting emotional states. Our results\nshow that the discrete latent structure of the VQ-VAE outperforms a\nstate-of-the-art Informer architecture in unsupervised suicide detection, while\nmatching its performance in supervised emotion prediction when the latent\ndimensionality is increased, though at the cost of reduced unsupervised\naccuracy. This trade-off highlights the need for future FMs to integrate hybrid\ndiscrete-continuous structures for balanced performance across tasks.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 14:01:16 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Oliver', 'Rodrigo', ''], ['P\u00e9rez-Sabater', 'Josu\u00e9', ''], ['Paz-Arbaizar', 'Leire', ''], ['Lancho', 'Alejandro', ''], ['Art\u00e9s', 'Antonio', ''], ['Olmos', 'Pablo M.', '']]","extracted_entities":"[{'text': 'FMs', 'label': 'Foundation Model'}, {'text': 'FMs', 'label': 'Foundation Model'}, {'text': 'VQ-VAE', 'label': 'Foundation Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'VQ-VAE', 'label': 'Foundation Model'}, {'text': 'FMs', 'label': 'Foundation Model'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.1525,"submitter":"Quentin Nater","authors":"Quentin Nater, Mourad Khayati, Jacques Pasquier","title":"ImputeGAP: A Comprehensive Library for Time Series Imputation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.DB","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  With the prevalence of sensor failures, imputation--the process of estimating\nmissing values--has emerged as the cornerstone of time series data preparation.\nWhile numerous imputation algorithms have been developed to address these data\ngaps, existing libraries provide limited support. Furthermore, they often lack\nthe ability to simulate realistic patterns of time series missing data and fail\nto account for the impact of imputation on subsequent downstream analysis.\n  This paper introduces ImputeGAP, a comprehensive library for time series\nimputation that supports a diverse range of imputation methods and modular\nmissing data simulation catering to datasets with varying characteristics. The\nlibrary includes extensive customization options, such as automated\nhyperparameter tuning, benchmarking, explainability, downstream evaluation, and\ncompatibility with popular time series frameworks.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 14:24:20 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Nater', 'Quentin', ''], ['Khayati', 'Mourad', ''], ['Pasquier', 'Jacques', '']]","extracted_entities":"[{'text': 'automated\\nhyperparameter tuning', 'label': 'Fine-tuning'}, {'text': 'benchmarking', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"automated\nhyperparameter tuning","similarity_score":0.5582404137}
{"id":2503.1531,"submitter":"Mike Walmsley","authors":"Euclid Collaboration: M. Walmsley, M. Huertas-Company, L. Quilley, K.\n  L. Masters, S. Kruk, K. A. Remmelgas, J. J. Popp, E. Romelli, D. O'Ryan, H.\n  J. Dickinson, C. J. Lintott, S. Serjeant, R. J. Smethurst, B. Simmons, J.\n  Shingirai Makechemu, I. L. Garland, H. Roberts, K. Mantha, L. F. Fortson, T.\n  G\\'eron, W. Keel, E. M. Baeten, C. Macmillan, J. Bovy, S. Casas, C. De Leo,\n  H. Dom\\'inguez S\\'anchez, J. Katona, A. Kov\\'acs, N. Aghanim, B. Altieri, A.\n  Amara, S. Andreon, N. Auricchio, H. Aussel, C. Baccigalupi, M. Baldi, A.\n  Balestra, S. Bardelli, A. Basset, P. Battaglia, R. Bender, A. Biviano, A.\n  Bonchi, E. Branchini, M. Brescia, J. Brinchmann, S. Camera, G.\n  Ca\\~nas-Herrera, V. Capobianco, C. Carbone, J. Carretero, F. J. Castander, M.\n  Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C.\n  Colodro-Conde, G. Congedo, C. J. Conselice, L. Conversi, Y. Copin, F.\n  Courbin, H. M. Courtois, M. Cropper, A. Da Silva, H. Degaudenzi, G. De Lucia,\n  A. M. Di Giorgio, C. Dolding, H. Dole, F. Dubath, C. A. J. Duncan, X. Dupac,\n  S. Dusini, A. Ealet, S. Escoffier, M. Fabricius, M. Farina, R. Farinelli, F.\n  Faustini, F. Finelli, P. Fosalba, S. Fotopoulou, M. Frailis, E. Franceschi,\n  S. Galeotta, K. George, B. Gillis, C. Giocoli, P. G\\'omez-Alvarez, J.\n  Gracia-Carpio, B. R. Granett, A. Grazian, F. Grupp, S. Gwyn, S. V. H. Haugan,\n  H. Hoekstra, W. Holmes, I. M. Hook, F. Hormuth, A. Hornstrup, P. Hudelot, K.\n  Jahnke, M. Jhabvala, B. Joachimi, E. Keih\\\"anen, S. Kermiche, A. Kiessling,\n  R. Kohley, B. Kubik, K. Kuijken, M. K\\\"ummel, M. Kunz, H. Kurki-Suonio, O.\n  Lahav, Q. Le Boulc'h, A. M. C. Le Brun, D. Le Mignant, P. Liebing, S. Ligori,\n  P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, E. Maiorano, O.\n  Mansutti, S. Marcin, O. Marggraf, M. Martinelli, N. Martinet, F. Marulli, R.\n  Massey, S. Maurogordato, H. J. McCracken, E. Medinaceli, S. Mei, M. Melchior,\n  Y. Mellier, M. Meneghetti, E. Merlin, G. Meylan, A. Mora, M. Moresco, L.\n  Moscardini, R. Nakajima, C. Neissner, R. C. Nichol, S.-M. Niemi, J. W.\n  Nightingale, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, W. J. Percival,\n  V. Pettorino, S. Pires, G. Polenta, M. Poncet, L. A. Popa, L. Pozzetti, F.\n  Raison, R. Rebolo, A. Renzi, J. Rhodes, G. Riccio, M. Roncarelli, B.\n  Rusholme, R. Saglia, Z. Sakr, A. G. S\\'anchez, D. Sapone, B. Sartoris, J. A.\n  Schewtschenko, P. Schneider, T. Schrabback, M. Scodeggio, A. Secroun, G.\n  Seidel, M. Seiffert, S. Serrano, P. Simon, C. Sirignano, G. Sirri, L. Stanco,\n  J. Steinwagner, P. Tallada-Cresp\\'i, D. Tavagnacco, A. N. Taylor, H. I.\n  Teplitz, I. Tereno, N. Tessore, S. Toft, R. Toledo-Moreo, F. Torradeflot, I.\n  Tutusaus, E. A. Valentijn, L. Valenziano, J. Valiviita, T. Vassallo, G.\n  Verdoes Kleijn, A. Veropalumbo, Y. Wang, J. Weller, A. Zacchei, G. Zamorani,\n  F. M. Zerbi, I. A. Zinchenko, E. Zucca, V. Allevato, M. Ballardini, M.\n  Bolzonella, E. Bozzo, C. Burigana, R. Cabanac, A. Cappi, D. Di Ferdinando, J.\n  A. Escartin Vigo, L. Gabarra, J. Mart\\'in-Fleitas, S. Matthew, N. Mauri, R.\n  B. Metcalf, A. Pezzotta, M. P\\\"ontinen, C. Porciani, I. Risso, V. Scottez, M.\n  Sereno, M. Tenti, M. Viel, M. Wiesmann, Y. Akrami, I. T. Andika, S. Anselmi,\n  M. Archidiacono, F. Atrio-Barandela, C. Benoist, K. Benson, D. Bertacca, M.\n  Bethermin, L. Bisigello, A. Blanchard, L. Blot, H. B\\\"ohringer, M. L. Brown,\n  S. Bruton, F. Buitrago, A. Calabro, B. Camacho Quevedo, F. Caro, C. S.\n  Carvalho, T. Castro, F. Cogato, A. R. Cooray, O. Cucciati, S. Davini, F. De\n  Paolis, G. Desprez, A. D\\'iaz-S\\'anchez, J. J. Diaz, S. Di Domizio, J. M.\n  Diego, P.-A. Duc, A. Enia, Y. Fang, A. G. Ferrari, A. Finoguenov, A. Fontana,\n  A. Franco, K. Ganga, J. Garc\\'ia-Bellido, T. Gasparetto, V. Gautard, E.\n  Gaztanaga, F. Giacomini, G. Gozaliasl, M. Guidi, C. M. Gutierrez, A. Hall, W.\n  G. Hartley, S. Hemmati, C. Hern\\'andez-Monteagudo, H. Hildebrandt, J. Hjorth,\n  J. J. E. Kajava, Y. Kang, V. Kansal, D. Karagiannis, K. Kiiveri, C. C.\n  Kirkpatrick, J. Le Graet, L. Legrand, M. Lembo, F. Lepori, G. Leroy, G. F.\n  Lesci, J. Lesgourgues, L. Leuzzi, T. I. Liaudat, A. Loureiro, J.\n  Macias-Perez, G. Maggio, M. Magliocchetti, F. Mannucci, R. Maoli, C. J. A. P.\n  Martins, L. Maurin, M. Miluzio, P. Monaco, C. Moretti, G. Morgante, C.\n  Murray, S. Nadathur, K. Naidoo, A. Navarro-Alsina, S. Nesseris, F.\n  Passalacqua, K. Paterson, L. Patrizii, A. Pisani, D. Potter, S. Quai, M.\n  Radovich, P.-F. Rocci, G. Rodighiero, S. Sacquegna, M. Sahl\\'en, D. B.\n  Sanders, E. Sarpa, C. Scarlata, J. Schaye, A. Schneider, M. Schultheis, D.\n  Sciotti, E. Sellentin, F. Shankar, L. C. Smith, K. Tanidis, G. Testera, R.\n  Teyssier, S. Tosi, A. Troja, M. Tucci, C. Valieri, A. Venhola, D. Vergani, G.\n  Verza, P. Vielzeuf, N. A. Walton, E. Soubrie, D. Scott","title":"Euclid Quick Data Release (Q1): First visual morphology catalogue","comments":"Data: https:\/\/doi.org\/10.5281\/zenodo.15002907. Paper submitted as\n  part of the A&A Special Issue `Euclid Quick Data Release (Q1)'. 16 pages, 15\n  figures, plus appendices","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.GA","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present a detailed visual morphology catalogue for Euclid's Quick Release\n1 (Q1). Our catalogue includes galaxy features such as bars, spiral arms, and\nongoing mergers, for the 378000 bright ($I_E < 20.5$) or extended (area $\\geq\n700\\,$pixels) galaxies in Q1. The catalogue was created by finetuning the\nZoobot galaxy foundation models on annotations from an intensive one month\ncampaign by Galaxy Zoo volunteers. Our measurements are fully automated and\nhence fully scaleable. This catalogue is the first 0.4% of the approximately\n100 million galaxies where Euclid will ultimately resolve detailed morphology.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:27:05 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Euclid Collaboration', '', ''], ['Walmsley', 'M.', ''], ['Huertas-Company', 'M.', ''], ['Quilley', 'L.', ''], ['Masters', 'K. L.', ''], ['Kruk', 'S.', ''], ['Remmelgas', 'K. A.', ''], ['Popp', 'J. J.', ''], ['Romelli', 'E.', ''], [\"O'Ryan\", 'D.', ''], ['Dickinson', 'H. J.', ''], ['Lintott', 'C. J.', ''], ['Serjeant', 'S.', ''], ['Smethurst', 'R. J.', ''], ['Simmons', 'B.', ''], ['Makechemu', 'J. Shingirai', ''], ['Garland', 'I. L.', ''], ['Roberts', 'H.', ''], ['Mantha', 'K.', ''], ['Fortson', 'L. F.', ''], ['G\u00e9ron', 'T.', ''], ['Keel', 'W.', ''], ['Baeten', 'E. M.', ''], ['Macmillan', 'C.', ''], ['Bovy', 'J.', ''], ['Casas', 'S.', ''], ['De Leo', 'C.', ''], ['S\u00e1nchez', 'H. Dom\u00ednguez', ''], ['Katona', 'J.', ''], ['Kov\u00e1cs', 'A.', ''], ['Aghanim', 'N.', ''], ['Altieri', 'B.', ''], ['Amara', 'A.', ''], ['Andreon', 'S.', ''], ['Auricchio', 'N.', ''], ['Aussel', 'H.', ''], ['Baccigalupi', 'C.', ''], ['Baldi', 'M.', ''], ['Balestra', 'A.', ''], ['Bardelli', 'S.', ''], ['Basset', 'A.', ''], ['Battaglia', 'P.', ''], ['Bender', 'R.', ''], ['Biviano', 'A.', ''], ['Bonchi', 'A.', ''], ['Branchini', 'E.', ''], ['Brescia', 'M.', ''], ['Brinchmann', 'J.', ''], ['Camera', 'S.', ''], ['Ca\u00f1as-Herrera', 'G.', ''], ['Capobianco', 'V.', ''], ['Carbone', 'C.', ''], ['Carretero', 'J.', ''], ['Castander', 'F. J.', ''], ['Castellano', 'M.', ''], ['Castignani', 'G.', ''], ['Cavuoti', 'S.', ''], ['Chambers', 'K. C.', ''], ['Cimatti', 'A.', ''], ['Colodro-Conde', 'C.', ''], ['Congedo', 'G.', ''], ['Conselice', 'C. J.', ''], ['Conversi', 'L.', ''], ['Copin', 'Y.', ''], ['Courbin', 'F.', ''], ['Courtois', 'H. M.', ''], ['Cropper', 'M.', ''], ['Da Silva', 'A.', ''], ['Degaudenzi', 'H.', ''], ['De Lucia', 'G.', ''], ['Di Giorgio', 'A. M.', ''], ['Dolding', 'C.', ''], ['Dole', 'H.', ''], ['Dubath', 'F.', ''], ['Duncan', 'C. A. J.', ''], ['Dupac', 'X.', ''], ['Dusini', 'S.', ''], ['Ealet', 'A.', ''], ['Escoffier', 'S.', ''], ['Fabricius', 'M.', ''], ['Farina', 'M.', ''], ['Farinelli', 'R.', ''], ['Faustini', 'F.', ''], ['Finelli', 'F.', ''], ['Fosalba', 'P.', ''], ['Fotopoulou', 'S.', ''], ['Frailis', 'M.', ''], ['Franceschi', 'E.', ''], ['Galeotta', 'S.', ''], ['George', 'K.', ''], ['Gillis', 'B.', ''], ['Giocoli', 'C.', ''], ['G\u00f3mez-Alvarez', 'P.', ''], ['Gracia-Carpio', 'J.', ''], ['Granett', 'B. R.', ''], ['Grazian', 'A.', ''], ['Grupp', 'F.', ''], ['Gwyn', 'S.', ''], ['Haugan', 'S. V. H.', ''], ['Hoekstra', 'H.', ''], ['Holmes', 'W.', ''], ['Hook', 'I. M.', ''], ['Hormuth', 'F.', ''], ['Hornstrup', 'A.', ''], ['Hudelot', 'P.', ''], ['Jahnke', 'K.', ''], ['Jhabvala', 'M.', ''], ['Joachimi', 'B.', ''], ['Keih\u00e4nen', 'E.', ''], ['Kermiche', 'S.', ''], ['Kiessling', 'A.', ''], ['Kohley', 'R.', ''], ['Kubik', 'B.', ''], ['Kuijken', 'K.', ''], ['K\u00fcmmel', 'M.', ''], ['Kunz', 'M.', ''], ['Kurki-Suonio', 'H.', ''], ['Lahav', 'O.', ''], [\"Boulc'h\", 'Q. Le', ''], ['Brun', 'A. M. C. Le', ''], ['Mignant', 'D. Le', ''], ['Liebing', 'P.', ''], ['Ligori', 'S.', ''], ['Lilje', 'P. B.', ''], ['Lindholm', 'V.', ''], ['Lloro', 'I.', ''], ['Mainetti', 'G.', ''], ['Maino', 'D.', ''], ['Maiorano', 'E.', ''], ['Mansutti', 'O.', ''], ['Marcin', 'S.', ''], ['Marggraf', 'O.', ''], ['Martinelli', 'M.', ''], ['Martinet', 'N.', ''], ['Marulli', 'F.', ''], ['Massey', 'R.', ''], ['Maurogordato', 'S.', ''], ['McCracken', 'H. J.', ''], ['Medinaceli', 'E.', ''], ['Mei', 'S.', ''], ['Melchior', 'M.', ''], ['Mellier', 'Y.', ''], ['Meneghetti', 'M.', ''], ['Merlin', 'E.', ''], ['Meylan', 'G.', ''], ['Mora', 'A.', ''], ['Moresco', 'M.', ''], ['Moscardini', 'L.', ''], ['Nakajima', 'R.', ''], ['Neissner', 'C.', ''], ['Nichol', 'R. C.', ''], ['Niemi', 'S. -M.', ''], ['Nightingale', 'J. W.', ''], ['Padilla', 'C.', ''], ['Paltani', 'S.', ''], ['Pasian', 'F.', ''], ['Pedersen', 'K.', ''], ['Percival', 'W. J.', ''], ['Pettorino', 'V.', ''], ['Pires', 'S.', ''], ['Polenta', 'G.', ''], ['Poncet', 'M.', ''], ['Popa', 'L. A.', ''], ['Pozzetti', 'L.', ''], ['Raison', 'F.', ''], ['Rebolo', 'R.', ''], ['Renzi', 'A.', ''], ['Rhodes', 'J.', ''], ['Riccio', 'G.', ''], ['Roncarelli', 'M.', ''], ['Rusholme', 'B.', ''], ['Saglia', 'R.', ''], ['Sakr', 'Z.', ''], ['S\u00e1nchez', 'A. G.', ''], ['Sapone', 'D.', ''], ['Sartoris', 'B.', ''], ['Schewtschenko', 'J. A.', ''], ['Schneider', 'P.', ''], ['Schrabback', 'T.', ''], ['Scodeggio', 'M.', ''], ['Secroun', 'A.', ''], ['Seidel', 'G.', ''], ['Seiffert', 'M.', ''], ['Serrano', 'S.', ''], ['Simon', 'P.', ''], ['Sirignano', 'C.', ''], ['Sirri', 'G.', ''], ['Stanco', 'L.', ''], ['Steinwagner', 'J.', ''], ['Tallada-Cresp\u00ed', 'P.', ''], ['Tavagnacco', 'D.', ''], ['Taylor', 'A. N.', ''], ['Teplitz', 'H. I.', ''], ['Tereno', 'I.', ''], ['Tessore', 'N.', ''], ['Toft', 'S.', ''], ['Toledo-Moreo', 'R.', ''], ['Torradeflot', 'F.', ''], ['Tutusaus', 'I.', ''], ['Valentijn', 'E. A.', ''], ['Valenziano', 'L.', ''], ['Valiviita', 'J.', ''], ['Vassallo', 'T.', ''], ['Kleijn', 'G. Verdoes', ''], ['Veropalumbo', 'A.', ''], ['Wang', 'Y.', ''], ['Weller', 'J.', ''], ['Zacchei', 'A.', ''], ['Zamorani', 'G.', ''], ['Zerbi', 'F. M.', ''], ['Zinchenko', 'I. A.', ''], ['Zucca', 'E.', ''], ['Allevato', 'V.', ''], ['Ballardini', 'M.', ''], ['Bolzonella', 'M.', ''], ['Bozzo', 'E.', ''], ['Burigana', 'C.', ''], ['Cabanac', 'R.', ''], ['Cappi', 'A.', ''], ['Di Ferdinando', 'D.', ''], ['Vigo', 'J. A. Escartin', ''], ['Gabarra', 'L.', ''], ['Mart\u00edn-Fleitas', 'J.', ''], ['Matthew', 'S.', ''], ['Mauri', 'N.', ''], ['Metcalf', 'R. B.', ''], ['Pezzotta', 'A.', ''], ['P\u00f6ntinen', 'M.', ''], ['Porciani', 'C.', ''], ['Risso', 'I.', ''], ['Scottez', 'V.', ''], ['Sereno', 'M.', ''], ['Tenti', 'M.', ''], ['Viel', 'M.', ''], ['Wiesmann', 'M.', ''], ['Akrami', 'Y.', ''], ['Andika', 'I. T.', ''], ['Anselmi', 'S.', ''], ['Archidiacono', 'M.', ''], ['Atrio-Barandela', 'F.', ''], ['Benoist', 'C.', ''], ['Benson', 'K.', ''], ['Bertacca', 'D.', ''], ['Bethermin', 'M.', ''], ['Bisigello', 'L.', ''], ['Blanchard', 'A.', ''], ['Blot', 'L.', ''], ['B\u00f6hringer', 'H.', ''], ['Brown', 'M. L.', ''], ['Bruton', 'S.', ''], ['Buitrago', 'F.', ''], ['Calabro', 'A.', ''], ['Quevedo', 'B. Camacho', ''], ['Caro', 'F.', ''], ['Carvalho', 'C. S.', ''], ['Castro', 'T.', ''], ['Cogato', 'F.', ''], ['Cooray', 'A. R.', ''], ['Cucciati', 'O.', ''], ['Davini', 'S.', ''], ['De Paolis', 'F.', ''], ['Desprez', 'G.', ''], ['D\u00edaz-S\u00e1nchez', 'A.', ''], ['Diaz', 'J. J.', ''], ['Di Domizio', 'S.', ''], ['Diego', 'J. M.', ''], ['Duc', 'P. -A.', ''], ['Enia', 'A.', ''], ['Fang', 'Y.', ''], ['Ferrari', 'A. G.', ''], ['Finoguenov', 'A.', ''], ['Fontana', 'A.', ''], ['Franco', 'A.', ''], ['Ganga', 'K.', ''], ['Garc\u00eda-Bellido', 'J.', ''], ['Gasparetto', 'T.', ''], ['Gautard', 'V.', ''], ['Gaztanaga', 'E.', ''], ['Giacomini', 'F.', ''], ['Gozaliasl', 'G.', ''], ['Guidi', 'M.', ''], ['Gutierrez', 'C. M.', ''], ['Hall', 'A.', ''], ['Hartley', 'W. G.', ''], ['Hemmati', 'S.', ''], ['Hern\u00e1ndez-Monteagudo', 'C.', ''], ['Hildebrandt', 'H.', ''], ['Hjorth', 'J.', ''], ['Kajava', 'J. J. E.', ''], ['Kang', 'Y.', ''], ['Kansal', 'V.', ''], ['Karagiannis', 'D.', ''], ['Kiiveri', 'K.', ''], ['Kirkpatrick', 'C. C.', ''], ['Graet', 'J. Le', ''], ['Legrand', 'L.', ''], ['Lembo', 'M.', ''], ['Lepori', 'F.', ''], ['Leroy', 'G.', ''], ['Lesci', 'G. F.', ''], ['Lesgourgues', 'J.', ''], ['Leuzzi', 'L.', ''], ['Liaudat', 'T. I.', ''], ['Loureiro', 'A.', ''], ['Macias-Perez', 'J.', ''], ['Maggio', 'G.', ''], ['Magliocchetti', 'M.', ''], ['Mannucci', 'F.', ''], ['Maoli', 'R.', ''], ['Martins', 'C. J. A. P.', ''], ['Maurin', 'L.', ''], ['Miluzio', 'M.', ''], ['Monaco', 'P.', ''], ['Moretti', 'C.', ''], ['Morgante', 'G.', ''], ['Murray', 'C.', ''], ['Nadathur', 'S.', ''], ['Naidoo', 'K.', ''], ['Navarro-Alsina', 'A.', ''], ['Nesseris', 'S.', ''], ['Passalacqua', 'F.', ''], ['Paterson', 'K.', ''], ['Patrizii', 'L.', ''], ['Pisani', 'A.', ''], ['Potter', 'D.', ''], ['Quai', 'S.', ''], ['Radovich', 'M.', ''], ['Rocci', 'P. -F.', ''], ['Rodighiero', 'G.', ''], ['Sacquegna', 'S.', ''], ['Sahl\u00e9n', 'M.', ''], ['Sanders', 'D. B.', ''], ['Sarpa', 'E.', ''], ['Scarlata', 'C.', ''], ['Schaye', 'J.', ''], ['Schneider', 'A.', ''], ['Schultheis', 'M.', ''], ['Sciotti', 'D.', ''], ['Sellentin', 'E.', ''], ['Shankar', 'F.', ''], ['Smith', 'L. C.', ''], ['Tanidis', 'K.', ''], ['Testera', 'G.', ''], ['Teyssier', 'R.', ''], ['Tosi', 'S.', ''], ['Troja', 'A.', ''], ['Tucci', 'M.', ''], ['Valieri', 'C.', ''], ['Venhola', 'A.', ''], ['Vergani', 'D.', ''], ['Verza', 'G.', ''], ['Vielzeuf', 'P.', ''], ['Walton', 'N. A.', ''], ['Soubrie', 'E.', ''], ['Scott', 'D.', '']]","extracted_entities":"[{'text': 'finetuning', 'label': 'Fine-tuning'}, {'text': 'Zoobot galaxy foundation models', 'label': 'Foundation Model'}]","assigned_concept":"Fine-tuning","matched_keyword":"finetuning","similarity_score":0.5753726363}
{"id":2503.15352,"submitter":"Abhi Kamboj","authors":"Abhi Kamboj, Minh N. Do","title":"Leveraging Perfect Multimodal Alignment and Gaussian Assumptions for\n  Cross-modal Transfer","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV eess.SP","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Multimodal alignment aims to construct a joint latent vector space where two\nmodalities representing the same concept map to the same vector. We formulate\nthis as an inverse problem and show that under certain conditions perfect\nalignment can be achieved. We then address a specific application of alignment\nreferred to as cross-modal transfer. Unsupervised cross-modal transfer aims to\nleverage a model trained with one modality to perform inference on another\nmodality, without any labeled fine-tuning on the new modality. Assuming that\nsemantic classes are represented as a mixture of Gaussians in the latent space,\nwe show how cross-modal transfer can be performed by projecting the data points\nfrom the representation space onto different subspaces representing each\nmodality. Our experiments on synthetic multimodal Gaussian data verify the\neffectiveness of our perfect alignment and cross-modal transfer method. We hope\nthese findings inspire further exploration of the applications of perfect\nalignment and the use of Gaussian models for cross-modal learning.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:51:17 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Kamboj', 'Abhi', ''], ['Do', 'Minh N.', '']]","extracted_entities":"[{'text': 'Unsupervised cross-modal transfer', 'label': 'Few-shot Learning'}, {'text': 'labeled fine-tuning', 'label': 'Fine-tuning'}, {'text': 'cross-modal learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"labeled fine-tuning","similarity_score":0.8374010324}
{"id":2503.1539,"submitter":"Yumin Zhang","authors":"Yumin Zhang, Yan Gao, Haoran Duan, Hanqing Guo, Tejal Shah, Rajiv\n  Ranjan, and Bo Wei","title":"FedSCA: Federated Tuning with Similarity-guided Collaborative\n  Aggregation for Heterogeneous Medical Image Segmentation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Transformer-based foundation models (FMs) have recently demonstrated\nremarkable performance in medical image segmentation. However, scaling these\nmodels is challenging due to the limited size of medical image datasets within\nisolated hospitals, where data centralization is restricted due to privacy\nconcerns. These constraints, combined with the data-intensive nature of FMs,\nhinder their broader application. Integrating federated learning (FL) with\nfoundation models (FLFM) fine-tuning offers a potential solution to these\nchallenges by enabling collaborative model training without data sharing, thus\nallowing FMs to take advantage of a diverse pool of sensitive medical image\ndata across hospitals\/clients. However, non-independent and identically\ndistributed (non-IID) data among clients, paired with computational and\ncommunication constraints in federated environments, presents an additional\nchallenge that limits further performance improvements and remains inadequately\naddressed in existing studies. In this work, we propose a novel FLFM\nfine-tuning framework, \\underline{\\textbf{Fed}}erated tuning with\n\\underline{\\textbf{S}}imilarity-guided \\underline{\\textbf{C}}ollaborative\n\\underline{\\textbf{A}}ggregation (FedSCA), encompassing all phases of the FL\nprocess. This includes (1) specially designed parameter-efficient fine-tuning\n(PEFT) for local client training to enhance computational efficiency; (2)\npartial low-level adapter transmission for communication efficiency; and (3)\nsimilarity-guided collaborative aggregation (SGCA) on the server side to\naddress non-IID issues. Extensive experiments on three FL benchmarks for\nmedical image segmentation demonstrate the effectiveness of our proposed\nFedSCA, establishing new SOTA performance.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:27:29 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhang', 'Yumin', ''], ['Gao', 'Yan', ''], ['Duan', 'Haoran', ''], ['Guo', 'Hanqing', ''], ['Shah', 'Tejal', ''], ['Ranjan', 'Rajiv', ''], ['Wei', 'Bo', '']]","extracted_entities":"[{'text': 'FMs', 'label': 'Foundation Model'}, {'text': 'privacy\\nconcerns', 'label': 'AI Ethics'}, {'text': 'FMs', 'label': 'Foundation Model'}, {'text': 'federated learning', 'label': 'Few-shot Learning'}, {'text': 'FMs', 'label': 'Foundation Model'}, {'text': 'FedSCA', 'label': 'Fine-tuning'}, {'text': 'FL', 'label': 'Few-shot Learning'}, {'text': 'specially designed parameter-efficient fine-tuning', 'label': 'Fine-tuning'}, {'text': 'FL', 'label': 'Few-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"specially designed parameter-efficient fine-tuning","similarity_score":0.7575091124}
{"id":2503.154,"submitter":"Jiakun Yan","authors":"Jiakun Yan, Marc Snir","title":"Contemplating a Lightweight Communication Interface for Asynchronous\n  Many-Task Systems","comments":"Accepted as a short paper by WAMTA25 (wamta25.github.io)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Asynchronous Many-Task Systems (AMTs) exhibit different communication\npatterns from traditional High-Performance Computing (HPC) applications,\ncharacterized by asynchrony, concurrency, and multithreading. Existing\ncommunication libraries usually do not support AMTs' communication requirements\nin the most direct and efficient ways. The Lightweight Communication Interface\n(LCI) is an experimental communication library aiming to push for efficient\ncommunication support for AMTs. This paper presents the design for a new LCI\nC++ interface and its rationale. With a new C++ \\emph{objectized flexible\nfunctions} idiom, the new interface aims for the following features: (a) a\nconcise but expressive interface for all common point-to-point communication\nprimitives and completion mechanisms, (b) a fine-grained resource mapping\nscheme for library interoperation, multithreaded performance isolation, and\nflexibility (c) a set of optional parameters and overridable classes for users\nto incrementally fine-tune the runtime behavior.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:40:42 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Yan', 'Jiakun', ''], ['Snir', 'Marc', '']]","extracted_entities":"[{'text': 'incrementally fine-tune', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"incrementally fine-tune","similarity_score":0.6911386847}
{"id":2503.15436,"submitter":"Ritwick Banerjee","authors":"Ritwick Banerjee, Bryan Andrews, and Erich Kummerfeld","title":"An extensive simulation study evaluating the interaction of resampling\n  techniques across multiple causal discovery contexts","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ME cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Despite the accelerating presence of exploratory causal analysis in modern\nscience and medicine, the available non-experimental methods for validating\ncausal models are not well characterized. One of the most popular methods is to\nevaluate the stability of model features after resampling the data, similar to\nresampling methods for estimating confidence intervals in statistics. Many\naspects of this approach have received little to no attention, however, such as\nwhether the choice of resampling method should depend on the sample size,\nalgorithms being used, or algorithm tuning parameters. We present theoretical\nresults proving that certain resampling methods closely emulate the assignment\nof specific values to algorithm tuning parameters. We also report the results\nof extensive simulation experiments, which verify the theoretical result and\nprovide substantial data to aid researchers in further characterizing\nresampling in the context of causal discovery analysis. Together, the\ntheoretical work and simulation results provide specific guidance on how\nresampling methods and tuning parameters should be selected in practice.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:18:18 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Banerjee', 'Ritwick', ''], ['Andrews', 'Bryan', ''], ['Kummerfeld', 'Erich', '']]","extracted_entities":"[{'text': 'algorithm tuning parameters', 'label': 'Fine-tuning'}, {'text': 'algorithm tuning parameters', 'label': 'Fine-tuning'}, {'text': 'resampling methods', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"algorithm tuning parameters","similarity_score":0.6079602242}
{"id":2503.15438,"submitter":"Yang Tan","authors":"Yang Tan, Chen Liu, Jingyuan Gao, Banghao Wu, Mingchen Li, Ruilin\n  Wang, Lingrong Zhang, Huiqun Yu, Guisheng Fan, Liang Hong, Bingxin Zhou","title":"VenusFactory: A Unified Platform for Protein Engineering Data Retrieval\n  and Language Model Fine-Tuning","comments":"12 pages, 1 figure, 8 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI q-bio.QM","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Natural language processing (NLP) has significantly influenced scientific\ndomains beyond human language, including protein engineering, where pre-trained\nprotein language models (PLMs) have demonstrated remarkable success. However,\ninterdisciplinary adoption remains limited due to challenges in data\ncollection, task benchmarking, and application. This work presents\nVenusFactory, a versatile engine that integrates biological data retrieval,\nstandardized task benchmarking, and modular fine-tuning of PLMs. VenusFactory\nsupports both computer science and biology communities with choices of both a\ncommand-line execution and a Gradio-based no-code interface, integrating $40+$\nprotein-related datasets and $40+$ popular PLMs. All implementations are\nopen-sourced on https:\/\/github.com\/tyang816\/VenusFactory.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:19:07 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Tan', 'Yang', ''], ['Liu', 'Chen', ''], ['Gao', 'Jingyuan', ''], ['Wu', 'Banghao', ''], ['Li', 'Mingchen', ''], ['Wang', 'Ruilin', ''], ['Zhang', 'Lingrong', ''], ['Yu', 'Huiqun', ''], ['Fan', 'Guisheng', ''], ['Hong', 'Liang', ''], ['Zhou', 'Bingxin', '']]","extracted_entities":"[{'text': 'modular fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"modular fine-tuning","similarity_score":0.7369710207}
{"id":2503.15586,"submitter":"Zeqi Gu","authors":"Zeqi Gu, Difan Liu, Timothy Langlois, Matthew Fisher, Abe Davis","title":"How to Train Your Dragon: Automatic Diffusion-Based Rigging for\n  Characters with Diverse Topologies","comments":"Accepted to Eurographics 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GR cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent diffusion-based methods have achieved impressive results on animating\nimages of human subjects. However, most of that success has built on\nhuman-specific body pose representations and extensive training with labeled\nreal videos. In this work, we extend the ability of such models to animate\nimages of characters with more diverse skeletal topologies. Given a small\nnumber (3-5) of example frames showing the character in different poses with\ncorresponding skeletal information, our model quickly infers a rig for that\ncharacter that can generate images corresponding to new skeleton poses. We\npropose a procedural data generation pipeline that efficiently samples training\ndata with diverse topologies on the fly. We use it, along with a novel skeleton\nrepresentation, to train our model on articulated shapes spanning a large space\nof textures and topologies. Then during fine-tuning, our model rapidly adapts\nto unseen target characters and generalizes well to rendering new poses, both\nfor realistic and more stylized cartoon appearances. To better evaluate\nperformance on this novel and challenging task, we create the first 2D video\ndataset that contains both humanoid and non-humanoid subjects with per-frame\nkeypoint annotations. With extensive experiments, we demonstrate the superior\nquality of our results. Project page: https:\/\/traindragondiffusion.github.io\/\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:46:36 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Gu', 'Zeqi', ''], ['Liu', 'Difan', ''], ['Langlois', 'Timothy', ''], ['Fisher', 'Matthew', ''], ['Davis', 'Abe', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.15755,"submitter":"Rosa Sinaasappel","authors":"R. Sinaasappel, K.R.Prathyusha, H. Tuazon, E.Mirzahossein, P.Illien,\n  S. Bhamla, A.Deblais","title":"Collecting Particles in Confined Spaces by Active Filamentous Matter","comments":"9 pages, 6 figures, 10 pages of supplementary information","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.soft","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The potential of compliant and adaptable active matter for particle transport\npresents a promising avenue for the development of efficient, autonomous\nsystems. However, achieving optimal task efficiency often depends on external\ncontrol mechanisms, which can limit the autonomy of such systems. In this\nstudy, we draw inspiration from Tubifex tubifex and Lumbriculus variegatus,\ncentimeter-sized worms that exhibit an extraordinary ability to aggregate\ndispersed particles within confined environments. By observing their natural\nbehaviors, we identify a simple yet effective particle collection strategy\ndriven by flexibility and activity. Using these biological insights, we develop\nlarger-scale robotic systems and simulations that replicate the particle\naggregation dynamics of living worms. Our results reveal that coupling between\nactivity and flexibility governs the efficiency of particle clustering, and\nthis principle applies universally across biological, robotic, and simulated\nfilaments. These results allow us to offer new particle collection strategies\nby tuning the design elements like topology or bending stiffness of soft active\nfilaments.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 00:17:56 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Sinaasappel', 'R.', ''], ['Prathyusha', 'K. R.', ''], ['Tuazon', 'H.', ''], ['Mirzahossein', 'E.', ''], ['Illien', 'P.', ''], ['Bhamla', 'S.', ''], ['Deblais', 'A.', '']]","extracted_entities":"[{'text': 'tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"tuning","similarity_score":0.8449009061}
{"id":2503.15781,"submitter":"Yuci Han","authors":"Yuci Han, Charles Toth, Alper Yilmaz","title":"UAS Visual Navigation in Large and Unseen Environments via a Meta Agent","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The aim of this work is to develop an approach that enables Unmanned Aerial\nSystem (UAS) to efficiently learn to navigate in large-scale urban environments\nand transfer their acquired expertise to novel environments. To achieve this,\nwe propose a meta-curriculum training scheme. First, meta-training allows the\nagent to learn a master policy to generalize across tasks. The resulting model\nis then fine-tuned on the downstream tasks. We organize the training curriculum\nin a hierarchical manner such that the agent is guided from coarse to fine\ntowards the target task. In addition, we introduce Incremental Self-Adaptive\nReinforcement learning (ISAR), an algorithm that combines the ideas of\nincremental learning and meta-reinforcement learning (MRL). In contrast to\ntraditional reinforcement learning (RL), which focuses on acquiring a policy\nfor a specific task, MRL aims to learn a policy with fast transfer ability to\nnovel tasks. However, the MRL training process is time consuming, whereas our\nproposed ISAR algorithm achieves faster convergence than the conventional MRL\nalgorithm. We evaluate the proposed methodologies in simulated environments and\ndemonstrate that using this training philosophy in conjunction with the ISAR\nalgorithm significantly improves the convergence speed for navigation in\nlarge-scale cities and the adaptation proficiency in novel environments.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 01:44:59 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Han', 'Yuci', ''], ['Toth', 'Charles', ''], ['Yilmaz', 'Alper', '']]","extracted_entities":"[{'text': 'fine-tuned', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuned","similarity_score":0.8707774282}
{"id":2503.15863,"submitter":"Jonathon Yuly","authors":"Jonathon L. Yuly","title":"Method for bioinspired electron bifurcation by semiconductor\n  electrochemistry","comments":"18 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mes-hall physics.bio-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Electron bifurcating enzymes oxidize a two-electron donor, pushing one\nelectron thermodynamically uphill to reduce a low-potential acceptor by\nleveraging the downhill flow of the other electron to a high-potential\nacceptor. Electron bifurcation can achieve near 100\\% energy conversion\nefficiency if operating in a near-reversible regime. Theories of charge\ntransport and heterogeneous electron transfer reveal that bioinspired electron\nbifurcation is possible in tailored semiconductor electrochemical junctions:\nthree-way n-p-electrolyte junctions. A two-electron species is oxidized at the\nsemiconducting surface, injecting the resulting charges into the semiconductor.\nIf the junction is properly configured, the electrons will spontaneously\nbifurcate into the n- and p-doped regions. If a bias is applied across these\nregions, the semiconductor-electrolyte junction will transduce energy by\npushing half of the current to higher potential and half to lower potential.\nEnergy wasting short circuit processes are be defeated using the carrier\ndistributions and dynamics that occur naturally in these junctions.\nFurthermore, bifurcating junctions seem to require only fundamental\nelectrochemical processes that have been observed in other contexts, and does\nnot require fine-tuning of kinetic rate constants (although tuning may improve\nperformance). Theory and simulation of bifurcating junctions reveals critical\ndesign principles and suggests that $\\sim 100 \\mu\\text{A}\/\\text{cm}^2 - 1\n\\hspace{2 pt} m\\text{A\/cm}^2$ of bifurcated current is a reasonable goal for\nbifurcating junctions, but further enhancement seems possible.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 05:34:25 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Yuly', 'Jonathon L.', '']]","extracted_entities":"[{'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.1587,"submitter":"Ali Anaissi","authors":"Yuxin Miao, Xinyuan Yang, Hongda Fan, Yichun Li, Yishu Hong, Xiechen\n  Guo, Ali Braytee, Weidong Huang, Ali Anaissi","title":"FedSAF: A Federated Learning Framework for Enhanced Gastric Cancer\n  Detection and Privacy Preservation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Gastric cancer is one of the most commonly diagnosed cancers and has a high\nmortality rate. Due to limited medical resources, developing machine learning\nmodels for gastric cancer recognition provides an efficient solution for\nmedical institutions. However, such models typically require large sample sizes\nfor training and testing, which can challenge patient privacy. Federated\nlearning offers an effective alternative by enabling model training across\nmultiple institutions without sharing sensitive patient data. This paper\naddresses the limited sample size of publicly available gastric cancer data\nwith a modified data processing method. This paper introduces FedSAF, a novel\nfederated learning algorithm designed to improve the performance of existing\nmethods, particularly in non-independent and identically distributed (non-IID)\ndata scenarios. FedSAF incorporates attention-based message passing and the\nFisher Information Matrix to enhance model accuracy, while a model splitting\nfunction reduces computation and transmission costs. Hyperparameter tuning and\nablation studies demonstrate the effectiveness of this new algorithm, showing\nimprovements in test accuracy on gastric cancer datasets, with FedSAF\noutperforming existing federated learning methods like FedAMP, FedAvg, and\nFedProx. The framework's robustness and generalization ability were further\nvalidated across additional datasets (SEED, BOT, FashionMNIST, and CIFAR-10),\nachieving high performance in diverse environments.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 05:48:48 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Miao', 'Yuxin', ''], ['Yang', 'Xinyuan', ''], ['Fan', 'Hongda', ''], ['Li', 'Yichun', ''], ['Hong', 'Yishu', ''], ['Guo', 'Xiechen', ''], ['Braytee', 'Ali', ''], ['Huang', 'Weidong', ''], ['Anaissi', 'Ali', '']]","extracted_entities":"[{'text': 'Federated\\nlearning', 'label': 'Zero-shot Learning'}, {'text': 'attention-based message passing', 'label': 'Attention mechanism'}, {'text': 'Hyperparameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"Hyperparameter tuning","similarity_score":0.6193697453}
{"id":2503.15878,"submitter":"Jiaqi Leng","authors":"Jiaqi Leng, Yufan Zheng, Zhiyuan Jia, Lei Fan, Chaoyue Zhao, Yuxiang\n  Peng, Xiaodi Wu","title":"Quantum Hamiltonian Descent for Non-smooth Optimization","comments":"48 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC quant-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Non-smooth optimization models play a fundamental role in various\ndisciplines, including engineering, science, management, and finance. However,\nclassical algorithms for solving such models often struggle with convergence\nspeed, scalability, and parameter tuning, particularly in high-dimensional and\nnon-convex settings. In this paper, we explore how quantum mechanics can be\nleveraged to overcome these limitations. Specifically, we investigate the\ntheoretical properties of the Quantum Hamiltonian Descent (QHD) algorithm for\nnon-smooth optimization in both continuous and discrete time. First, we propose\ncontinuous-time variants of the general QHD algorithm and establish their\nglobal convergence and convergence rate for non-smooth convex and strongly\nconvex problems through a novel Lyapunov function design. Furthermore, we prove\nthe finite-time global convergence of continuous-time QHD for non-smooth\nnon-convex problems under mild conditions (i.e., locally Lipschitz). In\naddition, we propose discrete-time QHD, a fully digitized implementation of QHD\nvia operator splitting (i.e., product formula). We find that discrete-time QHD\nexhibits similar convergence properties even with large time steps. Finally,\nnumerical experiments validate our theoretical findings and demonstrate the\ncomputational advantages of QHD over classical non-smooth non-convex\noptimization algorithms.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 06:02:33 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Leng', 'Jiaqi', ''], ['Zheng', 'Yufan', ''], ['Jia', 'Zhiyuan', ''], ['Fan', 'Lei', ''], ['Zhao', 'Chaoyue', ''], ['Peng', 'Yuxiang', ''], ['Wu', 'Xiaodi', '']]","extracted_entities":"[{'text': 'scalability', 'label': 'Scaling law'}, {'text': 'parameter tuning', 'label': 'Fine-tuning'}, {'text': 'quantum mechanics', 'label': 'quantisation'}]","assigned_concept":"Fine-tuning","matched_keyword":"parameter tuning","similarity_score":0.6959539652}
{"id":2503.15887,"submitter":"Haochen Wang","authors":"Haochen Wang and Kai Hu and Liangcai Gao","title":"DocVideoQA: Towards Comprehensive Understanding of Document-Centric\n  Videos through Question Answering","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Remote work and online courses have become important methods of knowledge\ndissemination, leading to a large number of document-based instructional\nvideos. Unlike traditional video datasets, these videos mainly feature\nrich-text images and audio that are densely packed with information closely\ntied to the visual content, requiring advanced multimodal understanding\ncapabilities. However, this domain remains underexplored due to dataset\navailability and its inherent complexity. In this paper, we introduce the\nDocVideoQA task and dataset for the first time, comprising 1454 videos across\n23 categories with a total duration of about 828 hours. The dataset is\nannotated with 154k question-answer pairs generated manually and via GPT,\nassessing models' comprehension, temporal awareness, and modality integration\ncapabilities. Initially, we establish a baseline using open-source MLLMs.\nRecognizing the challenges in modality comprehension for document-centric\nvideos, we present DV-LLaMA, a robust video MLLM baseline. Our method enhances\nunimodal feature extraction with diverse instruction-tuning data and employs\ncontrastive learning to strengthen modality integration. Through fine-tuning,\nthe LLM is equipped with audio-visual capabilities, leading to significant\nimprovements in document-centric video understanding. Extensive testing on the\nDocVideoQA dataset shows that DV-LLaMA significantly outperforms existing\nmodels. We'll release the code and dataset to facilitate future research.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 06:21:25 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wang', 'Haochen', ''], ['Hu', 'Kai', ''], ['Gao', 'Liangcai', '']]","extracted_entities":"[{'text': 'open-source MLLMs', 'label': 'Open-source LLMs'}, {'text': 'DV-LLaMA', 'label': 'LLM'}, {'text': 'contrastive learning', 'label': 'Few-shot Learning'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'DV-LLaMA', 'label': 'LLM'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.15924,"submitter":"Peiyi Lin","authors":"Peiyi Lin, Fukai Zhang, Kai Niu, Hao Fu","title":"Towards Automatic Continual Learning: A Self-Adaptive Framework for\n  Continual Instruction Tuning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Continual instruction tuning enables large language models (LLMs) to learn\nincrementally while retaining past knowledge, whereas existing methods\nprimarily focus on how to retain old knowledge rather than on selecting which\nnew knowledge to learn. In domain-specific contexts, maintaining data quality\nand managing system constraints remain key challenges. To address these issues,\nwe propose an automated continual instruction tuning framework that dynamically\nfilters incoming data, which identify and reduce redundant data across\nsuccessive updates. Our approach utilizes a small proxy model for efficient\nperplexity-based filtering, and updates the proxy to ensure that the filtering\ncriteria remain aligned with the evolving state of the deployed model. Compared\nto existing static data selection methods, our framework can effectively handle\nincrementally acquired data and shifting distributions. Additionally, it\naddresses practical deployment challenges by enabling seamless model updates,\nsupporting version rollback and incorporating automatic checkpoint evaluation.\nWe evaluated the system in real-world medical scenarios. It reduced\ncomputational costs by 66.7% and improved model performance, and achieved\nautonomous updates, thus demonstrating its effectiveness for automatic\ncontinual instruction tuning.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:00:41 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Lin', 'Peiyi', ''], ['Zhang', 'Fukai', ''], ['Niu', 'Kai', ''], ['Fu', 'Hao', '']]","extracted_entities":"[{'text': 'Continual instruction tuning', 'label': 'Fine-tuning'}, {'text': 'continual instruction tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"Continual instruction tuning","similarity_score":0.5665868521}
{"id":2503.1594,"submitter":"Lichao Mou","authors":"Yaxiong Chen, Chuang Du, Chunlei Li, Jingliang Hu, Yilei Shi, Shengwu\n  Xiong, Xiao Xiang Zhu, Lichao Mou","title":"UniCrossAdapter: Multimodal Adaptation of CLIP for Radiology Report\n  Generation","comments":"MICCAI 2024 Workshop","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Automated radiology report generation aims to expedite the tedious and\nerror-prone reporting process for radiologists. While recent works have made\nprogress, learning to align medical images and textual findings remains\nchallenging due to the relative scarcity of labeled medical data. For example,\ndatasets for this task are much smaller than those used for image captioning in\ncomputer vision. In this work, we propose to transfer representations from\nCLIP, a large-scale pre-trained vision-language model, to better capture\ncross-modal semantics between images and texts. However, directly applying CLIP\nis suboptimal due to the domain gap between natural images and radiology. To\nenable efficient adaptation, we introduce UniCrossAdapter, lightweight adapter\nmodules that are incorporated into CLIP and fine-tuned on the target task while\nkeeping base parameters fixed. The adapters are distributed across modalities\nand their interaction to enhance vision-language alignment. Experiments on two\npublic datasets demonstrate the effectiveness of our approach, advancing\nstate-of-the-art in radiology report generation. The proposed transfer learning\nframework provides a means of harnessing semantic knowledge from large-scale\npre-trained models to tackle data-scarce medical vision-language tasks. Code is\navailable at https:\/\/github.com\/chauncey-tow\/MRG-CLIP.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:28:53 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chen', 'Yaxiong', ''], ['Du', 'Chuang', ''], ['Li', 'Chunlei', ''], ['Hu', 'Jingliang', ''], ['Shi', 'Yilei', ''], ['Xiong', 'Shengwu', ''], ['Zhu', 'Xiao Xiang', ''], ['Mou', 'Lichao', '']]","extracted_entities":"[{'text': 'CLIP', 'label': 'Large Language Model'}, {'text': 'CLIP', 'label': 'Large Language Model'}, {'text': 'CLIP', 'label': 'Large Language Model'}, {'text': 'fine-tuned', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuned","similarity_score":0.8707774282}
{"id":2503.16023,"submitter":"Zenghui Yuan","authors":"Zenghui Yuan and Jiawen Shi and Pan Zhou and Neil Zhenqiang Gong and\n  Lichao Sun","title":"BadToken: Token-level Backdoor Attacks to Multi-modal Large Language\n  Models","comments":"This paper is accepted by CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multi-modal large language models (MLLMs) extend large language models (LLMs)\nto process multi-modal information, enabling them to generate responses to\nimage-text inputs. MLLMs have been incorporated into diverse multi-modal\napplications, such as autonomous driving and medical diagnosis, via\nplug-and-play without fine-tuning. This deployment paradigm increases the\nvulnerability of MLLMs to backdoor attacks. However, existing backdoor attacks\nagainst MLLMs achieve limited effectiveness and stealthiness. In this work, we\npropose BadToken, the first token-level backdoor attack to MLLMs. BadToken\nintroduces two novel backdoor behaviors: Token-substitution and Token-addition,\nwhich enable flexible and stealthy attacks by making token-level modifications\nto the original output for backdoored inputs. We formulate a general\noptimization problem that considers the two backdoor behaviors to maximize the\nattack effectiveness. We evaluate BadToken on two open-source MLLMs and various\ntasks. Our results show that our attack maintains the model's utility while\nachieving high attack success rates and stealthiness. We also show the\nreal-world threats of BadToken in two scenarios, i.e., autonomous driving and\nmedical diagnosis. Furthermore, we consider defenses including fine-tuning and\ninput purification. Our results highlight the threat of our attack.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 10:39:51 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Yuan', 'Zenghui', ''], ['Shi', 'Jiawen', ''], ['Zhou', 'Pan', ''], ['Gong', 'Neil Zhenqiang', ''], ['Sun', 'Lichao', '']]","extracted_entities":"[{'text': 'Multi-modal large language models', 'label': 'Large Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.16081,"submitter":"Yuting Zhang","authors":"Zhiyuan Liu, Yuting Zhang, Feng Liu, Changwang Zhang, Ying Sun, Jun\n  Wang","title":"OThink-MR1: Stimulating multimodal generalized reasoning capabilities\n  through dynamic reinforcement learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.IR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multimodal Language Models have gained significant traction for their ability\nto process diverse input data types and generate coherent, contextually\nrelevant outputs across various applications. While supervised fine-tuning\n(SFT) has been the predominant approach to enhance MLLM capabilities in\ntask-specific optimization, it often falls short in fostering crucial\ngeneralized reasoning abilities. Despite the potential of reinforcement\nlearning (RL) to address these limitations, it faces two issues: (1) its\ngeneralized capabilities in multimodal tasks remain underexplored. (2) its\ntraining constraints such as constant Kullback-Leibler or clamp strategy easily\nlead to suboptimal bottleneck. To adress these issues, we introduce OThink-MR1,\na framework that extends RL to MLLMs, enabling them to achieve deeper\nunderstanding and reasoning across multimodal tasks. We design a dynamic\nKullback-Leibler strategy that significantly enhances RL performance,\nsurpassing SFT in same-task evaluations. Also, we are the first to reveal that\nRL exhibits remarkable cross-task generalization capabilities, which shows that\nmodels post-trained with RL on one multimodal task can be effectively\ntransfered to another tasks. Finally, extensive experiments demonstrate the\ngreat reasoning ability of our proposed OThink-MR1.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 12:22:18 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Liu', 'Zhiyuan', ''], ['Zhang', 'Yuting', ''], ['Liu', 'Feng', ''], ['Zhang', 'Changwang', ''], ['Sun', 'Ying', ''], ['Wang', 'Jun', '']]","extracted_entities":"[{'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'SFT', 'label': 'BERT'}]","assigned_concept":"Fine-tuning","matched_keyword":"supervised fine-tuning","similarity_score":0.7449287176}
{"id":2503.16169,"submitter":"Louis-Adrien Dufr\\`ene","authors":"Louis-Adrien Dufr\\`ene, Quentin Lampin, Guillaume Larue","title":"Learning Linear Block Codes with Gradient Quantization","comments":"13 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This study investigates the problem of learning linear block codes optimized\nfor Belief-Propagation decoders significantly improving performance compared to\nthe state-of-the-art. Our previous research is extended with an enhanced system\ndesign that facilitates a more effective learning process for the parity check\nmatrix. We simplify the input dataset, restrict the number of parameters to\nlearn and improve the gradient back-propagation within the model. We also\nintroduce novel optimizers specifically designed for discrete-valued weights.\nBased on conventional gradient computation, these optimizers provide discrete\nweights updates, enabling finer control and improving explainability of the\nlearning process. Through these changes, we consistently achieve improved code\nperformance, provided appropriately chosen hyper-parameters. To rigorously\nevaluate the performance of learned codes in the context of short to medium\nblock lengths, we propose a comprehensive code performance assessment\nframework. This framework enables a fair comparison between our learning\nmethodology and random search approaches, ensuring statistical significance in\nour results. The proposed model pave the way for a new approach to the\nefficient learning of linear block codes tailored to specific decoder\nstructures.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 14:10:46 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Dufr\u00e8ne', 'Louis-Adrien', ''], ['Lampin', 'Quentin', ''], ['Larue', 'Guillaume', '']]","extracted_entities":"[{'text': 'finer control', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"finer control","similarity_score":0.5030783415}
{"id":2503.16219,"submitter":"Quy-Anh Dang","authors":"Quy-Anh Dang and Chris Ngo","title":"Reinforcement Learning for Reasoning in Small LLMs: What Works and What\n  Doesn't","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Enhancing the reasoning capabilities of large language models (LLMs)\ntypically relies on massive computational resources and extensive datasets,\nlimiting accessibility for resource-constrained settings. Our study\ninvestigates the potential of reinforcement learning (RL) to improve reasoning\nin small LLMs, focusing on a 1.5-billion-parameter model,\nDeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA\nA40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy\nOptimization (GRPO) algorithm and curating a compact, high-quality mathematical\nreasoning dataset, we conducted three experiments to explore model behavior and\nperformance. Our results demonstrate rapid reasoning gains - e.g., AMC23\naccuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing\no1-preview - using only 7,000 samples and a $42 training cost, compared to\nthousands of dollars for baseline models. However, challenges such as\noptimization instability and length constraints emerged with prolonged\ntraining. These findings highlight the efficacy of RL-based fine-tuning for\nsmall LLMs, offering a cost-effective alternative to large-scale approaches. We\nrelease our code and datasets as open-source resources, providing insights into\ntrade-offs and laying a foundation for scalable, reasoning-capable LLMs in\nresource-limited environments. All are available at\nhttps:\/\/github.com\/knoveleng\/open-rs.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 15:13:23 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Dang', 'Quy-Anh', ''], ['Ngo', 'Chris', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'RL-based fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"RL-based fine-tuning","similarity_score":0.7032202482}
{"id":2503.16252,"submitter":"Liwen Zhang","authors":"Zhaowei Liu, Xin Guo, Fangqi Lou, Lingfeng Zeng, Jinyi Niu, Zixuan\n  Wang, Jiajie Xu, Weige Cai, Ziwei Yang, Xueqian Zhao, Chao Li, Sheng Xu,\n  Dezhi Chen, Yun Chen, Zuo Bai and Liwen Zhang","title":"Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Reasoning large language models are rapidly evolving across various domains.\nHowever, their capabilities in handling complex financial tasks still require\nin-depth exploration. In this paper, we introduce Fin-R1, a reasoning large\nlanguage model specifically designed for the financial sector. Fin-R1 is built\nusing a two-stage architecture, leveraging a financial reasoning dataset\ndistilled and processed based on DeepSeek-R1. Through supervised fine-tuning\n(SFT) and reinforcement learning (RL) training, it demonstrates performance\nclose to DeepSeek-R1 with a parameter size of 7 billion across a range of\nfinancial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA\nand ConvFinQA tasks between those LLMs in our evaluation, surpassing larger\nmodels in other tasks as well. Fin-R1 showcases strong reasoning and\ndecision-making capabilities, providing solutions to various problems\nencountered in the financial domain. Our code is available at\nhttps:\/\/github.com\/SUFE-AIFLM-Lab\/Fin-R1.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 15:46:18 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Liu', 'Zhaowei', ''], ['Guo', 'Xin', ''], ['Lou', 'Fangqi', ''], ['Zeng', 'Lingfeng', ''], ['Niu', 'Jinyi', ''], ['Wang', 'Zixuan', ''], ['Xu', 'Jiajie', ''], ['Cai', 'Weige', ''], ['Yang', 'Ziwei', ''], ['Zhao', 'Xueqian', ''], ['Li', 'Chao', ''], ['Xu', 'Sheng', ''], ['Chen', 'Dezhi', ''], ['Chen', 'Yun', ''], ['Bai', 'Zuo', ''], ['Zhang', 'Liwen', '']]","extracted_entities":"[{'text': 'Fin-R1', 'label': 'Large Language Model'}, {'text': 'Fin-R1', 'label': 'Large Language Model'}, {'text': 'supervised fine-tuning\\n(SFT)', 'label': 'Fine-tuning'}, {'text': 'DeepSeek-R1', 'label': 'Large Language Model'}, {'text': 'Fin-R1', 'label': 'Large Language Model'}]","assigned_concept":"Fine-tuning","matched_keyword":"supervised fine-tuning\n(SFT)","similarity_score":0.6747050285}
{"id":2503.16253,"submitter":"Ioannis Adamopoulos Dr.","authors":"Antonios Valamontes, Emmanuel Markoulakis, and Ioannis Adamopoulos","title":"Superluminal Dark Photons as a Solution to the GRB 221009A Anomaly","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.HE gr-qc","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The detection of exceptionally high-energy {\\gamma}-photons (up to 18 TeV)\nfrom GRB 221009A by the LHAASO Collaboration challenges conventional physics.\nPhoton-axion-like particle (ALP) oscillations have been proposed to explain\nthis anomaly, but they rely on specific parameter tuning. We present an\nalternative explanation involving superluminal dark photons. Building on the\nframeworks of Markoulakis and Valamontes, we propose that dark photons\nfacilitated faster-than-light (FTL) propagation of information, allowing\n{\\gamma}-photons to bypass extragalactic background light (EBL) attenuation.\nThis hypothesis aligns with cosmological observations and experimental results,\nincluding those from the LHC, providing a robust framework for addressing the\nGRB 221009A anomaly.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 15:46:50 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Valamontes', 'Antonios', ''], ['Markoulakis', 'Emmanuel', ''], ['Adamopoulos', 'Ioannis', '']]","extracted_entities":"[{'text': 'specific parameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"specific parameter tuning","similarity_score":0.6627812982}
{"id":2503.16309,"submitter":"Vivek Gopalakrishnan","authors":"Vivek Gopalakrishnan, Neel Dey, David-Dimitris Chlorogiannis, Andrew\n  Abumoussa, Anna M. Larson, Darren B. Orbach, Sarah Frisken, Polina Golland","title":"Rapid patient-specific neural networks for intraoperative X-ray to\n  volume registration","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV physics.med-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The integration of artificial intelligence in image-guided interventions\nholds transformative potential, promising to extract 3D geometric and\nquantitative information from conventional 2D imaging modalities during complex\nprocedures. Achieving this requires the rapid and precise alignment of 2D\nintraoperative images (e.g., X-ray) with 3D preoperative volumes (e.g., CT,\nMRI). However, current 2D\/3D registration methods fail across the broad\nspectrum of procedures dependent on X-ray guidance: traditional optimization\ntechniques require custom parameter tuning for each subject, whereas neural\nnetworks trained on small datasets do not generalize to new patients or require\nlabor-intensive manual annotations, increasing clinical burden and precluding\napplication to new anatomical targets. To address these challenges, we present\nxvr, a fully automated framework for training patient-specific neural networks\nfor 2D\/3D registration. xvr uses physics-based simulation to generate abundant\nhigh-quality training data from a patient's own preoperative volumetric\nimaging, thereby overcoming the inherently limited ability of supervised models\nto generalize to new patients and procedures. Furthermore, xvr requires only 5\nminutes of training per patient, making it suitable for emergency interventions\nas well as planned procedures. We perform the largest evaluation of a 2D\/3D\nregistration algorithm on real X-ray data to date and find that xvr robustly\ngeneralizes across a diverse dataset comprising multiple anatomical structures,\nimaging modalities, and hospitals. Across surgical tasks, xvr achieves\nsubmillimeter-accurate registration at intraoperative speeds, improving upon\nexisting methods by an order of magnitude. xvr is released as open-source\nsoftware freely available at https:\/\/github.com\/eigenvivek\/xvr.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 16:33:45 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Gopalakrishnan', 'Vivek', ''], ['Dey', 'Neel', ''], ['Chlorogiannis', 'David-Dimitris', ''], ['Abumoussa', 'Andrew', ''], ['Larson', 'Anna M.', ''], ['Orbach', 'Darren B.', ''], ['Frisken', 'Sarah', ''], ['Golland', 'Polina', '']]","extracted_entities":"[{'text': 'custom parameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"custom parameter tuning","similarity_score":0.5887513757}
{"id":2503.16334,"submitter":"Ying Shen","authors":"Ying Shen, Lifu Huang","title":"LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates","comments":"16 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent findings reveal that much of the knowledge in a Transformer-based\nLarge Language Model (LLM) is encoded in its feed-forward (FFN) layers, where\neach FNN layer can be interpreted as the summation of sub-updates, each\ncorresponding to a weighted column vector from the FFN's value parameter matrix\nthat often encodes human-interpretable concepts. In light of this, we\nhypothesize that model performance and behaviors can be further enhanced and\ncontrolled by modulating the contributions of these sub-updates based on their\nrelevance to the input or target output style, and propose LLMBRACES, a novel\nand efficient method that computes relevance scores associated with value\nvectors in FFN layers and leverages these scores to dynamically adjust the\ncontribution of sub-updates. By optimizing sub-update contributions, LLMBRACES\nrefines the prediction process, leading to more accurate and reliable outputs,\nmuch like a 'brace' providing support and stability. Moreover, LLMBRACES can be\nextended to support conditional control over generation characteristics, such\nas sentiment, thereby offering fine-grained steering of LLM outputs. Extensive\nexperiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and\nLlama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both\nfine-tuning and zero-shot settings while requiring significantly fewer tunable\nparameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in\nsentiment-controlled generation and toxicity reduction, highlighting its\npotential for flexible, controlled text generation across applications.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 16:55:26 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Shen', 'Ying', ''], ['Huang', 'Lifu', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLMBRACES', 'label': 'LLM'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'Llama3-8B-demonstrate', 'label': 'Llama'}, {'text': 'LLMBRACES', 'label': 'LLM'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}, {'text': 'zero-shot settings', 'label': 'Zero-shot Learning'}]","assigned_concept":"Fine-tuning","matched_keyword":"fine-tuning","similarity_score":1.0000001192}
{"id":2503.16335,"submitter":"Seshu Babu Barma Mr","authors":"Seshu Babu Barma, Mohanakrishnan Hariharan, Satish Arvapalli","title":"Enhancing Software Quality Assurance with an Adaptive Differential\n  Evolution based Quantum Variational Autoencoder-Transformer Model","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.ET","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  An AI-powered quality engineering platform uses artificial intelligence to\nboost software quality assessments through automated defect prediction and\noptimized performance alongside improved feature extraction. Existing models\nresult in difficulties addressing noisy data types together with imbalances,\npattern recognition complexities, ineffective feature extraction, and\ngeneralization weaknesses. To overcome those existing challenges in this\nresearch, we develop a new model Adaptive Differential Evolution based Quantum\nVariational Autoencoder-Transformer Model (ADE-QVAET), that combines a Quantum\nVariational Autoencoder-Transformer (QVAET) to obtain high-dimensional latent\nfeatures and maintain sequential dependencies together with contextual\nrelationships, resulting in superior defect prediction accuracy. Adaptive\nDifferential Evolution (ADE) Optimization utilizes an adaptive parameter tuning\nmethod that enhances model convergence and predictive performance. ADE-QVAET\nintegrates advanced AI techniques to create a robust solution for scalable and\naccurate software defect prediction that represents a top-level AI-driven\ntechnology for quality engineering applications. The proposed ADE-QVAET model\nattains high accuracy, precision, recall, and f1-score during the training\npercentage (TP) 90 of 98.08%, 92.45%, 94.67%, and 98.12%.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 16:55:38 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Barma', 'Seshu Babu', ''], ['Hariharan', 'Mohanakrishnan', ''], ['Arvapalli', 'Satish', '']]","extracted_entities":"[{'text': 'adaptive parameter tuning\\nmethod', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"adaptive parameter tuning\nmethod","similarity_score":0.5371303558}
{"id":2503.16418,"submitter":"Liming Jiang","authors":"Liming Jiang, Qing Yan, Yumin Jia, Zichuan Liu, Hao Kang, Xin Lu","title":"InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity","comments":"Project page: https:\/\/bytedance.github.io\/InfiniteYou\/ Code and\n  model: https:\/\/github.com\/bytedance\/InfiniteYou","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Achieving flexible and high-fidelity identity-preserved image generation\nremains formidable, particularly with advanced Diffusion Transformers (DiTs)\nlike FLUX. We introduce InfiniteYou (InfU), one of the earliest robust\nframeworks leveraging DiTs for this task. InfU addresses significant issues of\nexisting methods, such as insufficient identity similarity, poor text-image\nalignment, and low generation quality and aesthetics. Central to InfU is\nInfuseNet, a component that injects identity features into the DiT base model\nvia residual connections, enhancing identity similarity while maintaining\ngeneration capabilities. A multi-stage training strategy, including pretraining\nand supervised fine-tuning (SFT) with synthetic single-person-multiple-sample\n(SPMS) data, further improves text-image alignment, ameliorates image quality,\nand alleviates face copy-pasting. Extensive experiments demonstrate that InfU\nachieves state-of-the-art performance, surpassing existing baselines. In\naddition, the plug-and-play design of InfU ensures compatibility with various\nexisting methods, offering a valuable contribution to the broader community.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:59:34 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Jiang', 'Liming', ''], ['Yan', 'Qing', ''], ['Jia', 'Yumin', ''], ['Liu', 'Zichuan', ''], ['Kang', 'Hao', ''], ['Lu', 'Xin', '']]","extracted_entities":"[{'text': 'FLUX', 'label': 'Transformers'}, {'text': 'DiTs', 'label': 'Transformers'}, {'text': 'pretraining', 'label': 'Fine-tuning'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"supervised fine-tuning","similarity_score":0.7449287176}
{"id":2503.16429,"submitter":"Xiaoyang Wu","authors":"Xiaoyang Wu, Daniel DeTone, Duncan Frost, Tianwei Shen, Chris Xie, Nan\n  Yang, Jakob Engel, Richard Newcombe, Hengshuang Zhao, Julian Straub","title":"Sonata: Self-Supervised Learning of Reliable Point Representations","comments":"CVPR 2025, produced by Pointcept x Meta, project page:\n  https:\/\/xywu.me\/sonata\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper, we question whether we have a reliable self-supervised point\ncloud model that can be used for diverse 3D tasks via simple linear probing,\neven with limited data and minimal computation. We find that existing 3D\nself-supervised learning approaches fall short when evaluated on representation\nquality through linear probing. We hypothesize that this is due to what we term\nthe \"geometric shortcut\", which causes representations to collapse to low-level\nspatial features. This challenge is unique to 3D and arises from the sparse\nnature of point cloud data. We address it through two key strategies: obscuring\nspatial information and enhancing the reliance on input features, ultimately\ncomposing a Sonata of 140k point clouds through self-distillation. Sonata is\nsimple and intuitive, yet its learned representations are strong and reliable:\nzero-shot visualizations demonstrate semantic grouping, alongside strong\nspatial reasoning through nearest-neighbor relationships. Sonata demonstrates\nexceptional parameter and data efficiency, tripling linear probing accuracy\n(from 21.8% to 72.5%) on ScanNet and nearly doubling performance with only 1%\nof the data compared to previous approaches. Full fine-tuning further advances\nSOTA across both 3D indoor and outdoor perception tasks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:59:59 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wu', 'Xiaoyang', ''], ['DeTone', 'Daniel', ''], ['Frost', 'Duncan', ''], ['Shen', 'Tianwei', ''], ['Xie', 'Chris', ''], ['Yang', 'Nan', ''], ['Engel', 'Jakob', ''], ['Newcombe', 'Richard', ''], ['Zhao', 'Hengshuang', ''], ['Straub', 'Julian', '']]","extracted_entities":"[{'text': 'self-distillation', 'label': 'Knowledge distillation'}, {'text': 'zero-shot visualizations', 'label': 'Few-shot Learning'}, {'text': 'Full fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Fine-tuning","matched_keyword":"Full fine-tuning","similarity_score":0.9569243193}
{"id":2410.14675,"submitter":"Yukun Huang","authors":"Yukun Huang, Sanxing Chen, Hongyi Cai, Bhuwan Dhingra","title":"To Trust or Not to Trust? Enhancing Large Language Models' Situated\n  Faithfulness to External Contexts","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) are often augmented with external contexts, such\nas those used in retrieval-augmented generation (RAG). However, these contexts\ncan be inaccurate or intentionally misleading, leading to conflicts with the\nmodel's internal knowledge. We argue that robust LLMs should demonstrate\nsituated faithfulness, dynamically calibrating their trust in external\ninformation based on their confidence in the internal knowledge and the\nexternal context to resolve knowledge conflicts. To benchmark this capability,\nwe evaluate LLMs across several QA datasets, including a newly created dataset\nfeaturing in-the-wild incorrect contexts sourced from Reddit posts. We show\nthat when provided with both correct and incorrect contexts, both open-source\nand proprietary models tend to overly rely on external information, regardless\nof its factual accuracy. To enhance situated faithfulness, we propose two\napproaches: Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence\nReasoning (RCR). SCR enables models to self-assess the confidence of external\ninformation relative to their own internal knowledge to produce the most\naccurate answer. RCR, in contrast, extracts explicit confidence signals from\nthe LLM and determines the final answer using predefined rules. Our results\nshow that for LLMs with strong reasoning capabilities, such as GPT-4o and\nGPT-4o mini, SCR outperforms RCR, achieving improvements of up to 24.2% over a\ndirect input augmentation baseline. Conversely, for a smaller model like\nLlama-3-8B, RCR outperforms SCR. Fine-tuning SCR with our proposed Confidence\nReasoning Direct Preference Optimization (CR-DPO) method improves performance\non both seen and unseen datasets, yielding an average improvement of 8.9% on\nLlama-3-8B. In addition to quantitative results, we offer insights into the\nrelative strengths of SCR and RCR.\n","versions":"[{'version': 'v1', 'created': 'Fri, 18 Oct 2024 17:59:47 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 04:47:58 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Huang', 'Yukun', ''], ['Chen', 'Sanxing', ''], ['Cai', 'Hongyi', ''], ['Dhingra', 'Bhuwan', '']]","extracted_entities":"[{'text': 'RAG', 'label': 'RAG'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.01478,"submitter":"Yijie Xu","authors":"Lu Dai, Yijie Xu, Jinhui Ye, Hao Liu, Hui Xiong","title":"SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity\n  Reduction","comments":"ICLR 2025 Spotlight","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Large Language Models (LLMs) have demonstrated improved generation\nperformance by incorporating externally retrieved knowledge, a process known as\nretrieval-augmented generation (RAG). Despite the potential of this approach,\nexisting studies evaluate RAG effectiveness by 1) assessing retrieval and\ngeneration components jointly, which obscures retrieval's distinct\ncontribution, or 2) examining retrievers using traditional metrics such as\nNDCG, which creates a gap in understanding retrieval's true utility in the\noverall generation process. To address the above limitations, in this work, we\nintroduce an automatic evaluation method that measures retrieval quality\nthrough the lens of information gain within the RAG framework. Specifically, we\npropose Semantic Perplexity (SePer), a metric that captures the LLM's internal\nbelief about the correctness of the retrieved information. We quantify the\nutility of retrieval by the extent to which it reduces semantic perplexity\npost-retrieval. Extensive experiments demonstrate that SePer not only aligns\nclosely with human preferences but also offers a more precise and efficient\nevaluation of retrieval utility across diverse RAG scenarios.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 12:37:34 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Mar 2025 07:51:56 GMT'}, {'version': 'v3', 'created': 'Wed, 5 Mar 2025 05:24:54 GMT'}, {'version': 'v4', 'created': 'Wed, 12 Mar 2025 08:49:58 GMT'}, {'version': 'v5', 'created': 'Thu, 20 Mar 2025 11:28:41 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Dai', 'Lu', ''], ['Xu', 'Yijie', ''], ['Ye', 'Jinhui', ''], ['Liu', 'Hao', ''], ['Xiong', 'Hui', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'retrieval-augmented generation', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.05592,"submitter":"Huatong Song","authors":"Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen,\n  Wayne Xin Zhao, Lei Fang, Ji-Rong Wen","title":"R1-Searcher: Incentivizing the Search Capability in LLMs via\n  Reinforcement Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.IR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Existing Large Reasoning Models (LRMs) have shown the potential of\nreinforcement learning (RL) to enhance the complex reasoning capabilities of\nLarge Language Models~(LLMs). While they achieve remarkable performance on\nchallenging tasks such as mathematics and coding, they often rely on their\ninternal knowledge to solve problems, which can be inadequate for\ntime-sensitive or knowledge-intensive questions, leading to inaccuracies and\nhallucinations. To address this, we propose \\textbf{R1-Searcher}, a novel\ntwo-stage outcome-based RL approach designed to enhance the search capabilities\nof LLMs. This method allows LLMs to autonomously invoke external search systems\nto access additional knowledge during the reasoning process. Our framework\nrelies exclusively on RL, without requiring process rewards or distillation for\na cold start. % effectively generalizing to out-of-domain datasets and\nsupporting both Base and Instruct models. Our experiments demonstrate that our\nmethod significantly outperforms previous strong RAG methods, even when\ncompared to the closed-source GPT-4o-mini.\n","versions":"[{'version': 'v1', 'created': 'Fri, 7 Mar 2025 17:14:44 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 08:32:24 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Song', 'Huatong', ''], ['Jiang', 'Jinhao', ''], ['Min', 'Yingqian', ''], ['Chen', 'Jie', ''], ['Chen', 'Zhipeng', ''], ['Zhao', 'Wayne Xin', ''], ['Fang', 'Lei', ''], ['Wen', 'Ji-Rong', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'distillation', 'label': 'Knowledge distillation'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'GPT-4o-mini', 'label': 'GPT-4'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.10677,"submitter":"Mingyue Cheng","authors":"Mingyue Cheng, Yucong Luo, Jie Ouyang, Qi Liu, Huijie Liu, Li Li, Shuo\n  Yu, Bohou Zhang, Jiawei Cao, Jie Ma, Daoyu Wang, Enhong Chen","title":"A Survey on Knowledge-Oriented Retrieval-Augmented Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Retrieval-Augmented Generation (RAG) has gained significant attention in\nrecent years for its potential to enhance natural language understanding and\ngeneration by combining large-scale retrieval systems with generative models.\nRAG leverages external knowledge sources, such as documents, databases, or\nstructured data, to improve model performance and generate more accurate and\ncontextually relevant outputs. This survey aims to provide a comprehensive\noverview of RAG by examining its fundamental components, including retrieval\nmechanisms, generation processes, and the integration between the two. We\ndiscuss the key characteristics of RAG, such as its ability to augment\ngenerative models with dynamic external knowledge, and the challenges\nassociated with aligning retrieved information with generative objectives. We\nalso present a taxonomy that categorizes RAG methods, ranging from basic\nretrieval-augmented approaches to more advanced models incorporating\nmulti-modal data and reasoning capabilities. Additionally, we review the\nevaluation benchmarks and datasets commonly used to assess RAG systems, along\nwith a detailed exploration of its applications in fields such as question\nanswering, summarization, and information retrieval. Finally, we highlight\nemerging research directions and opportunities for improving RAG systems, such\nas enhanced retrieval efficiency, model interpretability, and domain-specific\nadaptations. This paper concludes by outlining the prospects for RAG in\naddressing real-world challenges and its potential to drive further\nadvancements in natural language processing.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 01:59:35 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 11:24:11 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Cheng', 'Mingyue', ''], ['Luo', 'Yucong', ''], ['Ouyang', 'Jie', ''], ['Liu', 'Qi', ''], ['Liu', 'Huijie', ''], ['Li', 'Li', ''], ['Yu', 'Shuo', ''], ['Zhang', 'Bohou', ''], ['Cao', 'Jiawei', ''], ['Ma', 'Jie', ''], ['Wang', 'Daoyu', ''], ['Chen', 'Enhong', '']]","extracted_entities":"[{'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'summarization', 'label': 'Knowledge distillation'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.12759,"submitter":"Jerry Huang","authors":"Jerry Huang, Siddarth Madala, Risham Sidhu, Cheng Niu, Julia\n  Hockenmaier, Tong Zhang","title":"RAG-RL: Advancing Retrieval-Augmented Generation via RL and Curriculum\n  Learning","comments":"11 Pages, 3 Figures, Preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent research highlights the challenges retrieval models face in retrieving\nuseful contexts and the limitations of generation models in effectively\nutilizing those contexts in retrieval-augmented generation (RAG) settings. To\naddress these challenges, we introduce RAG-RL, the first reasoning language\nmodel (RLM) specifically trained for RAG. RAG-RL demonstrates that stronger\nanswer generation models can identify relevant contexts within larger sets of\nretrieved information -- thereby alleviating the burden on retrievers -- while\nalso being able to utilize those contexts more effectively. Moreover, we show\nthat curriculum design in the reinforcement learning (RL) post-training process\nis a powerful approach to enhancing model performance. We benchmark our method\non two open-domain question-answering datasets and achieve state-of-the-art\nresults, surpassing previous SOTA generative reader models. In addition, we\noffers empirical insights into various curriculum learning strategies,\nproviding a deeper understanding of their impact on model performance.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 02:53:42 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Huang', 'Jerry', ''], ['Madala', 'Siddarth', ''], ['Sidhu', 'Risham', ''], ['Niu', 'Cheng', ''], ['Hockenmaier', 'Julia', ''], ['Zhang', 'Tong', '']]","extracted_entities":"[{'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.1331,"submitter":"Matteo Esposito","authors":"Matteo Esposito and Xiaozhou Li and Sergio Moreschini and Noman Ahmad\n  and Tomas Cerny and Karthik Vaidhyanathan and Valentina Lenarduzzi and Davide\n  Taibi","title":"Generative AI for Software Architecture. Applications, Trends,\n  Challenges, and Future Directions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI cs.DC cs.ET","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Context: Generative Artificial Intelligence (GenAI) is transforming much of\nsoftware development, yet its application in software architecture is still in\nits infancy, and no prior study has systematically addressed the topic. Aim: We\naim to systematically synthesize the use, rationale, contexts, usability, and\nfuture challenges of GenAI in software architecture. Method: We performed a\nmultivocal literature review (MLR), analyzing peer-reviewed and gray\nliterature, identifying current practices, models, adoption contexts, and\nreported challenges, extracting themes via open coding. Results: Our review\nidentified significant adoption of GenAI for architectural decision support and\narchitectural reconstruction. OpenAI GPT models are predominantly applied, and\nthere is consistent use of techniques such as few-shot prompting and\nretrieved-augmented generation (RAG). GenAI has been applied mostly to initial\nstages of the Software Development Life Cycle (SDLC), such as\nRequirements-to-Architecture and Architecture-to-Code. Monolithic and\nmicroservice architectures were the dominant targets. However, rigorous testing\nof GenAI outputs was typically missing from the studies. Among the most\nfrequent challenges are model precision, hallucinations, ethical aspects,\nprivacy issues, lack of architecture-specific datasets, and the absence of\nsound evaluation frameworks. Conclusions: GenAI shows significant potential in\nsoftware design, but several challenges remain on its path to greater adoption.\nResearch efforts should target designing general evaluation methodologies,\nhandling ethics and precision, increasing transparency and explainability, and\npromoting architecture-specific datasets and benchmarks to bridge the gap\nbetween theoretical possibilities and practical use.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:49:30 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Esposito', 'Matteo', ''], ['Li', 'Xiaozhou', ''], ['Moreschini', 'Sergio', ''], ['Ahmad', 'Noman', ''], ['Cerny', 'Tomas', ''], ['Vaidhyanathan', 'Karthik', ''], ['Lenarduzzi', 'Valentina', ''], ['Taibi', 'Davide', '']]","extracted_entities":"[{'text': 'few-shot prompting', 'label': 'Prompting'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'ethical aspects', 'label': 'AI Ethics'}, {'text': 'ethics', 'label': 'AI Ethics'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.13402,"submitter":"Farhad Rezazadeh","authors":"Farhad Rezazadeh, Amir Ashtari Gargari, Sandra Lagen, Houbing Song,\n  Dusit Niyato, and Lingjia Liu","title":"Toward Generative 6G Simulation: An Experimental Multi-Agent LLM and\n  ns-3 Integration","comments":"6 pages, 4 figures, 4 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.NI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The move toward open Sixth-Generation (6G) networks necessitates a novel\napproach to full-stack simulation environments for evaluating complex\ntechnology developments before prototyping and real-world implementation. This\npaper introduces an innovative approach\\footnote{A lightweight, mock version of\nthe code is available on GitHub at that combines a multi-agent framework with\nthe Network Simulator 3 (ns-3) to automate and optimize the generation,\ndebugging, execution, and analysis of complex 5G network scenarios. Our\nframework orchestrates a suite of specialized agents -- namely, the Simulation\nGeneration Agent, Test Designer Agent, Test Executor Agent, and Result\nInterpretation Agent -- using advanced LangChain coordination. The Simulation\nGeneration Agent employs a structured chain-of-thought (CoT) reasoning process,\nleveraging LLMs and retrieval-augmented generation (RAG) to translate natural\nlanguage simulation specifications into precise ns-3 scripts. Concurrently, the\nTest Designer Agent generates comprehensive automated test suites by\nintegrating knowledge retrieval techniques with dynamic test case synthesis.\nThe Test Executor Agent dynamically deploys and runs simulations, managing\ndependencies and parsing detailed performance metrics. At the same time, the\nResult Interpretation Agent utilizes LLM-driven analysis to extract actionable\ninsights from the simulation outputs. By integrating external resources such as\nlibrary documentation and ns-3 testing frameworks, our experimental approach\ncan enhance simulation accuracy and adaptability, reducing reliance on\nextensive programming expertise. A detailed case study using the ns-3 5G-LENA\nmodule validates the effectiveness of the proposed approach. The code\ngeneration process converges in an average of 1.8 iterations, has a syntax\nerror rate of 17.0%, a mean response time of 7.3 seconds, and receives a human\nevaluation score of 7.5.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:34:04 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Rezazadeh', 'Farhad', ''], ['Gargari', 'Amir Ashtari', ''], ['Lagen', 'Sandra', ''], ['Song', 'Houbing', ''], ['Niyato', 'Dusit', ''], ['Liu', 'Lingjia', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'LLM'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.13563,"submitter":"Pingyu Wu","authors":"Pingyu Wu, Daiheng Gao, Jing Tang, Huimin Chen, Wenbo Zhou, Weiming\n  Zhang and Nenghai Yu","title":"MES-RAG: Bringing Multi-modal, Entity-Storage, and Secure Enhancements\n  to RAG","comments":"NAACL 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.IR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Retrieval-Augmented Generation (RAG) improves Large Language Models (LLMs) by\nusing external knowledge, but it struggles with precise entity information\nretrieval. In this paper, we proposed MES-RAG framework, which enhances\nentity-specific query handling and provides accurate, secure, and consistent\nresponses. MES-RAG introduces proactive security measures that ensure system\nintegrity by applying protections prior to data access. Additionally, the\nsystem supports real-time multi-modal outputs, including text, images, audio,\nand video, seamlessly integrating into existing RAG architectures. Experimental\nresults demonstrate that MES-RAG significantly improves both accuracy and\nrecall, highlighting its effectiveness in advancing the security and utility of\nquestion-answering, increasing accuracy to 0.83 (+0.25) on targeted task. Our\ncode and data are available at https:\/\/github.com\/wpydcr\/MES-RAG.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 08:09:42 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wu', 'Pingyu', ''], ['Gao', 'Daiheng', ''], ['Tang', 'Jing', ''], ['Chen', 'Huimin', ''], ['Zhou', 'Wenbo', ''], ['Zhang', 'Weiming', ''], ['Yu', 'Nenghai', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.13654,"submitter":"Manisha Mukherjee","authors":"Manisha Mukherjee and Vincent J. Hellendoorn","title":"SOSecure: Safer Code Generation with RAG and StackOverflow Discussions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.CR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) are widely used for automated code generation.\nTheir reliance on infrequently updated pretraining data leaves them unaware of\nnewly discovered vulnerabilities and evolving security standards, making them\nprone to producing insecure code. In contrast, developer communities on Stack\nOverflow (SO) provide an ever-evolving repository of knowledge, where security\nvulnerabilities are actively discussed and addressed through collective\nexpertise. These community-driven insights remain largely untapped by LLMs.\nThis paper introduces SOSecure, a Retrieval-Augmented Generation (RAG) system\nthat leverages the collective security expertise found in SO discussions to\nimprove the security of LLM-generated code. We build a security-focused\nknowledge base by extracting SO answers and comments that explicitly identify\nvulnerabilities. Unlike common uses of RAG, SOSecure triggers after code has\nbeen generated to find discussions that identify flaws in similar code. These\nare used in a prompt to an LLM to consider revising the code. Evaluation across\nthree datasets (SALLM, LLMSecEval, and LMSys) show that SOSecure achieves\nstrong fix rates of 71.7%, 91.3%, and 96.7% respectively, compared to prompting\nGPT-4 without relevant discussions (49.1%, 56.5%, and 37.5%), and outperforms\nmultiple other baselines. SOSecure operates as a language-agnostic complement\nto existing LLMs, without requiring retraining or fine-tuning, making it easy\nto deploy. Our results underscore the importance of maintaining active\ndeveloper forums, which have dropped substantially in usage with LLM adoptions.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 19:03:36 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Mukherjee', 'Manisha', ''], ['Hellendoorn', 'Vincent J.', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.13882,"submitter":"Linwei Zheng","authors":"Zhengsheng Guo, Linwei Zheng, Xinyang Chen, Xuefeng Bai, Kehai Chen,\n  Min Zhang","title":"MoK-RAG: Mixture of Knowledge Paths Enhanced Retrieval-Augmented\n  Generation for Embodied AI Environments","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  While human cognition inherently retrieves information from diverse and\nspecialized knowledge sources during decision-making processes, current\nRetrieval-Augmented Generation (RAG) systems typically operate through\nsingle-source knowledge retrieval, leading to a cognitive-algorithmic\ndiscrepancy. To bridge this gap, we introduce MoK-RAG, a novel multi-source RAG\nframework that implements a mixture of knowledge paths enhanced retrieval\nmechanism through functional partitioning of a large language model (LLM)\ncorpus into distinct sections, enabling retrieval from multiple specialized\nknowledge paths. Applied to the generation of 3D simulated environments, our\nproposed MoK-RAG3D enhances this paradigm by partitioning 3D assets into\ndistinct sections and organizing them based on a hierarchical knowledge tree\nstructure. Different from previous methods that only use manual evaluation, we\npioneered the introduction of automated evaluation methods for 3D scenes. Both\nautomatic and human evaluations in our experiments demonstrate that MoK-RAG3D\ncan assist Embodied AI agents in generating diverse scenes.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 04:27:02 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Guo', 'Zhengsheng', ''], ['Zheng', 'Linwei', ''], ['Chen', 'Xinyang', ''], ['Bai', 'Xuefeng', ''], ['Chen', 'Kehai', ''], ['Zhang', 'Min', '']]","extracted_entities":"[{'text': 'MoK-RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"MoK-RAG","similarity_score":0.7114078999}
{"id":2503.13964,"submitter":"Siwei Han","authors":"Siwei Han, Peng Xia, Ruiyi Zhang, Tong Sun, Yun Li, Hongtu Zhu, Huaxiu\n  Yao","title":"MDocAgent: A Multi-Modal Multi-Agent Framework for Document\n  Understanding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Document Question Answering (DocQA) is a very common task. Existing methods\nusing Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and\nRetrieval Augmented Generation (RAG) often prioritize information from a single\nmodal, failing to effectively integrate textual and visual cues. These\napproaches struggle with complex multi-modal reasoning, limiting their\nperformance on real-world documents. We present MDocAgent (A Multi-Modal\nMulti-Agent Framework for Document Understanding), a novel RAG and multi-agent\nframework that leverages both text and image. Our system employs five\nspecialized agents: a general agent, a critical agent, a text agent, an image\nagent and a summarizing agent. These agents engage in multi-modal context\nretrieval, combining their individual insights to achieve a more comprehensive\nunderstanding of the document's content. This collaborative approach enables\nthe system to synthesize information from both textual and visual components,\nleading to improved accuracy in question answering. Preliminary experiments on\nfive benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of\nour MDocAgent, achieve an average improvement of 12.1% compared to current\nstate-of-the-art method. This work contributes to the development of more\nrobust and comprehensive DocQA systems capable of handling the complexities of\nreal-world documents containing rich textual and visual information. Our data\nand code are available at https:\/\/github.com\/aiming-lab\/MDocAgent.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 06:57:21 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Han', 'Siwei', ''], ['Xia', 'Peng', ''], ['Zhang', 'Ruiyi', ''], ['Sun', 'Tong', ''], ['Li', 'Yun', ''], ['Zhu', 'Hongtu', ''], ['Yao', 'Huaxiu', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'Large Vision Language Models', 'label': 'Large Language Model'}, {'text': 'Retrieval Augmented Generation (RAG)', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.14382,"submitter":"Takehito Utsuro","authors":"Rikuto Tsuchida, Hibiki Yokoyama, Takehito Utsuro","title":"Good\/Evil Reputation Judgment of Celebrities by LLMs via Retrieval\n  Augmented Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  The purpose of this paper is to examine whether large language models (LLMs)\ncan understand what is good and evil with respect to judging good\/evil\nreputation of celebrities. Specifically, we first apply a large language model\n(namely, ChatGPT) to the task of collecting sentences that mention the target\ncelebrity from articles about celebrities on Web pages. Next, the collected\nsentences are categorized based on their contents by ChatGPT, where ChatGPT\nassigns a category name to each of those categories. Those assigned category\nnames are referred to as \"aspects\" of each celebrity. Then, by applying the\nframework of retrieval augmented generation (RAG), we show that the large\nlanguage model is quite effective in the task of judging good\/evil reputation\nof aspects and descriptions of each celebrity. Finally, also in terms of\nproving the advantages of the proposed method over existing services\nincorporating RAG functions, we show that the proposed method of judging\ngood\/evil of aspects\/descriptions of each celebrity significantly outperform an\nexisting service incorporating RAG functions.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:15:55 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Tsuchida', 'Rikuto', ''], ['Yokoyama', 'Hibiki', ''], ['Utsuro', 'Takehito', '']]","extracted_entities":"[{'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'ChatGPT', 'label': 'ChatGPT'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.14603,"submitter":"Kyle Duffy","authors":"Yazeed Alnumay, Alexandre Barbet, Anna Bialas, William Darling, Shaan\n  Desai, Joan Devassy, Kyle Duffy, Stephanie Howe, Olivia Lasche, Justin Lee,\n  Anirudh Shrinivason, Jennifer Tracey","title":"Command R7B Arabic: A Small, Enterprise Focused, Multilingual, and\n  Culturally Aware Arabic LLM","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Building high-quality large language models (LLMs) for enterprise Arabic\napplications remains challenging due to the limited availability of digitized\nArabic data. In this work, we present a data synthesis and refinement strategy\nto help address this problem, namely, by leveraging synthetic data generation\nand human-in-the-loop annotation to expand our Arabic training corpus. We\nfurther present our iterative post training recipe that is essential to\nachieving state-of-the-art performance in aligning the model with human\npreferences, a critical aspect to enterprise use cases. The culmination of this\neffort is the release of a small, 7B, open-weight model that outperforms\nsimilarly sized peers in head-to-head comparisons and on Arabic-focused\nbenchmarks covering cultural knowledge, instruction following, RAG, and\ncontextual faithfulness.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 18:03:49 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Alnumay', 'Yazeed', ''], ['Barbet', 'Alexandre', ''], ['Bialas', 'Anna', ''], ['Darling', 'William', ''], ['Desai', 'Shaan', ''], ['Devassy', 'Joan', ''], ['Duffy', 'Kyle', ''], ['Howe', 'Stephanie', ''], ['Lasche', 'Olivia', ''], ['Lee', 'Justin', ''], ['Shrinivason', 'Anirudh', ''], ['Tracey', 'Jennifer', '']]","extracted_entities":"[{'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.14649,"submitter":"Wenqi Jiang","authors":"Wenqi Jiang, Suvinay Subramanian, Cat Graves, Gustavo Alonso, Amir\n  Yazdanbakhsh, Vidushi Dadu","title":"RAGO: Systematic Performance Optimization for Retrieval-Augmented\n  Generation Serving","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR cs.AI cs.CL cs.DC","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Retrieval-augmented generation (RAG), which combines large language models\n(LLMs) with retrievals from external knowledge databases, is emerging as a\npopular approach for reliable LLM serving. However, efficient RAG serving\nremains an open challenge due to the rapid emergence of many RAG variants and\nthe substantial differences in workload characteristics across them. In this\npaper, we make three fundamental contributions to advancing RAG serving. First,\nwe introduce RAGSchema, a structured abstraction that captures the wide range\nof RAG algorithms, serving as a foundation for performance optimization.\nSecond, we analyze several representative RAG workloads with distinct\nRAGSchema, revealing significant performance variability across these\nworkloads. Third, to address this variability and meet diverse performance\nrequirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a\nsystem optimization framework for efficient RAG serving. Our evaluation shows\nthat RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in\ntime-to-first-token latency compared to RAG systems built on LLM-system\nextensions.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 18:58:13 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Jiang', 'Wenqi', ''], ['Subramanian', 'Suvinay', ''], ['Graves', 'Cat', ''], ['Alonso', 'Gustavo', ''], ['Yazdanbakhsh', 'Amir', ''], ['Dadu', 'Vidushi', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'LLM-system\\nextensions', 'label': 'Foundation Model'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.14802,"submitter":"Md Shahir Zaoad","authors":"Md Shahir Zaoad, Niamat Zawad, Priyanka Ranade, Richard Krogman,\n  Latifur Khan, James Holt","title":"Graph-Based Re-ranking: Emerging Techniques, Limitations, and\n  Opportunities","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Knowledge graphs have emerged to be promising datastore candidates for\ncontext augmentation during Retrieval Augmented Generation (RAG). As a result,\ntechniques in graph representation learning have been simultaneously explored\nalongside principal neural information retrieval approaches, such as two-phased\nretrieval, also known as re-ranking. While Graph Neural Networks (GNNs) have\nbeen proposed to demonstrate proficiency in graph learning for re-ranking,\nthere are ongoing limitations in modeling and evaluating input graph structures\nfor training and evaluation for passage and document ranking tasks. In this\nsurvey, we review emerging GNN-based ranking model architectures along with\ntheir corresponding graph representation construction methodologies. We\nconclude by providing recommendations on future research based on\ncommunity-wide challenges and opportunities.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 00:28:54 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zaoad', 'Md Shahir', ''], ['Zawad', 'Niamat', ''], ['Ranade', 'Priyanka', ''], ['Krogman', 'Richard', ''], ['Khan', 'Latifur', ''], ['Holt', 'James', '']]","extracted_entities":"[{'text': 'RAG', 'label': 'RAG'}, {'text': 'two-phased\\nretrieval', 'label': 'Few-shot Learning'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.15204,"submitter":"Tittaya Mairittha","authors":"Tittaya Mairittha, Tanakon Sawanglok, Panuwit Raden, Sorrawit Treesuk","title":"When Pigs Get Sick: Multi-Agent AI for Swine Disease Detection","comments":"14 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC cs.AI cs.CL cs.IR cs.MA","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Swine disease surveillance is critical to the sustainability of global\nagriculture, yet its effectiveness is frequently undermined by limited\nveterinary resources, delayed identification of cases, and variability in\ndiagnostic accuracy. To overcome these barriers, we introduce a novel\nAI-powered, multi-agent diagnostic system that leverages Retrieval-Augmented\nGeneration (RAG) to deliver timely, evidence-based disease detection and\nclinical guidance. By automatically classifying user inputs into either\nKnowledge Retrieval Queries or Symptom-Based Diagnostic Queries, the system\nensures targeted information retrieval and facilitates precise diagnostic\nreasoning. An adaptive questioning protocol systematically collects relevant\nclinical signs, while a confidence-weighted decision fusion mechanism\nintegrates multiple diagnostic hypotheses to generate robust disease\npredictions and treatment recommendations. Comprehensive evaluations\nencompassing query classification, disease diagnosis, and knowledge retrieval\ndemonstrate that the system achieves high accuracy, rapid response times, and\nconsistent reliability. By providing a scalable, AI-driven diagnostic\nframework, this approach enhances veterinary decision-making, advances\nsustainable livestock management practices, and contributes substantively to\nthe realization of global food security.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 13:47:25 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Mairittha', 'Tittaya', ''], ['Sawanglok', 'Tanakon', ''], ['Raden', 'Panuwit', ''], ['Treesuk', 'Sorrawit', '']]","extracted_entities":"[{'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.15231,"submitter":"Jingyi Chen","authors":"Jingyi Chen, Songqiang Chen, Jialun Cao, Jiasi Shen, Shing-Chi Cheung","title":"When LLMs Meet API Documentation: Can Retrieval Augmentation Aid Code\n  Generation Just as It Helps Developers?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Retrieval-augmented generation (RAG) has increasingly shown its power in\nextending large language models' (LLMs') capability beyond their pre-trained\nknowledge. Existing works have shown that RAG can help with software\ndevelopment tasks such as code generation, code update, and test generation.\nYet, the effectiveness of adapting LLMs to fast-evolving or less common API\nlibraries using RAG remains unknown. To bridge this gap, we take an initial\nstep to study this unexplored yet practical setting - when developers code with\na less common library, they often refer to its API documentation; likewise,\nwhen LLMs are allowed to look up API documentation via RAG, to what extent can\nLLMs be advanced? To mimic such a setting, we select four less common\nopen-source Python libraries with a total of 1017 eligible APIs. We study the\nfactors that affect the effectiveness of using the documentation of less common\nAPI libraries as additional knowledge for retrieval and generation. Our\nintensive study yields interesting findings: (1) RAG helps improve LLMs'\nperformance by 83%-220%. (2) Example code contributes the most to advance LLMs,\ninstead of the descriptive texts and parameter lists in the API documentation.\n(3) LLMs could sometimes tolerate mild noises (typos in description or\nincorrect parameters) by referencing their pre-trained knowledge or document\ncontext. Finally, we suggest that developers pay more attention to the quality\nand diversity of the code examples in the API documentation. The study sheds\nlight on future low-code software development workflows.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 14:08:47 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Chen', 'Jingyi', ''], ['Chen', 'Songqiang', ''], ['Cao', 'Jialun', ''], ['Shen', 'Jiasi', ''], ['Cheung', 'Shing-Chi', '']]","extracted_entities":"[{'text': 'RAG', 'label': 'RAG'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.15548,"submitter":"Zhongliang Yang","authors":"Pengcheng Zhou, Yinglun Feng, Zhongliang Yang","title":"Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The widespread adoption of Retrieval-Augmented Generation (RAG) systems in\nreal-world applications has heightened concerns about the confidentiality and\nintegrity of their proprietary knowledge bases. These knowledge bases, which\nplay a critical role in enhancing the generative capabilities of Large Language\nModels (LLMs), are increasingly vulnerable to breaches that could compromise\nsensitive information. To address these challenges, this paper proposes an\nadvanced encryption methodology designed to protect RAG systems from\nunauthorized access and data leakage. Our approach encrypts both textual\ncontent and its corresponding embeddings prior to storage, ensuring that all\ndata remains securely encrypted. This mechanism restricts access to authorized\nentities with the appropriate decryption keys, thereby significantly reducing\nthe risk of unintended data exposure. Furthermore, we demonstrate that our\nencryption strategy preserves the performance and functionality of RAG\npipelines, ensuring compatibility across diverse domains and applications. To\nvalidate the robustness of our method, we provide comprehensive security proofs\nthat highlight its resilience against potential threats and vulnerabilities.\nThese proofs also reveal limitations in existing approaches, which often lack\nrobustness, adaptability, or reliance on open-source models. Our findings\nsuggest that integrating advanced encryption techniques into the design and\ndeployment of RAG systems can effectively enhance privacy safeguards. This\nresearch contributes to the ongoing discourse on improving security measures\nfor AI-driven services and advocates for stricter data protection standards\nwithin RAG architectures.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 07:45:05 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zhou', 'Pengcheng', ''], ['Feng', 'Yinglun', ''], ['Yang', 'Zhongliang', '']]","extracted_entities":"[{'text': 'Large Language\\nModels', 'label': 'Large Language Model'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'embeddings', 'label': 'Embedding'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.15569,"submitter":"Yun Tang","authors":"Jinsheng Yuan, Yun Tang, Weisi Guo","title":"RAG-based User Profiling for Precision Planning in Mixed-precision\n  Over-the-Air Federated Learning","comments":"5 pages, 4 figures, 2 tables, submitted to IEEE VTC 2025 fall for\n  possible publication","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.HC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Mixed-precision computing, a widely applied technique in AI, offers a larger\ntrade-off space between accuracy and efficiency. The recent purposed\nMixed-Precision Over-the-Air Federated Learning (MP-OTA-FL) enables clients to\noperate at appropriate precision levels based on their heterogeneous hardware,\ntaking advantages of the larger trade-off space while covering the quantization\noverheads in the mixed-precision modulation scheme for the OTA aggregation\nprocess. A key to further exploring the potential of the MP-OTA-FL framework is\nthe optimization of client precision levels. The choice of precision level\nhinges on multifaceted factors including hardware capability, potential client\ncontribution, and user satisfaction, among which factors can be difficult to\ndefine or quantify.\n  In this paper, we propose a RAG-based User Profiling for precision planning\nframework that integrates retrieval-augmented LLMs and dynamic client profiling\nto optimize satisfaction and contributions. This includes a hybrid interface\nfor gathering device\/user insights and an RAG database storing historical\nquantization decisions with feedback. Experiments show that our method boosts\nsatisfaction, energy savings, and global model accuracy in MP-OTA-FL systems.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 09:26:11 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Yuan', 'Jinsheng', ''], ['Tang', 'Yun', ''], ['Guo', 'Weisi', '']]","extracted_entities":"[{'text': 'MP-OTA-FL', 'label': 'Few-shot Learning'}, {'text': 'quantization\\noverheads', 'label': 'quantisation'}, {'text': 'MP-OTA-FL', 'label': 'Few-shot Learning'}, {'text': 'retrieval-augmented LLMs', 'label': 'LLMs'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'quantization', 'label': 'quantisation'}, {'text': 'MP-OTA-FL', 'label': 'Few-shot Learning'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.15664,"submitter":"Hisashi Johno","authors":"Hisashi Johno, Yuki Johno, Akitomo Amakawa, Junichi Sato, Ryota\n  Tozuka, Atsushi Komaba, Hiroaki Watanabe, Hiroki Watanabe, Chihiro Goto,\n  Hiroyuki Morisaka, Hiroshi Onishi, Kazunori Nakamoto","title":"Enhancing Pancreatic Cancer Staging with Large Language Models: The Role\n  of Retrieval-Augmented Generation","comments":"11 pages, 6 figures, 2 tables, 6 supplementary files","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Purpose: Retrieval-augmented generation (RAG) is a technology to enhance the\nfunctionality and reliability of large language models (LLMs) by retrieving\nrelevant information from reliable external knowledge (REK). RAG has gained\ninterest in radiology, and we previously reported the utility of NotebookLM, an\nLLM with RAG (RAG-LLM), for lung cancer staging. However, since the comparator\nLLM differed from NotebookLM's internal model, it remained unclear whether its\nadvantage stemmed from RAG or inherent model differences. To better isolate\nRAG's impact and assess its utility across different cancers, we compared\nNotebookLM with its internal LLM, Gemini 2.0 Flash, in a pancreatic cancer\nstaging experiment.\n  Materials and Methods: A summary of Japan's pancreatic cancer staging\nguidelines was used as REK. We compared three groups - REK+\/RAG+ (NotebookLM\nwith REK), REK+\/RAG- (Gemini 2.0 Flash with REK), and REK-\/RAG- (Gemini 2.0\nFlash without REK) - in staging 100 fictional pancreatic cancer cases based on\nCT findings. Staging criteria included TNM classification, local invasion\nfactors, and resectability classification. In REK+\/RAG+, retrieval accuracy was\nquantified based on the sufficiency of retrieved REK excerpts.\n  Results: REK+\/RAG+ achieved a staging accuracy of 70%, outperforming\nREK+\/RAG- (38%) and REK-\/RAG- (35%). For TNM classification, REK+\/RAG+ attained\n80% accuracy, exceeding REK+\/RAG- (55%) and REK-\/RAG- (50%). Additionally,\nREK+\/RAG+ explicitly presented retrieved REK excerpts, achieving a retrieval\naccuracy of 92%.\n  Conclusion: NotebookLM, a RAG-LLM, outperformed its internal LLM, Gemini 2.0\nFlash, in a pancreatic cancer staging experiment, suggesting that RAG may\nimprove LLM's staging accuracy. Furthermore, its ability to retrieve and\npresent REK excerpts provides transparency for physicians, highlighting its\napplicability for clinical diagnosis and classification.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 19:29:47 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Johno', 'Hisashi', ''], ['Johno', 'Yuki', ''], ['Amakawa', 'Akitomo', ''], ['Sato', 'Junichi', ''], ['Tozuka', 'Ryota', ''], ['Komaba', 'Atsushi', ''], ['Watanabe', 'Hiroaki', ''], ['Watanabe', 'Hiroki', ''], ['Goto', 'Chihiro', ''], ['Morisaka', 'Hiroyuki', ''], ['Onishi', 'Hiroshi', ''], ['Nakamoto', 'Kazunori', '']]","extracted_entities":"[{'text': 'RAG', 'label': 'RAG'}, {'text': 'REK', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'NotebookLM', 'label': 'LLM'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'REK', 'label': 'RAG'}, {'text': 'REK', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'REK', 'label': 'RAG'}, {'text': 'REK', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'REK', 'label': 'RAG'}, {'text': 'REK', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'REK', 'label': 'RAG'}, {'text': 'REK', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'REK', 'label': 'RAG'}, {'text': 'REK', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'REK', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'REK', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'REK', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'REK', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'REK', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'REK', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'REK', 'label': 'RAG'}, {'text': 'NotebookLM', 'label': 'LLM'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'REK', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.15879,"submitter":"DongGeon Lee","authors":"DongGeon Lee, Ahjeong Park, Hyeri Lee, Hyeonseo Nam, Yunho Maeng","title":"Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid\n  Question Answering","comments":"Accepted to NAACL 2025 SRW","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.IR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Non-factoid question-answering (NFQA) poses a significant challenge due to\nits open-ended nature, diverse intents, and the need for multi-aspect\nreasoning, which renders conventional factoid QA approaches, including\nretrieval-augmented generation (RAG), inadequate. Unlike factoid questions,\nnon-factoid questions (NFQs) lack definitive answers and require synthesizing\ninformation from multiple sources across various reasoning dimensions. To\naddress these limitations, we introduce Typed-RAG, a type-aware multi-aspect\ndecomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies\nNFQs into distinct types -- such as debate, experience, and comparison -- and\napplies aspect-based decomposition to refine retrieval and generation\nstrategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and\naggregating the results, Typed-RAG generates more informative and contextually\nrelevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark\ndataset covering diverse NFQ types. Experimental results demonstrate that\nTyped-RAG outperforms baselines, thereby highlighting the importance of\ntype-aware decomposition for effective retrieval and generation in NFQA. Our\ncode and dataset are available at\n\\href{https:\/\/github.com\/TeamNLP\/Typed-RAG}{https:\/\/github.com\/TeamNLP\/Typed-RAG}.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 06:04:12 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Lee', 'DongGeon', ''], ['Park', 'Ahjeong', ''], ['Lee', 'Hyeri', ''], ['Nam', 'Hyeonseo', ''], ['Maeng', 'Yunho', '']]","extracted_entities":"[{'text': 'RAG', 'label': 'RAG'}, {'text': 'Typed-RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.15888,"submitter":"Baolong Bi","authors":"Baolong Bi, Shenghua Liu, Yiwei Wang, Yilong Xu, Junfeng Fang, Lingrui\n  Mei, Xueqi Cheng","title":"Parameters vs. Context: Fine-Grained Control of Knowledge Reliance in\n  Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Retrieval-Augmented Generation (RAG) mitigates hallucinations in Large\nLanguage Models (LLMs) by integrating external knowledge. However, conflicts\nbetween parametric knowledge and retrieved context pose challenges,\nparticularly when retrieved information is unreliable or the model's internal\nknowledge is outdated. In such cases, LLMs struggle to determine whether to\nrely more on their own parameters or the conflicted context. To address this,\nwe propose **CK-PLUG**, a plug-and-play method for controlling LLMs' reliance\non parametric and contextual knowledge. We introduce a novel knowledge\nconsistency metric, Confidence Gain, which detects knowledge conflicts by\nmeasuring entropy shifts in token probability distributions after context\ninsertion. CK-PLUG then enables fine-grained control over knowledge preference\nby adjusting the probability distribution of tokens with negative confidence\ngain through a single tuning parameter. Experiments demonstrate CK-PLUG's\nability to significantly regulate knowledge reliance in counterfactual RAG\nscenarios while maintaining generation fluency and knowledge accuracy. For\ninstance, on Llama3-8B, memory recall (MR) of RAG response can be adjusted\nwithin a broad range (9.9%-71.9%), compared to the baseline of 42.1%. Moreover,\nCK-PLUG supports adaptive control based on the model's confidence in both\ninternal and external knowledge, achieving consistent performance improvements\nacross various general RAG tasks. Our code is available at:\n$\\href{https:\/\/github.com\/byronBBL\/CK-PLUG}{\\text{this https URL}}$.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 06:26:28 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Bi', 'Baolong', ''], ['Liu', 'Shenghua', ''], ['Wang', 'Yiwei', ''], ['Xu', 'Yilong', ''], ['Fang', 'Junfeng', ''], ['Mei', 'Lingrui', ''], ['Cheng', 'Xueqi', '']]","extracted_entities":"[{'text': 'Large\\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'fine-grained control', 'label': 'Fine-tuning'}, {'text': 'single tuning parameter', 'label': 'Fine-tuning'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'Llama3-8B', 'label': 'Llama'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.16071,"submitter":"Jiale Wei","authors":"Jiale Wei, Shuchi Wu, Ruochen Liu, Xiang Ying, Jingbo Shang, Fangbo\n  Tao","title":"Tuning LLMs by RAG Principles: Towards LLM-native Memory","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.IR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Memory, additional information beyond the training of large language models\n(LLMs), is crucial to various real-world applications, such as personal\nassistant. The two mainstream solutions to incorporate memory into the\ngeneration process are long-context LLMs and retrieval-augmented generation\n(RAG). In this paper, we first systematically compare these two types of\nsolutions on three renovated\/new datasets and show that (1) long-context\nsolutions, although more expensive, shall be easier to capture the big picture\nand better answer queries which require considering the memory as a whole; and\n(2) when the queries concern specific information, RAG solutions shall be more\ncompetitive especially when the keywords can be explicitly matched. Therefore,\nwe propose a novel method RAG-Tuned-LLM which fine-tunes a relative small\n(e.g., 7B) LLM using the data generated following the RAG principles, so it can\ncombine the advantages of both solutions. Extensive experiments on three\ndatasets demonstrate that RAG-Tuned-LLM can beat long-context LLMs and RAG\nmethods across a wide range of query types.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 12:04:40 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wei', 'Jiale', ''], ['Wu', 'Shuchi', ''], ['Liu', 'Ruochen', ''], ['Ying', 'Xiang', ''], ['Shang', 'Jingbo', ''], ['Tao', 'Fangbo', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.16161,"submitter":"Alex-R\\u{a}zvan Ispas","authors":"Alex-Razvan Ispas, Charles-Elie Simon, Fabien Caspani, Vincent Guigue","title":"Towards Lighter and Robust Evaluation for Retrieval Augmented Generation","comments":"17 pages, 5 figures, published at 1st workshop of Quantify\n  Uncertainty and Hallucination in Foundation Models: The Next Frontier in\n  Reliable AI at ICLR 25","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models are prompting us to view more NLP tasks from a\ngenerative perspective. At the same time, they offer a new way of accessing\ninformation, mainly through the RAG framework. While there have been notable\nimprovements for the autoregressive models, overcoming hallucination in the\ngenerated answers remains a continuous problem. A standard solution is to use\ncommercial LLMs, such as GPT4, to evaluate these algorithms. However, such\nframeworks are expensive and not very transparent. Therefore, we propose a\nstudy which demonstrates the interest of open-weight models for evaluating RAG\nhallucination. We develop a lightweight approach using smaller, quantized LLMs\nto provide an accessible and interpretable metric that gives continuous scores\nfor the generated answer with respect to their correctness and faithfulness.\nThis score allows us to question decisions' reliability and explore thresholds\nto develop a new AUC metric as an alternative to correlation with human\njudgment.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:58:32 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Ispas', 'Alex-Razvan', ''], ['Simon', 'Charles-Elie', ''], ['Caspani', 'Fabien', ''], ['Guigue', 'Vincent', '']]","extracted_entities":"[{'text': 'prompting', 'label': 'Prompting'}, {'text': 'RAG', 'label': 'RAG'}, {'text': 'GPT4', 'label': 'GPT'}, {'text': 'RAG', 'label': 'RAG'}]","assigned_concept":"RAG","matched_keyword":"RAG","similarity_score":1.0000001192}
{"id":2503.16,"submitter":"Haohua Que","authors":"Haojia Gao, Haohua Que, Hoiian Au, Weihao Shan, Mingkai Liu, Yusen\n  Qin, Lei Mu, Rong Zhao, Xinghua Yang, Qi Wei and Fei Qiao","title":"SenseExpo: Efficient Autonomous Exploration with Prediction Information\n  from Lightweight Neural Networks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper proposes SenseExpo, an efficient autonomous exploration framework\nbased on a lightweight prediction network, which addresses the limitations of\ntraditional methods in computational overhead and environmental generalization.\nBy integrating Generative Adversarial Networks (GANs), Transformer, and Fast\nFourier Convolution (FFC), we designed a lightweight prediction model with\nmerely 709k parameters. Our smallest model achieves better performance on the\nKTH dataset than U-net (24.5M) and LaMa (51M), delivering PSNR 9.026 and SSIM\n0.718, particularly representing a 38.7% PSNR improvement over the\n51M-parameter LaMa model. Cross-domain testing demonstrates its strong\ngeneralization capability, with an FID score of 161.55 on the HouseExpo\ndataset, significantly outperforming comparable methods. Regarding exploration\nefficiency, on the KTH dataset,SenseExpo demonstrates approximately a 67.9%\ntime reduction in exploration time compared to MapEx. On the MRPB 1.0 dataset,\nSenseExpo achieves 77.1% time reduction roughly compared to MapEx. Deployed as\na plug-and-play ROS node, the framework seamlessly integrates with existing\nnavigation systems, providing an efficient solution for resource-constrained\ndevices.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 10:07:51 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Gao', 'Haojia', ''], ['Que', 'Haohua', ''], ['Au', 'Hoiian', ''], ['Shan', 'Weihao', ''], ['Liu', 'Mingkai', ''], ['Qin', 'Yusen', ''], ['Mu', 'Lei', ''], ['Zhao', 'Rong', ''], ['Yang', 'Xinghua', ''], ['Wei', 'Qi', ''], ['Qiao', 'Fei', '']]","extracted_entities":"[{'text': 'Transformer', 'label': 'Generative Pre-trained Transformer (GPT)'}]","assigned_concept":"Generative Pre-trained Transformer (GPT)","matched_keyword":"Transformer","similarity_score":0.5537428856}
{"id":2211.14312,"submitter":"Zahra Shamsi","authors":"Zahra Shamsi, Drew Bryant, Jacob Wilson, Xiaoyu Qu, Avinava Dubey,\n  Konik Kothari, Mostafa Dehghani, Mariya Chavarha, Valerii Likhosherstov,\n  Brian Williams, Michael Frumkin, Fred Appelbaum, Krzysztof Choromanski, Ali\n  Bashir, Min Fang","title":"Karyotype AI for Precision Oncology","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"q-bio.QM cs.CV cs.LG eess.IV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present a machine learning method capable of accurately detecting\nchromosome abnormalities that cause blood cancers directly from microscope\nimages of the metaphase stage of cell division. The pipeline is built on a\nseries of fine-tuned Vision Transformers. Current state of the art (and\nstandard clinical practice) requires expensive, manual expert analysis, whereas\nour pipeline takes only 15 seconds per metaphase image. Using a novel\npretraining-finetuning strategy to mitigate the challenge of data scarcity, we\nachieve a high precision-recall score of 94% AUC for the clinically significant\ndel(5q) and t(9;22) anomalies. Our method also unlocks zero-shot detection of\nrare aberrations based on model latent embeddings. The ability to quickly,\naccurately, and scalably diagnose genetic abnormalities directly from metaphase\nimages could transform karyotyping practice and improve patient outcomes. We\nwill make code publicly available.\n","versions":"[{'version': 'v1', 'created': 'Sun, 20 Nov 2022 04:59:23 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Nov 2022 19:16:06 GMT'}, {'version': 'v3', 'created': 'Thu, 19 Oct 2023 20:58:13 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 17:19:33 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Shamsi', 'Zahra', ''], ['Bryant', 'Drew', ''], ['Wilson', 'Jacob', ''], ['Qu', 'Xiaoyu', ''], ['Dubey', 'Avinava', ''], ['Kothari', 'Konik', ''], ['Dehghani', 'Mostafa', ''], ['Chavarha', 'Mariya', ''], ['Likhosherstov', 'Valerii', ''], ['Williams', 'Brian', ''], ['Frumkin', 'Michael', ''], ['Appelbaum', 'Fred', ''], ['Choromanski', 'Krzysztof', ''], ['Bashir', 'Ali', ''], ['Fang', 'Min', '']]","extracted_entities":"[{'text': 'zero-shot detection', 'label': 'Zero-shot Learning'}, {'text': 'model latent embeddings', 'label': 'Embedding'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot detection","similarity_score":0.7759316564}
{"id":2406.11624,"submitter":"\\\"Omer \\c{S}ahin Ta\\c{s}","authors":"Omer Sahin Tas and Royden Wagner","title":"Words in Motion: Extracting Interpretable Control Vectors for Motion\n  Transformers","comments":"ICLR 2025 camera-ready. Our implementation is available at\n  github.com\/kit-mrt\/future-motion","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Transformer-based models generate hidden states that are difficult to\ninterpret. In this work, we analyze hidden states and modify them at inference,\nwith a focus on motion forecasting. We use linear probing to analyze whether\ninterpretable features are embedded in hidden states. Our experiments reveal\nhigh probing accuracy, indicating latent space regularities with functionally\nimportant directions. Building on this, we use the directions between hidden\nstates with opposing features to fit control vectors. At inference, we add our\ncontrol vectors to hidden states and evaluate their impact on predictions.\nRemarkably, such modifications preserve the feasibility of predictions. We\nfurther refine our control vectors using sparse autoencoders (SAEs). This leads\nto more linear changes in predictions when scaling control vectors. Our\napproach enables mechanistic interpretation as well as zero-shot generalization\nto unseen dataset characteristics with negligible computational overhead.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Jun 2024 15:07:55 GMT'}, {'version': 'v2', 'created': 'Mon, 14 Oct 2024 22:39:55 GMT'}, {'version': 'v3', 'created': 'Thu, 5 Dec 2024 11:47:49 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 12:06:17 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Tas', 'Omer Sahin', ''], ['Wagner', 'Royden', '']]","extracted_entities":"[{'text': 'zero-shot generalization', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot generalization","similarity_score":0.8053859472}
{"id":2406.12757,"submitter":"Shuo Xu","authors":"Shuo Xu and Sai Wang and Xinyue Hu and Yutian Lin and Sibei Yang and\n  Yu Wu","title":"MAC: A Benchmark for Multiple Attributes Compositional Zero-Shot\n  Learning","comments":"13pages,5figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Compositional Zero-Shot Learning (CZSL) aims to learn semantic primitives\n(attributes and objects) from seen compositions and recognize unseen\nattribute-object compositions. Existing CZSL datasets focus on single\nattributes, neglecting the fact that objects naturally exhibit multiple\ninterrelated attributes. Their narrow attribute scope and single attribute\nlabeling introduce annotation biases, misleading the learning of attributes and\ncausing inaccurate evaluation. To address these issues, we introduce the\nMulti-Attribute Composition (MAC) dataset, encompassing 22,838 images and\n17,627 compositions with comprehensive and representative attribute\nannotations. MAC shows complex relationship between attributes and objects,\nwith each attribute type linked to an average of 82.2 object types, and each\nobject type associated with 31.4 attribute types. Based on MAC, we propose\nmulti-attribute compositional zero-shot learning that requires deeper semantic\nunderstanding and advanced attribute associations, establishing a more\nrealistic and challenging benchmark for CZSL. We also propose Multi-attribute\nVisual-Primitive Integrator (MVP-Integrator), a robust baseline for\nmulti-attribute CZSL, which disentangles semantic primitives and performs\neffective visual-primitive association. Experimental results demonstrate that\nMVP-Integrator significantly outperforms existing CZSL methods on MAC with\nimproved inference efficiency.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Jun 2024 16:24:48 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 16:51:43 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 06:49:14 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Xu', 'Shuo', ''], ['Wang', 'Sai', ''], ['Hu', 'Xinyue', ''], ['Lin', 'Yutian', ''], ['Yang', 'Sibei', ''], ['Wu', 'Yu', '']]","extracted_entities":"[{'text': 'Compositional Zero-Shot Learning', 'label': 'Zero-shot Learning'}, {'text': 'CZSL', 'label': 'Zero-shot Learning'}, {'text': 'CZSL', 'label': 'Zero-shot Learning'}, {'text': 'multi-attribute compositional zero-shot learning', 'label': 'Zero-shot Learning'}, {'text': 'CZSL', 'label': 'Zero-shot Learning'}, {'text': 'CZSL', 'label': 'Zero-shot Learning'}, {'text': 'CZSL', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"Compositional Zero-Shot Learning","similarity_score":0.8895157576}
{"id":2408.12629,"submitter":"Zhenyu Lu","authors":"Zhenyu Lu, Hao Tang","title":"Continual Gesture Learning without Data via Synthetic Feature Sampling","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Data-Free Class Incremental Learning (DFCIL) aims to enable models to\ncontinuously learn new classes while retraining knowledge of old classes, even\nwhen the training data for old classes is unavailable. Although explored\nprimarily with image datasets by researchers, this study focuses on\ninvestigating DFCIL for skeleton-based gesture classification due to its\nsignificant real-world implications, particularly considering the growing\nprevalence of VR\/AR headsets where gestures serve as the primary means of\ncontrol and interaction. In this work, we made an intriguing observation:\nskeleton models trained with base classes(even very limited) demonstrate strong\ngeneralization capabilities to unseen classes without requiring additional\ntraining. Building on this insight, we developed Synthetic Feature Replay (SFR)\nthat can sample synthetic features from class prototypes to replay for old\nclasses and augment for new classes (under a few-shot setting). Our proposed\nmethod showcases significant advancements over the state-of-the-art, achieving\nup to 15% enhancements in mean accuracy across all steps and largely mitigating\nthe accuracy imbalance between base classes and new classes.\n","versions":"[{'version': 'v1', 'created': 'Wed, 21 Aug 2024 18:44:15 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 20:54:43 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Lu', 'Zhenyu', ''], ['Tang', 'Hao', '']]","extracted_entities":"[{'text': 'DFCIL', 'label': 'Few-shot Learning'}, {'text': 'base classes', 'label': 'Foundation Model'}, {'text': 'few-shot setting', 'label': 'Zero-shot Learning'}, {'text': 'base classes', 'label': 'Foundation Model'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"few-shot setting","similarity_score":0.5505114198}
{"id":2411.11223,"submitter":"Haoxing Chen","authors":"Haoxing Chen and Zizheng Huang and Yan Hong and Yanshuo Wang and\n  Zhongcai Lyu and Zhuoer Xu and Jun Lan and Zhangxuan Gu","title":"Efficient Transfer Learning for Video-language Foundation Models","comments":"Accepted by CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Pre-trained vision-language models provide a robust foundation for efficient\ntransfer learning across various downstream tasks. In the field of video action\nrecognition, mainstream approaches often introduce additional modules to\ncapture temporal information. Although the additional modules increase the\ncapacity of model, enabling it to better capture video-specific inductive\nbiases, existing methods typically introduce a substantial number of new\nparameters and are prone to catastrophic forgetting of previously acquired\ngeneralizable knowledge. In this paper, we propose a parameter-efficient\nMulti-modal Spatio-Temporal Adapter (MSTA) to enhance the alignment between\ntextual and visual representations, achieving a balance between generalizable\nknowledge and task-specific adaptation. Furthermore, to mitigate over-fitting\nand enhance generalizability, we introduce a spatio-temporal description-guided\nconsistency constraint.This constraint involves providing template inputs\n(e.g., \"a video of \\{\\textbf{cls}\\}\") to the trainable language branch and\nLLM-generated spatio-temporal descriptions to the pre-trained language branch,\nenforcing output consistency between the branches. This approach reduces\noverfitting to downstream tasks and enhances the distinguishability of the\ntrainable branch within the spatio-temporal semantic space. We evaluate the\neffectiveness of our approach across four tasks: zero-shot transfer, few-shot\nlearning, base-to-novel generalization, and fully-supervised learning. Compared\nto many state-of-the-art methods, our MSTA achieves outstanding performance\nacross all evaluations, while using only 2-7\\% of the trainable parameters in\nthe original model.\n","versions":"[{'version': 'v1', 'created': 'Mon, 18 Nov 2024 01:25:58 GMT'}, {'version': 'v2', 'created': 'Wed, 18 Dec 2024 08:49:16 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 06:44:59 GMT'}, {'version': 'v4', 'created': 'Tue, 18 Mar 2025 02:51:43 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Chen', 'Haoxing', ''], ['Huang', 'Zizheng', ''], ['Hong', 'Yan', ''], ['Wang', 'Yanshuo', ''], ['Lyu', 'Zhongcai', ''], ['Xu', 'Zhuoer', ''], ['Lan', 'Jun', ''], ['Gu', 'Zhangxuan', '']]","extracted_entities":"[{'text': 'zero-shot transfer', 'label': 'Zero-shot Learning'}, {'text': 'few-shot\\nlearning', 'label': 'Zero-shot Learning'}, {'text': 'fully-supervised learning', 'label': 'Zero-shot Learning'}, {'text': 'MSTA', 'label': 'Foundation Model'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"few-shot\nlearning","similarity_score":0.8116950989}
{"id":2501.02189,"submitter":"Zongxia Li","authors":"Zongxia Li, Xiyang Wu, Hongyang Du, Huy Nghiem, Guangyao Shi","title":"A Survey of State of the Art Large Vision Language Models: Alignment,\n  Benchmark, Evaluations and Challenges","comments":"22 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL cs.LG cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multimodal Vision Language Models (VLMs) have emerged as a transformative\ntechnology at the intersection of computer vision and natural language\nprocessing, enabling machines to perceive and reason about the world through\nboth visual and textual modalities. For example, models such as CLIP, Claude,\nand GPT-4V demonstrate strong reasoning and understanding abilities on visual\nand textual data and beat classical single modality vision models on zero-shot\nclassification. Despite their rapid advancements in research and growing\npopularity in applications, a comprehensive survey of existing studies on VLMs\nis notably lacking, particularly for researchers aiming to leverage VLMs in\ntheir specific domains. To this end, we provide a systematic overview of VLMs\nin the following aspects: model information of the major VLMs developed over\nthe past five years (2019-2024); the main architectures and training methods of\nthese VLMs; summary and categorization of the popular benchmarks and evaluation\nmetrics of VLMs; the applications of VLMs including embodied agents, robotics,\nand video generation; the challenges and issues faced by current VLMs such as\nhallucination, fairness, and safety. Detailed collections including papers and\nmodel repository links are listed in\nhttps:\/\/github.com\/zli12321\/Vision-Language-Models-Overview.\n","versions":"[{'version': 'v1', 'created': 'Sat, 4 Jan 2025 04:59:33 GMT'}, {'version': 'v2', 'created': 'Fri, 10 Jan 2025 17:43:10 GMT'}, {'version': 'v3', 'created': 'Wed, 29 Jan 2025 00:26:29 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Mar 2025 19:50:08 GMT'}, {'version': 'v5', 'created': 'Mon, 17 Mar 2025 02:24:48 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Li', 'Zongxia', ''], ['Wu', 'Xiyang', ''], ['Du', 'Hongyang', ''], ['Nghiem', 'Huy', ''], ['Shi', 'Guangyao', '']]","extracted_entities":"[{'text': 'GPT-4V', 'label': 'GPT'}, {'text': 'zero-shot\\nclassification', 'label': 'Zero-shot Learning'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'safety', 'label': 'AI Ethics'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot\nclassification","similarity_score":0.8807804585}
{"id":2502.19739,"submitter":"Di Liu","authors":"Di Liu, Teng Deng, Giljoo Nam, Yu Rong, Stanislav Pidhorskyi, Junxuan\n  Li, Jason Saragih, Dimitris N. Metaxas, Chen Cao","title":"LUCAS: Layered Universal Codec Avatars","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Photorealistic 3D head avatar reconstruction faces critical challenges in\nmodeling dynamic face-hair interactions and achieving cross-identity\ngeneralization, particularly during expressions and head movements. We present\nLUCAS, a novel Universal Prior Model (UPM) for codec avatar modeling that\ndisentangles face and hair through a layered representation. Unlike previous\nUPMs that treat hair as an integral part of the head, our approach separates\nthe modeling of the hairless head and hair into distinct branches. LUCAS is the\nfirst to introduce a mesh-based UPM, facilitating real-time rendering on\ndevices. Our layered representation also improves the anchor geometry for\nprecise and visually appealing Gaussian renderings. Experimental results\nindicate that LUCAS outperforms existing single-mesh and Gaussian-based avatar\nmodels in both quantitative and qualitative assessments, including evaluations\non held-out subjects in zero-shot driving scenarios. LUCAS demonstrates\nsuperior dynamic performance in managing head pose changes, expression\ntransfer, and hairstyle variations, thereby advancing the state-of-the-art in\n3D head avatar reconstruction.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 04:07:27 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 20:38:27 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Liu', 'Di', ''], ['Deng', 'Teng', ''], ['Nam', 'Giljoo', ''], ['Rong', 'Yu', ''], ['Pidhorskyi', 'Stanislav', ''], ['Li', 'Junxuan', ''], ['Saragih', 'Jason', ''], ['Metaxas', 'Dimitris N.', ''], ['Cao', 'Chen', '']]","extracted_entities":"[{'text': 'layered representation', 'label': 'Embedding'}, {'text': 'layered representation', 'label': 'Embedding'}, {'text': 'zero-shot driving scenarios', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot driving scenarios","similarity_score":0.6070929766}
{"id":2503.14494,"submitter":"Inkyu Shin","authors":"Inkyu Shin, Chenglin Yang, Liang-Chieh Chen","title":"Deeply Supervised Flow-Based Generative Models","comments":"Project website at https:\/\/deepflow-project.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Flow based generative models have charted an impressive path across multiple\nvisual generation tasks by adhering to a simple principle: learning velocity\nrepresentations of a linear interpolant. However, we observe that training\nvelocity solely from the final layer output underutilizes the rich inter layer\nrepresentations, potentially impeding model convergence. To address this\nlimitation, we introduce DeepFlow, a novel framework that enhances velocity\nrepresentation through inter layer communication. DeepFlow partitions\ntransformer layers into balanced branches with deep supervision and inserts a\nlightweight Velocity Refiner with Acceleration (VeRA) block between adjacent\nbranches, which aligns the intermediate velocity features within transformer\nblocks. Powered by the improved deep supervision via the internal velocity\nalignment, DeepFlow converges 8 times faster on ImageNet with equivalent\nperformance and further reduces FID by 2.6 while halving training time compared\nto previous flow based models without a classifier free guidance. DeepFlow also\noutperforms baselines in text to image generation tasks, as evidenced by\nevaluations on MSCOCO and zero shot GenEval.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:58:08 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Shin', 'Inkyu', ''], ['Yang', 'Chenglin', ''], ['Chen', 'Liang-Chieh', '']]","extracted_entities":"[{'text': 'zero shot GenEval', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero shot GenEval","similarity_score":0.554028511}
{"id":2503.14809,"submitter":"Jeff Jewett","authors":"Jeff Jewett and Sandhya Saisubramanian","title":"Learning with Expert Abstractions for Efficient Multi-Task Continuous\n  Control","comments":"12 pages, 6 figures. Submitted to RLC 2025. Code and experiments at\n  https:\/\/github.com\/Intelligent-Reliable-Autonomous-Systems\/gcrs-expert-abstractions","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Decision-making in complex, continuous multi-task environments is often\nhindered by the difficulty of obtaining accurate models for planning and the\ninefficiency of learning purely from trial and error. While precise environment\ndynamics may be hard to specify, human experts can often provide high-fidelity\nabstractions that capture the essential high-level structure of a task and user\npreferences in the target environment. Existing hierarchical approaches often\ntarget discrete settings and do not generalize across tasks. We propose a\nhierarchical reinforcement learning approach that addresses these limitations\nby dynamically planning over the expert-specified abstraction to generate\nsubgoals to learn a goal-conditioned policy. To overcome the challenges of\nlearning under sparse rewards, we shape the reward based on the optimal state\nvalue in the abstract model. This structured decision-making process enhances\nsample efficiency and facilitates zero-shot generalization. Our empirical\nevaluation on a suite of procedurally generated continuous control environments\ndemonstrates that our approach outperforms existing hierarchical reinforcement\nlearning methods in terms of sample efficiency, task completion rate,\nscalability to complex tasks, and generalization to novel scenarios.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 00:44:23 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Jewett', 'Jeff', ''], ['Saisubramanian', 'Sandhya', '']]","extracted_entities":"[{'text': 'zero-shot generalization', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot generalization","similarity_score":0.8053859472}
{"id":2503.14919,"submitter":"Junyu Shi","authors":"Junyu Shi and Lijiang Liu and Yong Sun and Zhiyuan Zhang and Jinni\n  Zhou and Qiang Nie","title":"GenM$^3$: Generative Pretrained Multi-path Motion Model for Text\n  Conditional Human Motion Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Scaling up motion datasets is crucial to enhance motion generation\ncapabilities. However, training on large-scale multi-source datasets introduces\ndata heterogeneity challenges due to variations in motion content. To address\nthis, we propose Generative Pretrained Multi-path Motion Model (GenM$^3$), a\ncomprehensive framework designed to learn unified motion representations.\nGenM$^3$ comprises two components: 1) a Multi-Expert VQ-VAE (MEVQ-VAE) that\nadapts to different dataset distributions to learn a unified discrete motion\nrepresentation, and 2) a Multi-path Motion Transformer (MMT) that improves\nintra-modal representations by using separate modality-specific pathways, each\nwith densely activated experts to accommodate variations within that modality,\nand improves inter-modal alignment by the text-motion shared pathway. To enable\nlarge-scale training, we integrate and unify 11 high-quality motion datasets\n(approximately 220 hours of motion data) and augment it with textual\nannotations (nearly 10,000 motion sequences labeled by a large language model\nand 300+ by human experts). After training on our integrated dataset, GenM$^3$\nachieves a state-of-the-art FID of 0.035 on the HumanML3D benchmark, surpassing\nstate-of-the-art methods by a large margin. It also demonstrates strong\nzero-shot generalization on IDEA400 dataset, highlighting its effectiveness and\nadaptability across diverse motion scenarios.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 05:56:52 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Shi', 'Junyu', ''], ['Liu', 'Lijiang', ''], ['Sun', 'Yong', ''], ['Zhang', 'Zhiyuan', ''], ['Zhou', 'Jinni', ''], ['Nie', 'Qiang', '']]","extracted_entities":"[{'text': 'zero-shot generalization', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot generalization","similarity_score":0.8053859472}
{"id":2503.15004,"submitter":"Tristan Wirth","authors":"Annalena Bl\\\"ansdorf, Tristan Wirth, Arne Rak, Thomas P\\\"ollabauer,\n  Volker Knauthe, Arjan Kuijper","title":"Semantic Segmentation of Transparent and Opaque Drinking Glasses with\n  the Help of Zero-shot Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Segmenting transparent structures in images is challenging since they are\ndifficult to distinguish from the background. Common examples are drinking\nglasses, which are a ubiquitous part of our lives and appear in many different\nshapes and sizes. In this work we propose TransCaGNet, a modified version of\nthe zero-shot model CaGNet. We exchange the segmentation backbone with the\narchitecture of Trans4Trans to be capable of segmenting transparent objects.\nSince some glasses are rarely captured, we use zeroshot learning to be able to\ncreate semantic segmentations of glass categories not given during training. We\npropose a novel synthetic dataset covering a diverse set of different\nenvironmental conditions. Additionally we capture a real-world evaluation\ndataset since most applications take place in the real world. Comparing our\nmodel with Zeg-Clip we are able to show that TransCaGNet produces better mean\nIoU and accuracy values while ZegClip outperforms it mostly for unseen classes.\nTo improve the segmentation results, we combine the semantic segmentation of\nthe models with the segmentation results of SAM 2. Our evaluation emphasizes\nthat distinguishing between different classes is challenging for the models due\nto similarity, points of view, or coverings. Taking this behavior into account,\nwe assign glasses multiple possible categories. The modification leads to an\nimprovement up to 13.68% for the mean IoU and up to 17.88% for the mean\naccuracy values on the synthetic dataset. Using our difficult synthetic dataset\nfor training, the models produce even better results on the real-world dataset.\nThe mean IoU is improved up to 5.55% and the mean accuracy up to 5.72% on the\nreal-world dataset.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 08:54:14 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Bl\u00e4nsdorf', 'Annalena', ''], ['Wirth', 'Tristan', ''], ['Rak', 'Arne', ''], ['P\u00f6llabauer', 'Thomas', ''], ['Knauthe', 'Volker', ''], ['Kuijper', 'Arjan', '']]","extracted_entities":"[{'text': 'zeroshot learning', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zeroshot learning","similarity_score":0.8522433639}
{"id":2503.15485,"submitter":"David Chan","authors":"Zineng Tang, Long Lian, Seun Eisape, XuDong Wang, Roei Herzig, Adam\n  Yala, Alane Suhr, Trevor Darrell, David M. Chan","title":"TULIP: Towards Unified Language-Image Pretraining","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image\/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code\/checkpoints are available at\nhttps:\/\/tulip-berkeley.github.io\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:58:57 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Tang', 'Zineng', ''], ['Lian', 'Long', ''], ['Eisape', 'Seun', ''], ['Wang', 'XuDong', ''], ['Herzig', 'Roei', ''], ['Yala', 'Adam', ''], ['Suhr', 'Alane', ''], ['Darrell', 'Trevor', ''], ['Chan', 'David M.', '']]","extracted_entities":"[{'text': 'enhanced image-image\\nand text-text contrastive learning', 'label': 'Few-shot Learning'}, {'text': 'few-shot\\nclassification', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"few-shot\nclassification","similarity_score":0.7306973934}
{"id":2503.15578,"submitter":"Jiexia Ye","authors":"Jiexia Ye, Weiqi Zhang, Ziyue Li, Jia Li, Fugee Tsung","title":"Sparseformer: a Transferable Transformer with Multi-granularity Token\n  Sparsification for Medical Time Series Classification","comments":"3 figures, 16 pages, 5 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Medical time series (MedTS) classification is crucial for improved diagnosis\nin healthcare, and yet it is challenging due to the varying granularity of\npatterns, intricate inter-channel correlation, information redundancy, and\nlabel scarcity. While existing transformer-based models have shown promise in\ntime series analysis, they mainly focus on forecasting and fail to fully\nexploit the distinctive characteristics of MedTS data. In this paper, we\nintroduce Sparseformer, a transformer specifically designed for MedTS\nclassification. We propose a sparse token-based dual-attention mechanism that\nenables global modeling and token compression, allowing dynamic focus on the\nmost informative tokens while distilling redundant features. This mechanism is\nthen applied to the multi-granularity, cross-channel encoding of medical\nsignals, capturing intra- and inter-granularity correlations and inter-channel\nconnections. The sparsification design allows our model to handle heterogeneous\ninputs of varying lengths and channels directly. Further, we introduce an\nadaptive label encoder to address label space misalignment across datasets,\nequipping our model with cross-dataset transferability to alleviate the medical\nlabel scarcity issue. Our model outperforms 12 baselines across seven medical\ndatasets under supervised learning. In the few-shot learning experiments, our\nmodel also achieves superior average results. In addition, the in-domain and\ncross-domain experiments among three diagnostic scenarios demonstrate our\nmodel's zero-shot learning capability. Collectively, these findings underscore\nthe robustness and transferability of our model in various medical\napplications.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 13:22:42 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Ye', 'Jiexia', ''], ['Zhang', 'Weiqi', ''], ['Li', 'Ziyue', ''], ['Li', 'Jia', ''], ['Tsung', 'Fugee', '']]","extracted_entities":"[{'text': 'Sparseformer', 'label': 'Transformer-based model'}, {'text': 'sparse token-based dual-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'supervised learning', 'label': 'Attention mechanism'}, {'text': 'zero-shot learning', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot learning","similarity_score":1.0000001192}
{"id":2503.15683,"submitter":"Yanis Benidir","authors":"Benidir Yanis, Gonthier Nicolas, Mallet Clement","title":"The Change You Want To Detect: Semantic Change Detection In Earth\n  Observation With Hybrid Data Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Bi-temporal change detection at scale based on Very High Resolution (VHR)\nimages is crucial for Earth monitoring. This remains poorly addressed so far:\nmethods either require large volumes of annotated data (semantic case), or are\nlimited to restricted datasets (binary set-ups). Most approaches do not exhibit\nthe versatility required for temporal and spatial adaptation: simplicity in\narchitecture design and pretraining on realistic and comprehensive datasets.\nSynthetic datasets are the key solution but still fail to handle complex and\ndiverse scenes. In this paper, we present HySCDG a generative pipeline for\ncreating a large hybrid semantic change detection dataset that contains both\nreal VHR images and inpainted ones, along with land cover semantic map at both\ndates and the change map. Being semantically and spatially guided, HySCDG\ngenerates realistic images, leading to a comprehensive and hybrid\ntransfer-proof dataset FSC-180k. We evaluate FSC-180k on five change detection\ncases (binary and semantic), from zero-shot to mixed and sequential training,\nand also under low data regime training. Experiments demonstrate that\npretraining on our hybrid dataset leads to a significant performance boost,\noutperforming SyntheWorld, a fully synthetic dataset, in every configuration.\nAll codes, models, and data are available here:\n$\\href{https:\/\/yb23.github.io\/projects\/cywd\/}{https:\/\/yb23.github.io\/projects\/cywd\/}$.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 20:32:37 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Yanis', 'Benidir', ''], ['Nicolas', 'Gonthier', ''], ['Clement', 'Mallet', '']]","extracted_entities":"[{'text': 'HySCDG', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'HySCDG', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'FSC-180k', 'label': 'Large Language Model'}, {'text': 'FSC-180k', 'label': 'Large Language Model'}, {'text': 'zero-shot', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot","similarity_score":0.7800109386}
{"id":2503.15905,"submitter":"Wang Jiyuan","authors":"Jiyuan Wang, Chunyu Lin, Cheng Guan, Lang Nie, Jing He, Haodong Li,\n  Kang Liao, Yao Zhao","title":"Jasmine: Harnessing Diffusion Prior for Self-supervised Depth Estimation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this paper, we propose Jasmine, the first Stable Diffusion (SD)-based\nself-supervised framework for monocular depth estimation, which effectively\nharnesses SD's visual priors to enhance the sharpness and generalization of\nunsupervised prediction. Previous SD-based methods are all supervised since\nadapting diffusion models for dense prediction requires high-precision\nsupervision. In contrast, self-supervised reprojection suffers from inherent\nchallenges (e.g., occlusions, texture-less regions, illumination variance), and\nthe predictions exhibit blurs and artifacts that severely compromise SD's\nlatent priors. To resolve this, we construct a novel surrogate task of hybrid\nimage reconstruction. Without any additional supervision, it preserves the\ndetail priors of SD models by reconstructing the images themselves while\npreventing depth estimation from degradation. Furthermore, to address the\ninherent misalignment between SD's scale and shift invariant estimation and\nself-supervised scale-invariant depth estimation, we build the Scale-Shift GRU.\nIt not only bridges this distribution gap but also isolates the fine-grained\ntexture of SD output against the interference of reprojection loss. Extensive\nexperiments demonstrate that Jasmine achieves SoTA performance on the KITTI\nbenchmark and exhibits superior zero-shot generalization across multiple\ndatasets.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 07:15:49 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wang', 'Jiyuan', ''], ['Lin', 'Chunyu', ''], ['Guan', 'Cheng', ''], ['Nie', 'Lang', ''], ['He', 'Jing', ''], ['Li', 'Haodong', ''], ['Liao', 'Kang', ''], ['Zhao', 'Yao', '']]","extracted_entities":"[{'text': 'zero-shot generalization', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"zero-shot generalization","similarity_score":0.8053859472}
{"id":2503.16188,"submitter":"Ming Li","authors":"Ming Li, Shitian Zhao, Jike Zhong, Yuxiang Lai, Kaipeng Zhang","title":"CLS-RL: Image Classification with Rule-Based Reinforcement Learning","comments":"Preprint, work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Classification is a core task in machine learning. Recent research has shown\nthat although Multimodal Large Language Models (MLLMs) are initially poor at\nimage classification, fine-tuning them with an adequate amount of data can\nsignificantly enhance their performance, making them comparable to SOTA\nclassification models. However, acquiring large-scale labeled data is\nexpensive. In this paper, we explore few-shot MLLM classification fine-tuning.\nWe found that SFT can cause severe overfitting issues and may even degrade\nperformance over the zero-shot approach. To address this challenge, inspired by\nthe recent successes in rule-based reinforcement learning, we propose CLS-RL,\nwhich uses verifiable signals as reward to fine-tune MLLMs. We discovered that\nCLS-RL outperforms SFT in most datasets and has a much higher average accuracy\non both base-to-new and few-shot learning setting. Moreover, we observed a\nfree-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular\ndataset, their performance on other distinct datasets may also improve over\nzero-shot models, even if those datasets differ in distribution and class\nnames. This suggests that RL-based methods effectively teach models the\nfundamentals of classification. Lastly, inspired by recent works in inference\ntime thinking, we re-examine the `thinking process' during fine-tuning, a\ncritical aspect of RL-based methods, in the context of visual classification.\nWe question whether such tasks require extensive thinking process during\nfine-tuning, proposing that this may actually detract from performance. Based\non this premise, we introduce the No-Thinking-CLS-RL method, which minimizes\nthinking processes during training by setting an equality accuracy reward. Our\nfindings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL\nmethod achieves superior in-domain performance and generalization capabilities\nthan CLS-RL.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 14:37:45 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Li', 'Ming', ''], ['Zhao', 'Shitian', ''], ['Zhong', 'Jike', ''], ['Lai', 'Yuxiang', ''], ['Zhang', 'Kaipeng', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'SFT', 'label': 'BERT'}, {'text': 'SFT', 'label': 'BERT'}, {'text': 'few-shot learning', 'label': 'Zero-shot Learning'}]","assigned_concept":"Zero-shot Learning","matched_keyword":"few-shot learning","similarity_score":0.8116950989}
{"id":2407.01082,"submitter":"Minh Nguyen","authors":"Minh Nguyen, Andrew Baker, Clement Neo, Allen Roush, Andreas Kirsch,\n  Ravid Shwartz-Ziv","title":"Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM\n  Outputs","comments":"Added acknowledgements and minor rewordings to make the\n  intro\/abstract more readable. No major change in length or content","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) generate text by sampling the next token from a\nprobability distribution over the vocabulary at each decoding step. Popular\nsampling methods like top-p (nucleus sampling) often struggle to balance\nquality and diversity, especially at higher temperatures which lead to\nincoherent or repetitive outputs. We propose min-p sampling, a dynamic\ntruncation method that adjusts the sampling threshold based on the model's\nconfidence by using the top token's probability as a scaling factor. Our\nexperiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative\nWriting show that min-p sampling improves both the quality and diversity of\ngenerated text across different model families (Mistral and Llama 3) and model\nsizes (1B to 123B parameters), especially at higher temperatures. Human\nevaluations further show a clear preference for min-p sampling, in both text\nquality and creativity. Min-p sampling has been adopted by popular open-source\nLLM frameworks, including Hugging Face Transformers, VLLM, and many others,\nhighlighting its significant impact on improving text generation quality.\n","versions":"[{'version': 'v1', 'created': 'Mon, 1 Jul 2024 08:37:25 GMT'}, {'version': 'v2', 'created': 'Sun, 13 Oct 2024 11:21:55 GMT'}, {'version': 'v3', 'created': 'Sun, 16 Mar 2025 17:12:44 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 09:39:39 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Nguyen', 'Minh', ''], ['Baker', 'Andrew', ''], ['Neo', 'Clement', ''], ['Roush', 'Allen', ''], ['Kirsch', 'Andreas', ''], ['Shwartz-Ziv', 'Ravid', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'scaling factor', 'label': 'Scaling law'}, {'text': 'Mistral', 'label': 'Mistral'}, {'text': 'Llama 3', 'label': 'Llama'}, {'text': 'Hugging Face Transformers', 'label': 'Transformers'}, {'text': 'VLLM', 'label': 'Open-source LLMs'}]","assigned_concept":"Mistral","matched_keyword":"Mistral","similarity_score":1.0}
{"id":2501.18532,"submitter":"Anmol Goel","authors":"Anmol Goel, Yaxi Hu, Iryna Gurevych, Amartya Sanyal","title":"Differentially Private Steering for Large Language Model Alignment","comments":"ICLR 2025 Camera Ready; Code: https:\/\/github.com\/UKPLab\/iclr2025-psa","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Aligning Large Language Models (LLMs) with human values and away from\nundesirable behaviors (such as hallucination) has become increasingly\nimportant. Recently, steering LLMs towards a desired behavior via activation\nediting has emerged as an effective method to mitigate harmful generations at\ninference-time. Activation editing modifies LLM representations by preserving\ninformation from positive demonstrations (e.g., truthful) and minimising\ninformation from negative demonstrations (e.g., hallucinations). When these\ndemonstrations come from a private dataset, the aligned LLM may leak private\ninformation contained in those private samples. In this work, we present the\nfirst study of aligning LLM behavior with private datasets. Our work proposes\nthe Private Steering for LLM Alignment (PSA) algorithm to edit LLM activations\nwith differential privacy (DP) guarantees. We conduct extensive experiments on\nseven different benchmarks with open-source LLMs of different sizes (0.5B to\n7B) and model families (LlaMa, Qwen, Mistral and Gemma). Our results show that\nPSA achieves DP guarantees for LLM alignment with minimal loss in performance,\nincluding alignment metrics, open-ended text generation quality, and\ngeneral-purpose reasoning. We also develop the first Membership Inference\nAttack (MIA) for evaluating and auditing the empirical privacy for the problem\nof LLM steering via activation editing. Our experiments support the theoretical\nguarantees by showing improved guarantees for our PSA algorithm compared to\nseveral existing non-private techniques.\n","versions":"[{'version': 'v1', 'created': 'Thu, 30 Jan 2025 17:58:36 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 09:58:49 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Goel', 'Anmol', ''], ['Hu', 'Yaxi', ''], ['Gurevych', 'Iryna', ''], ['Sanyal', 'Amartya', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'Private Steering', 'label': 'Prompting'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'Mistral', 'label': 'Mistral'}, {'text': 'Gemma', 'label': 'Llama'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]","assigned_concept":"Mistral","matched_keyword":"Mistral","similarity_score":1.0}
{"id":2503.11911,"submitter":"Naome Etori","authors":"Naome A. Etori, Kevin Lu, Randu Karisa and Arturs Kanepajs","title":"LAG-MMLU: Benchmarking Frontier LLM Understanding in Latvian and Giriama","comments":"Accepted at NoDaLiDa\/Baltic-HLT 2025.\n  https:\/\/hdl.handle.net\/10062\/107190","journal-ref":"Joint 25th Nordic Conference on Computational Linguistics and 11th\n  Baltic Conference on Human Language Technologies (NoDaLiDa\/Baltic-HLT 2025) :\n  Proceedings of the Conference: March 3-4, 2025","doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  As large language models (LLMs) rapidly advance, evaluating their performance\nis critical. LLMs are trained on multilingual data, but their reasoning\nabilities are mainly evaluated using English datasets. Hence, robust evaluation\nframeworks are needed using high-quality non-English datasets, especially\nlow-resource languages (LRLs). This study evaluates eight state-of-the-art\n(SOTA) LLMs on Latvian and Giriama using a Massive Multitask Language\nUnderstanding (MMLU) subset curated with native speakers for linguistic and\ncultural relevance. Giriama is benchmarked for the first time. Our evaluation\nshows that OpenAI's o1 model outperforms others across all languages, scoring\n92.8% in English, 88.8% in Latvian, and 70.8% in Giriama on 0-shot tasks.\nMistral-large (35.6%) and Llama-70B IT (41%) have weak performance, on both\nLatvian and Giriama. Our results underscore the need for localized benchmarks\nand human evaluations in advancing cultural AI contextualization.\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 22:50:50 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 04:01:37 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Etori', 'Naome A.', ''], ['Lu', 'Kevin', ''], ['Karisa', 'Randu', ''], ['Kanepajs', 'Arturs', '']]","extracted_entities":"[{'text': 'Giriama', 'label': 'Large Language Model'}, {'text': 'Giriama', 'label': 'Large Language Model'}, {'text': 'OpenAI', 'label': 'Open-source LLMs'}, {'text': 'Giriama', 'label': 'Large Language Model'}, {'text': '0-shot tasks', 'label': 'Zero-shot Learning'}, {'text': 'Mistral-large', 'label': 'Mistral'}, {'text': 'Giriama', 'label': 'Large Language Model'}]","assigned_concept":"Mistral","matched_keyword":"Mistral-large","similarity_score":0.7693821192}
{"id":2503.13428,"submitter":"Aaron Held","authors":"Aaron Held, Hyun Lim","title":"Black-hole binaries and waveforms in Quadratic Gravity","comments":"5 pages, including 3 figures and 1 table; references; 3 pages\n  supplementary material, including 3 figures","journal-ref":null,"doi":null,"report-no":"LA-UR-24-24999","categories":"gr-qc","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We report on the first numerical-relativity simulations of black-hole\nbinaries that deviate from General Relativity due to quadratic-curvature\ncorrections. Said theory of Quadratic Gravity propagates additional massive\nmodes and admits both Kerr and non-Kerr black-hole solutions. We chose the\nrespective masses \"at threshold\", i.e., such that (at least) one of the black\nholes dynamically transitions from the Kerr to the non-Kerr branch during the\nearly inspiral. The subsequent waveforms differ from their General Relativity\ncounterparts throughout inspiral, merger, and ringdown.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:55:14 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Held', 'Aaron', ''], ['Lim', 'Hyun', '']]","extracted_entities":"[{'text': 'inspiral', 'label': 'Mistral'}]","assigned_concept":"Mistral","matched_keyword":"inspiral","similarity_score":0.518586278}
{"id":2503.13572,"submitter":"Minghao Shao","authors":"Zeng Wang, Minghao Shao, Jitendra Bhandari, Likhitha Mankali, Ramesh\n  Karri, Ozgur Sinanoglu, Muhammad Shafique, Johann Knechtel","title":"VeriContaminated: Assessing LLM-Driven Verilog Coding for Data\n  Contamination","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AR cs.CR cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) have revolutionized code generation, achieving\nexceptional results on various established benchmarking frameworks. However,\nconcerns about data contamination - where benchmark data inadvertently leaks\ninto pre-training or fine-tuning datasets - raise questions about the validity\nof these evaluations. While this issue is known, limiting the industrial\nadoption of LLM-driven software engineering, hardware coding has received\nlittle to no attention regarding these risks. For the first time, we analyze\nstate-of-the-art (SOTA) evaluation frameworks for Verilog code generation\n(VerilogEval and RTLLM), using established methods for contamination detection\n(CCD and Min-K% Prob). We cover SOTA commercial and open-source LLMs\n(CodeGen2.5, Minitron 4b, Mistral 7b, phi-4 mini, LLaMA-{1,2,3.1},\nGPT-{2,3.5,4o}, Deepseek-Coder, and CodeQwen 1.5), in baseline and fine-tuned\nmodels (RTLCoder and Verigen). Our study confirms that data contamination is a\ncritical concern. We explore mitigations and the resulting trade-offs for code\nquality vs fairness (i.e., reducing contamination toward unbiased\nbenchmarking).\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 12:26:49 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wang', 'Zeng', ''], ['Shao', 'Minghao', ''], ['Bhandari', 'Jitendra', ''], ['Mankali', 'Likhitha', ''], ['Karri', 'Ramesh', ''], ['Sinanoglu', 'Ozgur', ''], ['Shafique', 'Muhammad', ''], ['Knechtel', 'Johann', '']]","extracted_entities":"[{'text': 'Mistral 7b', 'label': 'Mistral'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Mistral","matched_keyword":"Mistral 7b","similarity_score":0.7412112355}
{"id":2303.11299,"submitter":"Takaharu Otsuka","authors":"T. Otsuka, Y. Tsunoda, N. Shimizu, Y. Utsuno, T. Abe, H. Ueno","title":"Prevailing Triaxial Shapes in Atomic Nuclei and a Quantum Theory of\n  Rotation of Composite Objects","comments":"41 pages, 26 figures,revision from the v7 version. Additional\n  discussions on practical conservation of K quantum numbers in strongly\n  deformed nuclei","journal-ref":null,"doi":null,"report-no":null,"categories":"nucl-th nucl-ex","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  In the traditional view, heavy deformed nuclei are like axially-symmetric\nprolate ellipsoids, rotating about one of the short axes. In the present\npicture, their shapes may be triaxial. The triaxial shape yields complex\nrotations, which actually well reproduce experimental data, as confirmed by\nstate-of-the-art Configuration Interaction calculations. Two origins are\nsuggested for the triaxiality: (i) binding-energy gain by the symmetry\nrestoration for triaxial shapes, and (ii) another gain by specific components\nof the nuclear force, like tensor force and high-multipole (e.g. hexadecupole)\ncentral force. While the origin (i) produces basic smaller triaxiality for\nvirtually all deformed nuclei, the origin (ii) produces medium triaxiality for\na certain class of nuclei. An example of the former is 154Sm, a typical\nshowcase of axial symmetry but is now suggested to depict a modest yet finite\ntriaxiality. The latter, medium triaxiality, is discussed from various\nviewpoints for some exemplified nuclei including 166Er, and experimental\nfindings. Many-body structures of the gamma band and the double-gamma band are\nclarified. Regarding the general features of rotational states of deformed\nmany-body systems including triaxial ones, the well-known J(J+1) rule of\nrotational excitation energies is discussed, within the quantum mechanical\nmany-body theory, without resorting to the quantization of a rotating classical\nrigid body. The picture of prevailing triaxial shapes thus emerges, where the\nempirically known rotational-band pattern appears with good K quantum number,\nbut the internal structure is dfferent from conventional picture a la A. Bohr.\nThe possible relations to Davydov's rigid-triaxial-rotor model are mentioned.\n","versions":"[{'version': 'v1', 'created': 'Mon, 20 Mar 2023 17:38:41 GMT'}, {'version': 'v2', 'created': 'Fri, 19 May 2023 12:58:13 GMT'}, {'version': 'v3', 'created': 'Mon, 12 Jun 2023 23:52:27 GMT'}, {'version': 'v4', 'created': 'Tue, 9 Jul 2024 13:02:33 GMT'}, {'version': 'v5', 'created': 'Mon, 29 Jul 2024 09:43:56 GMT'}, {'version': 'v6', 'created': 'Mon, 30 Sep 2024 10:15:07 GMT'}, {'version': 'v7', 'created': 'Mon, 23 Dec 2024 16:48:46 GMT'}, {'version': 'v8', 'created': 'Thu, 20 Mar 2025 09:05:59 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Otsuka', 'T.', ''], ['Tsunoda', 'Y.', ''], ['Shimizu', 'N.', ''], ['Utsuno', 'Y.', ''], ['Abe', 'T.', ''], ['Ueno', 'H.', '']]","extracted_entities":"[{'text': 'quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization","similarity_score":0.8134455085}
{"id":2306.1722,"submitter":"Iliya Esin","authors":"Iliya Esin, \\'Etienne Lantagne-Hurtubise, Frederik Nathan, Gil Refael","title":"Quantum geometry and bounds on dissipation in slowly driven quantum\n  systems","comments":"5 pages, 3 figures + supplementary material","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.mes-hall cond-mat.stat-mech","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We show that energy dissipation in slowly-driven, Markovian quantum systems\nat low temperature is linked to the geometry of the driving protocol through\nthe quantum (or Fubini-Study) metric. Utilizing these findings, we establish\nlower bounds on dissipation rates in two-tone protocols, such as those employed\nfor topological frequency conversion. Notably, in appropriate limits these\nbounds are only determined by the topology of the protocol and an effective\nquality factor of the system-bath coupling. Our results bridge topological and\ngeometric phenomena with energy dissipation in open quantum systems, and\nfurther provide design principles for optimal driving protocols.\n","versions":"[{'version': 'v1', 'created': 'Thu, 29 Jun 2023 18:00:03 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Jul 2023 21:00:22 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 14:37:16 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Esin', 'Iliya', ''], ['Lantagne-Hurtubise', '\u00c9tienne', ''], ['Nathan', 'Frederik', ''], ['Refael', 'Gil', '']]","extracted_entities":"[{'text': 'quantum', 'label': 'quantisation'}, {'text': 'topological frequency conversion', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantum","similarity_score":0.5509136915}
{"id":2309.09545,"submitter":"Jiadong Liang","authors":"Xiang Li, Jiadong Liang, Xinyun Chen and Zhihua Zhang","title":"Convergence and Inference of Stream SGD, with Applications to Queueing\n  Systems and Inventory Control","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Stream stochastic gradient descent (SGD) is a simple and efficient method for\nsolving online optimization problems in operations research (OR), where data is\ngenerated by parameter-dependent Markov chains. Unlike traditional approaches\nwhich require increasing batch sizes during iterations, stream SGD uses a\nsingle sample per iteration, significantly improving sample efficiency. This\npaper establishes a systematic framework for analyzing stream SGD, leveraging\nthe Poisson equation solution to address gradient bias and statistical\ndependence. We prove optimal O(1\/T) convergence rates and the state-of-the-art\nO(log T) regret, while also introducing an online inference method for\nuncertainty quantification and supporting it by a novel functional central\nlimit theorem. We propose a novel Wasserstein-type divergence to describe the\nframework's conditions, which makes the assumptions in question directly\nverified via coupling techniques tailored to underlying OR models. We consider\napplications in queueing systems and inventory management, demonstrating the\npracticality and broad relevance, as well as providing new insights into the\neffectiveness of stream SGD in OR fields.\n","versions":"[{'version': 'v1', 'created': 'Mon, 18 Sep 2023 07:42:47 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 02:39:34 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Li', 'Xiang', ''], ['Liang', 'Jiadong', ''], ['Chen', 'Xinyun', ''], ['Zhang', 'Zhihua', '']]","extracted_entities":"[{'text': 'uncertainty quantification', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"uncertainty quantification","similarity_score":0.5714546442}
{"id":2402.01549,"submitter":"Meng Ruoyu","authors":"Ruoyu Meng and Aditya Ramamoorthy","title":"Quantum advantage in zero-error function computation with side\n  information","comments":"Added about 16 more pages about discussion in variable-length\n  classical codes (VLC): Problem setting, optimal rate, comparison with quantum\n  code","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IT math.CO math.IT quant-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We consider the problem of zero-error function computation with side\ninformation. Alice and Bob have correlated sources $X,Y$ with joint p.m.f.\n$p_{XY}(\\cdot, \\cdot)$. Bob wants to calculate $f(X,Y)$ with zero error. Alice\nencodes $m$-length blocks $(m \\geq 1)$ of her observations to Bob over\nerror-free channels, which can be classical or quantum. We consider two\nclassical settings. (i) Alice communicates via a fixed length code (FLC), and\n(ii) Alice communicates via a variable length code (VLC). In the FLC scenario,\nthe minimum communication rate depends on the asymptotic growth of the\nchromatic number of an appropriately defined $m$-instance ``confusion graph''\n$G^{(m)}$. In the VLC scenario, the corresponding rate is characterized by the\nasymptotics of the chromatic entropy of $G^{(m)}$. %and has single-letter\ncharacterization in terms of K\\\"orner's graph entropy if $G^{(m)}$ is $m$-times\ngraph OR product. In the quantum setting, we only consider fixed length codes;\nthe corresponding rate depends on the asymptotic growth of the orthogonal rank\nof the complement of $G^{(m)}$. The behavior of the communication rates depends\ncritically on $G^{(m)}$, which is shown to be sandwiched between $G^{\\boxtimes\nm}$ ($m$-times strong product) and $G^{\\lor m}$ ($m$-times OR product)\nrespectively. Our work presents necessary and sufficient conditions on the\nfunction $f(\\cdot, \\cdot)$ and joint p.m.f. $p_{XY}(\\cdot,\\cdot)$ such that\n$G^{(m)}$ equals either $G^{\\boxtimes m}$ or $G^{\\lor m}$. Our work explores\nthe multitude of possible behaviors of the quantum and classical (FLC\/VLC)\nrates in the single-instance case and the asymptotic (in $m$) case for several\nclasses of confusion graphs.\n","versions":"[{'version': 'v1', 'created': 'Fri, 2 Feb 2024 16:41:36 GMT'}, {'version': 'v2', 'created': 'Mon, 4 Mar 2024 23:02:26 GMT'}, {'version': 'v3', 'created': 'Tue, 14 Jan 2025 23:45:09 GMT'}, {'version': 'v4', 'created': 'Wed, 19 Mar 2025 17:16:54 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Meng', 'Ruoyu', ''], ['Ramamoorthy', 'Aditya', '']]","extracted_entities":"[{'text': 'Alice', 'label': 'ALBERT'}, {'text': 'Bob', 'label': 'ALBERT'}, {'text': 'Alice', 'label': 'ALBERT'}, {'text': 'Bob', 'label': 'ALBERT'}, {'text': 'quantum', 'label': 'quantisation'}, {'text': 'Alice', 'label': 'ALBERT'}, {'text': 'Alice', 'label': 'ALBERT'}, {'text': 'quantum', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantum","similarity_score":0.5509136915}
{"id":2405.14304,"submitter":"Mojtaba Bemana","authors":"Mojtaba Bemana, Thomas Leimk\\\"uhler, Karol Myszkowski, Hans-Peter\n  Seidel, Tobias Ritschel","title":"Bracket Diffusion: HDR Image Generation by Consistent LDR Denoising","comments":"11 pages, 14 figures, Accepted to Eurographics 2025, see\n  https:\/\/bracketdiffusion.mpi-inf.mpg.de","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GR cs.CV eess.IV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We demonstrate generating HDR images using the concerted action of multiple\nblack-box, pre-trained LDR image diffusion models. Relying on a pre-trained LDR\ngenerative diffusion models is vital as, first, there is no sufficiently large\nHDR image dataset available to re-train them, and, second, even if it was,\nre-training such models is impossible for most compute budgets. Instead, we\nseek inspiration from the HDR image capture literature that traditionally fuses\nsets of LDR images, called \"exposure brackets'', to produce a single HDR image.\nWe operate multiple denoising processes to generate multiple LDR brackets that\ntogether form a valid HDR result. The key to making this work is to introduce a\nconsistency term into the diffusion process to couple the brackets such that\nthey agree across the exposure range they share while accounting for possible\ndifferences due to the quantization error. We demonstrate state-of-the-art\nunconditional and conditional or restoration-type (LDR2HDR) generative modeling\nresults, yet in HDR.\n","versions":"[{'version': 'v1', 'created': 'Thu, 23 May 2024 08:24:22 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 14:54:28 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Bemana', 'Mojtaba', ''], ['Leimk\u00fchler', 'Thomas', ''], ['Myszkowski', 'Karol', ''], ['Seidel', 'Hans-Peter', ''], ['Ritschel', 'Tobias', '']]","extracted_entities":"[{'text': 'quantization error', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization error","similarity_score":0.6830846667}
{"id":2405.15741,"submitter":"Travis Whyte","authors":"Travis Whyte, David J. Wilson and Christopher E. Thomas","title":"Near-threshold states in coupled $DD^{\\ast}-D^{\\ast}D^{\\ast}$ scattering\n  from lattice QCD","comments":"31 pages, 13 figures, 14 tables","journal-ref":"Phys. Rev. D 111, 034511 (2025)","doi":"10.1103\/PhysRevD.111.034511","report-no":null,"categories":"hep-lat hep-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The first determination of doubly-charmed isospin-0 coupled-channel\n$DD^\\ast-D^\\ast D^\\ast$ scattering amplitudes from lattice QCD is presented.\nThe finite-volume spectrum is computed for three lattice volumes with a\nlight-quark mass corresponding to $m_\\pi\\approx 391$ MeV and is used to extract\nthe scattering amplitudes in $J^P = 1^+$ via the L\\\"{u}scher quantization\ncondition. By analytically continuing the scattering amplitudes to complex\nenergies, a $T_{cc}$ pole corresponding to a virtual bound state is found below\n$DD^\\ast$ threshold. We also find a second pole, $T_{cc}^\\prime$, corresponding\nto a resonance pole below the kinematically closed $D^\\ast D^\\ast$ channel, to\nwhich it has a strong coupling. A non-zero coupling is robustly found between\nthe $S$-wave $D D^\\ast$ and $D^\\ast D^\\ast$ channels producing a clear cusp in\nthe $D D^\\ast$ amplitude at the $D^\\ast D^\\ast$ threshold energy. This suggests\nthat the experimental $T_{cc}^\\prime$ should be observable in $D D^\\ast$ and\n$D^\\ast D^\\ast$ final states at ongoing experiments.\n","versions":"[{'version': 'v1', 'created': 'Fri, 24 May 2024 17:36:34 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:47:32 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Whyte', 'Travis', ''], ['Wilson', 'David J.', ''], ['Thomas', 'Christopher E.', '']]","extracted_entities":"[{'text': 'lattice QCD', 'label': 'quantisation'}, {'text': 'L\\\\\"{u}scher quantization\\ncondition', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"L\\\"{u}scher quantization\ncondition","similarity_score":0.5030887127}
{"id":2407.1969,"submitter":"Jun-Hui Zheng","authors":"Xi-Yu Chen, Lijia Jiang, Wen-Kai Bai, Tao Yang, and Jun-Hui Zheng","title":"Synthetic half-integer magnetic monopole and single-vortex dynamics in\n  spherical Bose-Einstein condensates","comments":"11 pages, 4 figures, 1 table","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.quant-gas nlin.PS quant-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Magnetic monopoles are crucial in explaining the quantization of electric\ncharges and quantum Hall effects, while artificially creating a minimal\nmagnetic monopole in experiments remains a challenge. Here, we come up with a\nflexible way to simulate a half-integer-type monopole in Bose gases and\ninvestigate the induced vortex dynamics on a sphere. We list the possible\nexperiment parameter settings for different isotopes and discuss their\nexperimental feasibility. With the assumption of a rigid monopole-vortex\nstructure, we analytically predict the vortex trajectory in an external\nmagnetic field. We then confirm the result by numerically solving the\nGross-Pitaevskii equation, which employs two gauges simultaneously (the Wu-Yang\napproach) to prevent singularity in the one-gauge method when a monopole is\npresent. The study offers significant insight into the characteristics of\nmonopoles and vortices, facilitating avenues for experimental validation.\n","versions":"[{'version': 'v1', 'created': 'Mon, 29 Jul 2024 04:20:24 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 01:50:25 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chen', 'Xi-Yu', ''], ['Jiang', 'Lijia', ''], ['Bai', 'Wen-Kai', ''], ['Yang', 'Tao', ''], ['Zheng', 'Jun-Hui', '']]","extracted_entities":"[{'text': 'quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization","similarity_score":0.8134455085}
{"id":2409.01402,"submitter":"Nikola Herceg","authors":"Nikola Herceg, Tajron Juri\\'c, A. Naveena Kumara, Andjelo Samsarov,\n  Ivica Smoli\\'c","title":"Noncommutative quasinormal modes of Schwarzschild black hole","comments":"38 pages, 6 figures, 23 tables; improved version, typos corrected,\n  references added","journal-ref":null,"doi":null,"report-no":"RBI-ThPhys-2024-16, ZTF-EP-24-07","categories":"gr-qc hep-th","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We study gravitational perturbations of the Schwarzschild metric in the\ncontext of noncommutative gravity. $r-\\varphi$ and $r-t$ noncommutativity are\nintroduced through a Moyal twist of the Hopf algebra of diffeomorphisms.\nDifferential geometric structures such as curvature tensors are also twisted.\nNoncommutative equations of motion are derived from the recently proposed NC\nvacuum Einstein equation. Here, in addition to previously calculated axial NC\npotential, we present the polar solution which generalizes the work done by\nZerilli. Quasinormal mode frequencies of the two potentials are calculated\nusing three methods: WKB, P\\\"oschl-Teller and Rosen-Morse. Notably, we apply\nthe WKB method up to the 13th order and determine the optimal order for each\nnoncommutative parameter value individually. Additionally, we provide\ncomprehensive error estimations for the higher-order WKB calculations, offering\ninsights into the accuracy of our results. By comparing the spectra, we\nconclude that the classical isospectrality of axial and polar modes is broken\nupon spacetime quantization. Isospectrality is restored in the eikonal limit.\n","versions":"[{'version': 'v1', 'created': 'Mon, 2 Sep 2024 18:00:01 GMT'}, {'version': 'v2', 'created': 'Fri, 13 Sep 2024 20:47:18 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 22:14:59 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Herceg', 'Nikola', ''], ['Juri\u0107', 'Tajron', ''], ['Kumara', 'A. Naveena', ''], ['Samsarov', 'Andjelo', ''], ['Smoli\u0107', 'Ivica', '']]","extracted_entities":"[{'text': 'spacetime quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"spacetime quantization","similarity_score":0.6639640331}
{"id":2412.10979,"submitter":"Ying Wang","authors":"Ying Wang and Jian Guo and Yanlong Zhao and Ji-feng Zhang","title":"Distributed Estimation with Quantized Measurements and Communication\n  over Markovian Switching Topologies","comments":"17 pages, 7 figures, submitted to Automatica","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper addresses distributed parameter estimation in stochastic dynamic\nsystems with quantized measurements, constrained by quantized communication and\nMarkovian switching directed topologies. To enable accurate recovery of the\noriginal signal from quantized communication signal, a persistent\nexcitation-compliant linear compression encoding method is introduced.\nLeveraging this encoding, this paper proposes an estimation-fusion type\nquantized distributed identification algorithm under a stochastic approximation\nframework. The algorithm operates in two phases: first, it estimates\nneighboring estimates using quantized communication information, then it\ncreates a fusion estimate by combining these estimates through a\nconsensus-based distributed stochastic approximation approach. To tackle the\ndifficulty caused by the coupling between these two estimates, two combined\nLyapunov functions are constructed to analyze the convergence performance.\nSpecifically, the mean-square convergence of the estimates is established under\na conditional expectation-type cooperative excitation condition and the union\ntopology containing a spanning tree. Besides, the convergence rate is derived\nto match the step size's order under suitable step-size coefficients.\nFurthermore, the impact of communication uncertainties including stochastic\ncommunication noise and Markov-switching rate is analyzed on the convergence\nrate. A numerical example illustrates the theoretical findings and highlights\nthe joint effect of sensors under quantized communication.\n","versions":"[{'version': 'v1', 'created': 'Sat, 14 Dec 2024 21:52:42 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 14:30:04 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wang', 'Ying', ''], ['Guo', 'Jian', ''], ['Zhao', 'Yanlong', ''], ['Zhang', 'Ji-feng', '']]","extracted_entities":"[{'text': 'quantized communication', 'label': 'quantisation'}, {'text': 'quantized communication', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantized communication","similarity_score":0.5989185572}
{"id":2501.03318,"submitter":"Igor Shovkovy","authors":"Ritesh Ghosh and Igor A. Shovkovy","title":"Neutrino energy and momentum emission from magnetized dense quark matter","comments":"31 pages, 7 multi-panel figures, v2: added several discussions; to\n  appear in JHEP","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph hep-th nucl-th","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Using first-principles field-theoretic methods, we investigate neutrino\nemission from strongly magnetized dense quark matter under conditions relevant\nto compact stars. We develop a customized approximation that fully accounts for\nthe Landau-level quantization of electron states while neglecting such\nquantization for quarks. This approach is well-justified in dense quark matter,\nwhere the chemical potentials of up and down quarks significantly exceed those\nof electrons. Our analysis provides a detailed exploration of the influence of\nstrong magnetic fields on neutrino emission, including both the modification of\nthe total emission rate and the emergence of emission asymmetry relative to the\nmagnetic field direction. We further examine the role of temperature in\nsmoothing the oscillatory behavior of neutrino emission as a function of\nmagnetic field strength. Additionally, we study the interplay between the\nLandau-level quantization of electrons and the Fermi-liquid effects of quarks\nin modifying the phase space of relevant weak processes. Finally, we briefly\ndiscuss the broader implications of magnetic fields on stellar cooling\nprocesses and the potential contribution of asymmetric neutrino emission to\npulsar kicks.\n","versions":"[{'version': 'v1', 'created': 'Mon, 6 Jan 2025 19:00:01 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 02:22:02 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Ghosh', 'Ritesh', ''], ['Shovkovy', 'Igor A.', '']]","extracted_entities":"[{'text': 'Landau-level quantization', 'label': 'quantisation'}, {'text': 'Landau-level quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"Landau-level quantization","similarity_score":0.5439216495}
{"id":2501.06377,"submitter":"Ronald Caplan","authors":"Ronald M. Caplan, Miko M. Stulajter, Jon A. Linker, Cooper Downs, Lisa\n  A. Upton, Bibhuti Kumar Jha, Raphael Attie, Charles N. Arge, Carl J. Henney","title":"Open-source Flux Transport (OFT). I. HipFT -- High-performance Flux\n  Transport","comments":"32 pages, 16 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.SR physics.comp-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Global solar photospheric magnetic maps play a critical role in solar and\nheliospheric physics research. Routine magnetograph measurements of the field\noccur only along the Sun-Earth line, leaving the far-side of the Sun\nunobserved. Surface Flux Transport (SFT) models attempt to mitigate this by\nmodeling the surface evolution of the field. While such models have long been\nestablished in the community (with several releasing public full-Sun maps),\nnone are open source. The Open Source Flux Transport (OFT) model seeks to fill\nthis gap by providing an open and user-extensible SFT model that also builds on\nthe knowledge of previous models with updated numerical and data\nacquisition\/assimilation methods along with additional user-defined features.\nIn this first of a series of papers on OFT, we introduce its computational\ncore: the High-performance Flux Transport (HipFT) code\n(github.com\/predsci\/hipft). HipFT implements advection, diffusion, and data\nassimilation in a modular design that supports a variety of flow models and\noptions. It can compute multiple realizations in a single run across model\nparameters to create ensembles of maps for uncertainty quantification and is\nhigh-performance through the use of multi-CPU and multi-GPU parallelism. HipFT\nis designed to enable users to easily write extensions, enhancing its\nflexibility and adaptability. We describe HipFT's model features, validations\nof its numerical methods, performance of its parallel and GPU-accelerated code\nimplementation, analysis\/post-processing options, and example use cases.\n","versions":"[{'version': 'v1', 'created': 'Fri, 10 Jan 2025 22:55:45 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 16:40:26 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Caplan', 'Ronald M.', ''], ['Stulajter', 'Miko M.', ''], ['Linker', 'Jon A.', ''], ['Downs', 'Cooper', ''], ['Upton', 'Lisa A.', ''], ['Jha', 'Bibhuti Kumar', ''], ['Attie', 'Raphael', ''], ['Arge', 'Charles N.', ''], ['Henney', 'Carl J.', '']]","extracted_entities":"[{'text': 'uncertainty quantification', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"uncertainty quantification","similarity_score":0.5714546442}
{"id":2501.08238,"submitter":"Xuanjun Chen","authors":"Xuanjun Chen, Jiawei Du, Haibin Wu, Lin Zhang, I-Ming Lin, I-Hsiang\n  Chiu, Wenze Ren, Yuan Tseng, Yu Tsao, Jyh-Shing Roger Jang, Hung-yi Lee","title":"CodecFake+: A Large-Scale Neural Audio Codec-Based Deepfake Speech\n  Dataset","comments":"Work in Progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD eess.AS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  With the rapid advancement of neural audio codecs, codec-based speech\ngeneration (CoSG) systems have become highly powerful. Unfortunately, CoSG also\nenables the creation of highly realistic deepfake speech, making it easier to\nmimic an individual's voice and spread misinformation. We refer to this\nemerging deepfake speech generated by CoSG systems as CodecFake. Detecting such\nCodecFake is an urgent challenge, yet most existing systems primarily focus on\ndetecting fake speech generated by traditional speech synthesis models. In this\npaper, we introduce CodecFake+, a large-scale dataset designed to advance\nCodecFake detection. To our knowledge, CodecFake+ is the largest dataset\nencompassing the most diverse range of codec architectures. The training set is\ngenerated through re-synthesis using 31 publicly available open-source codec\nmodels, while the evaluation set includes web-sourced data from 17 advanced\nCoSG models. We also propose a comprehensive taxonomy that categorizes codecs\nby their root components: vector quantizer, auxiliary objectives, and decoder\ntypes. Our proposed dataset and taxonomy enable detailed analysis at multiple\nlevels to discern the key factors for successful CodecFake detection. At the\nindividual codec level, we validate the effectiveness of using codec\nre-synthesized speech (CoRS) as training data for large-scale CodecFake\ndetection. At the taxonomy level, we show that detection performance is\nstrongest when the re-synthesis model incorporates disentanglement auxiliary\nobjectives or a frequency-domain decoder. Furthermore, from the perspective of\nusing all the CoRS training data, we show that our proposed taxonomy can be\nused to select better training data for improving detection performance.\nOverall, we envision that CodecFake+ will be a valuable resource for both\ngeneral and fine-grained exploration to develop better anti-spoofing models\nagainst CodecFake.\n","versions":"[{'version': 'v1', 'created': 'Tue, 14 Jan 2025 16:26:14 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 22:22:05 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Chen', 'Xuanjun', ''], ['Du', 'Jiawei', ''], ['Wu', 'Haibin', ''], ['Zhang', 'Lin', ''], ['Lin', 'I-Ming', ''], ['Chiu', 'I-Hsiang', ''], ['Ren', 'Wenze', ''], ['Tseng', 'Yuan', ''], ['Tsao', 'Yu', ''], ['Jang', 'Jyh-Shing Roger', ''], ['Lee', 'Hung-yi', '']]","extracted_entities":"[{'text': 'vector quantizer', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"vector quantizer","similarity_score":0.5825115442}
{"id":2503.01639,"submitter":"Sueda Taner","authors":"Sueda Taner, Ziyi Wang, and Christoph Studer","title":"Cauchy-Schwarz Regularizers","comments":"Accepted to ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We introduce a novel class of regularization functions, called Cauchy-Schwarz\n(CS) regularizers, which can be designed to induce a wide range of properties\nin solution vectors of optimization problems. To demonstrate the versatility of\nCS regularizers, we derive regularization functions that promote\ndiscrete-valued vectors, eigenvectors of a given matrix, and orthogonal\nmatrices. The resulting CS regularizers are simple, differentiable, and can be\nfree of spurious stationary points, making them suitable for gradient-based\nsolvers and large-scale optimization problems. In addition, CS regularizers\nautomatically adapt to the appropriate scale, which is, for example, beneficial\nwhen discretizing the weights of neural networks. To demonstrate the efficacy\nof CS regularizers, we provide results for solving underdetermined systems of\nlinear equations and weight quantization in neural networks. Furthermore, we\ndiscuss specializations, variations, and generalizations, which lead to an even\nbroader class of new and possibly more powerful regularizers.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 15:19:16 GMT'}, {'version': 'v2', 'created': 'Fri, 7 Mar 2025 14:31:52 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 10:01:57 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Taner', 'Sueda', ''], ['Wang', 'Ziyi', ''], ['Studer', 'Christoph', '']]","extracted_entities":"[{'text': 'CS regularizers', 'label': 'LLMs'}, {'text': 'weight quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"weight quantization","similarity_score":0.6230819225}
{"id":2503.08154,"submitter":"Tian Jin","authors":"Tian Jin, Enjun Du, Changwei Wang, Wenhao Xu, Ding Luo","title":"Structure-Activation Synergy: A Dual Efficiency Framework for\n  Parameter-Memory Optimized Transfer Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  While parameter-efficient transfer learning (PETL) successfully reduces\ntrainable parameters for adapting large pre-trained models, conventional\nmethods exhibit limited effectiveness in decreasing activation memory\nconsumption - a critical bottleneck for deployment on resource-constrained\ndevices. We present Structure-Activation Synergy (S2A), an innovative framework\nachieving dual optimization of parameters and memory through two synergistic\nmechanisms: (1) Structural activation modules (bias\/prompt\/side adaptations)\nthat strategically minimize both parametric complexity and intermediate feature\nstorage requirements, and (2) Derivative-aware 4-bit quantization for\nnon-parametric operators that maintains model fidelity through\ngradient-informed precision allocation. Extensive evaluations across multiple\narchitectures (ViT, Swin, ResNet) and datasets (ImageNet-1K, CIFAR, DomainNet)\ndemonstrate S2A's superior efficiency, reducing GPU memory consumption by 75\\%\n(4.2 average reduction) while maintaining 98.7\\% of full fine-tuning accuracy\nwith only 0.9\\% tunable parameters. This hardware-aware paradigm establishes\nnew state-of-the-art in efficient model adaptation, offering practical\ndeployment advantages through simultaneous parameter and memory optimization\nwithout compromising model capability\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 08:10:03 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 16:50:29 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Jin', 'Tian', ''], ['Du', 'Enjun', ''], ['Wang', 'Changwei', ''], ['Xu', 'Wenhao', ''], ['Luo', 'Ding', '']]","extracted_entities":"[{'text': 'Derivative-aware 4-bit quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"Derivative-aware 4-bit quantization","similarity_score":0.5734385848}
{"id":2503.09615,"submitter":"Pablo Arnault","authors":"Pablo Arnault","title":"Canonical quantization of the complex scalar field without making use of\n  its real and imaginary parts","comments":"38 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.gen-ph","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  We proceed to the canonical quantization of the complex scalar field without\nmaking use of its real and imaginary parts. Our motivation is to formally\nconnect, as tightly as possible, the quantum-field notions of particle and\nantiparticle - most prominently represented, formally, by creation and\nannihilation operators - to the initial classical field theory - whose main\nformal object is the field amplitude at a given spacetime point. Our point of\nview is that doing this via the use of the real and imaginary parts of the\nfield is not satisfying. The derivation demands to consider, just before\nquantization, the field and its complex conjugate as independent fields, which\nyields a system of two copies of independent complex scalar fields. One then\nproceeds to quantization with these two copies, which leads to the introduction\nof two families of creation and annihilation operators, corresponding to\nparticles on the one hand, and antiparticles on the other hand. One realizes\nthat having two such families is the only hope for being able to \"invert\" the\ndefinitions of the creation and annihilation in terms of the Fourier quantized\nfields, so as to obtain an expression of the direct-space fields in terms of\nthese creation and annihilation operators, because the real-field condition\nused in the case of a real scalar field does not hold for a complex scalar\nfield. This hope is then met by introducing the complex-conjugate constraint at\nthe quantum level, that is, that the second independent field copy is actually\nthe complex conjugate of the first. All standard results are then recovered in\na rigorous and purely deductive way. While we reckon our derivation exists in\nthe literature, we have not found it.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 01:30:16 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 16:39:19 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Arnault', 'Pablo', '']]","extracted_entities":"[{'text': 'canonical quantization', 'label': 'quantisation'}, {'text': 'quantization', 'label': 'quantisation'}, {'text': 'quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization","similarity_score":0.8134455085}
{"id":2503.09975,"submitter":"Joonhyung Lee","authors":"Joonhyung Lee, Shmulik Markovich-Golan, Daniel Ohayon, Yair Hanani,\n  Gunho Park, Byeongwook Kim, Asaf Karnieli, Uri Livne, Haihao Shen, Tai Huang,\n  Se Jung Kwon, Dongsoo Lee","title":"Faster Inference of LLMs using FP8 on the Intel Gaudi","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Low-precision data types are essential in modern neural networks during both\ntraining and inference as they enhance throughput and computational capacity by\nbetter exploiting available hardware resources. Despite the incorporation of\nFP8 in commercially available neural network accelerators, a comprehensive\nexposition of its underlying mechanisms, along with rigorous performance and\naccuracy evaluations, is still lacking. In this work, we contribute in three\nsignificant ways. First, we analyze the implementation details and quantization\noptions associated with FP8 for inference on the Intel Gaudi AI accelerator.\nSecond, we empirically quantify the throughput improvements afforded by the use\nof FP8 at both the operator level and in end-to-end scenarios. Third, we assess\nthe accuracy impact of various FP8 quantization methods. Our experimental\nresults indicate that the Intel Gaudi 2 accelerator consistently achieves high\ncomputational unit utilization, frequently exceeding 90% MFU, while incurring\nan accuracy degradation of less than 1%.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 02:21:39 GMT'}, {'version': 'v2', 'created': 'Fri, 14 Mar 2025 01:57:02 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 03:04:50 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Lee', 'Joonhyung', ''], ['Markovich-Golan', 'Shmulik', ''], ['Ohayon', 'Daniel', ''], ['Hanani', 'Yair', ''], ['Park', 'Gunho', ''], ['Kim', 'Byeongwook', ''], ['Karnieli', 'Asaf', ''], ['Livne', 'Uri', ''], ['Shen', 'Haihao', ''], ['Huang', 'Tai', ''], ['Kwon', 'Se Jung', ''], ['Lee', 'Dongsoo', '']]","extracted_entities":"[{'text': 'quantization\\noptions', 'label': 'quantisation'}, {'text': 'quantization methods', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization\noptions","similarity_score":0.7135424614}
{"id":2503.10199,"submitter":"Ruibiao Song","authors":"Ruibiao Song, Liying Zhang","title":"Optimal Estimation and Uncertainty Quantification for Stochastic Inverse\n  Problems via Variational Bayesian Methods","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.NA","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The Bayesian inversion method demonstrates significant potential for solving\ninverse problems, enabling both point estimation and uncertainty\nquantification. However, Bayesian maximum a posteriori (MAP) estimation may\nbecome unstable when handling data from diverse distributions (e.g., solutions\nof stochastic partial differential equations (SPDEs)). Additionally, Monte\nCarlo sampling methods are computationally expensive. To address these\nchallenges, we propose a novel two-stage optimization method based on optimal\ncontrol theory and variational Bayesian methods. This method not only achieves\nstable solutions for stochastic inverse problems but also efficiently\nquantifies the uncertainty of the solutions. In the first stage, we introduce a\nnew weighting formulation to ensure the stability of the Bayesian MAP\nestimation. In the second stage, we derive the necessary condition to\nefficiently quantify the uncertainty of the solutions, by combining the new\nweighting formula with variational inference. Furthermore, we establish an\nerror estimation theorem that relates the exact solution to the optimally\nestimated solution under different amounts of observed data. Finally, the\nefficiency of the proposed method is demonstrated through numerical examples.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 09:34:33 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 02:20:35 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 08:23:28 GMT'}, {'version': 'v4', 'created': 'Wed, 19 Mar 2025 10:23:49 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Song', 'Ruibiao', ''], ['Zhang', 'Liying', '']]","extracted_entities":"[{'text': 'uncertainty\\nquantification', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"uncertainty\nquantification","similarity_score":0.5714546442}
{"id":2503.13089,"submitter":"Baohao Liao","authors":"Baohao Liao and Christian Herold and Seyyed Hadi Hashemi and Stefan\n  Vasilev and Shahram Khadivi and Christof Monz","title":"ClusComp: A Simple Paradigm for Model Compression and Efficient\n  Finetuning","comments":"26 pages, 11 figures, 18 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  As large language models (LLMs) scale, model compression is crucial for edge\ndeployment and accessibility. Weight-only quantization reduces model size but\nsuffers from performance degradation at lower bit widths. Moreover, standard\nfinetuning is incompatible with quantized models, and alternative methods often\nfall short of full finetuning. In this paper, we propose ClusComp, a simple yet\neffective compression paradigm that clusters weight matrices into codebooks and\nfinetunes them block-by-block. ClusComp (1) achieves superior performance in\n2-4 bit quantization, (2) pushes compression to 1-bit while outperforming\nultra-low-bit methods with minimal finetuning, and (3) enables efficient\nfinetuning, even surpassing existing quantization-based approaches and rivaling\nfull FP16 finetuning. Notably, ClusComp supports compression and finetuning of\n70B LLMs on a single A6000-48GB GPU.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 11:52:16 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Liao', 'Baohao', ''], ['Herold', 'Christian', ''], ['Hashemi', 'Seyyed Hadi', ''], ['Vasilev', 'Stefan', ''], ['Khadivi', 'Shahram', ''], ['Monz', 'Christof', '']]","extracted_entities":"[{'text': 'Weight-only quantization', 'label': 'quantisation'}, {'text': 'standard\\nfinetuning', 'label': 'Fine-tuning'}, {'text': '2-4 bit quantization', 'label': 'quantisation'}, {'text': 'finetuning', 'label': 'Fine-tuning'}]","assigned_concept":"quantisation","matched_keyword":"2-4 bit quantization","similarity_score":0.6378128529}
{"id":2503.1363,"submitter":"Eren Volkan K\\\"u\\c{c}\\\"uk","authors":"Eren Volkan K\\\"u\\c{c}\\\"uk","title":"The Birth of Quantum Mechanics: A Historical Study Through the Canonical\n  Papers","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.hist-ph quant-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper explores the historical development of the theory of quantum\nmechanics between 1900 and 1927 by chronological examination of the\nfoundational papers and ideas. Beginning with Planck's introduction of energy\nquantisation in blackbody radiation, we follow the emergence of Einstein's\nlight quanta hypothesis, Bohr's atomic model, and the statistical implications\nof indistinguishable particles. Special emphasis is placed on the transition\nfrom the Old Quantum Theory to modern quantum mechanics, particularly through\nHeisenberg's matrix mechanics and Schr\\\"{o}dinger's wave mechanics. This study\naims to provide a structured historical account, offering insights into the\nconceptual transformations that led to quantum mechanics while making the\ndevelopment accessible to physicists, historians of science, and advanced\nstudents interested in the origins of modern quantum theory.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 18:26:50 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['K\u00fc\u00e7\u00fck', 'Eren Volkan', '']]","extracted_entities":"[{'text': 'energy\\nquantisation', 'label': 'quantisation'}, {'text': \"Bohr's atomic model\", 'label': 'Foundation Model'}]","assigned_concept":"quantisation","matched_keyword":"energy\nquantisation","similarity_score":0.6874095201}
{"id":2503.13909,"submitter":"Pavia Bera","authors":"Pavia Bera and Sanjukta Bhanja","title":"Quantification of Uncertainties in Probabilistic Deep Neural Network by\n  Implementing Boosting of Variational Inference","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ML","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Modern neural network architectures have achieved remarkable accuracies but\nremain highly dependent on their training data, often lacking interpretability\nin their learned mappings. While effective on large datasets, they tend to\noverfit on smaller ones. Probabilistic neural networks, such as those utilizing\nvariational inference, address this limitation by incorporating uncertainty\nestimation through weight distributions rather than point estimates. However,\nstandard variational inference often relies on a single-density approximation,\nwhich can lead to poor posterior estimates and hinder model performance. We\npropose Boosted Bayesian Neural Networks (BBNN), a novel approach that enhances\nneural network weight distribution approximations using Boosting Variational\nInference (BVI). By iteratively constructing a mixture of densities, BVI\nexpands the approximating family, enabling a more expressive posterior that\nleads to improved generalization and uncertainty estimation. While this\napproach increases computational complexity, it significantly enhances accuracy\nan essential tradeoff, particularly in high-stakes applications such as medical\ndiagnostics, where false negatives can have severe consequences. Our\nexperimental results demonstrate that BBNN achieves ~5% higher accuracy\ncompared to conventional neural networks while providing superior uncertainty\nquantification. This improvement highlights the effectiveness of leveraging a\nmixture-based variational family to better approximate the posterior\ndistribution, ultimately advancing probabilistic deep learning.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 05:11:21 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Bera', 'Pavia', ''], ['Bhanja', 'Sanjukta', '']]","extracted_entities":"[{'text': 'uncertainty\\nquantification', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"uncertainty\nquantification","similarity_score":0.5714546442}
{"id":2503.13917,"submitter":"Yujia Tong","authors":"Yujia Tong, Yuze Wang, Jingling Yuan, Chuang Hu","title":"Robust Machine Unlearning for Quantized Neural Networks via Adaptive\n  Gradient Reweighting with Similar Labels","comments":"15 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Model quantization enables efficient deployment of deep neural networks on\nedge devices through low-bit parameter representation, yet raises critical\nchallenges for implementing machine unlearning (MU) under data privacy\nregulations. Existing MU methods designed for full-precision models fail to\naddress two fundamental limitations in quantized networks: 1) Noise\namplification from label mismatch during data processing, and 2) Gradient\nimbalance between forgotten and retained data during training. These issues are\nexacerbated by quantized models' constrained parameter space and discrete\noptimization. We propose Q-MUL, the first dedicated unlearning framework for\nquantized models. Our method introduces two key innovations: 1) Similar Labels\nassignment replaces random labels with semantically consistent alternatives to\nminimize noise injection, and 2) Adaptive Gradient Reweighting dynamically\naligns parameter update contributions from forgotten and retained data. Through\nsystematic analysis of quantized model vulnerabilities, we establish\ntheoretical foundations for these mechanisms. Extensive evaluations on\nbenchmark datasets demonstrate Q-MUL's superiority over existing approaches.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 05:22:13 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Tong', 'Yujia', ''], ['Wang', 'Yuze', ''], ['Yuan', 'Jingling', ''], ['Hu', 'Chuang', '']]","extracted_entities":"[{'text': 'Model quantization', 'label': 'quantisation'}, {'text': 'data privacy\\nregulations', 'label': 'AI Ethics'}, {'text': 'discrete\\noptimization', 'label': 'Fine-tuning'}, {'text': 'Adaptive Gradient Reweighting', 'label': 'Fine-tuning'}]","assigned_concept":"quantisation","matched_keyword":"Model quantization","similarity_score":0.6441316009}
{"id":2503.13947,"submitter":"Sayak Nag","authors":"Sayak Nag, Udita Ghosh, Sarosij Bose, Calvin-Khang Ta, Jiachen Li,\n  Amit K Roy Chowdhury","title":"Conformal Prediction and MLLM aided Uncertainty Quantification in Scene\n  Graph Generation","comments":"Accepted at CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Scene Graph Generation (SGG) aims to represent visual scenes by identifying\nobjects and their pairwise relationships, providing a structured understanding\nof image content. However, inherent challenges like long-tailed class\ndistributions and prediction variability necessitate uncertainty quantification\nin SGG for its practical viability. In this paper, we introduce a novel\nConformal Prediction (CP) based framework, adaptive to any existing SGG method,\nfor quantifying their predictive uncertainty by constructing well-calibrated\nprediction sets over their generated scene graphs. These scene graph prediction\nsets are designed to achieve statistically rigorous coverage guarantees.\nAdditionally, to ensure these prediction sets contain the most practically\ninterpretable scene graphs, we design an effective MLLM-based post-processing\nstrategy for selecting the most visually and semantically plausible scene\ngraphs within these prediction sets. We show that our proposed approach can\nproduce diverse possible scene graphs from an image, assess the reliability of\nSGG methods, and improve overall SGG performance.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 06:27:57 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Nag', 'Sayak', ''], ['Ghosh', 'Udita', ''], ['Bose', 'Sarosij', ''], ['Ta', 'Calvin-Khang', ''], ['Li', 'Jiachen', ''], ['Chowdhury', 'Amit K Roy', '']]","extracted_entities":"[{'text': 'uncertainty quantification', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"uncertainty quantification","similarity_score":0.5714546442}
{"id":2503.13978,"submitter":"Jorge Baeza-Ballesteros","authors":"Jorge Baeza-Ballesteros, Pilar Hern\\'andez, Fernando Romero-L\\'opez","title":"The $\\pi\\pi$ scattering amplitude at large $N_\\text{c}$","comments":"63 pages including 3 appendices, 18 figures, 18 tables. Full list of\n  operators included as ancillary file","journal-ref":null,"doi":null,"report-no":"DESY-25-040, MIT-CTP\/5850","categories":"hep-lat hep-ph hep-th","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We study the scaling of meson-meson scattering amplitudes with the number of\ncolors, $N_\\text{c}$. We use lattice calculations in a theory with\n$N_\\text{f}=4$ degenerate flavors, with $N_\\text{c}=3-6$ and pion mass\n$M_\\pi\\approx 560$ MeV. We focus on three different scattering channels, two of\nwhich have the same quantum numbers as some tetraquark candidates recently\nfound at LHCb: the $T_{cs0}^0(2900)$, $T_{c\\bar{s}0}^{++}(2900)$,\n$T_{c\\bar{s}0}^0(2900)$ and $T_{c\\bar{s}1}^0(2900)$ states. Finite-volume\nenergies are extracted using a large set of operators, containing two-particle\noperators with the form of two pions or two vector mesons, and local tetraquark\noperators. The resulting energy spectra is used to constrain the\ninfinite-volume scattering amplitude by means of L\\\"uscher's quantization\ncondition. We consider polynomial parametrizations of the phase shift, as well\nas one-loop chiral perturbation theory (ChPT) predictions. We find that our\nlattice results follow the expected $N_\\text{c}$ scaling and are sensitive to\nsubleading $N_\\text{c}$ corrections. In addition, we constrain the scaling of\ndifferent combinations of low-energy constants from matching to large\n$N_\\text{c}$ ChPT. The results for the channel corresponding to a $(\\pi^+ D^+_s\n- K^+ D^+)$ state show evidence of a virtual bound state with energy\n$E_\\text{virtual}=1.63(10)M_\\pi$ for $N_\\text{c}=3$, while this pole disappears\nat $N_\\text{c}>3$. This may be connected to the exotic states found in\nexperiment.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 07:27:05 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Baeza-Ballesteros', 'Jorge', ''], ['Hern\u00e1ndez', 'Pilar', ''], ['Romero-L\u00f3pez', 'Fernando', '']]","extracted_entities":"[{'text': 'L\\\\\"uscher\\'s quantization\\ncondition', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"L\\\"uscher's quantization\ncondition","similarity_score":0.6183217764}
{"id":2503.13986,"submitter":"Pengfei Tian","authors":"Pengfei Tian, Fan Yang and Peng Ding","title":"Stratified Permutational Berry--Esseen Bounds and Their Applications to\n  Statistics","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.ST stat.TH","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The stratified linear permutation statistic arises in various statistics\nproblems, including stratified and post-stratified survey sampling, stratified\nand post-stratified experiments, conditional permutation tests, etc. Although\nwe can derive the Berry--Esseen bounds for the stratified linear permutation\nstatistic based on existing bounds for the non-stratified statistics, those\nbounds are not sharp, and moreover, this strategy does not work in general\nsettings with heterogeneous strata with varying sizes. We first use Stein's\nmethod to obtain a unified stratified permutational Berry--Esseen bound that\ncan accommodate heterogeneous strata. We then apply the bound to various\nstatistics problems, leading to stronger theoretical quantifications and\nthereby facilitating statistical inference in those problems.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 07:44:01 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Tian', 'Pengfei', ''], ['Yang', 'Fan', ''], ['Ding', 'Peng', '']]","extracted_entities":"[{'text': 'theoretical quantifications', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"theoretical quantifications","similarity_score":0.6950218081}
{"id":2503.14177,"submitter":"Mohamad Al Ahdab","authors":"Mohamad Al Ahdab, Zheng-Hua Tan, John Leth","title":"Distributions and Direct Parametrization for Stable Stochastic\n  State-Space Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ME cs.SY eess.SY","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We present a direct parametrization for continuous-time stochastic\nstate-space models that ensures external stability via the stochastic\nbounded-real lemma. Our formulation facilitates the construction of\nprobabilistic priors that enforce almost-sure stability which are suitable for\nsampling-based Bayesian inference methods. We validate our work with a\nsimulation example and demonstrate its ability to yield stable predictions with\nuncertainty quantification.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:51:11 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Ahdab', 'Mohamad Al', ''], ['Tan', 'Zheng-Hua', ''], ['Leth', 'John', '']]","extracted_entities":"[{'text': 'uncertainty quantification', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"uncertainty quantification","similarity_score":0.5714546442}
{"id":2503.14259,"submitter":"Ziyad Sheebaelhamd","authors":"Ziyad Sheebaelhamd, Michael Tschannen, Michael Muehlebach, Claire\n  Vernade","title":"Quantization-Free Autoregressive Action Transformer","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Current transformer-based imitation learning approaches introduce discrete\naction representations and train an autoregressive transformer decoder on the\nresulting latent code. However, the initial quantization breaks the continuous\nstructure of the action space thereby limiting the capabilities of the\ngenerative model. We propose a quantization-free method instead that leverages\nGenerative Infinite-Vocabulary Transformers (GIVT) as a direct, continuous\npolicy parametrization for autoregressive transformers. This simplifies the\nimitation learning pipeline while achieving state-of-the-art performance on a\nvariety of popular simulated robotics tasks. We enhance our policy roll-outs by\ncarefully studying sampling algorithms, further improving the results.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 13:50:35 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Sheebaelhamd', 'Ziyad', ''], ['Tschannen', 'Michael', ''], ['Muehlebach', 'Michael', ''], ['Vernade', 'Claire', '']]","extracted_entities":"[{'text': 'initial quantization', 'label': 'quantisation'}, {'text': 'Generative Infinite-Vocabulary Transformers', 'label': 'Transformers'}, {'text': 'autoregressive transformers', 'label': 'Transformers'}]","assigned_concept":"quantisation","matched_keyword":"initial quantization","similarity_score":0.6908357143}
{"id":2503.14344,"submitter":"Mrinal Kanti Roychowdhury","authors":"Shivam Dubey, Mrinal Kanti Roychowdhury, and Saurabh Verma","title":"Quantization for a condensation system","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.DS math.PR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  For a given $r \\in (0, +\\infty)$, the quantization dimension of order $r$, if\nit exists, denoted by $D_r(\\mu)$, represents the rate at which the $n$th\nquantization error of order $r$ approaches to zero as the number of elements\n$n$ in an optimal set of $n$-means for $\\mu$ tends to infinity. If $D_r(\\mu)$\ndoes not exist, we define $\\underline{D}_r(\\mu)$ and $\\overline{D}_r(\\mu)$ as\nthe lower and the upper quantization dimensions of $\\mu$ of order $r$,\nrespectively. In this paper, we investigate the quantization dimension of the\ncondensation measure $\\mu$ associated with a condensation system\n$(\\{S_j\\}_{j=1}^N, (p_j)_{j=0}^N, \\nu).$ We provide two examples: one where\n$\\nu$ is an infinite discrete distribution on $\\mathbb{R}$, and one where $\\nu$\nis a uniform distribution on $\\mathbb{R}$. For both the discrete and uniform\ndistributions $\\nu$, we determine the optimal sets of $n$-means, and calculate\nthe quantization dimensions of condensation measures $\\mu$, and show that the\n$D_r(\\mu)$-dimensional quantization coefficients do not exist. Moreover, we\ndemonstrate that the lower and upper quantization coefficients are finite and\npositive.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 15:24:28 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Dubey', 'Shivam', ''], ['Roychowdhury', 'Mrinal Kanti', ''], ['Verma', 'Saurabh', '']]","extracted_entities":"[{'text': 'quantization dimensions', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization dimensions","similarity_score":0.677269578}
{"id":2503.14387,"submitter":"Mahmoud Alawashra","authors":"Mahmoud Alawashra, Jan Ben\\'a\\v{c}ek, Martin Pohl, Mikhail Medvedev","title":"Electromagnetic field solver for QED polarization in super-strong\n  magnetic fields of magnetar and laser plasmas","comments":"Submitted to JCAP","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.plasm-ph astro-ph.HE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Super-strongly magnetized plasmas play a crucial role in extreme environments\nof magnetar and laboratory laser experiments, demanding comprehensive\nunderstanding of how quantum electrodynamic (QED) effects influence plasma\nbehaviour. Earlier analytical and semi-analytical calculations have shown that\nQED effects can significantly modify the plasma polarization mode behaviour\naround magnetars using analytical and semi-analytical calculations. In this\nwork, we present the first electromagnetic field solver that is valid beyond\nthe Schwinger limit. QED vacuum polarization in super-strong magnetic fields\nare modeled with nonlinear Maxwell equations. We show that electromagnetic\nwaves in simulations follow the analytical solutions well and reproduce the\nbirefringence effects of electromagnetic wave modes between the $O$ and $X$\npolarizations of perpendicular electromagnetic waves and those between $L$ and\n$R$ polarizations of parallel waves. This new framework can be applied to\nkinetic as well as in other types of computer simulations. The solver's key\nadvantage lies in its versatility, allowing it to be used in gyro-motion,\ngyro-center, and gyro-kinetic simulations, which do not resolve the cyclotron\nmotion, or in plasma studies with ground-level Landau quantization.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:22:33 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Alawashra', 'Mahmoud', ''], ['Ben\u00e1\u010dek', 'Jan', ''], ['Pohl', 'Martin', ''], ['Medvedev', 'Mikhail', '']]","extracted_entities":"[{'text': 'Landau quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"Landau quantization","similarity_score":0.5282343626}
{"id":2503.14598,"submitter":"Haoyang Gao","authors":"Haoyang Gao, Leigh S. Martin, Lillian B. Hughes, Nathaniel T. Leitao,\n  Piotr Put, Hengyun Zhou, Nazli U. Koyluoglu, Simon A. Meynell, Ania C.\n  Bleszynski Jayich, Hongkun Park, Mikhail D. Lukin","title":"Signal amplification in a solid-state quantum sensor via asymmetric\n  time-reversal of many-body dynamics","comments":"25 pages, 15 digures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.dis-nn","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Electronic spins of nitrogen vacancy (NV) centers in diamond constitute a\npromising system for micro- and nano-scale magnetic sensing, due to their\noperation under ambient conditions, ease of placement in close proximity to\nsensing targets, and biological compatibility. At high densities, the\nelectronic spins interact through dipolar coupling, which typically limits but\ncan also potentially enhance sensing performance. Here we report the\nexperimental demonstration of many-body signal amplification in a solid-state,\nroom temperature quantum sensor. Our approach utilizes time-reversed\ntwo-axis-twisting interactions, engineered through dynamical control of the\nquantization axis and Floquet engineering in a two-dimensional ensemble of NV\ncenters. Strikingly, we observe that the optimal amplification occurs when the\nbackward evolution time equals twice the forward evolution time, in sharp\ncontrast to the conventional Loschmidt echo. These observations can be\nunderstood as resulting from an underlying time-reversed mirror symmetry of the\nmicroscopic dynamics, providing key insights into signal amplification and\nopening the door towards entanglement-enhanced practical quantum sensing.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 18:01:25 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Gao', 'Haoyang', ''], ['Martin', 'Leigh S.', ''], ['Hughes', 'Lillian B.', ''], ['Leitao', 'Nathaniel T.', ''], ['Put', 'Piotr', ''], ['Zhou', 'Hengyun', ''], ['Koyluoglu', 'Nazli U.', ''], ['Meynell', 'Simon A.', ''], ['Jayich', 'Ania C. Bleszynski', ''], ['Park', 'Hongkun', ''], ['Lukin', 'Mikhail D.', '']]","extracted_entities":"[{'text': 'quantization axis', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization axis","similarity_score":0.6386812329}
{"id":2503.14663,"submitter":"Anni Zhou","authors":"Anni Zhou, Beyah Raheem, Rishikesan Kamaleswaran, Yao Xie","title":"Sepsyn-OLCP: An Online Learning-based Framework for Early Sepsis\n  Prediction with Uncertainty Quantification using Conformal Prediction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Sepsis is a life-threatening syndrome with high morbidity and mortality in\nhospitals. Early prediction of sepsis plays a crucial role in facilitating\nearly interventions for septic patients. However, early sepsis prediction\nsystems with uncertainty quantification and adaptive learning are scarce. This\npaper proposes Sepsyn-OLCP, a novel online learning algorithm for early sepsis\nprediction by integrating conformal prediction for uncertainty quantification\nand Bayesian bandits for adaptive decision-making. By combining the robustness\nof Bayesian models with the statistical uncertainty guarantees of conformal\nprediction methodologies, this algorithm delivers accurate and trustworthy\npredictions, addressing the critical need for reliable and adaptive systems in\nhigh-stakes healthcare applications such as early sepsis prediction. We\nevaluate the performance of Sepsyn-OLCP in terms of regret in stochastic bandit\nsetting, the area under the receiver operating characteristic curve (AUROC),\nand F-measure. Our results show that Sepsyn-OLCP outperforms existing\nindividual models, increasing AUROC of a neural network from 0.64 to 0.73\nwithout retraining and high computational costs. And the model selection policy\nconverges to the optimal strategy in the long run. We propose a novel\nreinforcement learning-based framework integrated with conformal prediction\ntechniques to provide uncertainty quantification for early sepsis prediction.\nThe proposed methodology delivers accurate and trustworthy predictions,\naddressing a critical need in high-stakes healthcare applications like early\nsepsis prediction.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 19:10:34 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhou', 'Anni', ''], ['Raheem', 'Beyah', ''], ['Kamaleswaran', 'Rishikesan', ''], ['Xie', 'Yao', '']]","extracted_entities":"[{'text': 'uncertainty quantification', 'label': 'quantisation'}, {'text': 'adaptive learning', 'label': 'Few-shot Learning'}, {'text': 'uncertainty quantification', 'label': 'quantisation'}, {'text': 'uncertainty quantification', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"uncertainty quantification","similarity_score":0.5714546442}
{"id":2503.14665,"submitter":"Parker Ewen","authors":"Parker Ewen, Hao Chen, Seth Isaacson, Joey Wilson, Katherine A.\n  Skinner, Ram Vasudevan","title":"These Magic Moments: Differentiable Uncertainty Quantification of\n  Radiance Field Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper introduces a novel approach to uncertainty quantification for\nradiance fields by leveraging higher-order moments of the rendering equation.\nUncertainty quantification is crucial for downstream tasks including view\nplanning and scene understanding, where safety and robustness are paramount.\nHowever, the high dimensionality and complexity of radiance fields pose\nsignificant challenges for uncertainty quantification, limiting the use of\nthese uncertainty quantification methods in high-speed decision-making. We\ndemonstrate that the probabilistic nature of the rendering process enables\nefficient and differentiable computation of higher-order moments for radiance\nfield outputs, including color, depth, and semantic predictions. Our method\noutperforms existing radiance field uncertainty estimation techniques while\noffering a more direct, computationally efficient, and differentiable\nformulation without the need for post-processing.Beyond uncertainty\nquantification, we also illustrate the utility of our approach in downstream\napplications such as next-best-view (NBV) selection and active ray sampling for\nneural radiance field training. Extensive experiments on synthetic and\nreal-world scenes confirm the efficacy of our approach, which achieves\nstate-of-the-art performance while maintaining simplicity.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 19:12:02 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Ewen', 'Parker', ''], ['Chen', 'Hao', ''], ['Isaacson', 'Seth', ''], ['Wilson', 'Joey', ''], ['Skinner', 'Katherine A.', ''], ['Vasudevan', 'Ram', '']]","extracted_entities":"[{'text': 'uncertainty quantification', 'label': 'quantisation'}, {'text': 'Uncertainty quantification', 'label': 'quantisation'}, {'text': 'uncertainty quantification', 'label': 'quantisation'}, {'text': 'uncertainty\\nquantification', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"uncertainty quantification","similarity_score":0.5714546442}
{"id":2503.14668,"submitter":"Mykhaylo Khoma","authors":"Mykhaylo Khoma","title":"Elastic and charge transfer cross sections for low to ultralow\n  $\\rm{H}(1s) + \\rm{H}^{+}$ collisions. Quantal and semiclassical calculations","comments":"15 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.atom-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The elastic scattering and resonant charge transfer integral cross sections\nin $\\rm{H}(1s) + \\rm{H^+}$ collisions are computed for the center-of-mass\nenergy range of $10^{-10}-10$ eV. Fully quantal and semiclassical approaches\nare utilized in these calculations. The reliability of the semiclassical\napproximation for very low collision energies is discussed. The results are\ncompared with available data from the literature.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 19:16:30 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Khoma', 'Mykhaylo', '']]","extracted_entities":"[{'text': 'Fully quantal and semiclassical approaches', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"Fully quantal and semiclassical approaches","similarity_score":0.6044770479}
{"id":2503.14697,"submitter":"Juan Sosa","authors":"Juan Sosa and Carlo Mart\\'inez","title":"Bayesian Sociality Models: A Scalable and Flexible Alternative for\n  Network Analysis","comments":"44 pages, 4 tables, 10 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ME stat.CO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Bayesian sociality models provide a scalable and flexible alternative for\nnetwork analysis, capturing degree heterogeneity through actor-specific\nparameters while mitigating the identifiability challenges of latent space\nmodels. This paper develops a comprehensive Bayesian inference framework,\nleveraging Markov chain Monte Carlo and variational inference to assess their\nefficiency-accuracy trade-offs. Through empirical and simulation studies, we\ndemonstrate the model's robustness in goodness-of-fit, predictive performance,\nclustering, and other key network analysis tasks. The Bayesian paradigm further\nenhances uncertainty quantification and interpretability, positioning sociality\nmodels as a powerful and generalizable tool for modern network science.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 20:00:46 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Sosa', 'Juan', ''], ['Mart\u00ednez', 'Carlo', '']]","extracted_entities":"[{'text': 'uncertainty quantification', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"uncertainty quantification","similarity_score":0.5714546442}
{"id":2503.14785,"submitter":"Nima Negarandeh","authors":"Nima Negarandeh, Carlos Mora, Ramin Bostanabad","title":"SEEK: Self-adaptive Explainable Kernel For Nonstationary Gaussian\n  Processes","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Gaussian processes (GPs) are powerful probabilistic models that define\nflexible priors over functions, offering strong interpretability and\nuncertainty quantification. However, GP models often rely on simple, stationary\nkernels which can lead to suboptimal predictions and miscalibrated uncertainty\nestimates, especially in nonstationary real-world applications. In this paper,\nwe introduce SEEK, a novel class of learnable kernels to model complex,\nnonstationary functions via GPs. Inspired by artificial neurons, SEEK is\nderived from first principles to ensure symmetry and positive\nsemi-definiteness, key properties of valid kernels. The proposed method\nachieves flexible and adaptive nonstationarity by learning a mapping from a set\nof base kernels. Compared to existing techniques, our approach is more\ninterpretable and much less prone to overfitting. We conduct comprehensive\nsensitivity analyses and comparative studies to demonstrate that our approach\nis not robust to only many of its design choices, but also outperforms existing\nstationary\/nonstationary kernels in both mean prediction accuracy and\nuncertainty quantification.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 23:30:02 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Negarandeh', 'Nima', ''], ['Mora', 'Carlos', ''], ['Bostanabad', 'Ramin', '']]","extracted_entities":"[{'text': 'uncertainty quantification', 'label': 'quantisation'}, {'text': 'uncertainty quantification', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"uncertainty quantification","similarity_score":0.5714546442}
{"id":2503.14794,"submitter":"Dougal Davis","authors":"Dougal Davis and Lucas Mason-Brown","title":"Hodge theory, intertwining functors, and the Orbit Method for real\n  reductive groups","comments":"64 pages. Comments welcome!","journal-ref":null,"doi":null,"report-no":null,"categories":"math.RT math.AG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We study the Hodge filtrations of Schmid and Vilonen on unipotent\nrepresentations of real reductive groups. We show that for various well-defined\nclasses of unipotent representations (including, for example, the oscillator\nrepresentations of metaplectic groups, the minimal representations of all\nsimple groups, and all unipotent representations of complex groups) the Hodge\nfiltration coincides with the quantization filtration predicted by the Orbit\nMethod. We deduce a number of longstanding conjectures about such\nrepresentations, including a proof that they are unitary and a description of\ntheir $K$-types in terms of co-adjoint orbits. The proofs rely heavily on\ncertain good homological properties of the Hodge filtrations on weakly\nunipotent representations, which are established using a Hodge-theoretic\nupgrade of the Beilinson-Bernstein theory of intertwining functors for\n$\\mathcal{D}$-modules on the flag variety. The latter consists of an action of\nthe affine Hecke algebra on a category of filtered monodromic\n$\\mathcal{D}$-modules, which we use to compare Hodge filtrations coming from\ndifferent localizations of the same representation. As an application of the\nsame methods, we also prove a new cohomology vanishing theorem for mixed Hodge\nmodules on partial flag varieties.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 00:01:22 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Davis', 'Dougal', ''], ['Mason-Brown', 'Lucas', '']]","extracted_entities":"[{'text': 'quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization","similarity_score":0.8134455085}
{"id":2503.15446,"submitter":"Yutaka Yoshida","authors":"Yutaka Yoshida","title":"Quantized Coulomb branch of 4d $\\mathcal{N}=2$ $Sp(N)$ gauge theory and\n  spherical DAHA of $(C_N^{\\vee}, C_N)$-type","comments":"34 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-th math-ph math.AG math.MP math.RT","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We study BPS loop operators in a 4d $\\mathcal{N}=2$ $Sp(N)$ gauge theory with\nfour hypermultiplets in the fundamental representation and one hypermultiplet\nin the anti-symmetric representation. The algebra of BPS loop operators in the\n$\\Omega$-background provides a deformation quantization of the Coulomb branch,\nwhich is expected to coincide with the quantized K-theoretic Coulomb branch in\nthe mathematical literature. For the rank-one case, i.e., $Sp(1) \\simeq SU(2)$,\nwe show that the quantization of the Coulomb branch, evaluated using the\nsupersymmetric localization formula, agrees with the polynomial representation\nof the spherical part of the double affine Hecke algebra (spherical DAHA) of\n$(C_1^{\\vee}, C_1)$-type. For higher-rank cases, where $N \\geq 2$, we\nconjecture that the quantized Coulomb branch of the 4d $\\mathcal{N}=2$ $Sp(N)$\ngauge theory is isomorphic to the spherical DAHA of $(C_N^{\\vee}, C_N)$-type .\nAs evidence for this conjecture, we demonstrate that the quantization of an 't\nHooft loop agrees with the Koornwinder operator in the polynomial\nrepresentation of the spherical DAHA.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:24:50 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Yoshida', 'Yutaka', '']]","extracted_entities":"[{'text': 'deformation quantization', 'label': 'quantisation'}, {'text': 'quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization","similarity_score":0.8134455085}
{"id":2503.15465,"submitter":"Ruichen Chen","authors":"Ruichen Chen, Keith G. Mills, Di Niu","title":"FP4DiT: Towards Effective Floating Point Quantization for Diffusion\n  Transformers","comments":"The code is available at https:\/\/github.com\/cccrrrccc\/FP4DiT","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Diffusion Models (DM) have revolutionized the text-to-image visual generation\nprocess. However, the large computational cost and model footprint of DMs\nhinders practical deployment, especially on edge devices. Post-training\nquantization (PTQ) is a lightweight method to alleviate these burdens without\nthe need for training or fine-tuning. While recent DM PTQ methods achieve W4A8\non integer-based PTQ, two key limitations remain: First, while most existing DM\nPTQ methods evaluate on classical DMs like Stable Diffusion XL, 1.5 or earlier,\nwhich use convolutional U-Nets, newer Diffusion Transformer (DiT) models like\nthe PixArt series, Hunyuan and others adopt fundamentally different transformer\nbackbones to achieve superior image synthesis. Second, integer (INT)\nquantization is prevailing in DM PTQ but doesn't align well with the network\nweight and activation distribution, while Floating-Point Quantization (FPQ) is\nstill under-investigated, yet it holds the potential to better align the weight\nand activation distributions in low-bit settings for DiT. In response, we\nintroduce FP4DiT, a PTQ method that leverages FPQ to achieve W4A6 quantization.\nSpecifically, we extend and generalize the Adaptive Rounding PTQ technique to\nadequately calibrate weight quantization for FPQ and demonstrate that DiT\nactivations depend on input patch data, necessitating robust online activation\nquantization techniques. Experimental results demonstrate that FP4DiT\noutperforms integer-based PTQ at W4A6 and W4A8 precision and generates\nconvincing visual content on PixArt-$\\alpha$, PixArt-$\\Sigma$ and Hunyuan in\nterms of several T2I metrics such as HPSv2 and CLIP.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:44:21 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Chen', 'Ruichen', ''], ['Mills', 'Keith G.', ''], ['Niu', 'Di', '']]","extracted_entities":"[{'text': 'Post-training\\nquantization', 'label': 'quantisation'}, {'text': 'PTQ', 'label': 'quantisation'}, {'text': 'PTQ', 'label': 'quantisation'}, {'text': 'integer (INT)\\nquantization', 'label': 'quantisation'}, {'text': 'Floating-Point Quantization', 'label': 'quantisation'}, {'text': 'FPQ', 'label': 'quantisation'}, {'text': 'FPQ', 'label': 'quantisation'}, {'text': 'PTQ', 'label': 'quantisation'}, {'text': 'FPQ', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"Post-training\nquantization","similarity_score":0.6493542194}
{"id":2503.15482,"submitter":"Richard Barney","authors":"Richard Barney, Djamil Lakhdar-Hamina, Victor Galitski","title":"Natural Quantization of Neural Networks","comments":"7 pages, 8 figures, 1 table","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.dis-nn cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We propose a natural quantization of a standard neural network, where the\nneurons correspond to qubits and the activation functions are implemented via\nquantum gates and measurements. The simplest quantized neural network\ncorresponds to applying single-qubit rotations, with the rotation angles being\ndependent on the weights and measurement outcomes of the previous layer. This\nrealization has the advantage of being smoothly tunable from the purely\nclassical limit with no quantum uncertainty (thereby reproducing the classical\nneural network exactly) to a quantum case, where superpositions introduce an\nintrinsic uncertainty in the network. We benchmark this architecture on a\nsubset of the standard MNIST dataset and find a regime of \"quantum advantage,\"\nwhere the validation error rate in the quantum realization is smaller than that\nin the classical model. We also consider another approach where quantumness is\nintroduced via weak measurements of ancilla qubits entangled with the neuron\nqubits. This quantum neural network also allows for smooth tuning of the degree\nof quantumness by controlling an entanglement angle, $g$, with $g=\\frac\\pi 2$\nreplicating the classical regime. We find that validation error is also\nminimized within the quantum regime in this approach. We also observe a quantum\ntransition, with sharp loss of the quantum network's ability to learn at a\ncritical point $g_c$. The proposed quantum neural networks are readily\nrealizable in present-day quantum computers on commercial datasets.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:57:11 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Barney', 'Richard', ''], ['Lakhdar-Hamina', 'Djamil', ''], ['Galitski', 'Victor', '']]","extracted_entities":"[{'text': 'natural quantization', 'label': 'quantisation'}, {'text': 'smoothly tunable', 'label': 'Fine-tuning'}, {'text': 'quantum advantage', 'label': 'quantisation'}, {'text': 'smooth tuning', 'label': 'Fine-tuning'}]","assigned_concept":"quantisation","matched_keyword":"natural quantization","similarity_score":0.7380500436}
{"id":2503.15591,"submitter":"Minsung Kim","authors":"Hee-Cheol Kim, Minsung Kim, Sung-Soo Kim, Kimyeong Lee, Xin Wang","title":"Probing Quantum Curves and Transitions in 5d SQFTs via Defects and\n  Blowup Equations","comments":"72 pages, 22 figures","journal-ref":null,"doi":null,"report-no":"KIAS-Q25003, USTC-ICTS\/PCFT-25-09","categories":"hep-th","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We investigate codimension-2 defect partition functions and quantum\nSeiberg-Witten curves in 5d rank-1 supersymmetric QFTs, including\nnon-Lagrangian and Kaluza-Klein theories. Using generalized blowup equations,\nwe compute defect partition functions in the $\\Omega$-background and show that,\nin the Nekrasov-Shatashvili limit, they satisfy certain difference equations\nthat encode the quantization of classical Seiberg-Witten curves. Furthermore,\nwe explore novel transitions in the defect partition functions and their\nrelation to coordinate transformations of quantum Seiberg-Witten curves, with a\nfocus on SL(2,$\\mathbb{Z}$) transformations and Hanany-Witten transitions.\nThese findings provide new insights into the interplay between codimension-2\ndefects, quantum curves, and the geometric structure of 5d supersymmetric QFTs.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 18:00:00 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Kim', 'Hee-Cheol', ''], ['Kim', 'Minsung', ''], ['Kim', 'Sung-Soo', ''], ['Lee', 'Kimyeong', ''], ['Wang', 'Xin', '']]","extracted_entities":"[{'text': 'quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization","similarity_score":0.8134455085}
{"id":2503.15642,"submitter":"Carlo Cepollaro","authors":"Fatemeh Bibak, Carlo Cepollaro, Nicol\\'as Medina S\\'anchez, Borivoje\n  Daki\\'c, \\v{C}aslav Brukner","title":"The classical limit of quantum mechanics through coarse-grained\n  measurements","comments":"Main text: 25 pages, Appendix: 2 pages, Figures: 5","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph physics.class-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We address the classical limit of quantum mechanics, focusing on its\nemergence through coarse-grained measurements when multiple outcomes are\nconflated into slots. We rigorously derive effective classical kinematics under\nsuch measurements, demonstrating that when the volume of the coarse-grained\nslot in phase space significantly exceeds Planck's constant, quantum states can\nbe effectively described by classical probability distributions. Furthermore,\nwe show that the dynamics, derived under coarse-grained observations and the\nlinear approximation of the quantum Hamiltonian around its classical values\nwithin the slots, is effectively described by a classical Hamiltonian following\nLiouville dynamics. The classical Hamiltonian obtained through this process is\nequivalent to the one from which the underlying quantum Hamiltonian is derived\nvia the (Dirac) quantization procedure, completing the quantization-classical\nlimit loop. The Ehrenfest time, marking the duration within which classical\nbehavior remains valid, is analyzed for various physical systems. The\nimplications of these findings are discussed in the context of both macroscopic\nand microscopic systems, revealing the mechanisms behind their observed\nclassicality. This work provides a comprehensive framework for understanding\nthe quantum-to-classical transition and addresses foundational questions about\nthe consistency of the quantization-classical limit cycle.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 18:59:58 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Bibak', 'Fatemeh', ''], ['Cepollaro', 'Carlo', ''], ['S\u00e1nchez', 'Nicol\u00e1s Medina', ''], ['Daki\u0107', 'Borivoje', ''], ['Brukner', '\u010caslav', '']]","extracted_entities":"[{'text': 'Dirac) quantization procedure', 'label': 'quantisation'}, {'text': 'quantization-classical\\nlimit loop', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"Dirac) quantization procedure","similarity_score":0.6254645586}
{"id":2503.15889,"submitter":"Cynthia Dong","authors":"Cynthia Dong, Hong Jia, Young D. Kwon, Georgios Rizos, Cecilia Mascolo","title":"LeanTTA: A Backpropagation-Free and Stateless Approach to Quantized\n  Test-Time Adaptation on Edge Devices","comments":"8 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While there are many advantages to deploying machine learning models on edge\ndevices, the resource constraints of mobile platforms, the dynamic nature of\nthe environment, and differences between the distribution of training versus\nin-the-wild data make such deployments challenging. Current test-time\nadaptation methods are often memory-intensive and not designed to be\nquantization-compatible or deployed on low-resource devices. To address these\nchallenges, we present LeanTTA, a novel backpropagation-free and stateless\nframework for quantized test-time adaptation tailored to edge devices. Our\napproach minimizes computational costs by dynamically updating normalization\nstatistics without backpropagation, which frees LeanTTA from the common pitfall\nof relying on large batches and historical data, making our method robust to\nrealistic deployment scenarios. Our approach is the first to enable further\ncomputational gains by combining partial adaptation with quantized module\nfusion. We validate our framework across sensor modalities, demonstrating\nsignificant improvements over state-of-the-art TTA methods, including a 15.7%\nerror reduction, peak memory usage of only 11.2MB for ResNet18, and fast\nadaptation within an order-of-magnitude of normal inference speeds on-device.\nLeanTTA provides a robust solution for achieving the right trade offs between\naccuracy and system efficiency in edge deployments, addressing the unique\nchallenges posed by limited data and varied operational conditions.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 06:27:09 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Dong', 'Cynthia', ''], ['Jia', 'Hong', ''], ['Kwon', 'Young D.', ''], ['Rizos', 'Georgios', ''], ['Mascolo', 'Cecilia', '']]","extracted_entities":"[{'text': 'quantization-compatible', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization-compatible","similarity_score":0.7064218521}
{"id":2503.16027,"submitter":"Yiming Yang","authors":"Yiming Yang, Deyu Ming, Serge Guillas","title":"Distribution of Deep Gaussian process Gradients and Sequential Design\n  for Simulators with Sharp Variations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.CO stat.AP stat.ME","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Deep Gaussian Processes (DGPs), multi-layered extensions of GPs, better\nemulate simulators with regime transitions or sharp changes than standard GPs.\nGradient information is crucial for tasks like sensitivity analysis and\ndimension reduction. Although gradient posteriors are well-defined in GPs,\nextending them to DGPs is challenging due to their hierarchical structure. We\npropose a novel method to approximate the DGP emulator's gradient distribution,\nenabling efficient gradient computation with uncertainty quantification (UQ).\nOur approach derives an analytical gradient mean and the covariance. The\nnumerical results show that our method outperforms GP and DGP with finite\ndifference methods in gradient accuracy, offering the extra unique benefit of\nUQ. Based on the gradient information, we further propose a sequential design\ncriterion to identify the sharp variation regions efficiently, with the\ngradient norm as a key indicator whose distribution can be readily evaluated in\nour framework. We evaluated the proposed sequential design using synthetic\nexamples and empirical applications, demonstrating its superior performance in\nemulating functions with sharp changes compared to existing design methods. The\nDGP gradient computation is seamlessly integrated into the advanced Python\npackage dgpsi for DGP emulation, along with the proposed sequential design\navailable at https:\/\/github.com\/yyimingucl\/DGP.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 10:48:56 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Yang', 'Yiming', ''], ['Ming', 'Deyu', ''], ['Guillas', 'Serge', '']]","extracted_entities":"[{'text': 'uncertainty quantification', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"uncertainty quantification","similarity_score":0.5714546442}
{"id":2503.16163,"submitter":"Shibo Jie","authors":"Shibo Jie, Yehui Tang, Kai Han, Zhi-Hong Deng, Jing Han","title":"SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/","abstract":"  Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 14:01:56 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Jie', 'Shibo', ''], ['Tang', 'Yehui', ''], ['Han', 'Kai', ''], ['Deng', 'Zhi-Hong', ''], ['Han', 'Jing', '']]","extracted_entities":"[{'text': 'eviction', 'label': 'quantisation'}, {'text': 'merging', 'label': 'quantisation'}, {'text': 'quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization","similarity_score":0.8134455085}
{"id":2503.16194,"submitter":"Ziyao Guo","authors":"Ziyao Guo, Kaipeng Zhang, Michael Qizhe Shieh","title":"Improving Autoregressive Image Generation through Coarse-to-Fine Token\n  Prediction","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Autoregressive models have shown remarkable success in image generation by\nadapting sequential prediction techniques from language modeling. However,\napplying these approaches to images requires discretizing continuous pixel data\nthrough vector quantization methods like VQ-VAE. To alleviate the quantization\nerrors that existed in VQ-VAE, recent works tend to use larger codebooks.\nHowever, this will accordingly expand vocabulary size, complicating the\nautoregressive modeling task. This paper aims to find a way to enjoy the\nbenefits of large codebooks without making autoregressive modeling more\ndifficult. Through empirical investigation, we discover that tokens with\nsimilar codeword representations produce similar effects on the final generated\nimage, revealing significant redundancy in large codebooks. Based on this\ninsight, we propose to predict tokens from coarse to fine (CTF), realized by\nassigning the same coarse label for similar tokens. Our framework consists of\ntwo stages: (1) an autoregressive model that sequentially predicts coarse\nlabels for each token in the sequence, and (2) an auxiliary model that\nsimultaneously predicts fine-grained labels for all tokens conditioned on their\ncoarse labels. Experiments on ImageNet demonstrate our method's superior\nperformance, achieving an average improvement of 59 points in Inception Score\ncompared to baselines. Notably, despite adding an inference step, our approach\nachieves faster sampling speeds.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 14:41:29 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Guo', 'Ziyao', ''], ['Zhang', 'Kaipeng', ''], ['Shieh', 'Michael Qizhe', '']]","extracted_entities":"[{'text': 'vector quantization methods', 'label': 'quantisation'}, {'text': 'VQ-VAE', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"vector quantization methods","similarity_score":0.5234082341}
{"id":2503.16198,"submitter":"Igor Pikovski","authors":"Vasileios Fragkos and Igor Pikovski","title":"Probing classical and quantum violations of the equivalence of active\n  and passive gravitational mass","comments":"13 pages, 5 figures, 2 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph gr-qc physics.atom-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The equivalence of active and passive (EAP) gravitational mass is one of the\nmost fundamental principles of gravity. But in contrast to the usual\nequivalence of inertial and (passive) gravitational mass, the EAP has not\nreceived much attention. Here we revisit this principle and show how it can be\nused to probe quantum gravity in laboratory-based experiments. We first examine\nhow the dynamics under EAP violations affects classical systems and show that\nnew laboratory tests can be performed, to improve over the current experimental\nbounds and to test new manifestations of EAP violations. We then extend the\nanalysis to the quantum domain, where quantized energy contributes to mass and\nthe EAP principle can thus shed light on how quantum source masses would\ngravitate. We show that experiments with cold polar molecules, and future\nexperiments with nuclear atomic clocks, can test the quantum EAP in a regime\nwhere quantum gravity phenomenology could become relevant. Our results open new\nopportunities for fundamental tests of gravity in high-precision laboratory\nexperiments that can shed light on foundational principles of gravity and its\ninterface with quantum theory.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 14:45:24 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Fragkos', 'Vasileios', ''], ['Pikovski', 'Igor', '']]","extracted_entities":"[{'text': 'quantized energy', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantized energy","similarity_score":0.5685881972}
{"id":2503.16222,"submitter":"Teresa Klatzer","authors":"Teresa Klatzer and Savvas Melidonis and Marcelo Pereyra and\n  Konstantinos C. Zygalakis","title":"Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson\n  Inverse Problems","comments":"31 pages, 17 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.CO cs.CV cs.NA math.NA stat.ML","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper introduces a novel plug-and-play (PnP) Langevin sampling\nmethodology for Bayesian inference in low-photon Poisson imaging problems, a\nchallenging class of problems with significant applications in astronomy,\nmedicine, and biology. PnP Langevin sampling algorithms offer a powerful\nframework for Bayesian image restoration, enabling accurate point estimation as\nwell as advanced inference tasks, including uncertainty quantification and\nvisualization analyses, and empirical Bayesian inference for automatic model\nparameter tuning. However, existing PnP Langevin algorithms are not well-suited\nfor low-photon Poisson imaging due to high solution uncertainty and poor\nregularity properties, such as exploding gradients and non-negativity\nconstraints. To address these challenges, we propose two strategies for\nextending Langevin PnP sampling to Poisson imaging models: (i) an accelerated\nPnP Langevin method that incorporates boundary reflections and a Poisson\nlikelihood approximation and (ii) a mirror sampling algorithm that leverages a\nRiemannian geometry to handle the constraints and the poor regularity of the\nlikelihood without approximations. The effectiveness of these approaches is\ndemonstrated through extensive numerical experiments and comparisons with\nstate-of-the-art methods.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 15:17:05 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Klatzer', 'Teresa', ''], ['Melidonis', 'Savvas', ''], ['Pereyra', 'Marcelo', ''], ['Zygalakis', 'Konstantinos C.', '']]","extracted_entities":"[{'text': 'uncertainty quantification', 'label': 'quantisation'}, {'text': 'automatic model\\nparameter tuning', 'label': 'Fine-tuning'}]","assigned_concept":"quantisation","matched_keyword":"uncertainty quantification","similarity_score":0.5714546442}
{"id":2503.16364,"submitter":"Naeim Zarezadeh","authors":"Z. Zarezadeh, N. Zarezadeh","title":"Neural Networks: According to the Principles of Grassmann Algebra","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  In this paper, we explore the algebra of quantum idempotents and the\nquantization of fermions which gives rise to a Hilbert space equal to the\nGrassmann algebra associated with the Lie algebra. Since idempotents carry\nrepresentations of the algebra under consideration, they form algebraic\nvarieties and smooth manifolds in the natural topology. In addition to the\nmotivation of linking up mathematical physics with machine learning, it is also\nshown that by using idempotents and invariant subspace of the corresponding\nalgebras, these representations encode and perhaps provide a probabilistic\ninterpretation of reasoning and relational paths in geometrical terms.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:21:23 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zarezadeh', 'Z.', ''], ['Zarezadeh', 'N.', '']]","extracted_entities":"[{'text': 'quantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"quantization","similarity_score":0.8134455085}
{"id":2503.1643,"submitter":"Yuqing Wang","authors":"Yuqing Wang, Zhijie Lin, Yao Teng, Yuanzhi Zhu, Shuhuai Ren, Jiashi\n  Feng, Xihui Liu","title":"Bridging Continuous and Discrete Tokens for Autoregressive Visual\n  Generation","comments":"Project page: https:\/\/yuqingwang1029.github.io\/TokenBridge","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Autoregressive visual generation models typically rely on tokenizers to\ncompress images into tokens that can be predicted sequentially. A fundamental\ndilemma exists in token representation: discrete tokens enable straightforward\nmodeling with standard cross-entropy loss, but suffer from information loss and\ntokenizer training instability; continuous tokens better preserve visual\ndetails, but require complex distribution modeling, complicating the generation\npipeline. In this paper, we propose TokenBridge, which bridges this gap by\nmaintaining the strong representation capacity of continuous tokens while\npreserving the modeling simplicity of discrete tokens. To achieve this, we\ndecouple discretization from the tokenizer training process through\npost-training quantization that directly obtains discrete tokens from\ncontinuous representations. Specifically, we introduce a dimension-wise\nquantization strategy that independently discretizes each feature dimension,\npaired with a lightweight autoregressive prediction mechanism that efficiently\nmodel the resulting large token space. Extensive experiments show that our\napproach achieves reconstruction and generation quality on par with continuous\nmethods while using standard categorical prediction. This work demonstrates\nthat bridging discrete and continuous paradigms can effectively harness the\nstrengths of both approaches, providing a promising direction for high-quality\nvisual generation with simple autoregressive modeling. Project page:\nhttps:\/\/yuqingwang1029.github.io\/TokenBridge.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:59:59 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wang', 'Yuqing', ''], ['Lin', 'Zhijie', ''], ['Teng', 'Yao', ''], ['Zhu', 'Yuanzhi', ''], ['Ren', 'Shuhuai', ''], ['Feng', 'Jiashi', ''], ['Liu', 'Xihui', '']]","extracted_entities":"[{'text': 'continuous tokens', 'label': 'Large Language Model'}, {'text': 'continuous tokens', 'label': 'Large Language Model'}, {'text': 'post-training quantization', 'label': 'quantisation'}, {'text': 'dimension-wise\\nquantization', 'label': 'quantisation'}]","assigned_concept":"quantisation","matched_keyword":"post-training quantization","similarity_score":0.6493542194}
{"id":2502.12918,"submitter":"Harish Doraiswamy","authors":"Sriram Dharwada, Himanshu Devrani, Jayant Haritsa, Harish Doraiswamy","title":"Query Rewriting via LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DB","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  When complex SQL queries suffer slow executions despite query optimization,\nDBAs typically invoke automated query rewriting tools to recommend ``lean''\nequivalents that are conducive to faster execution. The rewritings are usually\nachieved via transformation rules, but these rules are limited in scope and\ndifficult to update in a production system. Recently, LLM-based techniques have\nalso been suggested, but they are prone to semantic and syntactic errors.\n  We investigate here how the remarkable cognitive capabilities of LLMs can be\nleveraged for performant query rewriting while incorporating safeguards and\noptimizations to ensure correctness and efficiency. Our study shows that these\ngoals can be progressively achieved through incorporation of (a) an ensemble\nsuite of basic prompts, (b) database-sensitive prompts via redundancy removal\nand selectivity-based rewriting rules, and (c) LLM token probability-guided\nrewrite paths. Further, a suite of logic-based and statistical tools can be\nused to check for semantic violations in the rewrites prior to DBA\nconsideration.\n  We have implemented the above LLM-infused techniques in the LITHE system, and\nevaluated complex analytic queries from standard benchmarks on contemporary\ndatabase platforms. The results show significant performance improvements for\nslow queries, with regard to both abstract costing and actual execution, over\nboth SOTA techniques and the native query optimizer. For instance, with TPC-DS\non PostgreSQL, the geometric mean of the runtime speedups for slow queries was\nas high as 18.4 over the native optimizer, whereas SOTA delivered 6 in\ncomparison.\n  Overall, LITHE is a promising step toward viable LLM-based advisory tools for\nameliorating enterprise query performance.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Feb 2025 14:59:37 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 07:55:40 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Dharwada', 'Sriram', ''], ['Devrani', 'Himanshu', ''], ['Haritsa', 'Jayant', ''], ['Doraiswamy', 'Harish', '']]","extracted_entities":"[{'text': 'LLM-based techniques', 'label': 'LLM-based'}, {'text': 'LLMs', 'label': 'LLM-based'}, {'text': 'basic prompts', 'label': 'Prompting'}, {'text': 'database-sensitive prompts', 'label': 'Prompting'}]","assigned_concept":"LLM-based","matched_keyword":"LLMs","similarity_score":0.8177113533}
{"id":2503.15129,"submitter":"Man Fai Wong","authors":"Man Fai Wong, Chee Wei Tan","title":"Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code\n  Generation by Large Language Models","comments":null,"journal-ref":null,"doi":"10.1109\/TBDATA.2024.3524104","report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  This paper studies how AI-assisted programming and large language models\n(LLM) improve software developers' ability via AI tools (LLM agents) like\nGithub Copilot and Amazon CodeWhisperer, while integrating human feedback to\nenhance reinforcement learning (RLHF) with crowd-sourced computation to enhance\ntext-to-code generation. Additionally, we demonstrate that our Bayesian\noptimization framework supports AI alignment in code generation by distributing\nthe feedback collection burden, highlighting the value of collecting human\nfeedback of good quality. Our empirical evaluations demonstrate the efficacy of\nthis approach, showcasing how LLM agents can be effectively trained for\nimproved text-to-code generation. Our Bayesian optimization framework can be\ndesigned for general domain-specific languages, promoting the alignment of\nlarge language model capabilities with human feedback in AI-assisted\nprogramming for code generation.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:44:47 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Wong', 'Man Fai', ''], ['Tan', 'Chee Wei', '']]","extracted_entities":"[{'text': 'Github Copilot', 'label': 'Open-source LLMs'}, {'text': 'LLM agents', 'label': 'LLM-based'}]","assigned_concept":"LLM-based","matched_keyword":"LLM agents","similarity_score":0.7279455662}
{"id":2308.10711,"submitter":"Jordan Patracone","authors":"Sara Venturini, Marianna de Santis (UNIROMA), Jordan Patracone\n  (MALICE), Francesco Rinaldi (Unipd), Saverio Salzo (DIAG UNIROMA), Martin\n  Schmidt","title":"Relax and penalize: a new bilevel approach to mixed-binary\n  hyperparameter optimization","comments":null,"journal-ref":"Transactions on Machine Learning Research Journal, 2025","doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In recent years, bilevel approaches have become very popular to efficiently\nestimate high-dimensional hyperparameters of machine learning models. However,\nto date, binary parameters are handled by continuous relaxation and rounding\nstrategies, which could lead to inconsistent solutions. In this context, we\ntackle the challenging optimization of mixed-binary hyperparameters by\nresorting to an equivalent continuous bilevel reformulation based on an\nappropriate penalty term. We propose an algorithmic framework that, under\nsuitable assumptions, is guaranteed to provide mixed-binary solutions.\nMoreover, the generality of the method allows to safely use existing continuous\nbilevel solvers within the proposed framework. We evaluate the performance of\nour approach for two specific machine learning problems, i.e., the estimation\nof the group-sparsity structure in regression problems and the data\ndistillation problem. The reported results show that our method is competitive\nwith state-of-the-art approaches based on relaxation and rounding\n","versions":"[{'version': 'v1', 'created': 'Mon, 21 Aug 2023 13:24:52 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 07:59:35 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Venturini', 'Sara', '', 'UNIROMA'], ['de Santis', 'Marianna', '', 'UNIROMA'], ['Patracone', 'Jordan', '', 'MALICE'], ['Rinaldi', 'Francesco', '', 'Unipd'], ['Salzo', 'Saverio', '', 'DIAG UNIROMA'], ['Schmidt', 'Martin', '']]","extracted_entities":"[{'text': 'data\\ndistillation problem', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"data\ndistillation problem","similarity_score":0.6382700801}
{"id":2402.12265,"submitter":"Christophe Roux","authors":"Christophe Roux, Max Zimmer, Sebastian Pokutta","title":"On the Byzantine-Resilience of Distillation-Based Federated Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.DC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Federated Learning (FL) algorithms using Knowledge Distillation (KD) have\nreceived increasing attention due to their favorable properties with respect to\nprivacy, non-i.i.d. data and communication cost. These methods depart from\ntransmitting model parameters and instead communicate information about a\nlearning task by sharing predictions on a public dataset. In this work, we\nstudy the performance of such approaches in the byzantine setting, where a\nsubset of the clients act in an adversarial manner aiming to disrupt the\nlearning process. We show that KD-based FL algorithms are remarkably resilient\nand analyze how byzantine clients can influence the learning process. Based on\nthese insights, we introduce two new byzantine attacks and demonstrate their\nability to break existing byzantine-resilient methods. Additionally, we propose\na novel defence method which enhances the byzantine resilience of KD-based FL\nalgorithms. Finally, we provide a general framework to obfuscate attacks,\nmaking them significantly harder to detect, thereby improving their\neffectiveness. Our findings serve as an important building block in the\nanalysis of byzantine FL, contributing through the development of new attacks\nand new defence mechanisms, further advancing the robustness of KD-based FL\nalgorithms.\n","versions":"[{'version': 'v1', 'created': 'Mon, 19 Feb 2024 16:26:40 GMT'}, {'version': 'v2', 'created': 'Wed, 9 Oct 2024 12:38:26 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 14:08:19 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Roux', 'Christophe', ''], ['Zimmer', 'Max', ''], ['Pokutta', 'Sebastian', '']]","extracted_entities":"[{'text': 'Knowledge Distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Knowledge Distillation","similarity_score":1.0000002384}
{"id":2405.12419,"submitter":"Ali Bahri","authors":"Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, Milad Cheraghalikhani,\n  Gustavo Adolfo Vargas Hakim, David Osowiechi, Farzad Beizaee, Ismail Ben\n  Ayed, Christian Desrosiers","title":"GeoMask3D: Geometrically Informed Mask Selection for Self-Supervised\n  Point Cloud Learning in 3D","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  We introduce a pioneering approach to self-supervised learning for point\nclouds, employing a geometrically informed mask selection strategy called\nGeoMask3D (GM3D) to boost the efficiency of Masked Auto Encoders (MAE). Unlike\nthe conventional method of random masking, our technique utilizes a\nteacher-student model to focus on intricate areas within the data, guiding the\nmodel's focus toward regions with higher geometric complexity. This strategy is\ngrounded in the hypothesis that concentrating on harder patches yields a more\nrobust feature representation, as evidenced by the improved performance on\ndownstream tasks. Our method also presents a complete-to-partial feature-level\nknowledge distillation technique designed to guide the prediction of geometric\ncomplexity utilizing a comprehensive context from feature-level information.\nExtensive experiments confirm our method's superiority over State-Of-The-Art\n(SOTA) baselines, demonstrating marked improvements in classification, and\nfew-shot tasks.\n","versions":"[{'version': 'v1', 'created': 'Mon, 20 May 2024 23:53:42 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 05:35:35 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Bahri', 'Ali', ''], ['Yazdanpanah', 'Moslem', ''], ['Noori', 'Mehrdad', ''], ['Cheraghalikhani', 'Milad', ''], ['Hakim', 'Gustavo Adolfo Vargas', ''], ['Osowiechi', 'David', ''], ['Beizaee', 'Farzad', ''], ['Ayed', 'Ismail Ben', ''], ['Desrosiers', 'Christian', '']]","extracted_entities":"[{'text': 'complete-to-partial feature-level\\nknowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'few-shot tasks', 'label': 'Few-shot Learning'}]","assigned_concept":"Knowledge distillation","matched_keyword":"complete-to-partial feature-level\nknowledge distillation","similarity_score":0.8110019565}
{"id":2406.01658,"submitter":"Song Tang","authors":"Song Tang, Wenxin Su, Mao Ye, Jianwei Zhang, and Xiatian Zhu","title":"Proxy Denoising for Source-Free Domain Adaptation","comments":"This paper is accepted by ICLR 2025 (Oral, Top 1.8%)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model\nto an unlabeled target domain with no access to the source data. Inspired by\nthe success of large Vision-Language (ViL) models in many applications, the\nlatest research has validated ViL's benefit for SFDA by using their predictions\nas pseudo supervision. However, we observe that ViL's supervision could be\nnoisy and inaccurate at an unknown rate, introducing additional negative\neffects during adaption. To address this thus-far ignored challenge, we\nintroduce a novel Proxy Denoising (ProDe) approach. The key idea is to leverage\nthe ViL model as a proxy to facilitate the adaptation process towards the\nlatent domain-invariant space. We design a proxy denoising mechanism to correct\nViL's predictions, grounded on a proxy confidence theory that models the\ndynamic effect of proxy's divergence against the domain-invariant space during\nadaptation. To capitalize on the corrected proxy, we derive a mutual knowledge\ndistilling regularization. Extensive experiments show that ProDe significantly\noutperforms current state-of-the-art alternatives under the conventional closed\nset setting and more challenging open set, partial set, generalized SFDA,\nmulti-target, multi-source, and test-time settings. Our code and data are\navailable at https:\/\/github.com\/tntek\/source-free-domain-adaptation.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Jun 2024 17:36:36 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 13:42:34 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Tang', 'Song', ''], ['Su', 'Wenxin', ''], ['Ye', 'Mao', ''], ['Zhang', 'Jianwei', ''], ['Zhu', 'Xiatian', '']]","extracted_entities":"[{'text': 'ViL', 'label': 'Large Language Model'}, {'text': 'ViL', 'label': 'Large Language Model'}, {'text': 'ViL', 'label': 'Large Language Model'}, {'text': 'ViL', 'label': 'Large Language Model'}, {'text': 'mutual knowledge\\ndistilling regularization', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"mutual knowledge\ndistilling regularization","similarity_score":0.6160212159}
{"id":2406.03146,"submitter":"Erik Landolsi","authors":"Erik Landolsi, Fredrik Kahl","title":"Tiny models from tiny data: Textual and null-text inversion for few-shot\n  distillation","comments":"24 pages (13 main pages + references and appendix)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Few-shot learning deals with problems such as image classification using very\nfew training examples. Recent vision foundation models show excellent few-shot\ntransfer abilities, but are large and slow at inference. Using knowledge\ndistillation, the capabilities of high-performing but slow models can be\ntransferred to tiny, efficient models. However, common distillation methods\nrequire a large set of unlabeled data, which is not available in the few-shot\nsetting. To overcome this lack of data, there has been a recent interest in\nusing synthetic data. We expand on this line of research by presenting a novel\ndiffusion model inversion technique (TINT) combining the diversity of textual\ninversion with the specificity of null-text inversion. Using this method in a\nfew-shot distillation pipeline leads to state-of-the-art accuracy among small\nstudent models on popular benchmarks, while being significantly faster than\nprior work. Popular few-shot benchmarks involve evaluation over a large number\nof episodes, which is computationally cumbersome for methods involving\nsynthetic data generation. We also present a theoretical analysis on how the\naccuracy estimator variance depends on the number of episodes and query\nexamples, and use these results to lower the computational effort required for\nmethod evaluation. Finally, to further motivate the use of generative models in\nfew-shot distillation, we demonstrate that our method outperforms training on\nreal data mined from the dataset used in the original diffusion model training.\nSource code is available at https:\/\/github.com\/pixwse\/tiny2.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Jun 2024 11:01:42 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 12:04:41 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Landolsi', 'Erik', ''], ['Kahl', 'Fredrik', '']]","extracted_entities":"[{'text': 'Few-shot learning', 'label': 'Few-shot Learning'}, {'text': 'Recent vision foundation models', 'label': 'Foundation Model'}, {'text': 'knowledge\\ndistillation', 'label': 'Knowledge distillation'}, {'text': 'few-shot distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge\ndistillation","similarity_score":1.0000002384}
{"id":2406.05704,"submitter":"Xinhao Zhong","authors":"Xinhao Zhong, Hao Fang, Bin Chen, Xulin Gu, Meikang Qiu, Shuhan Qi,\n  Shu-Tao Xia","title":"Hierarchical Features Matter: A Deep Exploration of Progressive\n  Parameterization Method for Dataset Distillation","comments":"Accepted to CVPR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Dataset distillation is an emerging dataset reduction method, which condenses\nlarge-scale datasets while maintaining task accuracy. Current parameterization\nmethods achieve enhanced performance under extremely high compression ratio by\noptimizing determined synthetic dataset in informative feature domain. However,\nthey limit themselves to a fixed optimization space for distillation,\nneglecting the diverse guidance across different informative latent spaces. To\novercome this limitation, we propose a novel parameterization method dubbed\nHierarchical Parameterization Distillation (H-PD), to systematically explore\nhierarchical feature within provided feature space (e.g., layers within\npre-trained generative adversarial networks). We verify the correctness of our\ninsights by applying the hierarchical optimization strategy on GAN-based\nparameterization method. In addition, we introduce a novel class-relevant\nfeature distance metric to alleviate the computational burden associated with\nsynthetic dataset evaluation, bridging the gap between synthetic and original\ndatasets. Experimental results demonstrate that the proposed H-PD achieves a\nsignificant performance improvement under various settings with equivalent time\nconsumption, and even surpasses current generative distillation using diffusion\nmodels under extreme compression ratios IPC=1 and IPC=10.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Jun 2024 09:15:54 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Jun 2024 11:11:07 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 04:23:38 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhong', 'Xinhao', ''], ['Fang', 'Hao', ''], ['Chen', 'Bin', ''], ['Gu', 'Xulin', ''], ['Qiu', 'Meikang', ''], ['Qi', 'Shuhan', ''], ['Xia', 'Shu-Tao', '']]","extracted_entities":"[{'text': 'Dataset distillation', 'label': 'Knowledge distillation'}, {'text': 'Hierarchical Parameterization Distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Dataset distillation","similarity_score":0.6805129051}
{"id":2408.0198,"submitter":"Gongchu Li","authors":"Gongchu Li, Lei Chen, Si-Qi Zhang, Xu-Song Hong, Huaqing Xu, Yuancheng\n  Liu, You Zhou, Geng Chen, Chuan-Feng Li, Alioscia Hamma, Guang-Can Guo","title":"Measurement Induced Magic Resources","comments":"5 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Magic states and magic gates are crucial for achieving universal computation,\nbut some important questions about how magic resources should be implemented to\nattain quantum advantage have remained unexplored, for instance, in the context\nof Measurement-based Quantum Computation (MQC) with only single-qubit\nmeasurements. This work bridges the gap between MQC and the resource theory of\nmagic by introducing the concept of ``invested'' and ``potential\" magic\nresources. The former quantifies the magic cost associated with the MQC\nframework, serving both as a witness of magic resources and an upper bound for\nthe realization of a desired unitary transformation. Potential magic resources\nrepresent the maximum achievable magic resource in a given graph structure\ndefining the MQC. We utilize these concepts to analyze the magic resource\nrequirements of the Quantum Fourier Transform (QFT) and provide a fresh\nperspective on the universality of MQC of different resource states,\nhighlighting the crucial role of non-Pauli measurements for injecting magic. We\ndemonstrate experimentally our theoretical predictions in a high-fidelity\nfour-photon setup and demonstrate the efficiency of MQC in generating magic\nstates, surpassing the limitations of conventional magic state injection\nmethods. Our findings pave the way for future research exploring magic resource\noptimization and novel distillation schemes within the MQC framework,\ncontributing to the advancement of fault-tolerant universal quantum\ncomputation.\n","versions":"[{'version': 'v1', 'created': 'Sun, 4 Aug 2024 09:57:33 GMT'}, {'version': 'v2', 'created': 'Fri, 23 Aug 2024 17:48:04 GMT'}, {'version': 'v3', 'created': 'Thu, 29 Aug 2024 05:53:10 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 01:59:47 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Li', 'Gongchu', ''], ['Chen', 'Lei', ''], ['Zhang', 'Si-Qi', ''], ['Hong', 'Xu-Song', ''], ['Xu', 'Huaqing', ''], ['Liu', 'Yuancheng', ''], ['Zhou', 'You', ''], ['Chen', 'Geng', ''], ['Li', 'Chuan-Feng', ''], ['Hamma', 'Alioscia', ''], ['Guo', 'Guang-Can', '']]","extracted_entities":"[{'text': 'Quantum Fourier Transform (QFT)', 'label': 'quantisation'}, {'text': 'novel distillation schemes', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"novel distillation schemes","similarity_score":0.6211229563}
{"id":2408.12526,"submitter":"Weiyan Wang","authors":"Weiyan Wang, Yilun Jin, Yiming Zhang, Victor Junqiu Wei, Han Tian, Li\n  Chen, Jinbao Xue, Yangyu Tao, Di Wang, Kai Chen","title":"Exploiting Student Parallelism for Efficient GPU Inference of BERT-like\n  Models in Online Services","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Due to high accuracy, BERT-like models have been widely adopted by text\nmining and web searching. However, large BERT-like models suffer from\ninefficient online inference, facing the following two problems on GPUs: (1)\ntheir high accuracy relies on the large model depth, which linearly increases\nthe sequential computation on GPUs; (2) stochastic and dynamic online workloads\ncause extra costs from batching and paddings. Therefore, we present \\sys for\nthe real-world setting of GPU inference on online workloads. At its core, \\sys\nadopts stacking distillation and boosting ensemble, distilling the original\ndeep model into a group of shallow but virtually stacked student models running\nin parallel. This enables \\sys to achieve a lower model depth (e.g., two\nlayers) than the others and the lowest inference latency while maintaining\naccuracy. In addition, adaptive student pruning realizes dynamic student\nnumbers according to changing online workloads. Especially for occasional\nworkload bursts, it can temporarily decrease the student number with minimal\naccuracy loss to improve system throughput. We conduct comprehensive\nexperiments to verify the effectiveness, whose results show that \\sys\noutperforms the baselines by $4.1\\times\\sim 1.6\\times$ in latency while\nmaintaining accuracy and achieves up to $22.27\\times$ higher throughput for\nworkload bursts.\n","versions":"[{'version': 'v1', 'created': 'Thu, 22 Aug 2024 16:31:32 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2025 12:08:13 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 08:33:28 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wang', 'Weiyan', ''], ['Jin', 'Yilun', ''], ['Zhang', 'Yiming', ''], ['Wei', 'Victor Junqiu', ''], ['Tian', 'Han', ''], ['Chen', 'Li', ''], ['Xue', 'Jinbao', ''], ['Tao', 'Yangyu', ''], ['Wang', 'Di', ''], ['Chen', 'Kai', '']]","extracted_entities":"[{'text': 'stacking distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"stacking distillation","similarity_score":0.592784524}
{"id":2408.14506,"submitter":"Haoxuan Wang","authors":"Zhenghao Zhao, Haoxuan Wang, Yuzhang Shang, Kai Wang, Yan Yan","title":"Distilling Long-tailed Datasets","comments":"CVPR 2025. Code is available at https:\/\/github.com\/ichbill\/LTDD","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Dataset distillation aims to synthesize a small, information-rich dataset\nfrom a large one for efficient model training. However, existing dataset\ndistillation methods struggle with long-tailed datasets, which are prevalent in\nreal-world scenarios. By investigating the reasons behind this unexpected\nresult, we identified two main causes: 1) The distillation process on\nimbalanced datasets develops biased gradients, leading to the synthesis of\nsimilarly imbalanced distilled datasets. 2) The experts trained on such\ndatasets perform suboptimally on tail classes, resulting in misguided\ndistillation supervision and poor-quality soft-label initialization. To address\nthese issues, we first propose Distribution-agnostic Matching to avoid directly\nmatching the biased expert trajectories. It reduces the distance between the\nstudent and the biased expert trajectories and prevents the tail class bias\nfrom being distilled to the synthetic dataset. Moreover, we improve the\ndistillation guidance with Expert Decoupling, which jointly matches the\ndecoupled backbone and classifier to improve the tail class performance and\ninitialize reliable soft labels. This work pioneers the field of long-tailed\ndataset distillation, marking the first effective effort to distill long-tailed\ndatasets.\n","versions":"[{'version': 'v1', 'created': 'Sat, 24 Aug 2024 15:36:36 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 01:46:48 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhao', 'Zhenghao', ''], ['Wang', 'Haoxuan', ''], ['Shang', 'Yuzhang', ''], ['Wang', 'Kai', ''], ['Yan', 'Yan', '']]","extracted_entities":"[{'text': 'Dataset distillation', 'label': 'Knowledge distillation'}, {'text': 'distillation', 'label': 'Knowledge distillation'}, {'text': 'Expert Decoupling', 'label': 'DistilBERT'}, {'text': 'long-tailed\\ndataset distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"distillation","similarity_score":0.7657151222}
{"id":2409.20237,"submitter":"Muhammad Saif Ullah Khan","authors":"Shalini Sarode, Muhammad Saif Ullah Khan, Tahira Shehzadi, Didier\n  Stricker, Muhammad Zeshan Afzal","title":"Classroom-Inspired Multi-Mentor Distillation with Adaptive Learning\n  Strategies","comments":"Accepted in IntelliSys 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We propose ClassroomKD, a novel multi-mentor knowledge distillation framework\ninspired by classroom environments to enhance knowledge transfer between the\nstudent and multiple mentors with different knowledge levels. Unlike\ntraditional methods that rely on fixed mentor-student relationships, our\nframework dynamically selects and adapts the teaching strategies of diverse\nmentors based on their effectiveness for each data sample. ClassroomKD\ncomprises two main modules: the Knowledge Filtering (KF) module and the\nMentoring module. The KF Module dynamically ranks mentors based on their\nperformance for each input, activating only high-quality mentors to minimize\nerror accumulation and prevent information loss. The Mentoring Module adjusts\nthe distillation strategy by tuning each mentor's influence according to the\ndynamic performance gap between the student and mentors, effectively modulating\nthe learning pace. Extensive experiments on image classification (CIFAR-100 and\nImageNet) and 2D human pose estimation (COCO Keypoints and MPII Human Pose)\ndemonstrate that ClassroomKD outperforms existing knowledge distillation\nmethods for different network architectures. Our results highlight that a\ndynamic and adaptive approach to mentor selection and guidance leads to more\neffective knowledge transfer, paving the way for enhanced model performance\nthrough distillation.\n","versions":"[{'version': 'v1', 'created': 'Mon, 30 Sep 2024 12:20:07 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 11:23:58 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Sarode', 'Shalini', ''], ['Khan', 'Muhammad Saif Ullah', ''], ['Shehzadi', 'Tahira', ''], ['Stricker', 'Didier', ''], ['Afzal', 'Muhammad Zeshan', '']]","extracted_entities":"[{'text': 'knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'knowledge distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge distillation","similarity_score":1.0000002384}
{"id":2410.17215,"submitter":"Yuxian Gu","authors":"Yuxian Gu, Hao Zhou, Fandong Meng, Jie Zhou, Minlie Huang","title":"MiniPLM: Knowledge Distillation for Pre-Training Language Models","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Knowledge distillation (KD) is widely used to train small, high-performing\nstudent language models (LMs) using large teacher LMs. While effective in\nfine-tuning, KD during pre-training faces efficiency, flexibility, and\neffectiveness issues. Existing methods either incur high computational costs\ndue to online teacher inference, require tokenization matching between teacher\nand student LMs, or risk losing the difficulty and diversity of the\nteacher-generated training data. In this work, we propose MiniPLM, a KD\nframework for pre-training LMs by refining the training data distribution with\nthe teacher LM's knowledge. For efficiency, MiniPLM performs offline teacher\ninference, allowing KD for multiple student LMs without adding training costs.\nFor flexibility, MiniPLM operates solely on the training corpus, enabling KD\nacross model families. For effectiveness, MiniPLM leverages the differences\nbetween large and small LMs to enhance the training data difficulty and\ndiversity, helping student LMs acquire versatile and sophisticated knowledge.\nExtensive experiments demonstrate that MiniPLM boosts the student LMs'\nperformance on 9 common downstream tasks, improves language modeling\ncapabilities, and reduces pre-training computation. The benefit of MiniPLM\nextends to larger training scales, evidenced by the scaling curve\nextrapolation. Further analysis reveals that MiniPLM supports KD across model\nfamilies and enhances the pre-training data utilization. Our code, data, and\nmodels can be found at https:\/\/github.com\/thu-coai\/MiniPLM.\n","versions":"[{'version': 'v1', 'created': 'Tue, 22 Oct 2024 17:40:32 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Oct 2024 14:45:26 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 00:03:30 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Gu', 'Yuxian', ''], ['Zhou', 'Hao', ''], ['Meng', 'Fandong', ''], ['Zhou', 'Jie', ''], ['Huang', 'Minlie', '']]","extracted_entities":"[{'text': 'Knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'KD', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Knowledge distillation","similarity_score":1.0000002384}
{"id":2410.17579,"submitter":"Mridul Gupta","authors":"Mridul Gupta and Samyak Jain and Vansh Ramani and Hariprasad Kodamana\n  and Sayan Ranu","title":"Bonsai: Gradient-free Graph Distillation for Node Classification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Graph distillation has emerged as a promising avenue to enable scalable\ntraining of GNNs by compressing the training dataset while preserving essential\ngraph characteristics. Our study uncovers significant shortcomings in current\ngraph distillation techniques. First, the majority of the algorithms\nparadoxically require training on the full dataset to perform distillation.\nSecond, due to their gradient-emulating approach, these methods require fresh\ndistillation for any change in hyperparameters or GNN architecture, limiting\ntheir flexibility and reusability. Finally, they fail to achieve substantial\nsize reduction due to synthesizing fully-connected, edge-weighted graphs. To\naddress these challenges, we present Bonsai, a novel graph distillation method\nempowered by the observation that \\textit{computation trees} form the\nfundamental processing units of message-passing GNNs. Bonsai distills datasets\nby encoding a careful selection of \\textit{exemplar} trees that maximize the\nrepresentation of all computation trees in the training set. This unique\napproach imparts Bonsai as the first linear-time, model-agnostic graph\ndistillation algorithm for node classification that outperforms existing\nbaselines across $6$ real-world datasets on accuracy, while being $22$ times\nfaster on average. Bonsai is grounded in rigorous mathematical guarantees on\nthe adopted approximation strategies making it robust to GNN architectures,\ndatasets, and parameters.\n","versions":"[{'version': 'v1', 'created': 'Wed, 23 Oct 2024 06:08:45 GMT'}, {'version': 'v2', 'created': 'Thu, 24 Oct 2024 05:24:53 GMT'}, {'version': 'v3', 'created': 'Wed, 5 Mar 2025 17:09:46 GMT'}, {'version': 'v4', 'created': 'Wed, 19 Mar 2025 06:20:44 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Gupta', 'Mridul', ''], ['Jain', 'Samyak', ''], ['Ramani', 'Vansh', ''], ['Kodamana', 'Hariprasad', ''], ['Ranu', 'Sayan', '']]","extracted_entities":"[{'text': 'Graph distillation', 'label': 'Knowledge distillation'}, {'text': 'graph distillation', 'label': 'Knowledge distillation'}, {'text': 'graph distillation', 'label': 'Knowledge distillation'}, {'text': 'graph\\ndistillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Graph distillation","similarity_score":0.6216475368}
{"id":2411.10077,"submitter":"Jiwoong Yang","authors":"Jiwoong Yang and Haejun Chung and Ikbeom Jang","title":"Hierarchical Mutual Distillation for Multi-View Fusion: Learning from\n  All Possible View Combinations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Multi-view learning often faces challenges in effectively leveraging images\ncaptured from different angles and locations. This challenge is particularly\npronounced when addressing inconsistencies and uncertainties between views. In\nthis paper, we propose a novel Multi-View Uncertainty-Weighted Mutual\nDistillation (MV-UWMD) method. Our method enhances prediction consistency by\nperforming hierarchical mutual distillation across all possible view\ncombinations, including single-view, partial multi-view, and full multi-view\npredictions. This introduces an uncertainty-based weighting mechanism through\nmutual distillation, allowing effective exploitation of unique information from\neach view while mitigating the impact of uncertain predictions. We extend a\nCNN-Transformer hybrid architecture to facilitate robust feature learning and\nintegration across multiple view combinations. We conducted extensive\nexperiments using a large, unstructured dataset captured from diverse,\nnon-fixed viewpoints. The results demonstrate that MV-UWMD improves prediction\naccuracy and consistency compared to existing multi-view learning approaches.\n","versions":"[{'version': 'v1', 'created': 'Fri, 15 Nov 2024 09:45:32 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 10:17:16 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Yang', 'Jiwoong', ''], ['Chung', 'Haejun', ''], ['Jang', 'Ikbeom', '']]","extracted_entities":"[{'text': 'Multi-view learning', 'label': 'Few-shot Learning'}, {'text': 'hierarchical mutual distillation', 'label': 'Knowledge distillation'}, {'text': 'mutual distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"mutual distillation","similarity_score":0.6689100266}
{"id":2411.16064,"submitter":"Peihua Deng","authors":"Peihua Deng, Jiehua Zhang, Xichun Sheng, Chenggang Yan, Yaoqi Sun,\n  Ying Fu, Liang Li","title":"Multi-Granularity Class Prototype Topology Distillation for\n  Class-Incremental Source-Free Unsupervised Domain Adaptation","comments":"Accepted by CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper explores the Class-Incremental Source-Free Unsupervised Domain\nAdaptation (CI-SFUDA) problem, where the unlabeled target data come\nincrementally without access to labeled source instances. This problem poses\ntwo challenges, the interference of similar source-class knowledge in\ntarget-class representation learning and the shocks of new target knowledge to\nold ones. To address them, we propose the Multi-Granularity Class Prototype\nTopology Distillation (GROTO) algorithm, which effectively transfers the source\nknowledge to the class-incremental target domain. Concretely, we design the\nmulti-granularity class prototype self-organization module and the prototype\ntopology distillation module. First, we mine the positive classes by modeling\naccumulation distributions. Next, we introduce multi-granularity class\nprototypes to generate reliable pseudo-labels, and exploit them to promote the\npositive-class target feature self-organization. Second, the positive-class\nprototypes are leveraged to construct the topological structures of source and\ntarget feature spaces. Then, we perform the topology distillation to\ncontinually mitigate the shocks of new target knowledge to old ones. Extensive\nexperiments demonstrate that our proposed method achieves state-of-the-art\nperformance on three public datasets.\n","versions":"[{'version': 'v1', 'created': 'Mon, 25 Nov 2024 03:28:09 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 12:35:16 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 08:34:36 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Deng', 'Peihua', ''], ['Zhang', 'Jiehua', ''], ['Sheng', 'Xichun', ''], ['Yan', 'Chenggang', ''], ['Sun', 'Yaoqi', ''], ['Fu', 'Ying', ''], ['Li', 'Liang', '']]","extracted_entities":"[{'text': 'target-class representation learning', 'label': 'Few-shot Learning'}, {'text': 'Multi-Granularity Class Prototype\\nTopology Distillation', 'label': 'Knowledge distillation'}, {'text': 'multi-granularity class\\nprototypes', 'label': 'LLMs'}, {'text': 'topology distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"topology distillation","similarity_score":0.6254238486}
{"id":2411.17002,"submitter":"Shambhavi Mishra","authors":"Shambhavi Mishra, Julio Silva-Rodr{\\i}guez, Ismail Ben Ayed, Marco\n  Pedersoli, Jose Dolz","title":"Words Matter: Leveraging Individual Text Embeddings for Code Generation\n  in CLIP Test-Time Adaptation","comments":"Added additional figures to communicate the algorithm","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Vision-language foundation models, such as CLIP, have shown unprecedented\nzero-shot performance across a wide range of tasks. Nevertheless, these models\nmay be unreliable under distributional shifts, as their performance is\nsignificantly degraded. In this work, we explore how to efficiently leverage\nclass text information to mitigate these distribution drifts encountered by\nlarge pre-trained vision-language models (VLMs) during test-time inference. In\nparticular, we propose to generate pseudo-labels for the test-time samples by\nexploiting generic class text embeddings as fixed centroids of a label\nassignment problem, which is efficiently solved with Optimal Transport.\nFurthermore, the proposed adaptation method (CLIP-OT) integrates a multiple\ntemplate knowledge distillation approach, which replicates multi-view\ncontrastive learning strategies in unsupervised representation learning but\nwithout incurring additional computational complexity. Extensive experiments on\nmultiple popular test-time adaptation benchmarks presenting diverse complexity\nempirically show the superiority of CLIP-OT, achieving performance gains of up\nto 7% over recent state-of-the-art methods, yet being computationally and\nmemory efficient.\n","versions":"[{'version': 'v1', 'created': 'Tue, 26 Nov 2024 00:15:37 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 11:02:05 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Mishra', 'Shambhavi', ''], ['Silva-Rodr\u0131guez', 'Julio', ''], ['Ayed', 'Ismail Ben', ''], ['Pedersoli', 'Marco', ''], ['Dolz', 'Jose', '']]","extracted_entities":"[{'text': 'Vision-language foundation models', 'label': 'Foundation Model'}, {'text': 'CLIP', 'label': 'Foundation Model'}, {'text': 'generic class text embeddings', 'label': 'Embedding'}, {'text': 'multiple\\ntemplate knowledge distillation approach', 'label': 'Knowledge distillation'}, {'text': 'multi-view\\ncontrastive learning strategies', 'label': 'Few-shot Learning'}, {'text': 'unsupervised representation learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Knowledge distillation","matched_keyword":"multiple\ntemplate knowledge distillation approach","similarity_score":0.7329477072}
{"id":2412.08949,"submitter":"Xinyue Liu","authors":"Xinyue Liu, Jianyuan Wang, Biao Leng, Shuo Zhang","title":"Multimodal Industrial Anomaly Detection by Crossmodal Reverse\n  Distillation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Knowledge distillation (KD) has been widely studied in unsupervised\nIndustrial Image Anomaly Detection (AD), but its application to unsupervised\nmultimodal AD remains underexplored. Existing KD-based methods for multimodal\nAD that use fused multimodal features to obtain teacher representations face\nchallenges. Anomalies in one modality may not be effectively captured in the\nfused teacher features, leading to detection failures. Besides, these methods\ndo not fully leverage the rich intra- and inter-modality information. In this\npaper, we propose Crossmodal Reverse Distillation (CRD) based on Multi-branch\ndesign to realize Multimodal Industrial AD. By assigning independent branches\nto each modality, our method enables finer detection of anomalies within each\nmodality. Furthermore, we enhance the interaction between modalities during the\ndistillation process by designing Crossmodal Filter and Amplifier. With the\nidea of crossmodal mapping, the student network is allowed to better learn\nnormal features while anomalies in all modalities are ensured to be effectively\ndetected. Experimental verifications on the MVTec 3D-AD dataset demonstrate\nthat our method achieves state-of-the-art performance in multimodal anomaly\ndetection and localization.\n","versions":"[{'version': 'v1', 'created': 'Thu, 12 Dec 2024 05:26:50 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 02:17:32 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Liu', 'Xinyue', ''], ['Wang', 'Jianyuan', ''], ['Leng', 'Biao', ''], ['Zhang', 'Shuo', '']]","extracted_entities":"[{'text': 'Knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'Crossmodal Reverse Distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Knowledge distillation","similarity_score":1.0000002384}
{"id":2412.11365,"submitter":"Wonyons Seo","authors":"Wonyong Seo, Jihyong Oh, Munchurl Kim","title":"BiM-VFI: Bidirectional Motion Field-Guided Frame Interpolation for Video\n  with Non-uniform Motions","comments":"The last two authors are co-corresponding authors","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Existing Video Frame interpolation (VFI) models tend to suffer from\ntime-to-location ambiguity when trained with video of non-uniform motions, such\nas accelerating, decelerating, and changing directions, which often yield\nblurred interpolated frames. In this paper, we propose (i) a novel motion\ndescription map, Bidirectional Motion field (BiM), to effectively describe\nnon-uniform motions; (ii) a BiM-guided Flow Net (BiMFN) with Content-Aware\nUpsampling Network (CAUN) for precise optical flow estimation; and (iii)\nKnowledge Distillation for VFI-centric Flow supervision (KDVCF) to supervise\nthe motion estimation of VFI model with VFI-centric teacher flows. The proposed\nVFI is called a Bidirectional Motion field-guided VFI (BiM-VFI) model.\nExtensive experiments show that our BiM-VFI model significantly surpasses the\nrecent state-of-the-art VFI methods by 26% and 45% improvements in LPIPS and\nSTLPIPS respectively, yielding interpolated frames with much fewer blurs at\narbitrary time instances.\n","versions":"[{'version': 'v1', 'created': 'Mon, 16 Dec 2024 01:37:51 GMT'}, {'version': 'v2', 'created': 'Sun, 29 Dec 2024 08:11:31 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 09:04:14 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Seo', 'Wonyong', ''], ['Oh', 'Jihyong', ''], ['Kim', 'Munchurl', '']]","extracted_entities":"[{'text': 'Knowledge Distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Knowledge Distillation","similarity_score":1.0000002384}
{"id":2501.01709,"submitter":"Jiajun Cao","authors":"Jiajun Cao, Yuan Zhang, Tao Huang, Ming Lu, Qizhe Zhang, Ruichuan An,\n  Ningning MA, Shanghang Zhang","title":"MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders","comments":"Accepted by CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Visual encoders are fundamental components in vision-language models (VLMs),\neach showcasing unique strengths derived from various pre-trained visual\nfoundation models. To leverage the various capabilities of these encoders,\nrecent studies incorporate multiple encoders within a single VLM, leading to a\nconsiderable increase in computational cost. In this paper, we present\nMixture-of-Visual-Encoder Knowledge Distillation (MoVE-KD), a novel framework\nthat distills the unique proficiencies of multiple vision encoders into a\nsingle, efficient encoder model. Specifically, to mitigate conflicts and retain\nthe unique characteristics of each teacher encoder, we employ low-rank\nadaptation (LoRA) and mixture-of-experts (MoEs) to selectively activate\nspecialized knowledge based on input features, enhancing both adaptability and\nefficiency. To regularize the KD process and enhance performance, we propose an\nattention-based distillation strategy that adaptively weighs the different\nencoders and emphasizes valuable visual tokens, reducing the burden of\nreplicating comprehensive but distinct features from multiple teachers.\nComprehensive experiments on popular VLMs, such as LLaVA and LLaVA-NeXT,\nvalidate the effectiveness of our method. Our code is available at:\nhttps:\/\/github.com\/hey-cjj\/MoVE-KD.\n","versions":"[{'version': 'v1', 'created': 'Fri, 3 Jan 2025 09:10:34 GMT'}, {'version': 'v2', 'created': 'Fri, 14 Mar 2025 05:52:36 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 07:34:44 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Cao', 'Jiajun', ''], ['Zhang', 'Yuan', ''], ['Huang', 'Tao', ''], ['Lu', 'Ming', ''], ['Zhang', 'Qizhe', ''], ['An', 'Ruichuan', ''], ['MA', 'Ningning', ''], ['Zhang', 'Shanghang', '']]","extracted_entities":"[{'text': 'Mixture-of-Visual-Encoder Knowledge Distillation', 'label': 'Knowledge distillation'}, {'text': 'attention-based distillation strategy', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Mixture-of-Visual-Encoder Knowledge Distillation","similarity_score":0.754773736}
{"id":2503.02321,"submitter":"Haishan Huang","authors":"Pengchen Liang, Leijun Shi, Huiping Yao, Bin Pu, Jianguo Chen, Lei\n  Zhao, Haishan Huang, Zhuangzhuang Chen, Zhaozhao Xu, Lite Xu, Qing Chang,\n  Yiwei Li","title":"Semantic Prior Distillation with Vision Foundation Model for Enhanced\n  Rapid Bone Scintigraphy Image Restoration","comments":"12 pages, 9 figures, 8 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Rapid bone scintigraphy is an essential tool for diagnosing skeletal diseases\nand tumor metastasis in pediatric patients, as it reduces scan time and\nminimizes patient discomfort. However, rapid scans often result in poor image\nquality, potentially affecting diagnosis due to reduced resolution and detail,\nwhich make it challenging to identify and evaluate finer anatomical structures.\nTo address this issue, we propose the first application of SAM-based semantic\npriors for medical image restoration, leveraging the Segment Anything Model\n(SAM) to enhance rapid bone scintigraphy images in pediatric populations. Our\nmethod comprises two cascaded networks, $f^{IR1}$ and $f^{IR2}$, augmented by\nthree key modules: a Semantic Prior Integration (SPI) module, a Semantic\nKnowledge Distillation (SKD) module, and a Semantic Consistency Module (SCM).\nThe SPI and SKD modules incorporate domain-specific semantic information from a\nfine-tuned SAM, while the SCM maintains consistent semantic feature\nrepresentation throughout the cascaded networks. In addition, we will release a\nnovel Rapid Bone Scintigraphy dataset called RBS, the first dataset dedicated\nto rapid bone scintigraphy image restoration in pediatric patients. RBS\nconsists of 137 pediatric patients aged between 0.5 and 16 years who underwent\nboth standard and rapid bone scans. The dataset includes scans performed at 20\ncm\/min (standard) and 40 cm\/min (rapid), representing a $2\\times$ acceleration.\nWe conducted extensive experiments on both the publicly available endoscopic\ndataset and RBS. The results demonstrate that our method outperforms all\nexisting methods across various metrics, including PSNR, SSIM, FID, and LPIPS.\n","versions":"[{'version': 'v1', 'created': 'Tue, 4 Mar 2025 06:23:22 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 05:23:43 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Liang', 'Pengchen', ''], ['Shi', 'Leijun', ''], ['Yao', 'Huiping', ''], ['Pu', 'Bin', ''], ['Chen', 'Jianguo', ''], ['Zhao', 'Lei', ''], ['Huang', 'Haishan', ''], ['Chen', 'Zhuangzhuang', ''], ['Xu', 'Zhaozhao', ''], ['Xu', 'Lite', ''], ['Chang', 'Qing', ''], ['Li', 'Yiwei', '']]","extracted_entities":"[{'text': 'Semantic\\nKnowledge Distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Semantic\nKnowledge Distillation","similarity_score":0.8579530716}
{"id":2503.04843,"submitter":"Herv\\'e Turlier","authors":"Alessandro Pasqui, Sajjad Mahdavi, Benoit Vianay, Alexandra Colin,\n  Alex McDougall, R\\'emi Dumollard, Yekaterina A. Miroshnikova, Elsa Labrune\n  and Herv\\'e Turlier","title":"Self-Supervised Z-Slice Augmentation for 3D Bio-Imaging via Knowledge\n  Distillation","comments":"25 pages, 5 figures, 1 table","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI eess.IV q-bio.QM","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Three-dimensional biological microscopy has significantly advanced our\nunderstanding of complex biological structures. However, limitations due to\nmicroscopy techniques, sample properties or phototoxicity often result in poor\nz-resolution, hindering accurate cellular measurements. Here, we introduce\nZAugNet, a fast, accurate, and self-supervised deep learning method for\nenhancing z-resolution in biological images. By performing nonlinear\ninterpolation between consecutive slices, ZAugNet effectively doubles\nresolution with each iteration. Compared on several microscopy modalities and\nbiological objects, it outperforms competing methods on most metrics. Our\nmethod leverages a generative adversarial network (GAN) architecture combined\nwith knowledge distillation to maximize prediction speed without compromising\naccuracy. We also developed ZAugNet+, an extended version enabling continuous\ninterpolation at arbitrary distances, making it particularly useful for\ndatasets with nonuniform slice spacing. Both ZAugNet and ZAugNet+ provide\nhigh-performance, scalable z-slice augmentation solutions for large-scale 3D\nimaging. They are available as open-source frameworks in PyTorch, with an\nintuitive Colab notebook interface for easy access by the scientific community.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 17:50:35 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 21:52:46 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Pasqui', 'Alessandro', ''], ['Mahdavi', 'Sajjad', ''], ['Vianay', 'Benoit', ''], ['Colin', 'Alexandra', ''], ['McDougall', 'Alex', ''], ['Dumollard', 'R\u00e9mi', ''], ['Miroshnikova', 'Yekaterina A.', ''], ['Labrune', 'Elsa', ''], ['Turlier', 'Herv\u00e9', '']]","extracted_entities":"[{'text': 'knowledge distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge distillation","similarity_score":1.0000002384}
{"id":2503.1066,"submitter":"Khoi Do","authors":"Khoi Do, Binh-Son Hua","title":"Text-to-3D Generation using Jensen-Shannon Score Distillation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Score distillation sampling is an effective technique to generate 3D models\nfrom text prompts, utilizing pre-trained large-scale text-to-image diffusion\nmodels as guidance. However, the produced 3D assets tend to be over-saturating,\nover-smoothing, with limited diversity. These issues are results from a reverse\nKullback-Leibler (KL) divergence objective, which makes the optimization\nunstable and results in mode-seeking behavior. In this paper, we derive a\nbounded score distillation objective based on Jensen-Shannon divergence (JSD),\nwhich stabilizes the optimization process and produces high-quality 3D\ngeneration. JSD can match well generated and target distribution, therefore\nmitigating mode seeking. We provide a practical implementation of JSD by\nutilizing the theory of generative adversarial networks to define an\napproximate objective function for the generator, assuming the discriminator is\nwell trained. By assuming the discriminator following a log-odds classifier, we\npropose a minority sampling algorithm to estimate the gradients of our proposed\nobjective, providing a practical implementation for JSD. We conduct both\ntheoretical and empirical studies to validate our method. Experimental results\non T3Bench demonstrate that our method can produce high-quality and diversified\n3D assets.\n","versions":"[{'version': 'v1', 'created': 'Sat, 8 Mar 2025 13:27:18 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 17:15:23 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Do', 'Khoi', ''], ['Hua', 'Binh-Son', '']]","extracted_entities":"[{'text': 'Score distillation', 'label': 'Knowledge distillation'}, {'text': 'text prompts', 'label': 'Prompting'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Score distillation","similarity_score":0.6502460837}
{"id":2503.11439,"submitter":"Seo Jin Lee","authors":"Sanghyun Jo, Seo Jin Lee, Seungwoo Lee, Seohyung Hong, Hyungseok Seo,\n  Kyungsu Kim","title":"COIN: Confidence Score-Guided Distillation for Annotation-Free Cell\n  Segmentation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Cell instance segmentation (CIS) is crucial for identifying individual cell\nmorphologies in histopathological images, providing valuable insights for\nbiological and medical research. While unsupervised CIS (UCIS) models aim to\nreduce the heavy reliance on labor-intensive image annotations, they fail to\naccurately capture cell boundaries, causing missed detections and poor\nperformance. Recognizing the absence of error-free instances as a key\nlimitation, we present COIN (COnfidence score-guided INstance distillation), a\nnovel annotation-free framework with three key steps: (1) Increasing the\nsensitivity for the presence of error-free instances via unsupervised semantic\nsegmentation with optimal transport, leveraging its ability to discriminate\nspatially minor instances, (2) Instance-level confidence scoring to measure the\nconsistency between model prediction and refined mask and identify highly\nconfident instances, offering an alternative to ground truth annotations, and\n(3) Progressive expansion of confidence with recursive self-distillation.\nExtensive experiments across six datasets show COIN outperforming existing UCIS\nmethods, even surpassing semi- and weakly-supervised approaches across all\nmetrics on the MoNuSeg and TNBC datasets. The code is available at\nhttps:\/\/github.com\/shjo-april\/COIN.\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 14:27:24 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 01:59:06 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Jo', 'Sanghyun', ''], ['Lee', 'Seo Jin', ''], ['Lee', 'Seungwoo', ''], ['Hong', 'Seohyung', ''], ['Seo', 'Hyungseok', ''], ['Kim', 'Kyungsu', '']]","extracted_entities":"[{'text': 'COnfidence score-guided INstance distillation', 'label': 'Knowledge distillation'}, {'text': 'recursive self-distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"recursive self-distillation","similarity_score":0.6176365614}
{"id":2503.12914,"submitter":"Zhuoqun Su","authors":"Zhuoqun Su, Huimin Lu, Shuaifeng Jiao, Junhao Xiao, Yaonan Wang,\n  Xieyuanli Chen","title":"Efficient Multimodal 3D Object Detector via Instance-Level Contrastive\n  Distillation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multimodal 3D object detectors leverage the strengths of both geometry-aware\nLiDAR point clouds and semantically rich RGB images to enhance detection\nperformance. However, the inherent heterogeneity between these modalities,\nincluding unbalanced convergence and modal misalignment, poses significant\nchallenges. Meanwhile, the large size of the detection-oriented feature also\nconstrains existing fusion strategies to capture long-range dependencies for\nthe 3D detection tasks. In this work, we introduce a fast yet effective\nmultimodal 3D object detector, incorporating our proposed Instance-level\nContrastive Distillation (ICD) framework and Cross Linear Attention Fusion\nModule (CLFM). ICD aligns instance-level image features with LiDAR\nrepresentations through object-aware contrastive distillation, ensuring\nfine-grained cross-modal consistency. Meanwhile, CLFM presents an efficient and\nscalable fusion strategy that enhances cross-modal global interactions within\nsizable multimodal BEV features. Extensive experiments on the KITTI and\nnuScenes 3D object detection benchmarks demonstrate the effectiveness of our\nmethods. Notably, our 3D object detector outperforms state-of-the-art (SOTA)\nmethods while achieving superior efficiency. The implementation of our method\nhas been released as open-source at: https:\/\/github.com\/nubot-nudt\/ICD-Fusion.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 08:26:11 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Su', 'Zhuoqun', ''], ['Lu', 'Huimin', ''], ['Jiao', 'Shuaifeng', ''], ['Xiao', 'Junhao', ''], ['Wang', 'Yaonan', ''], ['Chen', 'Xieyuanli', '']]","extracted_entities":"[{'text': 'object-aware contrastive distillation', 'label': 'Knowledge distillation'}, {'text': 'CLFM', 'label': 'LLM'}]","assigned_concept":"Knowledge distillation","matched_keyword":"object-aware contrastive distillation","similarity_score":0.6323325038}
{"id":2503.13008,"submitter":"Torbj\\\"orn Nordling","authors":"David E. Hernandez, Jose Ramon Chang, Torbj\\\"orn E. M. Nordling","title":"Knowledge Distillation: Enhancing Neural Network Compression with\n  Integrated Gradients","comments":"15 pages, 3 figures, conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Efficient deployment of deep neural networks on resource-constrained devices\ndemands advanced compression techniques that preserve accuracy and\ninteroperability. This paper proposes a machine learning framework that\naugments Knowledge Distillation (KD) with Integrated Gradients (IG), an\nattribution method, to optimise the compression of convolutional neural\nnetworks. We introduce a novel data augmentation strategy where IG maps,\nprecomputed from a teacher model, are overlaid onto training images to guide a\ncompact student model toward critical feature representations. This approach\nleverages the teacher's decision-making insights, enhancing the student's\nability to replicate complex patterns with reduced parameters. Experiments on\nCIFAR-10 demonstrate the efficacy of our method: a student model, compressed\n4.1-fold from the MobileNet-V2 teacher, achieves 92.5% classification accuracy,\nsurpassing the baseline student's 91.4% and traditional KD approaches, while\nreducing inference latency from 140 ms to 13 ms--a tenfold speedup. We perform\nhyperparameter optimisation for efficient learning. Comprehensive ablation\nstudies dissect the contributions of KD and IG, revealing synergistic effects\nthat boost both performance and model explainability. Our method's emphasis on\nfeature-level guidance via IG distinguishes it from conventional KD, offering a\ndata-driven solution for mining transferable knowledge in neural architectures.\nThis work contributes to machine learning by providing a scalable,\ninterpretable compression technique, ideal for edge computing applications\nwhere efficiency and transparency are paramount.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 10:07:50 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Hernandez', 'David E.', ''], ['Chang', 'Jose Ramon', ''], ['Nordling', 'Torbj\u00f6rn E. M.', '']]","extracted_entities":"[{'text': 'Knowledge Distillation', 'label': 'Knowledge distillation'}, {'text': 'hyperparameter optimisation', 'label': 'Fine-tuning'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Knowledge Distillation","similarity_score":1.0000002384}
{"id":2503.1306,"submitter":"Sparsh Mittal","authors":"Harshal Kausadikar and Tanvi Kale and Onkar Susladkar and Sparsh\n  Mittal","title":"Historic Scripts to Modern Vision: A Novel Dataset and A VLM Framework\n  for Transliteration of Modi Script to Devanagari","comments":"Under submission at a conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  In medieval India, the Marathi language was written using the Modi script.\nThe texts written in Modi script include extensive knowledge about medieval\nsciences, medicines, land records and authentic evidence about Indian history.\nAround 40 million documents are in poor condition and have not yet been\ntransliterated. Furthermore, only a few experts in this domain can\ntransliterate this script into English or Devanagari. Most of the past research\npredominantly focuses on individual character recognition. A system that can\ntransliterate Modi script documents to Devanagari script is needed. We propose\nthe MoDeTrans dataset, comprising 2,043 images of Modi script documents\naccompanied by their corresponding textual transliterations in Devanagari. We\nfurther introduce MoScNet (\\textbf{Mo}di \\textbf{Sc}ript \\textbf{Net}work), a\nnovel Vision-Language Model (VLM) framework for transliterating Modi script\nimages into Devanagari text. MoScNet leverages Knowledge Distillation, where a\nstudent model learns from a teacher model to enhance transliteration\nperformance. The final student model of MoScNet has better performance than the\nteacher model while having 163$\\times$ lower parameters. Our work is the first\nto perform direct transliteration from the handwritten Modi script to the\nDevanagari script. MoScNet also shows competitive results on the optical\ncharacter recognition (OCR) task.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 11:07:29 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Kausadikar', 'Harshal', ''], ['Kale', 'Tanvi', ''], ['Susladkar', 'Onkar', ''], ['Mittal', 'Sparsh', '']]","extracted_entities":"[{'text': 'Knowledge Distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Knowledge Distillation","similarity_score":1.0000002384}
{"id":2503.13077,"submitter":"Amir Masoud Baghi","authors":"Amir Baghi, Jens Sj\\\"olund, Joakim Bergdahl, Linus Gissl\\'en and\n  Alessandro Sestini","title":"Towards Better Sample Efficiency in Multi-Agent Reinforcement Learning\n  via Exploration","comments":"8 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.MA","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multi-agent reinforcement learning has shown promise in learning cooperative\nbehaviors in team-based environments. However, such methods often demand\nextensive training time. For instance, the state-of-the-art method TiZero takes\n40 days to train high-quality policies for a football environment. In this\npaper, we hypothesize that better exploration mechanisms can improve the sample\nefficiency of multi-agent methods. We propose two different approaches for\nbetter exploration in TiZero: a self-supervised intrinsic reward and a random\nnetwork distillation bonus. Additionally, we introduce architectural\nmodifications to the original algorithm to enhance TiZero's computational\nefficiency. We evaluate the sample efficiency of these approaches through\nextensive experiments. Our results show that random network distillation\nimproves training sample efficiency by 18.8% compared to the original TiZero.\nFurthermore, we evaluate the qualitative behavior of the models produced by\nboth variants against a heuristic AI, with the self-supervised reward\nencouraging possession and random network distillation leading to a more\noffensive performance. Our results highlights the applicability of our random\nnetwork distillation variant in practical settings. Lastly, due to the nature\nof the proposed method, we acknowledge its use beyond football simulation,\nespecially in environments with strong multi-agent and strategic aspects.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 11:32:28 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Baghi', 'Amir', ''], ['Sj\u00f6lund', 'Jens', ''], ['Bergdahl', 'Joakim', ''], ['Gissl\u00e9n', 'Linus', ''], ['Sestini', 'Alessandro', '']]","extracted_entities":"[{'text': 'Multi-agent reinforcement learning', 'label': 'Few-shot Learning'}, {'text': 'random\\nnetwork distillation', 'label': 'Knowledge distillation'}, {'text': 'random network distillation', 'label': 'Knowledge distillation'}, {'text': 'random network distillation', 'label': 'Knowledge distillation'}, {'text': 'random\\nnetwork distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"random\nnetwork distillation","similarity_score":0.6021347046}
{"id":2503.13156,"submitter":"Youssef Mourchid","authors":"Zakariae Zrimek, Youssef Mourchid, Mohammed El Hassouni","title":"DynSTG-Mamba: Dynamic Spatio-Temporal Graph Mamba with Cross-Graph\n  Knowledge Distillation for Gait Disorders Recognition","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Gait disorder recognition plays a crucial role in the early diagnosis and\nmonitoring of movement disorders. Existing approaches, including\nspatio-temporal graph convolutional networks (ST-GCNs), often face high memory\ndemands and struggle to capture complex spatio-temporal dependencies, limiting\ntheir efficiency in clinical applications. To address these challenges, we\nintroduce DynSTG-Mamba (Dynamic Spatio-Temporal Graph Mamba), a novel framework\nthat combines DF-STGNN and STG-Mamba to enhance motion sequence modeling. The\nDF-STGNN incorporates a dynamic spatio-temporal filter that adaptively adjusts\nspatial connections between skeletal joints and temporal interactions across\ndifferent movement phases. This approach ensures better feature propagation\nthrough dynamic graph structures by considering the hierarchical nature and\ndynamics of skeletal gait data. Meanwhile, STG-Mamba, an extension of Mamba\nadapted for skeletal motion data, ensures a continuous propagation of states,\nfacilitating the capture of long-term dependencies while reducing computational\ncomplexity. To reduce the number of model parameters and computational costs\nwhile maintaining consistency, we propose Cross-Graph Relational Knowledge\nDistillation, a novel knowledge transfer mechanism that aligns relational\ninformation between teacher (large architecture) and student models (small\narchitecture) while using shared memory. This ensures that the interactions and\nmovement patterns of the joints are accurately preserved in the motion\nsequences. We validate our DynSTG-Mamba on KOA-NM, PD-WALK, and ATAXIA\ndatasets, where it outperforms state-of-the-art approaches by achieving in\nterms of Accuracy, F1-score, and Recall. Our results highlight the efficiency\nand robustness of our approach, offering a lightweight yet highly accurate\nsolution for automated gait analysis and movement disorder assessment.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 13:26:47 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Zrimek', 'Zakariae', ''], ['Mourchid', 'Youssef', ''], ['Hassouni', 'Mohammed El', '']]","extracted_entities":"[{'text': 'Cross-Graph Relational Knowledge\\nDistillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Cross-Graph Relational Knowledge\nDistillation","similarity_score":0.72131598}
{"id":2503.13319,"submitter":"Shitong Shao","authors":"Shitong Shao, Hongwei Yi, Hanzhong Guo, Tian Ye, Daquan Zhou, Michael\n  Lingelbach, Zhiqiang Xu, Zeke Xie","title":"MagicDistillation: Weak-to-Strong Video Distillation for Large-Scale\n  Portrait Few-Step Synthesis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Fine-tuning open-source large-scale VDMs for the portrait video synthesis\ntask can result in significant improvements across multiple dimensions, such as\nvisual quality and natural facial motion dynamics. Despite their advancements,\nhow to achieve step distillation and reduce the substantial computational\noverhead of large-scale VDMs remains unexplored. To fill this gap, this paper\nproposes Weak-to-Strong Video Distillation (W2SVD) to mitigate both the issue\nof insufficient training memory and the problem of training collapse observed\nin vanilla DMD during the training process. Specifically, we first leverage\nLoRA to fine-tune the fake diffusion transformer (DiT) to address the\nout-of-memory issue. Then, we employ the W2S distribution matching to adjust\nthe real DiT's parameter, subtly shifting it toward the fake DiT's parameter.\nThis adjustment is achieved by utilizing the weak weight of the low-rank\nbranch, effectively alleviate the conundrum where the video synthesized by the\nfew-step generator deviates from the real data distribution, leading to\ninaccuracies in the KL divergence approximation. Additionally, we minimize the\ndistance between the fake data distribution and the ground truth distribution\nto further enhance the visual quality of the synthesized videos. As\nexperimentally demonstrated on HunyuanVideo, W2SVD surpasses the standard\nEuler, LCM, DMD and even the 28-step standard sampling in FID\/FVD and VBench in\n1\/4-step video synthesis. The project page is in\nhttps:\/\/w2svd.github.io\/W2SVD\/.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:58:27 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Shao', 'Shitong', ''], ['Yi', 'Hongwei', ''], ['Guo', 'Hanzhong', ''], ['Ye', 'Tian', ''], ['Zhou', 'Daquan', ''], ['Lingelbach', 'Michael', ''], ['Xu', 'Zhiqiang', ''], ['Xie', 'Zeke', '']]","extracted_entities":"[{'text': 'open-source large-scale VDMs', 'label': 'Open-source LLMs'}, {'text': 'step distillation', 'label': 'Knowledge distillation'}, {'text': 'Weak-to-Strong Video Distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"step distillation","similarity_score":0.684684217}
{"id":2503.13828,"submitter":"Lichao Mou","authors":"Chunlei Li, Yilei Shi, Jingliang Hu, Xiao Xiang Zhu, Lichao Mou","title":"Scale-Aware Contrastive Reverse Distillation for Unsupervised Medical\n  Anomaly Detection","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Unsupervised anomaly detection using deep learning has garnered significant\nresearch attention due to its broad applicability, particularly in medical\nimaging where labeled anomalous data are scarce. While earlier approaches\nleverage generative models like autoencoders and generative adversarial\nnetworks (GANs), they often fall short due to overgeneralization. Recent\nmethods explore various strategies, including memory banks, normalizing flows,\nself-supervised learning, and knowledge distillation, to enhance\ndiscrimination. Among these, knowledge distillation, particularly reverse\ndistillation, has shown promise. Following this paradigm, we propose a novel\nscale-aware contrastive reverse distillation model that addresses two key\nlimitations of existing reverse distillation methods: insufficient feature\ndiscriminability and inability to handle anomaly scale variations.\nSpecifically, we introduce a contrastive student-teacher learning approach to\nderive more discriminative representations by generating and exploring\nout-of-normal distributions. Further, we design a scale adaptation mechanism to\nsoftly weight contrastive distillation losses at different scales to account\nfor the scale variation issue. Extensive experiments on benchmark datasets\ndemonstrate state-of-the-art performance, validating the efficacy of the\nproposed method. Code is available at https:\/\/github.com\/MedAITech\/SCRD4AD.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 02:10:20 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Li', 'Chunlei', ''], ['Shi', 'Yilei', ''], ['Hu', 'Jingliang', ''], ['Zhu', 'Xiao Xiang', ''], ['Mou', 'Lichao', '']]","extracted_entities":"[{'text': 'self-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'reverse\\ndistillation', 'label': 'Knowledge distillation'}, {'text': 'reverse distillation', 'label': 'Knowledge distillation'}, {'text': 'reverse distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge distillation","similarity_score":1.0000002384}
{"id":2503.14097,"submitter":"Haoxin Yang","authors":"Weihong Chen, Xuemiao Xu, Haoxin Yang, Yi Xie, Peng Xiao, Cheng Xu,\n  Huaidong Zhang, Pheng-Ann Heng","title":"SCJD: Sparse Correlation and Joint Distillation for Efficient 3D Human\n  Pose Estimation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Existing 3D Human Pose Estimation (HPE) methods achieve high accuracy but\nsuffer from computational overhead and slow inference, while knowledge\ndistillation methods fail to address spatial relationships between joints and\ntemporal correlations in multi-frame inputs. In this paper, we propose Sparse\nCorrelation and Joint Distillation (SCJD), a novel framework that balances\nefficiency and accuracy for 3D HPE. SCJD introduces Sparse Correlation Input\nSequence Downsampling to reduce redundancy in student network inputs while\npreserving inter-frame correlations. For effective knowledge transfer, we\npropose Dynamic Joint Spatial Attention Distillation, which includes Dynamic\nJoint Embedding Distillation to enhance the student's feature representation\nusing the teacher's multi-frame context feature, and Adjacent Joint Attention\nDistillation to improve the student network's focus on adjacent joint\nrelationships for better spatial understanding. Additionally, Temporal\nConsistency Distillation aligns the temporal correlations between teacher and\nstudent networks through upsampling and global supervision. Extensive\nexperiments demonstrate that SCJD achieves state-of-the-art performance. Code\nis available at https:\/\/github.com\/wileychan\/SCJD.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 10:14:49 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Chen', 'Weihong', ''], ['Xu', 'Xuemiao', ''], ['Yang', 'Haoxin', ''], ['Xie', 'Yi', ''], ['Xiao', 'Peng', ''], ['Xu', 'Cheng', ''], ['Zhang', 'Huaidong', ''], ['Heng', 'Pheng-Ann', '']]","extracted_entities":"[{'text': 'Sparse Correlation Input\\nSequence Downsampling', 'label': 'Knowledge distillation'}, {'text': 'Dynamic Joint Spatial Attention Distillation', 'label': 'Knowledge distillation'}, {'text': 'Dynamic\\nJoint Embedding Distillation', 'label': 'Knowledge distillation'}, {'text': 'Adjacent Joint Attention\\nDistillation', 'label': 'Knowledge distillation'}, {'text': 'Temporal\\nConsistency Distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Temporal\nConsistency Distillation","similarity_score":0.6452623606}
{"id":2503.14293,"submitter":"Sakib Matin","authors":"Sakib Matin, Emily Shinkle, Yulia Pimonova, Galen T. Craven,\n  Aleksandra Pachalieva, Ying Wai Li, Kipton Barros, Nicholas Lubbers","title":"Ensemble Knowledge Distillation for Machine Learning Interatomic\n  Potentials","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.chem-ph cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Machine learning interatomic potentials (MLIPs) are a promising tool to\naccelerate atomistic simulations and molecular property prediction. The quality\nof MLIPs strongly depends on the quantity of available training data as well as\nthe quantum chemistry (QC) level of theory used to generate that data. Datasets\ngenerated with high-fidelity QC methods, such as coupled cluster, are typically\nrestricted to small molecules and may be missing energy gradients. With this\nlimited quantity of data, it is often difficult to train good MLIP models. We\npresent an ensemble knowledge distillation (EKD) method to improve MLIP\naccuracy when trained to energy-only datasets. In our EKD approach, first,\nmultiple teacher models are trained to QC energies and then used to generate\natomic forces for all configurations in the dataset. Next, a student MLIP is\ntrained to both QC energies and to ensemble-averaged forces generated by the\nteacher models. We apply this workflow on the ANI-1ccx dataset which consists\nof organic molecules with configuration energies computed at the coupled\ncluster level of theory. The resulting student MLIPs achieve new\nstate-of-the-art accuracy on the out-of-sample COMP6 benchmark and improved\nstability for molecular dynamics simulations. The EKD approach for MLIP is\nbroadly applicable for chemical, biomolecular and materials science\nsimulations.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 14:32:51 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:03:39 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Matin', 'Sakib', ''], ['Shinkle', 'Emily', ''], ['Pimonova', 'Yulia', ''], ['Craven', 'Galen T.', ''], ['Pachalieva', 'Aleksandra', ''], ['Li', 'Ying Wai', ''], ['Barros', 'Kipton', ''], ['Lubbers', 'Nicholas', '']]","extracted_entities":"[{'text': 'MLIPs', 'label': 'LLMs'}, {'text': 'MLIPs', 'label': 'LLMs'}, {'text': 'ensemble knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'MLIPs', 'label': 'LLMs'}, {'text': 'EKD', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"ensemble knowledge distillation","similarity_score":0.7902268171}
{"id":2503.14405,"submitter":"Mert B\\\"ulent Sar{\\i}y{\\i}ld{\\i}z","authors":"Mert Bulent Sariyildiz, Philippe Weinzaepfel, Thomas Lucas, Pau de\n  Jorge, Diane Larlus, Yannis Kalantidis","title":"DUNE: Distilling a Universal Encoder from Heterogeneous 2D and 3D\n  Teachers","comments":"Accepted to CVPR-2025. Project page:\n  https:\/\/europe.naverlabs.com\/dune","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent multi-teacher distillation methods have unified the encoders of\nmultiple foundation models into a single encoder, achieving competitive\nperformance on core vision tasks like classification, segmentation, and depth\nestimation. This led us to ask: Could similar success be achieved when the pool\nof teachers also includes vision models specialized in diverse tasks across\nboth 2D and 3D perception? In this paper, we define and investigate the problem\nof heterogeneous teacher distillation, or co-distillation, a challenging\nmulti-teacher distillation scenario where teacher models vary significantly in\nboth (a) their design objectives and (b) the data they were trained on. We\nexplore data-sharing strategies and teacher-specific encoding, and introduce\nDUNE, a single encoder excelling in 2D vision, 3D understanding, and 3D human\nperception. Our model achieves performance comparable to that of its larger\nteachers, sometimes even outperforming them, on their respective tasks.\nNotably, DUNE surpasses MASt3R in Map-free Visual Relocalization with a much\nsmaller encoder.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:47:27 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Sariyildiz', 'Mert Bulent', ''], ['Weinzaepfel', 'Philippe', ''], ['Lucas', 'Thomas', ''], ['de Jorge', 'Pau', ''], ['Larlus', 'Diane', ''], ['Kalantidis', 'Yannis', '']]","extracted_entities":"[{'text': 'multi-teacher distillation', 'label': 'Knowledge distillation'}, {'text': 'heterogeneous teacher distillation', 'label': 'Knowledge distillation'}, {'text': 'co-distillation', 'label': 'Knowledge distillation'}, {'text': 'multi-teacher distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"multi-teacher distillation","similarity_score":0.7170355916}
{"id":2503.14489,"submitter":"Jinghao Zhou","authors":"Jensen (Jinghao) Zhou, Hang Gao, Vikram Voleti, Aaryaman Vasishta,\n  Chun-Han Yao, Mark Boss, Philip Torr, Christian Rupprecht, Varun Jampani","title":"Stable Virtual Camera: Generative View Synthesis with Diffusion Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We present Stable Virtual Camera (Seva), a generalist diffusion model that\ncreates novel views of a scene, given any number of input views and target\ncameras. Existing works struggle to generate either large viewpoint changes or\ntemporally smooth samples, while relying on specific task configurations. Our\napproach overcomes these limitations through simple model design, optimized\ntraining recipe, and flexible sampling strategy that generalize across view\nsynthesis tasks at test time. As a result, our samples maintain high\nconsistency without requiring additional 3D representation-based distillation,\nthus streamlining view synthesis in the wild. Furthermore, we show that our\nmethod can generate high-quality videos lasting up to half a minute with\nseamless loop closure. Extensive benchmarking demonstrates that Seva\noutperforms existing methods across different datasets and settings.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:57:22 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Jensen', '', '', 'Jinghao'], ['Zhou', '', ''], ['Gao', 'Hang', ''], ['Voleti', 'Vikram', ''], ['Vasishta', 'Aaryaman', ''], ['Yao', 'Chun-Han', ''], ['Boss', 'Mark', ''], ['Torr', 'Philip', ''], ['Rupprecht', 'Christian', ''], ['Jampani', 'Varun', '']]","extracted_entities":"[{'text': '3D representation-based distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"3D representation-based distillation","similarity_score":0.55683285}
{"id":2503.1472,"submitter":"Vihaan Misra","authors":"Vihaan Misra, Peter Schaldenbrand, Jean Oh","title":"ShapeShift: Towards Text-to-Shape Arrangement Synthesis with\n  Content-Aware Geometric Constraints","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While diffusion-based models excel at generating photorealistic images from\ntext, a more nuanced challenge emerges when constrained to using only a fixed\nset of rigid shapes, akin to solving tangram puzzles or arranging real-world\nobjects to match semantic descriptions. We formalize this problem as\nshape-based image generation, a new text-guided image-to-image translation task\nthat requires rearranging the input set of rigid shapes into non-overlapping\nconfigurations and visually communicating the target concept. Unlike\npixel-manipulation approaches, our method, ShapeShift, explicitly parameterizes\neach shape within a differentiable vector graphics pipeline, iteratively\noptimizing placement and orientation through score distillation sampling from\npretrained diffusion models. To preserve arrangement clarity, we introduce a\ncontent-aware collision resolution mechanism that applies minimal semantically\ncoherent adjustments when overlaps occur, ensuring smooth convergence toward\nphysically valid configurations. By bridging diffusion-based semantic guidance\nwith explicit geometric constraints, our approach yields interpretable\ncompositions where spatial relationships clearly embody the textual prompt.\nExtensive experiments demonstrate compelling results across diverse scenarios,\nwith quantitative and qualitative advantages over alternative techniques.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 20:48:58 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Misra', 'Vihaan', ''], ['Schaldenbrand', 'Peter', ''], ['Oh', 'Jean', '']]","extracted_entities":"[{'text': 'score distillation', 'label': 'Knowledge distillation'}, {'text': 'textual prompt', 'label': 'Prompting'}]","assigned_concept":"Knowledge distillation","matched_keyword":"score distillation","similarity_score":0.6502460837}
{"id":2503.14833,"submitter":"Zihao Liu","authors":"Zihao Liu, Xing Liu, Yizhai Zhang, Zhengxiong Liu, Panfeng Huang","title":"Curiosity-Diffuser: Curiosity Guide Diffusion Models for Reliability","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  One of the bottlenecks in robotic intelligence is the instability of neural\nnetwork models, which, unlike control models, lack a well-defined convergence\ndomain and stability. This leads to risks when applying intelligence in the\nphysical world. Specifically, imitation policy based on neural network may\ngenerate hallucinations, leading to inaccurate behaviors that impact the safety\nof real-world applications. To address this issue, this paper proposes the\nCuriosity-Diffuser, aimed at guiding the conditional diffusion model to\ngenerate trajectories with lower curiosity, thereby improving the reliability\nof policy. The core idea is to use a Random Network Distillation (RND)\ncuriosity module to assess whether the model's behavior aligns with the\ntraining data, and then minimize curiosity by classifier guidance diffusion to\nreduce overgeneralization during inference. Additionally, we propose a\ncomputationally efficient metric for evaluating the reliability of the policy,\nmeasuring the similarity between the generated behaviors and the training\ndataset, to facilitate research about reliability learning. Finally, simulation\nverify the effectiveness and applicability of the proposed method to a variety\nof scenarios, showing that Curiosity-Diffuser significantly improves task\nperformance and produces behaviors that are more similar to the training data.\nThe code for this work is available at: github.com\/CarlDegio\/Curiosity-Diffuser\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 02:25:36 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Liu', 'Zihao', ''], ['Liu', 'Xing', ''], ['Zhang', 'Yizhai', ''], ['Liu', 'Zhengxiong', ''], ['Huang', 'Panfeng', '']]","extracted_entities":"[{'text': 'Random Network Distillation (RND)', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Random Network Distillation (RND)","similarity_score":0.5356193185}
{"id":2503.14975,"submitter":"Zihan Cao","authors":"Zihan Cao, Yu Zhong, Liang-Jian Deng","title":"Taming Flow Matching with Unbalanced Optimal Transport into Fast\n  Pansharpening","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Pansharpening, a pivotal task in remote sensing for fusing high-resolution\npanchromatic and multispectral imagery, has garnered significant research\ninterest. Recent advancements employing diffusion models based on stochastic\ndifferential equations (SDEs) have demonstrated state-of-the-art performance.\nHowever, the inherent multi-step sampling process of SDEs imposes substantial\ncomputational overhead, hindering practical deployment. While existing methods\nadopt efficient samplers, knowledge distillation, or retraining to reduce\nsampling steps (e.g., from 1,000 to fewer steps), such approaches often\ncompromise fusion quality. In this work, we propose the Optimal Transport Flow\nMatching (OTFM) framework, which integrates the dual formulation of unbalanced\noptimal transport (UOT) to achieve one-step, high-quality pansharpening. Unlike\nconventional OT formulations that enforce rigid distribution alignment, UOT\nrelaxes marginal constraints to enhance modeling flexibility, accommodating the\nintrinsic spectral and spatial disparities in remote sensing data. Furthermore,\nwe incorporate task-specific regularization into the UOT objective, enhancing\nthe robustness of the flow model. The OTFM framework enables simulation-free\ntraining and single-step inference while maintaining strict adherence to\npansharpening constraints. Experimental evaluations across multiple datasets\ndemonstrate that OTFM matches or exceeds the performance of previous\nregression-based models and leading diffusion-based methods while only needing\none sampling step. Codes are available at https:\/\/github.com\/294coder\/PAN-OTFM.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 08:10:49 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Cao', 'Zihan', ''], ['Zhong', 'Yu', ''], ['Deng', 'Liang-Jian', '']]","extracted_entities":"[{'text': 'knowledge distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge distillation","similarity_score":1.0000002384}
{"id":2503.15056,"submitter":"Jong Chul Ye","authors":"Suhyeon Lee, Kwanyoung Kim, Jong Chul Ye","title":"Single-Step Bidirectional Unpaired Image Translation Using Implicit\n  Bridge Consistency Distillation","comments":"25 pages, 16 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Unpaired image-to-image translation has seen significant progress since the\nintroduction of CycleGAN. However, methods based on diffusion models or\nSchr\\\"odinger bridges have yet to be widely adopted in real-world applications\ndue to their iterative sampling nature. To address this challenge, we propose a\nnovel framework, Implicit Bridge Consistency Distillation (IBCD), which enables\nsingle-step bidirectional unpaired translation without using adversarial loss.\nIBCD extends consistency distillation by using a diffusion implicit bridge\nmodel that connects PF-ODE trajectories between distributions. Additionally, we\nintroduce two key improvements: 1) distribution matching for consistency\ndistillation and 2) adaptive weighting method based on distillation difficulty.\nExperimental results demonstrate that IBCD achieves state-of-the-art\nperformance on benchmark datasets in a single generation step. Project page\navailable at https:\/\/hyn2028.github.io\/project_page\/IBCD\/index.html\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 09:48:04 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Lee', 'Suhyeon', ''], ['Kim', 'Kwanyoung', ''], ['Ye', 'Jong Chul', '']]","extracted_entities":"[{'text': 'consistency distillation', 'label': 'Knowledge distillation'}, {'text': 'consistency\\ndistillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"consistency distillation","similarity_score":0.6887969971}
{"id":2503.15082,"submitter":"Ziyu Meng","authors":"Le Ma, Ziyu Meng, Tengyu Liu, Yuhan Li, Ran Song, Wei Zhang, Siyuan\n  Huang","title":"StyleLoco: Generative Adversarial Distillation for Natural Humanoid\n  Robot Locomotion","comments":"9 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Humanoid robots are anticipated to acquire a wide range of locomotion\ncapabilities while ensuring natural movement across varying speeds and\nterrains. Existing methods encounter a fundamental dilemma in learning humanoid\nlocomotion: reinforcement learning with handcrafted rewards can achieve agile\nlocomotion but produces unnatural gaits, while Generative Adversarial Imitation\nLearning (GAIL) with motion capture data yields natural movements but suffers\nfrom unstable training processes and restricted agility. Integrating these\napproaches proves challenging due to the inherent heterogeneity between expert\npolicies and human motion datasets. To address this, we introduce StyleLoco, a\nnovel two-stage framework that bridges this gap through a Generative\nAdversarial Distillation (GAD) process. Our framework begins by training a\nteacher policy using reinforcement learning to achieve agile and dynamic\nlocomotion. It then employs a multi-discriminator architecture, where distinct\ndiscriminators concurrently extract skills from both the teacher policy and\nmotion capture data. This approach effectively combines the agility of\nreinforcement learning with the natural fluidity of human-like movements while\nmitigating the instability issues commonly associated with adversarial\ntraining. Through extensive simulation and real-world experiments, we\ndemonstrate that StyleLoco enables humanoid robots to perform diverse\nlocomotion tasks with the precision of expertly trained policies and the\nnatural aesthetics of human motion, successfully transferring styles across\ndifferent movement types while maintaining stable locomotion across a broad\nspectrum of command inputs.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 10:27:44 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Ma', 'Le', ''], ['Meng', 'Ziyu', ''], ['Liu', 'Tengyu', ''], ['Li', 'Yuhan', ''], ['Song', 'Ran', ''], ['Zhang', 'Wei', ''], ['Huang', 'Siyuan', '']]","extracted_entities":"[{'text': 'reinforcement learning', 'label': 'Zero-shot Learning'}, {'text': 'Generative Adversarial Imitation\\nLearning', 'label': 'Few-shot Learning'}, {'text': 'Generative\\nAdversarial Distillation', 'label': 'Knowledge distillation'}, {'text': 'reinforcement learning', 'label': 'Zero-shot Learning'}, {'text': 'reinforcement learning', 'label': 'Zero-shot Learning'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Generative\nAdversarial Distillation","similarity_score":0.6142471433}
{"id":2503.15106,"submitter":"Amir Hamza","authors":"Amir Hamza, Andrea Caraffa, Davide Boscaini, Fabio Poiesi","title":"Distilling 3D distinctive local descriptors for 6D pose estimation","comments":"Project Website: https:\/\/tev-fbk.github.io\/dGeDi\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Three-dimensional local descriptors are crucial for encoding geometric\nsurface properties, making them essential for various point cloud understanding\ntasks. Among these descriptors, GeDi has demonstrated strong zero-shot 6D pose\nestimation capabilities but remains computationally impractical for real-world\napplications due to its expensive inference process. Can we retain GeDi's\neffectiveness while significantly improving its efficiency? In this paper, we\nexplore this question by introducing a knowledge distillation framework that\ntrains an efficient student model to regress local descriptors from a GeDi\nteacher. Our key contributions include: an efficient large-scale training\nprocedure that ensures robustness to occlusions and partial observations while\noperating under compute and storage constraints, and a novel loss formulation\nthat handles weak supervision from non-distinctive teacher descriptors. We\nvalidate our approach on five BOP Benchmark datasets and demonstrate a\nsignificant reduction in inference time while maintaining competitive\nperformance with existing methods, bringing zero-shot 6D pose estimation closer\nto real-time feasibility. Project Website: https:\/\/tev-fbk.github.io\/dGeDi\/\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:04:37 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 08:27:13 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Hamza', 'Amir', ''], ['Caraffa', 'Andrea', ''], ['Boscaini', 'Davide', ''], ['Poiesi', 'Fabio', '']]","extracted_entities":"[{'text': 'knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'zero-shot 6D pose estimation', 'label': 'Zero-shot Learning'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge distillation","similarity_score":1.0000002384}
{"id":2503.15144,"submitter":"Zhe Zhu","authors":"Xing He, Zhe Zhu, Liangliang Nan, Honghua Chen, Jing Qin, Mingqiang\n  Wei","title":"PointSFDA: Source-free Domain Adaptation for Point Cloud Completion","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Conventional methods for point cloud completion, typically trained on\nsynthetic datasets, face significant challenges when applied to\nout-of-distribution real-world scans. In this paper, we propose an effective\nyet simple source-free domain adaptation framework for point cloud completion,\ntermed \\textbf{PointSFDA}. Unlike unsupervised domain adaptation that reduces\nthe domain gap by directly leveraging labeled source data, PointSFDA uses only\na pretrained source model and unlabeled target data for adaptation, avoiding\nthe need for inaccessible source data in practical scenarios. Being the first\nsource-free domain adaptation architecture for point cloud completion, our\nmethod offers two core contributions. First, we introduce a coarse-to-fine\ndistillation solution to explicitly transfer the global geometry knowledge\nlearned from the source dataset. Second, as noise may be introduced due to\ndomain gaps, we propose a self-supervised partial-mask consistency training\nstrategy to learn local geometry information in the target domain. Extensive\nexperiments have validated that our method significantly improves the\nperformance of state-of-the-art networks in cross-domain shape completion. Our\ncode is available at\n\\emph{\\textcolor{magenta}{https:\/\/github.com\/Starak-x\/PointSFDA}}.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 12:09:45 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['He', 'Xing', ''], ['Zhu', 'Zhe', ''], ['Nan', 'Liangliang', ''], ['Chen', 'Honghua', ''], ['Qin', 'Jing', ''], ['Wei', 'Mingqiang', '']]","extracted_entities":"[{'text': 'coarse-to-fine\\ndistillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"coarse-to-fine\ndistillation","similarity_score":0.6290684342}
{"id":2503.15295,"submitter":"Aoting Zhang","authors":"Aoting Zhang, Dongbao Yang, Chang Liu, Xiaopeng Hong, Miao Shang, Yu\n  Zhou","title":"DCA: Dividing and Conquering Amnesia in Incremental Object Detection","comments":"Accepted by AAAI 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Incremental object detection (IOD) aims to cultivate an object detector that\ncan continuously localize and recognize novel classes while preserving its\nperformance on previous classes. Existing methods achieve certain success by\nimproving knowledge distillation and exemplar replay for transformer-based\ndetection frameworks, but the intrinsic forgetting mechanisms remain\nunderexplored. In this paper, we dive into the cause of forgetting and discover\nforgetting imbalance between localization and recognition in transformer-based\nIOD, which means that localization is less-forgetting and can generalize to\nfuture classes, whereas catastrophic forgetting occurs primarily on\nrecognition. Based on these insights, we propose a Divide-and-Conquer Amnesia\n(DCA) strategy, which redesigns the transformer-based IOD into a\nlocalization-then-recognition process. DCA can well maintain and transfer the\nlocalization ability, leaving decoupled fragile recognition to be specially\nconquered. To reduce feature drift in recognition, we leverage semantic\nknowledge encoded in pre-trained language models to anchor class\nrepresentations within a unified feature space across incremental tasks. This\ninvolves designing a duplex classifier fusion and embedding class semantic\nfeatures into the recognition decoding process in the form of queries.\nExtensive experiments validate that our approach achieves state-of-the-art\nperformance, especially for long-term incremental scenarios. For example, under\nthe four-step setting on MS-COCO, our DCA strategy significantly improves the\nfinal AP by 6.9%.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:17:14 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhang', 'Aoting', ''], ['Yang', 'Dongbao', ''], ['Liu', 'Chang', ''], ['Hong', 'Xiaopeng', ''], ['Shang', 'Miao', ''], ['Zhou', 'Yu', '']]","extracted_entities":"[{'text': 'knowledge distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge distillation","similarity_score":1.0000002384}
{"id":2503.15361,"submitter":"Tao Hu","authors":"Qingsen Yan, Tao Hu, Genggeng Chen, Wei Dong, Yanning Zhang","title":"Boosting HDR Image Reconstruction via Semantic Knowledge Transfer","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recovering High Dynamic Range (HDR) images from multiple Low Dynamic Range\n(LDR) images becomes challenging when the LDR images exhibit noticeable\ndegradation and missing content. Leveraging scene-specific semantic priors\noffers a promising solution for restoring heavily degraded regions. However,\nthese priors are typically extracted from sRGB Standard Dynamic Range (SDR)\nimages, the domain\/format gap poses a significant challenge when applying it to\nHDR imaging. To address this issue, we propose a general framework that\ntransfers semantic knowledge derived from SDR domain via self-distillation to\nboost existing HDR reconstruction. Specifically, the proposed framework first\nintroduces the Semantic Priors Guided Reconstruction Model (SPGRM), which\nleverages SDR image semantic knowledge to address ill-posed problems in the\ninitial HDR reconstruction results. Subsequently, we leverage a\nself-distillation mechanism that constrains the color and content information\nwith semantic knowledge, aligning the external outputs between the baseline and\nSPGRM. Furthermore, to transfer the semantic knowledge of the internal\nfeatures, we utilize a semantic knowledge alignment module (SKAM) to fill the\nmissing semantic contents with the complementary masks. Extensive experiments\ndemonstrate that our method can significantly improve the HDR imaging quality\nof existing methods.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:01:27 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Yan', 'Qingsen', ''], ['Hu', 'Tao', ''], ['Chen', 'Genggeng', ''], ['Dong', 'Wei', ''], ['Zhang', 'Yanning', '']]","extracted_entities":"[{'text': 'self-distillation', 'label': 'Knowledge distillation'}, {'text': 'self-distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"self-distillation","similarity_score":0.6929070354}
{"id":2503.15414,"submitter":"Can Peng","authors":"Can Peng, Qianhui Men, Pramit Saha, Qianye Yang, Cheng Ouyang, J.\n  Alison Noble","title":"Federated Continual 3D Segmentation With Single-round Communication","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Federated learning seeks to foster collaboration among distributed clients\nwhile preserving the privacy of their local data. Traditionally, federated\nlearning methods assume a fixed setting in which client data and learning\nobjectives remain constant. However, in real-world scenarios, new clients may\njoin, and existing clients may expand the segmentation label set as task\nrequirements evolve. In such a dynamic federated analysis setup, the\nconventional federated communication strategy of model aggregation per\ncommunication round is suboptimal. As new clients join, this strategy requires\nretraining, linearly increasing communication and computation overhead. It also\nimposes requirements for synchronized communication, which is difficult to\nachieve among distributed clients. In this paper, we propose a federated\ncontinual learning strategy that employs a one-time model aggregation at the\nserver through multi-model distillation. This approach builds and updates the\nglobal model while eliminating the need for frequent server communication. When\nintegrating new data streams or onboarding new clients, this approach\nefficiently reuses previous client models, avoiding the need to retrain the\nglobal model across the entire federation. By minimizing communication load and\nbypassing the need to put unchanged clients online, our approach relaxes\nsynchronization requirements among clients, providing an efficient and scalable\nfederated analysis framework suited for real-world applications. Using\nmulti-class 3D abdominal CT segmentation as an application task, we demonstrate\nthe effectiveness of the proposed approach.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:56:34 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Peng', 'Can', ''], ['Men', 'Qianhui', ''], ['Saha', 'Pramit', ''], ['Yang', 'Qianye', ''], ['Ouyang', 'Cheng', ''], ['Noble', 'J. Alison', '']]","extracted_entities":"[{'text': 'multi-model distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"multi-model distillation","similarity_score":0.6623165607}
{"id":2503.15457,"submitter":"Yuanzhi Zhu","authors":"Yuanzhi Zhu, Xi Wang, St\\'ephane Lathuili\\`ere, Vicky Kalogeiton","title":"Di$\\mathtt{[M]}$O: Distilling Masked Diffusion Models into One-step\n  Generator","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Masked Diffusion Models (MDMs) have emerged as a powerful generative modeling\ntechnique. Despite their remarkable results, they typically suffer from slow\ninference with several steps. In this paper, we propose Di$\\mathtt{[M]}$O, a\nnovel approach that distills masked diffusion models into a one-step generator.\nDi$\\mathtt{[M]}$O addresses two key challenges: (1) the intractability of using\nintermediate-step information for one-step generation, which we solve through\ntoken-level distribution matching that optimizes model output logits by an\n'on-policy framework' with the help of an auxiliary model; and (2) the lack of\nentropy in the initial distribution, which we address through a token\ninitialization strategy that injects randomness while maintaining similarity to\nteacher training distribution. We show Di$\\mathtt{[M]}$O's effectiveness on\nboth class-conditional and text-conditional image generation, impressively\nachieving performance competitive to multi-step teacher outputs while\ndrastically reducing inference time. To our knowledge, we are the first to\nsuccessfully achieve one-step distillation of masked diffusion models and the\nfirst to apply discrete distillation to text-to-image generation, opening new\npaths for efficient generative modeling.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:36:54 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhu', 'Yuanzhi', ''], ['Wang', 'Xi', ''], ['Lathuili\u00e8re', 'St\u00e9phane', ''], ['Kalogeiton', 'Vicky', '']]","extracted_entities":"[{'text': 'discrete distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"discrete distillation","similarity_score":0.6536120772}
{"id":2503.15666,"submitter":"Kyle Vedder","authors":"Kyle Vedder","title":"Toward Scalable, Flexible Scene Flow for Point Clouds","comments":"PhD Thesis","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Scene flow estimation is the task of describing 3D motion between temporally\nsuccessive observations. This thesis aims to build the foundation for building\nscene flow estimators with two important properties: they are scalable, i.e.\nthey improve with access to more data and computation, and they are flexible,\ni.e. they work out-of-the-box in a variety of domains and on a variety of\nmotion patterns without requiring significant hyperparameter tuning.\n  In this dissertation we present several concrete contributions towards this.\nIn Chapter 1 we contextualize scene flow and its prior methods. In Chapter 2 we\npresent a blueprint to build and scale feedforward scene flow estimators\nwithout requiring expensive human annotations via large scale distillation from\npseudolabels provided by strong unsupervised test-time optimization methods. In\nChapter 3 we introduce a benchmark to better measure estimate quality across\ndiverse object types, better bringing into focus what we care about and expect\nfrom scene flow estimators, and use this benchmark to host a public challenge\nthat produced significant progress. In Chapter 4 we present a state-of-the-art\nunsupervised scene flow estimator that introduces a new, full sequence problem\nformulation and exhibits great promise in adjacent domains like 3D point\ntracking. Finally, in Chapter 5 I philosophize about what's next for scene flow\nand its potential future broader impacts.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 19:33:14 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Vedder', 'Kyle', '']]","extracted_entities":"[{'text': 'hyperparameter tuning', 'label': 'Fine-tuning'}, {'text': 'large scale distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"large scale distillation","similarity_score":0.6810040474}
{"id":2503.15676,"submitter":"Taehyoung Kim","authors":"C\\'edric Vincent, Taehyoung Kim, Henri Mee{\\ss}","title":"High Temporal Consistency through Semantic Similarity Propagation in\n  Semi-Supervised Video Semantic Segmentation for Autonomous Flight","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Semantic segmentation from RGB cameras is essential to the perception of\nautonomous flying vehicles. The stability of predictions through the captured\nvideos is paramount to their reliability and, by extension, to the\ntrustworthiness of the agents. In this paper, we propose a lightweight video\nsemantic segmentation approach-suited to onboard real-time inference-achieving\nhigh temporal consistency on aerial data through Semantic Similarity\nPropagation across frames. SSP temporally propagates the predictions of an\nefficient image segmentation model with global registration alignment to\ncompensate for camera movements. It combines the current estimation and the\nprior prediction with linear interpolation using weights computed from the\nfeatures similarities of the two frames. Because data availability is a\nchallenge in this domain, we propose a consistency-aware Knowledge Distillation\ntraining procedure for sparsely labeled datasets with few annotations. Using a\nlarge image segmentation model as a teacher to train the efficient SSP, we\nleverage the strong correlations between labeled and unlabeled frames in the\nsame training videos to obtain high-quality supervision on all frames. KD-SSP\nobtains a significant temporal consistency increase over the base image\nsegmentation model of 12.5% and 6.7% TC on UAVid and RuralScapes respectively,\nwith higher accuracy and comparable inference speed. On these aerial datasets,\nKD-SSP provides a superior segmentation quality and inference speed trade-off\nthan other video methods proposed for general applications and shows\nconsiderably higher consistency. The code will be made publicly available upon\nacceptance.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 20:12:07 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Vincent', 'C\u00e9dric', ''], ['Kim', 'Taehyoung', ''], ['Mee\u00df', 'Henri', '']]","extracted_entities":"[{'text': 'consistency-aware Knowledge Distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"consistency-aware Knowledge Distillation","similarity_score":0.8514709473}
{"id":2503.15697,"submitter":"Efstathios Karypidis","authors":"Panagiota Moraiti, Efstathios Karypidis","title":"Technical Report for the 5th CLVision Challenge at CVPR: Addressing the\n  Class-Incremental with Repetition using Unlabeled Data -- 4th Place Solution","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper outlines our approach to the 5th CLVision challenge at CVPR, which\naddresses the Class-Incremental with Repetition (CIR) scenario. In contrast to\ntraditional class incremental learning, this novel setting introduces unique\nchallenges and research opportunities, particularly through the integration of\nunlabeled data into the training process. In the CIR scenario, encountered\nclasses may reappear in later learning experiences, and each experience may\ninvolve only a subset of the overall class distribution. Additionally, the\nunlabeled data provided during training may include instances of unseen\nclasses, or irrelevant classes which should be ignored. Our approach focuses on\nretaining previously learned knowledge by utilizing knowledge distillation and\npseudo-labeling techniques. The key characteristic of our method is the\nexploitation of unlabeled data during training, in order to maintain optimal\nperformance on instances of previously encountered categories and reduce the\ndetrimental effects of catastrophic forgetting. Our method achieves an average\naccuracy of 16.68\\% during the pre-selection phase and 21.19% during the final\nevaluation phase, outperforming the baseline accuracy of 9.39%. We provide the\nimplementation code at\nhttps:\/\/github.com\/panagiotamoraiti\/continual-learning-challenge-2024 .\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 21:11:57 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Moraiti', 'Panagiota', ''], ['Karypidis', 'Efstathios', '']]","extracted_entities":"[{'text': 'traditional class incremental learning', 'label': 'Few-shot Learning'}, {'text': 'knowledge distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"knowledge distillation","similarity_score":1.0000002384}
{"id":2503.15737,"submitter":"Heming Zhang","authors":"Heming Zhang, Wenyu Li, Di Huang, Yinjie Tang, Yixin Chen, Philip\n  Payne, Fuhai Li","title":"KoGNER: A Novel Framework for Knowledge Graph Distillation on Biomedical\n  Named Entity Recognition","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Named Entity Recognition (NER) is a fundamental task in Natural Language\nProcessing (NLP) that plays a crucial role in information extraction, question\nanswering, and knowledge-based systems. Traditional deep learning-based NER\nmodels often struggle with domain-specific generalization and suffer from data\nsparsity issues. In this work, we introduce Knowledge Graph distilled for Named\nEntity Recognition (KoGNER), a novel approach that integrates Knowledge Graph\n(KG) distillation into NER models to enhance entity recognition performance.\nOur framework leverages structured knowledge representations from KGs to enrich\ncontextual embeddings, thereby improving entity classification and reducing\nambiguity in entity detection. KoGNER employs a two-step process: (1) Knowledge\nDistillation, where external knowledge sources are distilled into a lightweight\nrepresentation for seamless integration with NER models, and (2) Entity-Aware\nAugmentation, which integrates contextual embeddings that have been enriched\nwith knowledge graph information directly into GNN, thereby improving the\nmodel's ability to understand and represent entity relationships. Experimental\nresults on benchmark datasets demonstrate that KoGNER achieves state-of-the-art\nperformance, outperforming finetuned NER models and LLMs by a significant\nmargin. These findings suggest that leveraging knowledge graphs as auxiliary\ninformation can significantly improve NER accuracy, making KoGNER a promising\ndirection for future research in knowledge-aware NLP.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 22:59:36 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zhang', 'Heming', ''], ['Li', 'Wenyu', ''], ['Huang', 'Di', ''], ['Tang', 'Yinjie', ''], ['Chen', 'Yixin', ''], ['Payne', 'Philip', ''], ['Li', 'Fuhai', '']]","extracted_entities":"[{'text': 'contextual embeddings', 'label': 'contextual Embedding'}, {'text': 'Knowledge\\nDistillation', 'label': 'Knowledge distillation'}, {'text': 'contextual embeddings', 'label': 'contextual Embedding'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Knowledge\nDistillation","similarity_score":1.0000002384}
{"id":2503.15843,"submitter":"Tianyi Hao","authors":"Tianyi Hao, Amanda Xu, Swamit Tannu","title":"Reducing T Gates with Unitary Synthesis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cs.ET","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Quantum error correction is essential for achieving practical quantum\ncomputing but has a significant computational overhead. Among fault-tolerant\n(FT) gate operations, non-Clifford gates, such as $T$, are particularly\nexpensive due to their reliance on magic state distillation. These costly $T$\ngates appear frequently in FT circuits as many quantum algorithms require\narbitrary single-qubit rotations, such as $R_x$ and $R_z$ gates, which must be\ndecomposed into a sequence of $T$ and Clifford gates. In many quantum circuits,\n$R_x$ and $R_z$ gates can be fused to form a single $U3$ unitary. However,\nexisting synthesis methods, such as gridsynth, rely on indirect decompositions,\nrequiring separate $R_z$ decompositions that result in a threefold increase in\n$T$ count.\n  This work presents a novel FT synthesis algorithm that directly synthesizes\narbitrary single-qubit unitaries, avoiding the overhead of separate $R_z$\ndecompositions. By leveraging tensor network-based search, our approach enables\nnative $U3$ synthesis, reducing the $T$ count, Clifford gate count, and\napproximation error. Compared to gridsynth-based circuit synthesis, for 187\nrepresentative benchmarks, our design reduces the $T$ count by up to\n$3.5\\times$, and Clifford gates by $7\\times$, resulting in up to $4\\times$\nimprovement in overall circuit infidelity.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 04:53:54 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Hao', 'Tianyi', ''], ['Xu', 'Amanda', ''], ['Tannu', 'Swamit', '']]","extracted_entities":"[{'text': 'magic state distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"magic state distillation","similarity_score":0.6210391521}
{"id":2503.15855,"submitter":"Hyojun Go","authors":"Hyojun Go, Byeongjun Park, Hyelin Nam, Byung-Hoon Kim, Hyungjin Chung,\n  Changick Kim","title":"VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting\n  Generation with Flexible Pose and Multi-View Joint Modeling","comments":"Project page: https:\/\/gohyojun15.github.io\/VideoRFSplat\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We propose VideoRFSplat, a direct text-to-3D model leveraging a video\ngeneration model to generate realistic 3D Gaussian Splatting (3DGS) for\nunbounded real-world scenes. To generate diverse camera poses and unbounded\nspatial extent of real-world scenes, while ensuring generalization to arbitrary\ntext prompts, previous methods fine-tune 2D generative models to jointly model\ncamera poses and multi-view images. However, these methods suffer from\ninstability when extending 2D generative models to joint modeling due to the\nmodality gap, which necessitates additional models to stabilize training and\ninference. In this work, we propose an architecture and a sampling strategy to\njointly model multi-view images and camera poses when fine-tuning a video\ngeneration model. Our core idea is a dual-stream architecture that attaches a\ndedicated pose generation model alongside a pre-trained video generation model\nvia communication blocks, generating multi-view images and camera poses through\nseparate streams. This design reduces interference between the pose and image\nmodalities. Additionally, we propose an asynchronous sampling strategy that\ndenoises camera poses faster than multi-view images, allowing rapidly denoised\nposes to condition multi-view generation, reducing mutual ambiguity and\nenhancing cross-modal consistency. Trained on multiple large-scale real-world\ndatasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms\nexisting text-to-3D direct generation methods that heavily depend on post-hoc\nrefinement via score distillation sampling, achieving superior results without\nsuch refinement.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 05:26:09 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Go', 'Hyojun', ''], ['Park', 'Byeongjun', ''], ['Nam', 'Hyelin', ''], ['Kim', 'Byung-Hoon', ''], ['Chung', 'Hyungjin', ''], ['Kim', 'Changick', '']]","extracted_entities":"[{'text': 'arbitrary\\ntext prompts', 'label': 'Knowledge distillation'}, {'text': 'RealEstate10K', 'label': 'Large Language Model'}, {'text': 'MVImgNet', 'label': 'Large Language Model'}, {'text': 'DL3DV-10K', 'label': 'Large Language Model'}, {'text': 'ACID', 'label': 'Large Language Model'}, {'text': 'VideoRFSplat', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'score distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"score distillation","similarity_score":0.6502460837}
{"id":2503.1593,"submitter":"Artur Terzyk","authors":"Samer Al-Gharabli, Nafisah Al-Rifai, Simona Jurevi\\v{c}i\\=ute, Aivaras\n  Kareiva, Artur P. Terzyk, Emil Korczeniewski, Ewa Olewnik-Kruszkowska,\n  Zuzanna Flanc, Waldemar Jankowski, Wojciech Kujawski, Joanna Kujawa","title":"1-Adamantanamine implementation in surface engineering of biomimetic\n  PVDF-based membranes for enhanced membrane distillation","comments":"56 pages, 17 figures, 5 tables","journal-ref":"Desalination, 596, 118331, 2025","doi":"10.1016\/j.desal.2024.118331","report-no":null,"categories":"cond-mat.mtrl-sci","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Membrane distillation (MD) stands at the forefront of desalination\ntechnology, harnessing the power of phase change to separate water vapor from\nsaline using minimal energy resources efficiently. In response to this\nchallenge, membranes with tuned pores morphology and surface chemistry with\nbiomimetic 3D pine-like structures with improved affinity to water\n(desalination) and\/or hazardous VOC (VOC removal) were developed and studied\nsystematically. By implementing VIPS-PVDF membranes and a green modifier of\n1-adamantanamine for the first time, membranes with a revolutionary network\narchitecture were generated. The modifier was introduced either physically to\nthe polymeric matrix or chemically through covalent attachment onto the surface\nand inside the porous structure. As a result, membranes that defy wetting under\nextreme hydrostatic pressures (>11.5 bar) were produced while preserving\nunparalleled vapor transport efficiency. The 1-adamantanamine promotes\ntransport and enhances the affinity to the VOC, ensuring excellent membrane\nperformance at different applications of the MD process. Transport was enhanced\nmore than 3.6 times and separation factor beta changed from 3.48 to 15.22 for\nMTBE removal and from 2.0 to 3.46 for EtOH removal when comparing pristine PVDF\nwith membrane chemically modified with 1-adamantanamine (PVDF_Ch02). The\nprocess separation index during the MTBE removal changed from 20 kg m-2 h-1\n(PVDF) to 297 kg m-2 h-1 (PVDF_Ch02). All materials were highly stable and\ndurable during the MD applications. This innovative approach not only\nrevolutionizes desalination but also holds immense promise for diverse\napplications beyond, particularly in the realm of wastewater treatment. A study\nof the icing process on a cold plate with new membranes provided deeper insight\ninto the icing mechanism and the role of membrane LEP in it.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:14:03 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Al-Gharabli', 'Samer', ''], ['Al-Rifai', 'Nafisah', ''], ['Jurevi\u010di\u016bte', 'Simona', ''], ['Kareiva', 'Aivaras', ''], ['Terzyk', 'Artur P.', ''], ['Korczeniewski', 'Emil', ''], ['Olewnik-Kruszkowska', 'Ewa', ''], ['Flanc', 'Zuzanna', ''], ['Jankowski', 'Waldemar', ''], ['Kujawski', 'Wojciech', ''], ['Kujawa', 'Joanna', '']]","extracted_entities":"[{'text': 'Membrane distillation', 'label': 'Knowledge distillation'}, {'text': 'MD', 'label': 'Knowledge distillation'}, {'text': 'MD', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"Membrane distillation","similarity_score":0.567135036}
{"id":2503.16302,"submitter":"Zeqiang Lai","authors":"Zeqiang Lai, Yunfei Zhao, Zibo Zhao, Haolin Liu, Fuyun Wang, Huiwen\n  Shi, Xianghui Yang, Qinxiang Lin, Jinwei Huang, Yuhong Liu, Jie Jiang,\n  Chunchao Guo, Xiangyu Yue","title":"Unleashing Vecset Diffusion Model for Fast Shape Generation","comments":"Technical report","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI eess.IV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps:\/\/github.com\/Tencent\/FlashVDM.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 16:23:44 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Lai', 'Zeqiang', ''], ['Zhao', 'Yunfei', ''], ['Zhao', 'Zibo', ''], ['Liu', 'Haolin', ''], ['Wang', 'Fuyun', ''], ['Shi', 'Huiwen', ''], ['Yang', 'Xianghui', ''], ['Lin', 'Qinxiang', ''], ['Huang', 'Jinwei', ''], ['Liu', 'Yuhong', ''], ['Jiang', 'Jie', ''], ['Guo', 'Chunchao', ''], ['Yue', 'Xiangyu', '']]","extracted_entities":"[{'text': 'stabilizing consistency distillation', 'label': 'Knowledge distillation'}, {'text': 'Progressive Flow Distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"stabilizing consistency distillation","similarity_score":0.6148293018}
{"id":2503.16322,"submitter":"Songhua Liu","authors":"Ruonan Yu and Songhua Liu and Zhenxiong Tan and Xinchao Wang","title":"Ultra-Resolution Adaptation with Ease","comments":"Technical Report. Codes are available\n  \\href{https:\/\/github.com\/Huage001\/URAE}{here}","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Text-to-image diffusion models have achieved remarkable progress in recent\nyears. However, training models for high-resolution image generation remains\nchallenging, particularly when training data and computational resources are\nlimited. In this paper, we explore this practical problem from two key\nperspectives: data and parameter efficiency, and propose a set of key\nguidelines for ultra-resolution adaptation termed \\emph{URAE}. For data\nefficiency, we theoretically and empirically demonstrate that synthetic data\ngenerated by some teacher models can significantly promote training\nconvergence. For parameter efficiency, we find that tuning minor components of\nthe weight matrices outperforms widely-used low-rank adapters when synthetic\ndata are unavailable, offering substantial performance gains while maintaining\nefficiency. Additionally, for models leveraging guidance distillation, such as\nFLUX, we show that disabling classifier-free guidance, \\textit{i.e.}, setting\nthe guidance scale to 1 during adaptation, is crucial for satisfactory\nperformance. Extensive experiments validate that URAE achieves comparable\n2K-generation performance to state-of-the-art closed-source models like FLUX1.1\n[Pro] Ultra with only 3K samples and 2K iterations, while setting new\nbenchmarks for 4K-resolution generation. Codes are available\n\\href{https:\/\/github.com\/Huage001\/URAE}{here}.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 16:44:43 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Yu', 'Ruonan', ''], ['Liu', 'Songhua', ''], ['Tan', 'Zhenxiong', ''], ['Wang', 'Xinchao', '']]","extracted_entities":"[{'text': 'guidance distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Knowledge distillation","matched_keyword":"guidance distillation","similarity_score":0.6952179074}
{"id":2405.01217,"submitter":"Chenying Liu","authors":"Chenying Liu, Conrad Albrecht, Yi Wang, Xiao Xiang Zhu","title":"CromSS: Cross-modal pre-training with noisy labels for remote sensing\n  image segmentation","comments":"The 1st short version was accepted as an oral presentation by ICLR\n  2024 ML4RS workshop. The 2nd extended version was accepted by IEEE TGRS","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We explore the potential of large-scale noisily labeled data to enhance\nfeature learning by pretraining semantic segmentation models within a\nmulti-modal framework for geospatial applications. We propose a novel\nCross-modal Sample Selection (CromSS) method, a weakly supervised pretraining\nstrategy designed to improve feature representations through cross-modal\nconsistency and noise mitigation techniques. Unlike conventional pretraining\napproaches, CromSS exploits massive amounts of noisy and easy-to-come-by labels\nfor improved feature learning beneficial to semantic segmentation tasks. We\ninvestigate middle and late fusion strategies to optimize the multi-modal\npretraining architecture design. We also introduce a cross-modal sample\nselection module to mitigate the adverse effects of label noise, which employs\na cross-modal entangling strategy to refine the estimated confidence masks\nwithin each modality to guide the sampling process. Additionally, we introduce\na spatial-temporal label smoothing technique to counteract overconfidence for\nenhanced robustness against noisy labels. To validate our approach, we\nassembled the multi-modal dataset, NoLDO-S12, which consists of a large-scale\nnoisy label subset from Google's Dynamic World (DW) dataset for pretraining and\ntwo downstream subsets with high-quality labels from Google DW and\nOpenStreetMap (OSM) for transfer learning. Experimental results on two\ndownstream tasks and the publicly available DFC2020 dataset demonstrate that\nwhen effectively utilized, the low-cost noisy labels can significantly enhance\nfeature learning for segmentation tasks. All data, code, and pretrained weights\nwill be made publicly available.\n","versions":"[{'version': 'v1', 'created': 'Thu, 2 May 2024 11:58:06 GMT'}, {'version': 'v2', 'created': 'Mon, 3 Mar 2025 07:38:09 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 07:26:04 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Liu', 'Chenying', ''], ['Albrecht', 'Conrad', ''], ['Wang', 'Yi', ''], ['Zhu', 'Xiao Xiang', '']]","extracted_entities":"[{'text': 'NoLDO-S12', 'label': 'Large Language Model'}, {'text': 'transfer learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"transfer learning","similarity_score":0.5694054365}
{"id":2408.16939,"submitter":"Mohammadamin Banayeeanzade","authors":"Amin Banayeeanzade, Mahdi Soltanolkotabi, Mohammad Rostami","title":"Theoretical Insights into Overparameterized Models in Multi-Task and\n  Replay-Based Continual Learning","comments":"TMLR camera-ready version","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multi-task learning (MTL) is a machine learning paradigm that aims to improve\nthe generalization performance of a model on multiple related tasks by training\nit simultaneously on those tasks. Unlike MTL, where the model has instant\naccess to the training data of all tasks, continual learning (CL) involves\nadapting to new sequentially arriving tasks over time without forgetting the\npreviously acquired knowledge. Despite the wide practical adoption of CL and\nMTL and extensive literature on both areas, there remains a gap in the\ntheoretical understanding of these methods when used with overparameterized\nmodels such as deep neural networks. This paper studies the overparameterized\nlinear models as a proxy for more complex models. We develop theoretical\nresults describing the effect of various system parameters on the model's\nperformance in an MTL setup. Specifically, we study the impact of model size,\ndataset size, and task similarity on the generalization error and knowledge\ntransfer. Additionally, we present theoretical results to characterize the\nperformance of replay-based CL models. Our results reveal the impact of buffer\nsize and model capacity on the forgetting rate in a CL setup and help shed\nlight on some of the state-of-the-art CL methods. Finally, through extensive\nempirical evaluations, we demonstrate that our theoretical findings are also\napplicable to deep neural networks, offering valuable guidance for designing\nMTL and CL models in practice.\n","versions":"[{'version': 'v1', 'created': 'Thu, 29 Aug 2024 23:22:40 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 18:13:46 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Banayeeanzade', 'Amin', ''], ['Soltanolkotabi', 'Mahdi', ''], ['Rostami', 'Mohammad', '']]","extracted_entities":"[{'text': 'Multi-task learning', 'label': 'Few-shot Learning'}, {'text': 'MTL', 'label': 'Few-shot Learning'}, {'text': 'MTL', 'label': 'Few-shot Learning'}, {'text': 'continual learning', 'label': 'Few-shot Learning'}, {'text': 'MTL', 'label': 'Few-shot Learning'}, {'text': 'MTL', 'label': 'Few-shot Learning'}, {'text': 'CL', 'label': 'Few-shot Learning'}, {'text': 'MTL', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"Multi-task learning","similarity_score":0.5245423317}
{"id":2412.03871,"submitter":"Chu Myaet Thwal","authors":"Chu Myaet Thwal, Ye Lin Tun, Minh N. H. Nguyen, Eui-Nam Huh and Choong\n  Seon Hong","title":"CLIP-PING: Boosting Lightweight Vision-Language Models with Proximus\n  Intrinsic Neighbors Guidance","comments":"14 pages, 5 figures, 24 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Beyond the success of Contrastive Language-Image Pre-training (CLIP), recent\ntrends mark a shift toward exploring the applicability of lightweight\nvision-language models for resource-constrained scenarios. These models often\ndeliver suboptimal performance when relying solely on a single image-text\ncontrastive learning objective, spotlighting the need for more effective\ntraining mechanisms that guarantee robust cross-modal feature alignment. In\nthis work, we propose CLIP-PING: Contrastive Language-Image Pre-training with\nProximus Intrinsic Neighbors Guidance, a novel yet simple and efficient\ntraining paradigm designed to boost the performance of lightweight\nvision-language models with minimal computational overhead and lower data\ndemands. CLIP-PING bootstraps unimodal features extracted from arbitrary\npre-trained encoders to obtain intrinsic guidance of proximus neighbor samples,\ni.e., nearest-neighbor (NN) and cross nearest-neighbor (XNN). We find that\nextra contrastive supervision from these neighbors substantially boosts\ncross-modal alignment, enabling lightweight models to learn more generic\nfeatures with rich semantic diversity. Extensive experiments reveal that\nCLIP-PING notably surpasses its peers in zero-shot generalization and\ncross-modal retrieval tasks. Specifically, a 5.5% gain on zero-shot ImageNet1K\nclassification with 10.7% (I2T) and 5.7% (T2I) on Flickr30K retrieval, compared\nto the original CLIP when using ViT-XS image encoder trained on 3 million\n(image, text) pairs. Moreover, CLIP-PING showcases a strong transferability\nunder the linear evaluation protocol across several downstream tasks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 5 Dec 2024 04:58:28 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 02:30:05 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Thwal', 'Chu Myaet', ''], ['Tun', 'Ye Lin', ''], ['Nguyen', 'Minh N. H.', ''], ['Huh', 'Eui-Nam', ''], ['Hong', 'Choong Seon', '']]","extracted_entities":"[{'text': 'zero-shot generalization', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"zero-shot generalization","similarity_score":0.6819497943}
{"id":2501.15473,"submitter":"Liu Yingtian","authors":"Yingtian Liu, Yong Li, Junheng Peng, Mingwei Wang","title":"Semi-Supervised Learning for AVO Inversion with Strong Spatial Feature\n  Constraints","comments":"The manuscript has been submitted to IEEE Transactions on Geoscience\n  and Remote Sensing for reviewing","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.geo-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  One-dimensional convolution is a widely used deep learning technique in\nprestack amplitude variation with offset (AVO) inversion; however, it lacks\nlateral continuity. Although two-dimensional convolution improves lateral\ncontinuity, due to the sparsity of well-log data, the model only learns weak\nspatial features and fails to explore the spatial correlations in seismic data\nfully. To overcome these challenges, we propose a novel AVO inversion method\nbased on semi-supervised learning with strong spatial feature constraints\n(SSFC-SSL). First, two-dimensional predicted values are obtained through the\ninversion network, and the predicted values at well locations are sparsely\nrepresented using well-log labels. Subsequently, a label-annihilation operator\nis introduced, enabling the predicted values at non-well locations to learn the\nspatial features of well locations through the neural network. Ultimately, a\ntwo-way strong spatial feature mapping between non-well locations and well\nlocations is achieved. Additionally, to reduce the dependence on well-log\nlabels, we combine the semi-supervised learning strategy with a low-frequency\nmodel, further enhancing the robustness of the method. Experimental results on\nboth synthetic example and field data demonstrate that the proposed method\nsignificantly improves lateral continuity and inversion accuracy compared to\none- and two-dimensional deep learning techniques.\n","versions":"[{'version': 'v1', 'created': 'Sun, 26 Jan 2025 10:21:11 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 04:19:59 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Liu', 'Yingtian', ''], ['Li', 'Yong', ''], ['Peng', 'Junheng', ''], ['Wang', 'Mingwei', '']]","extracted_entities":"[{'text': 'semi-supervised learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"semi-supervised learning","similarity_score":0.5018399954}
{"id":2502.2058,"submitter":"Maher Hanut","authors":"Maher Hanut, Jonathan Kadmon","title":"Training Large Neural Networks With Low-Dimensional Error Feedback","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG q-bio.NC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Training deep neural networks typically relies on backpropagating high\ndimensional error signals a computationally intensive process with little\nevidence supporting its implementation in the brain. However, since most tasks\ninvolve low-dimensional outputs, we propose that low-dimensional error signals\nmay suffice for effective learning. To test this hypothesis, we introduce a\nnovel local learning rule based on Feedback Alignment that leverages indirect,\nlow-dimensional error feedback to train large networks. Our method decouples\nthe backward pass from the forward pass, enabling precise control over error\nsignal dimensionality while maintaining high-dimensional representations. We\nbegin with a detailed theoretical derivation for linear networks, which forms\nthe foundation of our learning framework, and extend our approach to nonlinear,\nconvolutional, and transformer architectures. Remarkably, we demonstrate that\neven minimal error dimensionality on the order of the task dimensionality can\nachieve performance matching that of traditional backpropagation. Furthermore,\nour rule enables efficient training of convolutional networks, which have\npreviously been resistant to Feedback Alignment methods, with minimal error.\nThis breakthrough not only paves the way toward more biologically accurate\nmodels of learning but also challenges the conventional reliance on\nhigh-dimensional gradient signals in neural network training. Our findings\nsuggest that low-dimensional error signals can be as effective as\nhigh-dimensional ones, prompting a reevaluation of gradient-based learning in\nhigh-dimensional systems. Ultimately, our work offers a fresh perspective on\nneural network optimization and contributes to understanding learning\nmechanisms in both artificial and biological systems.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 22:45:41 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 12:41:58 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 11:00:14 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Hanut', 'Maher', ''], ['Kadmon', 'Jonathan', '']]","extracted_entities":"[{'text': 'gradient-based learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"gradient-based learning","similarity_score":0.5312626958}
{"id":2503.07158,"submitter":"Hua Wei","authors":"Longchao Da, Tiejin Chen, Zhuoheng Li, Shreyas Bachiraju, Huaiyuan\n  Yao, Li Li, Yushun Dong, Xiyang Hu, Zhengzhong Tu, Dongjie Wang, Yue Zhao,\n  Xuanyu (Ben) Zhou, Ram Pendyala, Benjamin Stabler, Yezhou Yang, Xuesong Zhou,\n  Hua Wei","title":"Generative AI in Transportation Planning: A Survey","comments":"55 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The integration of generative artificial intelligence (GenAI) into\ntransportation planning has the potential to revolutionize tasks such as demand\nforecasting, infrastructure design, policy evaluation, and traffic simulation.\nHowever, there is a critical need for a systematic framework to guide the\nadoption of GenAI in this interdisciplinary domain. In this survey, we, a\nmultidisciplinary team of researchers spanning computer science and\ntransportation engineering, present the first comprehensive framework for\nleveraging GenAI in transportation planning. Specifically, we introduce a new\ntaxonomy that categorizes existing applications and methodologies into two\nperspectives: transportation planning tasks and computational techniques. From\nthe transportation planning perspective, we examine the role of GenAI in\nautomating descriptive, predictive, generative, simulation, and explainable\ntasks to enhance mobility systems. From the computational perspective, we\ndetail advancements in data preparation, domain-specific fine-tuning, and\ninference strategies, such as retrieval-augmented generation and zero-shot\nlearning tailored to transportation applications. Additionally, we address\ncritical challenges, including data scarcity, explainability, bias mitigation,\nand the development of domain-specific evaluation frameworks that align with\ntransportation goals like sustainability, equity, and system efficiency. This\nsurvey aims to bridge the gap between traditional transportation planning\nmethodologies and modern AI techniques, fostering collaboration and innovation.\nBy addressing these challenges and opportunities, we seek to inspire future\nresearch that ensures ethical, equitable, and impactful use of generative AI in\ntransportation planning.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Mar 2025 10:33:31 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 06:50:45 GMT'}, {'version': 'v3', 'created': 'Fri, 14 Mar 2025 06:56:22 GMT'}, {'version': 'v4', 'created': 'Tue, 18 Mar 2025 05:03:23 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Da', 'Longchao', '', 'Ben'], ['Chen', 'Tiejin', '', 'Ben'], ['Li', 'Zhuoheng', '', 'Ben'], ['Bachiraju', 'Shreyas', '', 'Ben'], ['Yao', 'Huaiyuan', '', 'Ben'], ['Li', 'Li', '', 'Ben'], ['Dong', 'Yushun', '', 'Ben'], ['Hu', 'Xiyang', '', 'Ben'], ['Tu', 'Zhengzhong', '', 'Ben'], ['Wang', 'Dongjie', '', 'Ben'], ['Zhao', 'Yue', '', 'Ben'], ['Xuanyu', '', '', 'Ben'], ['Zhou', '', ''], ['Pendyala', 'Ram', ''], ['Stabler', 'Benjamin', ''], ['Yang', 'Yezhou', ''], ['Zhou', 'Xuesong', ''], ['Wei', 'Hua', '']]","extracted_entities":"[{'text': 'domain-specific fine-tuning', 'label': 'Fine-tuning'}, {'text': 'zero-shot\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'equity', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Few-shot Learning","matched_keyword":"zero-shot\nlearning","similarity_score":0.8116950989}
{"id":2503.07667,"submitter":"Wei Dai","authors":"Wei Dai, Peilin Chen, Malinda Lu, Daniel Li, Haowen Wei, Hejie Cui,\n  Paul Pu Liang","title":"CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation\n  Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV eess.SP","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Recent advances in clinical AI have enabled remarkable progress across many\nclinical domains. However, existing benchmarks and models are primarily limited\nto a small set of modalities and tasks, which hinders the development of\nlarge-scale multimodal methods that can make holistic assessments of patient\nhealth and well-being. To bridge this gap, we introduce Clinical Large-Scale\nIntegrative Multimodal Benchmark (CLIMB), a comprehensive clinical benchmark\nunifying diverse clinical data across imaging, language, temporal, and graph\nmodalities. CLIMB comprises 4.51 million patient samples totaling 19.01\nterabytes distributed across 2D imaging, 3D video, time series, graphs, and\nmultimodal data. Through extensive empirical evaluation, we demonstrate that\nmultitask pretraining significantly improves performance on understudied\ndomains, achieving up to 29% improvement in ultrasound and 23% in ECG analysis\nover single-task learning. Pretraining on CLIMB also effectively improves\nmodels' generalization capability to new tasks, and strong unimodal encoder\nperformance translates well to multimodal performance when paired with\ntask-appropriate fusion strategies. Our findings provide a foundation for new\narchitecture designs and pretraining strategies to advance clinical AI\nresearch. Code is released at https:\/\/github.com\/DDVD233\/climb.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 01:45:05 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 05:05:56 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Dai', 'Wei', ''], ['Chen', 'Peilin', ''], ['Lu', 'Malinda', ''], ['Li', 'Daniel', ''], ['Wei', 'Haowen', ''], ['Cui', 'Hejie', ''], ['Liang', 'Paul Pu', '']]","extracted_entities":"[{'text': 'single-task learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"single-task learning","similarity_score":0.5836779475}
{"id":2503.0864,"submitter":"Emily Xiao","authors":"Emily Xiao, Chin-Jou Li, Yilin Zhang, Graham Neubig, Amanda Bertsch","title":"Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention","comments":"Preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 17:30:58 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 17:13:42 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Xiao', 'Emily', ''], ['Li', 'Chin-Jou', ''], ['Zhang', 'Yilin', ''], ['Neubig', 'Graham', ''], ['Bertsch', 'Amanda', '']]","extracted_entities":"[{'text': 'Many-shot in-context learning', 'label': 'Few-shot Learning'}, {'text': 'finetuning', 'label': 'Fine-tuning'}, {'text': 'Dynamic\\nBlock-Sparse Attention', 'label': 'Attention mechanism'}, {'text': 'many-shot\\nin-context learning', 'label': 'Few-shot Learning'}, {'text': 'block-sparse attention', 'label': 'Attention mechanism'}, {'text': 'finetuning', 'label': 'Fine-tuning'}, {'text': 'finetuning', 'label': 'Fine-tuning'}, {'text': 'many-shot ICL', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"Many-shot in-context learning","similarity_score":0.6954048276}
{"id":2503.09722,"submitter":"Daniel Pfrommer","authors":"Max Simchowitz, Daniel Pfrommer, and Ali Jadbabaie","title":"The Pitfalls of Imitation Learning when Actions are Continuous","comments":"98 pages, 2 figures, updated introduction","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.SY eess.SY stat.ML","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We study the problem of imitating an expert demonstrator in a discrete-time,\ncontinuous state-and-action control system. We show that, even if the dynamics\nare stable (i.e. contracting exponentially quickly), and the expert is smooth\nand deterministic, any smooth, deterministic imitator policy necessarily\nsuffers error on execution that is exponentially larger, as a function of\nproblem horizon, than the error under the distribution of expert training data.\nOur negative result applies to both behavior cloning and offline-RL algorithms,\nunless they produce highly \"improper\" imitator policies--those which are\nnon-smooth, non-Markovian, or which exhibit highly state-dependent\nstochasticity--or unless the expert trajectory distribution is sufficiently\n\"spread.\" We provide experimental evidence of the benefits of these more\ncomplex policy parameterizations, explicating the benefits of today's popular\npolicy parameterizations in robot learning (e.g. action-chunking and Diffusion\nPolicies). We also establish a host of complementary negative and positive\nresults for imitation in control systems.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 18:11:37 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 14:37:53 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Simchowitz', 'Max', ''], ['Pfrommer', 'Daniel', ''], ['Jadbabaie', 'Ali', '']]","extracted_entities":"[{'text': 'robot learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"robot learning","similarity_score":0.5184552073}
{"id":2503.12873,"submitter":"Dehai Zhao","authors":"Dehai Zhao, Zhenchang Xing, Qinghua Lu, Xiwei Xu, Liming Zhu","title":"SeeAction: Towards Reverse Engineering How-What-Where of HCI Actions\n  from Screencasts for UI Automation","comments":"Accepted by IEEE\/ACM International Conference on Software Engineering\n  2025 (ICSE 2025, Distinguished paper award)","journal-ref":"ICSE 2025","doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  UI automation is a useful technique for UI testing, bug reproduction, and\nrobotic process automation. Recording user actions with an application assists\nrapid development of UI automation scripts, but existing recording techniques\nare intrusive, rely on OS or GUI framework accessibility support, or assume\nspecific app implementations. Reverse engineering user actions from screencasts\nis non-intrusive, but a key reverse-engineering step is currently missing -\nrecognizing human-understandable structured user actions ([command] [widget]\n[location]) from action screencasts. To fill the gap, we propose a deep\nlearning-based computer vision model that can recognize 11 commands and 11\nwidgets, and generate location phrases from action screencasts, through joint\nlearning and multi-task learning. We label a large dataset with 7260\nvideo-action pairs, which record user interactions with Word, Zoom, Firefox,\nPhotoshop, and Windows 10 Settings. Through extensive experiments, we confirm\nthe effectiveness and generality of our model, and demonstrate the usefulness\nof a screencast-to-action-script tool built upon our model for bug\nreproduction.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 07:07:38 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Zhao', 'Dehai', ''], ['Xing', 'Zhenchang', ''], ['Lu', 'Qinghua', ''], ['Xu', 'Xiwei', ''], ['Zhu', 'Liming', '']]","extracted_entities":"[{'text': 'multi-task learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"multi-task learning","similarity_score":0.5245423317}
{"id":2503.12974,"submitter":"Xueying Jiang","authors":"Xueying Jiang, Wenhao Li, Xiaoqin Zhang, Ling Shao, Shijian Lu","title":"Exploring 3D Activity Reasoning and Planning: From Implicit Human\n  Intentions to Route-Aware Planning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  3D activity reasoning and planning has attracted increasing attention in\nhuman-robot interaction and embodied AI thanks to the recent advance in\nmultimodal learning. However, most existing works share two constraints: 1)\nheavy reliance on explicit instructions with little reasoning on implicit user\nintention; 2) negligence of inter-step route planning on robot moves. To bridge\nthe gaps, we propose 3D activity reasoning and planning, a novel 3D task that\nreasons the intended activities from implicit instructions and decomposes them\ninto steps with inter-step routes and planning under the guidance of\nfine-grained 3D object shapes and locations from scene segmentation. We tackle\nthe new 3D task from two perspectives. First, we construct ReasonPlan3D, a\nlarge-scale benchmark that covers diverse 3D scenes with rich implicit\ninstructions and detailed annotations for multi-step task planning, inter-step\nroute planning, and fine-grained segmentation. Second, we design a novel\nframework that introduces progressive plan generation with contextual\nconsistency across multiple steps, as well as a scene graph that is updated\ndynamically for capturing critical objects and their spatial relations.\nExtensive experiments demonstrate the effectiveness of our benchmark and\nframework in reasoning activities from implicit human instructions, producing\naccurate stepwise task plans, and seamlessly integrating route planning for\nmulti-step moves. The dataset and code will be released.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 09:33:58 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Jiang', 'Xueying', ''], ['Li', 'Wenhao', ''], ['Zhang', 'Xiaoqin', ''], ['Shao', 'Ling', ''], ['Lu', 'Shijian', '']]","extracted_entities":"[{'text': 'multimodal learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"multimodal learning","similarity_score":0.5063463449}
{"id":2503.13026,"submitter":"Changxu Cheng","authors":"Tao Wang, Changxu Cheng, Lingfeng Wang, Senda Chen, Wuyue Zhao","title":"HiMTok: Learning Hierarchical Mask Tokens for Image Segmentation with\n  Large Multimodal Model","comments":"technical report","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  The remarkable performance of large multimodal models (LMMs) has attracted\nsignificant interest from the image segmentation community. To align with the\nnext-token-prediction paradigm, current LMM-driven segmentation methods either\nuse object boundary points to represent masks or introduce special segmentation\ntokens, whose hidden states are decoded by a segmentation model requiring the\noriginal image as input. However, these approaches often suffer from inadequate\nmask representation and complex architectures, limiting the potential of LMMs.\nIn this work, we propose the Hierarchical Mask Tokenizer (HiMTok), which\nrepresents segmentation masks with up to 32 tokens and eliminates the need for\nthe original image during mask de-tokenization. HiMTok allows for compact and\ncoarse-to-fine mask representations, aligning well with the LLM\nnext-token-prediction paradigm and facilitating the direct acquisition of\nsegmentation capabilities. We develop a 3-stage training recipe for progressive\nlearning of segmentation and visual capabilities, featuring a hierarchical mask\nloss for effective coarse-to-fine learning. Additionally, we enable\nbidirectional information flow, allowing conversion between bounding boxes and\nmask tokens to fully leverage multi-task training potential. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\nacross various segmentation tasks,while also enhancing visual grounding and\nmaintaining overall visual understanding.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 10:29:08 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wang', 'Tao', ''], ['Cheng', 'Changxu', ''], ['Wang', 'Lingfeng', ''], ['Chen', 'Senda', ''], ['Zhao', 'Wuyue', '']]","extracted_entities":"[{'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'coarse-to-fine learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"coarse-to-fine learning","similarity_score":0.5909110904}
{"id":2503.13134,"submitter":"Prakhar Bhardwaj","authors":"Prakhar Bhardwaj, Sheethal Bhat, Andreas Maier","title":"Enhancing zero-shot learning in medical imaging: integrating clip with\n  advanced techniques for improved chest x-ray analysis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Due to the large volume of medical imaging data, advanced AI methodologies\nare needed to assist radiologists in diagnosing thoracic diseases from chest\nX-rays (CXRs). Existing deep learning models often require large, labeled\ndatasets, which are scarce in medical imaging due to the time-consuming and\nexpert-driven annotation process. In this paper, we extend the existing\napproach to enhance zero-shot learning in medical imaging by integrating\nContrastive Language-Image Pre-training (CLIP) with Momentum Contrast (MoCo),\nresulting in our proposed model, MoCoCLIP. Our method addresses challenges\nposed by class-imbalanced and unlabeled datasets, enabling improved detection\nof pulmonary pathologies. Experimental results on the NIH ChestXray14 dataset\ndemonstrate that MoCoCLIP outperforms the state-of-the-art CheXZero model,\nachieving relative improvement of approximately 6.5%. Furthermore, on the\nCheXpert dataset, MoCoCLIP demonstrates superior zero-shot performance,\nachieving an average AUC of 0.750 compared to CheXZero with 0.746 AUC,\nhighlighting its enhanced generalization capabilities on unseen data.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 12:59:34 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Bhardwaj', 'Prakhar', ''], ['Bhat', 'Sheethal', ''], ['Maier', 'Andreas', '']]","extracted_entities":"[{'text': 'zero-shot learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"zero-shot learning","similarity_score":0.8116950989}
{"id":2503.13289,"submitter":"Nathan P. Lawrence","authors":"Thomas Banker, Nathan P. Lawrence, Ali Mesbah","title":"Local-Global Learning of Interpretable Control Policies: The Interface\n  between MPC and Reinforcement Learning","comments":"Preprint for ACC 2025 tutorial","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Making optimal decisions under uncertainty is a shared problem among distinct\nfields. While optimal control is commonly studied in the framework of dynamic\nprogramming, it is approached with differing perspectives of the Bellman\noptimality condition. In one perspective, the Bellman equation is used to\nderive a global optimality condition useful for iterative learning of control\npolicies through interactions with an environment. Alternatively, the Bellman\nequation is also widely adopted to derive tractable optimization-based control\npolicies that satisfy a local notion of optimality. By leveraging ideas from\nthe two perspectives, we present a local-global paradigm for optimal control\nsuited for learning interpretable local decision makers that approximately\nsatisfy the global Bellman equation. The benefits and practical complications\nin local-global learning are discussed. These aspects are exemplified through\ncase studies, which give an overview of two distinct strategies for unifying\nreinforcement learning and model predictive control. We discuss the challenges\nand trade-offs in these local-global strategies, towards highlighting future\nresearch opportunities for safe and optimal decision-making under uncertainty.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:38:41 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Banker', 'Thomas', ''], ['Lawrence', 'Nathan P.', ''], ['Mesbah', 'Ali', '']]","extracted_entities":"[{'text': 'local-global learning', 'label': 'Few-shot Learning'}, {'text': 'reinforcement learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"local-global learning","similarity_score":0.5398771167}
{"id":2503.13942,"submitter":"Bouarfa Mahi Dr","authors":"Bouarfa Mahi Quantiota","title":"Structured Knowledge Accumulation: An Autonomous Framework for\n  Layer-Wise Entropy Reduction in Neural Learning","comments":"16 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.NE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce the Structured Knowledge Accumulation (SKA) framework, which\nreinterprets entropy as a dynamic, layer-wise measure of knowledge alignment in\nneural networks. Instead of relying on traditional gradient-based optimization,\nSKA defines entropy in terms of knowledge vectors and their influence on\ndecision probabilities across multiple layers. This formulation naturally leads\nto the emergence of activation functions such as the sigmoid as a consequence\nof entropy minimization. Unlike conventional backpropagation, SKA allows each\nlayer to optimize independently by aligning its knowledge representation with\nchanges in decision probabilities. As a result, total network entropy decreases\nin a hierarchical manner, allowing knowledge structures to evolve\nprogressively. This approach provides a scalable, biologically plausible\nalternative to gradient-based learning, bridging information theory and\nartificial intelligence while offering promising applications in\nresource-constrained and parallel computing environments.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 06:14:20 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Quantiota', 'Bouarfa Mahi', '']]","extracted_entities":"[{'text': 'gradient-based learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"gradient-based learning","similarity_score":0.5312626958}
{"id":2503.13987,"submitter":"Lichao Mou","authors":"Yaxiong Chen, Yujie Wang, Zixuan Zheng, Jingliang Hu, Yilei Shi,\n  Shengwu Xiong, Xiao Xiang Zhu, Lichao Mou","title":"Striving for Simplicity: Simple Yet Effective Prior-Aware\n  Pseudo-Labeling for Semi-Supervised Ultrasound Image Segmentation","comments":"MICCAI 2024","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Medical ultrasound imaging is ubiquitous, but manual analysis struggles to\nkeep pace. Automated segmentation can help but requires large labeled datasets,\nwhich are scarce. Semi-supervised learning leveraging both unlabeled and\nlimited labeled data is a promising approach. State-of-the-art methods use\nconsistency regularization or pseudo-labeling but grow increasingly complex.\nWithout sufficient labels, these models often latch onto artifacts or allow\nanatomically implausible segmentations. In this paper, we present a simple yet\neffective pseudo-labeling method with an adversarially learned shape prior to\nregularize segmentations. Specifically, we devise an encoder-twin-decoder\nnetwork where the shape prior acts as an implicit shape model, penalizing\nanatomically implausible but not ground-truth-deviating predictions. Without\nbells and whistles, our simple approach achieves state-of-the-art performance\non two benchmarks under different partition protocols. We provide a strong\nbaseline for future semi-supervised medical image segmentation. Code is\navailable at https:\/\/github.com\/WUTCM-Lab\/Shape-Prior-Semi-Seg.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 07:44:09 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Chen', 'Yaxiong', ''], ['Wang', 'Yujie', ''], ['Zheng', 'Zixuan', ''], ['Hu', 'Jingliang', ''], ['Shi', 'Yilei', ''], ['Xiong', 'Shengwu', ''], ['Zhu', 'Xiao Xiang', ''], ['Mou', 'Lichao', '']]","extracted_entities":"[{'text': 'Semi-supervised learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"Semi-supervised learning","similarity_score":0.5018399954}
{"id":2503.14013,"submitter":"Pengcheng Zhou","authors":"Pengcheng Zhou, Lantian Zhang, Wei Li","title":"Boosting Semi-Supervised Medical Image Segmentation via Masked Image\n  Consistency and Discrepancy Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Semi-supervised learning is of great significance in medical image\nsegmentation by exploiting unlabeled data. Among its strategies, the\nco-training framework is prominent. However, previous co-training studies\npredominantly concentrate on network initialization variances and pseudo-label\ngeneration, while overlooking the equilibrium between information interchange\nand model diversity preservation. In this paper, we propose the Masked Image\nConsistency and Discrepancy Learning (MICD) framework with three key modules.\nThe Masked Cross Pseudo Consistency (MCPC) module enriches context perception\nand small sample learning via pseudo-labeling across masked-input branches. The\nCross Feature Consistency (CFC) module fortifies information exchange and model\nrobustness by ensuring decoder feature consistency. The Cross Model Discrepancy\n(CMD) module utilizes EMA teacher networks to oversee outputs and preserve\nbranch diversity. Together, these modules address existing limitations by\nfocusing on fine-grained local information and maintaining diversity in a\nheterogeneous framework. Experiments on two public medical image datasets, AMOS\nand Synapse, demonstrate that our approach outperforms state-of-the-art\nmethods.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 08:20:35 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Zhou', 'Pengcheng', ''], ['Zhang', 'Lantian', ''], ['Li', 'Wei', '']]","extracted_entities":"[{'text': 'Semi-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'small sample learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"small sample learning","similarity_score":0.6798064709}
{"id":2503.14911,"submitter":"Siyuan Yan","authors":"Siyuan Yan, Ming Hu, Yiwen Jiang, Xieji Li, Hao Fei, Philipp Tschandl,\n  Harald Kittler, Zongyuan Ge","title":"Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical\n  Ontology Knowledge for Dermatology","comments":"23 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The emergence of vision-language models has transformed medical AI, enabling\nunprecedented advances in diagnostic capability and clinical applications.\nHowever, progress in dermatology has lagged behind other medical domains due to\nthe lack of standard image-text pairs. Existing dermatological datasets are\nlimited in both scale and depth, offering only single-label annotations across\na narrow range of diseases instead of rich textual descriptions, and lacking\nthe crucial clinical context needed for real-world applications. To address\nthese limitations, we present Derm1M, the first large-scale vision-language\ndataset for dermatology, comprising 1,029,761 image-text pairs. Built from\ndiverse educational resources and structured around a standard ontology\ncollaboratively developed by experts, Derm1M provides comprehensive coverage\nfor over 390 skin conditions across four hierarchical levels and 130 clinical\nconcepts with rich contextual information such as medical history, symptoms,\nand skin tone. To demonstrate Derm1M potential in advancing both AI research\nand clinical application, we pretrained a series of CLIP-like models,\ncollectively called DermLIP, on this dataset. The DermLIP family significantly\noutperforms state-of-the-art foundation models on eight diverse datasets across\nmultiple tasks, including zero-shot skin disease classification, clinical and\nartifacts concept identification, few-shot\/full-shot learning, and cross-modal\nretrieval. Our dataset and code will be public.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 05:30:01 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Yan', 'Siyuan', ''], ['Hu', 'Ming', ''], ['Jiang', 'Yiwen', ''], ['Li', 'Xieji', ''], ['Fei', 'Hao', ''], ['Tschandl', 'Philipp', ''], ['Kittler', 'Harald', ''], ['Ge', 'Zongyuan', '']]","extracted_entities":"[{'text': 'Derm1M', 'label': 'Large Language Model'}, {'text': 'Derm1M', 'label': 'Large Language Model'}, {'text': 'Derm1M', 'label': 'Large Language Model'}, {'text': 'DermLIP', 'label': 'Large Language Model'}, {'text': 'state-of-the-art foundation models', 'label': 'Foundation Model'}, {'text': 'zero-shot skin disease classification', 'label': 'Zero-shot Learning'}, {'text': 'few-shot\/full-shot learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"few-shot\/full-shot learning","similarity_score":0.9264668226}
{"id":2503.1506,"submitter":"Imanol G. Estepa","authors":"Imanol G. Estepa, Jes\\'us M. Rodr\\'iguez-de-Vera, Ignacio Saras\\'ua,\n  Bhalaji Nagarajan, Petia Radeva","title":"Conjuring Positive Pairs for Efficient Unification of Representation\n  Learning and Image Synthesis","comments":"The source code is available in https:\/\/github.com\/ImaGonEs\/Sorcen","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  While representation learning and generative modeling seek to understand\nvisual data, unifying both domains remains unexplored. Recent Unified\nSelf-Supervised Learning (SSL) methods have started to bridge the gap between\nboth paradigms. However, they rely solely on semantic token reconstruction,\nwhich requires an external tokenizer during training -- introducing a\nsignificant overhead. In this work, we introduce Sorcen, a novel unified SSL\nframework, incorporating a synergic Contrastive-Reconstruction objective. Our\nContrastive objective, \"Echo Contrast\", leverages the generative capabilities\nof Sorcen, eliminating the need for additional image crops or augmentations\nduring training. Sorcen \"generates\" an echo sample in the semantic token space,\nforming the contrastive positive pair. Sorcen operates exclusively on\nprecomputed tokens, eliminating the need for an online token transformation\nduring training, thereby significantly reducing computational overhead.\nExtensive experiments on ImageNet-1k demonstrate that Sorcen outperforms the\nprevious Unified SSL SoTA by 0.4%, 1.48 FID, 1.76%, and 1.53% on linear\nprobing, unconditional image generation, few-shot learning, and transfer\nlearning, respectively, while being 60.8% more efficient. Additionally, Sorcen\nsurpasses previous single-crop MIM SoTA in linear probing and achieves SoTA\nperformance in unconditional image generation, highlighting significant\nimprovements and breakthroughs in Unified SSL models.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 09:53:11 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 15:09:59 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Estepa', 'Imanol G.', ''], ['Rodr\u00edguez-de-Vera', 'Jes\u00fas M.', ''], ['Saras\u00faa', 'Ignacio', ''], ['Nagarajan', 'Bhalaji', ''], ['Radeva', 'Petia', '']]","extracted_entities":"[{'text': 'unconditional image generation', 'label': 'Zero-shot Learning'}, {'text': 'few-shot learning', 'label': 'Few-shot Learning'}, {'text': 'unconditional image generation', 'label': 'Zero-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"few-shot learning","similarity_score":1.0}
{"id":2503.15168,"submitter":"Javier Del Ser Dr.","authors":"Javier Del Ser, Jesus L. Lobo, Heimo M\\\"uller, Andreas Holzinger","title":"World Models in Artificial Intelligence: Sensing, Learning, and\n  Reasoning Like a Child","comments":"11 pages, 1 figure","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CV cs.ET cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  World Models help Artificial Intelligence (AI) predict outcomes, reason about\nits environment, and guide decision-making. While widely used in reinforcement\nlearning, they lack the structured, adaptive representations that even young\nchildren intuitively develop. Advancing beyond pattern recognition requires\ndynamic, interpretable frameworks inspired by Piaget's cognitive development\ntheory. We highlight six key research areas -- physics-informed learning,\nneurosymbolic learning, continual learning, causal inference, human-in-the-loop\nAI, and responsible AI -- as essential for enabling true reasoning in AI. By\nintegrating statistical learning with advances in these areas, AI can evolve\nfrom pattern recognition to genuine understanding, adaptation and reasoning\ncapabilities.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 12:50:40 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Del Ser', 'Javier', ''], ['Lobo', 'Jesus L.', ''], ['M\u00fcller', 'Heimo', ''], ['Holzinger', 'Andreas', '']]","extracted_entities":"[{'text': 'reinforcement\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'physics-informed learning', 'label': 'Few-shot Learning'}, {'text': 'neurosymbolic learning', 'label': 'Few-shot Learning'}, {'text': 'continual learning', 'label': 'Zero-shot Learning'}, {'text': 'statistical learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"statistical learning","similarity_score":0.5051944256}
{"id":2503.15367,"submitter":"Jacopo Talpini","authors":"Jacopo Talpini and Marco Savi and Giovanni Neglia","title":"FedBEns: One-Shot Federated Learning based on Bayesian Ensemble","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  One-Shot Federated Learning (FL) is a recent paradigm that enables multiple\nclients to cooperatively learn a global model in a single round of\ncommunication with a central server. In this paper, we analyze the One-Shot FL\nproblem through the lens of Bayesian inference and propose FedBEns, an\nalgorithm that leverages the inherent multimodality of local loss functions to\nfind better global models. Our algorithm leverages a mixture of Laplace\napproximations for the clients' local posteriors, which the server then\naggregates to infer the global model. We conduct extensive experiments on\nvarious datasets, demonstrating that the proposed method outperforms competing\nbaselines that typically rely on unimodal approximations of the local losses.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:05:52 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Talpini', 'Jacopo', ''], ['Savi', 'Marco', ''], ['Neglia', 'Giovanni', '']]","extracted_entities":"[{'text': 'One-Shot Federated Learning', 'label': 'Few-shot Learning'}, {'text': 'One-Shot FL', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"One-Shot Federated Learning","similarity_score":0.6215429902}
{"id":2503.15415,"submitter":"Giovanni Floreale Mr","authors":"Giovanni Floreale, Piero Baraldi, Enrico Zio, Olga Fink","title":"Automated Processing of eXplainable Artificial Intelligence Outputs in\n  Deep Learning Models for Fault Diagnostics of Large Infrastructures","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Deep Learning (DL) models processing images to recognize the health state of\nlarge infrastructure components can exhibit biases and rely on non-causal\nshortcuts. eXplainable Artificial Intelligence (XAI) can address these issues\nbut manually analyzing explanations generated by XAI techniques is\ntime-consuming and prone to errors. This work proposes a novel framework that\ncombines post-hoc explanations with semi-supervised learning to automatically\nidentify anomalous explanations that deviate from those of correctly classified\nimages and may therefore indicate model abnormal behaviors. This significantly\nreduces the workload for maintenance decision-makers, who only need to manually\nreclassify images flagged as having anomalous explanations. The proposed\nframework is applied to drone-collected images of insulator shells for power\ngrid infrastructure monitoring, considering two different Convolutional Neural\nNetworks (CNNs), GradCAM explanations and Deep Semi-Supervised Anomaly\nDetection. The average classification accuracy on two faulty classes is\nimproved by 8% and maintenance operators are required to manually reclassify\nonly 15% of the images. We compare the proposed framework with a\nstate-of-the-art approach based on the faithfulness metric: the experimental\nresults obtained demonstrate that the proposed framework consistently achieves\nF_1 scores larger than those of the faithfulness-based approach. Additionally,\nthe proposed framework successfully identifies correct classifications that\nresult from non-causal shortcuts, such as the presence of ID tags printed on\ninsulator shells.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:57:00 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Floreale', 'Giovanni', ''], ['Baraldi', 'Piero', ''], ['Zio', 'Enrico', ''], ['Fink', 'Olga', '']]","extracted_entities":"[{'text': 'semi-supervised learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"semi-supervised learning","similarity_score":0.5018399954}
{"id":2503.15679,"submitter":"Rahul Sundar","authors":"Rahul Sundar, Didier Lucor, and Sunetra Sarkar","title":"Sequential learning based PINNs to overcome temporal domain complexities\n  in unsteady flow past flapping wings","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.flu-dyn cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  For a data-driven and physics combined modelling of unsteady flow systems\nwith moving immersed boundaries, Sundar {\\it et al.} introduced an immersed\nboundary-aware (IBA) framework, combining Physics-Informed Neural Networks\n(PINNs) and the immersed boundary method (IBM). This approach was beneficial\nbecause it avoided case-specific transformations to a body-attached reference\nframe. Building on this, we now address the challenges of long time integration\nin velocity reconstruction and pressure recovery by extending this IBA\nframework with sequential learning strategies. Key difficulties for PINNs in\nlong time integration include temporal sparsity, long temporal domains and rich\nspectral content. To tackle these, a moving boundary-enabled PINN is developed,\nproposing two sequential learning strategies: - a time marching with gradual\nincrease in time domain size, however, this approach struggles with error\naccumulation over long time domains; and - a time decomposition which divides\nthe temporal domain into smaller segments, combined with transfer learning it\neffectively reduces error propagation and computational complexity. The key\nfindings for modelling of incompressible unsteady flows past a flapping airfoil\ninclude: - for quasi-periodic flows, the time decomposition approach with\npreferential spatio-temporal sampling improves accuracy and efficiency for\npressure recovery and aerodynamic load reconstruction, and, - for long time\ndomains, decomposing it into smaller temporal segments and employing multiple\nsub-networks, simplifies the problem ensuring stability and reduced network\nsizes. This study highlights the limitations of traditional PINNs for long time\nintegration of flow-structure interaction problems and demonstrates the\nbenefits of decomposition-based strategies for addressing error accumulation,\ncomputational cost, and complex dynamics.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 20:20:50 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Sundar', 'Rahul', ''], ['Lucor', 'Didier', ''], ['Sarkar', 'Sunetra', '']]","extracted_entities":"[{'text': 'transfer learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"transfer learning","similarity_score":0.5694054365}
{"id":2503.15722,"submitter":"Sin-Yu Huang","authors":"Sin-Yu Huang, Renjie Liao, and Vincent W.S. Wong","title":"Leveraging MoE-based Large Language Model for Zero-Shot Multi-Task\n  Semantic Communication","comments":"Accepted by ICC 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multi-task semantic communication (SC) can reduce the computational resources\nin wireless systems since retraining is not required when switching between\ntasks. However, existing approaches typically rely on task-specific embeddings\nto identify the intended task, necessitating retraining the entire model when\ngiven a new task. Consequently, this drives the need for a multi-task SC system\nthat can handle new tasks without additional training, known as zero-shot\nlearning. Inspired by the superior zero-shot capabilities of large language\nmodels (LLMs), we leverage pre-trained instruction-tuned LLMs, referred to as\nfine-tuned language net (FLAN), to improve the generalization capability. We\nincorporate a mixture-of-experts (MoE) architecture in the FLAN model and\npropose MoE-FLAN-SC architecture for multi-task SC systems. Our proposed\nMoE-FLAN-SC architecture can further improve the performance of FLAN-T5 model\nwithout increasing the computational cost. Moreover, we design a multi-task\nfeature extraction module (FEM) which can adaptively extract relevant features\nacross various tasks given the provided features and signal-to-noise ratio\n(SNR). Simulation results show that our proposed MoE-FLAN-SC architecture\noutperforms three state-of-the-art models in terms of the average accuracy on\nfour different unseen tasks.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 22:28:43 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Huang', 'Sin-Yu', ''], ['Liao', 'Renjie', ''], ['Wong', 'Vincent W. S.', '']]","extracted_entities":"[{'text': 'task-specific embeddings', 'label': 'Embedding'}, {'text': 'zero-shot\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Few-shot Learning","matched_keyword":"zero-shot\nlearning","similarity_score":0.8116950989}
{"id":2503.15731,"submitter":"Kun Zhan","authors":"Yuqing Zhang, Qi Han, Ligeng Wang, Kai Cheng, Bo Wang, Kun Zhan","title":"Graph-Weighted Contrastive Learning for Semi-Supervised Hyperspectral\n  Image Classification","comments":"Journal of Electronic Imaging, 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Most existing graph-based semi-supervised hyperspectral image classification\nmethods rely on superpixel partitioning techniques. However, they suffer from\nmisclassification of certain pixels due to inaccuracies in superpixel\nboundaries, \\ie, the initial inaccuracies in superpixel partitioning limit\noverall classification performance. In this paper, we propose a novel\ngraph-weighted contrastive learning approach that avoids the use of superpixel\npartitioning and directly employs neural networks to learn hyperspectral image\nrepresentation. Furthermore, while many approaches require all graph nodes to\nbe available during training, our approach supports mini-batch training by\nprocessing only a subset of nodes at a time, reducing computational complexity\nand improving generalization to unseen nodes. Experimental results on three\nwidely-used datasets demonstrate the effectiveness of the proposed approach\ncompared to baselines relying on superpixel partitioning.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 22:55:52 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zhang', 'Yuqing', ''], ['Han', 'Qi', ''], ['Wang', 'Ligeng', ''], ['Cheng', 'Kai', ''], ['Wang', 'Bo', ''], ['Zhan', 'Kun', '']]","extracted_entities":"[{'text': 'mini-batch training', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"mini-batch training","similarity_score":0.52123034}
{"id":2503.15819,"submitter":"Junyi Shen","authors":"Junyi Shen, Tetsuro Miyazaki, Kenji Kawashima","title":"Control Pneumatic Soft Bending Actuator with Online Learning Pneumatic\n  Physical Reservoir Computing","comments":"8 pages, 13 figures, IEEE-RAS International Conference on Soft\n  Robotics (RoboSoft 2025)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.LG cs.SY eess.SY","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The intrinsic nonlinearities of soft robots present significant control but\nsimultaneously provide them with rich computational potential. Reservoir\ncomputing (RC) has shown effectiveness in online learning systems for\ncontrolling nonlinear systems such as soft actuators. Conventional RC can be\nextended into physical reservoir computing (PRC) by leveraging the nonlinear\ndynamics of soft actuators for computation. This paper introduces a PRC-based\nonline learning framework to control the motion of a pneumatic soft bending\nactuator, utilizing another pneumatic soft actuator as the PRC model. Unlike\nconventional designs requiring two RC models, the proposed control system\nemploys a more compact architecture with a single RC model. Additionally, the\nframework enables zero-shot online learning, addressing limitations of previous\nPRC-based control systems reliant on offline training. Simulations and\nexperiments validated the performance of the proposed system. Experimental\nresults indicate that the PRC model achieved superior control performance\ncompared to a linear model, reducing the root-mean-square error (RMSE) by an\naverage of over 37% in bending motion control tasks. The proposed PRC-based\nonline learning control framework provides a novel approach for harnessing\nphysical systems' inherent nonlinearities to enhance the control of soft\nactuators.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 03:09:46 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Shen', 'Junyi', ''], ['Miyazaki', 'Tetsuro', ''], ['Kawashima', 'Kenji', '']]","extracted_entities":"[{'text': 'zero-shot online learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"zero-shot online learning","similarity_score":0.71476686}
{"id":2503.15877,"submitter":"Tiange Xiang","authors":"Tiange Xiang, Kai Li, Chengjiang Long, Christian H\\\"ane, Peihong Guo,\n  Scott Delp, Ehsan Adeli, Li Fei-Fei","title":"Repurposing 2D Diffusion Models with Gaussian Atlas for 3D Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent advances in text-to-image diffusion models have been driven by the\nincreasing availability of paired 2D data. However, the development of 3D\ndiffusion models has been hindered by the scarcity of high-quality 3D data,\nresulting in less competitive performance compared to their 2D counterparts. To\naddress this challenge, we propose repurposing pre-trained 2D diffusion models\nfor 3D object generation. We introduce Gaussian Atlas, a novel representation\nthat utilizes dense 2D grids, enabling the fine-tuning of 2D diffusion models\nto generate 3D Gaussians. Our approach demonstrates successful transfer\nlearning from a pre-trained 2D diffusion model to a 2D manifold flattened from\n3D structures. To support model training, we compile GaussianVerse, a\nlarge-scale dataset comprising 205K high-quality 3D Gaussian fittings of\nvarious 3D objects. Our experimental results show that text-to-image diffusion\nmodels can be effectively adapted for 3D content generation, bridging the gap\nbetween 2D and 3D modeling.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 05:59:41 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Xiang', 'Tiange', ''], ['Li', 'Kai', ''], ['Long', 'Chengjiang', ''], ['H\u00e4ne', 'Christian', ''], ['Guo', 'Peihong', ''], ['Delp', 'Scott', ''], ['Adeli', 'Ehsan', ''], ['Fei-Fei', 'Li', '']]","extracted_entities":"[{'text': 'transfer\\nlearning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"transfer\nlearning","similarity_score":0.5694054365}
{"id":2503.16025,"submitter":"Yair Shpitzer","authors":"Yair Shpitzer, Gal Chechik, Idan Schwartz","title":"Single Image Iterative Subject-driven Generation and Editing","comments":"Project page is at https:\/\/siso-paper.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Personalizing image generation and editing is particularly challenging when\nwe only have a few images of the subject, or even a single image. A common\napproach to personalization is concept learning, which can integrate the\nsubject into existing models relatively quickly, but produces images whose\nquality tends to deteriorate quickly when the number of subject images is\nsmall. Quality can be improved by pre-training an encoder, but training\nrestricts generation to the training distribution, and is time consuming. It is\nstill an open hard challenge to personalize image generation and editing from a\nsingle image without training. Here, we present SISO, a novel, training-free\napproach based on optimizing a similarity score with an input subject image.\nMore specifically, SISO iteratively generates images and optimizes the model\nbased on loss of similarity with the given subject image until a satisfactory\nlevel of similarity is achieved, allowing plug-and-play optimization to any\nimage generator. We evaluated SISO in two tasks, image editing and image\ngeneration, using a diverse data set of personal subjects, and demonstrate\nsignificant improvements over existing methods in image quality, subject\nfidelity, and background preservation.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 10:45:04 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Shpitzer', 'Yair', ''], ['Chechik', 'Gal', ''], ['Schwartz', 'Idan', '']]","extracted_entities":"[{'text': 'concept learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Few-shot Learning","matched_keyword":"concept learning","similarity_score":0.5206945539}
{"id":2503.16106,"submitter":"Mohamad Hassan N C","authors":"Mohamad Hassan N C, Divyam Gupta, Mainak Singha, Sai Bhargav Rongali,\n  Ankit Jha, Muhammad Haris Khan, Biplab Banerjee","title":"OSLoPrompt: Bridging Low-Supervision Challenges and Open-Set Domain\n  Generalization in CLIP","comments":"Accepted to CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce Low-Shot Open-Set Domain Generalization (LSOSDG), a novel\nparadigm unifying low-shot learning with open-set domain generalization (ODG).\nWhile prompt-based methods using models like CLIP have advanced DG, they falter\nin low-data regimes (e.g., 1-shot) and lack precision in detecting open-set\nsamples with fine-grained semantics related to training classes. To address\nthese challenges, we propose OSLOPROMPT, an advanced prompt-learning framework\nfor CLIP with two core innovations. First, to manage limited supervision across\nsource domains and improve DG, we introduce a domain-agnostic prompt-learning\nmechanism that integrates adaptable domain-specific cues and visually guided\nsemantic attributes through a novel cross-attention module, besides being\nsupported by learnable domain- and class-generic visual prompts to enhance\ncross-modal adaptability. Second, to improve outlier rejection during\ninference, we classify unfamiliar samples as \"unknown\" and train specialized\nprompts with systematically synthesized pseudo-open samples that maintain\nfine-grained relationships to known classes, generated through a targeted query\nstrategy with off-the-shelf foundation models. This strategy enhances feature\nlearning, enabling our model to detect open samples with varied granularity\nmore effectively. Extensive evaluations across five benchmarks demonstrate that\nOSLOPROMPT establishes a new state-of-the-art in LSOSDG, significantly\noutperforming existing methods.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 12:51:19 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['C', 'Mohamad Hassan N', ''], ['Gupta', 'Divyam', ''], ['Singha', 'Mainak', ''], ['Rongali', 'Sai Bhargav', ''], ['Jha', 'Ankit', ''], ['Khan', 'Muhammad Haris', ''], ['Banerjee', 'Biplab', '']]","extracted_entities":"[{'text': 'low-shot learning', 'label': 'Few-shot Learning'}, {'text': 'visual prompts', 'label': 'Prompting'}, {'text': 'specialized\\nprompts', 'label': 'Prompting'}, {'text': 'off-the-shelf foundation models', 'label': 'Foundation Model'}]","assigned_concept":"Few-shot Learning","matched_keyword":"low-shot learning","similarity_score":0.8719492555}
{"id":2312.00846,"submitter":"Hanlin Chen","authors":"Hanlin Chen, Chen Li, Yunsong Wang, Gim Hee Lee","title":"NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting\n  Guidance","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Existing neural implicit surface reconstruction methods have achieved\nimpressive performance in multi-view 3D reconstruction by leveraging explicit\ngeometry priors such as depth maps or point clouds as regularization. However,\nthe reconstruction results still lack fine details because of the over-smoothed\ndepth map or sparse point cloud. In this work, we propose a neural implicit\nsurface reconstruction pipeline with guidance from 3D Gaussian Splatting to\nrecover highly detailed surfaces. The advantage of 3D Gaussian Splatting is\nthat it can generate dense point clouds with detailed structure. Nonetheless, a\nnaive adoption of 3D Gaussian Splatting can fail since the generated points are\nthe centers of 3D Gaussians that do not necessarily lie on the surface. We thus\nintroduce a scale regularizer to pull the centers close to the surface by\nenforcing the 3D Gaussians to be extremely thin. Moreover, we propose to refine\nthe point cloud from 3D Gaussians Splatting with the normal priors from the\nsurface predicted by neural implicit models instead of using a fixed set of\npoints as guidance. Consequently, the quality of surface reconstruction\nimproves from the guidance of the more accurate 3D Gaussian splatting. By\njointly optimizing the 3D Gaussian Splatting and the neural implicit model, our\napproach benefits from both representations and generates complete surfaces\nwith intricate details. Experiments on Tanks and Temples verify the\neffectiveness of our proposed method.\n","versions":"[{'version': 'v1', 'created': 'Fri, 1 Dec 2023 07:04:47 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 09:09:49 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Chen', 'Hanlin', ''], ['Li', 'Chen', ''], ['Wang', 'Yunsong', ''], ['Lee', 'Gim Hee', '']]","extracted_entities":"[{'text': 'neural implicit model', 'label': 'Neural Language Model'}]","assigned_concept":"Neural Language Model","matched_keyword":"neural implicit model","similarity_score":0.5472807884}
{"id":2503.12478,"submitter":"Zhiyu Liang","authors":"Zhiyu Liang, Dongrui Cai, Chenyuan Zhang, Zheng Liang, Chen Liang, Bo\n  Zheng, Shi Qiu, Jin Wang, Hongzhi Wang","title":"KDSelector: A Knowledge-Enhanced and Data-Efficient Model Selector\n  Learning Framework for Time Series Anomaly Detection","comments":"This paper has been accepted by SIGMOD 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.DB","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Model selection has been raised as an essential problem in the area of time\nseries anomaly detection (TSAD), because there is no single best TSAD model for\nthe highly heterogeneous time series in real-world applications. However,\ndespite the success of existing model selection solutions that train a\nclassification model (especially neural network, NN) using historical data as a\nselector to predict the correct TSAD model for each series, the NN-based\nselector learning methods used by existing solutions do not make full use of\nthe knowledge in the historical data and require iterating over all training\nsamples, which limits the accuracy and training speed of the selector. To\naddress these limitations, we propose KDSelector, a novel knowledge-enhanced\nand data-efficient framework for learning the NN-based TSAD model selector, of\nwhich three key components are specifically designed to integrate available\nknowledge into the selector and dynamically prune less important and redundant\nsamples during the learning. We develop a TSAD model selection system with\nKDSelector as the internal, to demonstrate how users improve the accuracy and\ntraining speed of their selectors by using KDSelector as a plug-and-play\nmodule. Our demonstration video is hosted at https:\/\/youtu.be\/2uqupDWvTF0.\n","versions":"[{'version': 'v1', 'created': 'Sun, 16 Mar 2025 12:13:19 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 03:06:28 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Liang', 'Zhiyu', ''], ['Cai', 'Dongrui', ''], ['Zhang', 'Chenyuan', ''], ['Liang', 'Zheng', ''], ['Liang', 'Chen', ''], ['Zheng', 'Bo', ''], ['Qiu', 'Shi', ''], ['Wang', 'Jin', ''], ['Wang', 'Hongzhi', '']]","extracted_entities":"[{'text': 'neural network', 'label': 'Neural Language Model'}, {'text': 'NN', 'label': 'Neural Language Model'}]","assigned_concept":"Neural Language Model","matched_keyword":"neural network","similarity_score":0.5788917542}
{"id":2503.12717,"submitter":"Peimeng Yin","authors":"Jiaxiong Hao, Yunqing Huang, Nianyu Yi, Peimeng Yin","title":"Neural network-enhanced $hr$-adaptive finite element algorithm for\n  parabolic equations","comments":"21 pages, 16 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.NA","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this paper, we present a novel enhancement to the conventional\n$hr$-adaptive finite element methods for parabolic equations, integrating\ntraditional $h$-adaptive and $r$-adaptive methods via neural networks. A major\nchallenge in $hr$-adaptive finite element methods lies in projecting the\nprevious step's finite element solution onto the updated mesh. This projection\ndepends on the new mesh and must be recomputed for each adaptive iteration. To\naddress this, we introduce a neural network to construct a mesh-free surrogate\nof the previous step finite element solution. Since the neural network is\nmesh-free, it only requires training once per time step, with its parameters\ninitialized using the optimizer from the previous time step. This approach\neffectively overcomes the interpolation challenges associated with non-nested\nmeshes in computation, making node insertion and movement more convenient and\nefficient. The new algorithm also emphasizes SIZING and GENERATE, allowing each\nrefinement to roughly double the number of mesh nodes of the previous iteration\nand then redistribute them to form a new mesh that effectively captures the\nsingularities. It significantly reduces the time required for repeated\nrefinement and achieves the desired accuracy in no more than seven\nspace-adaptive iterations per time step. Numerical experiments confirm the\nefficiency of the proposed algorithm in capturing dynamic changes of\nsingularities.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 01:07:37 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Hao', 'Jiaxiong', ''], ['Huang', 'Yunqing', ''], ['Yi', 'Nianyu', ''], ['Yin', 'Peimeng', '']]","extracted_entities":"[{'text': 'neural networks', 'label': 'Neural Language Model'}, {'text': 'neural network', 'label': 'Neural Language Model'}, {'text': 'neural network', 'label': 'Neural Language Model'}]","assigned_concept":"Neural Language Model","matched_keyword":"neural network","similarity_score":0.5788917542}
{"id":2503.14193,"submitter":"Miguel Icaza-Lizaola Dr.","authors":"M. Icaza-Lizaola, E. L. Sirks, Yong-Seon Song, Peder Norberg and Feng\n  Shi","title":"Populating Large N-body Simulations with LRGs Using Neural Networks","comments":"20 pages, 20 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.CO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The analysis of state-of-the-art cosmological surveys like the Dark Energy\nSpectroscopic Instrument (DESI) survey requires high-resolution, large-volume\nsimulations. However, the computational cost of hydrodynamical simulations at\nthese scales is prohibitive. Instead, dark matter (DM)-only simulations are\nused, with galaxies populated a posteriori, typically via halo occupation\ndistribution (HOD) models. While effective, HOD models are statistical in\nnature and lack full physical motivation.\n  In this work, we explore using neural networks (NNs) to learn the complex,\nphysically motivated relationships between DM haloes and galaxy properties.\nTrained on small-volume, high-resolution hydrodynamical simulations, our NN\npredicts galaxy properties in a larger DM-only simulation and determines which\ngalaxies should be classified as luminous red galaxies (LRGs).\n  Comparing the original LRG sample to the one generated by our NN, we find\nthat, while the subhalo mass distributions are similar, our NN selects fewer\nlow-mass subhaloes as LRG hosts, possibly due to the absence of baryonic\nfeedback effects in DM-only simulations. This feedback could brighten or redden\ngalaxies, altering their classification.\n  Finally, we generate a new LRG sample by fitting an HOD model to the\nNN-generated LRG sample. We verify that both the HOD- and NN-generated samples\npreserve a set of bias parameter relations, which assume that the higher-order\nparameters, $b_{s2}$ and $b_{3\\rm{nl}}$, are determined by the linear bias\nparameter $b_{1}$. These relations are commonly used to simplify clustering\nanalyses.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 12:12:15 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Icaza-Lizaola', 'M.', ''], ['Sirks', 'E. L.', ''], ['Song', 'Yong-Seon', ''], ['Norberg', 'Peder', ''], ['Shi', 'Feng', '']]","extracted_entities":"[{'text': 'neural networks', 'label': 'Neural Language Model'}, {'text': 'NNs', 'label': 'Neural Language Model'}, {'text': 'NN', 'label': 'Neural Language Model'}, {'text': 'NN', 'label': 'Neural Language Model'}]","assigned_concept":"Neural Language Model","matched_keyword":"neural networks","similarity_score":0.5532894135}
{"id":2503.15362,"submitter":"Fangmin Lu","authors":"Fangmin Lu, Zheng Chen and Kun Wang","title":"Nonlinear Optimal Guidance for Impact Time Control with Field-of-View\n  Constraint","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  An optimal guidance law for impact time control with field-of-view constraint\nis presented. The guidance law is derived by first converting the\ninequality-constrained nonlinear optimal control problem into an\nequality-constrained one through a saturation function. Based on Pontryagin's\nmaximum principle, a parameterized system satisfying the necessary optimality\nconditions is established. By propagating this system, a large number of\nextremal trajectories can be efficiently generated. These trajectories are then\nused to train a neural network that maps the current state and time-to-go to\nthe optimal guidance command. The trained neural network can generate optimal\ncommands within 0.1 milliseconds while satisfying the field-of-view constraint.\nNumerical simulations demonstrate that the proposed guidance law outperforms\nexisting methods and achieves nearly optimal performance in terms of control\neffort.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:02:20 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Lu', 'Fangmin', ''], ['Chen', 'Zheng', ''], ['Wang', 'Kun', '']]","extracted_entities":"[{'text': 'neural network', 'label': 'Neural Language Model'}, {'text': 'neural network', 'label': 'Neural Language Model'}]","assigned_concept":"Neural Language Model","matched_keyword":"neural network","similarity_score":0.5788917542}
{"id":2503.16083,"submitter":"Kim William Torre","authors":"Kim William Torre and Joost de Graaf","title":"Hydrodynamic Interactions in Particle Suspensions: A Perspective on\n  Stokesian Dynamics","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.flu-dyn cond-mat.soft","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Stokesian Dynamics (SD) is a numerical framework used for simulating\nhydrodynamic interactions in particle suspensions at low Reynolds number. It\ncombines far-field approximations with near-field lubrication corrections,\noffering a balance between accuracy and efficiency. This work reviews SD and\nprovides a perspective on future directions for this approach. We outline the\nmathematical foundations, the method's strengths and weaknesses, and the\ncomputational challenges that need to be overcome to work with SD effectively.\nWe also discuss recent advancements that improve the algorithm's efficiency,\nincluding the use of iterative solvers and matrix-free approaches. In addition,\nwe highlight the limitations of making stronger, albeit more cost-effective\napproximations to studying hydrodynamic interactions in dense suspensions than\nmade in SD, such as the two-body Rotne-Prager-Yamakawa (RPY) approximation. To\novercome these issues, we propose a hybrid framework that replaces SD's full\nmany-body computations with a neural network trained on SD data. That is, we\ncorrect the RPY approximation, while avoiding costly matrix inversions. We\ndemonstrate the potential of this method on a simple system, where we find a\nclose match to SD data while algorithmically outperforming RPY. Our work\nprovides an outlook on the way in which large-scale simulations of particle\nsuspensions can be performed in the foreseeable future.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 12:26:48 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Torre', 'Kim William', ''], ['de Graaf', 'Joost', '']]","extracted_entities":"[{'text': 'neural network', 'label': 'Neural Language Model'}]","assigned_concept":"Neural Language Model","matched_keyword":"neural network","similarity_score":0.5788917542}
{"id":2503.16206,"submitter":"Oliver D\\\"urr","authors":"Beate Sick and Oliver D\\\"urr","title":"Interpretable Neural Causal Models with TRAM-DAGs","comments":"Accepted at the CLeaR 2025 Conference","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The ultimate goal of most scientific studies is to understand the underlying\ncausal mechanism between the involved variables. Structural causal models\n(SCMs) are widely used to represent such causal mechanisms. Given an SCM,\ncausal queries on all three levels of Pearl's causal hierarchy can be answered:\n$L_1$ observational, $L_2$ interventional, and $L_3$ counterfactual. An\nessential aspect of modeling the SCM is to model the dependency of each\nvariable on its causal parents. Traditionally this is done by parametric\nstatistical models, such as linear or logistic regression models. This allows\nto handle all kinds of data types and fit interpretable models but bears the\nrisk of introducing a bias. More recently neural causal models came up using\nneural networks (NNs) to model the causal relationships, allowing the\nestimation of nearly any underlying functional form without bias. However,\ncurrent neural causal models are generally restricted to continuous variables\nand do not yield an interpretable form of the causal relationships.\nTransformation models range from simple statistical regressions to complex\nnetworks and can handle continuous, ordinal, and binary data. Here, we propose\nto use TRAMs to model the functional relationships in SCMs allowing us to\nbridge the gap between interpretability and flexibility in causal modeling. We\ncall this method TRAM-DAG and assume currently that the underlying directed\nacyclic graph is known. For the fully observed case, we benchmark TRAM-DAGs\nagainst state-of-the-art statistical and NN-based causal models. We show that\nTRAM-DAGs are interpretable but also achieve equal or superior performance in\nqueries ranging from $L_1$ to $L_3$ in the causal hierarchy. For the continuous\ncase, TRAM-DAGs allow for counterfactual queries for three common causal\nstructures, including unobserved confounding.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 14:51:04 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Sick', 'Beate', ''], ['D\u00fcrr', 'Oliver', '']]","extracted_entities":"[{'text': 'Structural causal models', 'label': 'Neural Language Model'}, {'text': 'neural causal models', 'label': 'Neural Language Model'}, {'text': 'neural networks', 'label': 'Neural Language Model'}, {'text': 'neural causal models', 'label': 'Neural Language Model'}]","assigned_concept":"Neural Language Model","matched_keyword":"neural networks","similarity_score":0.5532894135}
{"id":2410.1675,"submitter":"Antoine Godichon-Baggioni","authors":"Sobihan Surendran (LPSM (UMR\\_8001)), Antoine Godichon-Baggioni (LPSM\n  (UMR\\_8001)), Sylvain Le Corff (LPSM (UMR\\_8001), SU)","title":"Theoretical Convergence Guarantees for Variational Autoencoders","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Variational Autoencoders (VAE) are popular generative models used to sample\nfrom complex data distributions. Despite their empirical success in various\nmachine learning tasks, significant gaps remain in understanding their\ntheoretical properties, particularly regarding convergence guarantees. This\npaper aims to bridge that gap by providing non-asymptotic convergence\nguarantees for VAE trained using both Stochastic Gradient Descent and Adam\nalgorithms.We derive a convergence rate of $\\mathcal{O}(\\log n \/ \\sqrt{n})$,\nwhere $n$ is the number of iterations of the optimization algorithm, with\nexplicit dependencies on the batch size, the number of variational samples, and\nother key hyperparameters. Our theoretical analysis applies to both Linear VAE\nand Deep Gaussian VAE, as well as several VAE variants, including $\\beta$-VAE\nand IWAE. Additionally, we empirically illustrate the impact of hyperparameters\non convergence, offering new insights into the theoretical understanding of VAE\ntraining.\n","versions":"[{'version': 'v1', 'created': 'Tue, 22 Oct 2024 07:12:38 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 08:37:58 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Surendran', 'Sobihan', '', 'LPSM'], ['Godichon-Baggioni', 'Antoine', '', 'LPSM'], ['Corff', 'Sylvain Le', '', 'LPSM']]","extracted_entities":"[{'text': 'Adam', 'label': 'ALBERT'}, {'text': 'Deep Gaussian VAE', 'label': 'AI model'}]","assigned_concept":"ALBERT","matched_keyword":"Adam","similarity_score":0.5003848076}
{"id":2503.12909,"submitter":"Jasem Hamoud Mohamad","authors":"Jasem Hamoud, Duaa Abdullah","title":"Topological Indices With Degree Sequence $\\mathscr{D}$ of Tree","comments":"16 pages, 1 table","journal-ref":null,"doi":null,"report-no":null,"categories":"math.CO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this paper, we refer to a asymptotic degree sequence as\n$\\mathscr{D}=(d_1,d_2,\\dots,d_n)$. The examination of topological indices on\ntrees gives us a general overview through bounds to find the maximum and\nminimum bounds which reflect the maximum and minimum number of edges incident\nto every vertex in the graph, Albertson index known as $\\sum_{uv\\in E(G)}\\lvert\nd_u(G)-d_v(G) \\rvert$, Sigma index $\\sigma(G)$ among $\\mathscr{D}$ of tree $T$\nwhen $d_n\\geqslant \\dots \\geqslant d_1$. According to the first zegrb we show\nfor a degree sequence of order $n=4$,\n$\\operatorname{irr}(T)=M_1(T)^2-2\\sqrt{M_1(T)}+\\sum_{i=1}^4\\left|x_i-x_{i+1}\\right|-(b+c)-1$.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 08:18:00 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Hamoud', 'Jasem', ''], ['Abdullah', 'Duaa', '']]","extracted_entities":"[{'text': 'Albertson', 'label': 'ALBERT'}]","assigned_concept":"ALBERT","matched_keyword":"Albertson","similarity_score":0.7461081743}
{"id":2503.13027,"submitter":"Xingyue Ma","authors":"Zhijie Liu, Xingyue Ma, Lan Chen, Xiaohong Yan, Jun-Ming Liu,\n  Chun-Gang Duan, Jorge \\'I\\~niguez-Gonz\\'alez, Di Wu, Yurong Yang","title":"Giant energy density nitride dielectrics enabled by a\n  paraelectric-metaparaelectric phase transition","comments":"18 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci physics.comp-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Electrostatic dielectric capacitors are foundational to advance the\nelectronics and electric power devices due to their ultrafast\ncharging\/discharging capability and high-power density. However, the low energy\ndensity limits the potential for next generation devices in terms of\nminiaturization and integration. We propose a strategy that relies on inducing\na field-driven phase transition that we denote paraelectric-metaparaelectric,\nwhich yields an ultrahigh energy density in III-nitrides. III-nitride compounds\n(Al, Sc, B)N with certain cation concentrations possess a nonpolar hexagonal\nground phase which could transform into a polar wurtzite phase under a very\nlarge electric field, which is denoted as metaparaelectric with nearly null\nhysteresis P-E loop. This paraelectric-metaparaelectric transition leads to a\npolarization saturation at large electric field. The corresponding P-E loop\ndisplays a giant energy density of 308 J\/cm$^3$ with high efficiency nearly\n100%. The proposed paraelectric-metaparaelectric phase transition strategy in\nnitrides opens an avenue to design of next generation high performance\ndielectrics.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 10:29:17 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Liu', 'Zhijie', ''], ['Ma', 'Xingyue', ''], ['Chen', 'Lan', ''], ['Yan', 'Xiaohong', ''], ['Liu', 'Jun-Ming', ''], ['Duan', 'Chun-Gang', ''], ['\u00cd\u00f1iguez-Gonz\u00e1lez', 'Jorge', ''], ['Wu', 'Di', ''], ['Yang', 'Yurong', '']]","extracted_entities":"[{'text': 'Al', 'label': 'ALBERT'}, {'text': 'Sc', 'label': 'ALBERT'}, {'text': 'B', 'label': 'ALBERT'}]","assigned_concept":"ALBERT","matched_keyword":"Al","similarity_score":0.5163906813}
{"id":2503.13235,"submitter":"Christopher Woodgate","authors":"Christopher D. Woodgate, Hubert J. Naguszewski, David Redka, J\\'an\n  Min\\'ar, David Quigley, Julie B. Staunton","title":"Emergent B2 chemical orderings in the AlTiVNb and AlTiCrMo refractory\n  high-entropy superalloys studied via first-principles theory and atomistic\n  modelling","comments":"18 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci physics.app-ph physics.comp-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We study the thermodynamics and phase stability of the AlTiVNb and AlTiCrMo\nrefractory high-entropy superalloys using a combination of \\textit{ab initio}\nelectronic structure theory -- namely a concentration wave analysis -- and\natomistic Monte Carlo simulations. Our multiscale approach is suitable both for\nexamining atomic short-range order in the solid solution, as well as for\nstudying the emergence of long-range crystallographic order with decreasing\ntemperature. In both alloys considered in this work, in alignment with\nexperimental observations, we predict a B2 (CsCl) chemical ordering emerging at\nhigh temperatures, which is driven primarily by Al and Ti, with other elements\nexpressing weaker site preferences. The predicted B2 ordering temperature for\nAlTiVNb is higher than that for AlTiCrMo. These chemical orderings are\ndiscussed in terms of the alloys' electronic structure, with hybridisation\nbetween the $sp$ states of Al and the $d$ states of the transition metals\nunderstood to play an important role. Within our modelling, the chemically\nordered B2 phases for both alloys have an increased predicted residual\nresistivity compared to the A2 (disordered bcc) phases. These increased\nresistivity values are understood to originate in a reduction in the electronic\ndensity of states at the Fermi level, in conjunction with qualitative changes\nto the alloys' smeared-out Fermi surfaces. These results highlight the close\nconnections between composition, structure, and physical properties in this\ntechnologically relevant class of materials.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 14:48:54 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Woodgate', 'Christopher D.', ''], ['Naguszewski', 'Hubert J.', ''], ['Redka', 'David', ''], ['Min\u00e1r', 'J\u00e1n', ''], ['Quigley', 'David', ''], ['Staunton', 'Julie B.', '']]","extracted_entities":"[{'text': 'AlTiVNb', 'label': 'ALBERT'}, {'text': 'AlTiCrMo', 'label': 'ALBERT'}, {'text': 'Al', 'label': 'ALBERT'}, {'text': 'AlTiVNb', 'label': 'ALBERT'}, {'text': 'AlTiCrMo', 'label': 'ALBERT'}]","assigned_concept":"ALBERT","matched_keyword":"Al","similarity_score":0.5163906813}
{"id":2503.13647,"submitter":"Michele Amoretti","authors":"Giacomo Belli, Marco Mordacci, Michele Amoretti","title":"SRBB-Based Quantum State Preparation","comments":"9 pages, 8 figures, 6 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this work, a scalable algorithm for the approximate quantum state\npreparation problem is proposed, facing a challenge of fundamental importance\nin many topic areas of quantum computing. The algorithm uses a variational\nquantum circuit based on the Standard Recursive Block Basis (SRBB), a\nhierarchical construction for the matrix algebra of the $SU(2^n)$ group, which\nis capable of linking the variational parameters with the topology of the Lie\ngroup. Compared to the full algebra, using only diagonal components reduces the\nnumber of CNOTs by an exponential factor, as well as the circuit depth, in full\nagreement with the relaxation principle, inherent to the approximation\nmethodology, of minimizing resources while achieving high accuracy. The desired\nquantum state is then approximated by a scalable quantum neural network, which\nis designed upon the diagonal SRBB sub-algebra. This approach provides a new\nscheme for approximate quantum state preparation in a variational framework and\na specific use case for the SRBB hierarchy. The performance of the algorithm is\nassessed with different loss functions, like fidelity, trace distance, and\nFrobenius norm, in relation to two optimizers: Adam and Nelder-Mead. The\nresults highlight the potential of SRBB in close connection with the geometry\nof unitary groups, achieving high accuracy up to 4 qubits in simulation, but\nalso its current limitations with an increasing number of qubits. Additionally,\nthe approximate SRBB-based QSP algorithm has been tested on real quantum\ndevices to assess its performance with a small number of qubits.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 18:51:07 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Belli', 'Giacomo', ''], ['Mordacci', 'Marco', ''], ['Amoretti', 'Michele', '']]","extracted_entities":"[{'text': 'Adam', 'label': 'ALBERT'}]","assigned_concept":"ALBERT","matched_keyword":"Adam","similarity_score":0.5003848076}
{"id":2503.14016,"submitter":"Minyu Feng","authors":"Yusheng Li, Yichao Yao, Minyu Feng, Tina P. Benko, Matja\\v{z} Perc and\n  Jernej Zavr\\v{s}nik","title":"Epidemic Dynamics in Homes and Destinations under Recurrent Mobility\n  Patterns","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.soc-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The structure of heterogeneous networks and human mobility patterns\nprofoundly influence the spreading of endemic diseases. In small-scale\ncommunities, individuals engage in social interactions within confined\nenvironments, such as homes and workplaces, where daily routines facilitate\nvirus transmission through predictable mobility pathways. Here, we introduce a\nmetapopulation model grounded in a Microscopic Markov Chain Approach to\nsimulate susceptible--infected--susceptible dynamics within structured\npopulations. There are two primary types of nodes, homes and destinations,\nwhere individuals interact and transmit infections through recurrent mobility\npatterns. We derive analytical expressions for the epidemic threshold and\nvalidate our theoretical findings through comparative simulations on\nWatts--Strogatz and Barab\\'asi--Albert networks. The experimental results\nreveal a nonlinear relationship between mobility probability and the epidemic\nthreshold, indicating that further increases can inhibit disease transmission\nbeyond a certain critical mobility level.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 08:23:50 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Li', 'Yusheng', ''], ['Yao', 'Yichao', ''], ['Feng', 'Minyu', ''], ['Benko', 'Tina P.', ''], ['Perc', 'Matja\u017e', ''], ['Zavr\u0161nik', 'Jernej', '']]","extracted_entities":"[{'text': 'Albert', 'label': 'ALBERT'}]","assigned_concept":"ALBERT","matched_keyword":"Albert","similarity_score":1.0}
{"id":2503.16315,"submitter":"Michael Potter","authors":"Michael Potter, Beyza Kalkanl{\\i}, Deniz Erdo\\u{g}mu\\c{s}, and Michael\n  Everett","title":"Active Learning For Repairable Hardware Systems With Partial Coverage","comments":"Submitted to IEEE Reliability and Maintainability Symposium - Europe\n  2025","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.AP cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Identifying the optimal diagnostic test and hardware system instance to infer\nreliability characteristics using field data is challenging, especially when\nconstrained by fixed budgets and minimal maintenance cycles. Active Learning\n(AL) has shown promise for parameter inference with limited data and budget\nconstraints in machine learning\/deep learning tasks. However, AL for\nreliability model parameter inference remains underexplored for repairable\nhardware systems. It requires specialized AL Acquisition Functions (AFs) that\nconsider hardware aging and the fact that a hardware system consists of\nmultiple sub-systems, which may undergo only partial testing during a given\ndiagnostic test. To address these challenges, we propose a relaxed Mixed\nInteger Semidefinite Program (MISDP) AL AF that incorporates Diagnostic\nCoverage (DC), Fisher Information Matrices (FIMs), and diagnostic testing\nbudgets. Furthermore, we design empirical-based simulation experiments focusing\non two diagnostic testing scenarios: (1) partial tests of a hardware system\nwith overlapping subsystem coverage, and (2) partial tests where one diagnostic\ntest fully subsumes the subsystem coverage of another. We evaluate our proposed\napproach against the most widely used AL AF in the literature (entropy), as\nwell as several intuitive AL AFs tailored for reliability model parameter\ninference. Our proposed AF ranked best on average among the alternative AFs\nacross 6,000 experimental configurations, with respect to Area Under the Curve\n(AUC) of the Absolute Total Expected Event Error (ATEER) and Mean Squared Error\n(MSE) curves, with statistical significance calculated at a 0.05 alpha level\nusing a Friedman hypothesis test.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 16:38:16 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Potter', 'Michael', ''], ['Kalkanl\u0131', 'Beyza', ''], ['Erdo\u011fmu\u015f', 'Deniz', ''], ['Everett', 'Michael', '']]","extracted_entities":"[{'text': 'Active Learning', 'label': 'Zero-shot Learning'}, {'text': 'AL', 'label': 'ALBERT'}, {'text': 'AL', 'label': 'ALBERT'}, {'text': 'AL', 'label': 'ALBERT'}, {'text': 'AL', 'label': 'ALBERT'}]","assigned_concept":"ALBERT","matched_keyword":"AL","similarity_score":0.5163906813}
{"id":2207.04053,"submitter":"Ruta Binkyte","authors":"Ruta Binkyte, Ljupcho Grozdanovski, Sami Zhioua","title":"On the Need and Applicability of Causality for Fairness: A Unified\n  Framework for AI Auditing and Legal Analysis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CY","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  As Artificial Intelligence (AI) increasingly influences decisions in critical\nsocietal sectors, understanding and establishing causality becomes essential\nfor evaluating the fairness of automated systems. This article explores the\nsignificance of causal reasoning in addressing algorithmic discrimination,\nemphasizing both legal and societal perspectives. By reviewing landmark cases\nand regulatory frameworks, particularly within the European Union, we\nillustrate the challenges inherent in proving causal claims when confronted\nwith opaque AI decision-making processes. The discussion outlines practical\nobstacles and methodological limitations in applying causal inference to\nreal-world fairness scenarios, proposing actionable solutions to enhance\ntransparency, accountability, and fairness in algorithm-driven decisions.\n","versions":"[{'version': 'v1', 'created': 'Fri, 8 Jul 2022 10:37:22 GMT'}, {'version': 'v2', 'created': 'Wed, 1 Nov 2023 12:31:08 GMT'}, {'version': 'v3', 'created': 'Wed, 15 Nov 2023 10:37:30 GMT'}, {'version': 'v4', 'created': 'Wed, 19 Mar 2025 13:15:56 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Binkyte', 'Ruta', ''], ['Grozdanovski', 'Ljupcho', ''], ['Zhioua', 'Sami', '']]","extracted_entities":"[{'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'regulatory frameworks', 'label': 'AI Ethics'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"fairness","similarity_score":0.6551788449}
{"id":2208.06648,"submitter":"Vincent Jeanselme","authors":"Vincent Jeanselme, Maria De-Arteaga, Zhe Zhang, Jessica Barrett and\n  Brian Tom","title":"Imputation Strategies Under Clinical Presence: Impact on Algorithmic\n  Fairness","comments":"Full Journal Version under review; Presented at the conference\n  Machine Learning for Health (ML4H) 2022 Published in the Proceedings of\n  Machine Learning Research (193)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Machine learning risks reinforcing biases present in data and, as we argue in\nthis work, in what is absent from data. In healthcare, societal and decision\nbiases shape patterns in missing data, yet the algorithmic fairness\nimplications of group-specific missingness are poorly understood. The way we\naddress missingness in healthcare can have detrimental impacts on downstream\nalgorithmic fairness. Our work questions current recommendations and practices\naimed at handling missing data with a focus on their effect on algorithmic\nfairness, and offers a path forward. Specifically, we consider the theoretical\nunderpinnings of existing recommendations as well as their empirical predictive\nperformance and corresponding algorithmic fairness measured through subgroup\nperformances. Our results show that current practices for handling missingness\nlack principled foundations, are disconnected from the realities of missingness\nmechanisms in healthcare, and can be counterproductive. For example, we show\nthat favouring group-specific imputation strategy can be misguided and\nexacerbate prediction disparities. We then build on our findings to propose a\nframework for empirically guiding imputation choices, and an accompanying\nreporting framework. Our work constitutes an important contribution to recent\nefforts by regulators and practitioners to grapple with the realities of\nreal-world data, and to foster the responsible and transparent deployment of\nmachine learning systems. We demonstrate the practical utility of the proposed\nframework through experimentation on widely used datasets, where we show how\nthe proposed framework can guide the selection of imputation strategies,\nallowing us to choose among strategies that yield equal overall predictive\nperformance but present different algorithmic fairness properties.\n","versions":"[{'version': 'v1', 'created': 'Sat, 13 Aug 2022 13:34:05 GMT'}, {'version': 'v2', 'created': 'Fri, 11 Nov 2022 18:08:04 GMT'}, {'version': 'v3', 'created': 'Fri, 30 Jun 2023 21:42:26 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 23:15:24 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Jeanselme', 'Vincent', ''], ['De-Arteaga', 'Maria', ''], ['Zhang', 'Zhe', ''], ['Barrett', 'Jessica', ''], ['Tom', 'Brian', '']]","extracted_entities":"[{'text': 'algorithmic fairness', 'label': 'Model Bias and Fairness'}, {'text': 'algorithmic fairness', 'label': 'Model Bias and Fairness'}, {'text': 'algorithmic\\nfairness', 'label': 'Model Bias and Fairness'}, {'text': 'algorithmic fairness', 'label': 'Model Bias and Fairness'}, {'text': 'algorithmic fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"algorithmic fairness","similarity_score":0.7059931755}
{"id":2306.00636,"submitter":"Frederik Hytting J{\\o}rgensen","authors":"Frederik Hytting J{\\o}rgensen, Sebastian Weichwald, Jonas Peters","title":"Unfair Utilities and First Steps Towards Improving Them","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.CY cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Many fairness criteria constrain the policy or choice of predictors, which\ncan have unwanted consequences, in particular, when optimizing the policy under\nsuch constraints. Here, we advocate to instead focus on the utility function\nthe policy is optimizing for. We define value of information fairness and\npropose to not use utility functions that violate this criterion. This\nprinciple suggests to modify these utility functions such that they satisfy\nvalue of information fairness. We describe how this can be done and discuss\nconsequences for the corresponding optimal policies. We apply our framework to\nthought experiments and the COMPAS data. Focussing on the utility function\nprovides better answers than existing fairness notions: We are not aware of any\nintuitively fair policy that is disallowed by value of information fairness,\nand when we find that value of information fairness recommends an intuitively\nunfair policy, no existing fairness notion finds an intuitively fair policy.\n","versions":"[{'version': 'v1', 'created': 'Thu, 1 Jun 2023 13:00:13 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 21:29:31 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['J\u00f8rgensen', 'Frederik Hytting', ''], ['Weichwald', 'Sebastian', ''], ['Peters', 'Jonas', '']]","extracted_entities":"[{'text': 'value of information fairness', 'label': 'Model Bias and Fairness'}, {'text': 'value of information fairness', 'label': 'Model Bias and Fairness'}, {'text': 'value of information fairness', 'label': 'Model Bias and Fairness'}, {'text': 'value of information fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"value of information fairness","similarity_score":0.6915696859}
{"id":2311.04623,"submitter":"Hugo Panzo","authors":"Aksheytha Chelikavada, Hugo Panzo","title":"Limit theorems for fixed point biased permutations avoiding a pattern of\n  length three","comments":"18 pages, minor revisions and references added","journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR math.CO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We prove limit theorems for the number of fixed points occurring in a random\npattern-avoiding permutation distributed according to a one-parameter family of\nbiased distributions. The bias parameter exponentially tilts the distribution\ntowards favoring permutations with more or fewer fixed points than is typical\nunder the uniform distribution. One case we study features a phase transition\nwhere the limiting distribution changes abruptly from negative binomial to\nRayleigh to normal depending on the bias parameter.\n","versions":"[{'version': 'v1', 'created': 'Wed, 8 Nov 2023 11:56:21 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 03:22:23 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Chelikavada', 'Aksheytha', ''], ['Panzo', 'Hugo', '']]","extracted_entities":"[{'text': 'bias parameter', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"bias parameter","similarity_score":0.5030195713}
{"id":2406.18841,"submitter":"Saleh Afroogh","authors":"Junfeng Jiao, Saleh Afroogh, Yiming Xu, Connor Phillips","title":"Navigating LLM Ethics: Advancements, Challenges, and Future Directions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This study addresses ethical issues surrounding Large Language Models (LLMs)\nwithin the field of artificial intelligence. It explores the common ethical\nchallenges posed by both LLMs and other AI systems, such as privacy and\nfairness, as well as ethical challenges uniquely arising from LLMs. It\nhighlights challenges such as hallucination, verifiable accountability, and\ndecoding censorship complexity, which are unique to LLMs and distinct from\nthose encountered in traditional AI systems. The study underscores the need to\ntackle these complexities to ensure accountability, reduce biases, and enhance\ntransparency in the influential role that LLMs play in shaping information\ndissemination. It proposes mitigation strategies and future directions for LLM\nethics, advocating for interdisciplinary collaboration. It recommends ethical\nframeworks tailored to specific domains and dynamic auditing systems adapted to\ndiverse contexts. This roadmap aims to guide responsible development and\nintegration of LLMs, envisioning a future where ethical considerations govern\nAI advancements in society.\n","versions":"[{'version': 'v1', 'created': 'Tue, 14 May 2024 15:03:05 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Jun 2024 02:56:09 GMT'}, {'version': 'v3', 'created': 'Thu, 19 Sep 2024 22:21:11 GMT'}, {'version': 'v4', 'created': 'Tue, 18 Mar 2025 16:57:17 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Jiao', 'Junfeng', ''], ['Afroogh', 'Saleh', ''], ['Xu', 'Yiming', ''], ['Phillips', 'Connor', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'privacy and\\nfairness', 'label': 'Model Bias and Fairness'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'verifiable accountability', 'label': 'Model Bias and Fairness'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ethical\\nframeworks', 'label': 'AI Ethics'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"privacy and\nfairness","similarity_score":0.6426992416}
{"id":2407.06705,"submitter":"Israel Leyva-Mayorga","authors":"Israel Leyva-Mayorga, Fabio Saggese, Lintao Li, and Petar Popovski","title":"Integrating Atmospheric Sensing and Communications for Resource\n  Allocation in NTNs","comments":"Submitted for publication to IEEE Transactions on Wireless\n  Communications","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.NI eess.SP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The integration of Non-Terrestrial Networks (NTNs) with Low Earth Orbit (LEO)\nsatellite constellations into 5G and Beyond is essential to achieve truly\nglobal connectivity. A distinctive characteristic of LEO mega constellations is\nthat they constitute a global infrastructure with predictable dynamics, which\nenables the pre-planned allocation of radio resources. However, the different\nbands that can be used for ground-to-satellite communication are affected\ndifferently by atmospheric conditions such as precipitation, which introduces\nuncertainty on the attenuation of the communication links at high frequencies.\nBased on this, we present a compelling case for applying integrated sensing and\ncommunications (ISAC) in heterogeneous and multi-layer LEO satellite\nconstellations over wide areas. Specifically, we propose a sensing-assisted\ncommunications framework and frame structure that not only enables the accurate\nestimation of the atmospheric attenuation in the communication links through\nsensing but also leverages this information to determine the optimal serving\nsatellites and allocate resources efficiently for downlink communication with\nusers on the ground. The results show that, by dedicating an adequate amount of\nresources for sensing and solving the association and resource allocation\nproblems jointly, it is feasible to increase the average throughput by 59% and\nthe fairness by 700% when compared to solving these problems separately.\n","versions":"[{'version': 'v1', 'created': 'Tue, 9 Jul 2024 09:32:11 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 17:46:22 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Leyva-Mayorga', 'Israel', ''], ['Saggese', 'Fabio', ''], ['Li', 'Lintao', ''], ['Popovski', 'Petar', '']]","extracted_entities":"[{'text': 'fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"fairness","similarity_score":0.6551788449}
{"id":2409.0751,"submitter":"Julia Stoyanovich","authors":"Falaah Arif Khan, Denys Herasymuk, Nazar Protsiv, Julia Stoyanovich","title":"Still More Shades of Null: An Evaluation Suite for Responsible Missing\n  Value Imputation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CY cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Data missingness is a practical challenge of sustained interest to the\nscientific community. In this paper, we present Shades-of-Null, an evaluation\nsuite for responsible missing value imputation. Our work is novel in two ways\n(i) we model realistic and socially-salient missingness scenarios that go\nbeyond Rubin's classic Missing Completely at Random (MCAR), Missing At Random\n(MAR) and Missing Not At Random (MNAR) settings, to include multi-mechanism\nmissingness (when different missingness patterns co-exist in the data) and\nmissingness shift (when the missingness mechanism changes between training and\ntest) (ii) we evaluate imputers holistically, based on imputation quality and\nimputation fairness, as well as on the predictive performance, fairness and\nstability of the models that are trained and tested on the data\npost-imputation.\n  We use Shades-of-Null to conduct a large-scale empirical study involving\n29,736 experimental pipelines, and find that while there is no single\nbest-performing imputation approach for all missingness types, interesting\ntrade-offs arise between predictive performance, fairness and stability, based\non the combination of missingness scenario, imputer choice, and the\narchitecture of the predictive model. We make Shades-of-Null publicly\navailable, to enable researchers to rigorously evaluate missing value\nimputation methods on a wide range of metrics in plausible and socially\nmeaningful scenarios.\n","versions":"[{'version': 'v1', 'created': 'Wed, 11 Sep 2024 17:58:39 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Oct 2024 01:06:42 GMT'}, {'version': 'v3', 'created': 'Thu, 31 Oct 2024 23:50:54 GMT'}, {'version': 'v4', 'created': 'Wed, 5 Feb 2025 00:42:46 GMT'}, {'version': 'v5', 'created': 'Tue, 18 Mar 2025 17:46:41 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Khan', 'Falaah Arif', ''], ['Herasymuk', 'Denys', ''], ['Protsiv', 'Nazar', ''], ['Stoyanovich', 'Julia', '']]","extracted_entities":"[{'text': 'imputation fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"fairness","similarity_score":0.6551788449}
{"id":2409.10496,"submitter":"Vassilis Lyberatos","authors":"Theodoros Sotirou, Vassilis Lyberatos, Orfeas Menis Mastromichalakis,\n  Giorgos Stamou","title":"MusicLIME: Explainable Multimodal Music Understanding","comments":"GitHub repository: https:\/\/github.com\/IamTheo2000\/MusicLIME. To be\n  presented at ICASSP 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.AI cs.LG eess.AS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multimodal models are critical for music understanding tasks, as they capture\nthe complex interplay between audio and lyrics. However, as these models become\nmore prevalent, the need for explainability grows-understanding how these\nsystems make decisions is vital for ensuring fairness, reducing bias, and\nfostering trust. In this paper, we introduce MusicLIME, a model-agnostic\nfeature importance explanation method designed for multimodal music models.\nUnlike traditional unimodal methods, which analyze each modality separately\nwithout considering the interaction between them, often leading to incomplete\nor misleading explanations, MusicLIME reveals how audio and lyrical features\ninteract and contribute to predictions, providing a holistic view of the\nmodel's decision-making. Additionally, we enhance local explanations by\naggregating them into global explanations, giving users a broader perspective\nof model behavior. Through this work, we contribute to improving the\ninterpretability of multimodal music models, empowering users to make informed\nchoices, and fostering more equitable, fair, and transparent music\nunderstanding systems.\n","versions":"[{'version': 'v1', 'created': 'Mon, 16 Sep 2024 17:28:21 GMT'}, {'version': 'v2', 'created': 'Mon, 13 Jan 2025 10:34:16 GMT'}, {'version': 'v3', 'created': 'Thu, 30 Jan 2025 12:13:04 GMT'}, {'version': 'v4', 'created': 'Sat, 15 Feb 2025 10:00:31 GMT'}, {'version': 'v5', 'created': 'Mon, 17 Mar 2025 18:21:48 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Sotirou', 'Theodoros', ''], ['Lyberatos', 'Vassilis', ''], ['Mastromichalakis', 'Orfeas Menis', ''], ['Stamou', 'Giorgos', '']]","extracted_entities":"[{'text': 'fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"fairness","similarity_score":0.6551788449}
{"id":2410.17263,"submitter":"Arjun Subramonian","authors":"Arjun Subramonian, Samuel J. Bell, Levent Sagun, Elvis Dohmatob","title":"An Effective Theory of Bias Amplification","comments":"Accepted to ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CY stat.ML","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Machine learning models can capture and amplify biases present in data,\nleading to disparate test performance across social groups. To better\nunderstand, evaluate, and mitigate these biases, a deeper theoretical\nunderstanding of how model design choices and data distribution properties\ncontribute to bias is needed. In this work, we contribute a precise analytical\ntheory in the context of ridge regression, both with and without random\nprojections, where the former models feedforward neural networks in a\nsimplified regime. Our theory offers a unified and rigorous explanation of\nmachine learning bias, providing insights into phenomena such as bias\namplification and minority-group bias in various feature and parameter regimes.\nFor example, we observe that there may be an optimal regularization penalty or\ntraining time to avoid bias amplification, and there can be differences in test\nerror between groups that are not alleviated with increased parameterization.\nImportantly, our theoretical predictions align with empirical observations\nreported in the literature on machine learning bias. We extensively empirically\nvalidate our theory on synthetic and semi-synthetic datasets.\n","versions":"[{'version': 'v1', 'created': 'Mon, 7 Oct 2024 08:43:22 GMT'}, {'version': 'v2', 'created': 'Mon, 28 Oct 2024 16:24:30 GMT'}, {'version': 'v3', 'created': 'Tue, 29 Oct 2024 02:21:41 GMT'}, {'version': 'v4', 'created': 'Tue, 18 Mar 2025 17:56:58 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Subramonian', 'Arjun', ''], ['Bell', 'Samuel J.', ''], ['Sagun', 'Levent', ''], ['Dohmatob', 'Elvis', '']]","extracted_entities":"[{'text': 'minority-group bias', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"minority-group bias","similarity_score":0.5968053341}
{"id":2502.21001,"submitter":"Woo Kyoung Han","authors":"Woo Kyoung Han, Byeonghun Lee, Hyunmin Cho, Sunghoon Im, Kyong Hwan\n  Jin","title":"Towards Lossless Implicit Neural Representation via Bit Plane\n  Decomposition","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We quantify the upper bound on the size of the implicit neural representation\n(INR) model from a digital perspective. The upper bound of the model size\nincreases exponentially as the required bit-precision increases. To this end,\nwe present a bit-plane decomposition method that makes INR predict bit-planes,\nproducing the same effect as reducing the upper bound of the model size. We\nvalidate our hypothesis that reducing the upper bound leads to faster\nconvergence with constant model size. Our method achieves lossless\nrepresentation in 2D image and audio fitting, even for high bit-depth signals,\nsuch as 16-bit, which was previously unachievable. We pioneered the presence of\nbit bias, which INR prioritizes as the most significant bit (MSB). We expand\nthe application of the INR task to bit depth expansion, lossless image\ncompression, and extreme network quantization. Our source code is available at\nhttps:\/\/github.com\/WooKyoungHan\/LosslessINR\n","versions":"[{'version': 'v1', 'created': 'Fri, 28 Feb 2025 12:43:46 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 10:50:43 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Han', 'Woo Kyoung', ''], ['Lee', 'Byeonghun', ''], ['Cho', 'Hyunmin', ''], ['Im', 'Sunghoon', ''], ['Jin', 'Kyong Hwan', '']]","extracted_entities":"[{'text': 'bit bias', 'label': 'Model Bias and Fairness'}, {'text': 'bit depth expansion', 'label': 'quantisation'}, {'text': 'lossless image\\ncompression', 'label': 'quantisation'}, {'text': 'extreme network quantization', 'label': 'quantisation'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"bit bias","similarity_score":0.5483270884}
{"id":2503.12755,"submitter":"Jianfei Zhang","authors":"Longfei Wei, Fang Sheng, Jianfei Zhang","title":"Cohort-attention Evaluation Metric against Tied Data: Studying\n  Performance of Classification Models in Cancer Detection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CE stat.ML","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Artificial intelligence (AI) has significantly improved medical screening\naccuracy, particularly in cancer detection and risk assessment. However,\ntraditional classification metrics often fail to account for imbalanced data,\nvarying performance across cohorts, and patient-level inconsistencies, leading\nto biased evaluations. We propose the Cohort-Attention Evaluation Metrics (CAT)\nframework to address these challenges. CAT introduces patient-level assessment,\nentropy-based distribution weighting, and cohort-weighted sensitivity and\nspecificity. Key metrics like CATSensitivity (CATSen), CATSpecificity (CATSpe),\nand CATMean ensure balanced and fair evaluation across diverse populations.\nThis approach enhances predictive reliability, fairness, and interpretability,\nproviding a robust evaluation method for AI-driven medical screening models.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 02:50:40 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wei', 'Longfei', ''], ['Sheng', 'Fang', ''], ['Zhang', 'Jianfei', '']]","extracted_entities":"[{'text': 'CATSpecificity', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"fairness","similarity_score":0.6551788449}
{"id":2503.13335,"submitter":"Sang Truong","authors":"Sang Truong, Yuheng Tu, Percy Liang, Bo Li, Sanmi Koyejo","title":"Reliable and Efficient Amortized Model-based Evaluation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG stat.AP","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Comprehensive evaluations of language models (LM) during both development and\ndeployment phases are necessary because these models possess numerous\ncapabilities (e.g., mathematical reasoning, legal support, or medical\ndiagnostic) as well as safety risks (e.g., racial bias, toxicity, or\nmisinformation). The average score across a wide range of benchmarks provides a\nsignal that helps guide the use of these LMs in practice. Currently, holistic\nevaluations are costly due to the large volume of benchmark questions, making\nfrequent evaluations impractical. A popular attempt to lower the cost is to\ncompute the average score on a subset of the benchmark. This approach,\nunfortunately, often renders an unreliable measure of LM performance because\nthe average score is often confounded with the difficulty of the questions in\nthe benchmark subset. Item response theory (IRT) was designed to address this\nchallenge, providing a reliable measurement by careful controlling for question\ndifficulty. Unfortunately, question difficulty is expensive to estimate. Facing\nthis challenge, we train a model that predicts question difficulty from its\ncontent, enabling a reliable measurement at a fraction of the cost. In\naddition, we leverage this difficulty predictor to further improve the\nevaluation efficiency through training a question generator given a difficulty\nlevel. This question generator is essential in adaptive testing, where, instead\nof using a random subset of the benchmark questions, informative questions are\nadaptively chosen based on the current estimation of LLM performance.\nExperiments on 22 common natural language benchmarks and 172 LMs show that this\napproach is more reliable and efficient compared to current common practice.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 16:15:02 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Truong', 'Sang', ''], ['Tu', 'Yuheng', ''], ['Liang', 'Percy', ''], ['Li', 'Bo', ''], ['Koyejo', 'Sanmi', '']]","extracted_entities":"[{'text': 'racial bias', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"racial bias","similarity_score":0.5817146301}
{"id":2503.13662,"submitter":"Hasibul Jamil","authors":"Hasubil Jamil, Jacob Goldverg, Elvis Rodrigues, MD S Q Zulkar Nine,\n  and Tevfik Kosar","title":"Optimizing Data Transfer Performance and Energy Efficiency with Deep\n  Reinforcement Learning","comments":"Will be submitted to TPDS","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DC cs.NI cs.PF","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The rapid growth of data across fields of science and industry has increased\nthe need to improve the performance of end-to-end data transfers while using\nthe resources more efficiently. In this paper, we present a dynamic,\nmultiparameter reinforcement learning (RL) framework that adjusts\napplication-layer transfer settings during data transfers on shared networks.\nOur method strikes a balance between high throughput and low energy utilization\nby employing reward signals that focus on both energy efficiency and fairness.\nThe RL agents can pause and resume transfer threads as needed, pausing during\nheavy network use and resuming when resources are available, to prevent\noverload and save energy. We evaluate several RL techniques and compare our\nsolution with state-of-the-art methods by measuring computational overhead,\nadaptability, throughput, and energy consumption. Our experiments show up to\n25% increase in throughput and up to 40% reduction in energy usage at the end\nsystems compared to baseline methods, highlighting a fair and energy-efficient\nway to optimize data transfers in shared network environments.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 19:10:19 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Jamil', 'Hasubil', ''], ['Goldverg', 'Jacob', ''], ['Rodrigues', 'Elvis', ''], ['Nine', 'MD S Q Zulkar', ''], ['Kosar', 'Tevfik', '']]","extracted_entities":"[{'text': 'fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"fairness","similarity_score":0.6551788449}
{"id":2503.14539,"submitter":"Shahmar Mirishli","authors":"Shahmar Mirishli","title":"Ethical Implications of AI in Data Collection: Balancing Innovation with\n  Privacy","comments":null,"journal-ref":null,"doi":"10.36719\/2706-6185\/38\/40-55","report-no":null,"categories":"cs.CY cs.HC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This article examines the ethical and legal implications of artificial\nintelligence (AI) driven data collection, focusing on developments from 2023 to\n2024. It analyzes recent advancements in AI technologies and their impact on\ndata collection practices across various sectors. The study compares regulatory\napproaches in the European Union, the United States, and China, highlighting\nthe challenges in creating a globally harmonized framework for AI governance.\nKey ethical issues, including informed consent, algorithmic bias, and privacy\nprotection, are critically assessed in the context of increasingly\nsophisticated AI systems. The research explores case studies in healthcare,\nfinance, and smart cities to illustrate the practical challenges of AI\nimplementation. It evaluates the effectiveness of current legal frameworks and\nproposes solutions encompassing legal and policy recommendations, technical\nsafeguards, and ethical frameworks. The article emphasizes the need for\nadaptive governance and international cooperation to address the global nature\nof AI development while balancing innovation with the protection of individual\nrights and societal values.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 14:15:59 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Mirishli', 'Shahmar', '']]","extracted_entities":"[{'text': 'informed consent', 'label': 'AI Ethics'}, {'text': 'algorithmic bias', 'label': 'Model Bias and Fairness'}, {'text': 'privacy\\nprotection', 'label': 'AI Ethics'}, {'text': 'ethical frameworks', 'label': 'AI Ethics'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"algorithmic bias","similarity_score":0.6063995361}
{"id":2503.14827,"submitter":"Chejian Xu","authors":"Chejian Xu, Jiawei Zhang, Zhaorun Chen, Chulin Xie, Mintong Kang,\n  Yujin Potter, Zhun Wang, Zhuowen Yuan, Alexander Xiong, Zidi Xiong, Chenhui\n  Zhang, Lingzhi Yuan, Yi Zeng, Peiyang Xu, Chengquan Guo, Andy Zhou, Jeffrey\n  Ziwei Tan, Xuandong Zhao, Francesco Pinto, Zhen Xiang, Yu Gai, Zinan Lin, Dan\n  Hendrycks, Bo Li, Dawn Song","title":"MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation\n  Models","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multimodal foundation models (MMFMs) play a crucial role in various\napplications, including autonomous driving, healthcare, and virtual assistants.\nHowever, several studies have revealed vulnerabilities in these models, such as\ngenerating unsafe content by text-to-image models. Existing benchmarks on\nmultimodal models either predominantly assess the helpfulness of these models,\nor only focus on limited perspectives such as fairness and privacy. In this\npaper, we present the first unified platform, MMDT (Multimodal DecodingTrust),\ndesigned to provide a comprehensive safety and trustworthiness evaluation for\nMMFMs. Our platform assesses models from multiple perspectives, including\nsafety, hallucination, fairness\/bias, privacy, adversarial robustness, and\nout-of-distribution (OOD) generalization. We have designed various evaluation\nscenarios and red teaming algorithms under different tasks for each perspective\nto generate challenging data, forming a high-quality benchmark. We evaluate a\nrange of multimodal models using MMDT, and our findings reveal a series of\nvulnerabilities and areas for improvement across these perspectives. This work\nintroduces the first comprehensive and unique safety and trustworthiness\nevaluation platform for MMFMs, paving the way for developing safer and more\nreliable MMFMs and systems. Our platform and benchmark are available at\nhttps:\/\/mmdecodingtrust.github.io\/.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 01:59:44 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Xu', 'Chejian', ''], ['Zhang', 'Jiawei', ''], ['Chen', 'Zhaorun', ''], ['Xie', 'Chulin', ''], ['Kang', 'Mintong', ''], ['Potter', 'Yujin', ''], ['Wang', 'Zhun', ''], ['Yuan', 'Zhuowen', ''], ['Xiong', 'Alexander', ''], ['Xiong', 'Zidi', ''], ['Zhang', 'Chenhui', ''], ['Yuan', 'Lingzhi', ''], ['Zeng', 'Yi', ''], ['Xu', 'Peiyang', ''], ['Guo', 'Chengquan', ''], ['Zhou', 'Andy', ''], ['Tan', 'Jeffrey Ziwei', ''], ['Zhao', 'Xuandong', ''], ['Pinto', 'Francesco', ''], ['Xiang', 'Zhen', ''], ['Gai', 'Yu', ''], ['Lin', 'Zinan', ''], ['Hendrycks', 'Dan', ''], ['Li', 'Bo', ''], ['Song', 'Dawn', '']]","extracted_entities":"[{'text': 'Multimodal foundation models', 'label': 'Foundation Model'}, {'text': 'MMFMs', 'label': 'Foundation Model'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'privacy', 'label': 'Model Bias and Fairness'}, {'text': 'MMFMs', 'label': 'Foundation Model'}, {'text': 'safety', 'label': 'Model Bias and Fairness'}, {'text': 'hallucination', 'label': 'Model Bias and Fairness'}, {'text': 'fairness\/bias', 'label': 'Model Bias and Fairness'}, {'text': 'privacy', 'label': 'Model Bias and Fairness'}, {'text': 'adversarial robustness', 'label': 'Model Bias and Fairness'}, {'text': 'MMFMs', 'label': 'Foundation Model'}, {'text': 'MMFMs', 'label': 'Foundation Model'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"fairness\/bias","similarity_score":0.8463156223}
{"id":2503.15622,"submitter":"Gianmario Voria","authors":"Alessandra Parziale, Gianmario Voria, Giammaria Giordano, Gemma\n  Catolino, Gregorio Robles, Fabio Palomba","title":"Contextual Fairness-Aware Practices in ML: A Cost-Effective Empirical\n  Evaluation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  As machine learning (ML) systems become central to critical decision-making,\nconcerns over fairness and potential biases have increased. To address this,\nthe software engineering (SE) field has introduced bias mitigation techniques\naimed at enhancing fairness in ML models at various stages. Additionally,\nrecent research suggests that standard ML engineering practices can also\nimprove fairness; these practices, known as fairness-aware practices, have been\ncataloged across each stage of the ML development life cycle. However, fairness\nremains context-dependent, with different domains requiring customized\nsolutions. Furthermore, existing specific bias mitigation methods may sometimes\ndegrade model performance, raising ongoing discussions about the trade-offs\ninvolved.\n  In this paper, we empirically investigate fairness-aware practices from two\nperspectives: contextual and cost-effectiveness. The contextual evaluation\nexplores how these practices perform in various application domains,\nidentifying areas where specific fairness adjustments are particularly\neffective. The cost-effectiveness evaluation considers the trade-off between\nfairness improvements and potential performance costs. Our findings provide\ninsights into how context influences the effectiveness of fairness-aware\npractices. This research aims to guide SE practitioners in selecting practices\nthat achieve fairness with minimal performance costs, supporting the\ndevelopment of ethical ML systems.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 18:10:21 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Parziale', 'Alessandra', ''], ['Voria', 'Gianmario', ''], ['Giordano', 'Giammaria', ''], ['Catolino', 'Gemma', ''], ['Robles', 'Gregorio', ''], ['Palomba', 'Fabio', '']]","extracted_entities":"[{'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"fairness","similarity_score":0.6551788449}
{"id":2503.15949,"submitter":"Lichao Mou","authors":"Yaxiong Chen, Minghong Wei, Zixuan Zheng, Jingliang Hu, Yilei Shi,\n  Shengwu Xiong, Xiao Xiang Zhu, Lichao Mou","title":"CausalCLIPSeg: Unlocking CLIP's Potential in Referring Medical Image\n  Segmentation with Causal Intervention","comments":"MICCAI 2024","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Referring medical image segmentation targets delineating lesions indicated by\ntextual descriptions. Aligning visual and textual cues is challenging due to\ntheir distinct data properties. Inspired by large-scale pre-trained\nvision-language models, we propose CausalCLIPSeg, an end-to-end framework for\nreferring medical image segmentation that leverages CLIP. Despite not being\ntrained on medical data, we enforce CLIP's rich semantic space onto the medical\ndomain by a tailored cross-modal decoding method to achieve text-to-pixel\nalignment. Furthermore, to mitigate confounding bias that may cause the model\nto learn spurious correlations instead of meaningful causal relationships,\nCausalCLIPSeg introduces a causal intervention module which self-annotates\nconfounders and excavates causal features from inputs for segmentation\njudgments. We also devise an adversarial min-max game to optimize causal\nfeatures while penalizing confounding ones. Extensive experiments demonstrate\nthe state-of-the-art performance of our proposed method. Code is available at\nhttps:\/\/github.com\/WUTCM-Lab\/CausalCLIPSeg.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:46:24 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chen', 'Yaxiong', ''], ['Wei', 'Minghong', ''], ['Zheng', 'Zixuan', ''], ['Hu', 'Jingliang', ''], ['Shi', 'Yilei', ''], ['Xiong', 'Shengwu', ''], ['Zhu', 'Xiao Xiang', ''], ['Mou', 'Lichao', '']]","extracted_entities":"[{'text': 'CLIP', 'label': 'Large Language Model'}, {'text': 'CLIP', 'label': 'Large Language Model'}, {'text': 'confounding bias', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"confounding bias","similarity_score":0.5946897268}
{"id":2503.16063,"submitter":"Zhiyu Cao","authors":"Zhiyu Cao, Peifeng Li, Qiaoming Zhu, Yaxin Fan","title":"Two-stage Incomplete Utterance Rewriting on Editing Operation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Previous work on Incomplete Utterance Rewriting (IUR) has primarily focused\non generating rewritten utterances based solely on dialogue context, ignoring\nthe widespread phenomenon of coreference and ellipsis in dialogues. To address\nthis issue, we propose a novel framework called TEO (\\emph{Two-stage approach\non Editing Operation}) for IUR, in which the first stage generates editing\noperations and the second stage rewrites incomplete utterances utilizing the\ngenerated editing operations and the dialogue context. Furthermore, an\nadversarial perturbation strategy is proposed to mitigate cascading errors and\nexposure bias caused by the inconsistency between training and inference in the\nsecond stage. Experimental results on three IUR datasets show that our TEO\noutperforms the SOTA models significantly.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 11:56:14 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Cao', 'Zhiyu', ''], ['Li', 'Peifeng', ''], ['Zhu', 'Qiaoming', ''], ['Fan', 'Yaxin', '']]","extracted_entities":"[{'text': 'dialogue context', 'label': 'contextual Embedding'}, {'text': 'dialogue context', 'label': 'contextual Embedding'}, {'text': 'exposure bias', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"exposure bias","similarity_score":0.5364996791}
{"id":2503.16146,"submitter":"Talip Tolga Sar{\\i}","authors":"Talip Tolga Sar{\\i}, G\\\"okhan Se\\c{c}inti, Angelo Trotta","title":"Distributed Split Computing Using Diffusive Metrics for UAV Swarms","comments":"This work has been submitted to a IEEE journal for possible\n  publication","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.NI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In large-scale UAV swarms, dynamically executing machine learning tasks can\npose significant challenges due to network volatility and the heterogeneous\nresource constraints of each UAV. Traditional approaches often rely on\ncentralized orchestration to partition tasks among nodes. However, these\nmethods struggle with communication bottlenecks, latency, and reliability when\nthe swarm grows or the topology shifts rapidly. To overcome these limitations,\nwe propose a fully distributed, diffusive metric-based approach for split\ncomputing in UAV swarms. Our solution introduces a new iterative measure,\ntermed the aggregated gigaflops, capturing each node's own computing capacity\nalong with that of its neighbors without requiring global network knowledge. By\nforwarding partial inferences intelligently to underutilized nodes, we achieve\nimproved task throughput, lower latency, and enhanced energy efficiency.\nFurther, to handle sudden workload surges and rapidly changing node conditions,\nwe incorporate an early-exit mechanism that can adapt the inference pathway\non-the-fly. Extensive simulations demonstrate that our approach significantly\noutperforms baseline strategies across multiple performance indices, including\nlatency, fairness, and energy consumption. These results highlight the\nfeasibility of large-scale distributed intelligence in UAV swarms and provide a\nblueprint for deploying robust, scalable ML services in diverse aerial\nnetworks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:49:15 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Sar\u0131', 'Talip Tolga', ''], ['Se\u00e7inti', 'G\u00f6khan', ''], ['Trotta', 'Angelo', '']]","extracted_entities":"[{'text': 'fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"fairness","similarity_score":0.6551788449}
{"id":2503.16251,"submitter":"Dawood Wasif","authors":"Dawood Wasif, Terrence J. Moore, Jin-Hee Cho","title":"RESFL: An Uncertainty-Aware Framework for Responsible Federated Learning\n  by Balancing Privacy, Fairness and Utility in Autonomous Vehicles","comments":"Submitted to PETS 2025 (under review)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV cs.DC cs.ET","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Autonomous vehicles (AVs) increasingly rely on Federated Learning (FL) to\nenhance perception models while preserving privacy. However, existing FL\nframeworks struggle to balance privacy, fairness, and robustness, leading to\nperformance disparities across demographic groups. Privacy-preserving\ntechniques like differential privacy mitigate data leakage risks but worsen\nfairness by restricting access to sensitive attributes needed for bias\ncorrection. This work explores the trade-off between privacy and fairness in\nFL-based object detection for AVs and introduces RESFL, an integrated solution\noptimizing both. RESFL incorporates adversarial privacy disentanglement and\nuncertainty-guided fairness-aware aggregation. The adversarial component uses a\ngradient reversal layer to remove sensitive attributes, reducing privacy risks\nwhile maintaining fairness. The uncertainty-aware aggregation employs an\nevidential neural network to weight client updates adaptively, prioritizing\ncontributions with lower fairness disparities and higher confidence. This\nensures robust and equitable FL model updates. We evaluate RESFL on the FACET\ndataset and CARLA simulator, assessing accuracy, fairness, privacy resilience,\nand robustness under varying conditions. RESFL improves detection accuracy,\nreduces fairness disparities, and lowers privacy attack success rates while\ndemonstrating superior robustness to adversarial conditions compared to other\napproaches.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 15:46:03 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wasif', 'Dawood', ''], ['Moore', 'Terrence J.', ''], ['Cho', 'Jin-Hee', '']]","extracted_entities":"[{'text': 'Federated Learning', 'label': 'Few-shot Learning'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'RESFL', 'label': 'Neural Language Model'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'evidential neural network', 'label': 'Neural Language Model'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'privacy resilience', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"fairness","similarity_score":0.6551788449}
{"id":2503.16414,"submitter":"Dominik Peters","authors":"Christian Kroer and Dominik Peters","title":"Computing Lindahl Equilibrium for Public Goods with and without Funding\n  Caps","comments":"32 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GT","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Lindahl equilibrium is a solution concept for allocating a fixed budget\nacross several divisible public goods. It always lies in the core, meaning that\nthe equilibrium allocation satisfies desirable stability and proportional\nfairness properties. We consider a model where agents have separable linear\nutility functions over the public goods, and the output assigns to each good an\namount of spending, summing to at most the available budget.\n  In the uncapped setting, each of the public goods can absorb any amount of\nfunding. In this case, it is known that Lindahl equilibrium is equivalent to\nmaximizing Nash social welfare, and this allocation can be computed by a\npublic-goods variant of the proportional response dynamics. We introduce a new\nconvex programming formulation for computing this solution and show that it is\nrelated to Nash welfare maximization through duality and reformulation. We then\nshow that the proportional response dynamics is equivalent to running mirror\ndescent on our new formulation, thereby providing a new and immediate proof of\nthe convergence guarantee for the dynamics. Our new formulation has\nsimilarities to Shmyrev's convex program for Fisher market equilibrium.\n  In the capped setting, each public good has an upper bound on the amount of\nfunding it can receive. In this setting, existence of Lindahl equilibrium was\nonly known via fixed-point arguments. The existence of an efficient algorithm\ncomputing one has been a long-standing open question. We prove that our new\nconvex program continues to work when the cap constraints are added, and its\noptimal solutions are Lindahl equilibria. Thus, we establish that Lindahl\nequilibrium can be efficiently computed in the capped setting. Our result also\nimplies that approximately core-stable allocations can be efficiently computed\nfor the class of separable piecewise-linear concave (SPLC) utilities.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:59:21 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Kroer', 'Christian', ''], ['Peters', 'Dominik', '']]","extracted_entities":"[{'text': 'proportional\\nfairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Model Bias and Fairness","matched_keyword":"proportional\nfairness","similarity_score":0.7003743649}
{"id":2304.08247,"submitter":"Keno Bressem","authors":"Tianyu Han and Lisa C. Adams and Jens-Michalis Papaioannou and Paul\n  Grundmann and Tom Oberhauser and Alexei Figueroa and Alexander L\\\"oser and\n  Daniel Truhn and Keno K. Bressem","title":"MedAlpaca -- An Open-Source Collection of Medical Conversational AI\n  Models and Training Data","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  As large language models (LLMs) like OpenAI's GPT series continue to make\nstrides, we witness the emergence of artificial intelligence applications in an\never-expanding range of fields. In medicine, these LLMs hold considerable\npromise for improving medical workflows, diagnostics, patient care, and\neducation. Yet, there is an urgent need for open-source models that can be\ndeployed on-premises to safeguard patient privacy. In our work, we present an\ninnovative dataset consisting of over 160,000 entries, specifically crafted to\nfine-tune LLMs for effective medical applications. We investigate the impact of\nfine-tuning these datasets on publicly accessible pre-trained LLMs, and\nsubsequently, we juxtapose the performance of pre-trained-only models against\nthe fine-tuned models concerning the examinations that future medical doctors\nmust pass to achieve certification.\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Apr 2023 11:28:08 GMT'}, {'version': 'v2', 'created': 'Wed, 4 Oct 2023 23:28:00 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 21:31:51 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Han', 'Tianyu', ''], ['Adams', 'Lisa C.', ''], ['Papaioannou', 'Jens-Michalis', ''], ['Grundmann', 'Paul', ''], ['Oberhauser', 'Tom', ''], ['Figueroa', 'Alexei', ''], ['L\u00f6ser', 'Alexander', ''], ['Truhn', 'Daniel', ''], ['Bressem', 'Keno K.', '']]","extracted_entities":"[{'text': 'GPT', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT","similarity_score":1.0000001192}
{"id":2406.11139,"submitter":"Somnath Banerjee","authors":"Somnath Banerjee, Avik Halder, Rajarshi Mandal, Sayan Layek, Ian\n  Soboroff, Rima Hazra, Animesh Mukherjee","title":"Breaking Boundaries: Investigating the Effects of Model Editing on\n  Cross-linguistic Performance","comments":"Accepted at NAACL 2025 (Industry track)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The integration of pretrained language models (PLMs) like BERT and GPT has\nrevolutionized NLP, particularly for English, but it has also created\nlinguistic imbalances. This paper strategically identifies the need for\nlinguistic equity by examining several knowledge editing techniques in\nmultilingual contexts. We evaluate the performance of models such as Mistral,\nTowerInstruct, OpenHathi, Tamil-Llama, and Kan-Llama across languages including\nEnglish, German, French, Italian, Spanish, Hindi, Tamil, and Kannada. Our\nresearch identifies significant discrepancies in normal and merged models\nconcerning cross-lingual consistency. We employ strategies like 'each language\nfor itself' (ELFI) and 'each language for others' (ELFO) to stress-test these\nmodels. Our findings demonstrate the potential for LLMs to overcome linguistic\nbarriers, laying the groundwork for future research in achieving linguistic\ninclusivity in AI technologies.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Jun 2024 01:54:27 GMT'}, {'version': 'v2', 'created': 'Wed, 17 Jul 2024 18:37:54 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Feb 2025 07:25:50 GMT'}, {'version': 'v4', 'created': 'Tue, 18 Mar 2025 11:58:48 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Banerjee', 'Somnath', ''], ['Halder', 'Avik', ''], ['Mandal', 'Rajarshi', ''], ['Layek', 'Sayan', ''], ['Soboroff', 'Ian', ''], ['Hazra', 'Rima', ''], ['Mukherjee', 'Animesh', '']]","extracted_entities":"[{'text': 'BERT', 'label': 'BERT'}, {'text': 'GPT', 'label': 'GPT'}, {'text': 'Mistral', 'label': 'Mistral'}]","assigned_concept":"GPT","matched_keyword":"GPT","similarity_score":1.0000001192}
{"id":2407.18908,"submitter":"Boyi Li","authors":"Boyi Li and Ligeng Zhu and Ran Tian and Shuhan Tan and Yuxiao Chen and\n  Yao Lu and Yin Cui and Sushant Veer and Max Ehrlich and Jonah Philion and\n  Xinshuo Weng and Fuzhao Xue and Linxi Fan and Yuke Zhu and Jan Kautz and\n  Andrew Tao and Ming-Yu Liu and Sanja Fidler and Boris Ivanovic and Trevor\n  Darrell and Jitendra Malik and Song Han and Marco Pavone","title":"Wolf: Dense Video Captioning with a World Summarization Framework","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We propose Wolf, a WOrLd summarization Framework for accurate video\ncaptioning. Wolf is an automated captioning framework that adopts a\nmixture-of-experts approach, leveraging complementary strengths of Vision\nLanguage Models (VLMs). By utilizing both image and video models, our framework\ncaptures different levels of information and summarizes them efficiently. Our\napproach can be applied to enhance video understanding, auto-labeling, and\ncaptioning. To evaluate caption quality, we introduce CapScore, an LLM-based\nmetric to assess the similarity and quality of generated captions compared to\nthe ground truth captions. We further build four human-annotated datasets in\nthree domains: autonomous driving, general scenes, and robotics, to facilitate\ncomprehensive comparisons. We show that Wolf achieves superior captioning\nperformance compared to state-of-the-art approaches from the research community\n(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For\ninstance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise\nby 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,\nwe establish a benchmark for video captioning and introduce a leaderboard,\naiming to accelerate advancements in video understanding, captioning, and data\nalignment. Webpage: https:\/\/wolfv0.github.io\/.\n","versions":"[{'version': 'v1', 'created': 'Fri, 26 Jul 2024 17:59:09 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 17:56:05 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Li', 'Boyi', ''], ['Zhu', 'Ligeng', ''], ['Tian', 'Ran', ''], ['Tan', 'Shuhan', ''], ['Chen', 'Yuxiao', ''], ['Lu', 'Yao', ''], ['Cui', 'Yin', ''], ['Veer', 'Sushant', ''], ['Ehrlich', 'Max', ''], ['Philion', 'Jonah', ''], ['Weng', 'Xinshuo', ''], ['Xue', 'Fuzhao', ''], ['Fan', 'Linxi', ''], ['Zhu', 'Yuke', ''], ['Kautz', 'Jan', ''], ['Tao', 'Andrew', ''], ['Liu', 'Ming-Yu', ''], ['Fidler', 'Sanja', ''], ['Ivanovic', 'Boris', ''], ['Darrell', 'Trevor', ''], ['Malik', 'Jitendra', ''], ['Han', 'Song', ''], ['Pavone', 'Marco', '']]","extracted_entities":"[{'text': 'GPT-4V', 'label': 'GPT'}, {'text': 'GPT-4V', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4V","similarity_score":0.7028234005}
{"id":2408.09174,"submitter":"Jian Yang","authors":"Xianjie Wu, Jian Yang, Linzheng Chai, Ge Zhang, Jiaheng Liu, Xinrun\n  Du, Di Liang, Daixin Shu, Xianfu Cheng, Tianzhen Sun, Guanglin Niu, Tongliang\n  Li, Zhoujun Li","title":"TableBench: A Comprehensive and Complex Benchmark for Table Question\n  Answering","comments":"12 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advancements in Large Language Models (LLMs) have markedly enhanced\nthe interpretation and processing of tabular data, introducing previously\nunimaginable capabilities. Despite these achievements, LLMs still encounter\nsignificant challenges when applied in industrial scenarios, particularly due\nto the increased complexity of reasoning required with real-world tabular data,\nunderscoring a notable disparity between academic benchmarks and practical\napplications. To address this discrepancy, we conduct a detailed investigation\ninto the application of tabular data in industrial scenarios and propose a\ncomprehensive and complex benchmark TableBench, including 18 fields within four\nmajor categories of table question answering (TableQA) capabilities.\nFurthermore, we introduce TableLLM, trained on our meticulously constructed\ntraining set TableInstruct, achieving comparable performance with GPT-3.5.\nMassive experiments conducted on TableBench indicate that both open-source and\nproprietary LLMs still have significant room for improvement to meet real-world\ndemands, where the most advanced model, GPT-4, achieves only a modest score\ncompared to humans.\n","versions":"[{'version': 'v1', 'created': 'Sat, 17 Aug 2024 11:40:10 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 07:13:18 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wu', 'Xianjie', ''], ['Yang', 'Jian', ''], ['Chai', 'Linzheng', ''], ['Zhang', 'Ge', ''], ['Liu', 'Jiaheng', ''], ['Du', 'Xinrun', ''], ['Liang', 'Di', ''], ['Shu', 'Daixin', ''], ['Cheng', 'Xianfu', ''], ['Sun', 'Tianzhen', ''], ['Niu', 'Guanglin', ''], ['Li', 'Tongliang', ''], ['Li', 'Zhoujun', '']]","extracted_entities":"[{'text': 'TableInstruct', 'label': 'Embedding'}, {'text': 'GPT-4', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4","similarity_score":0.8572875857}
{"id":2410.18447,"submitter":"Zezhong Wang Mr.","authors":"Zezhong Wang, Xingshan Zeng, Weiwen Liu, Liangyou Li, Yasheng Wang,\n  Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong","title":"ToolFlow: Boosting LLM Tool-Calling Through Natural and Coherent\n  Dialogue Synthesis","comments":"Accepted by NAACL 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Supervised fine-tuning (SFT) is a common method to enhance the tool calling\ncapabilities of Large Language Models (LLMs), with the training data often\nbeing synthesized. The current data synthesis process generally involves\nsampling a set of tools, formulating a requirement based on these tools, and\ngenerating the call statements. However, tools sampled randomly lack relevance,\nmaking them difficult to combine and thus reducing the diversity of the data.\nAdditionally, current work overlooks the coherence between turns of dialogues,\nleading to a gap between the synthesized data and real-world scenarios. To\naddress these issues, we propose a Graph-based Sampling strategy to sample more\nrelevant tool combinations, and a Planned-generation strategy to create plans\nthat guide the synthesis of coherent dialogues. We integrate these two\nstrategies and enable multiple agents to synthesize the dialogue data\ninteractively, resulting in our tool-calling data synthesis pipeline ToolFlow.\nData quality assessments demonstrate improvements in the naturalness and\ncoherence of our synthesized dialogues. Finally, we apply SFT on LLaMA-3.1-8B\nusing 8,000 synthetic dialogues generated with ToolFlow. Results show that the\nmodel achieves tool-calling performance comparable to or even surpassing GPT-4,\nwhile maintaining strong general capabilities.\n","versions":"[{'version': 'v1', 'created': 'Thu, 24 Oct 2024 05:45:04 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 06:00:52 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wang', 'Zezhong', ''], ['Zeng', 'Xingshan', ''], ['Liu', 'Weiwen', ''], ['Li', 'Liangyou', ''], ['Wang', 'Yasheng', ''], ['Shang', 'Lifeng', ''], ['Jiang', 'Xin', ''], ['Liu', 'Qun', ''], ['Wong', 'Kam-Fai', '']]","extracted_entities":"[{'text': 'Supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'GPT-4', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4","similarity_score":0.8572875857}
{"id":2411.01414,"submitter":"Qi Hong Chen","authors":"QiHong Chen, Jiachen Yu, Jiawei Li, Jiecheng Deng, Justin Tian Jin\n  Chen, Iftekhar Ahmed","title":"A Deep Dive Into Large Language Model Code Generation Mistakes: What and\n  Why?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advancements in Large Language Models (LLMs) have led to their\nwidespread application in automated code generation. However, these models can\nstill generate defective code that deviates from the specification. Previous\nresearch has mainly focused on the mistakes in LLM-generated standalone\nfunctions, overlooking real-world software development situations where the\nsuccessful generation of the code requires software contexts such as external\ndependencies. In this paper, we considered both of these code generation\nsituations and identified a range of \\textit{non-syntactic mistakes} arising\nfrom LLMs' misunderstandings of coding question specifications. Seven\ncategories of non-syntactic mistakes were identified through extensive manual\nanalyses, four of which were missed by previous works. To better understand\nthese mistakes, we proposed six reasons behind these mistakes from various\nperspectives. Moreover, we explored the effectiveness of LLMs in detecting\nmistakes and their reasons. Our evaluation demonstrated that GPT-4 with the\nReAct prompting technique can achieve an F1 score of up to 0.65 when\nidentifying reasons for LLM's mistakes, such as misleading function signatures.\nWe believe that these findings offer valuable insights into enhancing the\nquality of LLM-generated code.\n","versions":"[{'version': 'v1', 'created': 'Sun, 3 Nov 2024 02:47:03 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 11:21:07 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chen', 'QiHong', ''], ['Yu', 'Jiachen', ''], ['Li', 'Jiawei', ''], ['Deng', 'Jiecheng', ''], ['Chen', 'Justin Tian Jin', ''], ['Ahmed', 'Iftekhar', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'ReAct prompting technique', 'label': 'Prompting'}]","assigned_concept":"GPT","matched_keyword":"GPT-4","similarity_score":0.8572875857}
{"id":2411.06946,"submitter":"Subhankar Maity","authors":"Aniket Deroy, Subhankar Maity","title":"Cancer-Answer: Empowering Cancer Care with Advanced Large Language\n  Models","comments":"Updated and Final Version","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Gastrointestinal (GI) tract cancers account for a substantial portion of the\nglobal cancer burden, where early diagnosis is critical for improved management\nand patient outcomes. The complex aetiologies and overlapping symptoms across\nGI cancers often delay diagnosis, leading to suboptimal treatment strategies.\nCancer-related queries are crucial for timely diagnosis, treatment, and patient\neducation, as access to accurate, comprehensive information can significantly\ninfluence outcomes. However, the complexity of cancer as a disease, combined\nwith the vast amount of available data, makes it difficult for clinicians and\npatients to quickly find precise answers. To address these challenges, we\nleverage large language models (LLMs) such as GPT-3.5 Turbo to generate\naccurate, contextually relevant responses to cancer-related queries.\nPre-trained with medical data, these models provide timely, actionable insights\nthat support informed decision-making in cancer diagnosis and care, ultimately\nimproving patient outcomes. We calculate two metrics: A1 (which represents the\nfraction of entities present in the model-generated answer compared to the gold\nstandard) and A2 (which represents the linguistic correctness and\nmeaningfulness of the model-generated answer with respect to the gold\nstandard), achieving maximum values of 0.546 and 0.881, respectively.\n","versions":"[{'version': 'v1', 'created': 'Mon, 11 Nov 2024 12:54:22 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 15:36:28 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Deroy', 'Aniket', ''], ['Maity', 'Subhankar', '']]","extracted_entities":"[{'text': 'GPT-3.5 Turbo', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-3.5 Turbo","similarity_score":0.6667470932}
{"id":2411.07521,"submitter":"Sina Bagheri Nezhad","authors":"Sina Bagheri Nezhad, Sayan Bandyapadhyay, Ameeta Agrawal","title":"Fair Summarization: Bridging Quality and Diversity in Extractive\n  Summaries","comments":"Accepted at AFLME@NeurIPS 2024 (non-archival) & C3NLP@NAACL 2025\n  (publication)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Fairness in multi-document summarization of user-generated content remains a\ncritical challenge in natural language processing (NLP). Existing summarization\nmethods often fail to ensure equitable representation across different social\ngroups, leading to biased outputs. In this paper, we introduce two novel\nmethods for fair extractive summarization: FairExtract, a clustering-based\napproach, and FairGPT, which leverages GPT-3.5-turbo with fairness constraints.\nWe evaluate these methods using Divsumm summarization dataset of White-aligned,\nHispanic, and African-American dialect tweets and compare them against relevant\nbaselines. The results obtained using a comprehensive set of summarization\nquality metrics such as SUPERT, BLANC, SummaQA, BARTScore, and UniEval, as well\nas a fairness metric F, demonstrate that FairExtract and FairGPT achieve\nsuperior fairness while maintaining competitive summarization quality.\nAdditionally, we introduce composite metrics (e.g., SUPERT+F, BLANC+F) that\nintegrate quality and fairness into a single evaluation framework, offering a\nmore nuanced understanding of the trade-offs between these objectives. Our code\nis available online.\n","versions":"[{'version': 'v1', 'created': 'Tue, 12 Nov 2024 03:37:53 GMT'}, {'version': 'v2', 'created': 'Wed, 13 Nov 2024 04:03:54 GMT'}, {'version': 'v3', 'created': 'Wed, 5 Feb 2025 23:34:44 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Mar 2025 16:55:48 GMT'}, {'version': 'v5', 'created': 'Tue, 18 Mar 2025 04:53:09 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Nezhad', 'Sina Bagheri', ''], ['Bandyapadhyay', 'Sayan', ''], ['Agrawal', 'Ameeta', '']]","extracted_entities":"[{'text': 'Fairness', 'label': 'Model Bias and Fairness'}, {'text': 'FairGPT', 'label': 'ChatGPT'}, {'text': 'GPT-3', 'label': 'GPT'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'FairGPT', 'label': 'ChatGPT'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"GPT","matched_keyword":"GPT-3","similarity_score":0.8771116138}
{"id":2411.09587,"submitter":"Akari Haga","authors":"Akari Haga, Akiyo Fukatsu, Miyu Oba, Arianna Bisazza, Yohei Oseki","title":"BabyLM Challenge: Exploring the Effect of Variation Sets on Language\n  Model Training Efficiency","comments":"Accepted by BabyLM challenge 2024 at CONLL 2024 (\n  https:\/\/aclanthology.org\/2024.conll-babylm.23 )","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While current large language models have achieved a remarkable success, their\ndata efficiency remains a challenge to overcome. Recently it has been suggested\nthat child-directed speech (CDS) can improve training data efficiency of modern\nlanguage models based on Transformer neural networks. However, it is not yet\nunderstood which specific properties of CDS are effective for training these\nmodels. In the context of the BabyLM Challenge, we focus on Variation Sets\n(VSs), sets of consecutive utterances expressing a similar intent with slightly\ndifferent words and structures, which are ubiquitous in CDS. To assess the\nimpact of VSs on training data efficiency, we augment CDS data with different\nproportions of artificial VSs and use these datasets to train an\nauto-regressive model, GPT-2. We find that the best proportion of VSs depends\non the evaluation benchmark: BLiMP and GLUE scores benefit from the presence of\nVSs, but EWOK scores do not. Additionally, the results vary depending on\nmultiple factors such as the number of epochs and the order of utterance\npresentation. Taken together, these findings suggest that VSs can have a\nbeneficial influence on language models, while leaving room for further\ninvestigation.\n","versions":"[{'version': 'v1', 'created': 'Thu, 14 Nov 2024 16:57:46 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 13:51:54 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Haga', 'Akari', ''], ['Fukatsu', 'Akiyo', ''], ['Oba', 'Miyu', ''], ['Bisazza', 'Arianna', ''], ['Oseki', 'Yohei', '']]","extracted_entities":"[{'text': 'Variation Sets', 'label': 'LLMs'}, {'text': 'VSs', 'label': 'LLMs'}, {'text': 'VSs', 'label': 'LLMs'}, {'text': 'GPT-2', 'label': 'GPT'}, {'text': 'VSs', 'label': 'LLMs'}, {'text': 'VSs', 'label': 'LLMs'}, {'text': 'VSs', 'label': 'LLMs'}]","assigned_concept":"GPT","matched_keyword":"GPT-2","similarity_score":0.8734456301}
{"id":2411.14299,"submitter":"Jitendra Bhandari","authors":"Jitendra Bhandari, Vineet Bhat, Yuheng He, Hamed Rahmani, Siddharth\n  Garg and Ramesh Karri","title":"Masala-CHAI: A Large-Scale SPICE Netlist Dataset for Analog Circuits by\n  Harnessing AI","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Masala-CHAI is the first fully automated framework leveraging large language\nmodels (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis\n(SPICE) netlists. It addresses a long-standing challenge in automating netlist\ngeneration for analog circuits within circuit design automation. Automating\nthis workflow could accelerate the creation of finetuned LLMs for analog\ncircuit design and verification. We identify key challenges in this automation\nand evaluate the multi-modal capabilities of state-of-the-art LLMs,\nparticularly GPT-4, to address these issues. We propose a three-step workflow\nto overcome current limitations: labeling analog circuits, prompt tuning, and\nnetlist verification. This approach aims to create an end-to-end SPICE netlist\ngenerator from circuit schematic images, tackling the long-standing hurdle of\naccurate netlist generation. Our framework demonstrates significant performance\nimprovements, tested on approximately 2,100 schematics of varying complexity.\nWe open-source this solution for community-driven development.\n","versions":"[{'version': 'v1', 'created': 'Thu, 21 Nov 2024 16:50:11 GMT'}, {'version': 'v2', 'created': 'Mon, 25 Nov 2024 20:42:40 GMT'}, {'version': 'v3', 'created': 'Tue, 4 Feb 2025 18:52:39 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 15:22:28 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Bhandari', 'Jitendra', ''], ['Bhat', 'Vineet', ''], ['He', 'Yuheng', ''], ['Rahmani', 'Hamed', ''], ['Garg', 'Siddharth', ''], ['Karri', 'Ramesh', '']]","extracted_entities":"[{'text': 'Masala-CHAI', 'label': 'Open-source LLMs'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'prompt tuning', 'label': 'Fine-tuning'}]","assigned_concept":"GPT","matched_keyword":"GPT-4","similarity_score":0.8572875857}
{"id":2412.04908,"submitter":"Zhijin Meng","authors":"Mohammed Althubyani, Zhijin Meng, Shengyuan Xie, Cha Seung, Imran\n  Razzak, Eduardo B. Sandoval, Baki Kocaballi, Francisco Cruz","title":"MERCI: Multimodal Emotional and peRsonal Conversational Interactions\n  Dataset","comments":"9 pages, 5 Figures, Rejected from International Conference of Human\n  Robot Interaction 2025, Melbourne, Australia","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC cs.ET cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  The integration of conversational agents into our daily lives has become\nincreasingly common, yet many of these agents cannot engage in deep\ninteractions with humans. Despite this, there is a noticeable shortage of\ndatasets that capture multimodal information from human-robot interaction\ndialogues. To address this gap, we have recorded a novel multimodal dataset\n(MERCI) that encompasses rich embodied interaction data. The process involved\nasking participants to complete a questionnaire and gathering their profiles on\nten topics, such as hobbies and favorite music. Subsequently, we initiated\nconversations between the robot and the participants, leveraging GPT-4 to\ngenerate contextually appropriate responses based on the participant's profile\nand emotional state, as determined by facial expression recognition and\nsentiment analysis. Automatic and user evaluations were conducted to assess the\noverall quality of the collected data. The results of both evaluations\nindicated a high level of naturalness, engagement, fluency, consistency, and\nrelevance in the conversation, as well as the robot's ability to provide\nempathetic responses. It is worth noting that the dataset is derived from\ngenuine interactions with the robot, involving participants who provided\npersonal information and conveyed actual emotions.\n","versions":"[{'version': 'v1', 'created': 'Fri, 6 Dec 2024 10:04:26 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 05:10:59 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Althubyani', 'Mohammed', ''], ['Meng', 'Zhijin', ''], ['Xie', 'Shengyuan', ''], ['Seung', 'Cha', ''], ['Razzak', 'Imran', ''], ['Sandoval', 'Eduardo B.', ''], ['Kocaballi', 'Baki', ''], ['Cruz', 'Francisco', '']]","extracted_entities":"[{'text': 'GPT-4', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4","similarity_score":0.8572875857}
{"id":2501.1819,"submitter":"Shuide Wen","authors":"ShuiDe Wen","title":"Economic Rationality under Specialization: Evidence of Decision Bias in\n  AI Agents","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  In the study by Chen et al. (2023) [01], the large language model GPT\ndemonstrated economic rationality comparable to or exceeding the average human\nlevel in tasks such as budget allocation and risk preference. Building on this\nfinding, this paper further incorporates specialized agents, such as\nbiotechnology experts and economists, for a horizontal comparison to explore\nwhether specialization can enhance or maintain economic rationality equivalent\nto that of GPT in similar decision-making scenarios. The results indicate that\nwhen agents invest more effort in specialized fields, their decision-making\nbehavior is more prone to 'rationality shift,' specifically manifested as\nincreased violations of GARP (Generalized Axiom of Revealed Preference),\ndecreased CCEI (Critical Cost Efficiency Index), and more significant decision\ndeviations under high-risk conditions. In contrast, GPT and more generalized\nbasic agents maintain a more stable and consistent level of rationality across\nmultiple tasks. This study reveals the inherent conflict between specialization\nand economic rationality, providing new insights for constructing AI\ndecision-making systems that balance specialization and generalization across\nvarious scenarios.\n","versions":"[{'version': 'v1', 'created': 'Thu, 30 Jan 2025 07:49:58 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 03:09:57 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wen', 'ShuiDe', '']]","extracted_entities":"[{'text': 'GPT', 'label': 'GPT'}, {'text': 'GPT', 'label': 'Large Language Model'}, {'text': 'GPT', 'label': 'Large Language Model'}]","assigned_concept":"GPT","matched_keyword":"GPT","similarity_score":1.0000001192}
{"id":2502.07601,"submitter":"Jiacong Xu","authors":"Jiacong Xu, Shao-Yuan Lo, Bardia Safaei, Vishal M. Patel, Isht Dwivedi","title":"Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large\n  Language Models","comments":"19 pages, 10 figures, accepted by CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Zero-Shot Anomaly Detection (ZSAD) is an emerging AD paradigm. Unlike the\ntraditional unsupervised AD setting that requires a large number of normal\nsamples to train a model, ZSAD is more practical for handling data-restricted\nreal-world scenarios. Recently, Multimodal Large Language Models (MLLMs) have\nshown revolutionary reasoning capabilities in various vision tasks. However,\nthe reasoning of image abnormalities remains underexplored due to the lack of\ncorresponding datasets and benchmarks. To facilitate research in AD &\nreasoning, we establish the first visual instruction tuning dataset,\nAnomaly-Instruct-125k, and the evaluation benchmark, VisA-D&R. Through\ninvestigation with our benchmark, we reveal that current MLLMs like GPT-4o\ncannot accurately detect and describe fine-grained anomalous details in images.\nTo address this, we propose Anomaly-OneVision (Anomaly-OV), the first\nspecialist visual assistant for ZSAD and reasoning. Inspired by human behavior\nin visual inspection, Anomaly-OV leverages a Look-Twice Feature Matching (LTFM)\nmechanism to adaptively select and emphasize abnormal visual tokens. Extensive\nexperiments demonstrate that Anomaly-OV achieves significant improvements over\nadvanced generalist models in both detection and reasoning. Extensions to\nmedical and 3D AD are provided for future study. The link to our project page:\nhttps:\/\/xujiacong.github.io\/Anomaly-OV\/\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Feb 2025 14:50:43 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 07:11:04 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Xu', 'Jiacong', ''], ['Lo', 'Shao-Yuan', ''], ['Safaei', 'Bardia', ''], ['Patel', 'Vishal M.', ''], ['Dwivedi', 'Isht', '']]","extracted_entities":"[{'text': 'ZSAD', 'label': 'Zero-shot Learning'}, {'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'ZSAD', 'label': 'Zero-shot Learning'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2502.1961,"submitter":"Matthew Toles","authors":"Matthew Toles, Nikhil Balwani, Rattandeep Singh, Valentina Giulia\n  Sartori Rodriguez, Zhou Yu","title":"Program Synthesis Dialog Agents for Interactive Decision-Making","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Many real-world eligibility problems, ranging from medical diagnosis to tax\nplanning, can be mapped to decision problems expressed in natural language,\nwherein a model must make a binary choice based on user features. Large-scale\ndomains such as legal codes or frequently updated funding opportunities render\nhuman annotation (e.g., web forms or decision trees) impractical, highlighting\nthe need for agents that can automatically assist in decision-making. Since\nrelevant information is often only known to the user, it is crucial that these\nagents ask the right questions. As agents determine when to terminate a\nconversation, they face a trade-off between accuracy and the number of\nquestions asked, a key metric for both user experience and cost. To evaluate\nthis task, we propose BeNYfits, a new benchmark for determining user\neligibility for multiple overlapping social benefits opportunities through\ninteractive decision-making. Our experiments show that current language models\nstruggle with frequent hallucinations, with GPT-4o scoring only 35.7 F1 using a\nReAct-style chain-of-thought. To address this, we introduce ProADA, a novel\napproach that leverages program synthesis to assist in decision-making by\nmapping dialog planning to a code generation problem and using gaps in\nstructured data to determine the best next action. Our agent, ProADA, improves\nthe F1 score to 55.6 while maintaining nearly the same number of dialog turns.\n","versions":"[{'version': 'v1', 'created': 'Wed, 26 Feb 2025 22:53:01 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 18:13:03 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Toles', 'Matthew', ''], ['Balwani', 'Nikhil', ''], ['Singh', 'Rattandeep', ''], ['Rodriguez', 'Valentina Giulia Sartori', ''], ['Yu', 'Zhou', '']]","extracted_entities":"[{'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'ReAct-style chain-of-thought', 'label': 'Chain of thought'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2503.06894,"submitter":"Xiaoqian Hu","authors":"Xiaoqian Hu","title":"A Deep Learning Approach for Augmenting Perceptional Understanding of\n  Histopathology Images","comments":"Accepted by International Conference on Semantic & Natural Language\n  Processing (SNLP 2025)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In Recent Years, Digital Technologies Have Made Significant Strides In\nAugmenting-Human-Health, Cognition, And Perception, Particularly Within The\nField Of Computational-Pathology. This Paper Presents A Novel Approach To\nEnhancing The Analysis Of Histopathology Images By Leveraging A\nMult-modal-Model That Combines Vision Transformers (Vit) With Gpt-2 For Image\nCaptioning. The Model Is Fine-Tuned On The Specialized Arch-Dataset, Which\nIncludes Dense Image Captions Derived From Clinical And Academic Resources, To\nCapture The Complexities Of Pathology Images Such As Tissue Morphologies,\nStaining Variations, And Pathological Conditions. By Generating Accurate,\nContextually Captions, The Model Augments The Cognitive Capabilities Of\nHealthcare Professionals, Enabling More Efficient Disease Classification,\nSegmentation, And Detection. The Model Enhances The Perception Of Subtle\nPathological Features In Images That Might Otherwise Go Unnoticed, Thereby\nImproving Diagnostic Accuracy. Our Approach Demonstrates The Potential For\nDigital Technologies To Augment Human Cognitive Abilities In Medical Image\nAnalysis, Providing Steps Toward More Personalized And Accurate Healthcare\nOutcomes.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Mar 2025 03:50:25 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 08:18:22 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Hu', 'Xiaoqian', '']]","extracted_entities":"[{'text': 'Vision Transformers', 'label': 'Transformers'}, {'text': 'Gpt-2', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"Gpt-2","similarity_score":0.8734456301}
{"id":2503.11509,"submitter":"Jonas Belouadi","authors":"Jonas Belouadi, Eddy Ilg, Margret Keuper, Hideki Tanaka, Masao\n  Utiyama, Raj Dabre, Steffen Eger, Simone Paolo Ponzetto","title":"TikZero: Zero-Shot Text-Guided Graphics Program Synthesis","comments":"Project page: https:\/\/github.com\/potamides\/DeTikZify","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  With the rise of generative AI, synthesizing figures from text captions\nbecomes a compelling application. However, achieving high geometric precision\nand editability requires representing figures as graphics programs in languages\nlike TikZ, and aligned training data (i.e., graphics programs with captions)\nremains scarce. Meanwhile, large amounts of unaligned graphics programs and\ncaptioned raster images are more readily available. We reconcile these\ndisparate data sources by presenting TikZero, which decouples graphics program\ngeneration from text understanding by using image representations as an\nintermediary bridge. It enables independent training on graphics programs and\ncaptioned images and allows for zero-shot text-guided graphics program\nsynthesis during inference. We show that our method substantially outperforms\nbaselines that can only operate with caption-aligned graphics programs.\nFurthermore, when leveraging caption-aligned graphics programs as a\ncomplementary training signal, TikZero matches or exceeds the performance of\nmuch larger models, including commercial systems like GPT-4o. Our code,\ndatasets, and select models are publicly available.\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 15:29:58 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 12:42:41 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Belouadi', 'Jonas', ''], ['Ilg', 'Eddy', ''], ['Keuper', 'Margret', ''], ['Tanaka', 'Hideki', ''], ['Utiyama', 'Masao', ''], ['Dabre', 'Raj', ''], ['Eger', 'Steffen', ''], ['Ponzetto', 'Simone Paolo', '']]","extracted_entities":"[{'text': 'zero-shot text-guided graphics program\\nsynthesis', 'label': 'Few-shot Learning'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'publicly available', 'label': 'Open-source LLMs'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2503.11951,"submitter":"Edward Chang","authors":"Edward Y. Chang, Longling Geng","title":"SagaLLM: Context Management, Validation, and Transaction Guarantees for\n  Multi-Agent LLM Planning","comments":"13 pages, 8 tables, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent LLM-based agent frameworks have demonstrated impressive capabilities\nin task delegation and workflow orchestration, but face significant challenges\nin maintaining context awareness and ensuring planning consistency. This paper\npresents SagaLLM, a structured multi-agent framework that addresses four\nfundamental limitations in current LLM approaches: inadequate self-validation,\ncontext narrowing, lacking transaction properties, and insufficient inter-agent\ncoordination. By implementing specialized context management agents and\nvalidation protocols, SagaLLM preserves critical constraints and state\ninformation throughout complex planning processes, enabling robust and\nconsistent decision-making even during disruptions. We evaluate our approach\nusing selected problems from the REALM benchmark, focusing on sequential and\nreactive planning scenarios that challenge both context retention and adaptive\nreasoning. Our experiments with state-of-the-art LLMs, Claude 3.7, DeepSeek R1,\nGPT-4o, and GPT-o1, demonstrate that while these models exhibit impressive\nreasoning capabilities, they struggle with maintaining global constraint\nawareness during complex planning tasks, particularly when adapting to\nunexpected changes. In contrast, the distributed cognitive architecture of\nSagaLLM shows significant improvements in planning consistency, constraint\nenforcement, and adaptation to disruptions in various scenarios.\n","versions":"[{'version': 'v1', 'created': 'Sat, 15 Mar 2025 01:43:03 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 05:00:47 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Chang', 'Edward Y.', ''], ['Geng', 'Longling', '']]","extracted_entities":"[{'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'GPT-o1', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2503.12769,"submitter":"Shenghao Fu","authors":"Shenghao Fu, Qize Yang, Yuan-Ming Li, Yi-Xing Peng, Kun-Yu Lin, Xihan\n  Wei, Jian-Fang Hu, Xiaohua Xie, Wei-Shi Zheng","title":"ViSpeak: Visual Instruction Feedback in Streaming Videos","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advances in Large Multi-modal Models (LMMs) are primarily focused on\noffline video understanding. Instead, streaming video understanding poses great\nchallenges to recent models due to its time-sensitive, omni-modal and\ninteractive characteristics. In this work, we aim to extend the streaming video\nunderstanding from a new perspective and propose a novel task named Visual\nInstruction Feedback in which models should be aware of visual contents and\nlearn to extract instructions from them. For example, when users wave their\nhands to agents, agents should recognize the gesture and start conversations\nwith welcome information. Thus, following instructions in visual modality\ngreatly enhances user-agent interactions. To facilitate research, we define\nseven key subtasks highly relevant to visual modality and collect the\nViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation.\nFurther, we propose the ViSpeak model, which is a SOTA streaming video\nunderstanding LMM with GPT-4o-level performance on various streaming video\nunderstanding benchmarks. After finetuning on our ViSpeak-Instruct dataset,\nViSpeak is equipped with basic visual instruction feedback ability, serving as\na solid baseline for future research.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 03:05:31 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Fu', 'Shenghao', ''], ['Yang', 'Qize', ''], ['Li', 'Yuan-Ming', ''], ['Peng', 'Yi-Xing', ''], ['Lin', 'Kun-Yu', ''], ['Wei', 'Xihan', ''], ['Hu', 'Jian-Fang', ''], ['Xie', 'Xiaohua', ''], ['Zheng', 'Wei-Shi', '']]","extracted_entities":"[{'text': 'Large Multi-modal Models', 'label': 'Large Language Model'}, {'text': 'GPT-4o-level', 'label': 'GPT'}, {'text': 'finetuning', 'label': 'Fine-tuning'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o-level","similarity_score":0.7187665105}
{"id":2503.13185,"submitter":"Cheng Wang","authors":"Dingning Liu, Cheng Wang, Peng Gao, Renrui Zhang, Xinzhu Ma, Yuan\n  Meng, Zhihui Wang","title":"3DAxisPrompt: Promoting the 3D Grounding and Reasoning in GPT-4o","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multimodal Large Language Models (MLLMs) exhibit impressive capabilities\nacross a variety of tasks, especially when equipped with carefully designed\nvisual prompts. However, existing studies primarily focus on logical reasoning\nand visual understanding, while the capability of MLLMs to operate effectively\nin 3D vision remains an ongoing area of exploration. In this paper, we\nintroduce a novel visual prompting method, called 3DAxisPrompt, to elicit the\n3D understanding capabilities of MLLMs in real-world scenes. More specifically,\nour method leverages the 3D coordinate axis and masks generated from the\nSegment Anything Model (SAM) to provide explicit geometric priors to MLLMs and\nthen extend their impressive 2D grounding and reasoning ability to real-world\n3D scenarios. Besides, we first provide a thorough investigation of the\npotential visual prompting formats and conclude our findings to reveal the\npotential and limits of 3D understanding capabilities in GPT-4o, as a\nrepresentative of MLLMs. Finally, we build evaluation environments with four\ndatasets, i.e., ScanRefer, ScanNet, FMB, and nuScene datasets, covering various\n3D tasks. Based on this, we conduct extensive quantitative and qualitative\nexperiments, which demonstrate the effectiveness of the proposed method.\nOverall, our study reveals that MLLMs, with the help of 3DAxisPrompt, can\neffectively perceive an object's 3D position in real-world scenarios.\nNevertheless, a single prompt engineering approach does not consistently\nachieve the best outcomes for all 3D tasks. This study highlights the\nfeasibility of leveraging MLLMs for 3D vision grounding\/reasoning with prompt\nengineering techniques.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 13:57:05 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Liu', 'Dingning', ''], ['Wang', 'Cheng', ''], ['Gao', 'Peng', ''], ['Zhang', 'Renrui', ''], ['Ma', 'Xinzhu', ''], ['Meng', 'Yuan', ''], ['Wang', 'Zhihui', '']]","extracted_entities":"[{'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': '3DAxisPrompt', 'label': 'Prompting'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': '3DAxisPrompt', 'label': 'Prompting'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2503.13262,"submitter":"Deyin Yi","authors":"Deyin Yi, Yihao Liu, Lang Cao, Mengyu Zhou, Haoyu Dong, Shi Han,\n  Dongmei Zhang","title":"TablePilot: Recommending Human-Preferred Tabular Data Analysis with\n  Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Tabular data analysis is crucial in many scenarios, yet efficiently\nidentifying the most relevant data analysis queries and results for a new table\nremains a significant challenge. The complexity of tabular data, diverse\nanalytical operations, and the demand for high-quality analysis make the\nprocess tedious. To address these challenges, we aim to recommend\nquery-code-result triplets tailored for new tables in tabular data analysis\nworkflows. In this paper, we present TablePilot, a pioneering tabular data\nanalysis framework leveraging large language models to autonomously generate\ncomprehensive and superior analytical results without relying on user profiles\nor prior interactions. The framework incorporates key designs in analysis\npreparation and analysis optimization to enhance accuracy. Additionally, we\npropose Rec-Align, a novel method to further improve recommendation quality and\nbetter align with human preferences. Experiments on DART, a dataset\nspecifically designed for comprehensive tabular data analysis recommendation,\ndemonstrate the effectiveness of our framework. Based on GPT-4o, the tuned\nTablePilot achieves 77.0% top-5 recommendation recall. Human evaluations\nfurther highlight its effectiveness in optimizing tabular data analysis\nworkflows.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:16:59 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 14:41:59 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 10:42:08 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Yi', 'Deyin', ''], ['Liu', 'Yihao', ''], ['Cao', 'Lang', ''], ['Zhou', 'Mengyu', ''], ['Dong', 'Haoyu', ''], ['Han', 'Shi', ''], ['Zhang', 'Dongmei', '']]","extracted_entities":"[{'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2503.13447,"submitter":"Qin Liu","authors":"Qin Liu, Wenxuan Zhou, Nan Xu, James Y. Huang, Fei Wang, Sheng Zhang,\n  Hoifung Poon, Muhao Chen","title":"MetaScale: Test-Time Scaling with Evolving Meta-Thoughts","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  One critical challenge for large language models (LLMs) for making complex\nreasoning is their reliance on matching reasoning patterns from training data,\ninstead of proactively selecting the most appropriate cognitive strategy to\nsolve a given task. Existing approaches impose fixed cognitive structures that\nenhance performance in specific tasks but lack adaptability across diverse\nscenarios. To address this limitation, we introduce METASCALE, a test-time\nscaling framework based on meta-thoughts -- adaptive thinking strategies\ntailored to each task. METASCALE initializes a pool of candidate meta-thoughts,\nthen iteratively selects and evaluates them using a multi-armed bandit\nalgorithm with upper confidence bound selection, guided by a reward model. To\nfurther enhance adaptability, a genetic algorithm evolves high-reward\nmeta-thoughts, refining and extending the strategy pool over time. By\ndynamically proposing and optimizing meta-thoughts at inference time, METASCALE\nimproves both accuracy and generalization across a wide range of tasks.\nExperimental results demonstrate that MetaScale consistently outperforms\nstandard inference approaches, achieving an 11% performance gain in win rate on\nArena-Hard for GPT-4o, surpassing o1-mini by 0.9% under style control. Notably,\nMETASCALE scales more effectively with increasing sampling budgets and produces\nmore structured, expert-level responses.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:59:54 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Liu', 'Qin', ''], ['Zhou', 'Wenxuan', ''], ['Xu', 'Nan', ''], ['Huang', 'James Y.', ''], ['Wang', 'Fei', ''], ['Zhang', 'Sheng', ''], ['Poon', 'Hoifung', ''], ['Chen', 'Muhao', '']]","extracted_entities":"[{'text': 'METASCALE', 'label': 'LLM'}, {'text': 'METASCALE', 'label': 'LLM'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'METASCALE', 'label': 'LLM'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2503.13687,"submitter":"Venera Adanova","authors":"A. Selvio\\u{g}lu, V. Adanova and M. Atagoziev","title":"Feature Extraction and Analysis for GPT-Generated Text","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  With the rise of advanced natural language models like GPT, distinguishing\nbetween human-written and GPT-generated text has become increasingly\nchallenging and crucial across various domains, including academia. The\nlong-standing issue of plagiarism has grown more pressing, now compounded by\nconcerns about the authenticity of information, as it is not always clear\nwhether the presented facts are genuine or fabricated. In this paper, we\npresent a comprehensive study of feature extraction and analysis for\ndifferentiating between human-written and GPT-generated text. By applying\nmachine learning classifiers to these extracted features, we evaluate the\nsignificance of each feature in detection. Our results demonstrate that human\nand GPT-generated texts exhibit distinct writing styles, which can be\neffectively captured by our features. Given sufficiently long text, the two can\nbe differentiated with high accuracy.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 19:52:43 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Selvio\u011flu', 'A.', ''], ['Adanova', 'V.', ''], ['Atagoziev', 'M.', '']]","extracted_entities":"[{'text': 'GPT', 'label': 'GPT'}, {'text': 'GPT-generated', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT","similarity_score":1.0000001192}
{"id":2503.13795,"submitter":"Konstantin Burlachenko","authors":"Konstantin Burlachenko, Peter Richt\\'arik","title":"BurTorch: Revisiting Training from First Principles by Coupling\n  Autodiff, Math Optimization, and Systems","comments":"46 pages, 7 figures, 19 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.MS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this work, we introduce BurTorch, a compact high-performance framework\ndesigned to optimize Deep Learning (DL) training on single-node workstations\nthrough an exceptionally efficient CPU-based backpropagation (Rumelhart et al.,\n1986; Linnainmaa, 1970) implementation. Although modern DL frameworks rely on\ncompilerlike optimizations internally, BurTorch takes a different path. It\nadopts a minimalist design and demonstrates that, in these circumstances,\nclassical compiled programming languages can play a significant role in DL\nresearch. By eliminating the overhead of large frameworks and making efficient\nimplementation choices, BurTorch achieves orders-of-magnitude improvements in\nperformance and memory efficiency when computing $\\nabla f(x)$ on a CPU.\nBurTorch features a compact codebase designed to achieve two key goals\nsimultaneously. First, it provides a user experience similar to script-based\nprogramming environments. Second, it dramatically minimizes runtime overheads.\nIn large DL frameworks, the primary source of memory overhead for relatively\nsmall computation graphs $f(x)$ is due to feature-heavy implementations. We\nbenchmarked BurTorch against widely used DL frameworks in their execution\nmodes: JAX (Bradbury et al., 2018), PyTorch (Paszke et al., 2019), TensorFlow\n(Abadi et al., 2016); and several standalone libraries: Autograd (Maclaurin et\nal., 2015), Micrograd (Karpathy, 2020), Apple MLX (Hannun et al., 2023). For\nsmall compute graphs, BurTorch outperforms best-practice solutions by up to\n$\\times 2000$ in runtime and reduces memory consumption by up to $\\times 3500$.\nFor a miniaturized GPT-3 model (Brown et al., 2020), BurTorch achieves up to a\n$\\times 20$ speedup and reduces memory up to $\\times 80$ compared to PyTorch.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 00:52:12 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Burlachenko', 'Konstantin', ''], ['Richt\u00e1rik', 'Peter', '']]","extracted_entities":"[{'text': 'GPT-3', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-3","similarity_score":0.8771116138}
{"id":2503.13956,"submitter":"Changli Tang","authors":"Yixuan Li, Changli Tang, Jimin Zhuang, Yudong Yang, Guangzhi Sun, Wei\n  Li, Zejun Ma, Chao Zhang","title":"Improving LLM Video Understanding with 16 Frames Per Second","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Human vision is dynamic and continuous. However, in video understanding with\nmultimodal large language models (LLMs), existing methods primarily rely on\nstatic features extracted from images sampled at a fixed low frame rate of\nframe-per-second (FPS) $\\leqslant$2, leading to critical visual information\nloss. In this paper, we introduce F-16, the first multimodal LLM designed for\nhigh-frame-rate video understanding. By increasing the frame rate to 16 FPS and\ncompressing visual tokens within each 1-second clip, F-16 efficiently captures\ndynamic visual features while preserving key semantic information. Experimental\nresults demonstrate that higher frame rates considerably enhance video\nunderstanding across multiple benchmarks, providing a new approach to improving\nvideo LLMs beyond scaling model size or training data. F-16 achieves\nstate-of-the-art performance among 7-billion-parameter video LLMs on both\ngeneral and fine-grained video understanding benchmarks, such as Video-MME and\nTemporalBench. Furthermore, F-16 excels in complex spatiotemporal tasks,\nincluding high-speed sports analysis (\\textit{e.g.}, basketball, football,\ngymnastics, and diving), outperforming SOTA proprietary visual models like\nGPT-4o and Gemini-1.5-pro. Additionally, we introduce a novel decoding method\nfor F-16 that enables highly efficient low-frame-rate inference without\nrequiring model retraining. Upon acceptance, we will release the source code,\nmodel checkpoints, and data.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 06:48:08 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Li', 'Yixuan', ''], ['Tang', 'Changli', ''], ['Zhuang', 'Jimin', ''], ['Yang', 'Yudong', ''], ['Sun', 'Guangzhi', ''], ['Li', 'Wei', ''], ['Ma', 'Zejun', ''], ['Zhang', 'Chao', '']]","extracted_entities":"[{'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2503.1419,"submitter":"Mingtian Tan","authors":"Mingtian Tan, Mike A. Merrill, Zack Gottesman, Tim Althoff, David\n  Evans, Tom Hartvigsen","title":"Inferring Event Descriptions from Time Series with Language Models","comments":"17 pages, 9 Figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Time series data measure how environments change over time and drive\ndecision-making in critical domains like finance and healthcare. When analyzing\ntime series, we often seek to understand the underlying events occurring in the\nmeasured environment. For example, one might ask: What caused a sharp drop in\nthe stock price? Events are often described with natural language, so we\nconduct the first study of whether Large Language Models (LLMs) can infer\nnatural language events from time series. We curate a new benchmark featuring\nwin probabilities collected from 4,200 basketball and American football games,\nfeaturing 1.7M timesteps with real value data and corresponding natural\nlanguage events. Building on the recent wave of using LLMs on time series, we\nevaluate 16 LLMs and find that they demonstrate promising abilities to infer\nevents from time series data. The open-weights DeepSeek-R1 32B model\noutperforms proprietary models like GPT-4o. Despite this impressive initial\nperformance, we also find clear avenues to improve recent models, as we\nidentify failures when altering the provided context, event sequence lengths,\nand evaluation strategy. (All resources needed to reproduce our work are\navailable: https:\/\/github.com\/BennyTMT\/GAMETime)\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 12:07:33 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Tan', 'Mingtian', ''], ['Merrill', 'Mike A.', ''], ['Gottesman', 'Zack', ''], ['Althoff', 'Tim', ''], ['Evans', 'David', ''], ['Hartvigsen', 'Tom', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2503.14281,"submitter":"Adam Storek","authors":"Adam \\v{S}torek, Mukur Gupta, Noopur Bhatt, Aditya Gupta, Janie Kim,\n  Prashast Srivastava, Suman Jana","title":"XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding\n  Assistants","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.LG cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  AI coding assistants are widely used for tasks like code generation, bug\ndetection, and comprehension. These tools now require large and complex\ncontexts, automatically sourced from various origins$\\unicode{x2014}$across\nfiles, projects, and contributors$\\unicode{x2014}$forming part of the prompt\nfed to underlying LLMs. This automatic context-gathering introduces new\nvulnerabilities, allowing attackers to subtly poison input to compromise the\nassistant's outputs, potentially generating vulnerable code, overlooking flaws,\nor introducing critical errors. We propose a novel attack, Cross-Origin Context\nPoisoning (XOXO), that is particularly challenging to detect as it relies on\nadversarial code modifications that are semantically equivalent. Traditional\nprogram analysis techniques struggle to identify these correlations since the\nsemantics of the code remain correct, making it appear legitimate. This allows\nattackers to manipulate code assistants into producing incorrect outputs,\nincluding vulnerabilities or backdoors, while shifting the blame to the victim\ndeveloper or tester. We introduce a novel, task-agnostic black-box attack\nalgorithm GCGS that systematically searches the transformation space using a\nCayley Graph, achieving an 83.09% attack success rate on average across five\ntasks and eleven models, including GPT-4o and Claude 3.5 Sonnet v2 used by many\npopular AI coding assistants. Furthermore, existing defenses, including\nadversarial fine-tuning, are ineffective against our attack, underscoring the\nneed for new security measures in LLM-powered coding tools.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 14:20:54 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['\u0160torek', 'Adam', ''], ['Gupta', 'Mukur', ''], ['Bhatt', 'Noopur', ''], ['Gupta', 'Aditya', ''], ['Kim', 'Janie', ''], ['Srivastava', 'Prashast', ''], ['Jana', 'Suman', '']]","extracted_entities":"[{'text': 'prompt', 'label': 'Prompting'}, {'text': 'Cayley Graph', 'label': 'Embedding'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'adversarial fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2503.14443,"submitter":"Yaroslav Zharov","authors":"Aleksandra Eliseeva, Alexander Kovrigin, Ilia Kholkin, Egor Bogomolov,\n  Yaroslav Zharov","title":"EnvBench: A Benchmark for Automated Environment Setup","comments":"Accepted at the DL4Code workshop at ICLR'25","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.SE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advances in Large Language Models (LLMs) have enabled researchers to\nfocus on practical repository-level tasks in software engineering domain. In\nthis work, we consider a cornerstone task for automating work with software\nrepositories-environment setup, i.e., a task of configuring a\nrepository-specific development environment on a system. Existing studies on\nenvironment setup introduce innovative agentic strategies, but their evaluation\nis often based on small datasets that may not capture the full range of\nconfiguration challenges encountered in practice. To address this gap, we\nintroduce a comprehensive environment setup benchmark EnvBench. It encompasses\n329 Python and 665 JVM-based (Java, Kotlin) repositories, with a focus on\nrepositories that present genuine configuration challenges, excluding projects\nthat can be fully configured by simple deterministic scripts. To enable further\nbenchmark extension and usage for model tuning, we implement two automatic\nmetrics: a static analysis check for missing imports in Python and a\ncompilation check for JVM languages. We demonstrate the applicability of our\nbenchmark by evaluating three environment setup approaches, including a simple\nzero-shot baseline and two agentic workflows, that we test with two powerful\nLLM backbones, GPT-4o and GPT-4o-mini. The best approach manages to\nsuccessfully configure 6.69% repositories for Python and 29.47% repositories\nfor JVM, suggesting that EnvBench remains challenging for current approaches.\nOur benchmark suite is publicly available at\nhttps:\/\/github.com\/JetBrains-Research\/EnvBench. The dataset and experiment\ntrajectories are available at https:\/\/jb.gg\/envbench.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:19:12 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Eliseeva', 'Aleksandra', ''], ['Kovrigin', 'Alexander', ''], ['Kholkin', 'Ilia', ''], ['Bogomolov', 'Egor', ''], ['Zharov', 'Yaroslav', '']]","extracted_entities":"[{'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'GPT-4o-mini', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2503.14484,"submitter":"Fardin Saad","authors":"Fardin Saad and Pradeep K. Murukannaiah and Munindar P. Singh","title":"Gricean Norms as a Basis for Effective Collaboration","comments":"Accepted to AAMAS 2025. 8 pages (excl. references), 9 figures\/tables.\n  (Appendix: 5 pages, 6 figures\/tables). Code available at:\n  https:\/\/github.com\/fardinsaad\/Gricean-Norms","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.MA cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Effective human-AI collaboration hinges not only on the AI agent's ability to\nfollow explicit instructions but also on its capacity to navigate ambiguity,\nincompleteness, invalidity, and irrelevance in communication. Gricean\nconversational and inference norms facilitate collaboration by aligning unclear\ninstructions with cooperative principles. We propose a normative framework that\nintegrates Gricean norms and cognitive frameworks -- common ground, relevance\ntheory, and theory of mind -- into large language model (LLM) based agents. The\nnormative framework adopts the Gricean maxims of quantity, quality, relation,\nand manner, along with inference, as Gricean norms to interpret unclear\ninstructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within\nthis framework, we introduce Lamoids, GPT-4 powered agents designed to\ncollaborate with humans. To assess the influence of Gricean norms in human-AI\ncollaboration, we evaluate two versions of a Lamoid: one with norms and one\nwithout. In our experiments, a Lamoid collaborates with a human to achieve\nshared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear\nand unclear natural language instructions. Our results reveal that the Lamoid\nwith Gricean norms achieves higher task accuracy and generates clearer, more\naccurate, and contextually relevant responses than the Lamoid without norms.\nThis improvement stems from the normative framework, which enhances the agent's\npragmatic reasoning, fostering effective human-AI collaboration and enabling\ncontext-aware communication in LLM-based agents.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:54:14 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Saad', 'Fardin', ''], ['Murukannaiah', 'Pradeep K.', ''], ['Singh', 'Munindar P.', '']]","extracted_entities":"[{'text': 'Lamoids', 'label': 'Llama'}, {'text': 'GPT-4', 'label': 'GPT'}, {'text': 'Lamoid', 'label': 'Llama'}, {'text': 'Lamoid', 'label': 'Llama'}, {'text': 'Lamoid', 'label': 'Llama'}, {'text': 'Lamoid', 'label': 'Llama'}]","assigned_concept":"GPT","matched_keyword":"GPT-4","similarity_score":0.8572875857}
{"id":2503.14495,"submitter":"Jiacheng Guo","authors":"Jiacheng Guo, Yue Wu, Jiahao Qiu, Kaixuan Huang, Xinzhe Juan, Ling\n  Yang, Mengdi Wang","title":"Temporal Consistency for LLM Reasoning Process Error Identification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Verification is crucial for effective mathematical reasoning. We present a\nnew temporal consistency method where verifiers iteratively refine their\njudgments based on the previous assessment. Unlike one-round verification or\nmulti-model debate approaches, our method leverages consistency in a sequence\nof self-reflection actions to improve verification accuracy. Empirical\nevaluations across diverse mathematical process error identification benchmarks\n(Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements\nover baseline methods. When applied to the recent DeepSeek R1 distilled models,\nour method demonstrates strong performance, enabling 7B\/8B distilled models to\noutperform all 70B\/72B models and GPT-4o on ProcessBench. Notably, the\ndistilled 14B model with our method achieves performance comparable to\nDeepseek-R1. Our codes are available at\nhttps:\/\/github.com\/jcguo123\/Temporal-Consistency\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:58:28 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Guo', 'Jiacheng', ''], ['Wu', 'Yue', ''], ['Qiu', 'Jiahao', ''], ['Huang', 'Kaixuan', ''], ['Juan', 'Xinzhe', ''], ['Yang', 'Ling', ''], ['Wang', 'Mengdi', '']]","extracted_entities":"[{'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2503.14933,"submitter":"Yi Luo","authors":"Yi Luo, Hamed Hooshangnejad, Xue Feng, Gaofeng Huang, Xiaojian Chen,\n  Rui Zhang, Quan Chen, Wil Ngwa, and Kai Ding","title":"A Language Vision Model Approach for Automated Tumor Contouring in\n  Radiation Oncology","comments":"19 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV physics.med-ph","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Background: Lung cancer ranks as the leading cause of cancer-related\nmortality worldwide. The complexity of tumor delineation, crucial for radiation\ntherapy, requires expertise often unavailable in resource-limited settings.\nArtificial Intelligence(AI), particularly with advancements in deep learning\n(DL) and natural language processing (NLP), offers potential solutions yet is\nchallenged by high false positive rates. Purpose: The Oncology Contouring\nCopilot (OCC) system is developed to leverage oncologist expertise for precise\ntumor contouring using textual descriptions, aiming to increase the efficiency\nof oncological workflows by combining the strengths of AI with human oversight.\nMethods: Our OCC system initially identifies nodule candidates from CT scans.\nEmploying Language Vision Models (LVMs) like GPT-4V, OCC then effectively\nreduces false positives with clinical descriptive texts, merging textual and\nvisual data to automate tumor delineation, designed to elevate the quality of\noncology care by incorporating knowledge from experienced domain experts.\nResults: Deployments of the OCC system resulted in a significant reduction in\nthe false discovery rate by 35.0%, a 72.4% decrease in false positives per\nscan, and an F1-score of 0.652 across our dataset for unbiased evaluation.\nConclusions: OCC represents a significant advance in oncology care,\nparticularly through the use of the latest LVMs to improve contouring results\nby (1) streamlining oncology treatment workflows by optimizing tumor\ndelineation, reducing manual processes; (2) offering a scalable and intuitive\nframework to reduce false positives in radiotherapy planning using LVMs; (3)\nintroducing novel medical language vision prompt techniques to minimize LVMs\nhallucinations with ablation study, and (4) conducting a comparative analysis\nof LVMs, highlighting their potential in addressing medical language vision\nchallenges.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 06:41:37 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Luo', 'Yi', ''], ['Hooshangnejad', 'Hamed', ''], ['Feng', 'Xue', ''], ['Huang', 'Gaofeng', ''], ['Chen', 'Xiaojian', ''], ['Zhang', 'Rui', ''], ['Chen', 'Quan', ''], ['Ngwa', 'Wil', ''], ['Ding', 'Kai', '']]","extracted_entities":"[{'text': 'GPT-4V', 'label': 'GPT'}, {'text': 'medical language vision prompt techniques', 'label': 'Prompting'}]","assigned_concept":"GPT","matched_keyword":"GPT-4V","similarity_score":0.7028234005}
{"id":2503.15024,"submitter":"Jin Wang","authors":"Jin Wang, Chenghui Lv, Xian Li, Shichao Dong, Huadong Li, kelu Yao,\n  Chao Li, Wenqi Shao, Ping Luo","title":"Forensics-Bench: A Comprehensive Forgery Detection Benchmark Suite for\n  Large Vision Language Models","comments":"31 pages, 19 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recently, the rapid development of AIGC has significantly boosted the\ndiversities of fake media spread in the Internet, posing unprecedented threats\nto social security, politics, law, and etc. To detect the ever-increasingly\ndiverse malicious fake media in the new era of AIGC, recent studies have\nproposed to exploit Large Vision Language Models (LVLMs) to design robust\nforgery detectors due to their impressive performance on a wide range of\nmultimodal tasks. However, it still lacks a comprehensive benchmark designed to\ncomprehensively assess LVLMs' discerning capabilities on forgery media. To fill\nthis gap, we present Forensics-Bench, a new forgery detection evaluation\nbenchmark suite to assess LVLMs across massive forgery detection tasks,\nrequiring comprehensive recognition, location and reasoning capabilities on\ndiverse forgeries. Forensics-Bench comprises 63,292 meticulously curated\nmulti-choice visual questions, covering 112 unique forgery detection types from\n5 perspectives: forgery semantics, forgery modalities, forgery tasks, forgery\ntypes and forgery models. We conduct thorough evaluations on 22 open-sourced\nLVLMs and 3 proprietary models GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet,\nhighlighting the significant challenges of comprehensive forgery detection\nposed by Forensics-Bench. We anticipate that Forensics-Bench will motivate the\ncommunity to advance the frontier of LVLMs, striving for all-around forgery\ndetectors in the era of AIGC. The deliverables will be updated at\nhttps:\/\/Forensics-Bench.github.io\/.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 09:21:44 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Wang', 'Jin', ''], ['Lv', 'Chenghui', ''], ['Li', 'Xian', ''], ['Dong', 'Shichao', ''], ['Li', 'Huadong', ''], ['Yao', 'kelu', ''], ['Li', 'Chao', ''], ['Shao', 'Wenqi', ''], ['Luo', 'Ping', '']]","extracted_entities":"[{'text': 'Large Vision Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'Gemini 1.5 Pro', 'label': 'GPT model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2503.15234,"submitter":"Wenlong Yu","authors":"Wenlong Yu, Qilong Wang, Chuang Liu, Dong Li, Qinghua Hu","title":"CoE: Chain-of-Explanation via Automatic Visual Concept Circuit\n  Description and Polysemanticity Quantification","comments":"Accepted by CVPR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Explainability is a critical factor influencing the wide deployment of deep\nvision models (DVMs). Concept-based post-hoc explanation methods can provide\nboth global and local insights into model decisions. However, current methods\nin this field face challenges in that they are inflexible to automatically\nconstruct accurate and sufficient linguistic explanations for global concepts\nand local circuits. Particularly, the intrinsic polysemanticity in semantic\nVisual Concepts (VCs) impedes the interpretability of concepts and DVMs, which\nis underestimated severely. In this paper, we propose a Chain-of-Explanation\n(CoE) approach to address these issues. Specifically, CoE automates the\ndecoding and description of VCs to construct global concept explanation\ndatasets. Further, to alleviate the effect of polysemanticity on model\nexplainability, we design a concept polysemanticity disentanglement and\nfiltering mechanism to distinguish the most contextually relevant concept\natoms. Besides, a Concept Polysemanticity Entropy (CPE), as a measure of model\ninterpretability, is formulated to quantify the degree of concept uncertainty.\nThe modeling of deterministic concepts is upgraded to uncertain concept atom\ndistributions. Finally, CoE automatically enables linguistic local explanations\nof the decision-making process of DVMs by tracing the concept circuit. GPT-4o\nand human-based experiments demonstrate the effectiveness of CPE and the\nsuperiority of CoE, achieving an average absolute improvement of 36% in terms\nof explainability scores.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 14:13:02 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Yu', 'Wenlong', ''], ['Wang', 'Qilong', ''], ['Liu', 'Chuang', ''], ['Li', 'Dong', ''], ['Hu', 'Qinghua', '']]","extracted_entities":"[{'text': 'CoE', 'label': 'contextual Embedding'}, {'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'CoE', 'label': 'Chain of thought'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2503.15342,"submitter":"Ritabrata Chakraborty","authors":"Ritabrata Chakraborty, Rajatsubhra Chakraborty, Ali Khaleghi Rahimian\n  and Thomas MacDougall","title":"TruthLens:A Training-Free Paradigm for DeepFake Detection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The proliferation of synthetic images generated by advanced AI models poses\nsignificant challenges in identifying and understanding manipulated visual\ncontent. Current fake image detection methods predominantly rely on binary\nclassification models that focus on accuracy while often neglecting\ninterpretability, leaving users without clear insights into why an image is\ndeemed real or fake. To bridge this gap, we introduce TruthLens, a novel\ntraining-free framework that reimagines deepfake detection as a visual\nquestion-answering (VQA) task. TruthLens utilizes state-of-the-art large\nvision-language models (LVLMs) to observe and describe visual artifacts and\ncombines this with the reasoning capabilities of large language models (LLMs)\nlike GPT-4 to analyze and aggregate evidence into informed decisions. By\nadopting a multimodal approach, TruthLens seamlessly integrates visual and\nsemantic reasoning to not only classify images as real or fake but also provide\ninterpretable explanations for its decisions. This transparency enhances trust\nand provides valuable insights into the artifacts that signal synthetic\ncontent. Extensive evaluations demonstrate that TruthLens outperforms\nconventional methods, achieving high accuracy on challenging datasets while\nmaintaining a strong emphasis on explainability. By reframing deepfake\ndetection as a reasoning-driven process, TruthLens establishes a new paradigm\nin combating synthetic media, combining cutting-edge performance with\ninterpretability to address the growing threats of visual disinformation.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:41:32 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Chakraborty', 'Ritabrata', ''], ['Chakraborty', 'Rajatsubhra', ''], ['Rahimian', 'Ali Khaleghi', ''], ['MacDougall', 'Thomas', '']]","extracted_entities":"[{'text': 'GPT-4', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4","similarity_score":0.8572875857}
{"id":2503.15478,"submitter":"Yifei Zhou Mr.","authors":"Yifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine,\n  Sainbayar Sukhbaatar, Xian Li","title":"SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning\n  Tasks","comments":"29 pages, 16 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language model (LLM) agents need to perform multi-turn interactions in\nreal-world tasks. However, existing multi-turn RL algorithms for optimizing LLM\nagents fail to perform effective credit assignment over multiple turns while\nleveraging the generalization capabilities of LLMs and it remains unclear how\nto develop such algorithms. To study this, we first introduce a new benchmark,\nColBench, where an LLM agent interacts with a human collaborator over multiple\nturns to solve realistic tasks in backend programming and frontend design.\nBuilding on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with\nStep-WisE Evaluation from Training-time information), that uses a carefully\ndesigned optimization objective to train a critic model with access to\nadditional training-time information. The critic provides step-level rewards\nfor improving the policy model. Our experiments demonstrate that SWEET-RL\nachieves a 6% absolute improvement in success and win rates on ColBench\ncompared to other state-of-the-art multi-turn RL algorithms, enabling\nLlama-3.1-8B to match or exceed the performance of GPT4-o in realistic\ncollaborative content creation.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:55:08 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhou', 'Yifei', ''], ['Jiang', 'Song', ''], ['Tian', 'Yuandong', ''], ['Weston', 'Jason', ''], ['Levine', 'Sergey', ''], ['Sukhbaatar', 'Sainbayar', ''], ['Li', 'Xian', '']]","extracted_entities":"[{'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'GPT4-o', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT4-o","similarity_score":0.7593444586}
{"id":2503.15579,"submitter":"Xingxuan Zhang","authors":"Xingxuan Zhang, Haoran Wang, Jiansheng Li, Yuan Xue, Shikai Guan,\n  Renzhe Xu, Hao Zou, Han Yu, Peng Cui","title":"Understanding the Generalization of In-Context Learning in Transformers:\n  An Empirical Study","comments":"32 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models (LLMs) like GPT-4 and LLaMA-3 utilize the powerful\nin-context learning (ICL) capability of Transformer architecture to learn on\nthe fly from limited examples. While ICL underpins many LLM applications, its\nfull potential remains hindered by a limited understanding of its\ngeneralization boundaries and vulnerabilities. We present a systematic\ninvestigation of transformers' generalization capability with ICL relative to\ntraining data coverage by defining a task-centric framework along three\ndimensions: inter-problem, intra-problem, and intra-task generalization.\nThrough extensive simulation and real-world experiments, encompassing tasks\nsuch as function fitting, API calling, and translation, we find that\ntransformers lack inter-problem generalization with ICL, but excel in\nintra-task and intra-problem generalization. When the training data includes a\ngreater variety of mixed tasks, it significantly enhances the generalization\nability of ICL on unseen tasks and even on known simple tasks. This guides us\nin designing training data to maximize the diversity of tasks covered and to\ncombine different tasks whenever possible, rather than solely focusing on the\ntarget task for testing.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 13:40:45 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zhang', 'Xingxuan', ''], ['Wang', 'Haoran', ''], ['Li', 'Jiansheng', ''], ['Xue', 'Yuan', ''], ['Guan', 'Shikai', ''], ['Xu', 'Renzhe', ''], ['Zou', 'Hao', ''], ['Yu', 'Han', ''], ['Cui', 'Peng', '']]","extracted_entities":"[{'text': 'GPT-4', 'label': 'GPT'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'Few-shot Learning'}, {'text': 'ICL', 'label': 'Few-shot Learning'}, {'text': 'ICL', 'label': 'contextual Embedding'}]","assigned_concept":"GPT","matched_keyword":"GPT-4","similarity_score":0.8572875857}
{"id":2503.15726,"submitter":"Joseph Emmanuel Dayo","authors":"Joseph Emmanuel DL Dayo, Michel Onasis S. Ogbinar and Prospero C.\n  Naval Jr","title":"Reinforcement Learning Environment with LLM-Controlled Adversary in D&D\n  5th Edition Combat","comments":"Preprint. Submitted to the 31st International Conference on Neural\n  Information Processing (ICONIP 2024)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The objective of this study is to design and implement a reinforcement\nlearning (RL) environment using D\\&D 5E combat scenarios to challenge smaller\nRL agents through interaction with a robust adversarial agent controlled by\nadvanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research\nemploys Deep Q-Networks (DQN) for the smaller agents, creating a testbed for\nstrategic AI development that also serves as an educational tool by simulating\ndynamic and unpredictable combat scenarios. We successfully integrated\nsophisticated language models into the RL framework, enhancing strategic\ndecision-making processes. Our results indicate that while RL agents generally\noutperform LLM-controlled adversaries in standard metrics, the strategic depth\nprovided by LLMs significantly enhances the overall AI capabilities in this\ncomplex, rule-based setting. The novelty of our approach and its implications\nfor mastering intricate environments and developing adaptive strategies are\ndiscussed, alongside potential innovations in AI-driven interactive\nsimulations. This paper aims to demonstrate how integrating LLMs can create\nmore robust and adaptable AI systems, providing valuable insights for further\nresearch and educational applications.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 22:48:20 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Dayo', 'Joseph Emmanuel DL', ''], ['Ogbinar', 'Michel Onasis S.', ''], ['Naval', 'Prospero C.', 'Jr']]","extracted_entities":"[{'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4o","similarity_score":0.7853888273}
{"id":2503.16024,"submitter":"Ruihan Yang","authors":"Ruihan Yang, Fanghua Ye, Jian Li, Siyu Yuan, Yikai Zhang, Zhaopeng Tu,\n  Xiaolong Li, Deqing Yang","title":"The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided\n  Improvement","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs) have recently transformed from text-based\nassistants to autonomous agents capable of planning, reasoning, and iteratively\nimproving their actions. While numerical reward signals and verifiers can\neffectively rank candidate actions, they often provide limited contextual\nguidance. In contrast, natural language feedback better aligns with the\ngenerative capabilities of LLMs, providing richer and more actionable\nsuggestions. However, parsing and implementing this feedback effectively can be\nchallenging for LLM-based agents. In this work, we introduce Critique-Guided\nImprovement (CGI), a novel two-player framework, comprising an actor model that\nexplores an environment and a critic model that generates detailed nature\nlanguage feedback. By training the critic to produce fine-grained assessments\nand actionable revisions, and the actor to utilize these critiques, our\napproach promotes more robust exploration of alternative strategies while\navoiding local optima. Experiments in three interactive environments show that\nCGI outperforms existing baselines by a substantial margin. Notably, even a\nsmall critic model surpasses GPT-4 in feedback quality. The resulting actor\nachieves state-of-the-art performance, demonstrating the power of explicit\niterative guidance to enhance decision-making in LLM-based agents.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 10:42:33 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Yang', 'Ruihan', ''], ['Ye', 'Fanghua', ''], ['Li', 'Jian', ''], ['Yuan', 'Siyu', ''], ['Zhang', 'Yikai', ''], ['Tu', 'Zhaopeng', ''], ['Li', 'Xiaolong', ''], ['Yang', 'Deqing', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GPT-4', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT-4","similarity_score":0.8572875857}
{"id":2503.16148,"submitter":"Indira Sen","authors":"Mats Faulborn, Indira Sen, Max Pellert, Andreas Spitz, and David\n  Garcia","title":"Only a Little to the Left: A Theory-grounded Measure of Political Bias\n  in Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Prompt-based language models like GPT4 and LLaMa have been used for a wide\nvariety of use cases such as simulating agents, searching for information, or\nfor content analysis. For all of these applications and others, political\nbiases in these models can affect their performance. Several researchers have\nattempted to study political bias in language models using evaluation suites\nbased on surveys, such as the Political Compass Test (PCT), often finding a\nparticular leaning favored by these models. However, there is some variation in\nthe exact prompting techniques, leading to diverging findings and most research\nrelies on constrained-answer settings to extract model responses. Moreover, the\nPolitical Compass Test is not a scientifically valid survey instrument. In this\nwork, we contribute a political bias measured informed by political science\ntheory, building on survey design principles to test a wide variety of input\nprompts, while taking into account prompt sensitivity. We then prompt 11\ndifferent open and commercial models, differentiating between instruction-tuned\nand non-instruction-tuned models, and automatically classify their political\nstances from 88,110 responses. Leveraging this dataset, we compute political\nbias profiles across different prompt variations and find that while PCT\nexaggerates bias in certain models like GPT3.5, measures of political bias are\noften unstable, but generally more left-leaning for instruction-tuned models.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:51:06 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Faulborn', 'Mats', ''], ['Sen', 'Indira', ''], ['Pellert', 'Max', ''], ['Spitz', 'Andreas', ''], ['Garcia', 'David', '']]","extracted_entities":"[{'text': 'GPT4', 'label': 'GPT'}]","assigned_concept":"GPT","matched_keyword":"GPT4","similarity_score":0.8764010072}
{"id":2304.13343,"submitter":"Xinnian Liang","authors":"Bing Wang, Xinnian Liang, Jian Yang, Hui Huang, Shuangzhi Wu, Peihao\n  Wu, Lu Lu, Zejun Ma, Zhoujun Li","title":"SCM: Enhancing Large Language Model with Self-Controlled Memory\n  Framework","comments":"Accepted by DASFAA 2025 main conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Large Language Models (LLMs) are constrained by their inability to process\nlengthy inputs, resulting in the loss of critical historical information. To\naddress this limitation, in this paper, we propose the Self-Controlled Memory\n(SCM) framework to enhance the ability of LLMs to maintain long-term memory and\nrecall relevant information. Our SCM framework comprises three key components:\nan LLM-based agent serving as the backbone of the framework, a memory stream\nstoring agent memories, and a memory controller updating memories and\ndetermining when and how to utilize memories from memory stream. Additionally,\nthe proposed SCM is able to process ultra-long texts without any modification\nor fine-tuning, which can integrate with any instruction following LLMs in a\nplug-and-play paradigm. Furthermore, we annotate a dataset to evaluate the\neffectiveness of SCM for handling lengthy inputs. The annotated dataset covers\nthree tasks: long-term dialogues, book summarization, and meeting\nsummarization. Experimental results demonstrate that our method achieves better\nretrieval recall and generates more informative responses compared to\ncompetitive baselines in long-term dialogues.\n(https:\/\/github.com\/wbbeyourself\/SCM4LLMs)\n","versions":"[{'version': 'v1', 'created': 'Wed, 26 Apr 2023 07:25:31 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Feb 2024 16:01:39 GMT'}, {'version': 'v3', 'created': 'Thu, 19 Sep 2024 13:38:51 GMT'}, {'version': 'v4', 'created': 'Tue, 18 Mar 2025 02:16:56 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wang', 'Bing', ''], ['Liang', 'Xinnian', ''], ['Yang', 'Jian', ''], ['Huang', 'Hui', ''], ['Wu', 'Shuangzhi', ''], ['Wu', 'Peihao', ''], ['Lu', 'Lu', ''], ['Ma', 'Zejun', ''], ['Li', 'Zhoujun', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM-based agent', 'label': 'LLM-based'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'book summarization', 'label': 'Knowledge distillation'}, {'text': 'meeting\\nsummarization', 'label': 'Knowledge distillation'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2305.10361,"submitter":"Eilam Shapira","authors":"Eilam Shapira, Omer Madmon, Reut Apel, Moshe Tennenholtz, Roi Reichart","title":"Human Choice Prediction in Language-based Persuasion Games:\n  Simulation-based Off-Policy Evaluation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.GT","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent advances in Large Language Models (LLMs) have spurred interest in\ndesigning LLM-based agents for tasks that involve interaction with human and\nartificial agents. This paper addresses a key aspect in the design of such\nagents: predicting human decisions in off-policy evaluation (OPE). We focus on\nlanguage-based persuasion games, where an expert aims to influence the\ndecision-maker through verbal messages. In our OPE framework, the prediction\nmodel is trained on human interaction data collected from encounters with one\nset of expert agents, and its performance is evaluated on interactions with a\ndifferent set of experts. Using a dedicated application, we collected a dataset\nof 87K decisions from humans playing a repeated decision-making game with\nartificial agents. To enhance off-policy performance, we propose a simulation\ntechnique involving interactions across the entire agent space and simulated\ndecision-makers. Our learning strategy yields significant OPE gains, e.g.,\nimproving prediction accuracy in the top 15% challenging cases by 7.1%. Our\ncode and the large dataset we collected and generated are submitted as\nsupplementary material and publicly available in our GitHub repository:\nhttps:\/\/github.com\/eilamshapira\/HumanChoicePrediction\n","versions":"[{'version': 'v1', 'created': 'Wed, 17 May 2023 16:38:11 GMT'}, {'version': 'v2', 'created': 'Tue, 23 May 2023 18:58:21 GMT'}, {'version': 'v3', 'created': 'Wed, 29 Nov 2023 13:46:53 GMT'}, {'version': 'v4', 'created': 'Wed, 28 Feb 2024 21:36:54 GMT'}, {'version': 'v5', 'created': 'Thu, 20 Mar 2025 14:27:22 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Shapira', 'Eilam', ''], ['Madmon', 'Omer', ''], ['Apel', 'Reut', ''], ['Tennenholtz', 'Moshe', ''], ['Reichart', 'Roi', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2311.04928,"submitter":"Marios Papachristou","authors":"Marios Papachristou, Longqi Yang, Chin-Chia Hsu","title":"Leveraging Large Language Models for Collective Decision-Making","comments":"To appear at ACM CSCW 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.HC cs.SI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  In various work contexts, such as meeting scheduling, collaborating, and\nproject planning, collective decision-making is essential but often challenging\ndue to diverse individual preferences, varying work focuses, and power dynamics\namong members. To address this, we propose a system leveraging Large Language\nModels (LLMs) to facilitate group decision-making by managing conversations and\nbalancing preferences among individuals. Our system aims to extract individual\npreferences from each member's conversation with the system and suggest options\nthat satisfy the preferences of the members. We specifically apply this system\nto corporate meeting scheduling. We create synthetic employee profiles and\nsimulate conversations at scale, leveraging LLMs to evaluate the system\nperformance as a novel approach to conducting a user study. Our results\nindicate efficient coordination with reduced interactions between the members\nand the LLM-based system. The system refines and improves its proposed options\nover time, ensuring that many of the members' individual preferences are\nsatisfied in an equitable way. Finally, we conduct a survey study involving\nhuman participants to assess our system's ability to aggregate preferences and\nreasoning about them. Our findings show that the system exhibits strong\nperformance in both dimensions.\n","versions":"[{'version': 'v1', 'created': 'Fri, 3 Nov 2023 18:27:21 GMT'}, {'version': 'v2', 'created': 'Wed, 24 Jan 2024 19:38:52 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 15:50:13 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Papachristou', 'Marios', ''], ['Yang', 'Longqi', ''], ['Hsu', 'Chin-Chia', '']]","extracted_entities":"[{'text': 'Large Language\\nModels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language\nModels","similarity_score":0.9664971828}
{"id":2312.14898,"submitter":"Laura Plein","authors":"Wendk\\^uuni C. Ou\\'edraogo, Laura Plein, Kader Kabor\\'e, Andrew Habib,\n  Jacques Klein, David Lo, Tegawend\\'e F. Bissyand\\'e","title":"Enriching Automatic Test Case Generation by Extracting Relevant Test\n  Inputs from Bug Reports","comments":"Accepted at Empirical Software Engineering (EMSE) journal on 4 March\n  2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  The quality of software is closely tied to the effectiveness of the tests it\nundergoes. Manual test writing, though crucial for bug detection, is\ntime-consuming, which has driven significant research into automated test case\ngeneration. However, current methods often struggle to generate relevant\ninputs, limiting the effectiveness of the tests produced. To address this, we\nintroduce BRMiner, a novel approach that leverages Large Language Models (LLMs)\nin combination with traditional techniques to extract relevant inputs from bug\nreports, thereby enhancing automated test generation tools. In this study, we\nevaluate BRMiner using the Defects4J benchmark and test generation tools such\nas EvoSuite and Randoop. Our results demonstrate that BRMiner achieves a\nRelevant Input Rate (RIR) of 60.03% and a Relevant Input Extraction Accuracy\nRate (RIEAR) of 31.71%, significantly outperforming methods that rely on LLMs\nalone. The integration of BRMiner's input enhances EvoSuite ability to generate\nmore effective test, leading to increased code coverage, with gains observed in\nbranch, instruction, method, and line coverage across multiple projects.\nFurthermore, BRMiner facilitated the detection of 58 unique bugs, including\nthose that were missed by traditional baseline approaches. Overall, BRMiner's\ncombination of LLM filtering with traditional input extraction techniques\nsignificantly improves the relevance and effectiveness of automated test\ngeneration, advancing the detection of bugs and enhancing code coverage,\nthereby contributing to higher-quality software development.\n","versions":"[{'version': 'v1', 'created': 'Fri, 22 Dec 2023 18:19:33 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 20:06:00 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Ou\u00e9draogo', 'Wendk\u00fbuni C.', ''], ['Plein', 'Laura', ''], ['Kabor\u00e9', 'Kader', ''], ['Habib', 'Andrew', ''], ['Klein', 'Jacques', ''], ['Lo', 'David', ''], ['Bissyand\u00e9', 'Tegawend\u00e9 F.', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'EvoSuite', 'label': 'Open-source LLMs'}, {'text': 'Randoop', 'label': 'Open-source LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'EvoSuite', 'label': 'Open-source LLMs'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2401.09002,"submitter":"Mingyu Jin","authors":"Dong Shu, Chong Zhang, Mingyu Jin, Zihao Zhou, Lingyao Li, Yongfeng\n  Zhang","title":"AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on\n  Large Language Models","comments":"Accepted by ACM SIGKDD Explorations 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Jailbreak attacks represent one of the most sophisticated threats to the\nsecurity of large language models (LLMs). To deal with such risks, we introduce\nan innovative framework that can help evaluate the effectiveness of jailbreak\nattacks on LLMs. Unlike traditional binary evaluations focusing solely on the\nrobustness of LLMs, our method assesses the attacking prompts' effectiveness.\nWe present two distinct evaluation frameworks: a coarse-grained evaluation and\na fine-grained evaluation. Each framework uses a scoring range from 0 to 1,\noffering unique perspectives and allowing for the assessment of attack\neffectiveness in different scenarios. Additionally, we develop a comprehensive\nground truth dataset specifically tailored for jailbreak prompts. This dataset\nis a crucial benchmark for our current study and provides a foundational\nresource for future research. By comparing with traditional evaluation methods,\nour study shows that the current results align with baseline metrics while\noffering a more nuanced and fine-grained assessment. It also helps identify\npotentially harmful attack prompts that might appear harmless in traditional\nevaluations. Overall, our work establishes a solid foundation for assessing a\nbroader range of attack prompts in prompt injection.\n","versions":"[{'version': 'v1', 'created': 'Wed, 17 Jan 2024 06:42:44 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Feb 2024 02:20:31 GMT'}, {'version': 'v3', 'created': 'Wed, 20 Mar 2024 14:08:39 GMT'}, {'version': 'v4', 'created': 'Wed, 31 Jul 2024 06:46:44 GMT'}, {'version': 'v5', 'created': 'Sat, 3 Aug 2024 06:39:25 GMT'}, {'version': 'v6', 'created': 'Tue, 18 Mar 2025 01:50:42 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Shu', 'Dong', ''], ['Zhang', 'Chong', ''], ['Jin', 'Mingyu', ''], ['Zhou', 'Zihao', ''], ['Li', 'Lingyao', ''], ['Zhang', 'Yongfeng', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'jailbreak prompts', 'label': 'Prompting'}, {'text': 'prompt injection', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2404.13274,"submitter":"Mar Gonzalez-Franco","authors":"Mustafa Doga Dogan, and Eric J. Gonzalez, and Karan Ahuja, and Ruofei\n  Du, and Andrea Cola\\c{c}o, and Johnny Lee, and Mar Gonzalez-Franco, and David\n  Kim","title":"Augmented Object Intelligence: Making the Analog World Interactable with\n  XR-Objects","comments":"2024 ACM Symposium on User Interface Software and Technology (UIST)","journal-ref":null,"doi":"10.1145\/3654777.3676379","report-no":null,"categories":"cs.HC cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Seamless integration of physical objects as interactive digital entities\nremains a challenge for spatial computing. This paper introduces Augmented\nObject Intelligence (AOI), a novel XR interaction paradigm designed to blur the\nlines between digital and physical by equipping real-world objects with the\nability to interact as if they were digital, where every object has the\npotential to serve as a portal to vast digital functionalities. Our approach\nutilizes object segmentation and classification, combined with the power of\nMultimodal Large Language Models (MLLMs), to facilitate these interactions. We\nimplement the AOI concept in the form of XR-Objects, an open-source prototype\nsystem that provides a platform for users to engage with their physical\nenvironment in rich and contextually relevant ways. This system enables analog\nobjects to not only convey information but also to initiate digital actions,\nsuch as querying for details or executing tasks. Our contributions are\nthreefold: (1) we define the AOI concept and detail its advantages over\ntraditional AI assistants, (2) detail the XR-Objects system's open-source\ndesign and implementation, and (3) show its versatility through a variety of\nuse cases and a user study.\n","versions":"[{'version': 'v1', 'created': 'Sat, 20 Apr 2024 05:14:52 GMT'}, {'version': 'v2', 'created': 'Tue, 23 Apr 2024 03:09:15 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Aug 2024 07:55:44 GMT'}, {'version': 'v4', 'created': 'Tue, 18 Mar 2025 23:29:40 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Dogan', 'Mustafa Doga', ''], ['Gonzalez', 'Eric J.', ''], ['Ahuja', 'Karan', ''], ['Du', 'Ruofei', ''], ['Cola\u00e7o', 'Andrea', ''], ['Lee', 'Johnny', ''], ['Gonzalez-Franco', 'Mar', ''], ['Kim', 'David', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models (MLLMs)', 'label': 'Large Language Model'}, {'text': 'XR-Objects', 'label': 'Open-source LLMs'}, {'text': 'XR-Objects', 'label': 'Open-source LLMs'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models (MLLMs)","similarity_score":0.7274739742}
{"id":2404.18212,"submitter":"Noam Rotstein","authors":"Navve Wasserman, Noam Rotstein, Roy Ganz, Ron Kimmel","title":"Paint by Inpaint: Learning to Add Image Objects by Removing Them First","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Image editing has advanced significantly with the introduction of\ntext-conditioned diffusion models. Despite this progress, seamlessly adding\nobjects to images based on textual instructions without requiring user-provided\ninput masks remains a challenge. We address this by leveraging the insight that\nremoving objects (Inpaint) is significantly simpler than its inverse process of\nadding them (Paint), attributed to inpainting models that benefit from\nsegmentation mask guidance. Capitalizing on this realization, by implementing\nan automated and extensive pipeline, we curate a filtered large-scale image\ndataset containing pairs of images and their corresponding object-removed\nversions. Using these pairs, we train a diffusion model to inverse the\ninpainting process, effectively adding objects into images. Unlike other\nediting datasets, ours features natural target images instead of synthetic ones\nwhile ensuring source-target consistency by construction. Additionally, we\nutilize a large Vision-Language Model to provide detailed descriptions of the\nremoved objects and a Large Language Model to convert these descriptions into\ndiverse, natural-language instructions. Our quantitative and qualitative\nresults show that the trained model surpasses existing models in both object\naddition and general editing tasks. Visit our project page for the released\ndataset and trained models at https:\/\/rotsteinnoam.github.io\/Paint-by-Inpaint.\n","versions":"[{'version': 'v1', 'created': 'Sun, 28 Apr 2024 15:07:53 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 13:48:18 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 06:59:54 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wasserman', 'Navve', ''], ['Rotstein', 'Noam', ''], ['Ganz', 'Roy', ''], ['Kimmel', 'Ron', '']]","extracted_entities":"[{'text': 'Large Language Model', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Model","similarity_score":1.0}
{"id":2404.184,"submitter":"Parshin Shojaee","authors":"Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani,\n  Chandan K Reddy","title":"LLM-SR: Scientific Equation Discovery via Programming with Large\n  Language Models","comments":"ICLR 2025 Oral","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL cs.NE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Mathematical equations have been unreasonably effective in describing complex\nnatural phenomena across various scientific disciplines. However, discovering\nsuch insightful equations from data presents significant challenges due to the\nnecessity of navigating extremely large combinatorial hypothesis spaces.\nCurrent methods of equation discovery, commonly known as symbolic regression\ntechniques, largely focus on extracting equations from data alone, often\nneglecting the domain-specific prior knowledge that scientists typically depend\non. They also employ limited representations such as expression trees,\nconstraining the search space and expressiveness of equations. To bridge this\ngap, we introduce LLM-SR, a novel approach that leverages the extensive\nscientific knowledge and robust code generation capabilities of Large Language\nModels (LLMs) to discover scientific equations from data. Specifically, LLM-SR\ntreats equations as programs with mathematical operators and combines LLMs'\nscientific priors with evolutionary search over equation programs. The LLM\niteratively proposes new equation skeleton hypotheses, drawing from its domain\nknowledge, which are then optimized against data to estimate parameters. We\nevaluate LLM-SR on four benchmark problems across diverse scientific domains\n(e.g., physics, biology), which we carefully designed to simulate the discovery\nprocess and prevent LLM recitation. Our results demonstrate that LLM-SR\ndiscovers physically accurate equations that significantly outperform\nstate-of-the-art symbolic regression baselines, particularly in out-of-domain\ntest settings. We also show that LLM-SR's incorporation of scientific priors\nenables more efficient equation space exploration than the baselines. Code and\ndata are available: https:\/\/github.com\/deep-symbolic-mathematics\/LLM-SR\n","versions":"[{'version': 'v1', 'created': 'Mon, 29 Apr 2024 03:30:06 GMT'}, {'version': 'v2', 'created': 'Sun, 2 Jun 2024 20:17:59 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 16:37:17 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Shojaee', 'Parshin', ''], ['Meidani', 'Kazem', ''], ['Gupta', 'Shashank', ''], ['Farimani', 'Amir Barati', ''], ['Reddy', 'Chandan K', '']]","extracted_entities":"[{'text': 'LLM-SR', 'label': 'LLM'}, {'text': 'Large Language\\nModels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM-SR', 'label': 'LLM'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM-SR', 'label': 'LLM'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language\nModels","similarity_score":0.9664971828}
{"id":2405.18376,"submitter":"Dongjie Chen","authors":"Dongjie Chen, Kartik Patwari, Zhengfeng Lai, Xiaoguang Zhu, Sen-ching\n  Cheung, Chen-Nee Chuah","title":"Empowering Source-Free Domain Adaptation via MLLM-Guided\n  Reliability-Based Curriculum Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model\nto a target domain using only unlabeled target data. Current SFDA methods face\nchallenges in effectively leveraging pre-trained knowledge and exploiting\ntarget domain data. Multimodal Large Language Models (MLLMs) offer remarkable\ncapabilities in understanding visual and textual information, but their\napplicability to SFDA poses challenges such as instruction-following failures,\nintensive computational demands, and difficulties in performance measurement\nprior to adaptation. To alleviate these issues, we propose\n$\\textbf{Reliability-based Curriculum Learning (RCL)}$, a novel framework that\nintegrates multiple MLLMs for knowledge exploitation via pseudo-labeling in\nSFDA. Our framework incorporates Reliable Knowledge Transfer, Self-correcting\nand MLLM-guided Knowledge Expansion, and Multi-hot Masking Refinement to\nprogressively exploit unlabeled data in the target domain. RCL achieves\nstate-of-the-art (SOTA) performance on multiple SFDA benchmarks, e.g.,\n$\\textbf{+9.4%}$ on DomainNet, demonstrating its effectiveness in enhancing\nadaptability and robustness without requiring access to source data. Our code\nis available at: https:\/\/github.com\/Dong-Jie-Chen\/RCL.\n","versions":"[{'version': 'v1', 'created': 'Tue, 28 May 2024 17:18:17 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 00:11:36 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Chen', 'Dongjie', ''], ['Patwari', 'Kartik', ''], ['Lai', 'Zhengfeng', ''], ['Zhu', 'Xiaoguang', ''], ['Cheung', 'Sen-ching', ''], ['Chuah', 'Chen-Nee', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'Multi-hot Masking Refinement', 'label': 'Zero-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2406.06918,"submitter":"Dewu Zheng","authors":"Dewu Zheng, Yanlin Wang, Ensheng Shi, Ruikai Zhang, Yuchi Ma, Hongyu\n  Zhang, Zibin Zheng","title":"HumanEvo: An Evolution-aware Benchmark for More Realistic Evaluation of\n  Repository-level Code Generation","comments":"To appear at ICSE 2025","journal-ref":"47th International Conference on Software Engineering (ICSE 2025)","doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  To evaluate the repository-level code generation capabilities of Large\nLanguage Models (LLMs) in complex real-world software development scenarios,\nmany evaluation methods have been developed. These methods typically leverage\ncontextual code from the latest version of a project to assist LLMs in\naccurately generating the desired function. However, such evaluation methods\nfail to consider the dynamic evolution of software projects over time, which we\nrefer to as evolution-ignored settings. This in turn results in inaccurate\nevaluation of LLMs' performance. In this paper, we conduct an empirical study\nto deeply understand LLMs' code generation performance within settings that\nreflect the evolution nature of software development. To achieve this, we first\nconstruct an evolution-aware repository-level code generation dataset, namely\nHumanEvo, equipped with an automated execution-based evaluation tool. Second,\nwe manually categorize HumanEvo according to dependency levels to more\ncomprehensively analyze the model's performance in generating functions with\ndifferent dependency levels. Third, we conduct extensive experiments on\nHumanEvo with seven representative and diverse LLMs to verify the effectiveness\nof the proposed benchmark. We obtain several important findings through our\nexperimental study. For example, we find that previous evolution-ignored\nevaluation methods result in inflated performance of LLMs, with performance\noverestimations ranging from 10.0% to 61.1% under different context acquisition\nmethods, compared to the evolution-aware evaluation approach. Based on the\nfindings, we give actionable suggestions for more realistic evaluation of LLMs\non code generation. We also build a shared evolution-aware code generation\ntoolbox to facilitate future research.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Jun 2024 03:19:18 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 04:58:23 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Zheng', 'Dewu', ''], ['Wang', 'Yanlin', ''], ['Shi', 'Ensheng', ''], ['Zhang', 'Ruikai', ''], ['Ma', 'Yuchi', ''], ['Zhang', 'Hongyu', ''], ['Zheng', 'Zibin', '']]","extracted_entities":"[{'text': 'Large\\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large\nLanguage Models","similarity_score":0.9664971828}
{"id":2406.10638,"submitter":"Yexin Liu","authors":"Yexin Liu, Zhengyang Liang, Yueze Wang, Xianfeng Wu, Feilong Tang,\n  Muyang He, Jian Li, Zheng Liu, Harry Yang, Sernam Lim, Bo Zhao","title":"Unveiling the Ignorance of MLLMs: Seeing Clearly, Answering Incorrectly","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multimodal Large Language Models (MLLMs) have displayed remarkable\nperformance in multi-modal tasks, particularly in visual comprehension.\nHowever, we reveal that MLLMs often generate incorrect answers even when they\nunderstand the visual content. To this end, we manually construct a benchmark\nwith 12 categories and design evaluation metrics that assess the degree of\nerror in MLLM responses even when the visual content is seemingly understood.\nBased on this benchmark, we test 15 leading MLLMs and analyze the distribution\nof attention maps and logits of some MLLMs. Our investigation identifies two\nprimary issues: 1) most instruction tuning datasets predominantly feature\nquestions that 'directly' relate to the visual content, leading to a bias in\nMLLMs' responses to other indirect questions, and 2) MLLMs' attention to visual\ntokens is notably lower than to system and question tokens. We further observe\nthat attention scores between questions and visual tokens as well as the\nmodel's confidence in the answers are lower in response to misleading questions\nthan to straightforward ones. To address the first challenge, we introduce a\npaired positive and negative data construction pipeline to diversify the\ndataset. For the second challenge, we propose to enhance the model's focus on\nvisual content during decoding by refining the text and visual prompt. For the\ntext prompt, we propose a content guided refinement strategy that performs\npreliminary visual content analysis to generate structured information before\nanswering the question. Additionally, we employ a visual attention refinement\nstrategy that highlights question-relevant visual tokens to increase the\nmodel's attention to visual content that aligns with the question. Extensive\nexperiments demonstrate that these challenges can be significantly mitigated\nwith our proposed dataset and techniques.\n","versions":"[{'version': 'v1', 'created': 'Sat, 15 Jun 2024 13:58:26 GMT'}, {'version': 'v2', 'created': 'Tue, 17 Dec 2024 06:48:10 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 05:52:59 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Liu', 'Yexin', ''], ['Liang', 'Zhengyang', ''], ['Wang', 'Yueze', ''], ['Wu', 'Xianfeng', ''], ['Tang', 'Feilong', ''], ['He', 'Muyang', ''], ['Li', 'Jian', ''], ['Liu', 'Zheng', ''], ['Yang', 'Harry', ''], ['Lim', 'Sernam', ''], ['Zhao', 'Bo', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'attention scores', 'label': 'Attention mechanism'}, {'text': 'visual prompt', 'label': 'Prompting'}, {'text': 'text prompt', 'label': 'Prompting'}, {'text': 'content guided refinement strategy', 'label': 'Fine-tuning'}, {'text': 'attention', 'label': 'Attention mechanism'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2406.13803,"submitter":"Sam Musker","authors":"Sam Musker, Alex Duchnowski, Rapha\\\"el Milli\\`ere, Ellie Pavlick","title":"LLMs as Models for Analogical Reasoning","comments":"The title has been changed from Semantic Structure-Mapping in LLM and\n  Human Analogical Reasoning to LLMs as Models for Analogical Reasoning to\n  improve clarity and accuracy","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Analogical reasoning-the capacity to identify and map structural\nrelationships between different domains-is fundamental to human cognition and\nlearning. Recent studies have shown that large language models (LLMs) can\nsometimes match humans in analogical reasoning tasks, opening the possibility\nthat analogical reasoning might emerge from domain general processes. However,\nit is still debated whether these emergent capacities are largely superficial\nand limited to simple relations seen during training or whether they rather\nencompass the flexible representational and mapping capabilities which are the\nfocus of leading cognitive models of analogy. In this study, we introduce novel\nanalogical reasoning tasks that require participants to map between\nsemantically contentful words and sequences of letters and other abstract\ncharacters. This task necessitates the ability to flexibly re-represent rich\nsemantic information-an ability which is known to be central to human analogy\nbut which is thus far not well-captured by existing cognitive theories and\nmodels. We assess the performance of both human participants and LLMs on tasks\nfocusing on reasoning from semantic structure and semantic content, introducing\nvariations that test the robustness of their analogical inferences. Advanced\nLLMs match human performance across several conditions, though humans and LLMs\nrespond differently to certain task variations and semantic distractors. Our\nresults thus provide new evidence that LLMs might offer a how-possibly\nexplanation of human analogical reasoning in contexts that are not yet well\nmodeled by existing theories, but that even today's best models are unlikely to\nyield how-actually explanations.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Jun 2024 20:07:37 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 17:49:06 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Musker', 'Sam', ''], ['Duchnowski', 'Alex', ''], ['Milli\u00e8re', 'Rapha\u00ebl', ''], ['Pavlick', 'Ellie', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2406.14703,"submitter":"Seungwon Lim","authors":"Seungbeen Lee, Seungwon Lim, Seungju Han, Giyeong Oh, Hyungjoo Chae,\n  Jiwan Chung, Minju Kim, Beong-woo Kwak, Yeonsoo Lee, Dongha Lee, Jinyoung\n  Yeo, Youngjae Yu","title":"Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality\n  Testset designed for LLMs with Psychometrics","comments":"Accepted to NAACL2025 Findings","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recent advancements in Large Language Models (LLMs) have led to their\nadaptation in various domains as conversational agents. We wonder: can\npersonality tests be applied to these agents to analyze their behavior, similar\nto humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice\nquestions designed to assess the personality of LLMs. TRAIT is built on two\npsychometrically validated small human questionnaires, Big Five Inventory (BFI)\nand Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a\nvariety of real-world scenarios. TRAIT also outperforms existing personality\ntests for LLMs in terms of reliability and validity, achieving the highest\nscores across four key metrics: Content Validity, Internal Validity, Refusal\nRate, and Reliability. Using TRAIT, we reveal two notable insights into\npersonalities of LLMs: 1) LLMs exhibit distinct and consistent personality,\nwhich is highly influenced by their training data (e.g., data used for\nalignment tuning), and 2) current prompting techniques have limited\neffectiveness in eliciting certain traits, such as high psychopathy or low\nconscientiousness, suggesting the need for further research in this direction.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Jun 2024 19:50:56 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Oct 2024 14:01:14 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 15:37:42 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Lee', 'Seungbeen', ''], ['Lim', 'Seungwon', ''], ['Han', 'Seungju', ''], ['Oh', 'Giyeong', ''], ['Chae', 'Hyungjoo', ''], ['Chung', 'Jiwan', ''], ['Kim', 'Minju', ''], ['Kwak', 'Beong-woo', ''], ['Lee', 'Yeonsoo', ''], ['Lee', 'Dongha', ''], ['Yeo', 'Jinyoung', ''], ['Yu', 'Youngjae', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'TRAIT', 'label': 'ChatGPT'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'alignment tuning', 'label': 'Fine-tuning'}, {'text': 'current prompting techniques', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2406.16144,"submitter":"Zezhong Wang Mr.","authors":"Zezhong Wang, Xingshan Zeng, Weiwen Liu, Yufei Wang, Liangyou Li,\n  Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong","title":"Chain-of-Probe: Examining the Necessity and Accuracy of CoT Step-by-Step","comments":"Accepted by Findings of NAACL 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Current research found the issue of Early Answering in large language models\n(LLMs), where the models already have an answer before generating the\nChain-of-Thought (CoT). This phenomenon suggests a potential lack of necessary\ndependency between the predicted answer and the reasoning process.\nConsequently, two important questions arise: (1) Is CoT still necessary if the\nmodel already has an answer? (2) Can the correctness of the answer serve as\nvalid evidence for the correctness of CoT? To address these questions, we\npropose a method, namely Chain-of-Probe (CoP), to probe changes in the mind\nduring the model's reasoning. The probing results show that in a significant\nnumber of question-answer cases, CoT appears to be unnecessary, and this\nnecessity correlates with the simplicity of the task, defined by reasoning\nsteps required. Furthermore, by analyzing patterns in mind change, we examine\nthe correctness of the model's reasoning. Our validation reveals that many\nresponses, although correct in their final answer, contain errors in their\nreasoning process. To this end, we propose a strategic approach based on CoP to\nprioritize answers with correct reasoning among multiple candidates, thereby\nbolstering the reliability of the model's reasoning.\n","versions":"[{'version': 'v1', 'created': 'Sun, 23 Jun 2024 15:50:22 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 06:04:48 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wang', 'Zezhong', ''], ['Zeng', 'Xingshan', ''], ['Liu', 'Weiwen', ''], ['Wang', 'Yufei', ''], ['Li', 'Liangyou', ''], ['Wang', 'Yasheng', ''], ['Shang', 'Lifeng', ''], ['Jiang', 'Xin', ''], ['Liu', 'Qun', ''], ['Wong', 'Kam-Fai', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'CoT', 'label': 'Chain of thought'}, {'text': 'CoT', 'label': 'Chain of thought'}, {'text': 'Chain-of-Probe', 'label': 'Chain of thought'}, {'text': 'CoT', 'label': 'Chain of thought'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2406.18326,"submitter":"Huixuan Zhang","authors":"Huixuan Zhang, Yun Lin, Xiaojun Wan","title":"PaCoST: Paired Confidence Significance Testing for Benchmark\n  Contamination Detection in Large Language Models","comments":"Accepted by EMNLP 2024 Findings","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs) are known to be trained on vast amounts of data,\nwhich may unintentionally or intentionally include data from commonly used\nbenchmarks. This inclusion can lead to cheatingly high scores on model\nleaderboards, yet result in disappointing performance in real-world\napplications. To address this benchmark contamination problem, we first propose\na set of requirements that practical contamination detection methods should\nfollow. Following these proposed requirements, we introduce PaCoST, a Paired\nConfidence Significance Testing to effectively detect benchmark contamination\nin LLMs. Our method constructs a counterpart for each piece of data with the\nsame distribution, and performs statistical analysis of the corresponding\nconfidence to test whether the model is significantly more confident under the\noriginal benchmark. We validate the effectiveness of PaCoST and apply it on\npopular open-source models and benchmarks. We find that almost all models and\nbenchmarks we tested are suspected contaminated more or less. We finally call\nfor new LLM evaluation methods.\n","versions":"[{'version': 'v1', 'created': 'Wed, 26 Jun 2024 13:12:40 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 08:57:39 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Zhang', 'Huixuan', ''], ['Lin', 'Yun', ''], ['Wan', 'Xiaojun', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2406.18842,"submitter":"Saleh Afroogh","authors":"Junfeng Jiao, Saleh Afroogh, Kevin Chen, David Atkinson, Amit\n  Dhurandhar","title":"The global landscape of academic guidelines for generative AI and Large\n  Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The integration of Generative Artificial Intelligence (GAI) and Large\nLanguage Models (LLMs) in academia has spurred a global discourse on their\npotential pedagogical benefits and ethical considerations. Positive reactions\nhighlight some potential, such as collaborative creativity, increased access to\neducation, and empowerment of trainers and trainees. However, negative\nreactions raise concerns about ethical complexities, balancing innovation and\nacademic integrity, unequal access, and misinformation risks. Through a\nsystematic survey and text-mining-based analysis of global and national\ndirectives, insights from independent research, and eighty university-level\nguidelines, this study provides a nuanced understanding of the opportunities\nand challenges posed by GAI and LLMs in education. It emphasizes the importance\nof balanced approaches that harness the benefits of these technologies while\naddressing ethical considerations and ensuring equitable access and educational\noutcomes. The paper concludes with recommendations for fostering responsible\ninnovation and ethical practices to guide the integration of GAI and LLMs in\nacademia.\n","versions":"[{'version': 'v1', 'created': 'Sun, 26 May 2024 15:28:24 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Jun 2024 02:54:06 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 16:42:30 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Jiao', 'Junfeng', ''], ['Afroogh', 'Saleh', ''], ['Chen', 'Kevin', ''], ['Atkinson', 'David', ''], ['Dhurandhar', 'Amit', '']]","extracted_entities":"[{'text': 'Large\\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ethical considerations', 'label': 'AI Ethics'}, {'text': 'ethical complexities', 'label': 'AI Ethics'}, {'text': 'GAI', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ethical considerations', 'label': 'AI Ethics'}, {'text': 'ethical practices', 'label': 'AI Ethics'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large\nLanguage Models","similarity_score":0.9664971828}
{"id":2407.0089,"submitter":"Shubhranshu Shekhar","authors":"Andrea Carriero and Davide Pettenuzzo and Shubhranshu Shekhar","title":"Macroeconomic Forecasting with Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"econ.EM cs.CL cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper presents a comparative analysis evaluating the accuracy of Large\nLanguage Models (LLMs) against traditional macro time series forecasting\napproaches. In recent times, LLMs have surged in popularity for forecasting due\nto their ability to capture intricate patterns in data and quickly adapt across\nvery different domains. However, their effectiveness in forecasting\nmacroeconomic time series data compared to conventional methods remains an area\nof interest. To address this, we conduct a rigorous evaluation of LLMs against\ntraditional macro forecasting methods, using as common ground the FRED-MD\ndatabase. Our findings provide valuable insights into the strengths and\nlimitations of LLMs in forecasting macroeconomic time series, shedding light on\ntheir applicability in real-world scenarios\n","versions":"[{'version': 'v1', 'created': 'Mon, 1 Jul 2024 01:25:26 GMT'}, {'version': 'v2', 'created': 'Mon, 6 Jan 2025 01:27:48 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 18:55:51 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Carriero', 'Andrea', ''], ['Pettenuzzo', 'Davide', ''], ['Shekhar', 'Shubhranshu', '']]","extracted_entities":"[{'text': 'Large\\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large\nLanguage Models","similarity_score":0.9664971828}
{"id":2407.09283,"submitter":"Sangpil Youm","authors":"Sangpil Youm, Brodie Mather, Chathuri Jayaweera, Juliana Prada, Bonnie\n  Dorr","title":"DAHRS: Divergence-Aware Hallucination-Remediated SRL Projection","comments":"15 pages, 6 figures, Accepted to The 29th International Conference on\n  Natural Language & Information Systems (NLDB 2024)","journal-ref":null,"doi":"10.1007\/978-3-031-70239-6_29","report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Semantic role labeling (SRL) enriches many downstream applications, e.g.,\nmachine translation, question answering, summarization, and stance\/belief\ndetection. However, building multilingual SRL models is challenging due to the\nscarcity of semantically annotated corpora for multiple languages. Moreover,\nstate-of-the-art SRL projection (XSRL) based on large language models (LLMs)\nyields output that is riddled with spurious role labels. Remediation of such\nhallucinations is not straightforward due to the lack of explainability of\nLLMs. We show that hallucinated role labels are related to naturally occurring\ndivergence types that interfere with initial alignments. We implement\nDivergence-Aware Hallucination-Remediated SRL projection (DAHRS), leveraging\nlinguistically-informed alignment remediation followed by greedy First-Come\nFirst-Assign (FCFA) SRL projection. DAHRS improves the accuracy of SRL\nprojection without additional transformer-based machinery, beating XSRL in both\nhuman and automatic comparisons, and advancing beyond headwords to accommodate\nphrase-level SRL projection (e.g., EN-FR, EN-ES). Using CoNLL-2009 as our\nground truth, we achieve a higher word-level F1 over XSRL: 87.6% vs. 77.3%\n(EN-FR) and 89.0% vs. 82.7% (EN-ES). Human phrase-level assessments yield 89.1%\n(EN-FR) and 91.0% (EN-ES). We also define a divergence metric to adapt our\napproach to other language pairs (e.g., English-Tagalog).\n","versions":"[{'version': 'v1', 'created': 'Fri, 12 Jul 2024 14:13:59 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 13:41:21 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Youm', 'Sangpil', ''], ['Mather', 'Brodie', ''], ['Jayaweera', 'Chathuri', ''], ['Prada', 'Juliana', ''], ['Dorr', 'Bonnie', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2407.21368,"submitter":"Danfeng Guo","authors":"Danfeng Guo and Demetri Terzopoulos","title":"Prompting Medical Large Vision-Language Models to Diagnose Pathologies\n  by Visual Question Answering","comments":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https:\/\/melba-journal.org\/2025:004","journal-ref":"Machine.Learning.for.Biomedical.Imaging. 3 (2025)","doi":"10.59275\/j.melba.2025-1a8b","report-no":null,"categories":"cs.CV cs.AI cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Vision-Language Models (LVLMs) have achieved significant success in\nrecent years, and they have been extended to the medical domain. Although\ndemonstrating satisfactory performance on medical Visual Question Answering\n(VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem,\nwhich makes them fail to diagnose complex pathologies. Moreover, they readily\nfail to learn minority pathologies due to imbalanced training data. We propose\ntwo prompting strategies for MLVLMs that reduce hallucination and improve VQA\nperformance. In the first strategy, we provide a detailed explanation of the\nqueried pathology. In the second strategy, we fine-tune a cheap, weak learner\nto achieve high performance on a specific metric, and textually provide its\njudgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our\nmethods significantly improve the diagnostic F1 score, with the highest\nincrease being 0.27. We also demonstrate that our prompting strategies can be\nextended to general LVLM domains. Based on POPE metrics, it effectively\nsuppresses the false negative predictions of existing LVLMs and improves Recall\nby approximately 0.07.\n","versions":"[{'version': 'v1', 'created': 'Wed, 31 Jul 2024 06:34:38 GMT'}, {'version': 'v2', 'created': 'Sat, 1 Mar 2025 06:14:00 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 00:27:45 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Guo', 'Danfeng', ''], ['Terzopoulos', 'Demetri', '']]","extracted_entities":"[{'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'prompting strategies', 'label': 'Prompting'}, {'text': 'MLVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Vision-Language Models","similarity_score":0.7742220759}
{"id":2408.10641,"submitter":"Yuxiao Wang","authors":"Yuxiao Wang, Yu Lei, Li Cui, Weiying Xue, Qi Liu, Zhenao Wei","title":"A Review of Human-Object Interaction Detection","comments":"Accepted by 2024 2nd International Conference on Computer, Vision and\n  Intelligent Technology (ICCVIT)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Human-object interaction (HOI) detection plays a key role in high-level\nvisual understanding, facilitating a deep comprehension of human activities.\nSpecifically, HOI detection aims to locate the humans and objects involved in\ninteractions within images or videos and classify the specific interactions\nbetween them. The success of this task is influenced by several key factors,\nincluding the accurate localization of human and object instances, as well as\nthe correct classification of object categories and interaction relationships.\nThis paper systematically summarizes and discusses the recent work in\nimage-based HOI detection. First, the mainstream datasets involved in HOI\nrelationship detection are introduced. Furthermore, starting with two-stage\nmethods and end-to-end one-stage detection approaches, this paper\ncomprehensively discusses the current developments in image-based HOI\ndetection, analyzing the strengths and weaknesses of these two methods.\nAdditionally, the advancements of zero-shot learning, weakly supervised\nlearning, and the application of large-scale language models in HOI detection\nare discussed. Finally, the current challenges in HOI detection are outlined,\nand potential research directions and future trends are explored.\n","versions":"[{'version': 'v1', 'created': 'Tue, 20 Aug 2024 08:32:39 GMT'}, {'version': 'v2', 'created': 'Mon, 9 Dec 2024 09:27:29 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 02:22:59 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wang', 'Yuxiao', ''], ['Lei', 'Yu', ''], ['Cui', 'Li', ''], ['Xue', 'Weiying', ''], ['Liu', 'Qi', ''], ['Wei', 'Zhenao', '']]","extracted_entities":"[{'text': 'zero-shot learning', 'label': 'Few-shot Learning'}, {'text': 'weakly supervised\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'large-scale language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large-scale language models","similarity_score":0.908618629}
{"id":2409.13642,"submitter":"Md Nakhla Rafi","authors":"Md Nakhla Rafi, Dong Jae Kim, Tse-Hsun Chen, Shaowei Wang","title":"A Multi-Agent Approach to Fault Localization via Graph-Based Retrieval\n  and Reflexion","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Identifying and resolving software faults remains a challenging and\nresource-intensive process. Traditional fault localization techniques, such as\nSpectrum-Based Fault Localization (SBFL), leverage statistical analysis of test\ncoverage but often suffer from limited accuracy. While learning-based\napproaches improve fault localization, they demand extensive training datasets\nand high computational resources. Recent advances in Large Language Models\n(LLMs) offer new opportunities by enhancing code understanding and reasoning.\nHowever, existing LLM-based fault localization techniques face significant\nchallenges, including token limitations, performance degradation with long\ninputs, and scalability issues in complex software systems. To overcome these\nobstacles, we propose LLM4FL, a multi-agent fault localization framework that\nutilizes three specialized LLM agents. First, the Context Extraction Agent\napplies an order-sensitive segmentation strategy to partition large coverage\ndata within the LLM's token limit, analyze failure context, and prioritize\nfailure-related methods. The Debugger Agent then processes the extracted data,\nwhich employs graph-based retrieval-augmented code navigation to reason about\nfailure causes and rank suspicious methods. Finally, the Reviewer Agent\nre-evaluates the identified faulty methods using verbal reinforcement learning,\nengaging in self-criticism and iterative refinement. Evaluated on the Defects4J\n(V2.0.0) benchmark, which includes 675 faults from 14 Java projects, LLM4FL\nachieves an 18.55\\% improvement in Top-1 accuracy over AutoFL and 4.82\\% over\nSoapFL. It outperforms supervised techniques such as DeepFL and Grace, all\nwithout requiring task-specific training. Furthermore, its coverage\nsegmentation and prompt chaining strategies enhance performance, increasing\nTop-1 accuracy by up to 22\\%.\n","versions":"[{'version': 'v1', 'created': 'Fri, 20 Sep 2024 16:47:34 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 04:22:52 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Rafi', 'Md Nakhla', ''], ['Kim', 'Dong Jae', ''], ['Chen', 'Tse-Hsun', ''], ['Wang', 'Shaowei', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'verbal reinforcement learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2409.18042,"submitter":"Kai Chen","authors":"Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu,\n  Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, Dingdong Wang, Kun Xiang, Haoyuan\n  Li, Haoli Bai, Jianhua Han, Xiaohui Li, Weike Jin, Nian Xie, Yu Zhang, James\n  T. Kwok, Hengshuang Zhao, Xiaodan Liang, Dit-Yan Yeung, Xiao Chen, Zhenguo\n  Li, Wei Zhang, Qun Liu, Jun Yao, Lanqing Hong, Lu Hou, Hang Xu","title":"EMOVA: Empowering Language Models to See, Hear and Speak with Vivid\n  Emotions","comments":"Accepted by CVPR 2025. Project Page: https:\/\/emova-ollm.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  GPT-4o, an omni-modal model that enables vocal conversations with diverse\nemotions and tones, marks a milestone for omni-modal foundation models.\nHowever, empowering Large Language Models to perceive and generate images,\ntexts, and speeches end-to-end with publicly available data remains challenging\nfor the open-source community. Existing vision-language models rely on external\ntools for speech processing, while speech-language models still suffer from\nlimited or totally without vision-understanding capabilities. To address this\ngap, we propose the EMOVA (EMotionally Omni-present Voice Assistant), to enable\nLarge Language Models with end-to-end speech abilities while maintaining the\nleading vision-language performance. With a semantic-acoustic disentangled\nspeech tokenizer, we surprisingly notice that omni-modal alignment can further\nenhance vision-language and speech abilities compared with the bi-modal aligned\ncounterparts. Moreover, a lightweight style module is introduced for the\nflexible speech style controls including emotions and pitches. For the first\ntime, EMOVA achieves state-of-the-art performance on both the vision-language\nand speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue\nwith vivid emotions.\n","versions":"[{'version': 'v1', 'created': 'Thu, 26 Sep 2024 16:44:02 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Oct 2024 06:25:52 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Mar 2025 14:51:04 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 08:47:39 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chen', 'Kai', ''], ['Gou', 'Yunhao', ''], ['Huang', 'Runhui', ''], ['Liu', 'Zhili', ''], ['Tan', 'Daxin', ''], ['Xu', 'Jing', ''], ['Wang', 'Chunwei', ''], ['Zhu', 'Yi', ''], ['Zeng', 'Yihan', ''], ['Yang', 'Kuo', ''], ['Wang', 'Dingdong', ''], ['Xiang', 'Kun', ''], ['Li', 'Haoyuan', ''], ['Bai', 'Haoli', ''], ['Han', 'Jianhua', ''], ['Li', 'Xiaohui', ''], ['Jin', 'Weike', ''], ['Xie', 'Nian', ''], ['Zhang', 'Yu', ''], ['Kwok', 'James T.', ''], ['Zhao', 'Hengshuang', ''], ['Liang', 'Xiaodan', ''], ['Yeung', 'Dit-Yan', ''], ['Chen', 'Xiao', ''], ['Li', 'Zhenguo', ''], ['Zhang', 'Wei', ''], ['Liu', 'Qun', ''], ['Yao', 'Jun', ''], ['Hong', 'Lanqing', ''], ['Hou', 'Lu', ''], ['Xu', 'Hang', '']]","extracted_entities":"[{'text': 'GPT-4o', 'label': 'GPT'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'open-source community', 'label': 'Open-source LLMs'}, {'text': 'vision-language models', 'label': 'Large Language Model'}, {'text': 'speech-language models', 'label': 'Large Language Model'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2409.19606,"submitter":"Defa Zhu","authors":"Defa Zhu, Hongzhi Huang, Zihao Huang, Yutao Zeng, Yunyao Mao, Banggu\n  Wu, Qiyang Min, Xun Zhou","title":"Hyper-Connections","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL cs.CV cs.NE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present hyper-connections, a simple yet effective method that can serve as\nan alternative to residual connections. This approach specifically addresses\ncommon drawbacks observed in residual connection variants, such as the seesaw\neffect between gradient vanishing and representation collapse. Theoretically,\nhyper-connections allow the network to adjust the strength of connections\nbetween features at different depths and dynamically rearrange layers. We\nconduct experiments focusing on the pre-training of large language models,\nincluding dense and sparse models, where hyper-connections show significant\nperformance improvements over residual connections. Additional experiments\nconducted on vision tasks also demonstrate similar improvements. We anticipate\nthat this method will be broadly applicable and beneficial across a wide range\nof AI problems.\n","versions":"[{'version': 'v1', 'created': 'Sun, 29 Sep 2024 07:57:07 GMT'}, {'version': 'v2', 'created': 'Thu, 28 Nov 2024 08:09:05 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 10:12:54 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Zhu', 'Defa', ''], ['Huang', 'Hongzhi', ''], ['Huang', 'Zihao', ''], ['Zeng', 'Yutao', ''], ['Mao', 'Yunyao', ''], ['Wu', 'Banggu', ''], ['Min', 'Qiyang', ''], ['Zhou', 'Xun', '']]","extracted_entities":"[{'text': 'residual connections', 'label': 'Embedding'}, {'text': 'large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2409.19753,"submitter":"Yike Wu","authors":"Yike Wu, Yi Huang, Nan Hu, Yuncheng Hua, Guilin Qi, Jiaoyan Chen, Jeff\n  Z. Pan","title":"CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex\n  Knowledge Graph Question Answering","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent studies have explored the use of Large Language Models (LLMs) with\nRetrieval Augmented Generation (RAG) for Knowledge Graph Question Answering\n(KGQA). They typically require rewriting retrieved subgraphs into natural\nlanguage formats comprehensible to LLMs. However, when tackling complex\nquestions, the knowledge rewritten by existing methods may include irrelevant\ninformation, omit crucial details, or fail to align with the question's\nsemantics. To address them, we propose a novel rewriting method CoTKR,\nChain-of-Thought Enhanced Knowledge Rewriting, for generating reasoning traces\nand corresponding knowledge in an interleaved manner, thereby mitigating the\nlimitations of single-step knowledge rewriting. Additionally, to bridge the\npreference gap between the knowledge rewriter and the question answering (QA)\nmodel, we propose a training strategy PAQAF, Preference Alignment from Question\nAnswering Feedback, for leveraging feedback from the QA model to further\noptimize the knowledge rewriter. We conduct experiments using various LLMs\nacross several KGQA benchmarks. Experimental results demonstrate that, compared\nwith previous knowledge rewriting methods, CoTKR generates the most beneficial\nknowledge representation for QA models, which significantly improves the\nperformance of LLMs in KGQA.\n","versions":"[{'version': 'v1', 'created': 'Sun, 29 Sep 2024 16:08:45 GMT'}, {'version': 'v2', 'created': 'Tue, 8 Oct 2024 14:20:15 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 09:37:10 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Wu', 'Yike', ''], ['Huang', 'Yi', ''], ['Hu', 'Nan', ''], ['Hua', 'Yuncheng', ''], ['Qi', 'Guilin', ''], ['Chen', 'Jiaoyan', ''], ['Pan', 'Jeff Z.', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2409.20089,"submitter":"Lei Yu","authors":"Lei Yu, Virginie Do, Karen Hambardzumyan, Nicola Cancedda","title":"Robust LLM safeguarding via refusal feature adversarial training","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL cs.CR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs) are vulnerable to adversarial attacks that can\nelicit harmful responses. Defending against such attacks remains challenging\ndue to the opacity of jailbreaking mechanisms and the high computational cost\nof training LLMs robustly. We demonstrate that adversarial attacks share a\nuniversal mechanism for circumventing LLM safeguards that works by ablating a\ndimension in the residual stream embedding space called the refusal feature. We\nfurther show that the operation of refusal feature ablation (RFA) approximates\nthe worst-case perturbation of offsetting model safety. Based on these\nfindings, we propose Refusal Feature Adversarial Training (ReFAT), a novel\nalgorithm that efficiently performs LLM adversarial training by simulating the\neffect of input-level attacks via RFA. Experiment results show that ReFAT\nsignificantly improves the robustness of three popular LLMs against a wide\nrange of adversarial attacks, with considerably less computational overhead\ncompared to existing adversarial training methods.\n","versions":"[{'version': 'v1', 'created': 'Mon, 30 Sep 2024 08:41:39 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 15:28:18 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Yu', 'Lei', ''], ['Do', 'Virginie', ''], ['Hambardzumyan', 'Karen', ''], ['Cancedda', 'Nicola', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'residual stream embedding space', 'label': 'Embedding'}, {'text': 'refusal feature', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2410.0265,"submitter":"Ali Satvaty","authors":"Ali Satvaty, Suzan Verberne, Fatih Turkmen","title":"Undesirable Memorization in Large Language Models: A Survey","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While recent research increasingly showcases the remarkable capabilities of\nLarge Language Models (LLMs), it is equally crucial to examine their associated\nrisks. Among these, privacy and security vulnerabilities are particularly\nconcerning, posing significant ethical and legal challenges. At the heart of\nthese vulnerabilities stands memorization, which refers to a model's tendency\nto store and reproduce phrases from its training data. This phenomenon has been\nshown to be a fundamental source to various privacy and security attacks\nagainst LLMs. In this paper, we provide a taxonomy of the literature on LLM\nmemorization, exploring it across three dimensions: granularity,\nretrievability, and desirability. Next, we discuss the metrics and methods used\nto quantify memorization, followed by an analysis of the causes and factors\nthat contribute to memorization phenomenon. We then explore strategies that are\nused so far to mitigate the undesirable aspects of this phenomenon. We conclude\nour survey by identifying potential research topics for the near future,\nincluding methods to balance privacy and performance, and the analysis of\nmemorization in specific LLM contexts such as conversational agents,\nretrieval-augmented generation, and diffusion language models. Given the rapid\nresearch pace in this field, we also maintain a dedicated repository of the\nreferences discussed in this survey which will be regularly updated to reflect\nthe latest developments.\n","versions":"[{'version': 'v1', 'created': 'Thu, 3 Oct 2024 16:34:46 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 18:50:38 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Satvaty', 'Ali', ''], ['Verberne', 'Suzan', ''], ['Turkmen', 'Fatih', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ethical and legal challenges', 'label': 'AI Ethics'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'desirability', 'label': 'Model Bias and Fairness'}, {'text': 'conversational agents', 'label': 'ChatGPT'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2410.03781,"submitter":"Romain Puech","authors":"Romain Puech, Jakub Macina, Julia Chatain, Mrinmaya Sachan, Manu Kapur","title":"Towards the Pedagogical Steering of Large Language Models for Tutoring:\n  A Case Study with Modeling Productive Failure","comments":"19 pages, 10 figures, 6 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC cs.AI cs.CY cs.MA","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  One-to-one tutoring is one of the most efficient methods of teaching. With\nthe growing popularity of Large Language Models (LLMs), there have been efforts\nto create LLM based conversational tutors which can expand the benefits of one\nto one tutoring to everyone. However, current LLMs are trained primarily to be\nhelpful assistants and lack crucial pedagogical skills. For example, they often\nquickly reveal the solution to the student and fail to plan for a richer multi\nturn pedagogical interaction. To use LLMs in pedagogical settings, they need to\nbe steered to use effective teaching strategies: a problem we introduce as\nPedagogical Steering. We develop StratL, an algorithm to optimize LLM prompts\nand steer it to follow a predefined multi-turn tutoring plan represented as a\ntransition graph. As a case study, we create a prototype tutor for high school\nmath following Productive Failure (PF), an advanced and effective learning\ndesign. To validate our approach in a real-world setting, we run a field study\nwith 17 high school students in Singapore and show that StratL succeeds in\nsteering the LLM to follow the PF tutoring strategy. Finally, we highlight\nchallenges in Pedagogical Steering of LLMs and offer opportunities for further\nimprovements by publishing a dataset of PF problems and our code.\n","versions":"[{'version': 'v1', 'created': 'Thu, 3 Oct 2024 16:15:41 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 19:44:51 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Puech', 'Romain', ''], ['Macina', 'Jakub', ''], ['Chatain', 'Julia', ''], ['Sachan', 'Mrinmaya', ''], ['Kapur', 'Manu', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Pedagogical Steering', 'label': 'Prompting'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'Pedagogical Steering', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2410.03834,"submitter":"Tao Feng","authors":"Tao Feng, Yanzhen Shen, Jiaxuan You","title":"GraphRouter: A Graph-based Router for LLM Selections","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The rapidly growing number and variety of Large Language Models (LLMs)\npresent significant challenges in efficiently selecting the appropriate LLM for\na given query, especially considering the trade-offs between performance and\ncomputational cost. Current LLM selection methods often struggle to generalize\nacross new LLMs and different tasks because of their limited ability to\nleverage contextual interactions among tasks, queries, and LLMs, as well as\ntheir dependence on a transductive learning framework. To address these\nshortcomings, we introduce a novel inductive graph framework, named as\nGraphRouter, which fully utilizes the contextual information among tasks,\nqueries, and LLMs to enhance the LLM selection process. GraphRouter constructs\na heterogeneous graph comprising task, query, and LLM nodes, with interactions\nrepresented as edges, which efficiently captures the contextual information\nbetween the query's requirements and the LLM's capabilities. Through an\ninnovative edge prediction mechanism, GraphRouter is able to predict attributes\n(the effect and cost of LLM response) of potential edges, allowing for\noptimized recommendations that adapt to both existing and newly introduced LLMs\nwithout requiring retraining. Comprehensive experiments across three distinct\neffect-cost weight scenarios have shown that GraphRouter substantially\nsurpasses existing routers, delivering a minimum performance improvement of\n12.3%. In addition, it achieves enhanced generalization across new LLMs\nsettings and supports diverse tasks with at least a 9.5% boost in effect and a\nsignificant reduction in computational demands. This work endeavors to apply a\ngraph-based approach for the contextual and adaptive selection of LLMs,\noffering insights for real-world applications. Our codes for GraphRouter is\nreleased at https:\/\/github.com\/ulab-uiuc\/GraphRouter.\n","versions":"[{'version': 'v1', 'created': 'Fri, 4 Oct 2024 18:02:48 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 15:08:47 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Feng', 'Tao', ''], ['Shen', 'Yanzhen', ''], ['You', 'Jiaxuan', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2410.05406,"submitter":"Carlo Bosio","authors":"Carlo Bosio and Mark W. Mueller","title":"Synthesizing Interpretable Control Policies through Large Language Model\n  Guided Search","comments":"8 pages, 7 figures, conference paper","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.SY eess.SY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The combination of Large Language Models (LLMs), systematic evaluation, and\nevolutionary algorithms has enabled breakthroughs in combinatorial optimization\nand scientific discovery. We propose to extend this powerful combination to the\ncontrol of dynamical systems, generating interpretable control policies capable\nof complex behaviors. With our novel method, we represent control policies as\nprograms in standard languages like Python. We evaluate candidate controllers\nin simulation and evolve them using a pre-trained LLM. Unlike conventional\nlearning-based control techniques, which rely on black-box neural networks to\nencode control policies, our approach enhances transparency and\ninterpretability. We still take advantage of the power of large AI models, but\nonly at the policy design phase, ensuring that all system components remain\ninterpretable and easily verifiable at runtime. Additionally, the use of\nstandard programming languages makes it straightforward for humans to finetune\nor adapt the controllers based on their expertise and intuition. We illustrate\nour method through its application to the synthesis of an interpretable control\npolicy for the pendulum swing-up and the ball in cup tasks. We make the code\navailable at\nhttps:\/\/github.com\/muellerlab\/synthesizing_interpretable_control_policies.git.\n","versions":"[{'version': 'v1', 'created': 'Mon, 7 Oct 2024 18:12:20 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 21:49:35 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Bosio', 'Carlo', ''], ['Mueller', 'Mark W.', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2410.06981,"submitter":"Michael Lan","authors":"Michael Lan, Philip Torr, Austin Meek, Ashkan Khakzar, David Krueger,\n  Fazl Barez","title":"Sparse Autoencoders Reveal Universal Feature Spaces Across Large\n  Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We investigate feature universality in large language models (LLMs), a\nresearch field that aims to understand how different models similarly represent\nconcepts in the latent spaces of their intermediate layers. Demonstrating\nfeature universality allows discoveries about latent representations to\ngeneralize across several models. However, comparing features across LLMs is\nchallenging due to polysemanticity, in which individual neurons often\ncorrespond to multiple features rather than distinct ones, making it difficult\nto disentangle and match features across different models. To address this\nissue, we employ a method known as dictionary learning by using sparse\nautoencoders (SAEs) to transform LLM activations into more interpretable spaces\nspanned by neurons corresponding to individual features. After matching feature\nneurons across models via activation correlation, we apply representational\nspace similarity metrics on SAE feature spaces across different LLMs. Our\nexperiments reveal significant similarities in SAE feature spaces across\nvarious LLMs, providing new evidence for feature universality.\n","versions":"[{'version': 'v1', 'created': 'Wed, 9 Oct 2024 15:18:57 GMT'}, {'version': 'v2', 'created': 'Fri, 31 Jan 2025 15:27:10 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 00:31:46 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Lan', 'Michael', ''], ['Torr', 'Philip', ''], ['Meek', 'Austin', ''], ['Khakzar', 'Ashkan', ''], ['Krueger', 'David', ''], ['Barez', 'Fazl', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'dictionary learning', 'label': 'Few-shot Learning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2410.08437,"submitter":"Daniel Bramblett","authors":"Rushang Karia, Daniel Bramblett, Daksh Dobhal, Siddharth Srivastava","title":"AutoEval: Autonomous Evaluation of LLMs for Truth Maintenance and\n  Reasoning Tasks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper presents AutoEval, a novel benchmark for scaling Large Language\nModel (LLM) assessment in formal tasks with clear notions of correctness, such\nas truth maintenance in translation and logical reasoning. AutoEval is the\nfirst benchmarking paradigm that offers several key advantages necessary for\nscaling objective evaluation of LLMs without human labeling: (a) ability to\nevaluate LLMs of increasing sophistication by auto-generating tasks at\ndifferent levels of difficulty; (b) auto-generation of ground truth that\neliminates dependence on expensive and time-consuming human annotation; (c) the\nuse of automatically generated, randomized datasets that mitigate the ability\nof successive LLMs to overfit to static datasets used in many contemporary\nbenchmarks. Empirical analysis shows that an LLM's performance on AutoEval is\nhighly indicative of its performance on a diverse array of other benchmarks\nfocusing on translation and reasoning tasks, making it a valuable autonomous\nevaluation paradigm in settings where hand-curated datasets can be hard to\nobtain and\/or update.\n","versions":"[{'version': 'v1', 'created': 'Fri, 11 Oct 2024 00:56:37 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 21:03:16 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Karia', 'Rushang', ''], ['Bramblett', 'Daniel', ''], ['Dobhal', 'Daksh', ''], ['Srivastava', 'Siddharth', '']]","extracted_entities":"[{'text': 'Large Language\\nModel', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language\nModel","similarity_score":1.0}
{"id":2410.09907,"submitter":"Alexandru-Iulius Jerpelea","authors":"Ecaterina \\c{S}tef\\u{a}nescu and Alexandru-Iulius Jerpelea","title":"Reddit is all you need: Authorship profiling for Romanian","comments":"10 pages, 5 tables and 1 figure, published and presented at The 19th\n  International Conference on Linguistic Resources and Tools for Natural\n  Language Processing (ConsILR 2024)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Authorship profiling is the process of identifying an author's\ncharacteristics based on their writings. This centuries old problem has become\nmore intriguing especially with recent developments in Natural Language\nProcessing (NLP). In this paper, we introduce a corpus of short texts in the\nRomanian language, annotated with certain author characteristic keywords; to\nour knowledge, the first of its kind. In order to do this, we exploit a social\nmedia platform called Reddit. We leverage its thematic community-based\nstructure (subreddits structure), which offers information about the author's\nbackground. We infer an user's demographic and some broad personal traits, such\nas age category, employment status, interests, and social orientation based on\nthe subreddit and other cues. We thus obtain a 23k+ samples corpus, extracted\nfrom 100+ Romanian subreddits. We analyse our dataset, and finally, we\nfine-tune and evaluate Large Language Models (LLMs) to prove baselines\ncapabilities for authorship profiling using the corpus, indicating the need for\nfurther research in the field. We publicly release all our resources.\n","versions":"[{'version': 'v1', 'created': 'Sun, 13 Oct 2024 16:27:31 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 19:48:28 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['\u015etef\u0103nescu', 'Ecaterina', ''], ['Jerpelea', 'Alexandru-Iulius', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2410.10491,"submitter":"Aritra Bhowmik","authors":"Aritra Bhowmik, Mohammad Mahdi Derakhshani, Dennis Koelma, Yuki M.\n  Asano, Martin R. Oswald, Cees G. M. Snoek","title":"TWIST & SCOUT: Grounding Multimodal LLM-Experts by Forget-Free Tuning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Spatial awareness is key to enable embodied multimodal AI systems. Yet,\nwithout vast amounts of spatial supervision, current Multimodal Large Language\nModels (MLLMs) struggle at this task. In this paper, we introduce TWIST &\nSCOUT, a framework that equips pre-trained MLLMs with visual grounding ability\nwithout forgetting their existing image and language understanding skills. To\nthis end, we propose TWIST, a twin-expert stepwise tuning module that modifies\nthe decoder of the language model using one frozen module pre-trained on image\nunderstanding tasks and another learnable one for visual grounding tasks. This\nallows the MLLM to retain previously learned knowledge and skills, while\nacquiring what is missing. To fine-tune the model effectively, we generate a\nhigh-quality synthetic dataset we call SCOUT, which mimics human reasoning in\nvisual grounding. This dataset provides rich supervision signals, describing a\nstep-by-step multimodal reasoning process, thereby simplifying the task of\nvisual grounding. We evaluate our approach on several standard benchmark\ndatasets, encompassing grounded image captioning, zero-shot localization, and\nvisual grounding tasks. Our method consistently delivers strong performance\nacross all tasks, while retaining the pre-trained image understanding\ncapabilities.\n","versions":"[{'version': 'v1', 'created': 'Mon, 14 Oct 2024 13:35:47 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 15:32:47 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Bhowmik', 'Aritra', ''], ['Derakhshani', 'Mohammad Mahdi', ''], ['Koelma', 'Dennis', ''], ['Asano', 'Yuki M.', ''], ['Oswald', 'Martin R.', ''], ['Snoek', 'Cees G. M.', '']]","extracted_entities":"[{'text': 'Multimodal Large Language\\nModels', 'label': 'Large Language Model'}, {'text': 'TWIST', 'label': 'Generative Pre-trained Transformer (GPT)'}, {'text': 'zero-shot localization', 'label': 'Zero-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language\nModels","similarity_score":0.7649828196}
{"id":2410.10624,"submitter":"Zechen Li","authors":"Zechen Li, Shohreh Deldari, Linyao Chen, Hao Xue and Flora D. Salim","title":"SensorLLM: Aligning Large Language Models with Motion Sensors for Human\n  Activity Recognition","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We introduce SensorLLM, a two-stage framework that enables Large Language\nModels (LLMs) to perform human activity recognition (HAR) from sensor data.\nDespite their strong reasoning and generalization capabilities, LLMs remain\nunderutilized for motion sensor data due to the lack of semantic context in\ntime-series, computational constraints, and challenges in processing numerical\ninputs. SensorLLM addresses these limitations through a Sensor-Language\nAlignment stage, where we introduce special tokens for each sensor channel and\nautomatically generate textual trend descriptions. This alignment enables LLMs\nto capture numerical variations, channel-specific features, and data of varying\nduration--without requiring human annotations. In the subsequent Task-Aware\nTuning stage, we refine the model for HAR classification, achieving performance\nthat matches or surpasses state-of-the-art methods. Our results demonstrate\nthat SensorLLM evolves into an effective sensor learner, reasoner, and\nclassifier through Sensor-Language Alignment, generalizing across diverse HAR\ndatasets. We believe this work establishes a foundation for future research on\ntime-series and text alignment, paving the way for foundation models in sensor\ndata analysis.\n","versions":"[{'version': 'v1', 'created': 'Mon, 14 Oct 2024 15:30:41 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 09:28:43 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Li', 'Zechen', ''], ['Deldari', 'Shohreh', ''], ['Chen', 'Linyao', ''], ['Xue', 'Hao', ''], ['Salim', 'Flora D.', '']]","extracted_entities":"[{'text': 'SensorLLM', 'label': 'Foundation Model'}, {'text': 'Large Language\\nModels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'SensorLLM', 'label': 'Foundation Model'}, {'text': 'Sensor-Language\\nAlignment', 'label': 'contextual Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Task-Aware\\nTuning stage', 'label': 'Fine-tuning'}, {'text': 'SensorLLM', 'label': 'Foundation Model'}, {'text': 'Sensor-Language Alignment', 'label': 'Embedding'}, {'text': 'foundation models', 'label': 'Foundation Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language\nModels","similarity_score":0.9664971828}
{"id":2410.11374,"submitter":"Yoonjeon Kim","authors":"Yoonjeon Kim, Soohyun Ryu, Yeonsung Jung, Hyunkoo Lee, Joowon Kim,\n  June Yong Yang, Jaeryong Hwang, Eunho Yang","title":"Preserve or Modify? Context-Aware Evaluation for Balancing Preservation\n  and Modification in Text-Guided Image Editing","comments":"accepted to CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The development of vision-language and generative models has significantly\nadvanced text-guided image editing, which seeks the preservation of core\nelements in the source image while implementing modifications based on the\ntarget text. However, existing metrics have a context-blindness problem,\nindiscriminately applying the same evaluation criteria on completely different\npairs of source image and target text, biasing towards either modification or\npreservation. Directional CLIP similarity, the only metric that considers both\nsource image and target text, is also biased towards modification aspects and\nattends to irrelevant editing regions of the image. We propose AugCLIP, a\ncontext-aware metric that adaptively coordinates preservation and modification\naspects, depending on the specific context of a given source image and target\ntext. This is done by deriving the CLIP representation of an ideally edited\nimage, that preserves the source image with necessary modifications to align\nwith target text. More specifically, using a multi-modal large language model,\nAugCLIP augments the textual descriptions of the source and target, then\ncalculates a modification vector through a hyperplane that separates source and\ntarget attributes in CLIP space. Extensive experiments on five benchmark\ndatasets, encompassing a diverse range of editing scenarios, show that AugCLIP\naligns remarkably well with human evaluation standards, outperforming existing\nmetrics. The code is available at https:\/\/github.com\/augclip\/augclip_eval.\n","versions":"[{'version': 'v1', 'created': 'Tue, 15 Oct 2024 08:12:54 GMT'}, {'version': 'v2', 'created': 'Wed, 4 Dec 2024 07:35:20 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 07:36:52 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Kim', 'Yoonjeon', ''], ['Ryu', 'Soohyun', ''], ['Jung', 'Yeonsung', ''], ['Lee', 'Hyunkoo', ''], ['Kim', 'Joowon', ''], ['Yang', 'June Yong', ''], ['Hwang', 'Jaeryong', ''], ['Yang', 'Eunho', '']]","extracted_entities":"[{'text': 'multi-modal large language model', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"multi-modal large language model","similarity_score":0.8085874915}
{"id":2410.12444,"submitter":"Mengze Hong","authors":"Mengze Hong, Chen Jason Zhang, Di Jiang, Yuanfeng Song, Lu Wang,\n  Yuanqin He, Zhiyang Su, Qing Li","title":"Expanding Chatbot Knowledge in Customer Service: Context-Aware Similar\n  Question Generation Using Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Service chatbots play an important role in enhancing customer support by\ndelivering timely responses to diverse queries. Traditionally, these chatbots\nrely on retrieval-based methods constrained by a predefined knowledge base of\nquestion-answer (QA) pairs to guarantee reliable responses. To effectively\nhandle varied customer inquiries, augmenting the knowledge base with similar\nquestions that maintain semantic consistency and linguistic variability is\ncrucial. This paper presents methodologies for a novel approach that utilizes\nLarge Language Models (LLMs) for generating similar questions and selecting an\noptimal subset of questions for knowledge base augmentation in industrial\nchatbots. Specifically, we define the SQG task in the context of LLM training\nand propose a one-to-many objective that incorporates contextual information.\nWe also introduce an optimization framework that selects a diverse subset of\nsimilar questions within predefined resource constraints. Experimental results\ndemonstrate significant improvements over traditional methods, achieving\ngreater semantic diversity while aligning with source QA pairs, with over 120%\nrelative improvement in meeting business-specific requirements with human\nevaluation. Combined with several best practices, we provide a robust,\napplication-driven solution for enhancing chatbot performance and improving\ncustomer service satisfaction.\n","versions":"[{'version': 'v1', 'created': 'Wed, 16 Oct 2024 10:48:14 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 06:22:38 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Hong', 'Mengze', ''], ['Zhang', 'Chen Jason', ''], ['Jiang', 'Di', ''], ['Song', 'Yuanfeng', ''], ['Wang', 'Lu', ''], ['He', 'Yuanqin', ''], ['Su', 'Zhiyang', ''], ['Li', 'Qing', '']]","extracted_entities":"[{'text': 'Service chatbots', 'label': 'ChatGPT'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2410.12782,"submitter":"Yida Yin","authors":"Yida Yin, Zekai Wang, Yuvan Sharma, Dantong Niu, Trevor Darrell, Roei\n  Herzig","title":"In-Context Learning Enables Robot Action Prediction in LLMs","comments":"Published in ICRA 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recently, Large Language Models (LLMs) have achieved remarkable success using\nin-context learning (ICL) in the language domain. However, leveraging the ICL\ncapabilities within LLMs to directly predict robot actions remains largely\nunexplored. In this paper, we introduce RoboPrompt, a framework that enables\noff-the-shelf text-only LLMs to directly predict robot actions through ICL\nwithout training. Our approach first heuristically identifies keyframes that\ncapture important moments from an episode. Next, we extract end-effector\nactions from these keyframes as well as the estimated initial object poses, and\nboth are converted into textual descriptions. Finally, we construct a\nstructured template to form ICL demonstrations from these textual descriptions\nand a task instruction. This enables an LLM to directly predict robot actions\nat test time. Through extensive experiments and analysis, RoboPrompt shows\nstronger performance over zero-shot and ICL baselines in simulated and\nreal-world settings. Our project page is available at\nhttps:\/\/davidyyd.github.io\/roboprompt.\n","versions":"[{'version': 'v1', 'created': 'Wed, 16 Oct 2024 17:56:49 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 10:43:54 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Yin', 'Yida', ''], ['Wang', 'Zekai', ''], ['Sharma', 'Yuvan', ''], ['Niu', 'Dantong', ''], ['Darrell', 'Trevor', ''], ['Herzig', 'Roei', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'RoboPrompt', 'label': 'Open-source LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'RoboPrompt', 'label': 'Open-source LLMs'}, {'text': 'zero-shot', 'label': 'Zero-shot Learning'}, {'text': 'ICL', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2410.13788,"submitter":"Michael J.Q. Zhang","authors":"Michael J.Q. Zhang, W. Bradley Knox, Eunsol Choi","title":"Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying\n  Questions","comments":"Presented at ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Large language models (LLMs) must often respond to highly ambiguous user\nrequests. In such cases, the LLM's best response may be to ask a clarifying\nquestion to elicit more information. Existing LLMs often respond by\npresupposing a single interpretation of such ambiguous requests, frustrating\nusers who intended a different interpretation. We speculate this is caused by\ncurrent preference data labeling practice, where LLM responses are evaluated\nonly on their prior contexts. To address this, we assign preference labels by\nsimulating their expected outcomes in future turns. This allows LLMs to learn\nto ask clarifying questions when it can generate responses that are tailored to\neach user interpretation in future turns. On open-domain QA datasets with\nmultiple annotations, we evaluate systems based on their ability to ask\nclarifying questions to recover each user's interpretation and expected answer.\nWe compare systems trained using our proposed preference labeling methods\nagainst standard methods, which assign preferences based on only prior context.\nOur method achieves a 5% improvement in F1 measured against the answer set from\ndifferent interpretations of each query, showing the value of modeling future\nconversation turns. We further demonstrate that our method can be used to train\nmodels to judiciously determine when to ask clarifying questions, directly\nanswering the question when clarification is unnecessary. In our experiments,\nwe find that our method achieves a 3% improvement in accuracy of such judgments\nover existing methods.\n","versions":"[{'version': 'v1', 'created': 'Thu, 17 Oct 2024 17:29:04 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 14:17:47 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Zhang', 'Michael J. Q.', ''], ['Knox', 'W. Bradley', ''], ['Choi', 'Eunsol', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2410.1725,"submitter":"Shota Onohara","authors":"Shota Onohara, Atsuyuki Miyai, Yuki Imajuku, Kazuki Egashira, Jeonghun\n  Baek, Xiang Yue, Graham Neubig, Kiyoharu Aizawa","title":"JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding\n  Benchmark for Culture-aware Evaluation","comments":"Accepted at NAACL 2025. Project page:\n  https:\/\/mmmu-japanese-benchmark.github.io\/JMMMU\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Accelerating research on Large Multimodal Models (LMMs) in non-English\nlanguages is crucial for enhancing user experiences across broader populations.\nIn this paper, we introduce JMMMU (Japanese MMMU), the first large-scale\nJapanese benchmark designed to evaluate LMMs on expert-level tasks based on the\nJapanese cultural context. To facilitate comprehensive culture-aware\nevaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA)\nsubset, where the culture-independent subjects (e.g., Math) are selected and\ntranslated into Japanese, enabling one-to-one comparison with its English\ncounterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly\ncrafted subjects that reflect Japanese cultural context. Using the CA subset,\nwe observe performance drop in many LMMs when evaluated in Japanese, which is\npurely attributable to language variation. Using the CS subset, we reveal their\ninadequate Japanese cultural understanding. Further, by combining both subsets,\nwe identify that some LMMs perform well on the CA subset but not on the CS\nsubset, exposing a shallow understanding of the Japanese language that lacks\ndepth in cultural understanding. We hope this work will not only help advance\nLMM performance in Japanese but also serve as a guideline to create\nhigh-standard, culturally diverse benchmarks for multilingual LMM development.\nThe project page is https:\/\/mmmu-japanese-benchmark.github.io\/JMMMU\/.\n","versions":"[{'version': 'v1', 'created': 'Tue, 22 Oct 2024 17:59:56 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 08:24:14 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Onohara', 'Shota', ''], ['Miyai', 'Atsuyuki', ''], ['Imajuku', 'Yuki', ''], ['Egashira', 'Kazuki', ''], ['Baek', 'Jeonghun', ''], ['Yue', 'Xiang', ''], ['Neubig', 'Graham', ''], ['Aizawa', 'Kiyoharu', '']]","extracted_entities":"[{'text': 'Large Multimodal Models', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Multimodal Models","similarity_score":0.5739125013}
{"id":2410.18325,"submitter":"Kim Sung-Bin","authors":"Kim Sung-Bin, Oh Hyun-Bin, JungMok Lee, Arda Senocak, Joon Son Chung,\n  Tae-Hyun Oh","title":"AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large\n  Language Models","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Following the success of Large Language Models (LLMs), expanding their\nboundaries to new modalities represents a significant paradigm shift in\nmultimodal understanding. Human perception is inherently multimodal, relying\nnot only on text but also on auditory and visual cues for a complete\nunderstanding of the world. In recognition of this fact, audio-visual LLMs have\nrecently emerged. Despite promising developments, the lack of dedicated\nbenchmarks poses challenges for understanding and evaluating models. In this\nwork, we show that audio-visual LLMs struggle to discern subtle relationships\nbetween audio and visual signals, leading to hallucinations and highlighting\nthe need for reliable benchmarks. To address this, we introduce AVHBench, the\nfirst comprehensive benchmark specifically designed to evaluate the perception\nand comprehension capabilities of audio-visual LLMs. Our benchmark includes\ntests for assessing hallucinations, as well as the cross-modal matching and\nreasoning abilities of these models. Our results reveal that most existing\naudio-visual LLMs struggle with hallucinations caused by cross-interactions\nbetween modalities, due to their limited capacity to perceive complex\nmultimodal signals and their relationships. Additionally, we demonstrate that\nsimple training with our AVHBench improves robustness of audio-visual LLMs\nagainst hallucinations. Dataset: https:\/\/github.com\/kaist-ami\/AVHBench\n","versions":"[{'version': 'v1', 'created': 'Wed, 23 Oct 2024 23:36:06 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 08:14:35 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Sung-Bin', 'Kim', ''], ['Hyun-Bin', 'Oh', ''], ['Lee', 'JungMok', ''], ['Senocak', 'Arda', ''], ['Chung', 'Joon Son', ''], ['Oh', 'Tae-Hyun', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'audio-visual LLMs', 'label': 'Large Language Model'}, {'text': 'audio-visual LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2410.19482,"submitter":"Jamie Hayes","authors":"Jamie Hayes, Marika Swanberg, Harsh Chaudhari, Itay Yona, Ilia\n  Shumailov, Milad Nasr, Christopher A. Choquette-Choo, Katherine Lee, A. Feder\n  Cooper","title":"Measuring memorization in language models via probabilistic extraction","comments":"NAACL 25","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs) are susceptible to memorizing training data,\nraising concerns about the potential extraction of sensitive information at\ngeneration time. Discoverable extraction is the most common method for\nmeasuring this issue: split a training example into a prefix and suffix, then\nprompt the LLM with the prefix, and deem the example extractable if the LLM\ngenerates the matching suffix using greedy sampling. This definition yields a\nyes-or-no determination of whether extraction was successful with respect to a\nsingle query. Though efficient to compute, we show that this definition is\nunreliable because it does not account for non-determinism present in more\nrealistic (non-greedy) sampling schemes, for which LLMs produce a range of\noutputs for the same prompt. We introduce probabilistic discoverable\nextraction, which, without additional cost, relaxes discoverable extraction by\nconsidering multiple queries to quantify the probability of extracting a target\nsequence. We evaluate our probabilistic measure across different models,\nsampling schemes, and training-data repetitions, and find that this measure\nprovides more nuanced information about extraction risk compared to traditional\ndiscoverable extraction.\n","versions":"[{'version': 'v1', 'created': 'Fri, 25 Oct 2024 11:37:04 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Mar 2025 14:25:10 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 15:35:56 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Hayes', 'Jamie', ''], ['Swanberg', 'Marika', ''], ['Chaudhari', 'Harsh', ''], ['Yona', 'Itay', ''], ['Shumailov', 'Ilia', ''], ['Nasr', 'Milad', ''], ['Choquette-Choo', 'Christopher A.', ''], ['Lee', 'Katherine', ''], ['Cooper', 'A. Feder', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2410.21113,"submitter":"Jo\\~ao Pereira","authors":"Joao Pereira, Vasco Lopes, David Semedo, Joao Neves","title":"Zero-Shot Action Recognition in Surveillance Videos","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The growing demand for surveillance in public spaces presents significant\nchallenges due to the shortage of human resources. Current AI-based video\nsurveillance systems heavily rely on core computer vision models that require\nextensive finetuning, which is particularly difficult in surveillance settings\ndue to limited datasets and difficult setting (viewpoint, low quality, etc.).\nIn this work, we propose leveraging Large Vision-Language Models (LVLMs), known\nfor their strong zero and few-shot generalization, to tackle video\nunderstanding tasks in surveillance. Specifically, we explore VideoLLaMA2, a\nstate-of-the-art LVLM, and an improved token-level sampling method,\nSelf-Reflective Sampling (Self-ReS). Our experiments on the UCF-Crime dataset\nshow that VideoLLaMA2 represents a significant leap in zero-shot performance,\nwith 20% boost over the baseline. Self-ReS additionally increases zero-shot\naction recognition performance to 44.6%. These results highlight the potential\nof LVLMs, paired with improved sampling techniques, for advancing surveillance\nvideo analysis in diverse scenarios.\n","versions":"[{'version': 'v1', 'created': 'Mon, 28 Oct 2024 15:13:53 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 13:30:27 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Pereira', 'Joao', ''], ['Lopes', 'Vasco', ''], ['Semedo', 'David', ''], ['Neves', 'Joao', '']]","extracted_entities":"[{'text': 'extensive finetuning', 'label': 'Fine-tuning'}, {'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'VideoLLaMA2', 'label': 'Large Language Model'}, {'text': 'Self-Reflective Sampling', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Vision-Language Models","similarity_score":0.7742220759}
{"id":2411.10156,"submitter":"Libo Wang","authors":"Libo Wang","title":"Mitigating Sycophancy in Decoder-Only Transformer Architectures:\n  Synthetic Data Intervention","comments":"The data set, experimental process, code and data results have been\n  uploaded to Github repository, the link is\n  https:\/\/github.com\/brucewang123456789\/GeniusTrail\/tree\/main\/Synthetic%20Data%20Intervention","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  To address the sycophancy problem caused by reinforcement learning from human\nfeedback in large language models, this research applies synthetic data\nintervention technology to the decoder-only transformer architecture. Based on\nthe research gaps in the existing literature, the researcher designed an\nexperimental process to reduce the tendency of models to cater by generating\ndiversified data, and used GPT4o as an experimental tool for verification. The\nexperiment used 100 true and false questions, and compared the performance of\nthe model trained with synthetic data intervention and the original untrained\nmodel on multiple indicators. The results show that the SDI training model\nsupports the technology in terms of accuracy rate and sycophancy rate and has\nsignificant effectiveness in reducing sycophancy phenomena.\n","versions":"[{'version': 'v1', 'created': 'Fri, 15 Nov 2024 12:59:46 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Nov 2024 11:52:09 GMT'}, {'version': 'v3', 'created': 'Fri, 17 Jan 2025 09:49:37 GMT'}, {'version': 'v4', 'created': 'Fri, 24 Jan 2025 19:52:57 GMT'}, {'version': 'v5', 'created': 'Thu, 20 Mar 2025 13:29:49 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wang', 'Libo', '']]","extracted_entities":"[{'text': 'reinforcement learning', 'label': 'Few-shot Learning'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'GPT4o', 'label': 'GPT'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2411.12951,"submitter":"Minjoon Jung","authors":"Minjoon Jung, Junbin Xiao, Byoung-Tak Zhang, Angela Yao","title":"On the Consistency of Video Large Language Models in Temporal\n  Comprehension","comments":"Accepted to CVPR'25","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Video large language models (Video-LLMs) can temporally ground language\nqueries and retrieve video moments. Yet, such temporal comprehension\ncapabilities are neither well-studied nor understood. So we conduct a study on\nprediction consistency -- a key indicator for robustness and trustworthiness of\ntemporal grounding. After the model identifies an initial moment within the\nvideo content, we apply a series of probes to check if the model's responses\nalign with this initial grounding as an indicator of reliable comprehension.\nOur results reveal that current Video-LLMs are sensitive to variations in video\ncontents, language queries, and task settings, unveiling severe deficiencies in\nmaintaining consistency. We further explore common prompting and\ninstruction-tuning methods as potential solutions, but find that their\nimprovements are often unstable. To that end, we propose event temporal\nverification tuning that explicitly accounts for consistency, and demonstrate\nsignificant improvements for both grounding and consistency. Our data and code\nare open-sourced at https:\/\/github.com\/minjoong507\/Consistency-of-Video-LLM.\n","versions":"[{'version': 'v1', 'created': 'Wed, 20 Nov 2024 00:47:17 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 02:21:38 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Jung', 'Minjoon', ''], ['Xiao', 'Junbin', ''], ['Zhang', 'Byoung-Tak', ''], ['Yao', 'Angela', '']]","extracted_entities":"[{'text': 'Video large language models', 'label': 'Large Language Model'}, {'text': 'Video-LLMs', 'label': 'Large Language Model'}, {'text': 'Video-LLMs', 'label': 'Large Language Model'}, {'text': 'event temporal\\nverification tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Large Language Model","matched_keyword":"Video large language models","similarity_score":0.76164186}
{"id":2411.18688,"submitter":"Soumya Suvra Ghosal Mr.","authors":"Soumya Suvra Ghosal, Souradip Chakraborty, Vaibhav Singh, Tianrui\n  Guan, Mengdi Wang, Ahmad Beirami, Furong Huang, Alvaro Velasquez, Dinesh\n  Manocha, Amrit Singh Bedi","title":"Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via\n  Inference-Time Alignment","comments":"Accepted to CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  With the widespread deployment of Multimodal Large Language Models (MLLMs)\nfor visual-reasoning tasks, improving their safety has become crucial. Recent\nresearch indicates that despite training-time safety alignment, these models\nremain vulnerable to jailbreak attacks. In this work, we first highlight an\nimportant safety gap to describe that alignment achieved solely through safety\ntraining may be insufficient against jailbreak attacks. To address this\nvulnerability, we propose Immune, an inference-time defense framework that\nleverages a safe reward model through controlled decoding to defend against\njailbreak attacks. Additionally, we provide a mathematical characterization of\nImmune, offering insights on why it improves safety against jailbreaks.\nExtensive evaluations on diverse jailbreak benchmarks using recent MLLMs reveal\nthat Immune effectively enhances model safety while preserving the model's\noriginal capabilities. For instance, against text-based jailbreak attacks on\nLLaVA-1.6, Immune reduces the attack success rate by 57.82% and 16.78% compared\nto the base MLLM and state-of-the-art defense strategy, respectively.\n","versions":"[{'version': 'v1', 'created': 'Wed, 27 Nov 2024 19:00:10 GMT'}, {'version': 'v2', 'created': 'Fri, 20 Dec 2024 18:48:27 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 16:07:09 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Ghosal', 'Soumya Suvra', ''], ['Chakraborty', 'Souradip', ''], ['Singh', 'Vaibhav', ''], ['Guan', 'Tianrui', ''], ['Wang', 'Mengdi', ''], ['Beirami', 'Ahmad', ''], ['Huang', 'Furong', ''], ['Velasquez', 'Alvaro', ''], ['Manocha', 'Dinesh', ''], ['Bedi', 'Amrit Singh', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2411.19146,"submitter":"Ido Galil","authors":"Akhiad Bercovich, Tomer Ronen, Talor Abramovich, Nir Ailon, Nave\n  Assaf, Mohammad Dabbah, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak\n  Golan, Netanel Haber, Ehud Karpas, Roi Koren, Itay Levy, Pavlo Molchanov,\n  Shahar Mor, Zach Moshe, Najeeb Nabwani, Omri Puny, Ran Rubin, Itamar Schen,\n  Ido Shahaf, Oren Tropp, Omer Ullman Argov, Ran Zilberstein, Ran El-Yaniv","title":"Puzzle: Distillation-Based NAS for Inference-Optimized LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs) offer remarkable capabilities, yet their high\ninference costs restrict wider adoption. While increasing parameter counts\nimproves accuracy, it also broadens the gap between state-of-the-art\ncapabilities and practical deployability. We present Puzzle, a hardware-aware\nframework that accelerates the inference of LLMs while preserving their\ncapabilities. Using neural architecture search (NAS) at a large-scale, Puzzle\noptimizes models with tens of billions of parameters. Our approach utilizes\nblockwise local knowledge distillation (BLD) for parallel architecture\nexploration and employs mixed-integer programming for precise constraint\noptimization.\n  We showcase our framework's impact via Llama-3.1-Nemotron-51B-Instruct\n(Nemotron-51B), a publicly available model derived from Llama-3.1-70B-Instruct.\nNemotron-51B achieves a 2.17x inference throughput speedup, fitting on a single\nNVIDIA H100 GPU while retaining 98.4% of the original model's benchmark\naccuracies. Notably, it is the most accurate model supporting single H100 GPU\ninference with large batch sizes, despite training on only 45B tokens, far\nfewer than the 15T used to train Llama-70B. Lastly, we derive\nLlama-3.3-Nemotron-49B-Super-Base to demonstrate Puzzle can retain long-context\nand that lightweight alignment on these derived models allows them to surpass\nthe parent model in specific capabilities. Our work establishes that powerful\nLLM models can be optimized for efficient deployment with only negligible loss\nin quality, underscoring that inference performance, not parameter count alone,\nshould guide model selection.\n","versions":"[{'version': 'v1', 'created': 'Thu, 28 Nov 2024 13:45:42 GMT'}, {'version': 'v2', 'created': 'Tue, 3 Dec 2024 09:06:33 GMT'}, {'version': 'v3', 'created': 'Sun, 8 Dec 2024 15:55:59 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 14:50:04 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Bercovich', 'Akhiad', ''], ['Ronen', 'Tomer', ''], ['Abramovich', 'Talor', ''], ['Ailon', 'Nir', ''], ['Assaf', 'Nave', ''], ['Dabbah', 'Mohammad', ''], ['Galil', 'Ido', ''], ['Geifman', 'Amnon', ''], ['Geifman', 'Yonatan', ''], ['Golan', 'Izhak', ''], ['Haber', 'Netanel', ''], ['Karpas', 'Ehud', ''], ['Koren', 'Roi', ''], ['Levy', 'Itay', ''], ['Molchanov', 'Pavlo', ''], ['Mor', 'Shahar', ''], ['Moshe', 'Zach', ''], ['Nabwani', 'Najeeb', ''], ['Puny', 'Omri', ''], ['Rubin', 'Ran', ''], ['Schen', 'Itamar', ''], ['Shahaf', 'Ido', ''], ['Tropp', 'Oren', ''], ['Argov', 'Omer Ullman', ''], ['Zilberstein', 'Ran', ''], ['El-Yaniv', 'Ran', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Puzzle', 'label': 'Open-source LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Puzzle', 'label': 'Open-source LLMs'}, {'text': 'blockwise local knowledge distillation', 'label': 'Knowledge distillation'}, {'text': 'Puzzle', 'label': 'Open-source LLMs'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2411.19488,"submitter":"Jun Gao","authors":"Jun Gao, Yongqi Li, Ziqiang Cao, Wenjie Li","title":"Interleaved-Modal Chain-of-Thought","comments":"CVPR 2025 Main Conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Chain-of-Thought (CoT) prompting elicits large language models (LLMs) to\nproduce a series of intermediate reasoning steps before arriving at the final\nanswer. However, when transitioning to vision-language models (VLMs), their\ntext-only rationales struggle to express the fine-grained associations with the\noriginal image. In this paper, we propose an image-incorporated multimodal\nChain-of-Thought, named \\textbf{Interleaved-modal Chain-of-Thought (ICoT)},\nwhich generates sequential reasoning steps consisting of paired visual and\ntextual rationales to infer the final answer. Intuitively, the novel ICoT\nrequires VLMs to enable the generation of fine-grained interleaved-modal\ncontent, which is hard for current VLMs to fulfill. Considering that the\nrequired visual information is usually part of the input image, we propose\n\\textbf{Attention-driven Selection (ADS)} to realize ICoT over existing VLMs.\nADS intelligently inserts regions of the input image to generate the\ninterleaved-modal reasoning steps with ignorable additional latency. ADS relies\nsolely on the attention map of VLMs without the need for parameterization, and\ntherefore it is a plug-and-play strategy that can be generalized to a spectrum\nof VLMs. We apply ADS to realize ICoT on two popular VLMs of different\narchitectures. Extensive evaluations of three benchmarks have shown that ICoT\nprompting achieves substantial performance (up to 14\\%) and interpretability\nimprovements compared to existing multimodal CoT prompting methods.\n","versions":"[{'version': 'v1', 'created': 'Fri, 29 Nov 2024 06:06:35 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 09:01:38 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Gao', 'Jun', ''], ['Li', 'Yongqi', ''], ['Cao', 'Ziqiang', ''], ['Li', 'Wenjie', '']]","extracted_entities":"[{'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'vision-language models', 'label': 'Large Language Model'}, {'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'ICoT', 'label': 'Chain of thought'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'ICoT', 'label': 'Chain of thought'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'attention map', 'label': 'Attention mechanism'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'ICoT', 'label': 'Chain of thought'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'ICoT\\nprompting', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2412.0125,"submitter":"Francesco Taioli","authors":"Francesco Taioli, Edoardo Zorzi, Gianni Franchi, Alberto Castellini,\n  Alessandro Farinelli, Marco Cristani, Yiming Wang","title":"Collaborative Instance Object Navigation: Leveraging\n  Uncertainty-Awareness to Minimize Human-Agent Dialogues","comments":"https:\/\/intelligolabs.github.io\/CoIN\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Language-driven instance object navigation assumes that human users initiate\nthe task by providing a detailed description of the target instance to the\nembodied agent. While this description is crucial for distinguishing the target\nfrom visually similar instances in a scene, providing it prior to navigation\ncan be demanding for human. To bridge this gap, we introduce Collaborative\nInstance object Navigation (CoIN), a new task setting where the agent actively\nresolve uncertainties about the target instance during navigation in natural,\ntemplate-free, open-ended dialogues with human. We propose a novel\ntraining-free method, Agent-user Interaction with UncerTainty Awareness\n(AIUTA), which operates independently from the navigation policy, and focuses\non the human-agent interaction reasoning with Vision-Language Models (VLMs) and\nLarge Language Models (LLMs). First, upon object detection, a Self-Questioner\nmodel initiates a self-dialogue within the agent to obtain a complete and\naccurate observation description with a novel uncertainty estimation technique.\nThen, an Interaction Trigger module determines whether to ask a question to the\nhuman, continue or halt navigation, minimizing user input. For evaluation, we\nintroduce CoIN-Bench, with a curated dataset designed for challenging\nmulti-instance scenarios. CoIN-Bench supports both online evaluation with\nhumans and reproducible experiments with simulated user-agent interactions. On\nCoIN-Bench, we show that AIUTA serves as a competitive baseline, while existing\nlanguage-driven instance navigation methods struggle in complex multi-instance\nscenes. Code and benchmark will be available upon acceptance at\nhttps:\/\/intelligolabs.github.io\/CoIN\/\n","versions":"[{'version': 'v1', 'created': 'Mon, 2 Dec 2024 08:16:38 GMT'}, {'version': 'v2', 'created': 'Sun, 16 Mar 2025 17:46:20 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 16:09:20 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Taioli', 'Francesco', ''], ['Zorzi', 'Edoardo', ''], ['Franchi', 'Gianni', ''], ['Castellini', 'Alberto', ''], ['Farinelli', 'Alessandro', ''], ['Cristani', 'Marco', ''], ['Wang', 'Yiming', '']]","extracted_entities":"[{'text': 'Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'Interaction Trigger module', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2412.01262,"submitter":"Michelle Elizabeth","authors":"Michelle Elizabeth, Morgan Veyret, Miguel Couceiro, Ondrej Dusek and\n  Lina M. Rojas-Barahona","title":"Exploring ReAct Prompting for Task-Oriented Dialogue: Insights and\n  Shortcomings","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs) gained immense popularity due to their\nimpressive capabilities in unstructured conversations. Empowering LLMs with\nadvanced prompting strategies such as reasoning and acting (ReAct) (Yao et al.,\n2022) has shown promise in solving complex tasks traditionally requiring\nreinforcement learning. In this work, we apply the ReAct strategy to guide LLMs\nperforming task-oriented dialogue (TOD). We evaluate ReAct-based LLMs\n(ReAct-LLMs) both in simulation and with real users. While ReAct-LLMs severely\nunderperform state-of-the-art approaches on success rate in simulation, this\ndifference becomes less pronounced in human evaluation. Moreover, compared to\nthe baseline, humans report higher subjective satisfaction with ReAct-LLM\ndespite its lower success rate, most likely thanks to its natural and\nconfidently phrased responses.\n","versions":"[{'version': 'v1', 'created': 'Mon, 2 Dec 2024 08:30:22 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 10:01:21 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Elizabeth', 'Michelle', ''], ['Veyret', 'Morgan', ''], ['Couceiro', 'Miguel', ''], ['Dusek', 'Ondrej', ''], ['Rojas-Barahona', 'Lina M.', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'reasoning and acting', 'label': 'Prompting'}, {'text': 'ReAct', 'label': 'Prompting'}, {'text': 'reinforcement learning', 'label': 'Few-shot Learning'}, {'text': 'ReAct', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ReAct-LLMs', 'label': 'LLM-based'}, {'text': 'ReAct-LLM', 'label': 'LLM-based'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2412.08897,"submitter":"Sam Adam-Day","authors":"Lewis Hammond and Sam Adam-Day","title":"Neural Interactive Proofs","comments":"ICLR'25 camera-ready version; 51 pages, 17 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We consider the problem of how a trusted, but computationally bounded agent\n(a 'verifier') can learn to interact with one or more powerful but untrusted\nagents ('provers') in order to solve a given task. More specifically, we study\nthe case in which agents are represented using neural networks and refer to\nsolutions of this problem as neural interactive proofs. First we introduce a\nunifying framework based on prover-verifier games, which generalises previously\nproposed interaction protocols. We then describe several new protocols for\ngenerating neural interactive proofs, and provide a theoretical comparison of\nboth new and existing approaches. Finally, we support this theory with\nexperiments in two domains: a toy graph isomorphism problem that illustrates\nthe key ideas, and a code validation task using large language models. In so\ndoing, we aim to create a foundation for future work on neural interactive\nproofs and their application in building safer AI systems.\n","versions":"[{'version': 'v1', 'created': 'Thu, 12 Dec 2024 03:21:53 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 17:16:02 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Hammond', 'Lewis', ''], ['Adam-Day', 'Sam', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2412.09668,"submitter":"Messi H.J. Lee","authors":"Messi H.J. Lee, Soyeon Jeon","title":"Vision-Language Models Generate More Homogeneous Stories for\n  Phenotypically Black Individuals","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Vision-Language Models (VLMs) extend Large Language Models' capabilities by\nintegrating image processing, but concerns persist about their potential to\nreproduce and amplify human biases. While research has documented how these\nmodels perpetuate stereotypes across demographic groups, most work has focused\non between-group biases rather than within-group differences. This study\ninvestigates homogeneity bias-the tendency to portray groups as more uniform\nthan they are-within Black Americans, examining how perceived racial\nphenotypicality influences VLMs' outputs. Using computer-generated images that\nsystematically vary in phenotypicality, we prompted VLMs to generate stories\nabout these individuals and measured text similarity to assess content\nhomogeneity. Our findings reveal three key patterns: First, VLMs generate\nsignificantly more homogeneous stories about Black individuals with higher\nphenotypicality compared to those with lower phenotypicality. Second, stories\nabout Black women consistently display greater homogeneity than those about\nBlack men across all models tested. Third, in two of three VLMs, this\nhomogeneity bias is primarily driven by a pronounced interaction where\nphenotypicality strongly influences content variation for Black women but has\nminimal impact for Black men. These results demonstrate how intersectionality\nshapes AI-generated representations and highlight the persistence of\nstereotyping that mirror documented biases in human perception, where increased\nracial phenotypicality leads to greater stereotyping and less individualized\nrepresentation.\n","versions":"[{'version': 'v1', 'created': 'Thu, 12 Dec 2024 18:53:49 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 15:50:45 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Lee', 'Messi H. J.', ''], ['Jeon', 'Soyeon', '']]","extracted_entities":"[{'text': 'Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'homogeneity bias-the', 'label': 'Model Bias and Fairness'}, {'text': 'prompted', 'label': 'Prompting'}, {'text': 'homogeneity bias', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2412.12687,"submitter":"Seungeun Oh","authors":"Seungeun Oh, Jinhyuk Kim, Jihong Park, Seung-Woo Ko, Tony Q. S. Quek,\n  Seong-Lyun Kim","title":"Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large\n  Language Models","comments":"7 pages, 6 figures; to be presented at IEEE International Conference\n  on Machine Learning for Communication and Networking (ICMLCN) 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.DC cs.IT cs.NI eess.SP math.IT","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper studies a hybrid language model (HLM) architecture that integrates\na small language model (SLM) operating on a mobile device with a large language\nmodel (LLM) hosted at the base station (BS) of a wireless network. The HLM\ntoken generation process follows the speculative inference principle: the SLM's\nvocabulary distribution is uploaded to the LLM, which either accepts or rejects\nit, with rejected tokens being resampled by the LLM. While this approach\nensures alignment between the vocabulary distributions of the SLM and LLM, it\nsuffers from low token throughput due to uplink transmission and the\ncomputation costs of running both language models. To address this, we propose\na novel HLM structure coined Uncertainty-aware opportunistic HLM (U-HLM),\nwherein the SLM locally measures its output uncertainty and skips both uplink\ntransmissions and LLM operations for tokens that are likely to be accepted.\nThis opportunistic skipping is enabled by our empirical finding of a linear\ncorrelation between the SLM's uncertainty and the LLM's rejection probability.\nWe analytically derive the uncertainty threshold and evaluate its expected risk\nof rejection. Simulations show that U-HLM reduces uplink transmissions and LLM\ncomputations by 45.93%, while achieving up to 97.54% of the LLM's inference\naccuracy and 2.54$\\times$ faster token throughput than HLM without skipping.\n","versions":"[{'version': 'v1', 'created': 'Tue, 17 Dec 2024 09:08:18 GMT'}, {'version': 'v2', 'created': 'Wed, 18 Dec 2024 08:14:35 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 10:50:58 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Oh', 'Seungeun', ''], ['Kim', 'Jinhyuk', ''], ['Park', 'Jihong', ''], ['Ko', 'Seung-Woo', ''], ['Quek', 'Tony Q. S.', ''], ['Kim', 'Seong-Lyun', '']]","extracted_entities":"[{'text': 'SLM', 'label': 'Large Language Model'}, {'text': 'large language\\nmodel', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'SLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'SLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'SLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'SLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language\nmodel","similarity_score":1.0}
{"id":2412.13871,"submitter":"Zonghao Guo","authors":"Yipeng Zhang, Yifan Liu, Zonghao Guo, Yidan Zhang, Xuesong Yang,\n  Xiaoying Zhang, Chi Chen, Jun Song, Bo Zheng, Yuan Yao, Zhiyuan Liu, Tat-Seng\n  Chua, Maosong Sun","title":"LLaVA-UHD v2: an MLLM Integrating High-Resolution Semantic Pyramid via\n  Hierarchical Window Transformer","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Vision transformers (ViTs) are widely employed in multimodal large language\nmodels (MLLMs) for visual encoding. However, they exhibit inferior performance\non tasks regarding fine-grained visual perception. We attribute this to the\nlimitations of ViTs in capturing diverse multi-modal visual levels, such as\nlow-level details. To address this issue, we present LLaVA-UHD v2, an MLLM with\nadvanced perception abilities by introducing a well-designed vision-language\nprojector, the Hierarchical window (Hiwin) transformer. Hiwin transformer\nenhances MLLM's ability to capture diverse multi-modal visual granularities, by\nincorporating our constructed high-resolution semantic pyramid. Specifically,\nHiwin transformer comprises two key modules: (i) a visual detail injection\nmodule, which progressively injects low-level visual details into high-level\nlanguage-aligned semantics features, thereby forming an inverse semantic\npyramid (ISP), and (ii) a hierarchical window attention module, which leverages\ncross-scale windows to condense multi-level semantics from the ISP. Extensive\nexperiments show that LLaVA-UHD v2 outperforms compared MLLMs on a wide range\nof benchmarks. Notably, our design achieves an average boost of 3.7% across 14\nbenchmarks compared with the baseline method, 9.3% on DocVQA for instance. All\nthe data and code will be publicly available to facilitate future research.\n","versions":"[{'version': 'v1', 'created': 'Wed, 18 Dec 2024 14:07:46 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 10:04:22 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhang', 'Yipeng', ''], ['Liu', 'Yifan', ''], ['Guo', 'Zonghao', ''], ['Zhang', 'Yidan', ''], ['Yang', 'Xuesong', ''], ['Zhang', 'Xiaoying', ''], ['Chen', 'Chi', ''], ['Song', 'Jun', ''], ['Zheng', 'Bo', ''], ['Yao', 'Yuan', ''], ['Liu', 'Zhiyuan', ''], ['Chua', 'Tat-Seng', ''], ['Sun', 'Maosong', '']]","extracted_entities":"[{'text': 'Vision transformers', 'label': 'Transformers'}, {'text': 'multimodal large language\\nmodels', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'Hiwin transformer', 'label': 'Transformers'}, {'text': 'Hiwin transformer', 'label': 'Transformers'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"multimodal large language\nmodels","similarity_score":0.7649828196}
{"id":2412.14672,"submitter":"Estelle Aflalo Guez","authors":"Estelle Aflalo, Gabriela Ben Melech Stan, Tiep Le, Man Luo, Shachar\n  Rosenman, Sayak Paul, Shao-Yen Tseng, Vasudev Lal","title":"FiVL: A Framework for Improved Vision-Language Alignment through the\n  Lens of Training, Evaluation and Explainability","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Vision Language Models (LVLMs) have achieved significant progress in\nintegrating visual and textual inputs for multimodal reasoning. However, a\nrecurring challenge is ensuring these models utilize visual information as\neffectively as linguistic content when both modalities are necessary to\nformulate an accurate answer. We hypothesize that hallucinations arise due to\nthe lack of effective visual grounding in current LVLMs. Furthermore, current\nvision-language benchmarks are not specifically measuring the degree to which\nthe answer require the visual input. This limitation makes it challenging to\nconfirm that the image is truly necessary, particularly in tasks like visual\nquestion answering. In this work, we introduce FiVL, a novel method for\nconstructing datasets designed to train LVLMs for enhanced visual grounding and\nalso evaluate their effectiveness in achieving it. We demonstrate the value of\nour datasets through three approaches. First, we introduce a novel training\ntask based on our augmented training dataset, resulting in better performance\nthan the baseline. Second, we present benchmarks to assess the model's ability\nto use image as substantive evidence, rather than relying solely on linguistic\npriors. Finally, we identify attention heads with the strongest vision-language\nalignment, enabling explainability on visual-driven hallucinations. The code is\navailable at https:\/\/github.com\/IntelLabs\/fivl.\n","versions":"[{'version': 'v1', 'created': 'Thu, 19 Dec 2024 09:24:10 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 12:04:30 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Aflalo', 'Estelle', ''], ['Stan', 'Gabriela Ben Melech', ''], ['Le', 'Tiep', ''], ['Luo', 'Man', ''], ['Rosenman', 'Shachar', ''], ['Paul', 'Sayak', ''], ['Tseng', 'Shao-Yen', ''], ['Lal', 'Vasudev', '']]","extracted_entities":"[{'text': 'Large Vision Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'attention heads', 'label': 'Attention mechanism'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Vision Language Models","similarity_score":0.7264450788}
{"id":2412.20227,"submitter":"Shuguang Chen","authors":"Shuguang Chen and Guang Lin","title":"LLM Reasoning Engine: Specialized Training for Enhanced Mathematical\n  Reasoning","comments":"Accepted to NAACL 2025 KnowledgeNLP","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) have shown remarkable performance in various\nnatural language processing tasks but face challenges in mathematical\nreasoning, where complex problem-solving requires both linguistic understanding\nand mathematical reasoning skills. Existing approaches to address this\nchallenge often rely on ensemble methods and suffer from the problem of data\nscarcity in target domains. In this work, we present a novel method to enhance\nLLMs' capabilities in mathematical reasoning tasks. Motivated by the need to\nbridge this gap, our approach incorporates a question paraphrase strategy,\nwhich aims at diversifying the linguistic forms of mathematical questions to\nimprove generalization. Additionally, specialized training objectives are\nemployed to guide the model's learning process, focusing on enhancing its\nunderstanding of mathematical concepts and reasoning processes. We conduct\nexperiments on four datasets using different LLMs, and demonstrate the\neffectiveness of our approach in improving LLMs' performance on mathematical\nreasoning tasks. Our findings underscore the significance of our methodology in\nthe advancement of large language models and its potential implications for\nreal-world applications that require mathematical reasoning abilities.\n","versions":"[{'version': 'v1', 'created': 'Sat, 28 Dec 2024 17:48:33 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:56:49 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Chen', 'Shuguang', ''], ['Lin', 'Guang', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2412.2076,"submitter":"Huihan Li","authors":"Huihan Li, Arnav Goel, Keyu He, Xiang Ren","title":"Attributing Culture-Conditioned Generations to Pretraining Corpora","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In open-ended generative tasks like narrative writing or dialogue, large\nlanguage models often exhibit cultural biases, showing limited knowledge and\ngenerating templated outputs for less prevalent cultures. Recent works show\nthat these biases may stem from uneven cultural representation in pretraining\ncorpora. This work investigates how pretraining leads to biased\nculture-conditioned generations by analyzing how models associate entities with\ncultures based on pretraining data patterns. We propose the MEMOed framework\n(MEMOrization from pretraining document) to determine whether a generation for\na culture arises from memorization. Using MEMOed on culture-conditioned\ngenerations about food and clothing for 110 cultures, we find that\nhigh-frequency cultures in pretraining data yield more generations with\nmemorized symbols, while some low-frequency cultures produce none.\nAdditionally, the model favors generating entities with extraordinarily high\nfrequency regardless of the conditioned culture, reflecting biases toward\nfrequent pretraining terms irrespective of relevance. We hope that the MEMOed\nframework and our insights will inspire more works on attributing model\nperformance on pretraining data.\n","versions":"[{'version': 'v1', 'created': 'Mon, 30 Dec 2024 07:09:25 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 19:08:17 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Li', 'Huihan', ''], ['Goel', 'Arnav', ''], ['He', 'Keyu', ''], ['Ren', 'Xiang', '']]","extracted_entities":"[{'text': 'large\\nlanguage models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large\nlanguage models","similarity_score":0.9664971828}
{"id":2501.00959,"submitter":"Saleh Afroogh","authors":"Junfeng Jiao, Saleh Afroogh, Kevin Chen, David Atkinson, Amit\n  Dhurandhar","title":"IGGA: A Dataset of Industrial Guidelines and Policy Statements for\n  Generative AIs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper introduces IGGA, a dataset of 160 industry guidelines and policy\nstatements for the use of Generative AIs (GAIs) and Large Language Models\n(LLMs) in industry and workplace settings, collected from official company\nwebsites, and trustworthy news sources. The dataset contains 104,565 words and\nserves as a valuable resource for natural language processing tasks commonly\napplied in requirements engineering, such as model synthesis, abstraction\nidentification, and document structure assessment. Additionally, IGGA can be\nfurther annotated to function as a benchmark for various tasks, including\nambiguity detection, requirements categorization, and the identification of\nequivalent requirements. Our methodologically rigorous approach ensured a\nthorough examination, with a selection of reputable and influential companies\nthat represent a diverse range of global institutions across six continents.\nThe dataset captures perspectives from fourteen industry sectors, including\ntechnology, finance, and both public and private institutions, offering a broad\nspectrum of insights into the integration of GAIs and LLMs in industry.\n","versions":"[{'version': 'v1', 'created': 'Wed, 1 Jan 2025 21:31:47 GMT'}, {'version': 'v2', 'created': 'Fri, 3 Jan 2025 19:17:56 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 16:44:15 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Jiao', 'Junfeng', ''], ['Afroogh', 'Saleh', ''], ['Chen', 'Kevin', ''], ['Atkinson', 'David', ''], ['Dhurandhar', 'Amit', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2501.02063,"submitter":"Saleh Afroogh","authors":"Junfeng Jiao, Saleh Afroogh, Kevin Chen, David Atkinson, Amit\n  Dhurandhar","title":"AGGA: A Dataset of Academic Guidelines for Generative AI and Large\n  Language Models","comments":"arXiv admin note: text overlap with arXiv:2406.18842,\n  arXiv:2501.00959","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This study introduces AGGA, a dataset comprising 80 academic guidelines for\nthe use of Generative AIs (GAIs) and Large Language Models (LLMs) in academic\nsettings, meticulously collected from official university websites. The dataset\ncontains 188,674 words and serves as a valuable resource for natural language\nprocessing tasks commonly applied in requirements engineering, such as model\nsynthesis, abstraction identification, and document structure assessment.\nAdditionally, AGGA can be further annotated to function as a benchmark for\nvarious tasks, including ambiguity detection, requirements categorization, and\nthe identification of equivalent requirements. Our methodologically rigorous\napproach ensured a thorough examination, with a selection of universities that\nrepresent a diverse range of global institutions, including top-ranked\nuniversities across six continents. The dataset captures perspectives from a\nvariety of academic fields, including humanities, technology, and both public\nand private institutions, offering a broad spectrum of insights into the\nintegration of GAIs and LLMs in academia.\n","versions":"[{'version': 'v1', 'created': 'Fri, 3 Jan 2025 19:16:36 GMT'}, {'version': 'v2', 'created': 'Tue, 7 Jan 2025 19:12:22 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 16:45:54 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Jiao', 'Junfeng', ''], ['Afroogh', 'Saleh', ''], ['Chen', 'Kevin', ''], ['Atkinson', 'David', ''], ['Dhurandhar', 'Amit', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2501.11425,"submitter":"Siyu Yuan","authors":"Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, Jiecao Chen","title":"Agent-R: Training Language Model Agents to Reflect via Iterative\n  Self-Training","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Large Language Models (LLMs) agents are increasingly pivotal for addressing\ncomplex tasks in interactive environments. Existing work mainly focuses on\nenhancing performance through behavior cloning from stronger experts, yet such\napproaches often falter in real-world applications, mainly due to the inability\nto recover from errors. However, step-level critique data is difficult and\nexpensive to collect. Automating and dynamically constructing self-critique\ndatasets is thus crucial to empowering models with intelligent agent\ncapabilities. In this work, we propose an iterative self-training framework,\nAgent-R, that enables language Agent to Reflect on the fly. Unlike traditional\nmethods that reward or penalize actions based on correctness, Agent-R leverages\nMCTS to construct training data that recover correct trajectories from\nerroneous ones. A key challenge of agent reflection lies in the necessity for\ntimely revision rather than waiting until the end of a rollout. To address\nthis, we introduce a model-guided critique construction mechanism: the actor\nmodel identifies the first error step (within its current capability) in a\nfailed trajectory. Starting from it, we splice it with the adjacent correct\npath, which shares the same parent node in the tree. This strategy enables the\nmodel to learn reflection based on its current policy, therefore yielding\nbetter learning efficiency. To further explore the scalability of this\nself-improvement paradigm, we investigate iterative refinement of both error\ncorrection capabilities and dataset construction. Our findings demonstrate that\nAgent-R continuously improves the model's ability to recover from errors and\nenables timely error correction. Experiments on three interactive environments\nshow that Agent-R effectively equips agents to correct erroneous actions while\navoiding loops, achieving superior performance compared to baseline methods\n(+5.59%).\n","versions":"[{'version': 'v1', 'created': 'Mon, 20 Jan 2025 11:46:04 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 09:28:09 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Yuan', 'Siyu', ''], ['Chen', 'Zehui', ''], ['Xi', 'Zhiheng', ''], ['Ye', 'Junjie', ''], ['Du', 'Zhengyin', ''], ['Chen', 'Jiecao', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2501.12372,"submitter":"Yeounoh Chung","authors":"Yeounoh Chung, Gaurav T. Kakkar, Yu Gan, Brenton Milne, Fatma Ozcan","title":"Is Long Context All You Need? Leveraging LLM's Extended Context for\n  NL2SQL","comments":"13 pages, 6 figures, VLDB 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DB cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of natural language processing tasks. In particular, improvements in\nreasoning abilities and the expansion of context windows have opened new\navenues for leveraging these powerful models. NL2SQL is challenging in that the\nnatural language question is inherently ambiguous, while the SQL generation\nrequires a precise understanding of complex data schema and semantics. One\napproach to this semantic ambiguous problem is to provide more and sufficient\ncontextual information.\n  In this work, we explore the performance and the latency trade-offs of the\nextended context window (a.k.a., long context) offered by Google's\nstate-of-the-art LLM (\\textit{gemini-1.5-pro}). We study the impact of various\ncontextual information, including column example values, question and SQL query\npairs, user-provided hints, SQL documentation, and schema. To the best of our\nknowledge, this is the first work to study how the extended context window and\nextra contextual information can help NL2SQL generation with respect to both\naccuracy and latency cost. We show that long context LLMs are robust and do not\nget lost in the extended contextual information. Additionally, our long-context\nNL2SQL pipeline based on Google's \\textit{gemini-pro-1.5} achieve strong\nperformances on various benchmark datasets without finetuning and expensive\nself-consistency based techniques.\n","versions":"[{'version': 'v1', 'created': 'Tue, 21 Jan 2025 18:52:15 GMT'}, {'version': 'v2', 'created': 'Sat, 1 Feb 2025 02:00:46 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Feb 2025 23:39:12 GMT'}, {'version': 'v4', 'created': 'Fri, 7 Mar 2025 23:17:42 GMT'}, {'version': 'v5', 'created': 'Thu, 20 Mar 2025 17:39:13 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chung', 'Yeounoh', ''], ['Kakkar', 'Gaurav T.', ''], ['Gan', 'Yu', ''], ['Milne', 'Brenton', ''], ['Ozcan', 'Fatma', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'extended context window', 'label': 'contextual Embedding'}, {'text': 'long context', 'label': 'contextual Embedding'}, {'text': 'extended context window', 'label': 'contextual Embedding'}, {'text': 'finetuning', 'label': 'Fine-tuning'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2501.13947,"submitter":"Wenli Yang","authors":"Wenli Yang, Lilian Some, Michael Bain, Byeong Kang","title":"A Comprehensive Survey on Integrating Large Language Models with\n  Knowledge-Based Methods","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The rapid development of artificial intelligence has led to marked progress\nin the field. One interesting direction for research is whether Large Language\nModels (LLMs) can be integrated with structured knowledge-based systems. This\napproach aims to combine the generative language understanding of LLMs and the\nprecise knowledge representation systems by which they are integrated. This\narticle surveys the relationship between LLMs and knowledge bases, looks at how\nthey can be applied in practice, and discusses related technical, operational,\nand ethical challenges. Utilizing a comprehensive examination of the\nliterature, the study both identifies important issues and assesses existing\nsolutions. It demonstrates the merits of incorporating generative AI into\nstructured knowledge-base systems concerning data contextualization, model\naccuracy, and utilization of knowledge resources. The findings give a full list\nof the current situation of research, point out the main gaps, and propose\nhelpful paths to take. These insights contribute to advancing AI technologies\nand support their practical deployment across various sectors.\n","versions":"[{'version': 'v1', 'created': 'Sun, 19 Jan 2025 23:25:21 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 23:27:43 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Yang', 'Wenli', ''], ['Some', 'Lilian', ''], ['Bain', 'Michael', ''], ['Kang', 'Byeong', '']]","extracted_entities":"[{'text': 'Large Language\\nModels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'data contextualization', 'label': 'contextual Embedding'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language\nModels","similarity_score":0.9664971828}
{"id":2501.14892,"submitter":"Hang Luo","authors":"Hang Luo, Jian Zhang, Chujun Li","title":"Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in\n  Graph-Augmented LLMs","comments":"18 pages, 3 figures, 3 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In knowledge-intensive tasks, especially in high-stakes domains like medicine\nand law, it is critical not only to retrieve relevant information but also to\nprovide causal reasoning and explainability. Large language models (LLMs) have\nachieved remarkable performance in natural language understanding and\ngeneration tasks. However, they often suffer from limitations such as\ndifficulty in incorporating new knowledge, generating hallucinations, and\nexplaining their reasoning process. To address these challenges, integrating\nknowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has\nemerged as an effective solution. Traditional Graph RAG methods often rely on\nsimple graph traversal or semantic similarity, which do not capture causal\nrelationships or align well with the model's internal reasoning steps. This\npaper proposes a novel pipeline that filters large knowledge graphs to\nemphasize cause-effect edges, aligns the retrieval process with the model's\nchain-of-thought (CoT), and enhances reasoning through multi-stage path\nimprovements. Experiments on medical question-answering tasks show consistent\ngains, with up to a 10\\% absolute improvement across multiple large language\nmodels (LLMs). This approach demonstrates the value of combining causal\nreasoning with stepwise retrieval, leading to more interpretable and logically\ngrounded solutions for complex queries.\n","versions":"[{'version': 'v1', 'created': 'Fri, 24 Jan 2025 19:31:06 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 14:32:08 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Luo', 'Hang', ''], ['Zhang', 'Jian', ''], ['Li', 'Chujun', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Graph RAG', 'label': 'RAG'}, {'text': 'Graph RAG', 'label': 'RAG'}, {'text': 'chain-of-thought (CoT)', 'label': 'Chain of thought'}, {'text': 'large language\\nmodels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2501.1589,"submitter":"Karahan Sar{\\i}ta\\c{s}","authors":"Karahan Sar{\\i}ta\\c{s}, Peter Dayan, Tingke Shen, Surabhi S Nath","title":"Complexity in Complexity: Understanding Visual Complexity Through\n  Structure, Color, and Surprise","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Understanding how humans perceive visual complexity is a key area of study in\nvisual cognition. Previous approaches to modeling visual complexity assessments\nhave often resulted in intricate, difficult-to-interpret algorithms that employ\nnumerous features or sophisticated deep learning architectures. While these\ncomplex models achieve high performance on specific datasets, they often\nsacrifice interpretability, making it challenging to understand the factors\ndriving human perception of complexity. Recently (Shen, et al. 2024) proposed\nan interpretable segmentation-based model that accurately predicted complexity\nacross various datasets, supporting the idea that complexity can be explained\nsimply. In this work, we investigate the failure of their model to capture\nstructural, color and surprisal contributions to complexity. To this end, we\npropose Multi-Scale Sobel Gradient (MSG) which measures spatial intensity\nvariations, Multi-Scale Unique Color (MUC) which quantifies colorfulness across\nmultiple scales, and surprise scores generated using a Large Language Model. We\ntest our features on existing benchmarks and a novel dataset (Surprising Visual\nGenome) containing surprising images from Visual Genome. Our experiments\ndemonstrate that modeling complexity accurately is not as simple as previously\nthought, requiring additional perceptual and semantic factors to address\ndataset biases. Our model improves predictive performance while maintaining\ninterpretability, offering deeper insights into how visual complexity is\nperceived and assessed. Our code, analysis and data are available at\nhttps:\/\/github.com\/Complexity-Project\/Complexity-in-Complexity.\n","versions":"[{'version': 'v1', 'created': 'Mon, 27 Jan 2025 09:32:56 GMT'}, {'version': 'v2', 'created': 'Wed, 5 Feb 2025 19:36:23 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 12:06:51 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Sar\u0131ta\u015f', 'Karahan', ''], ['Dayan', 'Peter', ''], ['Shen', 'Tingke', ''], ['Nath', 'Surabhi S', '']]","extracted_entities":"[{'text': 'Large Language Model', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Model","similarity_score":1.0}
{"id":2502.06759,"submitter":"Gaetano Rossiello","authors":"Gaetano Rossiello, Nhan Pham, Michael Glass, Junkyu Lee, Dharmashankar\n  Subramanian","title":"Rationalization Models for Text-to-SQL","comments":"Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.DB","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Feb 2025 18:38:57 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Feb 2025 17:12:34 GMT'}, {'version': 'v3', 'created': 'Tue, 11 Mar 2025 17:37:30 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 13:46:48 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Rossiello', 'Gaetano', ''], ['Pham', 'Nhan', ''], ['Glass', 'Michael', ''], ['Lee', 'Junkyu', ''], ['Subramanian', 'Dharmashankar', '']]","extracted_entities":"[{'text': 'prompt', 'label': 'Prompting'}, {'text': 'large language model', 'label': 'Large Language Model'}, {'text': 'few-shot knowledge distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Large Language Model","matched_keyword":"large language model","similarity_score":1.0}
{"id":2502.07058,"submitter":"Zixin Tang","authors":"Zixin Tang, Chieh-Yang Huang, Tsung-Che Li, Ho Yin Sam Ng, Hen-Hsen\n  Huang, Ting-Hao 'Kenneth' Huang","title":"Using Contextually Aligned Online Reviews to Measure LLMs' Performance\n  Disparities Across Language Varieties","comments":"Accepted by 2025 Annual Conference of the Nations of the Americas\n  Chapter of the Association for Computational Linguistics (NAACL), theme track","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  A language can have different varieties. These varieties can affect the\nperformance of natural language processing (NLP) models, including large\nlanguage models (LLMs), which are often trained on data from widely spoken\nvarieties. This paper introduces a novel and cost-effective approach to\nbenchmark model performance across language varieties. We argue that\ninternational online review platforms, such as Booking.com, can serve as\neffective data sources for constructing datasets that capture comments in\ndifferent language varieties from similar real-world scenarios, like reviews\nfor the same hotel with the same rating using the same language (e.g., Mandarin\nChinese) but different language varieties (e.g., Taiwan Mandarin, Mainland\nMandarin). To prove this concept, we constructed a contextually aligned dataset\ncomprising reviews in Taiwan Mandarin and Mainland Mandarin and tested six LLMs\nin a sentiment analysis task. Our results show that LLMs consistently\nunderperform in Taiwan Mandarin.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Feb 2025 21:49:35 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Feb 2025 04:55:27 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 15:01:11 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Tang', 'Zixin', ''], ['Huang', 'Chieh-Yang', ''], ['Li', 'Tsung-Che', ''], ['Ng', 'Ho Yin Sam', ''], ['Huang', 'Hen-Hsen', ''], ['Huang', \"Ting-Hao 'Kenneth'\", '']]","extracted_entities":"[{'text': 'natural language processing (NLP) models', 'label': 'NLP model'}, {'text': 'large\\nlanguage models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large\nlanguage models","similarity_score":0.9664971828}
{"id":2502.0802,"submitter":"Ziyao Wang","authors":"Ziyao Wang, Muneeza Azmat, Ang Li, Raya Horesh, Mikhail Yurochkin","title":"Speculate, then Collaborate: Fusing Knowledge of Language Models during\n  Decoding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) often excel in specific domains but fall short\nin others due to the limitations of their training. Thus, enabling LLMs to\nsolve problems collaboratively by integrating their complementary knowledge\npromises to improve their performance across domains. To realize this\npotential, we introduce a novel Collaborative Speculative Decoding (CoSD)\nalgorithm that enables efficient LLM knowledge fusion at test time without\nrequiring additional model training. CoSD employs a draft model to generate\ninitial sequences and an easy-to-learn rule or decision tree to decide when to\ninvoke an assistant model to improve these drafts. CoSD not only enhances\nknowledge fusion but also improves inference efficiency, is transferable across\ndomains and models, and offers greater explainability. Experimental results\ndemonstrate that CoSD improves accuracy by up to 10\\% across benchmarks\ncompared to existing methods, providing a scalable and effective solution for\nLLM-based applications\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Feb 2025 23:40:53 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 16:26:10 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Wang', 'Ziyao', ''], ['Azmat', 'Muneeza', ''], ['Li', 'Ang', ''], ['Horesh', 'Raya', ''], ['Yurochkin', 'Mikhail', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2502.12509,"submitter":"Kangda Wei","authors":"Kangda Wei, Xi Shi, Jonathan Tong, Sai Ramana Reddy, Anandhavelu\n  Natarajan, Rajiv Jain, Aparna Garimella, Ruihong Huang","title":"LegalCore: A Dataset for Event Coreference Resolution in Legal Documents","comments":"Need company internal approval before public release","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Recognizing events and their coreferential mentions in a document is\nessential for understanding semantic meanings of text. The existing research on\nevent coreference resolution is mostly limited to news articles. In this paper,\nwe present the first dataset for the legal domain, LegalCore, which has been\nannotated with comprehensive event and event coreference information. The legal\ncontract documents we annotated in this dataset are several times longer than\nnews articles, with an average length of around 25k tokens per document. The\nannotations show that legal documents have dense event mentions and feature\nboth short-distance and super long-distance coreference links between event\nmentions. We further benchmark mainstream Large Language Models (LLMs) on this\ndataset for both event detection and event coreference resolution tasks, and\nfind that this dataset poses significant challenges for state-of-the-art\nopen-source and proprietary LLMs, which perform significantly worse than a\nsupervised baseline. We will publish the dataset as well as the code.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Feb 2025 03:47:53 GMT'}, {'version': 'v2', 'created': 'Mon, 3 Mar 2025 19:36:00 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Mar 2025 16:53:11 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 16:45:57 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wei', 'Kangda', ''], ['Shi', 'Xi', ''], ['Tong', 'Jonathan', ''], ['Reddy', 'Sai Ramana', ''], ['Natarajan', 'Anandhavelu', ''], ['Jain', 'Rajiv', ''], ['Garimella', 'Aparna', ''], ['Huang', 'Ruihong', '']]","extracted_entities":"[{'text': 'mainstream Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"mainstream Large Language Models","similarity_score":0.8480705023}
{"id":2502.13145,"submitter":"Bencheng Liao","authors":"Bencheng Liao and Hongyuan Tao and Qian Zhang and Tianheng Cheng and\n  Yingyue Li and Haoran Yin and Wenyu Liu and Xinggang Wang","title":"Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation","comments":"Code and model are available at https:\/\/github.com\/hustvl\/mmMamba","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps:\/\/github.com\/hustvl\/mmMamba\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Feb 2025 18:59:57 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 07:02:33 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Liao', 'Bencheng', ''], ['Tao', 'Hongyuan', ''], ['Zhang', 'Qian', ''], ['Cheng', 'Tianheng', ''], ['Li', 'Yingyue', ''], ['Yin', 'Haoran', ''], ['Liu', 'Wenyu', ''], ['Wang', 'Xinggang', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'progressive\\ndistillation', 'label': 'Knowledge distillation'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'Mamba', 'label': 'LLM'}, {'text': 'Mamba', 'label': 'LLM'}, {'text': 'HoVLE', 'label': 'LLM'}, {'text': 'HoVLE', 'label': 'LLM'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2502.16182,"submitter":"Shivank Garg","authors":"Shivank Garg, Ayush Singh, Shweta Singh, Paras Chopra","title":"IPO: Your Language Model is Secretly a Preference Classifier","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Reinforcement learning from human feedback (RLHF) has emerged as the primary\nmethod for aligning large language models (LLMs) with human preferences. While\nit enables LLMs to achieve human-level alignment, it often incurs significant\ncomputational and financial costs due to its reliance on training external\nreward models or human-labeled preferences. In this work, we propose Implicit\nPreference Optimization (IPO), an alternative approach that leverages\ngenerative LLMs as preference classifiers, thereby reducing the dependence on\nexternal human feedback or reward models to obtain preferences. We conduct a\ncomprehensive evaluation on the preference classification ability of LLMs using\nRewardBench, assessing models across different sizes, architectures, and\ntraining levels to validate our hypothesis. Furthermore, we investigate the\nself-improvement capabilities of LLMs by generating multiple responses for a\ngiven instruction and employing the model itself as a preference classifier for\nDirect Preference Optimization (DPO)-based training. Our findings demonstrate\nthat models trained through IPO achieve performance comparable to those\nutilizing state-of-the-art reward models for obtaining preferences.\n","versions":"[{'version': 'v1', 'created': 'Sat, 22 Feb 2025 10:59:11 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 10:52:45 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Garg', 'Shivank', ''], ['Singh', 'Ayush', ''], ['Singh', 'Shweta', ''], ['Chopra', 'Paras', '']]","extracted_entities":"[{'text': 'Reinforcement learning from human feedback', 'label': 'Zero-shot Learning'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2502.17848,"submitter":"Jianghao Chen","authors":"Jianghao Chen, Zhenlin Wei, Zhenjiang Ren, Ziyong Li, Jiajun Zhang","title":"LR$^2$Bench: Evaluating Long-chain Reflective Reasoning Capabilities of\n  Large Language Models via Constraint Satisfaction Problems","comments":"Submitted to ACL ARR 2025 February","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent progress in o1-like models has significantly enhanced the reasoning\nabilities of Large Language Models (LLMs), empowering them to tackle\nincreasingly complex tasks through reflection capabilities, such as making\nassumptions, backtracking, and self-refinement. However, effectively evaluating\nsuch reflection capabilities remains challenging due to the lack of appropriate\nbenchmarks. To bridge this gap, we introduce LR$^2$Bench, a novel benchmark\ndesigned to evaluate the Long-chain Reflective Reasoning capabilities of LLMs.\nLR$^2$Bench comprises 850 samples across six Constraint Satisfaction Problems\n(CSPs) where reflective reasoning is crucial for deriving solutions that meet\nall given constraints. Each type of task focuses on distinct constraint\npatterns, such as knowledge-based, logical, and spatial constraints, providing\na comprehensive evaluation of diverse problem-solving scenarios. We conduct\nextensive evaluation on both conventional models and o1-like models. Our\nexperimental results reveal that even the most advanced reasoning-specific\nmodels, such as DeepSeek-R1 and OpenAI o1-preview, struggle with tasks in\nLR$^2$Bench, achieving an average Exact Match score of only 20.0% and 23.6%,\nrespectively. These findings underscore the significant room for improvement in\nthe reflective reasoning capabilities of current LLMs. The leaderboard of our\nbenchmark is available at https:\/\/huggingface.co\/spaces\/UltraRonin\/LR2Bench\n","versions":"[{'version': 'v1', 'created': 'Tue, 25 Feb 2025 04:51:17 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 07:36:01 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Chen', 'Jianghao', ''], ['Wei', 'Zhenlin', ''], ['Ren', 'Zhenjiang', ''], ['Li', 'Ziyong', ''], ['Zhang', 'Jiajun', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'knowledge-based', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2502.20808,"submitter":"Wang Peijie","authors":"Peijie Wang, Zhong-Zhi Li, Fei Yin, Dekang Ran, Cheng-Lin Liu","title":"MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts","comments":"47 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multimodal Large Language Models (MLLMs) have shown promising capabilities in\nmathematical reasoning within visual contexts across various datasets. However,\nmost existing multimodal math benchmarks are limited to single-visual contexts,\nwhich diverges from the multi-visual scenarios commonly encountered in\nreal-world mathematical applications. To address this gap, we introduce\nMV-MATH: a meticulously curated dataset of 2,009 high-quality mathematical\nproblems. Each problem integrates multiple images interleaved with text,\nderived from authentic K-12 scenarios, and enriched with detailed annotations.\nMV-MATH includes multiple-choice, free-form, and multi-step questions, covering\n11 subject areas across 3 difficulty levels, and serves as a comprehensive and\nrigorous benchmark for assessing MLLMs' mathematical reasoning in multi-visual\ncontexts. Through extensive experimentation, we observe that MLLMs encounter\nsubstantial challenges in multi-visual math tasks, with a considerable\nperformance gap relative to human capabilities on MV-MATH. Furthermore, we\nanalyze the performance and error patterns of various models, providing\ninsights into MLLMs' mathematical reasoning capabilities within multi-visual\nsettings.\n","versions":"[{'version': 'v1', 'created': 'Fri, 28 Feb 2025 07:50:36 GMT'}, {'version': 'v2', 'created': 'Mon, 3 Mar 2025 03:43:03 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 14:02:51 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wang', 'Peijie', ''], ['Li', 'Zhong-Zhi', ''], ['Yin', 'Fei', ''], ['Ran', 'Dekang', ''], ['Liu', 'Cheng-Lin', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2503.00847,"submitter":"Johannes Daxenberger","authors":"Moritz Altemeyer, Steffen Eger, Johannes Daxenberger, Tim Altendorf,\n  Philipp Cimiano, Benjamin Schiller","title":"Argument Summarization and its Evaluation in the Era of Large Language\n  Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Large Language Models (LLMs) have revolutionized various Natural Language\nGeneration (NLG) tasks, including Argument Summarization (ArgSum), a key\nsubfield of Argument Mining (AM). This paper investigates the integration of\nstate-of-the-art LLMs into ArgSum, including for its evaluation. In particular,\nwe propose a novel prompt-based evaluation scheme, and validate it through a\nnovel human benchmark dataset. Our work makes three main contributions: (i) the\nintegration of LLMs into existing ArgSum frameworks, (ii) the development of a\nnew LLM-based ArgSum system, benchmarked against prior methods, and (iii) the\nintroduction of an advanced LLM-based evaluation scheme. We demonstrate that\nthe use of LLMs substantially improves both the generation and evaluation of\nargument summaries, achieving state-of-the-art results and advancing the field\nof ArgSum.\n","versions":"[{'version': 'v1', 'created': 'Sun, 2 Mar 2025 10:49:10 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 20:25:48 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Altemeyer', 'Moritz', ''], ['Eger', 'Steffen', ''], ['Daxenberger', 'Johannes', ''], ['Altendorf', 'Tim', ''], ['Cimiano', 'Philipp', ''], ['Schiller', 'Benjamin', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.0109,"submitter":"Haowen Pan","authors":"Haowen Pan, Xiaozhi Wang, Yixin Cao, Zenglin Shi, Xun Yang, Juanzi Li,\n  Meng Wang","title":"Precise Localization of Memories: A Fine-grained Neuron-level Knowledge\n  Editing Technique for LLMs","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Knowledge editing aims to update outdated information in Large Language\nModels (LLMs). A representative line of study is locate-then-edit methods,\nwhich typically employ causal tracing to identify the modules responsible for\nrecalling factual knowledge about entities. However, we find these methods are\noften sensitive only to changes in the subject entity, leaving them less\neffective at adapting to changes in relations. This limitation results in poor\nediting locality, which can lead to the persistence of irrelevant or inaccurate\nfacts, ultimately compromising the reliability of LLMs. We believe this issue\narises from the insufficient precision of knowledge localization. To address\nthis, we propose a Fine-grained Neuron-level Knowledge Editing (FiNE) method\nthat enhances editing locality without affecting overall success rates. By\nprecisely identifying and modifying specific neurons within feed-forward\nnetworks, FiNE significantly improves knowledge localization and editing.\nQuantitative experiments demonstrate that FiNE efficiently achieves better\noverall performance compared to existing techniques, providing new insights\ninto the localization and modification of knowledge within LLMs.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 01:30:28 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 07:34:41 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Pan', 'Haowen', ''], ['Wang', 'Xiaozhi', ''], ['Cao', 'Yixin', ''], ['Shi', 'Zenglin', ''], ['Yang', 'Xun', ''], ['Li', 'Juanzi', ''], ['Wang', 'Meng', '']]","extracted_entities":"[{'text': 'Large Language\\nModels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language\nModels","similarity_score":0.9664971828}
{"id":2503.01611,"submitter":"David Ponce","authors":"David Ponce, Thierry Etchegoyhen","title":"In-context Learning vs. Instruction Tuning: The Case of Small and\n  Multilingual Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Instruction following is a critical ability for Large Language Models to\nperform downstream tasks. The standard approach to instruction alignment has\nrelied on a specific phase of model tuning over curated instruction datasets,\noptionally complemented with an alignment step over human preferences. Recent\nwork has shown the potential of in-context learning (ICL) alternatives to guide\nbase models towards instruction following. This type of approach is\nparticularly relevant to extend instruction following across languages and\nmodels of varying sizes adapted to different types of usage. In this work we\ncompare ICL and instruction fine-tuning in English, French and Spanish, on\nSmall Language Models, and provide experimental results on applying Direct\nPreference Optimisation (DPO) over base models. Our results show that scenarios\ninvolving multilingual and smaller models result in downgraded ICL instruction\nfollowing performance, only partially mitigated by DPO alignment. This study\naims to further our understanding of current strengths and limitations of\nalternative methods for instruction following.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 14:47:23 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 15:32:53 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Ponce', 'David', ''], ['Etchegoyhen', 'Thierry', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}, {'text': 'base models', 'label': 'Foundation Model'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'instruction fine-tuning', 'label': 'Fine-tuning'}, {'text': 'ICL', 'label': 'contextual Embedding'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.01754,"submitter":"Guande Wu","authors":"Guande Wu, Huan Song, Yawei Wang, Qiaojing Yan, Yijun Tian, Lin Lee\n  Cheong, Panpan Xu","title":"SDRT: Enhance Vision-Language Models by Self-Distillation with Diverse\n  Reasoning Traces","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Reasoning is increasingly crucial for various tasks. While chain-of-thought\nprompting enables large language models to leverage reasoning effectively,\nharnessing the reasoning capabilities of Vision-Language Models (VLMs) remains\nchallenging. To solve this problem, we propose a novel self-distillation\nframework that enhances the reasoning capabilities of the model. The proposed\nframework introduces several key innovations. We start by employing a prompt\nlibrary tailored to visual reasoning tasks to generate diverse in-context\nquestions and utilize a two-step reasoning procedure to derive reasoning-guided\nresponses. These responses are then used for self-distillation, enabling the\nmodel to internalize the reasoning process. Additionally, we improve the model\narchitecture with several innovative components, including an intervention\nadapter for efficient parameter updates, a cross-modal skip connection to\nfacilitate information exchange between modalities, and an ensemble learning\nalgorithm to integrate diverse reasoning from multiple in-context questions.\nExtensive experiments show that our method significantly improves the baseline\nperformance across five VQA datasets.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 17:24:42 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 08:05:25 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 18:35:44 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wu', 'Guande', ''], ['Song', 'Huan', ''], ['Wang', 'Yawei', ''], ['Yan', 'Qiaojing', ''], ['Tian', 'Yijun', ''], ['Cheong', 'Lin Lee', ''], ['Xu', 'Panpan', '']]","extracted_entities":"[{'text': 'Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'prompt\\nlibrary', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Vision-Language Models","similarity_score":0.6500363946}
{"id":2503.03586,"submitter":"Chong Wang","authors":"Alperen Yildiz, Sin G. Teo, Yiling Lou, Yebo Feng, Chong Wang, Dinil\n  M. Divakaran","title":"Benchmarking LLMs and LLM-based Agents in Practical Vulnerability\n  Detection for Code Repositories","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) have shown promise in software vulnerability\ndetection, particularly on function-level benchmarks like Devign and BigVul.\nHowever, real-world detection requires interprocedural analysis, as\nvulnerabilities often emerge through multi-hop function calls rather than\nisolated functions. While repository-level benchmarks like ReposVul and VulEval\nintroduce interprocedural context, they remain computationally expensive, lack\npairwise evaluation of vulnerability fixes, and explore limited context\nretrieval, limiting their practicality.\n  We introduce JitVul, a JIT vulnerability detection benchmark linking each\nfunction to its vulnerability-introducing and fixing commits. Built from 879\nCVEs spanning 91 vulnerability types, JitVul enables comprehensive evaluation\nof detection capabilities. Our results show that ReAct Agents, leveraging\nthought-action-observation and interprocedural context, perform better than\nLLMs in distinguishing vulnerable from benign code. While prompting strategies\nlike Chain-of-Thought help LLMs, ReAct Agents require further refinement. Both\nmethods show inconsistencies, either misidentifying vulnerabilities or\nover-analyzing security guards, indicating significant room for improvement.\n","versions":"[{'version': 'v1', 'created': 'Wed, 5 Mar 2025 15:22:24 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 05:30:00 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Yildiz', 'Alperen', ''], ['Teo', 'Sin G.', ''], ['Lou', 'Yiling', ''], ['Feng', 'Yebo', ''], ['Wang', 'Chong', ''], ['Divakaran', 'Dinil M.', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'interprocedural context', 'label': 'contextual Embedding'}, {'text': 'interprocedural context', 'label': 'contextual Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompting strategies', 'label': 'Prompting'}, {'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.04784,"submitter":"Jiexiong Liu","authors":"Cheng Li, Jiexiong Liu, Yixuan Chen, Yanqin Jia, Zhepeng Li","title":"KunlunBaize: LLM with Multi-Scale Convolution and Multi-Token Prediction\n  Under TransformerX Framework","comments":"21 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models have demonstrated remarkable performance across various\ntasks, yet they face challenges such as low computational efficiency, gradient\nvanishing, and difficulties in capturing complex feature interactions. To\naddress these limitations, a novel framework has been proposed. This framework\nincorporates a learnable dense residual skip connection mechanism, a\nTransformerX module a transformer based component integrating multiscale\nconvolution and adaptive activation functions and a multitoken prediction\ninteraction module. The learnable dense residual connections enhance\ninformation flow and feature capture across layers. Within the TransformerX\nmodule, large convolutional kernels aggregate semantic information from\nextensive text segments, while smaller convolutions focus on local word order\nand syntactic structures. The adaptive activation function dynamically adjusts\nits parameters based on the semantic features of the input text, improving the\nmodel's ability to handle diverse semantic expressions and complex\nrelationships. The multitoken prediction module boosts data utilization and\naccelerates inference by predicting multiple future tokens. These components\nsignificantly enhance the performance and efficiency of large language models.\n","versions":"[{'version': 'v1', 'created': 'Thu, 27 Feb 2025 01:56:09 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Mar 2025 01:59:26 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 03:04:01 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Li', 'Cheng', ''], ['Liu', 'Jiexiong', ''], ['Chen', 'Yixuan', ''], ['Jia', 'Yanqin', ''], ['Li', 'Zhepeng', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.04872,"submitter":"Guangxiang Zhao","authors":"Lin Sun, Guangxiang Zhao, Xiaoqi Jian, Yuhan Wu, Weihong Lin, Yongfu\n  Zhu, Change Jia, Linglin Zhang, Jinzhu Wu, Junfeng Ran, Sai-er Hu, Zihan\n  Jiang, Junting Zhou, Wenrui Liu, Bin Cui, Tong Yang, Xiangzheng Zhang","title":"TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation","comments":"Preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The challenge of reducing the size of Large Language Models (LLMs) while\nmaintaining their performance has gained significant attention. However,\nexisting methods, such as model distillation and transfer learning, often fail\nto achieve high accuracy. To address this limitation, we introduce the\nBranch-Merge distillation approach, which enhances model compression through\ntwo phases: (1) the Branch Phase, where knowledge from a large teacher model is\n\\textit{selectively distilled} into specialized student models via\ndomain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where\nthese student models are merged to enable cross-domain knowledge transfer and\nimprove generalization. We validate our distillation approach using DeepSeek-R1\nas the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting\nmerged model, TinyR1-32B-Preview, outperforms its counterpart\nDeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics\n(+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving\nnear-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge\ndistillation approach provides a scalable solution for creating smaller,\nhigh-performing LLMs with reduced computational cost and time.\n","versions":"[{'version': 'v1', 'created': 'Thu, 6 Mar 2025 16:25:53 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 10:36:30 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Sun', 'Lin', ''], ['Zhao', 'Guangxiang', ''], ['Jian', 'Xiaoqi', ''], ['Wu', 'Yuhan', ''], ['Lin', 'Weihong', ''], ['Zhu', 'Yongfu', ''], ['Jia', 'Change', ''], ['Zhang', 'Linglin', ''], ['Wu', 'Jinzhu', ''], ['Ran', 'Junfeng', ''], ['Hu', 'Sai-er', ''], ['Jiang', 'Zihan', ''], ['Zhou', 'Junting', ''], ['Liu', 'Wenrui', ''], ['Cui', 'Bin', ''], ['Yang', 'Tong', ''], ['Zhang', 'Xiangzheng', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'model distillation', 'label': 'Knowledge distillation'}, {'text': 'transfer learning', 'label': 'Few-shot Learning'}, {'text': 'Branch-Merge distillation', 'label': 'Knowledge distillation'}, {'text': 'domain-specific supervised fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.07459,"submitter":"Xiangru Tang","authors":"Xiangru Tang, Daniel Shao, Jiwoong Sohn, Jiapeng Chen, Jiayi Zhang,\n  Jinyu Xiang, Fang Wu, Yilun Zhao, Chenglin Wu, Wenqi Shi, Arman Cohan, Mark\n  Gerstein","title":"MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) have shown impressive performance on existing\nmedical question-answering benchmarks. This high performance makes it\nincreasingly difficult to meaningfully evaluate and differentiate advanced\nmethods. We present MedAgentsBench, a benchmark that focuses on challenging\nmedical questions requiring multi-step clinical reasoning, diagnosis\nformulation, and treatment planning-scenarios where current models still\nstruggle despite their strong performance on standard tests. Drawing from seven\nestablished medical datasets, our benchmark addresses three key limitations in\nexisting evaluations: (1) the prevalence of straightforward questions where\neven base models achieve high performance, (2) inconsistent sampling and\nevaluation protocols across studies, and (3) lack of systematic analysis of the\ninterplay between performance, cost, and inference time. Through experiments\nwith various base models and reasoning methods, we demonstrate that the latest\nthinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in\ncomplex medical reasoning tasks. Additionally, advanced search-based agent\nmethods offer promising performance-to-cost ratios compared to traditional\napproaches. Our analysis reveals substantial performance gaps between model\nfamilies on complex questions and identifies optimal model selections for\ndifferent computational constraints. Our benchmark and evaluation framework are\npublicly available at https:\/\/github.com\/gersteinlab\/medagents-benchmark.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Mar 2025 15:38:44 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 01:30:56 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Tang', 'Xiangru', ''], ['Shao', 'Daniel', ''], ['Sohn', 'Jiwoong', ''], ['Chen', 'Jiapeng', ''], ['Zhang', 'Jiayi', ''], ['Xiang', 'Jinyu', ''], ['Wu', 'Fang', ''], ['Zhao', 'Yilun', ''], ['Wu', 'Chenglin', ''], ['Shi', 'Wenqi', ''], ['Cohan', 'Arman', ''], ['Gerstein', 'Mark', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'DeepSeek R1', 'label': 'Foundation Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.07604,"submitter":"Tianhe Lin","authors":"Tianhe Lin, Jian Xie, Siyu Yuan, Deqing Yang","title":"Implicit Reasoning in Transformers is Reasoning through Shortcuts","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Test-time compute is emerging as a new paradigm for enhancing language\nmodels' complex multi-step reasoning capabilities, as demonstrated by the\nsuccess of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit\nreasoning in test-time compute, implicit reasoning is more inference-efficient,\nrequiring fewer generated tokens. However, why does the advanced reasoning\ncapability fail to emerge in the implicit reasoning style? In this work, we\ntrain GPT-2 from scratch on a curated multi-step mathematical reasoning dataset\nand conduct analytical experiments to investigate how language models perform\nimplicit reasoning in multi-step tasks. Our findings reveal: 1) Language models\ncan perform step-by-step reasoning and achieve high accuracy in both in-domain\nand out-of-domain tests via implicit reasoning. However, this capability only\nemerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning\nabilities emerging from training on unfixed-pattern data tend to overfit a\nspecific pattern and fail to generalize further. Notably, this limitation is\nalso observed in state-of-the-art large language models. These findings suggest\nthat language models acquire implicit reasoning through shortcut learning,\nenabling strong performance on tasks with similar patterns while lacking\ngeneralization.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Mar 2025 17:58:31 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 12:08:17 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Lin', 'Tianhe', ''], ['Xie', 'Jian', ''], ['Yuan', 'Siyu', ''], ['Yang', 'Deqing', '']]","extracted_entities":"[{'text': 'OpenAI', 'label': 'Open-source LLMs'}, {'text': 'state-of-the-art large language models', 'label': 'Large Language Model'}, {'text': 'shortcut learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"state-of-the-art large language models","similarity_score":0.8680342436}
{"id":2503.08144,"submitter":"Fei Wang","authors":"Fei Wang, Chengcheng Chen, Hongyu Chen, Yugang Chang, Weiming Zeng","title":"Bring Remote Sensing Object Detect Into Nature Language Model: Using SFT\n  Method","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recently, large language models (LLMs) and vision-language models (VLMs) have\nachieved significant success, demonstrating remarkable capabilities in\nunderstanding various images and videos, particularly in classification and\ndetection tasks. However, due to the substantial differences between remote\nsensing images and conventional optical images, these models face considerable\nchallenges in comprehension, especially in detection tasks. Directly prompting\nVLMs with detection instructions often leads to unsatisfactory results. To\naddress this issue, this letter explores the application of VLMs for object\ndetection in remote sensing images. Specifically, we constructed supervised\nfine-tuning (SFT) datasets using publicly available remote sensing object\ndetection datasets, including SSDD, HRSID, and NWPU-VHR-10. In these new\ndatasets, we converted annotation information into JSON-compliant natural\nlanguage descriptions, facilitating more effective understanding and training\nfor the VLM. We then evaluate the detection performance of various fine-tuning\nstrategies for VLMs and derive optimized model weights for object detection in\nremote sensing images. Finally, we evaluate the model's prior knowledge\ncapabilities using natural language queries. Experimental results demonstrate\nthat, without modifying the model architecture, remote sensing object detection\ncan be effectively achieved using natural language alone. Additionally, the\nmodel exhibits the ability to perform certain vision question answering (VQA)\ntasks. Our datasets and related code will be released soon.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 08:02:54 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 13:21:00 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wang', 'Fei', ''], ['Chen', 'Chengcheng', ''], ['Chen', 'Hongyu', ''], ['Chang', 'Yugang', ''], ['Zeng', 'Weiming', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.09454,"submitter":"Malik Marmonier","authors":"Malik Marmonier, Rachel Bawden, Beno\\^it Sagot","title":"Explicit Learning and the LLM in Machine Translation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  This study explores the capacity of large language models (LLMs) for explicit\nlearning, a process involving the assimilation of metalinguistic explanations\nto carry out language tasks. Using constructed languages generated by\ncryptographic means as controlled test environments, we designed experiments to\nassess an LLM's ability to explicitly learn and apply grammar rules. Our\nresults demonstrate that while LLMs possess a measurable capacity for explicit\nlearning, this ability diminishes as the complexity of the linguistic phenomena\nat hand increases. Supervised fine-tuning on chains of thought significantly\nenhances LLM performance but struggles to generalize to typologically novel or\nmore complex linguistic features. These findings point to the need for more\ndiverse training sets and alternative fine-tuning strategies to further improve\nexplicit learning by LLMs.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 14:57:08 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 15:23:04 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Marmonier', 'Malik', ''], ['Bawden', 'Rachel', ''], ['Sagot', 'Beno\u00eet', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'explicit\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'explicit\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'Supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'chains of thought', 'label': 'Chain of thought'}, {'text': 'alternative fine-tuning strategies', 'label': 'Fine-tuning'}, {'text': 'explicit learning', 'label': 'Few-shot Learning'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.0962,"submitter":"Qitan Lv","authors":"Qitan Lv, Tianyu Liu, Hong Wang","title":"Exploiting Edited Large Language Models as General Scientific Optimizers","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models (LLMs) have been widely adopted in mathematical\noptimization in scientific scenarios for their extensive knowledge and advanced\nreasoning capabilities. Existing methods mainly focus on utilizing LLMs to\nsolve optimization problems in a prompt-based manner, which takes observational\nfeedback as additional textual descriptions. However, due to LLM's \\textbf{high\nsensitivity to the prompts} and \\textbf{tendency to get lost in lengthy\nprompts}, these methods struggle to effectively utilize the {observational}\nfeedback from each optimization step, which severely hinders the applications\nfor real-world scenarios. To address these challenges, we propose a\nconceptually simple and general {bi-level} optimization method, namely\n\\textbf{G}eneral \\textbf{S}cientific \\textbf{O}ptimizers (GSO). Specifically,\nGSO first utilizes inner-level simulators as experimental platforms to evaluate\nthe current solution and provide observational feedback. Then, LLMs serve as\nknowledgeable and versatile scientists, generating new solutions by refining\npotential errors from the feedback as the outer-level optimization. Finally,\nsimulations together with the expert knowledge in LLMs are jointly updated with\nbi-level interactions via model editing. Extensive experiments show that GSO\nconsistently outperforms existing state-of-the-art methods using \\textit{six}\ndifferent LLM backbones on \\textit{seven} different tasks, demonstrating the\neffectiveness and a wide range of applications.\n","versions":"[{'version': 'v1', 'created': 'Sat, 8 Mar 2025 18:01:11 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 05:40:49 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Lv', 'Qitan', ''], ['Liu', 'Tianyu', ''], ['Wang', 'Hong', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompt-based', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.09647,"submitter":"Ryan Wei Heng Quek","authors":"Ryan Quek Wei Heng, Edoardo Vittori, Keane Ong, Rui Mao, Erik Cambria,\n  Gianmarco Mengaldo","title":"Leveraging LLMS for Top-Down Sector Allocation In Automated Trading","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CE q-fin.PM","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper introduces a methodology leveraging Large Language Models (LLMs)\nfor sector-level portfolio allocation through systematic analysis of\nmacroeconomic conditions and market sentiment. Our framework emphasizes\ntop-down sector allocation by processing multiple data streams simultaneously,\nincluding policy documents, economic indicators, and sentiment patterns.\nEmpirical results demonstrate superior risk-adjusted returns compared to\ntraditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and\nportfolio return of 8.79% versus -0.61 and -1.39% respectively. These results\nsuggest that LLM-based systematic macro analysis presents a viable approach for\nenhancing automated portfolio allocation decisions at the sector level.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 08:41:36 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 14:37:14 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Heng', 'Ryan Quek Wei', ''], ['Vittori', 'Edoardo', ''], ['Ong', 'Keane', ''], ['Mao', 'Rui', ''], ['Cambria', 'Erik', ''], ['Mengaldo', 'Gianmarco', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.09657,"submitter":"Guanchen Li","authors":"Guanchen Li, Yixing Xu, Zeping Li, Ji Liu, Xuanwu Yin, Dong Li, Emad\n  Barsoum","title":"T\\'yr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via\n  Global Sparsity Distribution Optimization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Structural pruning enhances hardware-agnostic inference efficiency for large\nlanguage models (LLMs) but often struggles to maintain performance. Local\npruning performs efficient layer-by-layer compression but ignores global\ntopology. Global pruning has the potential to find the optimal solution\nalthough resource-intensive. However, existing methods tend to rank structural\nsaliency uniformly, ignoring inter-structure dependencies and failing to\nachieve end-to-end optimization. To address these limitations, we propose\nT\\'yr-the-Pruner, an efficient end-to-end search-based global structural\npruning framework. This framework constructs a supernet by repeatedly applying\nlocal pruning across a range of sparsity ratios to each layer in an LLM, with\nthe core goal of determining the optimal sparsity distribution under a target\noverall sparsity ratio. Concretely, we introduce an effective local pruning and\nan expectation error accumulation approach to improve supernet construction.\nFurthermore, we employ an iterative prune-and-search strategy with\ncoarse-to-fine sparsity granularity to ensure efficient search convergence.\nExperimental results show that T\\'yr-the-Pruner achieves state-of-the-art\nstructural pruning, retaining 97% of the dense model's performance while\nremoving a challenging 50% of Llama-3.1-70B's parameters.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 11:52:49 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 01:51:05 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Li', 'Guanchen', ''], ['Xu', 'Yixing', ''], ['Li', 'Zeping', ''], ['Liu', 'Ji', ''], ['Yin', 'Xuanwu', ''], ['Li', 'Dong', ''], ['Barsoum', 'Emad', '']]","extracted_entities":"[{'text': 'large\\nlanguage models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large\nlanguage models","similarity_score":0.9664971828}
{"id":2503.10167,"submitter":"Je Won Yeom","authors":"Hyunbin Jin, Je Won Yeom, Seunghyun Bae, Taesup Kim","title":"\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection\n  Decoding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs) exhibit strong reasoning abilities, often\nattributed to few-shot or zero-shot chain-of-thought (CoT) prompting. While\neffective, these methods require labor-intensive prompt engineering, raising\nthe question of whether reasoning can be induced without reliance on explicit\nprompts. In this work, we unlock the reasoning capabilities of LLMs without\nexplicit prompting. Inspired by zero-shot CoT and CoT-decoding, we propose a\nnovel decoding strategy that systematically nudges LLMs to continue reasoning,\nthereby preventing immature reasoning processes. Specifically, we monitor the\nmodel's generation and inject a designated phrase whenever it is likely to\nconclude its response prematurely, before completing the reasoning process. Our\nexperimental evaluations on diverse reasoning benchmarks demonstrate that our\nproposed strategy substantially improves LLM reasoning capabilities,\nhighlighting the potential of decoding-based interventions as an alternative to\ntraditional prompting techniques.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 08:46:32 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 00:25:47 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Jin', 'Hyunbin', ''], ['Yeom', 'Je Won', ''], ['Bae', 'Seunghyun', ''], ['Kim', 'Taesup', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'zero-shot CoT', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.10905,"submitter":"Zhuoyan Xu","authors":"Zhuoyan Xu, Khoi Duc Nguyen, Preeti Mukherjee, Saurabh Bagchi, Somali\n  Chaterji, Yingyu Liang, Yin Li","title":"Learning to Inference Adaptively for Multimodal Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multimodal Large Language Models (MLLMs) have shown impressive capabilities\nin reasoning, yet come with substantial computational cost, limiting their\ndeployment in resource-constrained settings. Despite recent efforts on\nimproving the efficiency of MLLMs, prior solutions fall short in responding to\nvarying runtime conditions, in particular changing resource availability (e.g.,\ncontention due to the execution of other programs on the device). To bridge\nthis gap, we introduce AdaLLaVA, an adaptive inference framework that learns to\ndynamically reconfigure operations in an MLLM during inference, accounting for\nthe input data and a latency budget. We conduct extensive experiments across\nbenchmarks involving question-answering, reasoning, and hallucination. Our\nresults show that AdaLLaVA effectively adheres to input latency budget,\nachieving varying accuracy and latency tradeoffs at runtime. Further, we\ndemonstrate that AdaLLaVA adapts to both input latency and content, can be\nintegrated with token selection for enhanced efficiency, and generalizes across\nMLLMs. Our project webpage with code release is at\nhttps:\/\/zhuoyan-xu.github.io\/ada-llava\/.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 21:39:38 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 20:35:28 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Xu', 'Zhuoyan', ''], ['Nguyen', 'Khoi Duc', ''], ['Mukherjee', 'Preeti', ''], ['Bagchi', 'Saurabh', ''], ['Chaterji', 'Somali', ''], ['Liang', 'Yingyu', ''], ['Li', 'Yin', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2503.11197,"submitter":"Gang Li","authors":"Gang Li, Jizhong Liu, Heinrich Dinkel, Yadong Niu, Junbo Zhang, Jian\n  Luan","title":"Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.AI cs.CL eess.AS","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps:\/\/github.com\/xiaomi-research\/r1-aqa and\nhttps:\/\/huggingface.co\/mispeech\/r1-aqa.\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 08:43:53 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 04:20:29 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 16:33:16 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Li', 'Gang', ''], ['Liu', 'Jizhong', ''], ['Dinkel', 'Heinrich', ''], ['Niu', 'Yadong', ''], ['Zhang', 'Junbo', ''], ['Luan', 'Jian', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'LALMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.11227,"submitter":"Jian Zhang","authors":"Jian Zhang, Bifan Wei, Shihao Qi, haiping Zhu, Jun Liu, Qika Lin","title":"GKG-LLM: A Unified Framework for Generalized Knowledge Graph\n  Construction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The construction of Generalized Knowledge Graph (GKG), including knowledge\ngraph, event knowledge graph and commonsense knowledge graph, is fundamental\nfor various natural language processing tasks. Current studies typically\nconstruct these types of graph separately, overlooking holistic insights and\npotential unification that could be beneficial in computing resources and usage\nperspectives. However, a key challenge in developing a unified framework for\nGKG is obstacles arising from task-specific differences. In this study, we\npropose a unified framework for constructing generalized knowledge graphs to\naddress this challenge. First, we collect data from 15 sub-tasks in 29 datasets\nacross the three types of graphs, categorizing them into in-sample,\ncounter-task, and out-of-distribution (OOD) data. Then, we propose a\nthree-stage curriculum learning fine-tuning framework, by iteratively injecting\nknowledge from the three types of graphs into the Large Language Models.\nExtensive experiments show that our proposed model improves the construction of\nall three graph types across in-domain, OOD and counter-task data.\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 09:23:22 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 06:41:34 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Zhang', 'Jian', ''], ['Wei', 'Bifan', ''], ['Qi', 'Shihao', ''], ['Zhu', 'haiping', ''], ['Liu', 'Jun', ''], ['Lin', 'Qika', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.1128,"submitter":"Bryan Wilie","authors":"Bryan Wilie, Samuel Cahyawijaya, Junxian He, Pascale Fung","title":"High-Dimensional Interlingual Representations of Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning.\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 10:39:27 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 12:16:42 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Wilie', 'Bryan', ''], ['Cahyawijaya', 'Samuel', ''], ['He', 'Junxian', ''], ['Fung', 'Pascale', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'single-language fine-tuning', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'scalable multilingual learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.11302,"submitter":"Michael Hanna","authors":"Michael Hanna, Sandro Pezzelle, Yonatan Belinkov","title":"Are formal and functional linguistic mechanisms dissociated in language\n  models?","comments":"35 pages, 10 figures, 3 tables. Code available at\n  https:\/\/github.com\/hannamw\/formal-functional-dissociation","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Although large language models (LLMs) are increasingly capable, these\ncapabilities are unevenly distributed: they excel at formal linguistic tasks,\nsuch as producing fluent, grammatical text, but struggle more with functional\nlinguistic tasks like reasoning and consistent fact retrieval. Inspired by\nneuroscience, recent work suggests that to succeed on both formal and\nfunctional linguistic tasks, LLMs should use different mechanisms for each;\nsuch localization could either be built-in or emerge spontaneously through\ntraining. In this paper, we ask: do current models, with fast-improving\nfunctional linguistic abilities, exhibit distinct localization of formal and\nfunctional linguistic mechanisms? We answer this by finding and comparing the\n\"circuits\", or minimal computational subgraphs, responsible for various formal\nand functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that\nwhile there is indeed little overlap between circuits for formal and functional\ntasks, there is also little overlap between formal linguistic tasks, as exists\nin the human brain. Thus, a single formal linguistic network, unified and\ndistinct from functional task circuits, remains elusive. However, in terms of\ncross-task faithfulness - the ability of one circuit to solve another's task -\nwe observe a separation between formal and functional mechanisms, suggesting\nthat shared mechanisms between formal tasks may exist.\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 11:11:03 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 12:17:11 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Hanna', 'Michael', ''], ['Pezzelle', 'Sandro', ''], ['Belinkov', 'Yonatan', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.11367,"submitter":"Insu Jang","authors":"Insu Jang and Runyu Lu and Nikhil Bansal and Ang Chen and Mosharaf\n  Chowdhury","title":"Cornstarch: Distributed Multimodal Training Must Be Multimodality-Aware","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multimodal large language models (MLLMs) extend the capabilities of large\nlanguage models (LLMs) by combining heterogeneous model architectures to handle\ndiverse modalities like images and audio. However, this inherent heterogeneity\nin MLLM model structure and data types makes makeshift extensions to existing\nLLM training frameworks unsuitable for efficient MLLM training.\n  In this paper, we present Cornstarch, the first general-purpose distributed\nMLLM training framework. Cornstarch facilitates modular MLLM construction,\nenables composable parallelization of constituent models, and introduces\nMLLM-specific optimizations to pipeline and context parallelism for efficient\ndistributed MLLM training. Our evaluation shows that Cornstarch outperforms\nstate-of-the-art solutions by up to $1.57\\times$ in terms of training\nthroughput.\n  Cornstarch is an open-source project available at\nhttps:\/\/github.com\/cornstarch-org\/Cornstarch.\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 13:07:45 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 12:39:15 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Jang', 'Insu', ''], ['Lu', 'Runyu', ''], ['Bansal', 'Nikhil', ''], ['Chen', 'Ang', ''], ['Chowdhury', 'Mosharaf', '']]","extracted_entities":"[{'text': 'Multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'large\\nlanguage models', 'label': 'Large Language Model'}, {'text': 'Cornstarch', 'label': 'Open-source LLMs'}, {'text': 'Cornstarch', 'label': 'Open-source LLMs'}, {'text': 'Cornstarch', 'label': 'Open-source LLMs'}, {'text': 'Cornstarch', 'label': 'Open-source LLMs'}, {'text': 'cornstarch-org', 'label': 'Open-source LLMs'}, {'text': 'Cornstarch', 'label': 'Open-source LLMs'}]","assigned_concept":"Large Language Model","matched_keyword":"large\nlanguage models","similarity_score":0.9664971828}
{"id":2503.1196,"submitter":"Jiawei Li","authors":"Jiawei Li, David Farag\\'o, Christian Petrov, Iftekhar Ahmed","title":"Consider What Humans Consider: Optimizing Commit Message Leveraging\n  Contexts Considered By Human","comments":"arXiv admin note: substantial text overlap with arXiv:2501.09861","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Commit messages are crucial in software development, supporting maintenance\ntasks and communication among developers. While Large Language Models (LLMs)\nhave advanced Commit Message Generation (CMG) using various software contexts,\nsome contexts developers consider to write high-quality commit messages are\noften missed by CMG techniques and can't be easily retrieved or even retrieved\nat all by automated tools. To address this, we propose Commit Message\nOptimization (CMO), which enhances human-written messages by leveraging LLMs\nand search-based optimization. CMO starts with human-written messages and\niteratively improves them by integrating key contexts and feedback from\nexternal evaluators. Our extensive evaluation shows CMO generates commit\nmessages that are significantly more Rational, Comprehensive, and Expressive\nwhile outperforming state-of-the-art CMG methods and human messages 40.3% to\n78.4% of the time. Moreover, CMO can support existing CMG techniques to further\nimprove message quality and generate high-quality messages when the\nhuman-written ones are left blank.\n","versions":"[{'version': 'v1', 'created': 'Sat, 15 Mar 2025 02:10:02 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 06:15:33 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Li', 'Jiawei', ''], ['Farag\u00f3', 'David', ''], ['Petrov', 'Christian', ''], ['Ahmed', 'Iftekhar', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'software contexts', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.12052,"submitter":"Zhiyao Sun","authors":"Zhiyao Sun, Yu-Hui Wen, Matthieu Lin, Ho-Jui Fang, Sheng Ye, Tian Lv,\n  Yong-Jin Liu","title":"Tailor: An Integrated Text-Driven CG-Ready Human and Garment Generation\n  System","comments":"Project page: https:\/\/human-tailor.github.io","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.GR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Creating detailed 3D human avatars with garments typically requires\nspecialized expertise and labor-intensive processes. Although recent advances\nin generative AI have enabled text-to-3D human\/clothing generation, current\nmethods fall short in offering accessible, integrated pipelines for producing\nready-to-use clothed avatars. To solve this, we introduce Tailor, an integrated\ntext-to-avatar system that generates high-fidelity, customizable 3D humans with\nsimulation-ready garments. Our system includes a three-stage pipeline. We first\nemploy a large language model to interpret textual descriptions into\nparameterized body shapes and semantically matched garment templates. Next, we\ndevelop topology-preserving deformation with novel geometric losses to adapt\ngarments precisely to body geometries. Furthermore, an enhanced texture\ndiffusion module with a symmetric local attention mechanism ensures both view\nconsistency and photorealistic details. Quantitative and qualitative\nevaluations demonstrate that Tailor outperforms existing SoTA methods in terms\nof fidelity, usability, and diversity. Code will be available for academic use.\n","versions":"[{'version': 'v1', 'created': 'Sat, 15 Mar 2025 08:58:02 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 06:08:49 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Sun', 'Zhiyao', ''], ['Wen', 'Yu-Hui', ''], ['Lin', 'Matthieu', ''], ['Fang', 'Ho-Jui', ''], ['Ye', 'Sheng', ''], ['Lv', 'Tian', ''], ['Liu', 'Yong-Jin', '']]","extracted_entities":"[{'text': 'large language model', 'label': 'Large Language Model'}, {'text': 'symmetric local attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Large Language Model","matched_keyword":"large language model","similarity_score":1.0}
{"id":2503.12374,"submitter":"Zhi Chen","authors":"Zhi Chen, Wei Ma, Lingxiao Jiang","title":"Unveiling Pitfalls: Understanding Why AI-driven Code Agents Fail at\n  GitHub Issue Resolution","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  AI-driven software development has rapidly advanced with the emergence of\nsoftware development agents that leverage large language models (LLMs) to\ntackle complex, repository-level software engineering tasks. These agents go\nbeyond just generation of final code; they engage in multi-step reasoning,\nutilize various tools for code modification and debugging, and interact with\nexecution environments to diagnose and iteratively resolve issues. However,\nmost existing evaluations focus primarily on static analyses of final code\noutputs, yielding limited insights into the agents' dynamic problem-solving\nprocesses. To fill this gap, we conduct an in-depth empirical study on 3,977\nsolving-phase trajectories and 3,931 testing-phase logs from 8 top-ranked\nagents evaluated on 500 GitHub issues in the SWE-Bench benchmark. Our\nexploratory analysis shows that Python execution errors during the issue\nresolution phase correlate with lower resolution rates and increased reasoning\noverheads. We have identified the most prevalent errors -- such as\nModuleNotFoundError and TypeError -- and highlighted particularly challenging\nerrors like OSError and database-related issues (e.g., IntegrityError) that\ndemand significantly more debugging effort. Furthermore, we have discovered 3\nbugs in the SWE-Bench platform that affect benchmark fairness and accuracy;\nthese issues have been reported to and confirmed by the maintainers. To promote\ntransparency and foster future research, we publicly share our datasets and\nanalysis scripts.\n","versions":"[{'version': 'v1', 'created': 'Sun, 16 Mar 2025 06:24:51 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 10:08:16 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Chen', 'Zhi', ''], ['Ma', 'Wei', ''], ['Jiang', 'Lingxiao', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'benchmark fairness and accuracy', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.12722,"submitter":"Jord Nguyen","authors":"Kenneth J. K. Ong, Lye Jia Jun, Hieu Minh \"Jord\" Nguyen, Seong Hah\n  Cho, Natalia P\\'erez-Campanero Antol\\'in","title":"Identifying Cooperative Personalities in Multi-agent Contexts through\n  Personality Steering with Representation Engineering","comments":"Poster, Technical AI Safety Conference 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.GT cs.MA","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  As Large Language Models (LLMs) gain autonomous capabilities, their\ncoordination in multi-agent settings becomes increasingly important. However,\nthey often struggle with cooperation, leading to suboptimal outcomes. Inspired\nby Axelrod's Iterated Prisoner's Dilemma (IPD) tournaments, we explore how\npersonality traits influence LLM cooperation. Using representation engineering,\nwe steer Big Five traits (e.g., Agreeableness, Conscientiousness) in LLMs and\nanalyze their impact on IPD decision-making. Our results show that higher\nAgreeableness and Conscientiousness improve cooperation but increase\nsusceptibility to exploitation, highlighting both the potential and limitations\nof personality-based steering for aligning AI agents.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 01:21:54 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Ong', 'Kenneth J. K.', ''], ['Jun', 'Lye Jia', ''], ['Nguyen', 'Hieu Minh \"Jord\"', ''], ['Cho', 'Seong Hah', ''], ['Antol\u00edn', 'Natalia P\u00e9rez-Campanero', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'Agreeableness', 'label': 'Model Bias and Fairness'}, {'text': 'Conscientiousness', 'label': 'Model Bias and Fairness'}, {'text': 'Agreeableness', 'label': 'Model Bias and Fairness'}, {'text': 'Conscientiousness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.12757,"submitter":"Christine Lee","authors":"Christine Lee, Jihye Choi, Bilge Mutlu","title":"MAP: Multi-user Personalization with Collaborative LLM-powered Agents","comments":"In Extended Abstracts of the CHI Conference on Human Factors in\n  Computing Systems (CHI EA '25), April 26-May 1, 2025, Yokohama, Japan","journal-ref":null,"doi":"10.1145\/3706599.3719853","report-no":null,"categories":"cs.HC cs.AI cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The widespread adoption of Large Language Models (LLMs) and LLM-powered\nagents in multi-user settings underscores the need for reliable, usable methods\nto accommodate diverse preferences and resolve conflicting directives. Drawing\non conflict resolution theory, we introduce a user-centered workflow for\nmulti-user personalization comprising three stages: Reflection, Analysis, and\nFeedback. We then present MAP -- a \\textbf{M}ulti-\\textbf{A}gent system for\nmulti-user \\textbf{P}ersonalization -- to operationalize this workflow. By\ndelegating subtasks to specialized agents, MAP (1) retrieves and reflects on\nrelevant user information, while enhancing reliability through agent-to-agent\ninteractions, (2) provides detailed analysis for improved transparency and\nusability, and (3) integrates user feedback to iteratively refine results. Our\nuser study findings (n=12) highlight MAP's effectiveness and usability for\nconflict resolution while emphasizing the importance of user involvement in\nresolution verification and failure management. This work highlights the\npotential of multi-agent systems to implement user-centered, multi-user\npersonalization workflows and concludes by offering insights for\npersonalization in multi-user contexts.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 02:52:10 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 19:15:44 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Lee', 'Christine', ''], ['Choi', 'Jihye', ''], ['Mutlu', 'Bilge', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.12797,"submitter":"Xinyu Ma","authors":"Xinyu Ma, Ziyang Ding, Zhicong Luo, Chi Chen, Zonghao Guo, Derek F.\n  Wong, Xiaoyi Feng, Maosong Sun","title":"DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs\n  for Knowledge-Intensive Visual Grounding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Human experts excel at fine-grained visual discrimination by leveraging\ndomain knowledge to refine perceptual features, a capability that remains\nunderdeveloped in current Multimodal Large Language Models (MLLMs). Despite\npossessing vast expert-level knowledge, MLLMs struggle to integrate reasoning\ninto visual perception, often generating direct responses without deeper\nanalysis. To bridge this gap, we introduce knowledge-intensive visual grounding\n(KVG), a novel visual grounding task that requires both fine-grained perception\nand domain-specific knowledge integration. To address the challenges of KVG, we\npropose DeepPerception, an MLLM enhanced with cognitive visual perception\ncapabilities. Our approach consists of (1) an automated data synthesis pipeline\nthat generates high-quality, knowledge-aligned training samples, and (2) a\ntwo-stage training framework combining supervised fine-tuning for cognitive\nreasoning scaffolding and reinforcement learning to optimize\nperception-cognition synergy. To benchmark performance, we introduce KVG-Bench\na comprehensive dataset spanning 10 domains with 1.3K manually curated test\ncases. Experimental results demonstrate that DeepPerception significantly\noutperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on\nKVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over\nbaseline approaches. Our findings highlight the importance of integrating\ncognitive processes into MLLMs for human-like visual perception and open new\ndirections for multimodal reasoning research. The data, codes, and models are\nreleased at https:\/\/github.com\/thunlp\/DeepPerception.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 04:06:34 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 05:06:22 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Ma', 'Xinyu', ''], ['Ding', 'Ziyang', ''], ['Luo', 'Zhicong', ''], ['Chen', 'Chi', ''], ['Guo', 'Zonghao', ''], ['Wong', 'Derek F.', ''], ['Feng', 'Xiaoyi', ''], ['Sun', 'Maosong', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'reinforcement learning', 'label': 'Few-shot Learning'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2503.12821,"submitter":"Mingyang Song","authors":"Mingyang Song, Xiaoye Qu, Jiawei Zhou, Yu Cheng","title":"From Head to Tail: Towards Balanced Representation in Large\n  Vision-Language Models through Adaptive Data Calibration","comments":"Accepted by CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Vision-Language Models (LVLMs) have achieved significant progress in\ncombining visual comprehension with language generation. Despite this success,\nthe training data of LVLMs still suffers from Long-Tail (LT) problems, where\nthe data distribution is highly imbalanced. Previous works have mainly focused\non traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as\nrecognition and classification. Nevertheless, the exploration of LVLM (e.g.\nLLaVA) and more general tasks (e.g. Visual Question Answering and Visual\nReasoning) remains under-explored. In this paper, we first conduct an in-depth\nanalysis of the LT issues in LVLMs and identify two core causes: the\noverrepresentation of head concepts and the underrepresentation of tail\nconcepts. Based on the above observation, we propose an $\\textbf{A}$daptive\n$\\textbf{D}$ata $\\textbf{R}$efinement Framework ($\\textbf{ADR}$), which\nconsists of two stages: $\\textbf{D}$ata $\\textbf{R}$ebalancing ($\\textbf{DR}$)\nand $\\textbf{D}$ata $\\textbf{S}$ynthesis ($\\textbf{DS}$). In the DR stage, we\nadaptively rebalance the redundant data based on entity distributions, while in\nthe DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and\nscarce images to supplement underrepresented portions. Through comprehensive\nevaluations across eleven benchmarks, our proposed ADR effectively mitigates\nthe long-tail problem in the training data, improving the average performance\nof LLaVA 1.5 relatively by 4.36%, without increasing the training data volume.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 05:01:09 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 06:02:39 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Song', 'Mingyang', ''], ['Qu', 'Xiaoye', ''], ['Zhou', 'Jiawei', ''], ['Cheng', 'Yu', '']]","extracted_entities":"[{'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Vision-Language Models","similarity_score":0.7742220759}
{"id":2503.12844,"submitter":"Junhyeok Kim","authors":"Junhyeok Kim, Jaewoo Park, Junhee Park, Sangeyl Lee, Jiwan Chung,\n  Jisung Kim, Ji Hoon Joung, Youngjae Yu","title":"GuideDog: A Real-World Egocentric Multimodal Dataset for Blind and\n  Low-Vision Accessibility-Aware Guidance","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Mobility remains a significant challenge for the 2.2 billion people worldwide\naffected by blindness and low vision (BLV), with 7% of visually impaired\nindividuals experiencing falls at least once a month. While recent advances in\nMultimodal Large Language Models (MLLMs) offer promising opportunities for BLV\nassistance, their development has been hindered by limited datasets. This\nlimitation stems from the fact that BLV-aware annotation requires specialized\ndomain knowledge and intensive labor. To address this gap, we introduce\nGuideDog, a novel accessibility-aware guide dataset containing 22K\nimage-description pairs (including 2K human-annotated pairs) that capture\ndiverse real-world scenes from a pedestrian's viewpoint. Our approach shifts\nthe annotation burden from generation to verification through a collaborative\nhuman-AI framework grounded in established accessibility standards,\nsignificantly improving efficiency while maintaining high-quality annotations.\nWe also develop GuideDogQA, a subset of 818 samples featuring multiple-choice\nquestions designed to evaluate fine-grained visual perception capabilities,\nspecifically object recognition and relative depth perception. Our experimental\nresults highlight the importance of accurate spatial understanding for\neffective BLV guidance. GuideDog and GuideDogQA will advance research in\nMLLM-based assistive technologies for BLV individuals while contributing to\nbroader applications in understanding egocentric scenes for robotics and\naugmented reality. The code and dataset will be publicly available.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 05:43:40 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Kim', 'Junhyeok', ''], ['Park', 'Jaewoo', ''], ['Park', 'Junhee', ''], ['Lee', 'Sangeyl', ''], ['Chung', 'Jiwan', ''], ['Kim', 'Jisung', ''], ['Joung', 'Ji Hoon', ''], ['Yu', 'Youngjae', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2503.12852,"submitter":"Aditi Tiwari","authors":"Aditi Tiwari and Klara Nahrstedt","title":"ACT360: An Efficient 360-Degree Action Detection and Summarization\n  Framework for Mission-Critical Training and Debriefing","comments":"9 pages, 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.MM","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Effective training and debriefing are critical in high-stakes,\nmission-critical environments such as disaster response, military simulations,\nand industrial safety, where precision and minimizing errors are paramount. The\ntraditional post-training analysis relies on manually reviewing 2D videos, a\ntime-consuming process that lacks comprehensive situational awareness. To\naddress these limitations, we introduce ACT360, a system that leverages\n360-degree videos and machine learning for automated action detection and\nstructured debriefing. ACT360 integrates 360YOWO, an enhanced You Only Watch\nOnce (YOWO) model with spatial attention and equirectangular-aware convolution\n(EAC) to mitigate panoramic video distortions. To enable deployment in\nresource-constrained environments, we apply quantization and model pruning,\nreducing the model size by 74% while maintaining robust accuracy (mAP drop of\nonly 1.5%, from 0.865 to 0.850) and improving inference speed. We validate our\napproach on a publicly available dataset of 55 labeled 360-degree videos\ncovering seven key operational actions, recorded across various real-world\ntraining sessions and environmental conditions. Additionally, ACT360 integrates\n360AIE (Action Insight Explorer), a web-based interface for automatic action\ndetection, retrieval, and textual summarization using large language models\n(LLMs), significantly enhancing post-incident analysis efficiency. ACT360\nserves as a generalized framework for mission-critical debriefing,\nincorporating EAC, spatial attention, summarization, and model optimization.\nThese innovations apply to any training environment requiring lightweight\naction detection and structured post-exercise analysis.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 06:12:36 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Tiwari', 'Aditi', ''], ['Nahrstedt', 'Klara', '']]","extracted_entities":"[{'text': 'spatial attention', 'label': 'Attention mechanism'}, {'text': 'equirectangular-aware convolution', 'label': 'Attention mechanism'}, {'text': 'quantization', 'label': 'quantisation'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'EAC', 'label': 'Attention mechanism'}, {'text': 'spatial attention', 'label': 'Attention mechanism'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.12854,"submitter":"Songjun Tu","authors":"Songjun Tu, Jiahao Lin, Xiangyu Tian, Qichao Zhang, Linjing Li, Yuqian\n  Fu, Nan Xu, Wei He, Xiangyuan Lan, Dongmei Jiang, Dongbin Zhao","title":"Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical\n  Investigation","comments":null,"journal-ref":"COLM 2025","doi":null,"report-no":"Submitted to COLM 2025","categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Recent advancements in post-training methodologies for large language models\n(LLMs) have highlighted reinforcement learning (RL) as a critical component for\nenhancing reasoning. However, the substantial computational costs associated\nwith RL-based approaches have led to growing interest in alternative paradigms,\nsuch as Direct Preference Optimization (DPO). In this study, we investigate the\neffectiveness of DPO in facilitating self-improvement for LLMs through\niterative preference-based learning. We demonstrate that a single round of DPO\nwith coarse filtering significantly enhances mathematical reasoning\nperformance, particularly for strong base model. Furthermore, we design an\niterative enhancement framework for both the generator and the reward model\n(RM), enabling their mutual improvement through online interaction across\nmultiple rounds of DPO. Finally, with simple verifiable rewards, our model\nDPO-VP achieves RL-level performance with significantly lower computational\noverhead. These findings highlight DPO as a scalable and cost-effective\nalternative to RL, offering a practical solution for enhancing LLM reasoning in\nresource-constrained situations.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 06:28:25 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Tu', 'Songjun', ''], ['Lin', 'Jiahao', ''], ['Tian', 'Xiangyu', ''], ['Zhang', 'Qichao', ''], ['Li', 'Linjing', ''], ['Fu', 'Yuqian', ''], ['Xu', 'Nan', ''], ['He', 'Wei', ''], ['Lan', 'Xiangyuan', ''], ['Jiang', 'Dongmei', ''], ['Zhao', 'Dongbin', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Direct Preference Optimization (DPO)', 'label': 'Few-shot Learning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'iterative preference-based learning', 'label': 'Few-shot Learning'}, {'text': 'DPO', 'label': 'Few-shot Learning'}, {'text': 'strong base model', 'label': 'Foundation Model'}, {'text': 'DPO', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.12884,"submitter":"Kento Ueda","authors":"Kento Ueda and Atsushi Matsuo","title":"Optimizing Ansatz Design in Quantum Generative Adversarial Networks\n  Using Large Language Models","comments":"8 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present a novel approach for improving the design of ansatzes in Quantum\nGenerative Adversarial Networks (qGANs) by leveraging Large Language Models\n(LLMs). By combining the strengths of LLMs with qGANs, our approach iteratively\nrefines ansatz structures to improve accuracy while reducing circuit depth and\nthe number of parameters. This study paves the way for further exploration in\nAI-driven quantum algorithm design. The flexibility of our proposed workflow\nextends to other quantum variational algorithms, providing a general framework\nfor optimizing quantum circuits in a variety of quantum computing tasks.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 07:29:05 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Ueda', 'Kento', ''], ['Matsuo', 'Atsushi', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'qGANs', 'label': 'LLM'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.12908,"submitter":"Xinyan Jiang","authors":"Xinyan Jiang, Hang Ye, Yongxin Zhu, Xiaoying Zheng, Zikang Chen, Jun\n  Gong","title":"HICD: Hallucination-Inducing via Attention Dispersion for Contrastive\n  Decoding to Mitigate Hallucinations in Large Language Models","comments":"Under review at ARR - February 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) often generate hallucinations, producing outputs\nthat are contextually inaccurate or factually incorrect. We introduce HICD, a\nnovel method designed to induce hallucinations for contrastive decoding to\nmitigate hallucinations. Unlike existing contrastive decoding methods, HICD\nselects attention heads crucial to the model's prediction as inducing heads,\nthen induces hallucinations by dispersing attention of these inducing heads and\ncompares the hallucinated outputs with the original outputs to obtain the final\nresult. Our approach significantly improves performance on tasks requiring\ncontextual faithfulness, such as context completion, reading comprehension, and\nquestion answering. It also improves factuality in tasks requiring accurate\nknowledge recall. We demonstrate that our inducing heads selection and\nattention dispersion method leads to more \"contrast-effective\" hallucinations\nfor contrastive decoding, outperforming other hallucination-inducing methods.\nOur findings provide a promising strategy for reducing hallucinations by\ninducing hallucinations in a controlled manner, enhancing the performance of\nLLMs in a wide range of tasks.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 08:17:28 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Jiang', 'Xinyan', ''], ['Ye', 'Hang', ''], ['Zhu', 'Yongxin', ''], ['Zheng', 'Xiaoying', ''], ['Chen', 'Zikang', ''], ['Gong', 'Jun', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'HICD', 'label': 'LLM'}, {'text': 'attention heads', 'label': 'Attention mechanism'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.12931,"submitter":"Rui Pu","authors":"Rui Pu, Chaozhuo Li, Rui Ha, Litian Zhang, Lirong Qiu, Xi Zhang","title":"MirrorGuard: Adaptive Defense Against Jailbreaks via Entropy-Guided\n  Mirror Crafting","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Defending large language models (LLMs) against jailbreak attacks is crucial\nfor ensuring their safe deployment. Existing defense strategies generally rely\non predefined static criteria to differentiate between harmful and benign\nprompts. However, such rigid rules are incapable of accommodating the inherent\ncomplexity and dynamic nature of real jailbreak attacks. In this paper, we\npropose a novel concept of ``mirror'' to enable dynamic and adaptive defense. A\nmirror refers to a dynamically generated prompt that mirrors the syntactic\nstructure of the input while ensuring semantic safety. The personalized\ndiscrepancies between the input prompts and their corresponding mirrors serve\nas the guiding principles for defense. A new defense paradigm, MirrorGuard, is\nfurther proposed to detect and calibrate risky inputs based on such mirrors. An\nentropy-based detection metric, Relative Input Uncertainty (RIU), is integrated\ninto MirrorGuard to quantify the discrepancies between input prompts and\nmirrors. MirrorGuard is evaluated on several popular datasets, demonstrating\nstate-of-the-art defense performance while maintaining general effectiveness.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 08:41:29 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Pu', 'Rui', ''], ['Li', 'Chaozhuo', ''], ['Ha', 'Rui', ''], ['Zhang', 'Litian', ''], ['Qiu', 'Lirong', ''], ['Zhang', 'Xi', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'input prompts', 'label': 'Prompting'}, {'text': 'input prompts', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.12941,"submitter":"Haiyang Guo","authors":"Haiyang Guo, Fanhu Zeng, Ziwei Xiang, Fei Zhu, Da-Han Wang, Xu-Yao\n  Zhang, Cheng-Lin Liu","title":"HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of\n  Multimodal Large Language Model","comments":"Preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Instruction tuning is widely used to improve a pre-trained Multimodal Large\nLanguage Model (MLLM) by training it on curated task-specific datasets,\nenabling better comprehension of human instructions. However, it is infeasible\nto collect all possible instruction datasets simultaneously in real-world\nscenarios. Thus, enabling MLLM with continual instruction tuning is essential\nfor maintaining their adaptability. However, existing methods often trade off\nmemory efficiency for performance gains, significantly compromising overall\nefficiency. In this paper, we propose a task-specific expansion and\ntask-general fusion framework based on the variations in Centered Kernel\nAlignment (CKA) similarity across different model layers when trained on\ndiverse datasets. Furthermore, we analyze the information leakage present in\nthe existing benchmark and propose a new and more challenging benchmark to\nrationally evaluate the performance of different methods. Comprehensive\nexperiments showcase a significant performance improvement of our method\ncompared to existing state-of-the-art methods. Our code will be public\navailable.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 08:56:03 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Guo', 'Haiyang', ''], ['Zeng', 'Fanhu', ''], ['Xiang', 'Ziwei', ''], ['Zhu', 'Fei', ''], ['Wang', 'Da-Han', ''], ['Zhang', 'Xu-Yao', ''], ['Liu', 'Cheng-Lin', '']]","extracted_entities":"[{'text': 'Instruction tuning', 'label': 'Fine-tuning'}, {'text': 'Multimodal Large\\nLanguage Model', 'label': 'Large Language Model'}, {'text': 'MLLM', 'label': 'Large Language Model'}, {'text': 'MLLM', 'label': 'Large Language Model'}, {'text': 'instruction tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large\nLanguage Model","similarity_score":0.7924776673}
{"id":2503.12972,"submitter":"Junming Liu","authors":"Junming Liu, Siyuan Meng, Yanting Gao, Song Mao, Pinlong Cai, Guohang\n  Yan, Yirong Chen, Zilin Bian, Botian Shi, Ding Wang","title":"Aligning Vision to Language: Text-Free Multimodal Knowledge Graph\n  Construction for Enhanced LLMs Reasoning","comments":"14 pages, 7 figures, 6 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multimodal reasoning in Large Language Models (LLMs) struggles with\nincomplete knowledge and hallucination artifacts, challenges that textual\nKnowledge Graphs (KGs) only partially mitigate due to their modality isolation.\nWhile Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal\nunderstanding, their practical construction is impeded by semantic narrowness\nof manual text annotations and inherent noise in visual-semantic entity\nlinkages. In this paper, we propose Vision-align-to-Language integrated\nKnowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances\nLLMs reasoning through cross-modal information supplementation. Specifically,\nwe cascade pre-trained Vision-Language Models (VLMs) to align image features\nwith text, transforming them into descriptions that encapsulate image-specific\ninformation. Furthermore, we developed a cross-modal similarity verification\nmechanism to quantify semantic consistency, effectively filtering out noise\nintroduced during feature alignment. Even without manually annotated image\ncaptions, the refined descriptions alone suffice to construct the MMKG.\nCompared to conventional MMKGs construction paradigms, our approach achieves\nsubstantial storage efficiency gains while maintaining direct entity-to-image\nlinkage capability. Experimental results on multimodal reasoning tasks\ndemonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art\nmodels. Our code is published at https:\/\/github.com\/Wings-Of-Disaster\/VaLiK.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 09:31:14 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Liu', 'Junming', ''], ['Meng', 'Siyuan', ''], ['Gao', 'Yanting', ''], ['Mao', 'Song', ''], ['Cai', 'Pinlong', ''], ['Yan', 'Guohang', ''], ['Chen', 'Yirong', ''], ['Bian', 'Zilin', ''], ['Shi', 'Botian', ''], ['Wang', 'Ding', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.12989,"submitter":"Palakorn Achananuparp","authors":"Palakorn Achananuparp, Ee-Peng Lim","title":"A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation\n  Classification Using Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.SI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Automatically annotating job data with standardized occupations from\ntaxonomies, known as occupation classification, is crucial for labor market\nanalysis. However, this task is often hindered by data scarcity and the\nchallenges of manual annotations. While large language models (LLMs) hold\npromise due to their extensive world knowledge and in-context learning\ncapabilities, their effectiveness depends on their knowledge of occupational\ntaxonomies, which remains unclear. In this study, we assess the ability of LLMs\nto generate precise taxonomic entities from taxonomy, highlighting their\nlimitations. To address these challenges, we propose a multi-stage framework\nconsisting of inference, retrieval, and reranking stages, which integrates\ntaxonomy-guided reasoning examples to enhance performance by aligning outputs\nwith taxonomic knowledge. Evaluations on a large-scale dataset show significant\nimprovements in classification accuracy. Furthermore, we demonstrate the\nframework's adaptability for multi-label skill classification. Our results\nindicate that the framework outperforms existing LLM-based methods, offering a\npractical and scalable solution for occupation classification and related tasks\nacross LLMs.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 09:44:50 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Achananuparp', 'Palakorn', ''], ['Lim', 'Ee-Peng', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.13038,"submitter":"Junjie Chen","authors":"Junjie Chen, Haitao Li, Zhumin Chu, Yiqun Liu, Qingyao Ai","title":"Overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) Task","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper, we provide an overview of the NTCIR-18 Automatic Evaluation of\nLLMs (AEOLLM) task. As large language models (LLMs) grow popular in both\nacademia and industry, how to effectively evaluate the capacity of LLMs becomes\nan increasingly critical but still challenging issue. Existing methods can be\ndivided into two types: manual evaluation, which is expensive, and automatic\nevaluation, which faces many limitations including task format (the majority\nbelong to multiple-choice questions) and evaluation criteria (occupied by\nreference-based metrics). To advance the innovation of automatic evaluation, we\npropose the AEOLLM task which focuses on generative tasks and encourages\nreference-free methods. Besides, we set up diverse subtasks such as dialogue\ngeneration, text expansion, summary generation and non-factoid question\nanswering to comprehensively test different methods. This year, we received 48\nruns from 4 teams in total. This paper will describe the background of the\ntask, the data set, the evaluation measures and the evaluation results,\nrespectively.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 10:42:34 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Chen', 'Junjie', ''], ['Li', 'Haitao', ''], ['Chu', 'Zhumin', ''], ['Liu', 'Yiqun', ''], ['Ai', 'Qingyao', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'dialogue\\ngeneration', 'label': 'ChatGPT'}, {'text': 'text expansion', 'label': 'ChatGPT'}, {'text': 'summary generation', 'label': 'ChatGPT'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.13081,"submitter":"Yasod Ginige","authors":"Likai Tang, Niruth Bogahawatta, Yasod Ginige, Jiarui Xu, Shixuan Sun,\n  Surangika Ranathunga, Suranga Seneviratne","title":"A Framework to Assess Multilingual Vulnerabilities of LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) are acquiring a wider range of capabilities,\nincluding understanding and responding in multiple languages. While they\nundergo safety training to prevent them from answering illegal questions,\nimbalances in training data and human evaluation resources can make these\nmodels more susceptible to attacks in low-resource languages (LRL). This paper\nproposes a framework to automatically assess the multilingual vulnerabilities\nof commonly used LLMs. Using our framework, we evaluated six LLMs across eight\nlanguages representing varying levels of resource availability. We validated\nthe assessments generated by our automated framework through human evaluation\nin two languages, demonstrating that the framework's results align with human\njudgments in most cases. Our findings reveal vulnerabilities in LRL; however,\nthese may pose minimal risk as they often stem from the model's poor\nperformance, resulting in incoherent responses.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 11:39:44 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Tang', 'Likai', ''], ['Bogahawatta', 'Niruth', ''], ['Ginige', 'Yasod', ''], ['Xu', 'Jiarui', ''], ['Sun', 'Shixuan', ''], ['Ranathunga', 'Surangika', ''], ['Seneviratne', 'Suranga', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.13107,"submitter":"Hao Yin","authors":"Hao Yin, Guangzong Si, Zilei Wang","title":"ClearSight: Visual Signal Enhancement for Object Hallucination\n  Mitigation in Multimodal Large language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Contrastive decoding strategies are widely used to mitigate object\nhallucinations in multimodal large language models (MLLMs). By reducing\nover-reliance on language priors, these strategies ensure that generated\ncontent remains closely grounded in visual inputs, producing contextually\naccurate outputs. Since contrastive decoding requires no additional training or\nexternal tools, it offers both computational efficiency and versatility, making\nit highly attractive. However, these methods present two main limitations: (1)\nbluntly suppressing language priors can compromise coherence and accuracy of\ngenerated content, and (2) processing contrastive inputs adds computational\nload, significantly slowing inference speed. To address these challenges, we\npropose Visual Amplification Fusion (VAF), a plug-and-play technique that\nenhances attention to visual signals within the model's middle layers, where\nmodality fusion predominantly occurs. This approach enables more effective\ncapture of visual features, reducing the model's bias toward language modality.\nExperimental results demonstrate that VAF significantly reduces hallucinations\nacross various MLLMs without affecting inference speed, while maintaining\ncoherence and accuracy in generated outputs.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 12:30:40 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Yin', 'Hao', ''], ['Si', 'Guangzong', ''], ['Wang', 'Zilei', '']]","extracted_entities":"[{'text': 'multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'Visual Amplification Fusion', 'label': 'contextual Embedding'}, {'text': 'attention', 'label': 'Attention mechanism'}, {'text': 'modality fusion', 'label': 'contextual Embedding'}, {'text': 'VAF', 'label': 'contextual Embedding'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"multimodal large language models","similarity_score":0.7649828196}
{"id":2503.13108,"submitter":"Hao Yin","authors":"Hao Yin, Guangzong Si, Zilei Wang","title":"Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways\n  to Faster Inference","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multimodal large language models (MLLMs) improve performance on\nvision-language tasks by integrating visual features from pre-trained vision\nencoders into large language models (LLMs). However, how MLLMs process and\nutilize visual information remains unclear. In this paper, a shift in the\ndominant flow of visual information is uncovered: (1) in shallow layers, strong\ninteractions are observed between image tokens and instruction tokens, where\nmost visual information is injected into instruction tokens to form cross-modal\nsemantic representations; (2) in deeper layers, image tokens primarily interact\nwith each other, aggregating the remaining visual information to optimize\nsemantic representations within visual modality. Based on these insights, we\npropose Hierarchical Modality-Aware Pruning (HiMAP), a plug-and-play inference\nacceleration method that dynamically prunes image tokens at specific layers,\nreducing computational costs by approximately 65% without sacrificing\nperformance. Our findings offer a new understanding of visual information\nprocessing in MLLMs and provide a state-of-the-art solution for efficient\ninference.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 12:31:23 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Yin', 'Hao', ''], ['Si', 'Guangzong', ''], ['Wang', 'Zilei', '']]","extracted_entities":"[{'text': 'Multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'LLM'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal large language models","similarity_score":0.7649828196}
{"id":2503.13111,"submitter":"Erik Daxberger","authors":"Erik Daxberger, Nina Wenzel, David Griffiths, Haiming Gang, Justin\n  Lazarow, Gefen Kohavi, Kai Kang, Marcin Eichner, Yinfei Yang, Afshin Dehghan,\n  Peter Grasch","title":"MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multimodal large language models (MLLMs) excel at 2D visual understanding but\nremain limited in their ability to reason about 3D space. In this work, we\nleverage large-scale high-quality 3D scene data with open-set annotations to\nintroduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation\nbenchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data\ncovers diverse spatial tasks including spatial relationship prediction, metric\nsize and distance estimation, and 3D grounding. We show that CA-VQA enables us\nto train MM-Spatial, a strong generalist MLLM that also achieves\nstate-of-the-art performance on 3D spatial understanding benchmarks, including\nour own. We show how incorporating metric depth and multi-view inputs (provided\nin CA-VQA) can further improve 3D understanding, and demonstrate that data\nalone allows our model to achieve depth perception capabilities comparable to\ndedicated monocular depth estimation models. We will publish our SFT dataset\nand benchmark.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 12:34:22 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Daxberger', 'Erik', ''], ['Wenzel', 'Nina', ''], ['Griffiths', 'David', ''], ['Gang', 'Haiming', ''], ['Lazarow', 'Justin', ''], ['Kohavi', 'Gefen', ''], ['Kang', 'Kai', ''], ['Eichner', 'Marcin', ''], ['Yang', 'Yinfei', ''], ['Dehghan', 'Afshin', ''], ['Grasch', 'Peter', '']]","extracted_entities":"[{'text': 'Multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal large language models","similarity_score":0.7649828196}
{"id":2503.13116,"submitter":"Zeng Wang","authors":"Zeng Wang, Minghao Shao, Mohammed Nabeel, Prithwish Basu Roy, Likhitha\n  Mankali, Jitendra Bhandari, Ramesh Karri, Ozgur Sinanoglu, Muhammad Shafique,\n  Johann Knechtel","title":"VeriLeaky: Navigating IP Protection vs Utility in Fine-Tuning for\n  LLM-Driven Verilog Coding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AR cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models (LLMs) offer significant potential for coding, yet\nfine-tuning (FT) with curated data is essential for niche languages like\nVerilog. Using proprietary intellectual property (IP) for FT presents a serious\nrisk, as FT data can be leaked through LLM inference. This leads to a critical\ndilemma for design houses: seeking to build externally accessible LLMs offering\ncompetitive Verilog coding, how can they leverage in-house IP to enhance FT\nutility while ensuring IP protection?\n  For the first time in the literature, we study this dilemma. Using LLaMA\n3.1-8B, we conduct in-house FT on a baseline Verilog dataset (RTLCoder)\nsupplemented with our own in-house IP, which is validated through multiple\ntape-outs. To rigorously assess IP leakage, we quantify structural similarity\n(AST\/Dolos) and functional equivalence (Synopsys Formality) between generated\ncodes and our in-house IP. We show that our IP can indeed be leaked, confirming\nthe threat. As defense, we evaluate logic locking of Verilog codes (ASSURE).\nThis offers some level of protection, yet reduces the IP's utility for FT and\ndegrades the LLM's performance. Our study shows the need for novel strategies\nthat are both effective and minimally disruptive to FT, an essential effort for\nenabling design houses to fully utilize their proprietary IP toward LLM-driven\nVerilog coding.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 12:38:03 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wang', 'Zeng', ''], ['Shao', 'Minghao', ''], ['Nabeel', 'Mohammed', ''], ['Roy', 'Prithwish Basu', ''], ['Mankali', 'Likhitha', ''], ['Bhandari', 'Jitendra', ''], ['Karri', 'Ramesh', ''], ['Sinanoglu', 'Ozgur', ''], ['Shafique', 'Muhammad', ''], ['Knechtel', 'Johann', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.1325,"submitter":"Zejia Zhang","authors":"Zejia Zhang, Bo Yang, Xinxing Chen, Weizhuang Shi, Haoyuan Wang, Wei\n  Luo and Jian Huang","title":"MindEye-OmniAssist: A Gaze-Driven LLM-Enhanced Assistive Robot System\n  for Implicit Intention Recognition and Task Execution","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.HC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  A promising effective human-robot interaction in assistive robotic systems is\ngaze-based control. However, current gaze-based assistive systems mainly help\nusers with basic grasping actions, offering limited support. Moreover, the\nrestricted intent recognition capability constrains the assistive system's\nability to provide diverse assistance functions. In this paper, we propose an\nopen implicit intention recognition framework powered by Large Language Model\n(LLM) and Vision Foundation Model (VFM), which can process gaze input and\nrecognize user intents that are not confined to predefined or specific\nscenarios. Furthermore, we implement a gaze-driven LLM-enhanced assistive robot\nsystem (MindEye-OmniAssist) that recognizes user's intentions through gaze and\nassists in completing task. To achieve this, the system utilizes open\nvocabulary object detector, intention recognition network and LLM to infer\ntheir full intentions. By integrating eye movement feedback and LLM, it\ngenerates action sequences to assist the user in completing tasks. Real-world\nexperiments have been conducted for assistive tasks, and the system achieved an\noverall success rate of 41\/55 across various undefined tasks. Preliminary\nresults show that the proposed method holds the potential to provide a more\nuser-friendly human-computer interaction interface and significantly enhance\nthe versatility and effectiveness of assistive systems by supporting more\ncomplex and diverse task.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:06:14 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Zhang', 'Zejia', ''], ['Yang', 'Bo', ''], ['Chen', 'Xinxing', ''], ['Shi', 'Weizhuang', ''], ['Wang', 'Haoyuan', ''], ['Luo', 'Wei', ''], ['Huang', 'Jian', '']]","extracted_entities":"[{'text': 'Large Language Model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'Vision Foundation Model', 'label': 'Foundation Model'}, {'text': 'LLM', 'label': 'LLM'}, {'text': 'LLM', 'label': 'LLM'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Model","similarity_score":1.0}
{"id":2503.13299,"submitter":"Yijun Liu","authors":"Yijun Liu, Jinzheng Yu, Yang Xu, Zhongyang Li, Qingfu Zhu","title":"A Survey on Transformer Context Extension: Approaches and Evaluation","comments":"preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Large language models (LLMs) based on Transformer have been widely applied in\nthe filed of natural language processing (NLP), demonstrating strong\nperformance, particularly in handling short text tasks. However, when it comes\nto long context scenarios, the performance of LLMs degrades due to some\nchallenges. To alleviate this phenomenon, there is a number of work proposed\nrecently. In this survey, we first list the challenges of applying pre-trained\nLLMs to process long contexts. Then systematically review the approaches\nrelated to long context and propose our taxonomy categorizing them into four\nmain types: positional encoding, context compression, retrieval augmented, and\nattention pattern. In addition to the approaches, we focus on the evaluation of\nlong context, organizing relevant data, tasks, and metrics based on existing\nlong context benchmarks. Finally, we summarize unresolved issues in the long\ncontext domain and put forward our views on future developments.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:44:09 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Liu', 'Yijun', ''], ['Yu', 'Jinzheng', ''], ['Xu', 'Yang', ''], ['Li', 'Zhongyang', ''], ['Zhu', 'Qingfu', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'positional encoding', 'label': 'contextual Embedding'}, {'text': 'context compression', 'label': 'contextual Embedding'}, {'text': 'retrieval augmented', 'label': 'contextual Embedding'}, {'text': 'attention pattern', 'label': 'Attention mechanism'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.13305,"submitter":"Chi Han","authors":"Chi Han, Heng Ji","title":"Computation Mechanism Behind LLM Position Generalization","comments":"8 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Most written natural languages are composed of sequences of words and\nsentences. Similar to humans, large language models (LLMs) exhibit flexibility\nin handling textual positions - a phenomenon we term position generalization.\nThey can understand texts with position perturbations and generalize to longer\ntexts than those encountered during training with the latest techniques. These\nphenomena suggest that LLMs handle positions tolerantly, but how LLMs\ncomputationally process positional relevance remains largely unexplored. This\nwork connects the linguistic phenomenon with LLMs' computational mechanisms. We\nshow how LLMs enforce certain computational mechanisms for the aforementioned\ntolerance in position perturbations. Despite the complex design of the\nself-attention mechanism, this work reveals that LLMs learn a counterintuitive\ndisentanglement of attention logits. Their values show a 0.959 linear\ncorrelation with an approximation of the arithmetic sum of positional relevance\nand semantic importance. Furthermore, we identify a prevalent pattern in\nintermediate features, which we prove theoretically enables this effect. The\npattern, which is different from how randomly initialized parameters would\nbehave, suggests that it is a learned behavior rather than a natural result of\nthe model architecture. Based on these findings, we provide computational\nexplanations and criteria for LLMs' position flexibilities. This work takes a\npioneering step in linking position generalization with modern LLMs' internal\nmechanisms.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:47:37 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Han', 'Chi', ''], ['Ji', 'Heng', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'self-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.1334,"submitter":"Xinyu Jessica Wang","authors":"Xinyu Jessica Wang, Christine Lee, Bilge Mutlu","title":"LearnMate: Enhancing Online Education with LLM-Powered Personalized\n  Learning Plans and Support","comments":"In Extended Abstracts of the CHI Conference on Human Factors in\n  Computing Systems (CHI EA '25), April 26-May 1, 2025, Yokohama, Japan","journal-ref":null,"doi":"10.1145\/3706599.3719857","report-no":null,"categories":"cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  With the increasing prevalence of online learning, adapting education to\ndiverse learner needs remains a persistent challenge. Recent advancements in\nartificial intelligence (AI), particularly large language models (LLMs),\npromise powerful tools and capabilities to enhance personalized learning in\nonline educational environments. In this work, we explore how LLMs can improve\npersonalized learning experiences by catering to individual user needs toward\nenhancing the overall quality of online education. We designed personalization\nguidelines based on the growing literature on personalized learning to ground\nLLMs in generating tailored learning plans. To operationalize these guidelines,\nwe implemented LearnMate, an LLM-based system that generates personalized\nlearning plans and provides users with real-time learning support. We discuss\nthe implications and future directions of this work, aiming to move beyond the\ntraditional one-size-fits-all approach by integrating LLM-based personalized\nsupport into online learning environments.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 16:18:23 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wang', 'Xinyu Jessica', ''], ['Lee', 'Christine', ''], ['Mutlu', 'Bilge', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LearnMate', 'label': 'LLM-based'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.13342,"submitter":"Ying Jiao","authors":"Ying Jiao, Luc De Raedt, and Giuseppe Marra","title":"Valid Text-to-SQL Generation with Unification-based DeepStochLog","comments":null,"journal-ref":"In International Conference on Neural-Symbolic Learning and\n  Reasoning (pp. 312-330). Cham: Springer Nature Switzerland (2024)","doi":"10.1007\/978-3-031-71167-1_17","report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models have been used to translate natural language questions\nto SQL queries. Without hard constraints on syntax and database schema, they\noccasionally produce invalid queries that are not executable. These failures\nlimit the usage of these systems in real-life scenarios. We propose a\nneurosymbolic framework that imposes SQL syntax and schema constraints with\nunification-based definite clause grammars and thus guarantees the generation\nof valid queries. Our framework also builds a bi-directional interface to\nlanguage models to leverage their natural language understanding abilities. The\nevaluation results on a subset of SQL grammars show that all our output queries\nare valid. This work is the first step towards extending language models with\nunification-based grammars. We demonstrate this extension enhances the\nvalidity, execution accuracy, and ground truth alignment of the underlying\nlanguage model by a large margin. Our code is available at\nhttps:\/\/github.com\/ML-KULeuven\/deepstochlog-lm.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 16:21:10 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Jiao', 'Ying', ''], ['De Raedt', 'Luc', ''], ['Marra', 'Giuseppe', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'unification-based definite clause grammars', 'label': 'Embedding'}, {'text': 'unification-based grammars', 'label': 'Embedding'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.13401,"submitter":"Alexander Ku","authors":"Alexander Ku, Declan Campbell, Xuechunzi Bai, Jiayi Geng, Ryan Liu,\n  Raja Marjieh, R. Thomas McCoy, Andrew Nam, Ilia Sucholutsky, Veniamin\n  Veselovsky, Liyi Zhang, Jian-Qiao Zhu, Thomas L. Griffiths","title":"Using the Tools of Cognitive Science to Understand Large Language Models\n  at Different Levels of Analysis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Modern artificial intelligence systems, such as large language models, are\nincreasingly powerful but also increasingly hard to understand. Recognizing\nthis problem as analogous to the historical difficulties in understanding the\nhuman mind, we argue that methods developed in cognitive science can be useful\nfor understanding large language models. We propose a framework for applying\nthese methods based on Marr's three levels of analysis. By revisiting\nestablished cognitive science techniques relevant to each level and\nillustrating their potential to yield insights into the behavior and internal\norganization of large language models, we aim to provide a toolkit for making\nsense of these new kinds of minds.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:33:54 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Ku', 'Alexander', ''], ['Campbell', 'Declan', ''], ['Bai', 'Xuechunzi', ''], ['Geng', 'Jiayi', ''], ['Liu', 'Ryan', ''], ['Marjieh', 'Raja', ''], ['McCoy', 'R. Thomas', ''], ['Nam', 'Andrew', ''], ['Sucholutsky', 'Ilia', ''], ['Veselovsky', 'Veniamin', ''], ['Zhang', 'Liyi', ''], ['Zhu', 'Jian-Qiao', ''], ['Griffiths', 'Thomas L.', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.13413,"submitter":"Dengyun Peng","authors":"Dengyun Peng, Yuhang Zhou, Qiguang Chen, Jinhao Liu, Jingjing Chen,\n  Libo Qin","title":"DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization\n  Framework from a Deep-Learning Perspective","comments":"Preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, largely driven by well-designed prompts. However, crafting and selecting\nsuch prompts often requires considerable human effort, significantly limiting\nits scalability. To mitigate this, recent studies have explored automated\nprompt optimization as a promising solution. Despite these efforts, existing\nmethods still face critical challenges in robustness, efficiency, and\ngeneralization. To systematically address these challenges, we first conduct an\nempirical analysis to identify the limitations of current reflection-based\nprompt optimization paradigm. Building on these insights, we propose 7\ninnovative approaches inspired by traditional deep learning paradigms for\nprompt optimization (DLPO), seamlessly integrating these concepts into\ntext-based gradient optimization. Through these advancements, we progressively\ntackle the aforementioned challenges and validate our methods through extensive\nexperimentation. We hope our study not only provides valuable guidance for\nfuture research but also offers a comprehensive understanding of the challenges\nand potential solutions in prompt optimization. Our code is available at\nhttps:\/\/github.com\/sfasfaffa\/DLPO.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:42:51 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 04:41:37 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 14:18:01 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Peng', 'Dengyun', ''], ['Zhou', 'Yuhang', ''], ['Chen', 'Qiguang', ''], ['Liu', 'Jinhao', ''], ['Chen', 'Jingjing', ''], ['Qin', 'Libo', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'prompts', 'label': 'Prompting'}, {'text': 'scalability', 'label': 'Scaling law'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.13444,"submitter":"Ye Liu","authors":"Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, Mike Zheng Shou","title":"VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning","comments":"Project Page: https:\/\/videomind.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Videos, with their unique temporal dimension, demand precise grounded\nunderstanding, where answers are directly linked to visual, interpretable\nevidence. Despite significant breakthroughs in reasoning capabilities within\nLarge Language Models, multi-modal reasoning - especially for videos - remains\nunexplored. In this work, we introduce VideoMind, a novel video-language agent\ndesigned for temporal-grounded video understanding. VideoMind incorporates two\nkey innovations: (i) We identify essential capabilities for video temporal\nreasoning and develop a role-based agentic workflow, including a planner for\ncoordinating different roles, a grounder for temporal localization, a verifier\nto assess temporal interval accuracy, and an answerer for question-answering.\n(ii) To efficiently integrate these diverse roles, we propose a novel\nChain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA\nadaptors while avoiding the overhead of multiple models, thus balancing\nefficiency and flexibility. Extensive experiments on 14 public benchmarks\ndemonstrate that our agent achieves state-of-the-art performance on diverse\nvideo understanding tasks, including 3 on grounded video question-answering, 6\non video temporal grounding, and 5 on general video question-answering,\nunderscoring its effectiveness in advancing video agent and long-form temporal\nreasoning.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:59:33 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Liu', 'Ye', ''], ['Lin', 'Kevin Qinghong', ''], ['Chen', 'Chang Wen', ''], ['Shou', 'Mike Zheng', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.13575,"submitter":"Kai Tong","authors":"Kai Tong, Kang Pan, Xiao Zhang, Erli Meng, Run He, Yawen Cui, Nuoyan\n  Guo, Huiping Zhuang","title":"Analytic Subspace Routing: How Recursive Least Squares Works in\n  Continual Learning of Large Language Model","comments":"11 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) possess encompassing capabilities that can\nprocess diverse language-related tasks. However, finetuning on LLMs will\ndiminish this general skills and continual finetuning will further cause severe\ndegradation on accumulated knowledge. Recently, Continual Learning (CL) in\nLarge Language Models (LLMs) arises which aims to continually adapt the LLMs to\nnew tasks while maintaining previously learned knowledge and inheriting general\nskills. Existing techniques either leverage previous data to replay, leading to\nextra computational costs, or utilize a single parameter-efficient module to\nlearn the downstream task, constraining new knowledge absorption with\ninterference between different tasks. Toward these issues, this paper proposes\nAnalytic Subspace Routing(ASR) to address these challenges. For each task, we\nisolate the learning within a subspace of deep layers' features via low-rank\nadaptation, eliminating knowledge interference between different tasks.\nAdditionally, we propose an analytic routing mechanism to properly utilize\nknowledge learned in different subspaces. Our approach employs Recursive Least\nSquares to train a multi-task router model, allowing the router to dynamically\nadapt to incoming data without requiring access to historical data. Also, the\nrouter effectively assigns the current task to an appropriate subspace and has\na non-forgetting property of previously learned tasks with a solid theoretical\nguarantee. Experimental results demonstrate that our method achieves\nnear-perfect retention of prior knowledge while seamlessly integrating new\ninformation, effectively overcoming the core limitations of existing methods.\nOur code will be released after acceptance.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 13:40:46 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Tong', 'Kai', ''], ['Pan', 'Kang', ''], ['Zhang', 'Xiao', ''], ['Meng', 'Erli', ''], ['He', 'Run', ''], ['Cui', 'Yawen', ''], ['Guo', 'Nuoyan', ''], ['Zhuang', 'Huiping', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'finetuning', 'label': 'Fine-tuning'}, {'text': 'continual finetuning', 'label': 'Fine-tuning'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.1358,"submitter":"Sijia Gu","authors":"Sijia Gu, Noor Nashid, Ali Mesbah","title":"LLM Test Generation via Iterative Hybrid Program Analysis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Automating unit test generation remains a significant challenge, particularly\nfor complex methods in real-world projects. While Large Language Models (LLMs)\nhave made strides in code generation, they struggle to achieve high branch\ncoverage due to their limited ability to reason about intricate control flow\nstructures. To address this limitation, we introduce Panta, a technique that\nemulates the iterative process human developers follow when analyzing code and\nconstructing test cases. Panta integrates static control flow analysis and\ndynamic code coverage analysis to systematically guide LLMs in identifying\nuncovered execution paths and generating better test cases. By incorporating an\niterative feedback-driven mechanism, our technique continuously refines test\ngeneration based on static and dynamic path coverage insights, ensuring more\ncomprehensive and effective testing. Our empirical evaluation, conducted on\nclasses with high cyclomatic complexity from open-source projects, demonstrates\nthat Panta achieves 26% higher line coverage and 23% higher branch coverage\ncompared to the state-of-the-art.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 16:10:38 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Gu', 'Sijia', ''], ['Nashid', 'Noor', ''], ['Mesbah', 'Ali', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'open-source projects', 'label': 'Open-source LLMs'}, {'text': 'Panta', 'label': 'LLM'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.13646,"submitter":"Chiara Plizzari","authors":"Chiara Plizzari, Alessio Tonioni, Yongqin Xian, Achin Kulshrestha,\n  Federico Tombari","title":"Omnia de EgoTempo: Benchmarking Temporal Understanding of Multi-Modal\n  LLMs in Egocentric Videos","comments":"Accepted to CVPR 2025. Dataset and code are available at\n  https:\/\/github.com\/google-research-datasets\/egotempo.git","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Understanding fine-grained temporal dynamics is crucial in egocentric videos,\nwhere continuous streams capture frequent, close-up interactions with objects.\nIn this work, we bring to light that current egocentric video\nquestion-answering datasets often include questions that can be answered using\nonly few frames or commonsense reasoning, without being necessarily grounded in\nthe actual video. Our analysis shows that state-of-the-art Multi-Modal Large\nLanguage Models (MLLMs) on these benchmarks achieve remarkably high performance\nusing just text or a single frame as input. To address these limitations, we\nintroduce EgoTempo, a dataset specifically designed to evaluate temporal\nunderstanding in the egocentric domain. EgoTempo emphasizes tasks that require\nintegrating information across the entire video, ensuring that models would\nneed to rely on temporal patterns rather than static cues or pre-existing\nknowledge. Extensive experiments on EgoTempo show that current MLLMs still fall\nshort in temporal reasoning on egocentric videos, and thus we hope EgoTempo\nwill catalyze new research in the field and inspire models that better capture\nthe complexity of temporal dynamics. Dataset and code are available at\nhttps:\/\/github.com\/google-research-datasets\/egotempo.git.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 18:50:36 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Plizzari', 'Chiara', ''], ['Tonioni', 'Alessio', ''], ['Xian', 'Yongqin', ''], ['Kulshrestha', 'Achin', ''], ['Tombari', 'Federico', '']]","extracted_entities":"[{'text': 'Multi-Modal Large\\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multi-Modal Large\nLanguage Models","similarity_score":0.7925285697}
{"id":2503.1366,"submitter":"Qian Meng","authors":"Qian Meng, Jin Peng Zhou, Kilian Q. Weinberger, and Hadas Kress-Gazit","title":"INPROVF: Leveraging Large Language Models to Repair High-level Robot\n  Controllers from Assumption Violations","comments":"To appear in ICLR 2025 Workshop: VerifAI: AI Verification in the\n  Wild; in submission to 2025 IEEE 21th International Conference on Automation\n  Science and Engineering (CASE), Los Angeles, CA, USA: IEEE, Aug. 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI cs.FL cs.SY eess.SY","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper presents INPROVF, an automatic framework that combines large\nlanguage models (LLMs) and formal methods to speed up the repair process of\nhigh-level robot controllers. Previous approaches based solely on formal\nmethods are computationally expensive and cannot scale to large state spaces.\nIn contrast, INPROVF uses LLMs to generate repair candidates, and formal\nmethods to verify their correctness. To improve the quality of these\ncandidates, our framework first translates the symbolic representations of the\nenvironment and controllers into natural language descriptions. If a candidate\nfails the verification, INPROVF provides feedback on potential unsafe behaviors\nor unsatisfied tasks, and iteratively prompts LLMs to generate improved\nsolutions. We demonstrate the effectiveness of INPROVF through 12 violations\nwith various workspaces, tasks, and state space sizes.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 19:08:36 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Meng', 'Qian', ''], ['Zhou', 'Jin Peng', ''], ['Weinberger', 'Kilian Q.', ''], ['Kress-Gazit', 'Hadas', '']]","extracted_entities":"[{'text': 'large\\nlanguage models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompts', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"large\nlanguage models","similarity_score":0.9664971828}
{"id":2503.13733,"submitter":"Dilshod Azizov","authors":"Daniil Orel, Dilshod Azizov, Preslav Nakov","title":"CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual,\n  Multi-Generator and Multi-Domain Settings","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large language models (LLMs) have revolutionized code generation, automating\nprogramming with remarkable efficiency. However, these advancements challenge\nprogramming skills, ethics, and assessment integrity, making the detection of\nLLM-generated code essential for maintaining accountability and standards.\nWhile, there has been some research on this problem, it generally lacks domain\ncoverage and robustness, and only covers a small number of programming\nlanguages. To this end, we propose a framework capable of distinguishing\nbetween human- and LLM-written code across multiple programming languages, code\ngenerators, and domains. We use a large-scale dataset from renowned platforms\nand LLM-based code generators, alongside applying rigorous data quality checks,\nfeature engineering, and comparative analysis using evaluation of traditional\nmachine learning models, pre-trained language models (PLMs), and LLMs for code\ndetection. We perform an evaluation on out-of-domain scenarios, such as\ndetecting the authorship and hybrid authorship of generated code and\ngeneralizing to unseen models, domains, and programming languages. Moreover,\nour extensive experiments show that our framework effectively distinguishes\nhuman- from LLM-written code and sets a new benchmark for this task.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 21:41:37 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Orel', 'Daniil', ''], ['Azizov', 'Dilshod', ''], ['Nakov', 'Preslav', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ethics', 'label': 'AI Ethics'}, {'text': 'renowned platforms', 'label': 'Open-source LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.13773,"submitter":"Tanmoy Sen","authors":"Haiying Shen, Tanmoy Sen","title":"Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 23:38:29 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Shen', 'Haiying', ''], ['Sen', 'Tanmoy', '']]","extracted_entities":"[{'text': 'Large Language Model', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Model","similarity_score":1.0}
{"id":2503.13792,"submitter":"Xinyu Tian","authors":"Xinyu Tian, Shu Zou, Zhaoyuan Yang, Jing Zhang","title":"Identifying and Mitigating Position Bias of Multi-image Vision-Language\n  Models","comments":"Accepted to CVPR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The evolution of Large Vision-Language Models (LVLMs) has progressed from\nsingle to multi-image reasoning. Despite this advancement, our findings\nindicate that LVLMs struggle to robustly utilize information across multiple\nimages, with predictions significantly affected by the alteration of image\npositions. To further explore this issue, we introduce Position-wise Question\nAnswering (PQA), a meticulously designed task to quantify reasoning\ncapabilities at each position. Our analysis reveals a pronounced position bias\nin LVLMs: open-source models excel in reasoning with images positioned later\nbut underperform with those in the middle or at the beginning, while\nproprietary models show improved comprehension for images at the beginning and\nend but struggle with those in the middle. Motivated by this, we propose SoFt\nAttention (SoFA), a simple, training-free approach that mitigates this bias by\nemploying linear interpolation between inter-image causal attention and\nbidirectional counterparts. Experimental results demonstrate that SoFA reduces\nposition bias and enhances the reasoning performance of existing LVLMs.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 00:45:02 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Tian', 'Xinyu', ''], ['Zou', 'Shu', ''], ['Yang', 'Zhaoyuan', ''], ['Zhang', 'Jing', '']]","extracted_entities":"[{'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'SoFt\\nAttention', 'label': 'Attention mechanism'}, {'text': 'inter-image causal attention', 'label': 'Attention mechanism'}, {'text': 'LVLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Vision-Language Models","similarity_score":0.7742220759}
{"id":2503.13793,"submitter":"Dipin Khati","authors":"Dipin Khati and Yijin Liu and David N. Palacio and Yixuan Zhang and\n  Denys Poshyvanyk","title":"Mapping the Trust Terrain: LLMs in Software Engineering -- Insights and\n  Perspectives","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Applications of Large Language Models (LLMs) are rapidly growing in industry\nand academia for various software engineering (SE) tasks. As these models\nbecome more integral to critical processes, ensuring their reliability and\ntrustworthiness becomes essential. Consequently, the concept of trust in these\nsystems is becoming increasingly critical. Well-calibrated trust is important,\nas excessive trust can lead to security vulnerabilities, and risks, while\ninsufficient trust can hinder innovation. However, the landscape of\ntrust-related concepts in LLMs in SE is relatively unclear, with concepts such\nas trust, distrust, and trustworthiness lacking clear conceptualizations in the\nSE community. To bring clarity to the current research status and identify\nopportunities for future work, we conducted a comprehensive review of $88$\npapers: a systematic literature review of $18$ papers focused on LLMs in SE,\ncomplemented by an analysis of 70 papers from broader trust literature.\nAdditionally, we conducted a survey study with 25 domain experts to gain\ninsights into practitioners' understanding of trust and identify gaps between\nexisting literature and developers' perceptions. The result of our analysis\nserves as a roadmap that covers trust-related concepts in LLMs in SE and\nhighlights areas for future exploration.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 00:49:43 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Khati', 'Dipin', ''], ['Liu', 'Yijin', ''], ['Palacio', 'David N.', ''], ['Zhang', 'Yixuan', ''], ['Poshyvanyk', 'Denys', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.13794,"submitter":"Yang Zhou","authors":"Yang Zhou, Shiyu Zhao, Yuxiao Chen, Zhenting Wang, Dimitris N. Metaxas","title":"LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large foundation models trained on large-scale visual-text data can\nsignificantly enhance Open Vocabulary Object Detection (OVD) through data\ngeneration. However, this may lead to biased synthetic data and overfitting to\nspecific configurations. It can sidestep biases of manually curated data\ngeneration by directly leveraging hidden states of Large Language Models\n(LLMs), which is surprisingly rarely explored. This paper presents a systematic\nmethod to enhance visual grounding by utilizing decoder layers of the LLM of a\nMLLM. We introduce a zero-initialized cross-attention adapter to enable\nefficient knowledge transfer from LLMs to object detectors, an new approach\ncalled LED (LLM Enhanced Open-Vocabulary Object Detection). We demonstrate that\nintermediate hidden states from early LLM layers retain strong spatial-semantic\ncorrelations that are beneficial to grounding tasks. Experiments show that our\nadaptation strategy significantly enhances the performance on complex free-form\ntext queries while remaining the same on plain categories. With our adaptation,\nQwen2-0.5B with Swin-T as the vision encoder improves GroundingDINO by 2.33% on\nOmnilabel, at the overhead of 8.7% more GFLOPs. Qwen2-0.5B with a larger vision\nencoder can further boost the performance by 6.22%. We further validate our\ndesign by ablating on varied adapter architectures, sizes of LLMs, and which\nlayers to add adaptation.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 00:50:40 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Zhou', 'Yang', ''], ['Zhao', 'Shiyu', ''], ['Chen', 'Yuxiao', ''], ['Wang', 'Zhenting', ''], ['Metaxas', 'Dimitris N.', '']]","extracted_entities":"[{'text': 'Large foundation models', 'label': 'Foundation Model'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.13804,"submitter":"Kai Guo","authors":"Kai Guo, Harry Shomer, Shenglai Zeng, Haoyu Han, Yu Wang, Jiliang Tang","title":"Empowering GraphRAG with Knowledge Filtering and Integration","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In recent years, large language models (LLMs) have revolutionized the field\nof natural language processing. However, they often suffer from knowledge gaps\nand hallucinations. Graph retrieval-augmented generation (GraphRAG) enhances\nLLM reasoning by integrating structured knowledge from external graphs.\nHowever, we identify two key challenges that plague GraphRAG:(1) Retrieving\nnoisy and irrelevant information can degrade performance and (2)Excessive\nreliance on external knowledge suppresses the model's intrinsic reasoning. To\naddress these issues, we propose GraphRAG-FI (Filtering and Integration),\nconsisting of GraphRAG-Filtering and GraphRAG-Integration. GraphRAG-Filtering\nemploys a two-stage filtering mechanism to refine retrieved information.\nGraphRAG-Integration employs a logits-based selection strategy to balance\nexternal knowledge from GraphRAG with the LLM's intrinsic reasoning,reducing\nover-reliance on retrievals. Experiments on knowledge graph QA tasks\ndemonstrate that GraphRAG-FI significantly improves reasoning performance\nacross multiple backbone models, establishing a more reliable and effective\nGraphRAG framework.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 01:29:55 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Guo', 'Kai', ''], ['Shomer', 'Harry', ''], ['Zeng', 'Shenglai', ''], ['Han', 'Haoyu', ''], ['Wang', 'Yu', ''], ['Tang', 'Jiliang', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GraphRAG', 'label': 'RAG'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'GraphRAG', 'label': 'RAG'}, {'text': 'GraphRAG-FI', 'label': 'RAG'}, {'text': 'GraphRAG', 'label': 'RAG'}, {'text': 'GraphRAG', 'label': 'RAG'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.13856,"submitter":"Kai Chen Nj","authors":"Kai Chen, Xinfeng Li, Tianpei Yang, Hewei Wang, Wei Dong, Yang Gao","title":"MDTeamGPT: A Self-Evolving LLM-based Multi-Agent Framework for\n  Multi-Disciplinary Team Medical Consultation","comments":"24 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) have made significant progress in various\nfields. However, challenges remain in Multi-Disciplinary Team (MDT) medical\nconsultations. Current research enhances reasoning through role assignment,\ntask decomposition, and accumulation of medical experience. Multi-role\ncollaboration in MDT consultations often results in excessively long dialogue\nhistories. This increases the model's cognitive burden and degrades both\nefficiency and accuracy. Some methods only store treatment histories. They do\nnot extract effective experience or reflect on errors. This limits knowledge\ngeneralization and system evolution. We propose a multi-agent MDT medical\nconsultation framework based on LLMs to address these issues. Our framework\nuses consensus aggregation and a residual discussion structure for multi-round\nconsultations. It also employs a Correct Answer Knowledge Base (CorrectKB) and\na Chain-of-Thought Knowledge Base (ChainKB) to accumulate consultation\nexperience. These mechanisms enable the framework to evolve and continually\nimprove diagnosis rationality and accuracy. Experimental results on the MedQA\nand PubMedQA datasets demonstrate that our framework achieves accuracies of\n90.1% and 83.9%, respectively, and that the constructed knowledge bases\ngeneralize effectively across test sets from both datasets.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 03:07:34 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Chen', 'Kai', ''], ['Li', 'Xinfeng', ''], ['Yang', 'Tianpei', ''], ['Wang', 'Hewei', ''], ['Dong', 'Wei', ''], ['Gao', 'Yang', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ChainKB', 'label': 'Chain of thought'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.13879,"submitter":"Wei Chen","authors":"Wei Chen, Han Ding, Meng Yuan, Zhao Zhang, Deqing Wang, Fuzhen Zhuang","title":"Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review\n  Generation via Cognitive Alignment","comments":"23 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The rapid growth of scholarly submissions has overwhelmed traditional peer\nreview systems, driving the need for intelligent automation to preserve\nscientific rigor. While large language models (LLMs) show promise in automating\nmanuscript critiques, their ability to synthesize high-stakes meta-reviews,\nwhich require conflict-aware reasoning and consensus derivation, remains\nunderdeveloped. Existing methods fail to effectively handle conflicting\nviewpoints within differing opinions, and often introduce additional cognitive\nbiases, such as anchoring effects and conformity bias.To overcome these\nlimitations, we propose the Cognitive Alignment Framework (CAF), a dual-process\narchitecture that transforms LLMs into adaptive scientific arbitrators. By\noperationalizing Kahneman's dual-process theory, CAF introduces a three-step\ncognitive pipeline: review initialization, incremental integration, and\ncognitive alignment.Empirical validation shows that CAF outperforms existing\nLLM-based methods, with sentiment consistency gains reaching up to 19.47\\% and\ncontent consistency improving by as much as 12.95\\%.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 04:13:11 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Chen', 'Wei', ''], ['Ding', 'Han', ''], ['Yuan', 'Meng', ''], ['Zhang', 'Zhao', ''], ['Wang', 'Deqing', ''], ['Zhuang', 'Fuzhen', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'anchoring effects', 'label': 'Model Bias and Fairness'}, {'text': 'conformity bias', 'label': 'Model Bias and Fairness'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.13962,"submitter":"Chengze Jiang","authors":"Chengze Jiang, Zhuangzhuang Wang, Minjing Dong, Jie Gui","title":"Survey of Adversarial Robustness in Multimodal Large Language Models","comments":"9 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multimodal Large Language Models (MLLMs) have demonstrated exceptional\nperformance in artificial intelligence by facilitating integrated understanding\nacross diverse modalities, including text, images, video, audio, and speech.\nHowever, their deployment in real-world applications raises significant\nconcerns about adversarial vulnerabilities that could compromise their safety\nand reliability. Unlike unimodal models, MLLMs face unique challenges due to\nthe interdependencies among modalities, making them susceptible to\nmodality-specific threats and cross-modal adversarial manipulations. This paper\nreviews the adversarial robustness of MLLMs, covering different modalities. We\nbegin with an overview of MLLMs and a taxonomy of adversarial attacks tailored\nto each modality. Next, we review key datasets and evaluation metrics used to\nassess the robustness of MLLMs. After that, we provide an in-depth review of\nattacks targeting MLLMs across different modalities. Our survey also identifies\ncritical challenges and suggests promising future research directions.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 06:54:59 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Jiang', 'Chengze', ''], ['Wang', 'Zhuangzhuang', ''], ['Dong', 'Minjing', ''], ['Gui', 'Jie', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2503.13983,"submitter":"Jiankang Wang","authors":"Jiankang Wang, Zhihan zhang, Zhihang Liu, Yang Li, Jiannan Ge, Hongtao\n  Xie, Yongdong Zhang","title":"SpaceVLLM: Endowing Multimodal Large Language Model with Spatio-Temporal\n  Video Grounding Capability","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multimodal large language models (MLLMs) have made remarkable progress in\neither temporal or spatial localization. However, they struggle to perform\nspatio-temporal video grounding. This limitation stems from two major\nchallenges. Firstly, it is difficult to extract accurate spatio-temporal\ninformation of each frame in the video. Secondly, the substantial number of\nvisual tokens makes it challenging to precisely map visual tokens of each frame\nto their corresponding spatial coordinates. To address these issues, we\nintroduce SpaceVLLM, a MLLM endowed with spatio-temporal video grounding\ncapability. Specifically, we adopt a set of interleaved Spatio-Temporal Aware\nQueries to capture temporal perception and dynamic spatial information.\nMoreover, we propose a Query-Guided Space Decoder to establish a corresponding\nconnection between the queries and spatial coordinates. Additionally, due to\nthe lack of spatio-temporal datasets, we construct the Unified Spatio-Temporal\nGrounding (Uni-STG) dataset, comprising 480K instances across three tasks. This\ndataset fully exploits the potential of MLLM to simultaneously facilitate\nlocalization in both temporal and spatial dimensions. Extensive experiments\ndemonstrate that SpaceVLLM achieves the state-of-the-art performance across 11\nbenchmarks covering temporal, spatial, spatio-temporal and video understanding\ntasks, highlighting the effectiveness of our approach. Our code, datasets and\nmodel will be released.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 07:40:36 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wang', 'Jiankang', ''], ['zhang', 'Zhihan', ''], ['Liu', 'Zhihang', ''], ['Li', 'Yang', ''], ['Ge', 'Jiannan', ''], ['Xie', 'Hongtao', ''], ['Zhang', 'Yongdong', '']]","extracted_entities":"[{'text': 'Multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal large language models","similarity_score":0.7649828196}
{"id":2503.14043,"submitter":"Guy Bar-Shalom","authors":"Guy Bar-Shalom, Fabrizio Frasca, Derek Lim, Yoav Gelberg, Yftah Ziser,\n  Ran El-Yaniv, Gal Chechik, Haggai Maron","title":"Learning on LLM Output Signatures for gray-box LLM Behavior Analysis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) have achieved widespread adoption, yet our\nunderstanding of their behavior remains limited, particularly in detecting data\ncontamination and hallucinations. While recently proposed probing techniques\nprovide insights through activation analysis, they require \"white-box\" access\nto model internals, often unavailable. Current \"gray-box\" approaches typically\nanalyze only the probability of the actual tokens in the sequence with simple\ntask-specific heuristics. Importantly, these methods overlook the rich\ninformation contained in the full token distribution at each processing step.\nTo address these limitations, we propose that gray-box analysis should leverage\nthe complete observable output of LLMs, consisting of both the previously used\ntoken probabilities as well as the complete token distribution sequences - a\nunified data type we term LOS (LLM Output Signature). To this end, we develop a\ntransformer-based approach to process LOS that theoretically guarantees\napproximation of existing techniques while enabling more nuanced analysis. Our\napproach achieves superior performance on hallucination and data contamination\ndetection in gray-box settings, significantly outperforming existing baselines.\nFurthermore, it demonstrates strong transfer capabilities across datasets and\nLLMs, suggesting that LOS captures fundamental patterns in LLM behavior. Our\ncode is available at: https:\/\/github.com\/BarSGuy\/LLM-Output-Signatures-Network.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 09:04:37 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Bar-Shalom', 'Guy', ''], ['Frasca', 'Fabrizio', ''], ['Lim', 'Derek', ''], ['Gelberg', 'Yoav', ''], ['Ziser', 'Yftah', ''], ['El-Yaniv', 'Ran', ''], ['Chechik', 'Gal', ''], ['Maron', 'Haggai', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.14075,"submitter":"Zhou Yu","authors":"Zhenwei Shao, Mingyang Wang, Zhou Yu, Wenwen Pan, Yan Yang, Tao Wei,\n  Hongyuan Zhang, Ning Mao, Wei Chen, Jun Yu","title":"Growing a Twig to Accelerate Large Vision-Language Models","comments":"17 pages, 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Large vision-language models (VLMs) have demonstrated remarkable capabilities\nin open-world multimodal understanding, yet their high computational overheads\npose great challenges for practical deployment. Some recent works have proposed\nmethods to accelerate VLMs by pruning redundant visual tokens guided by the\nattention maps of VLM's early layers. Despite the success of these token\npruning methods, they still suffer from two major shortcomings: (i)\nconsiderable accuracy drop due to insensitive attention signals in early\nlayers, and (ii) limited speedup when generating long responses (e.g., 30\ntokens). To address the limitations above, we present TwigVLM -- a simple and\ngeneral architecture by growing a lightweight twig upon an early layer of the\nbase VLM. Compared with most existing VLM acceleration methods purely based on\nvisual token pruning, our TwigVLM not only achieves better accuracy retention\nby employing a twig-guided token pruning (TTP) strategy, but also yields higher\ngeneration speed by utilizing a self-speculative decoding (SSD) strategy.\nTaking LLaVA-1.5-7B as the base VLM, experimental results show that TwigVLM\npreserves 96% of the original performance after pruning 88.9% of visual tokens\nand achieves 154% speedup in generating long responses, delivering\nsignificantly better performance in terms of both accuracy and speed over the\nstate-of-the-art VLM acceleration methods. Code will be made publicly\navailable.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 09:52:45 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Shao', 'Zhenwei', ''], ['Wang', 'Mingyang', ''], ['Yu', 'Zhou', ''], ['Pan', 'Wenwen', ''], ['Yang', 'Yan', ''], ['Wei', 'Tao', ''], ['Zhang', 'Hongyuan', ''], ['Mao', 'Ning', ''], ['Chen', 'Wei', ''], ['Yu', 'Jun', '']]","extracted_entities":"[{'text': 'Large vision-language models', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'attention maps', 'label': 'Attention mechanism'}, {'text': 'VLM', 'label': 'Large Language Model'}, {'text': 'VLM', 'label': 'Large Language Model'}, {'text': 'LLaVA-1.5-7B', 'label': 'Foundation Model'}, {'text': 'VLM', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large vision-language models","similarity_score":0.7742220759}
{"id":2503.14103,"submitter":"Jonas Oppenlaender","authors":"Jonas Oppenlaender","title":"DangerMaps: Personalized Safety Advice for Travel in Urban Environments\n  using a Retrieval-Augmented Language Model","comments":"17 pages, 7 figures, 1 table","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Planning a trip into a potentially unsafe area is a difficult task. We\nconducted a formative study on travelers' information needs, finding that most\nof them turn to search engines for trip planning. Search engines, however, fail\nto provide easily interpretable results adapted to the context and personal\ninformation needs of a traveler. Large language models (LLMs) create new\npossibilities for providing personalized travel safety advice. To explore this\nidea, we developed DangerMaps, a mapping system that assists its users in\nresearching the safety of an urban travel destination, whether it is pre-travel\nor on-location. DangerMaps plots safety ratings onto a map and provides\nexplanations on demand. This late breaking work specifically emphasizes the\nchallenges of designing real-world applications with large language models. We\nprovide a detailed description of our approach to prompt design and highlight\nfuture areas of research.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 10:18:07 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 10:20:29 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Oppenlaender', 'Jonas', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'prompt design', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.1414,"submitter":"Zining Wang","authors":"Zining Wang, Tongkun Guan, Pei Fu, Chen Duan, Qianyi Jiang, Zhentao\n  Guo, Shan Guo, Junfeng Luo, Wei Shen, Xiaokang Yang","title":"Marten: Visual Question Answering with Mask Generation for Multi-modal\n  Document Understanding","comments":"Accepted by CVPR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multi-modal Large Language Models (MLLMs) have introduced a novel dimension\nto document understanding, i.e., they endow large language models with visual\ncomprehension capabilities; however, how to design a suitable image-text\npre-training task for bridging the visual and language modality in\ndocument-level MLLMs remains underexplored. In this study, we introduce a novel\nvisual-language alignment method that casts the key issue as a Visual Question\nAnswering with Mask generation (VQAMask) task, optimizing two tasks\nsimultaneously: VQA-based text parsing and mask generation. The former allows\nthe model to implicitly align images and text at the semantic level. The latter\nintroduces an additional mask generator (discarded during inference) to\nexplicitly ensure alignment between visual texts within images and their\ncorresponding image regions at a spatially-aware level. Together, they can\nprevent model hallucinations when parsing visual text and effectively promote\nspatially-aware feature representation learning. To support the proposed\nVQAMask task, we construct a comprehensive image-mask generation pipeline and\nprovide a large-scale dataset with 6M data (MTMask6M). Subsequently, we\ndemonstrate that introducing the proposed mask generation task yields\ncompetitive document-level understanding performance. Leveraging the proposed\nVQAMask, we introduce Marten, a training-efficient MLLM tailored for\ndocument-level understanding. Extensive experiments show that our Marten\nconsistently achieves significant improvements among 8B-MLLMs in\ndocument-centric tasks. Code and datasets are available at\nhttps:\/\/github.com\/PriNing\/Marten.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:07:14 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wang', 'Zining', ''], ['Guan', 'Tongkun', ''], ['Fu', 'Pei', ''], ['Duan', 'Chen', ''], ['Jiang', 'Qianyi', ''], ['Guo', 'Zhentao', ''], ['Guo', 'Shan', ''], ['Luo', 'Junfeng', ''], ['Shen', 'Wei', ''], ['Yang', 'Xiaokang', '']]","extracted_entities":"[{'text': 'Multi-modal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'spatially-aware feature representation learning', 'label': 'Few-shot Learning'}, {'text': 'VQAMask', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"Multi-modal Large Language Models","similarity_score":0.7925285697}
{"id":2503.14153,"submitter":"Changran Xu","authors":"Changran Xu, Yi Liu, Yunhao Zhou, Shan Huang, Ningyi Xu, Qiang Xu","title":"Speculative Decoding for Verilog: Speed and Quality, All in One","comments":"Accepted by the 62nd Design Automation Conference (DAC 2025)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AR cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The rapid advancement of large language models (LLMs) has revolutionized code\ngeneration tasks across various programming languages. However, the unique\ncharacteristics of programming languages, particularly those like Verilog with\nspecific syntax and lower representation in training datasets, pose significant\nchallenges for conventional tokenization and decoding approaches. In this\npaper, we introduce a novel application of speculative decoding for Verilog\ncode generation, showing that it can improve both inference speed and output\nquality, effectively achieving speed and quality all in one. Unlike standard\nLLM tokenization schemes, which often fragment meaningful code structures, our\napproach aligns decoding stops with syntactically significant tokens, making it\neasier for models to learn the token distribution. This refinement addresses\ninherent tokenization issues and enhances the model's ability to capture\nVerilog's logical constructs more effectively. Our experimental results show\nthat our method achieves up to a 5.05x speedup in Verilog code generation and\nincreases pass@10 functional accuracy on RTLLM by up to 17.19% compared to\nconventional training strategies. These findings highlight speculative decoding\nas a promising approach to bridge the quality gap in code generation for\nspecialized programming languages.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:21:53 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Xu', 'Changran', ''], ['Liu', 'Yi', ''], ['Zhou', 'Yunhao', ''], ['Huang', 'Shan', ''], ['Xu', 'Ningyi', ''], ['Xu', 'Qiang', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.14189,"submitter":"Yongqi Li","authors":"Yongqi Li, Lu Yang, Jian Wang, Runyang You, Wenjie Li, Liqiang Nie","title":"Towards Harmless Multimodal Assistants with Blind Preference\n  Optimization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in multimodal understanding, reasoning, and interaction. Given the\nextensive applications of MLLMs, the associated safety issues have become\nincreasingly critical. Due to the effectiveness of preference optimization in\naligning MLLMs with human preferences, there is an urgent need for\nsafety-related preference data for MLLMs. To address this, we construct the\nMMSafe-PO preference dataset towards harmless multimodal assistants, featuring\nmultimodal instructions, the conversational format, and ranked paired responses\nfrom human feedback. We also identify two insightful observations: modality\nco-defense and modality cheating, which illustrate that MLLMs possess a certain\nlevel of inherent defense while still presenting unique safety challenges.\nBased on these observations, we propose the Blind Preference Optimization (BPO)\napproach. Comprehensive experiments on three benchmarks show that BPO\neffectively enhances the safety capabilities of MLLMs. Notably, BPO\nsignificantly improves the safety rate of the base MLLM by 45.0%, outperforming\nthe DPO approach. Additionally, applying BPO to the MMSafe-PO dataset greatly\nreduces the base MLLM's unsafe rate on other safety benchmarks (14.5% on\nMM-SafetyBench and 82.9% on HarmEval, demonstrating the effectiveness and\nrobustness of both the dataset and the approach. We release code and data at\nhttps:\/\/lu-yang666.github.io\/MMsafe-PO-Web\/.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 12:02:38 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Li', 'Yongqi', ''], ['Yang', 'Lu', ''], ['Wang', 'Jian', ''], ['You', 'Runyang', ''], ['Li', 'Wenjie', ''], ['Nie', 'Liqiang', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLM', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2503.14217,"submitter":"Tennison Liu","authors":"Tennison Liu, Nicolas Huynh, Mihaela van der Schaar","title":"Decision Tree Induction Through LLMs via Semantically-Aware Evolution","comments":"*Liu and Huynh contributed equally. Published as a conference paper\n  at ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Decision trees are a crucial class of models offering robust predictive\nperformance and inherent interpretability across various domains, including\nhealthcare, finance, and logistics. However, current tree induction methods\noften face limitations such as suboptimal solutions from greedy methods or\nprohibitive computational costs and limited applicability of exact optimization\napproaches. To address these challenges, we propose an evolutionary\noptimization method for decision tree induction based on genetic programming\n(GP). Our key innovation is the integration of semantic priors and\ndomain-specific knowledge about the search space into the optimization\nalgorithm. To this end, we introduce $\\texttt{LLEGO}$, a framework that\nincorporates semantic priors into genetic search operators through the use of\nLarge Language Models (LLMs), thereby enhancing search efficiency and targeting\nregions of the search space that yield decision trees with superior\ngeneralization performance. This is operationalized through novel genetic\noperators that work with structured natural language prompts, effectively\nutilizing LLMs as conditional generative models and sources of semantic\nknowledge. Specifically, we introduce $\\textit{fitness-guided}$ crossover to\nexploit high-performing regions, and $\\textit{diversity-guided}$ mutation for\nefficient global exploration of the search space. These operators are\ncontrolled by corresponding hyperparameters that enable a more nuanced balance\nbetween exploration and exploitation across the search space. Empirically, we\ndemonstrate across various benchmarks that $\\texttt{LLEGO}$ evolves\nsuperior-performing trees compared to existing tree induction methods, and\nexhibits significantly more efficient search performance compared to\nconventional GP approaches.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 12:52:03 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Liu', 'Tennison', ''], ['Huynh', 'Nicolas', ''], ['van der Schaar', 'Mihaela', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'structured natural language prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'LLM'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.14232,"submitter":"Yuyang Xue","authors":"Yuyang Xue, Edward Moroshko, Feng Chen, Steven McDonagh, Sotirios A.\n  Tsaftaris","title":"CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion\n  Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Text-to-Image diffusion models can produce undesirable content that\nnecessitates concept erasure techniques. However, existing methods struggle\nwith under-erasure, leaving residual traces of targeted concepts, or\nover-erasure, mistakenly eliminating unrelated but visually similar concepts.\nTo address these limitations, we introduce CRCE, a novel concept erasure\nframework that leverages Large Language Models to identify both semantically\nrelated concepts that should be erased alongside the target and distinct\nconcepts that should be preserved. By explicitly modeling coreferential and\nretained concepts semantically, CRCE enables more precise concept removal,\nwithout unintended erasure. Experiments demonstrate that CRCE outperforms\nexisting methods on diverse erasure tasks.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 13:09:01 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Xue', 'Yuyang', ''], ['Moroshko', 'Edward', ''], ['Chen', 'Feng', ''], ['McDonagh', 'Steven', ''], ['Tsaftaris', 'Sotirios A.', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.14234,"submitter":"Ruiyi Yang","authors":"Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D. Salim","title":"KG-IRAG: A Knowledge Graph-Based Iterative Retrieval-Augmented\n  Generation Framework for Temporal Reasoning","comments":"14 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.MA","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective\nin enhancing the performance of Large Language Models (LLMs) on tasks that\nrequire external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG\nimproves information retrieval for complex reasoning tasks, providing more\nprecise and comprehensive retrieval and generating more accurate responses to\nQAs. However, most RAG methods fall short in addressing multi-step reasoning,\nparticularly when both information extraction and inference are necessary. To\naddress this limitation, this paper presents Knowledge Graph-Based Iterative\nRetrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs\nwith iterative reasoning to improve LLMs' ability to handle queries involving\ntemporal and logical dependencies. Through iterative retrieval steps, KG-IRAG\nincrementally gathers relevant data from external KGs, enabling step-by-step\nreasoning. The proposed approach is particularly suited for scenarios where\nreasoning is required alongside dynamic temporal data extraction, such as\ndetermining optimal travel times based on weather conditions or traffic\npatterns. Experimental results show that KG-IRAG improves accuracy in complex\nreasoning tasks by effectively integrating external knowledge with iterative,\nlogic-based retrieval. Additionally, three new datasets: weatherQA-Irish,\nweatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's\nperformance, demonstrating its potential beyond traditional RAG applications.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 13:11:43 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 04:49:29 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Yang', 'Ruiyi', ''], ['Xue', 'Hao', ''], ['Razzak', 'Imran', ''], ['Hacid', 'Hakim', ''], ['Salim', 'Flora D.', '']]","extracted_entities":"[{'text': 'GraphRAG', 'label': 'RAG'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Knowledge Graphs', 'label': 'LLMs'}, {'text': 'GraphRAG', 'label': 'RAG'}, {'text': 'KGs', 'label': 'LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'KG-IRAG', 'label': 'RAG'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.14257,"submitter":"Guang Dai","authors":"Guang Dai, Pinhao Wang, Cheng Yao, Fangtian Ying","title":"InnerSelf: Designing Self-Deepfaked Voice for Emotional Well-being","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  One's own voice is one of the most frequently heard voices. Studies found\nthat hearing and talking to oneself have positive psychological effects.\nHowever, the design and implementation of self-voice for emotional regulation\nin HCI have yet to be explored. In this paper, we introduce InnerSelf, an\ninnovative voice system based on speech synthesis technologies and the Large\nLanguage Model. It allows users to engage in supportive and empathic dialogue\nwith their deepfake voice. By manipulating positive self-talk, our system aims\nto promote self-disclosure and regulation, reshaping negative thoughts and\nimproving emotional well-being.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 13:45:22 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Dai', 'Guang', ''], ['Wang', 'Pinhao', ''], ['Yao', 'Cheng', ''], ['Ying', 'Fangtian', '']]","extracted_entities":"[{'text': 'Large\\nLanguage Model', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large\nLanguage Model","similarity_score":1.0}
{"id":2503.14269,"submitter":"Ojasv Kamal","authors":"Vaibhav Aggarwal, Ojasv Kamal, Abhinav Japesh, Zhijing Jin, Bernhard\n  Sch\\\"olkopf","title":"DARS: Dynamic Action Re-Sampling to Enhance Coding Agent Performance by\n  Adaptive Tree Traversal","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) have revolutionized various domains, including\nnatural language processing, data analysis, and software development, by\nenabling automation. In software engineering, LLM-powered coding agents have\ngarnered significant attention due to their potential to automate complex\ndevelopment tasks, assist in debugging, and enhance productivity. However,\nexisting approaches often struggle with sub-optimal decision-making, requiring\neither extensive manual intervention or inefficient compute scaling strategies.\nTo improve coding agent performance, we present Dynamic Action Re-Sampling\n(DARS), a novel inference time compute scaling approach for coding agents, that\nis faster and more effective at recovering from sub-optimal decisions compared\nto baselines. While traditional agents either follow linear trajectories or\nrely on random sampling for scaling compute, our approach DARS works by\nbranching out a trajectory at certain key decision points by taking an\nalternative action given the history of the trajectory and execution feedback\nof the previous attempt from that point. We evaluate our approach on SWE-Bench\nLite benchmark, demonstrating that this scaling strategy achieves a pass@k\nscore of 55% with Claude 3.5 Sonnet V2. Our framework achieves a pass@1 rate of\n47%, outperforming state-of-the-art (SOTA) open-source frameworks.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 14:02:59 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Aggarwal', 'Vaibhav', ''], ['Kamal', 'Ojasv', ''], ['Japesh', 'Abhinav', ''], ['Jin', 'Zhijing', ''], ['Sch\u00f6lkopf', 'Bernhard', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLM-powered coding agents', 'label': 'LLM-powered'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.14324,"submitter":"Wei Song","authors":"Wei Song, Yuran Wang, Zijia Song, Yadong Li, Haoze Sun, Weipeng Chen,\n  Zenan Zhou, Jianhua Xu, Jiaqi Wang, Kaicheng Yu","title":"DualToken: Towards Unifying Visual Understanding and Generation with\n  Dual Visual Vocabularies","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The differing representation spaces required for visual understanding and\ngeneration pose a challenge in unifying them within the autoregressive paradigm\nof large language models. A vision tokenizer trained for reconstruction excels\nat capturing low-level perceptual details, making it well-suited for visual\ngeneration but lacking high-level semantic representations for understanding\ntasks. Conversely, a vision encoder trained via contrastive learning aligns\nwell with language but struggles to decode back into the pixel space for\ngeneration tasks. To bridge this gap, we propose DualToken, a method that\nunifies representations for both understanding and generation within a single\ntokenizer. However, directly integrating reconstruction and semantic objectives\nin a single tokenizer creates conflicts, leading to degraded performance in\nboth reconstruction quality and semantic performance. Instead of forcing a\nsingle codebook to handle both semantic and perceptual information, DualToken\ndisentangles them by introducing separate codebooks for high and low-level\nfeatures, effectively transforming their inherent conflict into a synergistic\nrelationship. As a result, DualToken achieves state-of-the-art performance in\nboth reconstruction and semantic tasks while demonstrating remarkable\neffectiveness in downstream MLLM understanding and generation tasks. Notably,\nwe also show that DualToken, as a unified tokenizer, surpasses the naive\ncombination of two distinct types vision encoders, providing superior\nperformance within a unified MLLM.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 14:56:46 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 12:58:33 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Song', 'Wei', ''], ['Wang', 'Yuran', ''], ['Song', 'Zijia', ''], ['Li', 'Yadong', ''], ['Sun', 'Haoze', ''], ['Chen', 'Weipeng', ''], ['Zhou', 'Zenan', ''], ['Xu', 'Jianhua', ''], ['Wang', 'Jiaqi', ''], ['Yu', 'Kaicheng', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'contrastive learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.1434,"submitter":"Yisen Xu","authors":"Yisen Xu, Feng Lin, Jinqiu Yang, Tse-Hsun (Peter) Chen, Nikolaos\n  Tsantalis","title":"MANTRA: Enhancing Automated Method-Level Refactoring with Contextual RAG\n  and Multi-Agent LLM Collaboration","comments":"10 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Maintaining and scaling software systems relies heavily on effective code\nrefactoring, yet this process remains labor-intensive, requiring developers to\ncarefully analyze existing codebases and prevent the introduction of new\ndefects. Although recent advancements have leveraged Large Language Models\n(LLMs) to automate refactoring tasks, current solutions are constrained in\nscope and lack mechanisms to guarantee code compilability and successful test\nexecution. In this work, we introduce MANTRA, a comprehensive LLM agent-based\nframework that automates method-level refactoring. MANTRA integrates\nContext-Aware Retrieval-Augmented Generation, coordinated Multi-Agent\nCollaboration, and Verbal Reinforcement Learning to emulate human\ndecision-making during refactoring while preserving code correctness and\nreadability. Our empirical study, conducted on 703 instances of \"pure\nrefactorings\" (i.e., code changes exclusively involving structural\nimprovements), drawn from 10 representative Java projects, covers the six most\nprevalent refactoring operations. Experimental results demonstrate that MANTRA\nsubstantially surpasses a baseline LLM model (RawGPT ), achieving an 82.8%\nsuccess rate (582\/703) in producing code that compiles and passes all tests,\ncompared to just 8.7% (61\/703) with RawGPT. Moreover, in comparison to\nIntelliJ's LLM-powered refactoring tool (EM-Assist), MANTRA exhibits a 50%\nimprovement in generating Extract Method transformations. A usability study\ninvolving 37 professional developers further shows that refactorings performed\nby MANTRA are perceived to be as readable and reusable as human-written code,\nand in certain cases, even more favorable. These results highlight the\npractical advantages of MANTRA and emphasize the growing potential of LLM-based\nsystems in advancing the automation of software refactoring tasks.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 15:16:51 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Xu', 'Yisen', '', 'Peter'], ['Lin', 'Feng', '', 'Peter'], ['Yang', 'Jinqiu', '', 'Peter'], ['Tse-Hsun', '', '', 'Peter'], ['Chen', '', ''], ['Tsantalis', 'Nikolaos', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Context-Aware Retrieval-Augmented Generation', 'label': 'Few-shot Learning'}, {'text': 'Verbal Reinforcement Learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.14391,"submitter":"Deniz Yuret","authors":"Shadi Hamdan and Deniz Yuret","title":"How much do LLMs learn from negative examples?","comments":"8 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models (LLMs) undergo a three-phase training process:\nunsupervised pre-training, supervised fine-tuning (SFT), and learning from\nhuman feedback (RLHF\/DPO). Notably, it is during the final phase that these\nmodels are exposed to negative examples -- incorrect, rejected, or suboptimal\nresponses to queries. This paper delves into the role of negative examples in\nthe training of LLMs, using a likelihood-ratio (Likra) model on multiple-choice\nquestion answering benchmarks to precisely manage the influence and the volume\nof negative examples. Our findings reveal three key insights: (1) During a\ncritical phase in training, Likra with negative examples demonstrates a\nsignificantly larger improvement per training example compared to SFT using\nonly positive examples. This leads to a sharp jump in the learning curve for\nLikra unlike the smooth and gradual improvement of SFT; (2) negative examples\nthat are plausible but incorrect (near-misses) exert a greater influence; and\n(3) while training with positive examples fails to significantly decrease the\nlikelihood of plausible but incorrect answers, training with negative examples\nmore accurately identifies them. These results indicate a potentially\nsignificant role for negative examples in improving accuracy and reducing\nhallucinations for LLMs.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:26:29 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Hamdan', 'Shadi', ''], ['Yuret', 'Deniz', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'SFT', 'label': 'Fine-tuning'}, {'text': 'SFT', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.14392,"submitter":"Qiantong Wang","authors":"Qiantong Wang","title":"From \"Hallucination\" to \"Suture\": Insights from Language Philosophy to\n  Enhance Large Language Models","comments":"7 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper explores hallucination phenomena in large language models (LLMs)\nthrough the lens of language philosophy and psychoanalysis. By incorporating\nLacan's concepts of the \"chain of signifiers\" and \"suture points,\" we propose\nthe Anchor-RAG framework as a novel approach to mitigate hallucinations. In\ncontrast to the predominant reliance on trial-and-error experiments, constant\nadjustments of mathematical formulas, or resource-intensive methods that\nemphasize quantity over quality, our approach returns to the fundamental\nprinciples of linguistics to analyze the root causes of hallucinations in LLMs.\nDrawing from robust theoretical foundations, we derive algorithms and models\nthat are not only effective in reducing hallucinations but also enhance LLM\nperformance and improve output quality. This paper seeks to establish a\ncomprehensive theoretical framework for understanding hallucinations in LLMs\nand aims to challenge the prevalent \"guess-and-test\" approach and rat race\nmentality in the field. We aspire to pave the way for a new era of\ninterpretable LLMs, offering deeper insights into the inner workings of\nlanguage-based AI systems.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:27:01 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wang', 'Qiantong', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'chain of signifiers', 'label': 'Chain of thought'}, {'text': 'Anchor-RAG framework', 'label': 'RAG'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.14408,"submitter":"Parisa Ghanad Torshizi","authors":"Parisa Ghanad Torshizi, Laura B. Hensel, Ari Shapiro, Stacy C.\n  Marsella","title":"Large Language Models for Virtual Human Gesture Selection","comments":"9 pages, 6 figures, Accepted at the AAMAS 2025 conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Co-speech gestures convey a wide variety of meanings and play an important\nrole in face-to-face human interactions. These gestures significantly influence\nthe addressee's engagement, recall, comprehension, and attitudes toward the\nspeaker. Similarly, they impact interactions between humans and embodied\nvirtual agents. The process of selecting and animating meaningful gestures has\nthus become a key focus in the design of these agents. However, automating this\ngesture selection process poses a significant challenge. Prior gesture\ngeneration techniques have varied from fully automated, data-driven methods,\nwhich often struggle to produce contextually meaningful gestures, to more\nmanual approaches that require crafting specific gesture expertise and are\ntime-consuming and lack generalizability. In this paper, we leverage the\nsemantic capabilities of Large Language Models to develop a gesture selection\napproach that suggests meaningful, appropriate co-speech gestures. We first\ndescribe how information on gestures is encoded into GPT-4. Then, we conduct a\nstudy to evaluate alternative prompting approaches for their ability to select\nmeaningful, contextually relevant gestures and to align them appropriately with\nthe co-speech utterance. Finally, we detail and demonstrate how this approach\nhas been implemented within a virtual agent system, automating the selection\nand subsequent animation of the selected gestures for enhanced human-agent\ninteractions.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:49:56 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Torshizi', 'Parisa Ghanad', ''], ['Hensel', 'Laura B.', ''], ['Shapiro', 'Ari', ''], ['Marsella', 'Stacy C.', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'alternative prompting approaches', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.14411,"submitter":"Siwei Zhang","authors":"Siwei Zhang, Yun Xiong, Yateng Tang, Xi Chen, Zian Jia, Zehao Gu,\n  Jiarong Xu, Jiawei Zhang","title":"Unifying Text Semantics and Graph Structures for Temporal\n  Text-attributed Graphs with Large Language Models","comments":"Submit to ICML2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Temporal graph neural networks (TGNNs) have shown remarkable performance in\ntemporal graph modeling. However, real-world temporal graphs often possess rich\ntextual information, giving rise to temporal text-attributed graphs (TTAGs).\nSuch combination of dynamic text semantics and evolving graph structures\nintroduces heightened complexity. Existing TGNNs embed texts statically and\nrely heavily on encoding mechanisms that biasedly prioritize structural\ninformation, overlooking the temporal evolution of text semantics and the\nessential interplay between semantics and structures for synergistic\nreinforcement. To tackle these issues, we present \\textbf{{Cross}}, a novel\nframework that seamlessly extends existing TGNNs for TTAG modeling. The key\nidea is to employ the advanced large language models (LLMs) to extract the\ndynamic semantics in text space and then generate expressive representations\nunifying both semantics and structures. Specifically, we propose a Temporal\nSemantics Extractor in the {Cross} framework, which empowers the LLM to offer\nthe temporal semantic understanding of node's evolving contexts of textual\nneighborhoods, facilitating semantic dynamics. Subsequently, we introduce the\nSemantic-structural Co-encoder, which collaborates with the above Extractor for\nsynthesizing illuminating representations by jointly considering both semantic\nand structural information while encouraging their mutual reinforcement.\nExtensive experimental results on four public datasets and one practical\nindustrial dataset demonstrate {Cross}'s significant effectiveness and\nrobustness.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 16:50:10 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Zhang', 'Siwei', ''], ['Xiong', 'Yun', ''], ['Tang', 'Yateng', ''], ['Chen', 'Xi', ''], ['Jia', 'Zian', ''], ['Gu', 'Zehao', ''], ['Xu', 'Jiarong', ''], ['Zhang', 'Jiawei', '']]","extracted_entities":"[{'text': 'Temporal graph neural networks', 'label': 'Neural Language Model'}, {'text': 'TGNNs', 'label': 'Neural Language Model'}, {'text': 'TGNNs', 'label': 'Neural Language Model'}, {'text': 'advanced large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"advanced large language models","similarity_score":0.9208456278}
{"id":2503.14432,"submitter":"Wei Fang","authors":"Wei Fang, Yang Zhang, Kaizhi Qian, James Glass, Yada Zhu","title":"PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via\n  Tool Play","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models (LLMs) are increasingly integrated with specialized\nexternal tools, yet many tasks demand zero-shot tool usage with minimal or\nnoisy documentation. Existing solutions rely on manual rewriting or labeled\ndata for validation, making them inapplicable in true zero-shot settings. To\naddress these challenges, we propose PLAY2PROMPT, an automated framework that\nsystematically \"plays\" with each tool to explore its input-output behaviors.\nThrough this iterative trial-and-error process, PLAY2PROMPT refines tool\ndocumentation and generates usage examples without any labeled data. These\nexamples not only guide LLM inference but also serve as validation to further\nenhance tool utilization. Extensive experiments on real-world tasks demonstrate\nthat PLAY2PROMPT significantly improves zero-shot tool performance across both\nopen and closed models, offering a scalable and effective solution for\ndomain-specific tool integration.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:09:57 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Fang', 'Wei', ''], ['Zhang', 'Yang', ''], ['Qian', 'Kaizhi', ''], ['Glass', 'James', ''], ['Zhu', 'Yada', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.14434,"submitter":"Nikhil Abhyankar","authors":"Nikhil Abhyankar, Parshin Shojaee, Chandan K. Reddy","title":"LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as\n  Evolutionary Optimizers","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL cs.NE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Automated feature engineering plays a critical role in improving predictive\nmodel performance for tabular learning tasks. Traditional automated feature\nengineering methods are limited by their reliance on pre-defined\ntransformations within fixed, manually designed search spaces, often neglecting\ndomain knowledge. Recent advances using Large Language Models (LLMs) have\nenabled the integration of domain knowledge into the feature engineering\nprocess. However, existing LLM-based approaches use direct prompting or rely\nsolely on validation scores for feature selection, failing to leverage insights\nfrom prior feature discovery experiments or establish meaningful reasoning\nbetween feature generation and data-driven performance. To address these\nchallenges, we propose LLM-FE, a novel framework that combines evolutionary\nsearch with the domain knowledge and reasoning capabilities of LLMs to\nautomatically discover effective features for tabular learning tasks. LLM-FE\nformulates feature engineering as a program search problem, where LLMs propose\nnew feature transformation programs iteratively, and data-driven feedback\nguides the search process. Our results demonstrate that LLM-FE consistently\noutperforms state-of-the-art baselines, significantly enhancing the performance\nof tabular prediction models across diverse classification and regression\nbenchmarks.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:11:24 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Abhyankar', 'Nikhil', ''], ['Shojaee', 'Parshin', ''], ['Reddy', 'Chandan K.', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'existing LLM-based approaches', 'label': 'LLM-based'}, {'text': 'direct prompting', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'tabular learning tasks', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.14478,"submitter":"Xinyu Fang","authors":"Xinyu Fang, Zhijian Chen, Kai Lan, Lixin Ma, Shengyuan Ding, Yingji\n  Liang, Xiangyu Zhao, Farong Wen, Zicheng Zhang, Guofeng Zhang, Haodong Duan,\n  Kai Chen, Dahua Lin","title":"Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM","comments":"Evaluation Code and dataset see\n  https:\/\/github.com\/open-compass\/Creation-MMBench","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https:\/\/github.com\/open-compass\/Creation-MMBench.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:51:34 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 17:03:25 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Fang', 'Xinyu', ''], ['Chen', 'Zhijian', ''], ['Lan', 'Kai', ''], ['Ma', 'Lixin', ''], ['Ding', 'Shengyuan', ''], ['Liang', 'Yingji', ''], ['Zhao', 'Xiangyu', ''], ['Wen', 'Farong', ''], ['Zhang', 'Zicheng', ''], ['Zhang', 'Guofeng', ''], ['Duan', 'Haodong', ''], ['Chen', 'Kai', ''], ['Lin', 'Dahua', '']]","extracted_entities":"[{'text': 'Large\\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'visual fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Large Language Model","matched_keyword":"Large\nLanguage Models","similarity_score":0.9664971828}
{"id":2503.14504,"submitter":"Yi-Fan Zhang","authors":"Tao Yu, Yi-Fan Zhang, Chaoyou Fu, Junkang Wu, Jinda Lu, Kun Wang,\n  Xingyu Lu, Yunhang Shen, Guibin Zhang, Dingjie Song, Yibo Yan, Tianlong Xu,\n  Qingsong Wen, Zhang Zhang, Yan Huang, Liang Wang, and Tieniu Tan","title":"Aligning Multimodal LLM with Human Preference: A Survey","comments":"https:\/\/github.com\/BradyFU\/Awesome-Multimodal-Large-Language-Models\/tree\/Alignment","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models (LLMs) can handle a wide variety of general tasks with\nsimple prompts, without the need for task-specific training. Multimodal Large\nLanguage Models (MLLMs), built upon LLMs, have demonstrated impressive\npotential in tackling complex tasks involving visual, auditory, and textual\ndata. However, critical issues related to truthfulness, safety, o1-like\nreasoning, and alignment with human preference remain insufficiently addressed.\nThis gap has spurred the emergence of various alignment algorithms, each\ntargeting different application scenarios and optimization goals. Recent\nstudies have shown that alignment algorithms are a powerful approach to\nresolving the aforementioned challenges. In this paper, we aim to provide a\ncomprehensive and systematic review of alignment algorithms for MLLMs.\nSpecifically, we explore four key aspects: (1) the application scenarios\ncovered by alignment algorithms, including general image understanding,\nmulti-image, video, and audio, and extended multimodal applications; (2) the\ncore factors in constructing alignment datasets, including data sources, model\nresponses, and preference annotations; (3) the benchmarks used to evaluate\nalignment algorithms; and (4) a discussion of potential future directions for\nthe development of alignment algorithms. This work seeks to help researchers\norganize current advancements in the field and inspire better alignment\nmethods. The project page of this paper is available at\nhttps:\/\/github.com\/BradyFU\/Awesome-Multimodal-Large-Language-Models\/tree\/Alignment.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:59:56 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Yu', 'Tao', ''], ['Zhang', 'Yi-Fan', ''], ['Fu', 'Chaoyou', ''], ['Wu', 'Junkang', ''], ['Lu', 'Jinda', ''], ['Wang', 'Kun', ''], ['Lu', 'Xingyu', ''], ['Shen', 'Yunhang', ''], ['Zhang', 'Guibin', ''], ['Song', 'Dingjie', ''], ['Yan', 'Yibo', ''], ['Xu', 'Tianlong', ''], ['Wen', 'Qingsong', ''], ['Zhang', 'Zhang', ''], ['Huang', 'Yan', ''], ['Wang', 'Liang', ''], ['Tan', 'Tieniu', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'simple prompts', 'label': 'Prompting'}, {'text': 'Multimodal Large\\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'Awesome-Multimodal-Large-Language-Models', 'label': 'Open-source LLMs'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.1453,"submitter":"Jiahui Geng","authors":"Qing Li, Jiahui Geng, Derui Zhu, Fengyu Cai, Chenyang Lyu, Fakhri\n  Karray","title":"SAUCE: Selective Concept Unlearning in Vision-Language Models with\n  Sparse Autoencoders","comments":"More comparative experiments are needed","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Unlearning methods for vision-language models (VLMs) have primarily adapted\ntechniques from large language models (LLMs), relying on weight updates that\ndemand extensive annotated forget sets. Moreover, these methods perform\nunlearning at a coarse granularity, often leading to excessive forgetting and\nreduced model utility. To address this issue, we introduce SAUCE, a novel\nmethod that leverages sparse autoencoders (SAEs) for fine-grained and selective\nconcept unlearning in VLMs. Briefly, SAUCE first trains SAEs to capture\nhigh-dimensional, semantically rich sparse features. It then identifies the\nfeatures most relevant to the target concept for unlearning. During inference,\nit selectively modifies these features to suppress specific concepts while\npreserving unrelated information. We evaluate SAUCE on two distinct VLMs,\nLLaVA-v1.5-7B and LLaMA-3.2-11B-Vision-Instruct, across two types of tasks:\nconcrete concept unlearning (objects and sports scenes) and abstract concept\nunlearning (emotions, colors, and materials), encompassing a total of 60\nconcepts. Extensive experiments demonstrate that SAUCE outperforms\nstate-of-the-art methods by 18.04% in unlearning quality while maintaining\ncomparable model utility. Furthermore, we investigate SAUCE's robustness\nagainst widely used adversarial attacks, its transferability across models, and\nits scalability in handling multiple simultaneous unlearning requests. Our\nfindings establish SAUCE as an effective and scalable solution for selective\nconcept unlearning in VLMs.\n","versions":"[{'version': 'v1', 'created': 'Sun, 16 Mar 2025 17:32:23 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 05:47:10 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Li', 'Qing', ''], ['Geng', 'Jiahui', ''], ['Zhu', 'Derui', ''], ['Cai', 'Fengyu', ''], ['Lyu', 'Chenyang', ''], ['Karray', 'Fakhri', '']]","extracted_entities":"[{'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'concrete concept unlearning', 'label': 'Few-shot Learning'}, {'text': 'abstract concept\\nunlearning', 'label': 'Few-shot Learning'}, {'text': 'VLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.14604,"submitter":"Sara Sarto","authors":"Sara Sarto, Marcella Cornia, Rita Cucchiara","title":"Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges\n  and Future Perspectives","comments":"Repo GitHub:\n  https:\/\/github.com\/aimagelab\/awesome-captioning-evaluation","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 18:03:56 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Sarto', 'Sara', ''], ['Cornia', 'Marcella', ''], ['Cucchiara', 'Rita', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2503.14674,"submitter":"Amirul Rahman","authors":"Liu Jing, Amirul Rahman","title":"Elevating Visual Question Answering through Implicitly Learned Reasoning\n  Pathways in LVLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Vision-Language Models (LVLMs) have shown remarkable progress in\nvarious multimodal tasks, yet they often struggle with complex visual reasoning\nthat requires multi-step inference. To address this limitation, we propose\nMF-SQ-LLaVA, a novel approach that enhances LVLMs by enabling implicit\nself-questioning through end-to-end training. Our method involves augmenting\nvisual question answering datasets with reasoning chains consisting of\nsub-question and answer pairs, and training the LVLM with a multi-task loss\nthat encourages the generation and answering of these intermediate steps, as\nwell as the prediction of the final answer. We conduct extensive experiments on\nthe ScienceQA and VQAv2 datasets, demonstrating that MF-SQ-LLaVA significantly\noutperforms existing state-of-the-art models, including the base LLaVA and the\noriginal SQ-LLaVA. Ablation studies further validate the contribution of each\ncomponent of our approach, and human evaluation confirms the improved accuracy\nand coherence of the reasoning process enabled by our method.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 19:29:07 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Jing', 'Liu', ''], ['Rahman', 'Amirul', '']]","extracted_entities":"[{'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'reasoning chains', 'label': 'Chain of thought'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Vision-Language Models","similarity_score":0.7742220759}
{"id":2503.14724,"submitter":"Sebastian Zhao","authors":"Sebastian Zhao, Alan Zhu, Hussein Mozannar, David Sontag, Ameet\n  Talwalkar, Valerie Chen","title":"CodingGenie: A Proactive LLM-Powered Programming Assistant","comments":"FSE Demo 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While developers increasingly adopt tools powered by large language models\n(LLMs) in day-to-day workflows, these tools still require explicit user\ninvocation. To seamlessly integrate LLM capabilities to a developer's workflow,\nwe introduce CodingGenie, a proactive assistant integrated into the code\neditor. CodingGenie autonomously provides suggestions, ranging from bug fixing\nto unit testing, based on the current code context and allows users to\ncustomize suggestions by providing a task description and selecting what\nsuggestions are shown. We demonstrate multiple use cases to show how proactive\nsuggestions from CodingGenie can improve developer experience, and also analyze\nthe cost of adding proactivity. We believe this open-source tool will enable\nfurther research into proactive assistants. CodingGenie is open-sourced at\nhttps:\/\/github.com\/sebzhao\/CodingGenie\/ and video demos are available at\nhttps:\/\/sebzhao.github.io\/CodingGenie\/.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 20:54:40 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhao', 'Sebastian', ''], ['Zhu', 'Alan', ''], ['Mozannar', 'Hussein', ''], ['Sontag', 'David', ''], ['Talwalkar', 'Ameet', ''], ['Chen', 'Valerie', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'CodingGenie', 'label': 'Open-source LLMs'}, {'text': 'CodingGenie', 'label': 'Open-source LLMs'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.14749,"submitter":"Sophia Hager","authors":"Sophia Hager, David Mueller, Kevin Duh, and Nicholas Andrews","title":"Uncertainty Distillation: Teaching Language Models to Express Semantic\n  Confidence","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  As large language models (LLMs) are increasingly used for factual\nquestion-answering, it becomes more important for LLMs to have the capability\nto communicate the likelihood that their answer is correct. For these\nverbalized expressions of uncertainty to be meaningful, they should reflect the\nerror rates at the expressed level of confidence. However, when prompted to\nexpress confidence, the error rates of current LLMs are inconsistent with their\ncommunicated confidences, highlighting the need for uncertainty quantification\nmethods. Many prior methods calculate lexical uncertainty, estimating a model's\nconfidence in the specific string it generated. In some cases, however, it may\nbe more useful to estimate semantic uncertainty, or the model's confidence in\nthe answer regardless of how it is verbalized. We propose a simple procedure,\nuncertainty distillation, to teach an LLM to verbalize calibrated semantic\nconfidences. Using held-out data to map initial uncertainty estimates to\nmeaningful probabilities, we create examples annotated with verbalized\nprobabilities for supervised fine-tuning. We demonstrate our method yields\nverbalized confidences that correlate with observed error rates with a small\nfine-tuned language model as well as with larger instruction-tuned models, and\nfind that our semantic uncertainty correlates well with lexical uncertainty on\nshort answers.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 21:29:29 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Hager', 'Sophia', ''], ['Mueller', 'David', ''], ['Duh', 'Kevin', ''], ['Andrews', 'Nicholas', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompted', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'uncertainty distillation', 'label': 'Knowledge distillation'}, {'text': 'supervised fine-tuning', 'label': 'Fine-tuning'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.14838,"submitter":"Chengran Yang","authors":"Chengran Yang, Zhensu Sun, Hong Jin Kang, Jieke Shi, David Lo","title":"Think Like Human Developers: Harnessing Community Knowledge for\n  Structured Code Reasoning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) have significantly advanced automated code\ngeneration, yet they struggle with complex coding tasks requiring multi-step\nlogical reasoning. High-quality reasoning data is crucial for improving LLMs'\nreasoning capabilities, but such datasets remain scarce. Existing approaches\neither rely on computationally expensive reinforcement learning (RL) or\nerror-prone reasoning chains synthesized by LLMs, posing challenges in\nscalability and accuracy.\n  To address this challenge, we propose SVRC (Structured and Validated\nReasoning Chains for Code Generation), a novel framework that mines,\nrestructures, and enriches reasoning chains from community-driven discussions\non software engineering platforms. SVRC refines unstructured and incomplete\ndiscussions of coding problems by aligning them with Software Development Life\nCycle (SDLC) principles, ensuring that reasoning chains capture real-world\nproblem-solving strategies and support iterative refinement.\n  To evaluate the effectiveness of SVRC, we introduce CodeThinker, an LLM\nfine-tuned on 12,444 reasoning-augmented samples generated by SVRC. Experiments\non LiveCodeBench show that CodeThinker surpasses its base model by 42.86\\% on\nmedium-level code problems in terms of pass@1 and outperforms GPT-4o-mini and\nGPT-4o by 73.14\\% and 115.86\\%, respectively. Our ablation study further\nhighlights that each component of SVRC contributes to the reasoning\ncapabilities of CodeThinker.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 02:45:13 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Yang', 'Chengran', ''], ['Sun', 'Zhensu', ''], ['Kang', 'Hong Jin', ''], ['Shi', 'Jieke', ''], ['Lo', 'David', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'error-prone reasoning chains', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'scalability', 'label': 'Scaling law'}, {'text': 'reasoning chains', 'label': 'Chain of thought'}, {'text': 'reasoning chains', 'label': 'Chain of thought'}, {'text': 'iterative refinement', 'label': 'Fine-tuning'}, {'text': 'CodeThinker', 'label': 'LLM'}, {'text': 'CodeThinker', 'label': 'LLM'}, {'text': 'GPT-4o-mini', 'label': 'GPT'}, {'text': 'GPT-4o', 'label': 'GPT-4'}, {'text': 'CodeThinker', 'label': 'LLM'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.14883,"submitter":"Yu Hui Kellie Sim","authors":"Kellie Yu Hui Sim, Kenny Tsu Wei Choo","title":"Envisioning an AI-Enhanced Mental Health Ecosystem","comments":"5 pages, 0 figures, accepted to the CHI'25 Envisioning the Future of\n  Interactive Health Workshop, to be published in HAL","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The rapid advancement of Large Language Models (LLMs), reasoning models, and\nagentic AI approaches coincides with a growing global mental health crisis,\nwhere increasing demand has not translated into adequate access to professional\nsupport, particularly for underserved populations. This presents a unique\nopportunity for AI to complement human-led interventions, offering scalable and\ncontext-aware support while preserving human connection in this sensitive\ndomain. We explore various AI applications in peer support, self-help\ninterventions, proactive monitoring, and data-driven insights, using a\nhuman-centred approach that ensures AI supports rather than replaces human\ninteraction. However, AI deployment in mental health fields presents challenges\nsuch as ethical concerns, transparency, privacy risks, and risks of\nover-reliance. We propose a hybrid ecosystem where where AI assists but does\nnot replace human providers, emphasising responsible deployment and evaluation.\nWe also present some of our early work and findings in several of these AI\napplications. Finally, we outline future research directions for refining\nAI-enhanced interventions while adhering to ethical and culturally sensitive\nguidelines.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 04:21:38 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Sim', 'Kellie Yu Hui', ''], ['Choo', 'Kenny Tsu Wei', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'ethical concerns', 'label': 'AI Ethics'}, {'text': 'transparency', 'label': 'AI Ethics'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.14887,"submitter":"Hang Li","authors":"Hang Li and Xiao Wang and Bevan Koopman and Guido Zuccon","title":"Pseudo-Relevance Feedback Can Improve Zero-Shot LLM-Based Dense\n  Retrieval","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Pseudo-relevance feedback (PRF) refines queries by leveraging initially\nretrieved documents to improve retrieval effectiveness. In this paper, we\ninvestigate how large language models (LLMs) can facilitate PRF for zero-shot\nLLM-based dense retrieval, extending the recently proposed PromptReps method.\nSpecifically, our approach uses LLMs to extract salient passage features-such\nas keywords and summaries-from top-ranked documents, which are then integrated\ninto PromptReps to produce enhanced query representations. Experiments on\npassage retrieval benchmarks demonstrate that incorporating PRF significantly\nboosts retrieval performance. Notably, smaller rankers with PRF can match the\neffectiveness of larger rankers without PRF, highlighting PRF's potential to\nimprove LLM-driven search while maintaining an efficient balance between\neffectiveness and resource usage.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 04:30:20 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Li', 'Hang', ''], ['Wang', 'Xiao', ''], ['Koopman', 'Bevan', ''], ['Zuccon', 'Guido', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'PRF', 'label': 'Few-shot Learning'}, {'text': 'PromptReps', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'PromptReps', 'label': 'Prompting'}, {'text': 'PRF', 'label': 'Few-shot Learning'}, {'text': 'PRF', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.14891,"submitter":"Honglin Lin","authors":"Honglin Lin, Zhuoshi Pan, Yu Li, Qizhi Pei, Xin Gao, Mengzhang Cai,\n  Conghui He, Lijun Wu","title":"MetaLadder: Ascending Mathematical Solution Quality via\n  Analogical-Problem Reasoning Transfer","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) have demonstrated promising capabilities in\nsolving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as\na vital component in guiding answer generation. Current paradigms typically\ngenerate CoT and answers directly for a given problem, diverging from human\nproblem-solving strategies to some extent. Humans often solve problems by\nrecalling analogous cases and leveraging their solutions to reason about the\ncurrent task. Inspired by this cognitive process, we propose\n\\textbf{MetaLadder}, a novel framework that explicitly prompts LLMs to recall\nand reflect on meta-problems, those structurally or semantically analogous\nproblems, alongside their CoT solutions before addressing the target problem.\nAdditionally, we introduce a problem-restating mechanism to enhance the model's\ncomprehension of the target problem by regenerating the original question,\nwhich further improves reasoning accuracy. Therefore, the model can achieve\nreasoning transfer from analogical problems, mimicking human-like \"learning\nfrom examples\" and generalization abilities. Extensive experiments on\nmathematical benchmarks demonstrate that our MetaLadder significantly boosts\nLLMs' problem-solving accuracy, largely outperforming standard CoT-based\nmethods (\\textbf{10.3\\%} accuracy gain) and other methods. Our code and data\nhas been released at https:\/\/github.com\/LHL3341\/MetaLadder.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 04:36:35 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Lin', 'Honglin', ''], ['Pan', 'Zhuoshi', ''], ['Li', 'Yu', ''], ['Pei', 'Qizhi', ''], ['Gao', 'Xin', ''], ['Cai', 'Mengzhang', ''], ['He', 'Conghui', ''], ['Wu', 'Lijun', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Chain-of-Thought (CoT)', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.14895,"submitter":"Shuo Li","authors":"Shuo Li, Jiajun Sun, Guodong Zheng, Xiaoran Fan, Yujiong Shen, Yi Lu,\n  Zhiheng Xi, Yuming Yang, Wenming Tan, Tao Ji, Tao Gui, Qi Zhang, Xuanjing\n  Huang","title":"Mitigating Object Hallucinations in MLLMs via Multi-Frequency\n  Perturbations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Recently, multimodal large language models (MLLMs) have demonstrated\nremarkable performance in visual-language tasks. However, the authenticity of\nthe responses generated by MLLMs is often compromised by object hallucinations.\nWe identify that a key cause of these hallucinations is the model's\nover-susceptibility to specific image frequency features in detecting objects.\nIn this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,\ncost-effective, and pluggable method that leverages both low-frequency and\nhigh-frequency features of images to perturb visual feature representations and\nexplicitly suppress redundant frequency-domain features during inference,\nthereby mitigating hallucinations. Experimental results demonstrate that our\nmethod significantly mitigates object hallucinations across various model\narchitectures. Furthermore, as a training-time method, MFP can be combined with\ninference-time methods to achieve state-of-the-art performance on the CHAIR\nbenchmark.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 04:39:45 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Li', 'Shuo', ''], ['Sun', 'Jiajun', ''], ['Zheng', 'Guodong', ''], ['Fan', 'Xiaoran', ''], ['Shen', 'Yujiong', ''], ['Lu', 'Yi', ''], ['Xi', 'Zhiheng', ''], ['Yang', 'Yuming', ''], ['Tan', 'Wenming', ''], ['Ji', 'Tao', ''], ['Gui', 'Tao', ''], ['Zhang', 'Qi', ''], ['Huang', 'Xuanjing', '']]","extracted_entities":"[{'text': 'multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"multimodal large language models","similarity_score":0.7649828196}
{"id":2503.149,"submitter":"Estrid He","authors":"Estrid He, Tabinda Sarwar, Ibrahim Khalil, Xun Yi, and Ke Wang","title":"Deep Contrastive Unlearning for Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The past a few years have witnessed the great success of large language\nmodels, demonstrating powerful capabilities in comprehending textual data and\ngenerating human-like languages. Large language models achieve success by being\ntrained on vast amounts of textual data, including online sources with\ncopyrighted content and user-generated knowledge. However, this comes at a\ncost: the potential risk of exposing users' privacy and violating copyright\nprotections. Thus, to safeguard individuals' \"right to be forgotten\", there has\nbeen increasing interests in machine unlearning -- the process of removing\ninformation carried by particular training samples from a model while not\ndeteriorating its predictive quality. This is a challenging task due to the\nblack-box nature of language models. Most existing studies focus on mitigating\nthe impact of those forgot samples upon a model's outputs, and do not\nexplicitly consider the geometric distributions of samples in the latent space\nof a model. To address this issue, we propose a machine unlearning framework,\nnamed Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models.\nOur proposed model achieves machine unlearning by directly optimizing the\nlatent space of a model. Comprehensive experiments on real-world datasets\ndemonstrate the effectiveness and efficiency of DeepCUT with consistent and\nsignificant improvement over baseline methods.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 04:58:45 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['He', 'Estrid', ''], ['Sarwar', 'Tabinda', ''], ['Khalil', 'Ibrahim', ''], ['Yi', 'Xun', ''], ['Wang', 'Ke', '']]","extracted_entities":"[{'text': 'large language\\nmodels', 'label': 'Large Language Model'}, {'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'machine unlearning', 'label': 'Few-shot Learning'}, {'text': 'machine unlearning', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"large language\nmodels","similarity_score":0.9664971828}
{"id":2503.14908,"submitter":"Haoyu Chen","authors":"Haoyu Chen, Xiaojie Xu, Wenbo Li, Jingjing Ren, Tian Ye, Songhua Liu,\n  Ying-Cong Chen, Lei Zhu, Xinchao Wang","title":"POSTA: A Go-to Framework for Customized Artistic Poster Generation","comments":"Accepted to CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GR cs.AI cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Poster design is a critical medium for visual communication. Prior work has\nexplored automatic poster design using deep learning techniques, but these\napproaches lack text accuracy, user customization, and aesthetic appeal,\nlimiting their applicability in artistic domains such as movies and\nexhibitions, where both clear content delivery and visual impact are essential.\nTo address these limitations, we present POSTA: a modular framework powered by\ndiffusion models and multimodal large language models (MLLMs) for customized\nartistic poster generation. The framework consists of three modules. Background\nDiffusion creates a themed background based on user input. Design MLLM then\ngenerates layout and typography elements that align with and complement the\nbackground style. Finally, to enhance the poster's aesthetic appeal, ArtText\nDiffusion applies additional stylization to key text elements. The final result\nis a visually cohesive and appealing poster, with a fully modular process that\nallows for complete customization. To train our models, we develop the\nPosterArt dataset, comprising high-quality artistic posters annotated with\nlayout, typography, and pixel-level stylized text segmentation. Our\ncomprehensive experimental analysis demonstrates POSTA's exceptional\ncontrollability and design diversity, outperforming existing models in both\ntext accuracy and aesthetic quality.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 05:22:38 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Chen', 'Haoyu', ''], ['Xu', 'Xiaojie', ''], ['Li', 'Wenbo', ''], ['Ren', 'Jingjing', ''], ['Ye', 'Tian', ''], ['Liu', 'Songhua', ''], ['Chen', 'Ying-Cong', ''], ['Zhu', 'Lei', ''], ['Wang', 'Xinchao', '']]","extracted_entities":"[{'text': 'multimodal large language models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"multimodal large language models","similarity_score":0.7649828196}
{"id":2503.14932,"submitter":"Ziyao Wang","authors":"Ziyao Wang, Yexiao He, Zheyu Shen, Yu Li, Guoheng Sun, Myungjin Lee,\n  Ang Li","title":"Prada: Black-Box LLM Adaptation with Private Data on\n  Resource-Constrained Devices","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.DC cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In recent years, Large Language Models (LLMs) have demonstrated remarkable\nabilities in various natural language processing tasks. However, adapting these\nmodels to specialized domains using private datasets stored on\nresource-constrained edge devices, such as smartphones and personal computers,\nremains challenging due to significant privacy concerns and limited\ncomputational resources. Existing model adaptation methods either compromise\ndata privacy by requiring data transmission or jeopardize model privacy by\nexposing proprietary LLM parameters. To address these challenges, we propose\nPrada, a novel privacy-preserving and efficient black-box LLM adaptation system\nusing private on-device datasets. Prada employs a lightweight proxy model\nfine-tuned with Low-Rank Adaptation (LoRA) locally on user devices. During\ninference, Prada leverages the logits offset, i.e., difference in outputs\nbetween the base and adapted proxy models, to iteratively refine outputs from a\nremote black-box LLM. This offset-based adaptation approach preserves both data\nprivacy and model privacy, as there is no need to share sensitive data or\nproprietary model parameters. Furthermore, we incorporate speculative decoding\nto further speed up the inference process of Prada, making the system\npractically deployable on bandwidth-constrained edge devices, enabling a more\npractical deployment of Prada. Extensive experiments on various downstream\ntasks demonstrate that Prada achieves performance comparable to centralized\nfine-tuning methods while significantly reducing computational overhead by up\nto 60% and communication costs by up to 80%.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 06:38:51 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Wang', 'Ziyao', ''], ['He', 'Yexiao', ''], ['Shen', 'Zheyu', ''], ['Li', 'Yu', ''], ['Sun', 'Guoheng', ''], ['Lee', 'Myungjin', ''], ['Li', 'Ang', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.14935,"submitter":"Chongjun Tu","authors":"Chongjun Tu, Lin Zhang, Pengtao Chen, Peng Ye, Xianfang Zeng, Wei\n  Cheng, Gang Yu, Tao Chen","title":"FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion\n  Understanding","comments":"FAVOR-Bench project page: https:\/\/favor-bench.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multimodal Large Language Models (MLLMs) have shown remarkable capabilities\nin video content understanding but still struggle with fine-grained motion\ncomprehension. To comprehensively assess the motion understanding ability of\nexisting MLLMs, we introduce FAVOR-Bench, comprising 1,776 videos with\nstructured manual annotations of various motions. Our benchmark includes both\nclose-ended and open-ended tasks. For close-ended evaluation, we carefully\ndesign 8,184 multiple-choice question-answer pairs spanning six distinct\nsub-tasks. For open-ended evaluation, we develop both a novel cost-efficient\nLLM-free and a GPT-assisted caption assessment method, where the former can\nenhance benchmarking interpretability and reproducibility. Comprehensive\nexperiments with 21 state-of-the-art MLLMs reveal significant limitations in\ntheir ability to comprehend and describe detailed temporal dynamics in video\nmotions. To alleviate this limitation, we further build FAVOR-Train, a dataset\nconsisting of 17,152 videos with fine-grained motion annotations. The results\nof finetuning Qwen2.5-VL on FAVOR-Train yield consistent improvements on\nmotion-related tasks of TVBench, MotionBench and our FAVOR-Bench. Comprehensive\nassessment results demonstrate that the proposed FAVOR-Bench and FAVOR-Train\nprovide valuable tools to the community for developing more powerful video\nunderstanding models. Project page:\n\\href{https:\/\/favor-bench.github.io\/}{https:\/\/favor-bench.github.io\/}.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 06:42:32 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Tu', 'Chongjun', ''], ['Zhang', 'Lin', ''], ['Chen', 'Pengtao', ''], ['Ye', 'Peng', ''], ['Zeng', 'Xianfang', ''], ['Cheng', 'Wei', ''], ['Yu', 'Gang', ''], ['Chen', 'Tao', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2503.14939,"submitter":"Tengjin Weng","authors":"Tengjin Weng, Jingyi Wang, Wenhao Jiang and Zhong Ming","title":"VisNumBench: Evaluating Number Sense of Multimodal Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Can Multimodal Large Language Models (MLLMs) develop an intuitive number\nsense similar to humans? Targeting this problem, we introduce Visual Number\nBenchmark (VisNumBench) to evaluate the number sense abilities of MLLMs across\na wide range of visual numerical tasks. VisNumBench consists of about 1,900\nmultiple-choice question-answer pairs derived from both synthetic and\nreal-world visual data, covering seven visual numerical attributes and four\ntypes of visual numerical estimation tasks. Our experiments on VisNumBench led\nto the following key findings: (i) The 17 MLLMs we tested, including\nopen-source models such as Qwen2.5-VL and InternVL2.5, as well as proprietary\nmodels like GPT-4o and Gemini 2.0 Flash, perform significantly below human\nlevels in number sense-related tasks. (ii) Multimodal mathematical models and\nmultimodal chain-of-thought (CoT) models did not exhibit significant\nimprovements in number sense abilities. (iii) Stronger MLLMs with larger\nparameter sizes and broader general abilities demonstrate modest gains in\nnumber sense abilities. We believe VisNumBench will serve as a valuable\nresource for the research community, encouraging further advancements in\nenhancing MLLMs' number sense abilities. All benchmark resources, including\ncode and datasets, will be publicly available at\nhttps:\/\/wwwtttjjj.github.io\/VisNumBench\/.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 07:07:43 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Weng', 'Tengjin', ''], ['Wang', 'Jingyi', ''], ['Jiang', 'Wenhao', ''], ['Ming', 'Zhong', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2503.14941,"submitter":"Qihui Zhang","authors":"Qihui Zhang, Munan Ning, Zheyuan Liu, Yanbo Wang, Jiayi Ye, Yue Huang,\n  Shuo Yang, Xiao Chen, Yibing Song, Li Yuan","title":"UPME: An Unsupervised Peer Review Framework for Multimodal Large\n  Language Model Evaluation","comments":"Accepted by CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Multimodal Large Language Models (MLLMs) have emerged to tackle the\nchallenges of Visual Question Answering (VQA), sparking a new research focus on\nconducting objective evaluations of these models. Existing evaluation methods\nface limitations due to the significant human workload required to design Q&A\npairs for visual images, which inherently restricts the scale and scope of\nevaluations. Although automated MLLM-as-judge approaches attempt to reduce the\nhuman workload through automatic evaluations, they often introduce biases. To\naddress these problems, we propose an Unsupervised Peer review MLLM Evaluation\nframework. It utilizes only image data, allowing models to automatically\ngenerate questions and conduct peer review assessments of answers from other\nmodels, effectively alleviating the reliance on human workload. Additionally,\nwe introduce the vision-language scoring system to mitigate the bias issues,\nwhich focuses on three aspects: (i) response correctness; (ii) visual\nunderstanding and reasoning; and (iii) image-text correlation. Experimental\nresults demonstrate that UPME achieves a Pearson correlation of 0.944 with\nhuman evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,\nindicating that our framework closely aligns with human-designed benchmarks and\ninherent human preferences.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 07:15:41 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhang', 'Qihui', ''], ['Ning', 'Munan', ''], ['Liu', 'Zheyuan', ''], ['Wang', 'Yanbo', ''], ['Ye', 'Jiayi', ''], ['Huang', 'Yue', ''], ['Yang', 'Shuo', ''], ['Chen', 'Xiao', ''], ['Song', 'Yibing', ''], ['Yuan', 'Li', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2503.14948,"submitter":"Hao Liang","authors":"Hao Liang, Zhipeng Dong, Yi Yang, Mengyin Fu","title":"ChatStitch: Visualizing Through Structures via Surround-View\n  Unsupervised Deep Image Stitching with Collaborative LLM-Agents","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.HC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Collaborative perception has garnered significant attention for its ability\nto enhance the perception capabilities of individual vehicles through the\nexchange of information with surrounding vehicle-agents. However, existing\ncollaborative perception systems are limited by inefficiencies in user\ninteraction and the challenge of multi-camera photorealistic visualization. To\naddress these challenges, this paper introduces ChatStitch, the first\ncollaborative perception system capable of unveiling obscured blind spot\ninformation through natural language commands integrated with external digital\nassets. To adeptly handle complex or abstract commands, ChatStitch employs a\nmulti-agent collaborative framework based on Large Language Models. For\nachieving the most intuitive perception for humans, ChatStitch proposes\nSV-UDIS, the first surround-view unsupervised deep image stitching method under\nthe non-global-overlapping condition. We conducted extensive experiments on the\nUDIS-D, MCOV-SLAM open datasets, and our real-world dataset. Specifically, our\nSV-UDIS method achieves state-of-the-art performance on the UDIS-D dataset for\n3, 4, and 5 image stitching tasks, with PSNR improvements of 9%, 17%, and 21%,\nand SSIM improvements of 8%, 18%, and 26%, respectively.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 07:25:21 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Liang', 'Hao', ''], ['Dong', 'Zhipeng', ''], ['Yang', 'Yi', ''], ['Fu', 'Mengyin', '']]","extracted_entities":"[{'text': 'ChatStitch', 'label': 'ChatGPT'}, {'text': 'ChatStitch', 'label': 'ChatGPT'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'ChatStitch', 'label': 'ChatGPT'}, {'text': 'MCOV-SLAM', 'label': 'Open-source LLMs'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.14996,"submitter":"Francesco Maria Molfese","authors":"Francesco Maria Molfese, Luca Moroni, Luca Gioffr\\`e, Alessandro\n  Scir\\`e, Simone Conia and Roberto Navigli","title":"Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM\n  Evaluation in Multiple-Choice Question Answering","comments":"17 pages (9 main), 11 figures, 21 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  One of the most widely used tasks to evaluate Large Language Models (LLMs) is\nMultiple-Choice Question Answering (MCQA). While open-ended question answering\ntasks are more challenging to evaluate, MCQA tasks are, in principle, easier to\nassess, as the model's answer is thought to be simple to extract and is\ndirectly compared to a set of predefined choices. However, recent studies have\nstarted to question the reliability of MCQA evaluation, showing that multiple\nfactors can significantly impact the reported performance of LLMs, especially\nwhen the model generates free-form text before selecting one of the answer\nchoices. In this work, we shed light on the inconsistencies of MCQA evaluation\nstrategies, which can lead to inaccurate and misleading model comparisons. We\nsystematically analyze whether existing answer extraction methods are aligned\nwith human judgment, and how they are influenced by answer constraints in the\nprompt across different domains. Our experiments demonstrate that traditional\nevaluation strategies often underestimate LLM capabilities, while LLM-based\nanswer extractors are prone to systematic errors. Moreover, we reveal a\nfundamental trade-off between including format constraints in the prompt to\nsimplify answer extraction and allowing models to generate free-form text to\nimprove reasoning. Our findings call for standardized evaluation methodologies\nand highlight the need for more reliable and consistent MCQA evaluation\npractices.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 08:45:03 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Molfese', 'Francesco Maria', ''], ['Moroni', 'Luca', ''], ['Gioffr\u00e8', 'Luca', ''], ['Scir\u00e8', 'Alessandro', ''], ['Conia', 'Simone', ''], ['Navigli', 'Roberto', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'LLM-based\\nanswer extractors', 'label': 'LLM-based'}, {'text': 'prompt', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15003,"submitter":"Amr Keleg","authors":"Amr Keleg","title":"LLM Alignment for the Arabs: A Homogenous Culture or Diverse Ones?","comments":"Accepted to the C3NLP workshop (Co-located with NAACL 2025)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models (LLMs) have the potential of being useful tools that\ncan automate tasks and assist humans. However, these models are more fluent in\nEnglish and more aligned with Western cultures, norms, and values.\nArabic-specific LLMs are being developed to better capture the nuances of the\nArabic language, as well as the views of the Arabs. Yet, Arabs are sometimes\nassumed to share the same culture. In this position paper, I discuss the\nlimitations of this assumption and provide preliminary thoughts for how to\nbuild systems that can better represent the cultural diversity within the Arab\nworld. The invalidity of the cultural homogeneity assumption might seem\nobvious, yet, it is widely adopted in developing multilingual and\nArabic-specific LLMs. I hope that this paper will encourage the NLP community\nto be considerate of the cultural diversity within various communities speaking\nthe same language.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 08:52:59 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Keleg', 'Amr', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.15019,"submitter":"Shengqiong Wu","authors":"Shengqiong Wu and Hao Fei and Jingkang Yang and Xiangtai Li and\n  Juncheng Li and Hanwang Zhang and Tat-seng Chua","title":"Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene","comments":"CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  The latest emerged 4D Panoptic Scene Graph (4D-PSG) provides an advanced-ever\nrepresentation for comprehensively modeling the dynamic 4D visual real world.\nUnfortunately, current pioneering 4D-PSG research can primarily suffer from\ndata scarcity issues severely, as well as the resulting out-of-vocabulary\nproblems; also, the pipeline nature of the benchmark generation method can lead\nto suboptimal performance. To address these challenges, this paper investigates\na novel framework for 4D-PSG generation that leverages rich 2D visual scene\nannotations to enhance 4D scene learning. First, we introduce a 4D Large\nLanguage Model (4D-LLM) integrated with a 3D mask decoder for end-to-end\ngeneration of 4D-PSG. A chained SG inference mechanism is further designed to\nexploit LLMs' open-vocabulary capabilities to infer accurate and comprehensive\nobject and relation labels iteratively. Most importantly, we propose a 2D-to-4D\nvisual scene transfer learning framework, where a spatial-temporal scene\ntranscending strategy effectively transfers dimension-invariant features from\nabundant 2D SG annotations to 4D scenes, effectively compensating for data\nscarcity in 4D-PSG. Extensive experiments on the benchmark data demonstrate\nthat we strikingly outperform baseline models by a large margin, highlighting\nthe effectiveness of our method.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 09:16:08 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Wu', 'Shengqiong', ''], ['Fei', 'Hao', ''], ['Yang', 'Jingkang', ''], ['Li', 'Xiangtai', ''], ['Li', 'Juncheng', ''], ['Zhang', 'Hanwang', ''], ['Chua', 'Tat-seng', '']]","extracted_entities":"[{'text': '4D Large\\nLanguage Model', 'label': 'Large Language Model'}, {'text': '4D-LLM', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"4D Large\nLanguage Model","similarity_score":0.8122194409}
{"id":2503.15044,"submitter":"Haoyi Li","authors":"Haoyi Li, Angela Yifei Yuan, Soyeon Caren Han, Christopher Leckie","title":"SPADE: Systematic Prompt Framework for Automated Dialogue Expansion in\n  Machine-Generated Text Detection","comments":"9 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The increasing capability of large language models (LLMs) to generate\nsynthetic content has heightened concerns about their misuse, driving the\ndevelopment of Machine-Generated Text (MGT) detection models. However, these\ndetectors face significant challenges due to the lack of systematically\ngenerated, high-quality datasets for training. To address this issue, we\npropose five novel data augmentation frameworks for synthetic user dialogue\ngeneration through a structured prompting approach, reducing the costs\nassociated with traditional data collection methods. Our proposed method yields\n14 new dialogue datasets, which we benchmark against seven MGT detection\nmodels. The results demonstrate improved generalization performance when\nutilizing a mixed dataset produced by our proposed augmentation framework.\nFurthermore, considering that real-world agents lack knowledge of future\nopponent utterances, we simulate online dialogue detection and examine the\nrelationship between chat history length and detection accuracy. We also\nbenchmark online detection performance with limited chat history on our\nframeworks. Our open-source datasets can be downloaded from\nhttps:\/\/github.com\/AngieYYF\/SPADE-customer-service-dialogue.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 09:32:52 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Li', 'Haoyi', ''], ['Yuan', 'Angela Yifei', ''], ['Han', 'Soyeon Caren', ''], ['Leckie', 'Christopher', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'structured prompting approach', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.15055,"submitter":"Arina Razmyslovich","authors":"Arina Razmyslovich, Kseniia Murasheva, Sofia Sedlova, Julien\n  Capitaine, Eugene Dmitriev","title":"ELTEX: A Framework for Domain-Driven Synthetic Data Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\nfor generating high-quality synthetic training data in specialized domains.\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\ntheir performance in specialized domains like cybersecurity remains limited by\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\nby systematically integrating explicit domain indicator extraction with dynamic\nprompting to preserve critical domain knowledge throughout the generation\nprocess. We demonstrate ELTEX's effectiveness in the context of\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\nvarious combinations of real and ELTEX-generated data. Our results show that\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\nboth standard classification metrics and uncertainty calibration, while\nrequiring significantly fewer computational resources. We release a curated\nsynthetic dataset of social media texts for cyberattack detection in\nblockchain. Our work demonstrates that domain-driven synthetic data generation\ncan effectively bridge the performance gap between resource-efficient models\nand larger architectures in specialized domains.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 09:46:54 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Razmyslovich', 'Arina', ''], ['Murasheva', 'Kseniia', ''], ['Sedlova', 'Sofia', ''], ['Capitaine', 'Julien', ''], ['Dmitriev', 'Eugene', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'dynamic\\nprompting', 'label': 'Prompting'}, {'text': 'GPT-4', 'label': 'GPT'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15091,"submitter":"Yao Cheng","authors":"Yao Cheng, Zhe Han, Fengyang Jiang, Huaizhen Wang, Fengyu Zhou,\n  Qingshan Yin, Lei Wei","title":"Intelligent Spatial Perception by Building Hierarchical 3D Scene Graphs\n  for Indoor Scenarios with the Help of LLMs","comments":"accepted by WRC SARA 2024","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper addresses the high demand in advanced intelligent robot navigation\nfor a more holistic understanding of spatial environments, by introducing a\nnovel system that harnesses the capabilities of Large Language Models (LLMs) to\nconstruct hierarchical 3D Scene Graphs (3DSGs) for indoor scenarios. The\nproposed framework constructs 3DSGs consisting of a fundamental layer with rich\nmetric-semantic information, an object layer featuring precise point-cloud\nrepresentation of object nodes as well as visual descriptors, and higher layers\nof room, floor, and building nodes. Thanks to the innovative application of\nLLMs, not only object nodes but also nodes of higher layers, e.g., room nodes,\nare annotated in an intelligent and accurate manner. A polling mechanism for\nroom classification using LLMs is proposed to enhance the accuracy and\nreliability of the room node annotation. Thorough numerical experiments\ndemonstrate the system's ability to integrate semantic descriptions with\ngeometric data, creating an accurate and comprehensive representation of the\nenvironment instrumental for context-aware navigation and task planning.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 10:40:28 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Cheng', 'Yao', ''], ['Han', 'Zhe', ''], ['Jiang', 'Fengyang', ''], ['Wang', 'Huaizhen', ''], ['Zhou', 'Fengyu', ''], ['Yin', 'Qingshan', ''], ['Wei', 'Lei', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15108,"submitter":"Mohamed Salim Aissi","authors":"Mohamed Salim Aissi, Clemence Grislain, Mohamed Chetouani, Olivier\n  Sigaud, Laure Soulier, Nicolas Thome","title":"VIPER: Visual Perception and Explainable Reasoning for Sequential\n  Decision-Making","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While Large Language Models (LLMs) excel at reasoning on text and\nVision-Language Models (VLMs) are highly effective for visual perception,\napplying those models for visual instruction-based planning remains a widely\nopen problem. In this paper, we introduce VIPER, a novel framework for\nmultimodal instruction-based planning that integrates VLM-based perception with\nLLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM\ngenerates textual descriptions of image observations, which are then processed\nby an LLM policy to predict actions based on the task goal. We fine-tune the\nreasoning module using behavioral cloning and reinforcement learning, improving\nour agent's decision-making capabilities. Experiments on the ALFWorld benchmark\nshow that VIPER significantly outperforms state-of-the-art visual\ninstruction-based planners while narrowing the gap with purely text-based\noracles. By leveraging text as an intermediate representation, VIPER also\nenhances explainability, paving the way for a fine-grained analysis of\nperception and reasoning components.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:05:42 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Aissi', 'Mohamed Salim', ''], ['Grislain', 'Clemence', ''], ['Chetouani', 'Mohamed', ''], ['Sigaud', 'Olivier', ''], ['Soulier', 'Laure', ''], ['Thome', 'Nicolas', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'behavioral cloning', 'label': 'Few-shot Learning'}, {'text': 'reinforcement learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15112,"submitter":"Wenji Fang","authors":"Shang Liu, Yao Lu, Wenji Fang, Mengming Li, Zhiyao Xie","title":"OpenLLM-RTL: Open Dataset and Benchmark for LLM-Aided Design RTL\n  Generation","comments":"ICCAD'24","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AR","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  The automated generation of design RTL based on large language model (LLM)\nand natural language instructions has demonstrated great potential in agile\ncircuit design. However, the lack of datasets and benchmarks in the public\ndomain prevents the development and fair evaluation of LLM solutions. This\npaper highlights our latest advances in open datasets and benchmarks from three\nperspectives: (1) RTLLM 2.0, an updated benchmark assessing LLM's capability in\ndesign RTL generation. The benchmark is augmented to 50 hand-crafted designs.\nEach design provides the design description, test cases, and a correct RTL\ncode. (2) AssertEval, an open-source benchmark assessing the LLM's assertion\ngeneration capabilities for RTL verification. The benchmark includes 18\ndesigns, each providing specification, signal definition, and correct RTL code.\n(3) RTLCoder-Data, an extended open-source dataset with 80K instruction-code\ndata samples. Moreover, we propose a new verification-based method to verify\nthe functionality correctness of training data samples. Based on this\ntechnique, we further release a dataset with 7K verified high-quality samples.\nThese three studies are integrated into one framework, providing off-the-shelf\nsupport for the development and evaluation of LLMs for RTL code generation and\nverification. Finally, extensive experiments indicate that LLM performance can\nbe boosted by enlarging the training dataset, improving data quality, and\nimproving the training scheme.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:12:53 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Liu', 'Shang', ''], ['Lu', 'Yao', ''], ['Fang', 'Wenji', ''], ['Li', 'Mengming', ''], ['Xie', 'Zhiyao', '']]","extracted_entities":"[{'text': 'large language model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language model","similarity_score":1.0}
{"id":2503.15113,"submitter":"Benjamin Estermann","authors":"Benjamin Estermann and Roger Wattenhofer","title":"Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs","comments":"Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) have demonstrated remarkable text generation\ncapabilities, and recent advances in training paradigms have led to\nbreakthroughs in their reasoning performance. In this work, we investigate how\nthe reasoning effort of such models scales with problem complexity. We use the\ninfinitely scalable Tents puzzle, which has a known linear-time solution, to\nanalyze this scaling behavior. Our results show that reasoning effort scales\nwith problem size, but only up to a critical problem complexity. Beyond this\nthreshold, the reasoning effort does not continue to increase, and may even\ndecrease. This observation highlights a critical limitation in the logical\ncoherence of current LLMs as problem complexity increases, and underscores the\nneed for strategies to improve reasoning scalability. Furthermore, our results\nreveal significant performance differences between current state-of-the-art\nreasoning models when faced with increasingly complex logical puzzles.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:13:51 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Estermann', 'Benjamin', ''], ['Wattenhofer', 'Roger', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15117,"submitter":"Shichen Li","authors":"Shichen Li, Zhongqing Wang, Zheyu Zhao, Yue Zhang, Peifeng Li","title":"Exploring Model Editing for LLM-based Aspect-Based Sentiment\n  Classification","comments":"AAAI2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Model editing aims at selectively updating a small subset of a neural model's\nparameters with an interpretable strategy to achieve desired modifications. It\ncan significantly reduce computational costs to adapt to large language models\n(LLMs). Given its ability to precisely target critical components within LLMs,\nmodel editing shows great potential for efficient fine-tuning applications. In\nthis work, we investigate model editing to serve an efficient method for\nadapting LLMs to solve aspect-based sentiment classification. Through causal\ninterventions, we trace and determine which neuron hidden states are essential\nfor the prediction of the model. By performing interventions and restorations\non each component of an LLM, we identify the importance of these components for\naspect-based sentiment classification. Our findings reveal that a distinct set\nof mid-layer representations is essential for detecting the sentiment polarity\nof given aspect words. Leveraging these insights, we develop a model editing\napproach that focuses exclusively on these critical parts of the LLM, leading\nto a more efficient method for adapting LLMs. Our in-domain and out-of-domain\nexperiments demonstrate that this approach achieves competitive results\ncompared to the currently strongest methods with significantly fewer trainable\nparameters, highlighting a more efficient and interpretable fine-tuning\nstrategy.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:21:37 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Li', 'Shichen', ''], ['Wang', 'Zhongqing', ''], ['Zhao', 'Zheyu', ''], ['Zhang', 'Yue', ''], ['Li', 'Peifeng', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.15126,"submitter":"Haoyu Ji","authors":"Haoyu Ji, Bowen Chen, Weihong Ren, Wenze Huang, Zhihao Yang, Zhiyong\n  Wang, and Honghai Liu","title":"Text-Derived Relational Graph-Enhanced Network for Skeleton-Based Action\n  Segmentation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Skeleton-based Temporal Action Segmentation (STAS) aims to segment and\nrecognize various actions from long, untrimmed sequences of human skeletal\nmovements. Current STAS methods typically employ spatio-temporal modeling to\nestablish dependencies among joints as well as frames, and utilize one-hot\nencoding with cross-entropy loss for frame-wise classification supervision.\nHowever, these methods overlook the intrinsic correlations among joints and\nactions within skeletal features, leading to a limited understanding of human\nmovements. To address this, we propose a Text-Derived Relational Graph-Enhanced\nNetwork (TRG-Net) that leverages prior graphs generated by Large Language\nModels (LLM) to enhance both modeling and supervision. For modeling, the\nDynamic Spatio-Temporal Fusion Modeling (DSFM) method incorporates Text-Derived\nJoint Graphs (TJG) with channel- and frame-level dynamic adaptation to\neffectively model spatial relations, while integrating spatio-temporal core\nfeatures during temporal modeling. For supervision, the Absolute-Relative\nInter-Class Supervision (ARIS) method employs contrastive learning between\naction features and text embeddings to regularize the absolute class\ndistributions, and utilizes Text-Derived Action Graphs (TAG) to capture the\nrelative inter-class relationships among action features. Additionally, we\npropose a Spatial-Aware Enhancement Processing (SAEP) method, which\nincorporates random joint occlusion and axial rotation to enhance spatial\ngeneralization. Performance evaluations on four public datasets demonstrate\nthat TRG-Net achieves state-of-the-art results.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:38:14 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Ji', 'Haoyu', ''], ['Chen', 'Bowen', ''], ['Ren', 'Weihong', ''], ['Huang', 'Wenze', ''], ['Yang', 'Zhihao', ''], ['Wang', 'Zhiyong', ''], ['Liu', 'Honghai', '']]","extracted_entities":"[{'text': 'Large Language\\nModels', 'label': 'Large Language Model'}, {'text': 'Text-Derived\\nJoint Graphs', 'label': 'Embedding'}, {'text': 'contrastive learning', 'label': 'Few-shot Learning'}, {'text': 'text embeddings', 'label': 'Embedding'}, {'text': 'Text-Derived Action Graphs', 'label': 'Embedding'}, {'text': 'four public datasets', 'label': 'Open-source LLMs'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language\nModels","similarity_score":0.9664971828}
{"id":2503.15176,"submitter":"Navya Sonal Agarwal","authors":"Navya Sonal Agarwal and Sanjay Kumar Sonbhadra","title":"A Review on Large Language Models for Visual Analytics","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC cs.CL cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper provides a comprehensive review of the integration of Large\nLanguage Models (LLMs) with visual analytics, addressing their foundational\nconcepts, capabilities, and wide-ranging applications. It begins by outlining\nthe theoretical underpinnings of visual analytics and the transformative\npotential of LLMs, specifically focusing on their roles in natural language\nunderstanding, natural language generation, dialogue systems, and text-to-media\ntransformations. The review further investigates how the synergy between LLMs\nand visual analytics enhances data interpretation, visualization techniques,\nand interactive exploration capabilities. Key tools and platforms including\nLIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized\nmultimodal models such as ChartLlama and CharXIV, are critically evaluated. The\npaper discusses their functionalities, strengths, and limitations in supporting\ndata exploration, visualization enhancement, automated reporting, and insight\nextraction. The taxonomy of LLM tasks, ranging from natural language\nunderstanding (NLU), natural language generation (NLG), to dialogue systems and\ntext-to-media transformations, is systematically explored. This review provides\na SWOT analysis of integrating Large Language Models (LLMs) with visual\nanalytics, highlighting strengths like accessibility and flexibility,\nweaknesses such as computational demands and biases, opportunities in\nmultimodal integration and user collaboration, and threats including privacy\nconcerns and skill degradation. It emphasizes addressing ethical considerations\nand methodological improvements for effective integration.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 13:02:01 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Agarwal', 'Navya Sonal', ''], ['Sonbhadra', 'Sanjay Kumar', '']]","extracted_entities":"[{'text': 'Large\\nLanguage Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LIDA', 'label': 'Open-source LLMs'}, {'text': 'Julius AI', 'label': 'Open-source LLMs'}, {'text': 'Zoho Analytics', 'label': 'Open-source LLMs'}, {'text': 'ChartLlama', 'label': 'Llama'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'privacy\\nconcerns', 'label': 'AI Ethics'}, {'text': 'ethical considerations', 'label': 'AI Ethics'}]","assigned_concept":"Large Language Model","matched_keyword":"Large\nLanguage Models","similarity_score":0.9664971828}
{"id":2503.15191,"submitter":"Hyunjun Kim He","authors":"Sejong Kim, Hyunseo Song, Hyunwoo Seo, Hyunjun Kim","title":"Optimizing Retrieval Strategies for Financial Question Answering\n  Documents in Retrieval-Augmented Generation Systems","comments":"15 pages, 3 figures, 11 tables. Accepted at ICLR 2025 Workshop on\n  Advances in Financial AI. Code available at\n  https:\/\/github.com\/seohyunwoo-0407\/GAR","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Retrieval-Augmented Generation (RAG) has emerged as a promising framework to\nmitigate hallucinations in Large Language Models (LLMs), yet its overall\nperformance is dependent on the underlying retrieval system. In the finance\ndomain, documents such as 10-K reports pose distinct challenges due to\ndomain-specific vocabulary and multi-hierarchical tabular data. In this work,\nwe introduce an efficient, end-to-end RAG pipeline that enhances retrieval for\nfinancial documents through a three-phase approach: pre-retrieval, retrieval,\nand post-retrieval. In the pre-retrieval phase, various query and corpus\npreprocessing techniques are employed to enrich input data. During the\nretrieval phase, we fine-tuned state-of-the-art (SOTA) embedding models with\ndomain-specific knowledge and implemented a hybrid retrieval strategy that\ncombines dense and sparse representations. Finally, the post-retrieval phase\nleverages Direct Preference Optimization (DPO) training and document selection\nmethods to further refine the results. Evaluations on seven financial question\nanswering datasets-FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA,\nand MultiHiertt-demonstrate substantial improvements in retrieval performance,\nleading to more accurate and contextually appropriate generation. These\nfindings highlight the critical role of tailored retrieval techniques in\nadvancing the effectiveness of RAG systems for financial applications. A fully\nreplicable pipeline is available on GitHub:\nhttps:\/\/github.com\/seohyunwoo-0407\/GAR.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 13:21:49 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Kim', 'Sejong', ''], ['Song', 'Hyunseo', ''], ['Seo', 'Hyunwoo', ''], ['Kim', 'Hyunjun', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15195,"submitter":"Giorgia Crosilla","authors":"Giorgia Crosilla, Lukas Klic and Giovanni Colavizza","title":"Benchmarking Large Language Models for Handwritten Text Recognition","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 13:33:29 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 15:49:10 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Crosilla', 'Giorgia', ''], ['Klic', 'Lukas', ''], ['Colavizza', 'Giovanni', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'zero-shot settings', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2503.15248,"submitter":"Nathalia Nascimento","authors":"Jomar Thomas Almonte, Santhosh Anitha Boominathan, Nathalia Nascimento","title":"Automated Non-Functional Requirements Generation in Software Engineering\n  with Large Language Models: A Comparative Study","comments":"11 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Neglecting non-functional requirements (NFRs) early in software development\ncan lead to critical challenges. Despite their importance, NFRs are often\noverlooked or difficult to identify, impacting software quality. To support\nrequirements engineers in eliciting NFRs, we developed a framework that\nleverages Large Language Models (LLMs) to derive quality-driven NFRs from\nfunctional requirements (FRs). Using a custom prompting technique within a\nDeno-based pipeline, the system identifies relevant quality attributes for each\nfunctional requirement and generates corresponding NFRs, aiding systematic\nintegration. A crucial aspect is evaluating the quality and suitability of\nthese generated requirements. Can LLMs produce high-quality NFR suggestions?\nUsing 34 functional requirements - selected as a representative subset of 3,964\nFRs-the LLMs inferred applicable attributes based on the ISO\/IEC 25010:2023\nstandard, generating 1,593 NFRs. A horizontal evaluation covered three\ndimensions: NFR validity, applicability of quality attributes, and\nclassification precision. Ten industry software quality evaluators, averaging\n13 years of experience, assessed a subset for relevance and quality. The\nevaluation showed strong alignment between LLM-generated NFRs and expert\nassessments, with median validity and applicability scores of 5.0 (means: 4.63\nand 4.59, respectively) on a 1-5 scale. In the classification task, 80.4% of\nLLM-assigned attributes matched expert choices, with 8.3% near misses and 11.3%\nmismatches. A comparative analysis of eight LLMs highlighted variations in\nperformance, with gemini-1.5-pro exhibiting the highest attribute accuracy,\nwhile llama-3.3-70B achieved higher validity and applicability scores. These\nfindings provide insights into the feasibility of using LLMs for automated NFR\ngeneration and lay the foundation for further exploration of AI-assisted\nrequirements engineering.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 14:23:22 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Almonte', 'Jomar Thomas', ''], ['Boominathan', 'Santhosh Anitha', ''], ['Nascimento', 'Nathalia', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'custom prompting technique', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15301,"submitter":"Huanyu Liu","authors":"Jia Li, Hao Zhu, Huanyu Liu, Xianjie Shi, He Zong, Yihong Dong, Kechi\n  Zhang, Siyuan Jiang, Zhi Jin, Ge Li","title":"aiXcoder-7B-v2: Training LLMs to Fully Utilize the Long Context in\n  Repository-level Code Completion","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Repository-level code completion aims to complete code based on the long\ncontexts of the repository. Existing studies extract long contexts from the\nrepository as inputs and leverage Large Language Models (LLMs) to generate\ncode. However, we reveal a severe limitation of LLMs, i.e., LLMs may ignore the\ninformation within long contexts in code completion. In other words, even the\ncontexts contain useful information (e.g., relevant APIs or similar code), LLMs\nmay fail to utilize this information. We think this limitation is caused by an\ninherent bias in LLMs, i.e., relying on nearby contexts and ignoring long-range\ncontexts. To address this, we propose a novel fine-tuning approach named CoLT.\nThe core idea of CoLT is to provide explicit supervision signals, which\nemphasize that long-range contexts may hold relevant information. Specifically,\nCoLT proposes a reinforcement learning-based training, which explicitly\nencourages models to utilize the information within long contexts and punishes\nmodels for ignoring long contexts. To support CoLT, we release CoLT-132K, a\nlarge-scale dataset with 132k samples across four languages, each containing\nlong-context inputs. We apply CoLT to a popular LLM - aiXcoder-7B and release\naiXcoder-7B-v2. We conduct extensive experiments on CoLT-132K and a public\nbenchmark - CrossCodeEval. Our experiments yield the results: 1. Effectiveness.\nCoLT substantially improves aiXcoder-7B. aiXcoder-7B-v2 outperforms aiXcoder-7B\nby up to 44% in exact match. aiXcoder-7B-v2 becomes the state-of-the-art 7B\nmodel in code completion and even surpasses larger models. 2. Generalizability.\nThe capability learned by CoLT can generalize to new languages. Besides, CoLT\nis model-agnostic and effectively improves multiple LLMs. 3. Enhanced Context\nUtilization Capability. CoLT significantly improves the capability of LLMs in\nutilizing the relevant information within long contexts.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:22:58 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Li', 'Jia', ''], ['Zhu', 'Hao', ''], ['Liu', 'Huanyu', ''], ['Shi', 'Xianjie', ''], ['Zong', 'He', ''], ['Dong', 'Yihong', ''], ['Zhang', 'Kechi', ''], ['Jiang', 'Siyuan', ''], ['Jin', 'Zhi', ''], ['Li', 'Ge', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'CoLT', 'label': 'Fine-tuning'}, {'text': 'reinforcement learning-based training', 'label': 'Few-shot Learning'}, {'text': 'CoLT', 'label': 'Fine-tuning'}, {'text': 'CrossCodeEval', 'label': 'Open-source LLMs'}, {'text': 'CoLT', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'CoLT', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15338,"submitter":"Junyi Ao","authors":"Junyi Ao, Dekun Chen, Xiaohai Tian, Wenjie Feng, Jun Zhang, Lu Lu,\n  Yuxuan Wang, Haizhou Li, Zhizheng Wu","title":"Solla: Towards a Speech-Oriented LLM That Hears Acoustic Context","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.AS cs.CL cs.SD","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) have recently shown remarkable ability to\nprocess not only text but also multimodal inputs such as speech and audio.\nHowever, most existing models primarily focus on analyzing input signals using\ntext instructions, overlooking scenarios in which speech instructions and audio\nare mixed and serve as inputs to the model. To address these challenges, we\nintroduce Solla, a novel framework designed to understand speech-based\nquestions and hear the acoustic context concurrently. Solla incorporates an\naudio tagging module to effectively identify and represent audio events, as\nwell as an ASR-assisted prediction method to improve comprehension of spoken\ncontent. To rigorously evaluate Solla and other publicly available models, we\npropose a new benchmark dataset called SA-Eval, which includes three tasks:\naudio event classification, audio captioning, and audio question answering.\nSA-Eval has diverse speech instruction with various speaking styles,\nencompassing two difficulty levels, easy and hard, to capture the range of\nreal-world acoustic conditions. Experimental results show that Solla performs\non par with or outperforms baseline models on both the easy and hard test sets,\nunderscoring its effectiveness in jointly understanding speech and audio.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:34:21 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Ao', 'Junyi', ''], ['Chen', 'Dekun', ''], ['Tian', 'Xiaohai', ''], ['Feng', 'Wenjie', ''], ['Zhang', 'Jun', ''], ['Lu', 'Lu', ''], ['Wang', 'Yuxuan', ''], ['Li', 'Haizhou', ''], ['Wu', 'Zhizheng', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Solla', 'label': 'Llama'}, {'text': 'Solla', 'label': 'Llama'}, {'text': 'Solla', 'label': 'Llama'}, {'text': 'Solla', 'label': 'Llama'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15341,"submitter":"Yuqi Zhu","authors":"Yuqi Zhu, Ge Li, Xue Jiang, Jia Li, Hong Mei, Zhi Jin, Yihong Dong","title":"Uncertainty-Guided Chain-of-Thought for Code Generation with LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Chain-of-Thought (CoT) reasoning has been demonstrated as an effective\ntechnique for improving the problem-solving capabilities of large language\nmodels (LLMs) in the context of code generation. However, existing CoT methods\noften exhibit a tendency toward \"overthinking\", where the LLM consistently\napplies reasoning strategies without adequately considering the task's\nunderlying complexity. This results in the LLMs allocating excessive\ncomputational resources, in terms of tokens, to relatively simple tasks or\nproblems where the correct answer is already evident. Additionally, this\noverthinking may lead LLMs down incorrect reasoning paths, resulting in\nincorrect code generation. In this paper, we introduce UnCertainty-Aware\nChain-of-Thought (UnCert-CoT), an LLM-based approach designed to enhance code\ngeneration by incorporating an uncertainty-aware CoT reasoning mechanism, which\nfocuses computational resources on targeting points where LLMs are more prone\nto error. We propose two confidence-based uncertainty measures: Entropy-based\nand Probability Differential-based methods. When uncertainty is high,\nUnCert-CoT activates CoT-decoding to generate multiple reasoning paths and\nselects the final code that exhibits the highest likelihood of correctness. In\ncontrast, LLM directly generates the code when uncertainty is low. This\nuncertainty judgment mechanism allows LLMs to prioritize complex tasks and\navoid unnecessary steps in simpler cases, thereby improving overall efficiency\nand accuracy in code generation. Our experimental results demonstrate that\nUnCert-CoT significantly enhances code generation accuracy on challenging\nbenchmark MHPP(Mostly Hard Python Problems), it achieves improvements up to\n6.1% on PassRate accuracy, particularly in situations where traditional LLMs\nare prone to errors.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:40:45 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhu', 'Yuqi', ''], ['Li', 'Ge', ''], ['Jiang', 'Xue', ''], ['Li', 'Jia', ''], ['Mei', 'Hong', ''], ['Jin', 'Zhi', ''], ['Dong', 'Yihong', '']]","extracted_entities":"[{'text': 'Chain-of-Thought', 'label': 'Chain of thought'}, {'text': 'large language\\nmodels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'UnCert-CoT', 'label': 'LLM-based'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'UnCert-CoT', 'label': 'LLM-based'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language\nmodels","similarity_score":0.9664971828}
{"id":2503.15358,"submitter":"Thomas Pickard","authors":"Thomas Pickard, Aline Villavicencio, Maggie Mi, Wei He, Dylan Phelps,\n  Carolina Scarton, Marco Idiart","title":"SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity\n  Representation","comments":"Preprint; SemEval-2025 proceedings to appear at ACL 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 15:58:46 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Pickard', 'Thomas', ''], ['Villavicencio', 'Aline', ''], ['Mi', 'Maggie', ''], ['He', 'Wei', ''], ['Phelps', 'Dylan', ''], ['Scarton', 'Carolina', ''], ['Idiart', 'Marco', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15369,"submitter":"Yinan Liang","authors":"Yinan Liang, Ziwei Wang, Xiuwei Xu, Jie Zhou, Jiwen Lu","title":"EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language\n  Models","comments":"Accepted by CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While multimodal large language models demonstrate strong performance in\ncomplex reasoning tasks, they pose significant challenges related to model\ncomplexity during deployment, especially for resource-limited devices. In this\npaper, we propose an automatic pruning method for large vision-language models\nto enhance the efficiency of multimodal reasoning. Conventional methods rely on\nthe training data of the original model to select the proper pruning ratio for\ndifferent network components. However, these methods are impractical for large\nvision-language models due to the unaffordable search costs caused by web-scale\ntraining corpus. In contrast, our approach only leverages a small number of\nsamples to search for the desired pruning policy by maximizing its\ngeneralization ability on unknown training data while maintaining the model\naccuracy, which enables the achievement of an optimal trade-off between\naccuracy and efficiency for large visual language models. Specifically, we\nformulate the generalization gap of the pruning strategy using the structural\nrisk minimization principle. Based on both task performance and generalization\ncapability, we iteratively search for the optimal pruning policy within a given\nsearch space and optimize the vision projector to evolve the search space with\nhigher upper bound of performance. We conduct extensive experiments on the\nScienceQA, Vizwiz, MM-vet, and LLaVA-Bench datasets for the task of visual\nquestion answering. Using only 64 samples for pruning policy search,\nEfficientLLaVA achieves an accuracy of 83.05% on ScienceQA, along with a\n$\\times$ 1.8 speedup compared to the dense LLaVA-v1.5-7B model.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:07:04 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Liang', 'Yinan', ''], ['Wang', 'Ziwei', ''], ['Xu', 'Xiuwei', ''], ['Zhou', 'Jie', ''], ['Lu', 'Jiwen', '']]","extracted_entities":"[{'text': 'large\\nvision-language models', 'label': 'Large Language Model'}, {'text': 'ScienceQA', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large\nvision-language models","similarity_score":0.7742220759}
{"id":2503.15426,"submitter":"Wei Tang","authors":"Wei Tang, Yanpeng Sun, Qinying Gu, Zechao Li","title":"Visual Position Prompt for MLLM based Visual Grounding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Although Multimodal Large Language Models (MLLMs) excel at various\nimage-related tasks, they encounter challenges in precisely aligning\ncoordinates with spatial information within images, particularly in\nposition-aware tasks such as visual grounding. This limitation arises from two\nkey factors. First, MLLMs lack explicit spatial references, making it difficult\nto associate textual descriptions with precise image locations. Second, their\nfeature extraction processes prioritize global context over fine-grained\nspatial details, leading to weak localization capability. To address this\nissue, we introduce VPP-LLaVA, an MLLM equipped with Visual Position Prompt\n(VPP) to improve its grounding capability. VPP-LLaVA integrates two\ncomplementary mechanisms. The global VPP overlays learnable, axis-like\nembeddings onto the input image to provide structured spatial cues. The local\nVPP focuses on fine-grained localization by incorporating position-aware\nqueries, which suggests probable object locations. We also introduce a VPP-SFT\ndataset with 0.6M samples, consolidating high-quality visual grounding data\ninto a compact format for efficient model training. Training on this dataset\nwith VPP enhances the model's performance, achieving state-of-the-art results\non standard grounding benchmarks despite using fewer training samples compared\nto other MLLMs like MiniGPT-v2, which rely on much larger datasets ($\\sim$21M\nsamples). The code and VPP-SFT dataset will be available at\nhttps:\/\/github.com\/WayneTomas\/VPP-LLaVA upon acceptance.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:08:13 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Tang', 'Wei', ''], ['Sun', 'Yanpeng', ''], ['Gu', 'Qinying', ''], ['Li', 'Zechao', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'Visual Position Prompt', 'label': 'Prompting'}, {'text': 'VPP', 'label': 'contextual Embedding'}, {'text': 'axis-like\\nembeddings', 'label': 'contextual Embedding'}, {'text': 'VPP', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2503.15463,"submitter":"Jianan Li","authors":"Jia-Nan Li, Jian Guan, Songhao Wu, Wei Wu, Rui Yan","title":"From 1,000,000 Users to Every User: Scaling Up Personalized Preference\n  for User-level Alignment","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:41:46 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Li', 'Jia-Nan', ''], ['Guan', 'Jian', ''], ['Wu', 'Songhao', ''], ['Wu', 'Wei', ''], ['Yan', 'Rui', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.15546,"submitter":"Shraddha Shah","authors":"Shraddha Pradipbhai Shah and Aditya Vilas Deshpande","title":"Enforcing Cybersecurity Constraints for LLM-driven Robot Agents for\n  Online Transactions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI cs.CY","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The integration of Large Language Models (LLMs) into autonomous robotic\nagents for conducting online transactions poses significant cybersecurity\nchallenges. This study aims to enforce robust cybersecurity constraints to\nmitigate the risks associated with data breaches, transaction fraud, and system\nmanipulation. The background focuses on the rise of LLM-driven robotic systems\nin e-commerce, finance, and service industries, alongside the vulnerabilities\nthey introduce. A novel security architecture combining blockchain technology\nwith multi-factor authentication (MFA) and real-time anomaly detection was\nimplemented to safeguard transactions. Key performance metrics such as\ntransaction integrity, response time, and breach detection accuracy were\nevaluated, showing improved security and system performance. The results\nhighlight that the proposed architecture reduced fraudulent transactions by\n90%, improved breach detection accuracy to 98%, and ensured secure transaction\nvalidation within a latency of 0.05 seconds. These findings emphasize the\nimportance of cybersecurity in the deployment of LLM-driven robotic systems and\nsuggest a framework adaptable to various online platforms.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 01:01:10 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Shah', 'Shraddha Pradipbhai', ''], ['Deshpande', 'Aditya Vilas', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15547,"submitter":"Juhee Kim","authors":"Juhee Kim, Woohyuk Choi, Byoungyoung Lee","title":"Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI cs.MA","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) are combined with plugins to create powerful LLM\nagents that provide a wide range of services. Unlike traditional software, LLM\nagent's behavior is determined at runtime by natural language prompts from\neither user or plugin's data. This flexibility enables a new computing paradigm\nwith unlimited capabilities and programmability, but also introduces new\nsecurity risks, vulnerable to privilege escalation attacks. Moreover, user\nprompt is prone to be interpreted in an insecure way by LLM agents, creating\nnon-deterministic behaviors that can be exploited by attackers. To address\nthese security risks, we propose Prompt Flow Integrity (PFI), a system\nsecurity-oriented solution to prevent privilege escalation in LLM agents.\nAnalyzing the architectural characteristics of LLM agents, PFI features three\nmitigation techniques -- i.e., untrusted data identification, enforcing least\nprivilege on LLM agents, and validating unsafe data flows. Our evaluation\nresult shows that PFI effectively mitigates privilege escalation attacks while\nsuccessfully preserving the utility of LLM agents.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 05:27:57 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Kim', 'Juhee', ''], ['Choi', 'Woohyuk', ''], ['Lee', 'Byoungyoung', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'natural language prompts', 'label': 'Prompting'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15552,"submitter":"Tharindu Kumarage","authors":"Tharindu Kumarage, Cameron Johnson, Jadie Adams, Lin Ai, Matthias\n  Kirchner, Anthony Hoogs, Joshua Garland, Julia Hirschberg, Arslan Basharat,\n  Huan Liu","title":"Personalized Attacks of Social Engineering in Multi-turn Conversations\n  -- LLM Agents for Simulation and Detection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The rapid advancement of conversational agents, particularly chatbots powered\nby Large Language Models (LLMs), poses a significant risk of social engineering\n(SE) attacks on social media platforms. SE detection in multi-turn, chat-based\ninteractions is considerably more complex than single-instance detection due to\nthe dynamic nature of these conversations. A critical factor in mitigating this\nthreat is understanding the mechanisms through which SE attacks operate,\nspecifically how attackers exploit vulnerabilities and how victims' personality\ntraits contribute to their susceptibility. In this work, we propose an\nLLM-agentic framework, SE-VSim, to simulate SE attack mechanisms by generating\nmulti-turn conversations. We model victim agents with varying personality\ntraits to assess how psychological profiles influence susceptibility to\nmanipulation. Using a dataset of over 1000 simulated conversations, we examine\nattack scenarios in which adversaries, posing as recruiters, funding agencies,\nand journalists, attempt to extract sensitive information. Based on this\nanalysis, we present a proof of concept, SE-OmniGuard, to offer personalized\nprotection to users by leveraging prior knowledge of the victims personality,\nevaluating attack strategies, and monitoring information exchanges in\nconversations to identify potential SE attempts.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 19:14:44 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Kumarage', 'Tharindu', ''], ['Johnson', 'Cameron', ''], ['Adams', 'Jadie', ''], ['Ai', 'Lin', ''], ['Kirchner', 'Matthias', ''], ['Hoogs', 'Anthony', ''], ['Garland', 'Joshua', ''], ['Hirschberg', 'Julia', ''], ['Basharat', 'Arslan', ''], ['Liu', 'Huan', '']]","extracted_entities":"[{'text': 'chatbots', 'label': 'ChatGPT'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.1556,"submitter":"Prashant Kulkarni","authors":"Prashant Kulkarni, Assaf Namer","title":"Temporal Context Awareness: A Defense Framework Against Multi-turn\n  Manipulation Attacks on Large Language Models","comments":"6 pages, 2 figures, IEEE CAI","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Large Language Models (LLMs) are increasingly vulnerable to sophisticated\nmulti-turn manipulation attacks, where adversaries strategically build context\nthrough seemingly benign conversational turns to circumvent safety measures and\nelicit harmful or unauthorized responses. These attacks exploit the temporal\nnature of dialogue to evade single-turn detection methods, representing a\ncritical security vulnerability with significant implications for real-world\ndeployments.\n  This paper introduces the Temporal Context Awareness (TCA) framework, a novel\ndefense mechanism designed to address this challenge by continuously analyzing\nsemantic drift, cross-turn intention consistency and evolving conversational\npatterns. The TCA framework integrates dynamic context embedding analysis,\ncross-turn consistency verification, and progressive risk scoring to detect and\nmitigate manipulation attempts effectively. Preliminary evaluations on\nsimulated adversarial scenarios demonstrate the framework's potential to\nidentify subtle manipulation patterns often missed by traditional detection\ntechniques, offering a much-needed layer of security for conversational AI\nsystems. In addition to outlining the design of TCA , we analyze diverse attack\nvectors and their progression across multi-turn conversation, providing\nvaluable insights into adversarial tactics and their impact on LLM\nvulnerabilities. Our findings underscore the pressing need for robust,\ncontext-aware defenses in conversational AI systems and highlight TCA framework\nas a promising direction for securing LLMs while preserving their utility in\nlegitimate applications. We make our implementation available to support\nfurther research in this emerging area of AI security.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 22:30:17 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Kulkarni', 'Prashant', ''], ['Namer', 'Assaf', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'dynamic context embedding', 'label': 'contextual Embedding'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15564,"submitter":"Tung Sum Thomas Kwok","authors":"Tung Sum Thomas Kwok and Chi-Hua Wang and Guang Cheng","title":"GReaTER: Generate Realistic Tabular data after data Enhancement and\n  Reduction","comments":"Accepted by Data Engineering Meets Large Language Models: Challenges\n  and Opportunities Workshop@ICDE2025 Workshop at ICDE 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Tabular data synthesis involves not only multi-table synthesis but also\ngenerating multi-modal data (e.g., strings and categories), which enables\ndiverse knowledge synthesis. However, separating numerical and categorical data\nhas limited the effectiveness of tabular data generation. The GReaT (Generate\nRealistic Tabular Data) framework uses Large Language Models (LLMs) to encode\nentire rows, eliminating the need to partition data types. Despite this, the\nframework's performance is constrained by two issues: (1) tabular data entries\nlack sufficient semantic meaning, limiting LLM's ability to leverage\npre-trained knowledge for in-context learning, and (2) complex multi-table\ndatasets struggle to establish effective relationships for collaboration. To\naddress these, we propose GReaTER (Generate Realistic Tabular Data after data\nEnhancement and Reduction), which includes: (1) a data semantic enhancement\nsystem that improves LLM's understanding of tabular data through mapping,\nenabling better in-context learning, and (2) a cross-table connecting method to\nestablish efficient relationships across complex tables. Experimental results\nshow that GReaTER outperforms the GReaT framework.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 04:16:05 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Kwok', 'Tung Sum Thomas', ''], ['Wang', 'Chi-Hua', ''], ['Cheng', 'Guang', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15566,"submitter":"Shijing Chen","authors":"Shijing Chen, Shoaib Jameel, Mohamed Reda Bouadjenek, Feilong Tang,\n  Usman Naseem, Basem Suleiman, Hakim Hacid, Flora D. Salim, Imran Razzak","title":"Enforcing Consistency and Fairness in Multi-level Hierarchical\n  Classification with a Mask-based Output Layer","comments":"14 pages, 14 figures. arXiv admin note: text overlap with\n  arXiv:2501.06827","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Traditional Multi-level Hierarchical Classification (MLHC) classifiers often\nrely on backbone models with $n$ independent output layers. This structure\ntends to overlook the hierarchical relationships between classes, leading to\ninconsistent predictions that violate the underlying taxonomy. Additionally,\nonce a backbone architecture for an MLHC classifier is selected, adapting the\nmodel to accommodate new tasks can be challenging. For example, incorporating\nfairness to protect sensitive attributes within a hierarchical classifier\nnecessitates complex adjustments to maintain the class hierarchy while\nenforcing fairness constraints. In this paper, we extend this concept to\nhierarchical classification by introducing a fair, model-agnostic layer\ndesigned to enforce taxonomy and optimize specific objectives, including\nconsistency, fairness, and exact match. Our evaluations demonstrate that the\nproposed layer not only improves the fairness of predictions but also enforces\nthe taxonomy, resulting in consistent predictions and superior performance.\nCompared to Large Language Models (LLMs) employing in-processing de-biasing\ntechniques and models without any bias correction, our approach achieves better\noutcomes in both fairness and accuracy, making it particularly valuable in\nsectors like e-commerce, healthcare, and education, where predictive\nreliability is crucial.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 06:30:04 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chen', 'Shijing', ''], ['Jameel', 'Shoaib', ''], ['Bouadjenek', 'Mohamed Reda', ''], ['Tang', 'Feilong', ''], ['Naseem', 'Usman', ''], ['Suleiman', 'Basem', ''], ['Hacid', 'Hakim', ''], ['Salim', 'Flora D.', ''], ['Razzak', 'Imran', '']]","extracted_entities":"[{'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15571,"submitter":"Pankaj Thorat","authors":"Pankaj Thorat, Adnan Qidwai, Adrija Dhar, Aishwariya Chakraborty,\n  Anand Eswaran, Hima Patel, Praveen Jayachandran","title":"LLM-Aided Customizable Profiling of Code Data Based On Programming\n  Language Concepts","comments":"21 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.ET cs.IR cs.LG cs.PL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Data profiling is critical in machine learning for generating descriptive\nstatistics, supporting both deeper understanding and downstream tasks like data\nvaluation and curation. This work addresses profiling specifically in the\ncontext of code datasets for Large Language Models (code-LLMs), where data\nquality directly influences tasks such as code generation and summarization.\nCharacterizing code datasets in terms of programming language concepts enables\nbetter insights and targeted data curation. Our proposed methodology decomposes\ncode data profiling into two phases: (1) an offline phase where LLMs are\nleveraged to derive and learn rules for extracting syntactic and semantic\nconcepts across various programming languages, including previously unseen or\nlow-resource languages, and (2) an online deterministic phase applying these\nderived rules for efficient real-time analysis. This hybrid approach is\ncustomizable, extensible to new syntactic and semantic constructs, and scalable\nto multiple languages. Experimentally, our LLM-aided method achieves a mean\naccuracy of 90.33% for syntactic extraction rules and semantic classification\naccuracies averaging 80% and 77% across languages and semantic concepts,\nrespectively.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 11:01:00 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Thorat', 'Pankaj', ''], ['Qidwai', 'Adnan', ''], ['Dhar', 'Adrija', ''], ['Chakraborty', 'Aishwariya', ''], ['Eswaran', 'Anand', ''], ['Patel', 'Hima', ''], ['Jayachandran', 'Praveen', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15655,"submitter":"Zefeng Lin","authors":"Zefeng Lin, Yi Xiao, Zhiqiang Mo, Qifan Zhang, Jie Wang, Jiayang Chen,\n  Jiajing Zhang, Hui Zhang, Zhengyi Liu, Xianyong Fang, Xiaohua Xu","title":"R$^2$: A LLM Based Novel-to-Screenplay Generation Framework with Causal\n  Plot Graphs","comments":"16 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Automatically adapting novels into screenplays is important for the TV, film,\nor opera industries to promote products with low costs. The strong performances\nof large language models (LLMs) in long-text generation call us to propose a\nLLM based framework Reader-Rewriter (R$^2$) for this task. However, there are\ntwo fundamental challenges here. First, the LLM hallucinations may cause\ninconsistent plot extraction and screenplay generation. Second, the\ncausality-embedded plot lines should be effectively extracted for coherent\nrewriting. Therefore, two corresponding tactics are proposed: 1) A\nhallucination-aware refinement method (HAR) to iteratively discover and\neliminate the affections of hallucinations; and 2) a causal plot-graph\nconstruction method (CPC) based on a greedy cycle-breaking algorithm to\nefficiently construct plot lines with event causalities. Recruiting those\nefficient techniques, R$^2$ utilizes two modules to mimic the human screenplay\nrewriting process: The Reader module adopts a sliding window and CPC to build\nthe causal plot graphs, while the Rewriter module generates first the scene\noutlines based on the graphs and then the screenplays. HAR is integrated into\nboth modules for accurate inferences of LLMs. Experimental results demonstrate\nthe superiority of R$^2$, which substantially outperforms three existing\napproaches (51.3%, 22.6%, and 57.1% absolute increases) in pairwise comparison\nat the overall win rate for GPT-4o.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 19:09:40 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Lin', 'Zefeng', ''], ['Xiao', 'Yi', ''], ['Mo', 'Zhiqiang', ''], ['Zhang', 'Qifan', ''], ['Wang', 'Jie', ''], ['Chen', 'Jiayang', ''], ['Zhang', 'Jiajing', ''], ['Zhang', 'Hui', ''], ['Liu', 'Zhengyi', ''], ['Fang', 'Xianyong', ''], ['Xu', 'Xiaohua', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'GPT-4o', 'label': 'GPT'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.15761,"submitter":"Mir Mohammad Khaleghi","authors":"Mir Mohammad Khaleghi, Mehran Safayani, Abdolreza Mirzaei","title":"GraPLUS: Graph-based Placement Using Semantics for Image Composition","comments":"17 pages, 3 figures, 6 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present GraPLUS (Graph-based Placement Using Semantics), a novel framework\nfor plausible object placement in images that leverages scene graphs and large\nlanguage models. Our approach uniquely combines graph-structured scene\nrepresentation with semantic understanding to determine contextually\nappropriate object positions. The framework employs GPT-2 to transform\ncategorical node and edge labels into rich semantic embeddings that capture\nboth definitional characteristics and typical spatial contexts, enabling\nnuanced understanding of object relationships and placement patterns. GraPLUS\nachieves placement accuracy of 92.1% and an FID score of 28.83 on the OPA\ndataset, outperforming state-of-the-art methods by 8.1% while maintaining\ncompetitive visual quality. In human evaluation studies involving 964 samples\nassessed by 19 participants, our method was preferred in 52.1% of cases,\nsignificantly outperforming previous approaches. The framework's key\ninnovations include: (i) leveraging pre-trained scene graph models that\ntransfer knowledge from other domains, (ii) edge-aware graph neural networks\nthat process scene semantics through structured relationships, (iii) a\ncross-modal attention mechanism that aligns categorical embeddings with\nenhanced scene features, and (iv) a multiobjective training strategy\nincorporating semantic consistency constraints.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 00:43:29 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Khaleghi', 'Mir Mohammad', ''], ['Safayani', 'Mehran', ''], ['Mirzaei', 'Abdolreza', '']]","extracted_entities":"[{'text': 'large\\nlanguage models', 'label': 'Large Language Model'}, {'text': 'GPT-2', 'label': 'GPT'}, {'text': 'rich semantic embeddings', 'label': 'contextual Embedding'}, {'text': 'cross-modal attention mechanism', 'label': 'Attention mechanism'}, {'text': 'categorical embeddings', 'label': 'contextual Embedding'}]","assigned_concept":"Large Language Model","matched_keyword":"large\nlanguage models","similarity_score":0.9664971828}
{"id":2503.15772,"submitter":"Vishisht Rao","authors":"Vishisht Rao, Aounon Kumar, Himabindu Lakkaraju, Nihar B. Shah","title":"Detecting LLM-Written Peer Reviews","comments":"26 pages, 1 figure","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DL cs.AI cs.CR","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Editors of academic journals and program chairs of conferences require peer\nreviewers to write their own reviews. However, there is growing concern about\nthe rise of lazy reviewing practices, where reviewers use large language models\n(LLMs) to generate reviews instead of writing them independently. Existing\ntools for detecting LLM-generated content are not designed to differentiate\nbetween fully LLM-generated reviews and those merely polished by an LLM. In\nthis work, we employ a straightforward approach to identify LLM-generated\nreviews - doing an indirect prompt injection via the paper PDF to ask the LLM\nto embed a watermark. Our focus is on presenting watermarking schemes and\nstatistical tests that maintain a bounded family-wise error rate, when a venue\nevaluates multiple reviews, with a higher power as compared to standard methods\nlike Bonferroni correction. These guarantees hold without relying on any\nassumptions about human-written reviews. We also consider various methods for\nprompt injection including font embedding and jailbreaking. We evaluate the\neffectiveness and various tradeoffs of these methods, including different\nreviewer defenses. We find a high success rate in the embedding of our\nwatermarks in LLM-generated reviews across models. We also find that our\napproach is resilient to common reviewer defenses, and that the bounds on error\nrates in our statistical tests hold in practice while having the power to flag\nLLM-generated reviews, while Bonferroni correction is infeasible.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 01:11:35 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Rao', 'Vishisht', ''], ['Kumar', 'Aounon', ''], ['Lakkaraju', 'Himabindu', ''], ['Shah', 'Nihar B.', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'indirect prompt injection', 'label': 'Prompting'}, {'text': 'font embedding', 'label': 'Embedding'}, {'text': 'jailbreaking', 'label': 'Embedding'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.15783,"submitter":"Tsunehiko Tanaka","authors":"Tsunehiko Tanaka, Edgar Simo-Serra","title":"Grammar and Gameplay-aligned RL for Game Description Generation with\n  LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Game Description Generation (GDG) is the task of generating a game\ndescription written in a Game Description Language (GDL) from natural language\ntext. Previous studies have explored generation methods leveraging the\ncontextual understanding capabilities of Large Language Models (LLMs); however,\naccurately reproducing the game features of the game descriptions remains a\nchallenge. In this paper, we propose reinforcement learning-based fine-tuning\nof LLMs for GDG (RLGDG). Our training method simultaneously improves\ngrammatical correctness and fidelity to game concepts by introducing both\ngrammar rewards and concept rewards. Furthermore, we adopt a two-stage training\nstrategy where Reinforcement Learning (RL) is applied following Supervised\nFine-Tuning (SFT). Experimental results demonstrate that our proposed method\nsignificantly outperforms baseline methods using SFT alone.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 01:47:33 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Tanaka', 'Tsunehiko', ''], ['Simo-Serra', 'Edgar', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Supervised\\nFine-Tuning (SFT)', 'label': 'Fine-tuning'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15807,"submitter":"Jiexiong Liu","authors":"Cheng Li, Jiexiong Liu, Yixuan Chen, Yanqin Jia","title":"Video-VoT-R1: An efficient video inference model integrating image\n  packing and AoE architecture","comments":"18 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In the field of video-language pretraining, existing models face numerous\nchallenges in terms of inference efficiency and multimodal data processing.\nThis paper proposes a KunLunBaize-VoT-R1 video inference model based on a\nlong-sequence image encoder, along with its training and application methods.\nBy integrating image packing technology, the Autonomy-of-Experts (AoE)\narchitecture, and combining the video of Thought (VoT), a large language model\n(LLM) trained with large-scale reinforcement learning, and multiple training\ntechniques, the efficiency and accuracy of the model in video inference tasks\nare effectively improved. Experiments show that this model performs\noutstandingly in multiple tests, providing a new solution for video-language\nunderstanding.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 02:50:57 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Li', 'Cheng', ''], ['Liu', 'Jiexiong', ''], ['Chen', 'Yixuan', ''], ['Jia', 'Yanqin', '']]","extracted_entities":"[{'text': 'large language model', 'label': 'Large Language Model'}, {'text': 'large-scale reinforcement learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"large language model","similarity_score":1.0}
{"id":2503.15816,"submitter":"Abduljaleel Adejumo","authors":"Abduljaleel Adejumo, Faegheh Yeganli, Clifford Broni-bediako, Aoran\n  Xiao, Naoto Yokoya and Mennatullah Siam","title":"A Vision Centric Remote Sensing Benchmark","comments":"6 PAGES, 7 figures, CVPR","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision-language tasks but their remote sensing (RS) counterpart are relatively\nunder explored. Unlike natural images, RS imagery presents unique challenges\nthat current MLLMs struggle to handle, particularly in visual grounding and\nspatial reasoning. This study investigates the limitations of CLIP-based MLLMs\nin RS, highlighting their failure to differentiate visually distinct yet\nsemantically similar RS images. To address this, we introduce a remote sensing\nmultimodal visual patterns (RSMMVP) benchmark. It is designed to evaluate MLLMs\nin RS tasks by identifying the CLIP-blind pairs, where CLIP-based models\nincorrectly assign high similarity scores to visually distinct RS images.\nThrough a visual question answering (VQA) evaluation, we analyze the\nperformance of state-of-the-art MLLMs, revealing significant limitations in RS\nspecific representation learning. The results provide valuable insights into\nthe weaknesses of CLIP-based visual encoding and offer a foundation for future\nresearch to develop more effective MLLMs tailored for remote sensing\napplications.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 03:03:46 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Adejumo', 'Abduljaleel', ''], ['Yeganli', 'Faegheh', ''], ['Broni-bediako', 'Clifford', ''], ['Xiao', 'Aoran', ''], ['Yokoya', 'Naoto', ''], ['Siam', 'Mennatullah', '']]","extracted_entities":"[{'text': 'Multimodal Large Language Models', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Multimodal Large Language Models","similarity_score":0.7649828196}
{"id":2503.15837,"submitter":"Shangqing Zhao","authors":"Shangqing Zhao, Yuhao Zhou, Yupei Ren, Zhe Chen, Chenghao Jia, Fang\n  Zhe, Zhaogaung Long, Shu Liu and Man Lan","title":"F\\`ux\\`i: A Benchmark for Evaluating Language Models on Ancient Chinese\n  Text Understanding and Generation","comments":"working in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Ancient Chinese text processing presents unique challenges for large language\nmodels (LLMs) due to its distinct linguistic features, complex structural\nconstraints, and rich cultural context. While existing benchmarks have\nprimarily focused on evaluating comprehension through multiple-choice\nquestions, there remains a critical gap in assessing models' generative\ncapabilities in classical Chinese. We introduce F\\`ux\\`i, a comprehensive\nbenchmark that evaluates both understanding and generation capabilities across\n21 diverse tasks. Our benchmark distinguishes itself through three key\ncontributions: (1) balanced coverage of both comprehension and generation\ntasks, including novel tasks like poetry composition and couplet completion,\n(2) specialized evaluation metrics designed specifically for classical Chinese\ntext generation, combining rule-based verification with fine-tuned LLM\nevaluators, and (3) a systematic assessment framework that considers both\nlinguistic accuracy and cultural authenticity. Through extensive evaluation of\nstate-of-the-art LLMs, we reveal significant performance gaps between\nunderstanding and generation tasks, with models achieving promising results in\ncomprehension but struggling considerably in generation tasks, particularly\nthose requiring deep cultural knowledge and adherence to classical formats. Our\nfindings highlight the current limitations in ancient Chinese text processing\nand provide insights for future model development. The benchmark, evaluation\ntoolkit, and baseline results are publicly available to facilitate research in\nthis domain.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 04:26:40 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zhao', 'Shangqing', ''], ['Zhou', 'Yuhao', ''], ['Ren', 'Yupei', ''], ['Chen', 'Zhe', ''], ['Jia', 'Chenghao', ''], ['Zhe', 'Fang', ''], ['Long', 'Zhaogaung', ''], ['Liu', 'Shu', ''], ['Lan', 'Man', '']]","extracted_entities":"[{'text': 'large language\\nmodels', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language\nmodels","similarity_score":0.9664971828}
{"id":2503.1584,"submitter":"Junle Li","authors":"Junle Li, Meiqi Tian, and Bingzhuo Zhong","title":"Automatic Generation of Safety-compliant Linear Temporal Logic via Large\n  Language Model: A Self-supervised Framework","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LO cs.FL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Ensuring safety in cyber-physical systems (CPS) poses a significant\nchallenge, especially when converting high-level tasks described by natural\nlanguage into formal specifications like Linear Temporal Logic (LTL). In\nparticular, the compliance of formal languages with respect to safety\nrestrictions imposed on CPS is crucial for system safety. In this paper, we\nintroduce AutoSafeLTL, a self-supervised framework that utilizes large language\nmodels (LLMs) to automate the generation of safety-compliant LTL. Our approach\nintegrates a Language Inclusion check with an automated counterexample-guided\nfeedback and modification mechanism, establishing a pipeline that verifies the\nsafety-compliance of the resulting LTL while preserving its logical consistency\nand semantic accuracy. To enhance the framework's understanding and correction\ncapabilities, we incorporate two additional Agent LLMs. Experimental results\ndemonstrate that AutoSafeLTL effectively guarantees safety-compliance for\ngenerated LTL, achieving a 0% violation rate against imposed safety\nconstraints.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 04:40:29 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Li', 'Junle', ''], ['Tian', 'Meiqi', ''], ['Zhong', 'Bingzhuo', '']]","extracted_entities":"[{'text': 'large language\\nmodels', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language\nmodels","similarity_score":0.9664971828}
{"id":2503.15846,"submitter":"Xuanming Cui","authors":"Xuanming Cui, Jaiminkumar Ashokbhai Bhoi, Chionh Wei Peng, Adriel\n  Kuek, Ser Nam Lim","title":"What can Off-the-Shelves Large Multi-Modal Models do for Dynamic Scene\n  Graph Generation?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Dynamic Scene Graph Generation (DSGG) for videos is a challenging task in\ncomputer vision. While existing approaches often focus on sophisticated\narchitectural design and solely use recall during evaluation, we take a closer\nlook at their predicted scene graphs and discover three critical issues with\nexisting DSGG methods: severe precision-recall trade-off, lack of awareness on\ntriplet importance, and inappropriate evaluation protocols. On the other hand,\nrecent advances of Large Multimodal Models (LMMs) have shown great capabilities\nin video understanding, yet they have not been tested on fine-grained,\nframe-wise understanding tasks like DSGG. In this work, we conduct the first\nsystematic analysis of Video LMMs for performing DSGG. Without relying on\nsophisticated architectural design, we show that LMMs with simple decoder-only\nstructure can be turned into State-of-the-Art scene graph generators that\neffectively overcome the aforementioned issues, while requiring little\nfinetuning (5-10% training data).\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 04:58:53 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Cui', 'Xuanming', ''], ['Bhoi', 'Jaiminkumar Ashokbhai', ''], ['Peng', 'Chionh Wei', ''], ['Kuek', 'Adriel', ''], ['Lim', 'Ser Nam', '']]","extracted_entities":"[{'text': 'Large Multimodal Models', 'label': 'Large Language Model'}, {'text': 'LMMs', 'label': 'Large Language Model'}, {'text': 'little\\nfinetuning', 'label': 'Fine-tuning'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Multimodal Models","similarity_score":0.5739125013}
{"id":2503.1585,"submitter":"Xiaoou Liu","authors":"Xiaoou Liu, Tiejin Chen, Longchao Da, Chacha Chen, Zhen Lin, Hua Wei","title":"Uncertainty Quantification and Confidence Calibration in Large Language\n  Models: A Survey","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) excel in text generation, reasoning, and\ndecision-making, enabling their adoption in high-stakes domains such as\nhealthcare, law, and transportation. However, their reliability is a major\nconcern, as they often produce plausible but incorrect responses. Uncertainty\nquantification (UQ) enhances trustworthiness by estimating confidence in\noutputs, enabling risk mitigation and selective prediction. However,\ntraditional UQ methods struggle with LLMs due to computational constraints and\ndecoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources,\nsuch as input ambiguity, reasoning path divergence, and decoding stochasticity,\nthat extend beyond classical aleatoric and epistemic uncertainty. To address\nthis, we introduce a new taxonomy that categorizes UQ methods based on\ncomputational efficiency and uncertainty dimensions (input, reasoning,\nparameter, and prediction uncertainty). We evaluate existing techniques, assess\ntheir real-world applicability, and identify open challenges, emphasizing the\nneed for scalable, interpretable, and robust UQ approaches to enhance LLM\nreliability.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 05:04:29 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Liu', 'Xiaoou', ''], ['Chen', 'Tiejin', ''], ['Da', 'Longchao', ''], ['Chen', 'Chacha', ''], ['Lin', 'Zhen', ''], ['Wei', 'Hua', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Uncertainty\\nquantification', 'label': 'quantisation'}, {'text': 'UQ', 'label': 'quantisation'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15871,"submitter":"Kyungho Bae","authors":"Kyungho Bae, Jinhyung Kim, Sihaeng Lee, Soonyoung Lee, Gunhee Lee, and\n  Jinwoo Choi","title":"MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through\n  Disentangled Spatial-Temporal Representations","comments":"Accepted for CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this work, we tackle action-scene hallucination in Video Large Language\nModels (Video-LLMs), where models incorrectly predict actions based on the\nscene context or scenes based on observed actions. We observe that existing\nVideo-LLMs often suffer from action-scene hallucination due to two main\nfactors. First, existing Video-LLMs intermingle spatial and temporal features\nby applying an attention operation across all tokens. Second, they use the\nstandard Rotary Position Embedding (RoPE), which causes the text tokens to\noveremphasize certain types of tokens depending on their sequential orders. To\naddress these issues, we introduce MASH-VLM, Mitigating Action-Scene\nHallucination in Video-LLMs through disentangled spatial-temporal\nrepresentations. Our approach includes two key innovations: (1) DST-attention,\na novel attention mechanism that disentangles the spatial and temporal tokens\nwithin the LLM by using masked attention to restrict direct interactions\nbetween the spatial and temporal tokens; (2) Harmonic-RoPE, which extends the\ndimensionality of the positional IDs, allowing the spatial and temporal tokens\nto maintain balanced positions relative to the text tokens. To evaluate the\naction-scene hallucination in Video-LLMs, we introduce the UNSCENE benchmark\nwith 1,320 videos and 4,078 QA pairs. Extensive experiments demonstrate that\nMASH-VLM achieves state-of-the-art results on the UNSCENE benchmark, as well as\non existing video understanding benchmarks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 05:48:59 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Bae', 'Kyungho', ''], ['Kim', 'Jinhyung', ''], ['Lee', 'Sihaeng', ''], ['Lee', 'Soonyoung', ''], ['Lee', 'Gunhee', ''], ['Choi', 'Jinwoo', '']]","extracted_entities":"[{'text': 'Video Large Language\\nModels', 'label': 'Large Language Model'}, {'text': 'Video-LLMs', 'label': 'Large Language Model'}, {'text': 'Video-LLMs', 'label': 'Large Language Model'}, {'text': 'Video-LLMs', 'label': 'Large Language Model'}, {'text': 'Rotary Position Embedding', 'label': 'contextual Embedding'}, {'text': 'Video-LLMs', 'label': 'Large Language Model'}, {'text': 'DST-attention', 'label': 'Attention mechanism'}, {'text': 'masked attention', 'label': 'Attention mechanism'}, {'text': 'Harmonic-RoPE', 'label': 'Attention mechanism'}, {'text': 'Video-LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Video Large Language\nModels","similarity_score":0.76164186}
{"id":2503.15876,"submitter":"Kai Chen","authors":"Kai Chen, Zebing Sun","title":"DeepPsy-Agent: A Stage-Aware and Deep-Thinking Emotional Support Agent\n  System","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper introduces DeepPsy-Agent, an innovative psychological support\nsystem that combines the three-stage helping theory in psychology with deep\nlearning techniques. The system consists of two core components: (1) a\nmulti-stage response-capable dialogue model (\\textit{deeppsy-chat}), which\nenhances reasoning capabilities through stage-awareness and deep-thinking\nanalysis to generate high-quality responses; and (2) a real-time stage\ntransition detection model that identifies contextual shifts to guide the\ndialogue towards more effective intervention stages. Based on 30,000 real\npsychological hotline conversations, we employ AI-simulated dialogues and\nexpert re-annotation strategies to construct a high-quality multi-turn dialogue\ndataset. Experimental results demonstrate that DeepPsy-Agent outperforms\ngeneral-purpose large language models (LLMs) in key metrics such as problem\nexposure completeness, cognitive restructuring success rate, and action\nadoption rate. Ablation studies further validate the effectiveness of\nstage-awareness and deep-thinking modules, showing that stage information\ncontributes 42.3\\% to performance, while the deep-thinking module increases\nroot-cause identification by 58.3\\% and reduces ineffective suggestions by\n72.1\\%. This system addresses critical challenges in AI-based psychological\nsupport through dynamic dialogue management and deep reasoning, advancing\nintelligent mental health services.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 05:59:29 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chen', 'Kai', ''], ['Sun', 'Zebing', '']]","extracted_entities":"[{'text': 'general-purpose large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"general-purpose large language models","similarity_score":0.8828719258}
{"id":2503.15904,"submitter":"Hung-Hsuan Chen","authors":"Evan Chen, Run-Jun Zhan, Yan-Bai Lin, Hung-Hsuan Chen","title":"From Structured Prompts to Open Narratives: Measuring Gender Bias in\n  LLMs Through Open-Ended Storytelling","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) have revolutionized natural language processing,\nyet concerns persist regarding their tendency to reflect or amplify social\nbiases present in their training data. This study introduces a novel evaluation\nframework to uncover gender biases in LLMs, focusing on their occupational\nnarratives. Unlike previous methods relying on structured scenarios or\ncarefully crafted prompts, our approach leverages free-form storytelling to\nreveal biases embedded in the models. Systematic analyses show an\noverrepresentation of female characters across occupations in six widely used\nLLMs. Additionally, our findings reveal that LLM-generated occupational gender\nrankings align more closely with human stereotypes than actual labor\nstatistics. These insights underscore the need for balanced mitigation\nstrategies to ensure fairness while avoiding the reinforcement of new\nstereotypes.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 07:15:45 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chen', 'Evan', ''], ['Zhan', 'Run-Jun', ''], ['Lin', 'Yan-Bai', ''], ['Chen', 'Hung-Hsuan', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'carefully crafted prompts', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15921,"submitter":"Fahao Chen","authors":"Fahao Chen, Peng Li, Tom H. Luan, Zhou Su, Jing Deng","title":"SPIN: Accelerating Large Language Model Inference with Heterogeneous\n  Speculative Models","comments":"Accepted by INFOCOM 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Speculative decoding has been shown as an effective way to accelerate Large\nLanguage Model (LLM) inference by using a Small Speculative Model (SSM) to\ngenerate candidate tokens in a so-called speculation phase, which are\nsubsequently verified by the LLM in a verification phase. However, current\nstate-of-the-art speculative decoding approaches have three key limitations:\nhandling requests with varying difficulty using homogeneous SSMs, lack of\nrobust support for batch processing, and insufficient holistic optimization for\nboth speculation and verification phases. In this paper, we introduce SPIN, an\nefficient LLM inference serving system based on speculative decoding, designed\nto address these challenges through three main innovations. First, SPIN\nimproves token speculation by using multiple heterogeneous SSMs, with a\nlearning-based algorithm for SSM selection that operates without prior\nknowledge of request difficulty. Second, SPIN employs a request decomposition\nmethod to minimize batching overhead during LLM verification. Finally, SPIN\norchestrates speculation and verification phases by pipelining their executions\non GPUs to achieve further acceleration. Experimental results demonstrate that\nSPIN significantly outperforms state-of-the-art methods, achieving a\nperformance increase of approximately 2.28X.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 07:57:57 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chen', 'Fahao', ''], ['Li', 'Peng', ''], ['Luan', 'Tom H.', ''], ['Su', 'Zhou', ''], ['Deng', 'Jing', '']]","extracted_entities":"[{'text': 'Large\\nLanguage Model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large\nLanguage Model","similarity_score":1.0}
{"id":2503.15937,"submitter":"Shiqi Jiang","authors":"Gaole Dai, Shiqi Jiang, Ting Cao, Yuanchun Li, Yuqing Yang, Rui Tan,\n  Mo Li, Lili Qiu","title":"Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical\n  Deployment","comments":"14 pages, 4 itertions","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We propose V-Droid, a mobile GUI task automation agent. Unlike previous\nmobile agents that utilize Large Language Models (LLMs) as generators to\ndirectly generate actions at each step, V-Droid employs LLMs as verifiers to\nevaluate candidate actions before making final decisions. To realize this novel\nparadigm, we introduce a comprehensive framework for constructing\nverifier-driven mobile agents: the discretized action space construction\ncoupled with the prefilling-only workflow to accelerate the verification\nprocess, the pair-wise progress preference training to significantly enhance\nthe verifier's decision-making capabilities, and the scalable human-agent joint\nannotation scheme to efficiently collect the necessary data at scale. V-Droid\nsets a new state-of-the-art task success rate across several public mobile task\nautomation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on\nMobileAgentBench, surpassing existing agents by 9.5%, 2.1%, and 9%,\nrespectively. Furthermore, V-Droid achieves an impressively low latency of 0.7\nseconds per step, making it the first mobile agent capable of delivering\nnear-real-time, effective decision-making capabilities.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:25:00 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Dai', 'Gaole', ''], ['Jiang', 'Shiqi', ''], ['Cao', 'Ting', ''], ['Li', 'Yuanchun', ''], ['Yang', 'Yuqing', ''], ['Tan', 'Rui', ''], ['Li', 'Mo', ''], ['Qiu', 'Lili', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'pair-wise progress preference training', 'label': 'Few-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15944,"submitter":"Jinyi Liu","authors":"Jinyi Liu, Yan Zheng, Rong Cheng, Qiyu Wu, Wei Guo, Fei Ni, Hebin\n  Liang, Yifu Yuan, Hangyu Mao, Fuzheng Zhang, Jianye Hao","title":"From Chaos to Order: The Atomic Reasoner Framework for Fine-grained\n  Reasoning in Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent advances in large language models (LLMs) have shown remarkable\nprogress, yet their capacity for logical ``slow-thinking'' reasoning persists\nas a critical research frontier. Current inference scaling paradigms suffer\nfrom two fundamental constraints: fragmented thought flows compromising logical\ncoherence, and intensively computational complexity that escalates with search\nspace dimensions. To overcome these limitations, we present \\textbf{Atomic\nReasoner} (\\textbf{AR}), a cognitive inference strategy that enables\nfine-grained reasoning through systematic atomic-level operations. AR\ndecomposes the reasoning process into atomic cognitive units, employing a\ncognitive routing mechanism to dynamically construct reasoning representations\nand orchestrate inference pathways. This systematic methodology implements\nstepwise, structured cognition, which ensures logical coherence while\nsignificantly reducing cognitive load, effectively simulating the cognitive\npatterns observed in human deep thinking processes. Extensive experimental\nresults demonstrate AR's superior reasoning capabilities without the\ncomputational burden of exhaustive solution searches, particularly excelling in\nlinguistic logic puzzles. These findings substantiate AR's effectiveness in\nenhancing LLMs' capacity for robust, long-sequence logical reasoning and\ndeliberation.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:34:53 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Liu', 'Jinyi', ''], ['Zheng', 'Yan', ''], ['Cheng', 'Rong', ''], ['Wu', 'Qiyu', ''], ['Guo', 'Wei', ''], ['Ni', 'Fei', ''], ['Liang', 'Hebin', ''], ['Yuan', 'Yifu', ''], ['Mao', 'Hangyu', ''], ['Zhang', 'Fuzheng', ''], ['Hao', 'Jianye', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Current inference scaling paradigms', 'label': 'Scaling law'}, {'text': 'fragmented thought flows', 'label': 'Chain of thought'}, {'text': 'logical\\ncoherence', 'label': 'Chain of thought'}, {'text': 'logical coherence', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.15948,"submitter":"Vasily Konovalov","authors":"Elisei Rykov, Kseniia Petrushina, Kseniia Titova, Alexander Panchenko,\n  Vasily Konovalov","title":"Don't Fight Hallucinations, Use Them: Estimating Image Realism using NLI\n  over Atomic Facts","comments":"Proceedings of De-Factify 4: 4nd Workshop on Multimodal Fact Checking\n  and Hate Speech Detection, co-located with AAAI-2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Quantifying the realism of images remains a challenging problem in the field\nof artificial intelligence. For example, an image of Albert Einstein holding a\nsmartphone violates common-sense because modern smartphone were invented after\nEinstein's death. We introduce a novel method for assessing image realism using\nLarge Vision-Language Models (LVLMs) and Natural Language Inference (NLI). Our\napproach is based on the premise that LVLMs may generate hallucinations when\nconfronted with images that defy common sense. Using LVLM to extract atomic\nfacts from these images, we obtain a mix of accurate facts and erroneous\nhallucinations. We proceed by calculating pairwise entailment scores among\nthese facts, subsequently aggregating these values to yield a singular reality\nscore. This process serves to identify contradictions between genuine facts and\nhallucinatory elements, signaling the presence of images that violate common\nsense. Our approach has achieved a new state-of-the-art performance in\nzero-shot mode on the WHOOPS! dataset.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:44:10 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Rykov', 'Elisei', ''], ['Petrushina', 'Kseniia', ''], ['Titova', 'Kseniia', ''], ['Panchenko', 'Alexander', ''], ['Konovalov', 'Vasily', '']]","extracted_entities":"[{'text': 'Albert Einstein', 'label': 'ALBERT'}, {'text': 'Large Vision-Language Models', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'LVLMs', 'label': 'Large Language Model'}, {'text': 'zero-shot mode', 'label': 'Zero-shot Learning'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Vision-Language Models","similarity_score":0.7742220759}
{"id":2503.15953,"submitter":"Mohammed Oualid Attaoui","authors":"Mohammed Attaoui and Fabrizio Pastore","title":"GAN-enhanced Simulation-driven DNN Testing in Absence of Ground Truth","comments":"15 pages, 8 figures, 13 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The generation of synthetic inputs via simulators driven by search algorithms\nis essential for cost-effective testing of Deep Neural Network (DNN) components\nfor safety-critical systems. However, in many applications, simulators are\nunable to produce the ground-truth data needed for automated test oracles and\nto guide the search process.\n  To tackle this issue, we propose an approach for the generation of inputs for\ncomputer vision DNNs that integrates a generative network to ensure simulator\nfidelity and employs heuristic-based search fitnesses that leverage\ntransformation consistency, noise resistance, surprise adequacy, and\nuncertainty estimation. We compare the performance of our fitnesses with that\nof a traditional fitness function leveraging ground truth; further, we assess\nhow the integration of a GAN not leveraging the ground truth impacts on test\nand retraining effectiveness.\n  Our results suggest that leveraging transformation consistency is the best\noption to generate inputs for both DNN testing and retraining; it maximizes\ninput diversity, spots the inputs leading to worse DNN performance, and leads\nto best DNN performance after retraining. Besides enabling simulator-based\ntesting in the absence of ground truth, our findings pave the way for testing\nsolutions that replace costly simulators with diffusion and large language\nmodels, which might be more affordable than simulators, but cannot generate\nground-truth data.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:49:10 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Attaoui', 'Mohammed', ''], ['Pastore', 'Fabrizio', '']]","extracted_entities":"[{'text': 'large language\\nmodels', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language\nmodels","similarity_score":0.9664971828}
{"id":2503.1599,"submitter":"Langming Liu","authors":"Langming Liu, Haibin Chen, Yuhao Wang, Yujin Yuan, Shilei Liu, Wenbo\n  Su, Xiangyu Zhao, Bo Zheng","title":"ECKGBench: Benchmarking Large Language Models in E-commerce Leveraging\n  Knowledge Graph","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models (LLMs) have demonstrated their capabilities across\nvarious NLP tasks. Their potential in e-commerce is also substantial, evidenced\nby practical implementations such as platform search, personalized\nrecommendations, and customer service. One primary concern associated with LLMs\nis their factuality (e.g., hallucination), which is urgent in e-commerce due to\nits significant impact on user experience and revenue. Despite some methods\nproposed to evaluate LLMs' factuality, issues such as lack of reliability, high\nconsumption, and lack of domain expertise leave a gap between effective\nassessment in e-commerce. To bridge the evaluation gap, we propose ECKGBench, a\ndataset specifically designed to evaluate the capacities of LLMs in e-commerce\nknowledge. Specifically, we adopt a standardized workflow to automatically\ngenerate questions based on a large-scale knowledge graph, guaranteeing\nsufficient reliability. We employ the simple question-answering paradigm,\nsubstantially improving the evaluation efficiency by the least input and output\ntokens. Furthermore, we inject abundant e-commerce expertise in each evaluation\nstage, including human annotation, prompt design, negative sampling, and\nverification. Besides, we explore the LLMs' knowledge boundaries in e-commerce\nfrom a novel perspective. Through comprehensive evaluations of several advanced\nLLMs on ECKGBench, we provide meticulous analysis and insights into leveraging\nLLMs for e-commerce.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 09:49:15 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Liu', 'Langming', ''], ['Chen', 'Haibin', ''], ['Wang', 'Yuhao', ''], ['Yuan', 'Yujin', ''], ['Liu', 'Shilei', ''], ['Su', 'Wenbo', ''], ['Zhao', 'Xiangyu', ''], ['Zheng', 'Bo', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ECKGBench', 'label': 'Open-source LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'prompt design', 'label': 'Prompting'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ECKGBench', 'label': 'Open-source LLMs'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.16022,"submitter":"Mario Sanz-Guerrero","authors":"Mario Sanz-Guerrero and Katharina von der Wense","title":"Corrective In-Context Learning: Evaluating Self-Correction in Large\n  Language Models","comments":"Accepted to the 6th Workshop on Insights from Negative Results in NLP\n  at NAACL 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In-context learning (ICL) has transformed the use of large language models\n(LLMs) for NLP tasks, enabling few-shot learning by conditioning on labeled\nexamples without finetuning. Despite its effectiveness, ICL is prone to errors,\nespecially for challenging examples. With the goal of improving the performance\nof ICL, we propose corrective in-context learning (CICL), an approach that\nincorporates a model's incorrect predictions alongside ground truth corrections\ninto the prompt, aiming to enhance classification accuracy through\nself-correction. However, contrary to our hypothesis, extensive experiments on\ntext classification tasks demonstrate that CICL consistently underperforms\nstandard ICL, with performance degrading as the proportion of corrections in\nthe prompt increases. Our findings indicate that CICL introduces confusion by\ndisrupting the model's task understanding, rather than refining its\npredictions. Additionally, we observe that presenting harder examples in\nstandard ICL does not improve performance, suggesting that example difficulty\nalone may not be a reliable criterion for effective selection. By presenting\nthese negative results, we provide important insights into the limitations of\nself-corrective mechanisms in LLMs and offer directions for future research.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 10:39:39 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Sanz-Guerrero', 'Mario', ''], ['von der Wense', 'Katharina', '']]","extracted_entities":"[{'text': 'In-context learning', 'label': 'Few-shot Learning'}, {'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'few-shot learning', 'label': 'Zero-shot Learning'}, {'text': 'finetuning', 'label': 'Fine-tuning'}, {'text': 'ICL', 'label': 'Zero-shot Learning'}, {'text': 'ICL', 'label': 'Few-shot Learning'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'CICL', 'label': 'Few-shot Learning'}, {'text': 'ICL', 'label': 'contextual Embedding'}, {'text': 'prompt', 'label': 'Prompting'}, {'text': 'CICL', 'label': 'Few-shot Learning'}, {'text': 'ICL', 'label': 'contextual Embedding'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.16041,"submitter":"Akinyemi Sadeeq Akintola","authors":"Bisola Faith Kayode, Akinyemi Sadeeq Akintola, Oluwole Fagbohun,\n  Egonna Anaesiuba-Bristol, Onyekachukwu Ojumah, Oluwagbade Odimayo, Toyese\n  Oloyede, Aniema Inyang, Teslim Kazeem, Habeeb Alli, Udodirim Ibem Offia,\n  Prisca Chinazor Amajuoyi","title":"GreenIQ: A Deep Search Platform for Comprehensive Carbon Market Analysis\n  and Automated Report Generation","comments":"12 Pages, 1 figure","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This study introduces GreenIQ, an AI-powered deep search platform designed to\nrevolutionise carbon market intelligence through autonomous analysis and\nautomated report generation. Carbon markets operate across diverse regulatory\nlandscapes, generating vast amounts of heterogeneous data from policy\ndocuments, industry reports, academic literature, and real-time trading\nplatforms. Traditional research approaches remain labour-intensive, slow, and\ndifficult to scale. GreenIQ addresses these limitations through a multi-agent\narchitecture powered by Large Language Models (LLMs), integrating five\nspecialised AI agents: a Main Researcher Agent for intelligent information\nretrieval, a Report Writing Agent for structured synthesis, a Final Reviewer\nAgent for accuracy verification, a Data Visualisation Agent for enhanced\ninterpretability, and a Translator Agent for multilingual adaptation. The\nsystem achieves seamless integration of structured and unstructured information\nwith AI-driven citation verification, ensuring high transparency and\nreliability. GreenIQ delivers a 99.2\\% reduction in processing time and a\n99.7\\% cost reduction compared to traditional research methodologies. A novel\nAI persona-based evaluation framework involving 16 domain-specific AI personas\nhighlights its superior cross-jurisdictional analytical capabilities and\nregulatory insight generation. GreenIQ sets new standards in AI-driven research\nsynthesis, policy analysis, and sustainability finance by streamlining carbon\nmarket research. It offers an efficient and scalable framework for\nenvironmental and financial intelligence, enabling more accurate, timely, and\ncost-effective decision-making in complex regulatory landscapes\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 11:19:43 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Kayode', 'Bisola Faith', ''], ['Akintola', 'Akinyemi Sadeeq', ''], ['Fagbohun', 'Oluwole', ''], ['Anaesiuba-Bristol', 'Egonna', ''], ['Ojumah', 'Onyekachukwu', ''], ['Odimayo', 'Oluwagbade', ''], ['Oloyede', 'Toyese', ''], ['Inyang', 'Aniema', ''], ['Kazeem', 'Teslim', ''], ['Alli', 'Habeeb', ''], ['Offia', 'Udodirim Ibem', ''], ['Amajuoyi', 'Prisca Chinazor', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.16094,"submitter":"Reem Masoud","authors":"Reem I. Masoud, Martin Ferianc, Philip Treleaven, Miguel Rodrigues","title":"Cultural Alignment in Large Language Models Using Soft Prompt Tuning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Model (LLM) alignment conventionally relies on supervised\nfine-tuning or reinforcement learning based alignment frameworks. These methods\ntypically require labeled or preference datasets and involve updating model\nweights to align the LLM with the training objective or reward model.\nMeanwhile, in social sciences such as cross-cultural studies, factor analysis\nis widely used to uncover underlying dimensions or latent variables that\nexplain observed patterns in survey data. The non-differentiable nature of\nthese measurements deriving from survey data renders the former alignment\nmethods infeasible for alignment with cultural dimensions. To overcome this, we\npropose a parameter efficient strategy that combines soft prompt tuning, which\nfreezes the model parameters while modifying the input prompt embeddings, with\nDifferential Evolution (DE), a black-box optimization method for cases where a\ndifferentiable objective is unattainable. This strategy ensures alignment\nconsistency without the need for preference data or model parameter updates,\nsignificantly enhancing efficiency and mitigating overfitting. Our method\ndemonstrates significant improvements in LLama-3-8B-Instruct's cultural\ndimensions across multiple regions, outperforming both the Naive LLM and the\nIn-context Learning (ICL) baseline, and effectively bridges computational\nmodels with human cultural nuances.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 12:34:01 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Masoud', 'Reem I.', ''], ['Ferianc', 'Martin', ''], ['Treleaven', 'Philip', ''], ['Rodrigues', 'Miguel', '']]","extracted_entities":"[{'text': 'Large Language Model', 'label': 'Large Language Model'}, {'text': 'LLM', 'label': 'Large Language Model'}, {'text': 'soft prompt tuning', 'label': 'Fine-tuning'}, {'text': 'input prompt embeddings', 'label': 'Embedding'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Model","similarity_score":1.0}
{"id":2503.16114,"submitter":"Chelse Swoopes","authors":"Chelse Swoopes, Tyler Holloway, Elena L. Glassman","title":"The Impact of Revealing Large Language Model Stochasticity on Trust,\n  Reliability, and Anthropomorphization","comments":"Accepted and presented at Trust and Reliance in Evolving Human-AI\n  Workflows (TREW) Workshop, CHI 2024","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Interfaces for interacting with large language models (LLMs) are often\ndesigned to mimic human conversations, typically presenting a single response\nto user queries. This design choice can obscure the probabilistic and\npredictive nature of these models, potentially fostering undue trust and\nover-anthropomorphization of the underlying model. In this paper, we\ninvestigate (i) the effect of displaying multiple responses simultaneously as a\ncountermeasure to these issues, and (ii) how a cognitive support\nmechanism-highlighting structural and semantic similarities across\nresponses-helps users deal with the increased cognitive load of that\nintervention. We conducted a within-subjects study in which participants\ninspected responses generated by an LLM under three conditions: one response,\nten responses with cognitive support, and ten responses without cognitive\nsupport. Participants then answered questions about workload, trust and\nreliance, and anthropomorphization. We conclude by reporting the results of\nthese studies and discussing future work and design opportunities for future\nLLM interfaces.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:00:56 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Swoopes', 'Chelse', ''], ['Holloway', 'Tyler', ''], ['Glassman', 'Elena L.', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.16131,"submitter":"Yingjian Chen","authors":"Feiyang Li, Yingjian Chen, Haoran Liu, Rui Yang, Han Yuan, Yuang\n  Jiang, Tianxiao Li, Edison Marrese Taylor, Hossein Rouhizadeh, Yusuke\n  Iwasawa, Douglas Teodoro, Yutaka Matsuo and Irene Li","title":"MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 33.89% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:25:03 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Li', 'Feiyang', ''], ['Chen', 'Yingjian', ''], ['Liu', 'Haoran', ''], ['Yang', 'Rui', ''], ['Yuan', 'Han', ''], ['Jiang', 'Yuang', ''], ['Li', 'Tianxiao', ''], ['Taylor', 'Edison Marrese', ''], ['Rouhizadeh', 'Hossein', ''], ['Iwasawa', 'Yusuke', ''], ['Teodoro', 'Douglas', ''], ['Matsuo', 'Yutaka', ''], ['Li', 'Irene', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.16167,"submitter":"Hong Yi Lin","authors":"Hong Yi Lin, Chunhua Liu, Haoyu Gao, Patanamon Thongtanunam, Christoph\n  Treude","title":"CodeReviewQA: The Code Review Comprehension Assessment for Large\n  Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  State-of-the-art large language models (LLMs) have demonstrated impressive\ncode generation capabilities but struggle with real-world software engineering\ntasks, such as revising source code to address code reviews, hindering their\npractical use. Code review comments are often implicit, ambiguous, and\ncolloquial, requiring models to grasp both code and human intent. This\nchallenge calls for evaluating large language models' ability to bridge both\ntechnical and conversational contexts. While existing work has employed the\nautomated code refinement (ACR) task to resolve these comments, current\nevaluation methods fall short, relying on text matching metrics that provide\nlimited insight into model failures and remain susceptible to training data\ncontamination. To address these limitations, we introduce a novel evaluation\nbenchmark, $\\textbf{CodeReviewQA}$ that enables us to conduct fine-grained\nassessment of model capabilities and mitigate data contamination risks. In\nCodeReviewQA, we decompose the generation task of code refinement into\n$\\textbf{three essential reasoning steps}$: $\\textit{change type recognition}$\n(CTR), $\\textit{change localisation}$ (CL), and $\\textit{solution\nidentification}$ (SI). Each step is reformulated as multiple-choice questions\nwith varied difficulty levels, enabling precise assessment of model\ncapabilities, while mitigating data contamination risks. Our comprehensive\nevaluation spans 72 recently released large language models on $\\textbf{900\nmanually curated, high-quality examples}$ across nine programming languages.\nOur results show that CodeReviewQA is able to expose specific model weaknesses\nin code review comprehension, disentangled from their generative automated code\nrefinement results.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 14:07:31 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Lin', 'Hong Yi', ''], ['Liu', 'Chunhua', ''], ['Gao', 'Haoyu', ''], ['Thongtanunam', 'Patanamon', ''], ['Treude', 'Christoph', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'large language models', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.16191,"submitter":"Yinon Goldshtein","authors":"Yinon Goldshtein, Gal Perelman, Assaf Schuster, Avi Ostfeld","title":"Large Language Models for Water Distribution Systems Modeling and\n  Decision-Making","comments":"Accepted to EWRI Congress 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.HC cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The design, operations, and management of water distribution systems (WDS)\ninvolve complex mathematical models. These models are continually improving due\nto computational advancements, leading to better decision-making and more\nefficient WDS management. However, the significant time and effort required for\nmodeling, programming, and analyzing results remain substantial challenges.\nAnother issue is the professional burden, which confines the interaction with\nmodels, databases, and other sophisticated tools to a small group of experts,\nthereby causing non-technical stakeholders to depend on these experts or make\ndecisions without modeling support. Furthermore, explaining model results is\nchallenging even for experts, as it is often unclear which conditions cause the\nmodel to reach a certain state or recommend a specific policy. The recent\nadvancements in Large Language Models (LLMs) open doors for a new stage in\nhuman-model interaction. This study proposes a framework of plain language\ninteractions with hydraulic and water quality models based on LLM-EPANET\narchitecture. This framework is tested with increasing levels of complexity of\nqueries to study the ability of LLMs to interact with WDS models, run complex\nsimulations, and report simulation results. The performance of the proposed\nframework is evaluated across several categories of queries and hyper-parameter\nconfigurations, demonstrating its potential to enhance decision-making\nprocesses in WDS management.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 14:39:11 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Goldshtein', 'Yinon', ''], ['Perelman', 'Gal', ''], ['Schuster', 'Assaf', ''], ['Ostfeld', 'Avi', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLM-EPANET\\narchitecture', 'label': 'LLM-based'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.16212,"submitter":"Qizhi Pei","authors":"Qizhi Pei, Lijun Wu, Zhuoshi Pan, Yu Li, Honglin Lin, Chenlin Ming,\n  Xin Gao, Conghui He, Rui Yan","title":"MathFusion: Enhancing Mathematic Problem-solving of LLM through\n  Instruction Fusion","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large Language Models (LLMs) have shown impressive progress in mathematical\nreasoning. While data augmentation is promising to enhance mathematical\nproblem-solving ability, current approaches are predominantly limited to\ninstance-level modifications-such as rephrasing or generating syntactic\nvariations-which fail to capture and leverage the intrinsic relational\nstructures inherent in mathematical knowledge. Inspired by human learning\nprocesses, where mathematical proficiency develops through systematic exposure\nto interconnected concepts, we introduce MathFusion, a novel framework that\nenhances mathematical reasoning through cross-problem instruction synthesis.\nMathFusion implements this through three fusion strategies: (1) sequential\nfusion, which chains related problems to model solution dependencies; (2)\nparallel fusion, which combines analogous problems to reinforce conceptual\nunderstanding; and (3) conditional fusion, which creates context-aware\nselective problems to enhance reasoning flexibility. By applying these\nstrategies, we generate a new dataset, \\textbf{MathFusionQA}, followed by\nfine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental\nresults demonstrate that MathFusion achieves substantial improvements in\nmathematical reasoning while maintaining high data efficiency, boosting\nperformance by 18.0 points in accuracy across diverse benchmarks while\nrequiring only 45K additional synthetic instructions, representing a\nsubstantial improvement over traditional single-instruction approaches. Our\ndatasets, models, and code are publicly available at\nhttps:\/\/github.com\/QizhiPei\/mathfusion.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 15:00:41 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Pei', 'Qizhi', ''], ['Wu', 'Lijun', ''], ['Pan', 'Zhuoshi', ''], ['Li', 'Yu', ''], ['Lin', 'Honglin', ''], ['Ming', 'Chenlin', ''], ['Gao', 'Xin', ''], ['He', 'Conghui', ''], ['Yan', 'Rui', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'parallel fusion', 'label': 'contextual Embedding'}, {'text': 'conditional fusion', 'label': 'contextual Embedding'}, {'text': 'Mistral-7B', 'label': 'Llama'}, {'text': 'Llama3-8B', 'label': 'Llama'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.16257,"submitter":"Keda Tao","authors":"Keda Tao, Haoxuan You, Yang Sui, Can Qin, Huan Wang","title":"Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models","comments":"12 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 15:52:43 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Tao', 'Keda', ''], ['You', 'Haoxuan', ''], ['Sui', 'Yang', ''], ['Qin', 'Can', ''], ['Wang', 'Huan', '']]","extracted_entities":"[{'text': 'Video large language models', 'label': 'Large Language Model'}, {'text': 'VideoLLMs', 'label': 'Large Language Model'}, {'text': 'KV cache quantization', 'label': 'quantisation'}, {'text': '2-bit KV\\nquantization', 'label': 'quantisation'}, {'text': 'VideoLLMs', 'label': 'Large Language Model'}, {'text': 'VidKV', 'label': 'quantisation'}, {'text': '2-bit quantization', 'label': 'quantisation'}, {'text': '1-bit\\nquantization', 'label': 'quantisation'}, {'text': 'VideoLLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Video large language models","similarity_score":0.76164186}
{"id":2503.16356,"submitter":"Ningyu Zhang","authors":"Yunzhi Yao, Jizhan Fang, Jia-Chen Gu, Ningyu Zhang, Shumin Deng,\n  Huajun Chen, Nanyun Peng","title":"CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CV cs.IR cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps:\/\/github.com\/zjunlp\/CaKE.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:14:34 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Yao', 'Yunzhi', ''], ['Fang', 'Jizhan', ''], ['Gu', 'Jia-Chen', ''], ['Zhang', 'Ningyu', ''], ['Deng', 'Shumin', ''], ['Chen', 'Huajun', ''], ['Peng', 'Nanyun', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'reasoning circuits', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'reasoning pathways', 'label': 'Chain of thought'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.16376,"submitter":"Leyang Wang","authors":"Leyang Wang and Joice Lin","title":"LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial\n  Images","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The success of modern machine learning, particularly in facial translation\nnetworks, is highly dependent on the availability of high-quality, paired,\nlarge-scale datasets. However, acquiring sufficient data is often challenging\nand costly. Inspired by the recent success of diffusion models in high-quality\nimage synthesis and advancements in Large Language Models (LLMs), we propose a\nnovel framework called LLM-assisted Paired Image Generation (LaPIG). This\nframework enables the construction of comprehensive, high-quality paired\nvisible and thermal images using captions generated by LLMs. Our method\nencompasses three parts: visible image synthesis with ArcFace embedding,\nthermal image translation using Latent Diffusion Models (LDMs), and caption\ngeneration with LLMs. Our approach not only generates multi-view paired visible\nand thermal images to increase data diversity but also produces high-quality\npaired data while maintaining their identity information. We evaluate our\nmethod on public datasets by comparing it with existing methods, demonstrating\nthe superiority of LaPIG.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:39:06 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wang', 'Leyang', ''], ['Lin', 'Joice', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'ArcFace embedding', 'label': 'Embedding'}, {'text': 'LLMs', 'label': 'LLM'}, {'text': 'public datasets', 'label': 'Open-source LLMs'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.16385,"submitter":"Yijia Luo","authors":"Yijia Luo, Yulin Song, Xingyao Zhang, Jiaheng Liu, Weixun Wang, GengRu\n  Chen, Wenbo Su, Bo Zheng","title":"Deconstructing Long Chain-of-Thought: A Structured Reasoning\n  Optimization Framework for Long CoT Distillation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/","abstract":"  Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities through long chain-of-thought (CoT)\nreasoning. The R1 distillation scheme has emerged as a promising approach for\ntraining cost-effective models with enhanced reasoning abilities. However, the\nunderlying mechanisms driving its effectiveness remain unclear. This study\nexamines the universality of distillation data and identifies key components\nthat enable the efficient transfer of long-chain reasoning capabilities in LLM\ndistillation. Our findings reveal that the effectiveness of long CoT reasoning\ndistillation from teacher models like Qwen-QwQ degrades significantly on\nnonhomologous models, challenging the assumed universality of current\ndistillation methods. To gain deeper insights into the structure and patterns\nof long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),\na distillation data enhancement framework. DLCoT consists of three key steps:\n(1) data segmentation to decompose complex long CoT structures, (2)\nsimplification by eliminating unsolvable and redundant solutions, and (3)\noptimization of intermediate error states. Our approach significantly improves\nmodel performance and token efficiency, facilitating the development of\nhigh-performance LLMs.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:46:38 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Luo', 'Yijia', ''], ['Song', 'Yulin', ''], ['Zhang', 'Xingyao', ''], ['Liu', 'Jiaheng', ''], ['Wang', 'Weixun', ''], ['Chen', 'GengRu', ''], ['Su', 'Wenbo', ''], ['Zheng', 'Bo', '']]","extracted_entities":"[{'text': 'large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'long chain-of-thought', 'label': 'Chain of thought'}, {'text': 'data segmentation', 'label': 'Knowledge distillation'}, {'text': 'LLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"large language models","similarity_score":0.9664971828}
{"id":2503.16401,"submitter":"Guanyu Chen","authors":"Guanyu Chen, Peiyang Wang, Tianren Zhang, Feng Chen","title":"Exploring the Hidden Reasoning Process of Large Language Models by\n  Misleading Them","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models (LLMs) and Vision language models (VLMs) have been able\nto perform various forms of reasoning tasks in a wide range of scenarios, but\nare they truly engaging in task abstraction and rule-based reasoning beyond\nmere memorization and pattern matching? To answer this question, we propose a\nnovel experimental approach, Misleading Fine-Tuning (MisFT), to examine whether\nLLMs\/VLMs perform abstract reasoning by altering their original understanding\nof fundamental rules. In particular, by constructing a dataset with math\nexpressions that contradict correct operation principles, we fine-tune the\nmodel to learn those contradictory rules and assess its generalization ability\non different test domains. Through a series of experiments, we find that\ncurrent LLMs\/VLMs are capable of effectively applying contradictory rules to\nsolve practical math word problems and math expressions represented by images,\nimplying the presence of an internal mechanism that abstracts before reasoning.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:54:42 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chen', 'Guanyu', ''], ['Wang', 'Peiyang', ''], ['Zhang', 'Tianren', ''], ['Chen', 'Feng', '']]","extracted_entities":"[{'text': 'Large language models', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'Vision language models', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'Misleading Fine-Tuning', 'label': 'Fine-tuning'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'VLMs', 'label': 'Large Language Model'}]","assigned_concept":"Large Language Model","matched_keyword":"Large language models","similarity_score":0.9664971828}
{"id":2503.16419,"submitter":"Yang Sui","authors":"Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang,\n  Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen (Henry) Zhong, Hanjie Chen, Xia\n  Hu","title":"Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models","comments":"Project Website:\n  https:\/\/github.com\/Eclipsess\/Awesome-Efficient-Reasoning-LLMs","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:59:38 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Sui', 'Yang', '', 'Henry'], ['Chuang', 'Yu-Neng', '', 'Henry'], ['Wang', 'Guanchu', '', 'Henry'], ['Zhang', 'Jiamu', '', 'Henry'], ['Zhang', 'Tianyi', '', 'Henry'], ['Yuan', 'Jiayi', '', 'Henry'], ['Liu', 'Hongyi', '', 'Henry'], ['Wen', 'Andrew', '', 'Henry'], ['Shaochen', '', '', 'Henry'], ['Zhong', '', ''], ['Chen', 'Hanjie', ''], ['Hu', 'Xia', '']]","extracted_entities":"[{'text': 'Large Language Models', 'label': 'Large Language Model'}, {'text': 'Large Reasoning Models', 'label': 'Large Language Model'}, {'text': 'OpenAI o1', 'label': 'Open-source LLMs'}, {'text': 'supervised\\nfine-tuning (SFT)', 'label': 'Fine-tuning'}, {'text': 'Chain-of-Thought (CoT)', 'label': 'Chain of thought'}, {'text': 'input\\nprompts-based', 'label': 'Prompting'}]","assigned_concept":"Large Language Model","matched_keyword":"Large Language Models","similarity_score":0.9664971828}
{"id":2503.15947,"submitter":"Tianyi Hu","authors":"Tianyi Hu, Qingxu Fu, Zhiqiang Pu, Yuan Wang, Tenghai Qiu","title":"Unreal-MAP: Unreal-Engine-Based General Platform for Multi-Agent\n  Reinforcement Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  In this paper, we propose Unreal Multi-Agent Playground (Unreal-MAP), an MARL\ngeneral platform based on the Unreal-Engine (UE). Unreal-MAP allows users to\nfreely create multi-agent tasks using the vast visual and physical resources\navailable in the UE community, and deploy state-of-the-art (SOTA) MARL\nalgorithms within them. Unreal-MAP is user-friendly in terms of deployment,\nmodification, and visualization, and all its components are open-source. We\nalso develop an experimental framework compatible with algorithms ranging from\nrule-based to learning-based provided by third-party frameworks. Lastly, we\ndeploy several SOTA algorithms in example tasks developed via Unreal-MAP, and\nconduct corresponding experimental analyses. We believe Unreal-MAP can play an\nimportant role in the MARL field by closely integrating existing algorithms\nwith user-customized tasks, thus advancing the field of MARL.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:40:41 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Hu', 'Tianyi', ''], ['Fu', 'Qingxu', ''], ['Pu', 'Zhiqiang', ''], ['Wang', 'Yuan', ''], ['Qiu', 'Tenghai', '']]","extracted_entities":"[{'text': 'open-source', 'label': 'Open-source LLMs'}]","assigned_concept":"Open-source LLMs","matched_keyword":"open-source","similarity_score":0.5279436111}
{"id":2503.134,"submitter":"Qi Zhang","authors":"Qi Zhang, Xiuyuan Chen, Ziyi He, Kun Wang, Lianming Wu, Hongxing Shen,\n  and Jianqi Sun","title":"U2AD: Uncertainty-based Unsupervised Anomaly Detection Framework for\n  Detecting T2 Hyperintensity in MRI Spinal Cord","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  T2 hyperintensities in spinal cord MR images are crucial biomarkers for\nconditions such as degenerative cervical myelopathy. However, current clinical\ndiagnoses primarily rely on manual evaluation. Deep learning methods have shown\npromise in lesion detection, but most supervised approaches are heavily\ndependent on large, annotated datasets. Unsupervised anomaly detection (UAD)\noffers a compelling alternative by eliminating the need for abnormal data\nannotations. However, existing UAD methods rely on curated normal datasets and\ntheir performance frequently deteriorates when applied to clinical datasets due\nto domain shifts. We propose an Uncertainty-based Unsupervised Anomaly\nDetection framework, termed U2AD, to address these limitations. Unlike\ntraditional methods, U2AD is designed to be trained and tested within the same\nclinical dataset, following a \"mask-and-reconstruction\" paradigm built on a\nVision Transformer-based architecture. We introduce an uncertainty-guided\nmasking strategy to resolve task conflicts between normal reconstruction and\nanomaly detection to achieve an optimal balance. Specifically, we employ a\nMonte-Carlo sampling technique to estimate reconstruction uncertainty mappings\nduring training. By iteratively optimizing reconstruction training under the\nguidance of both epistemic and aleatoric uncertainty, U2AD reduces overall\nreconstruction variance while emphasizing regions. Experimental results\ndemonstrate that U2AD outperforms existing supervised and unsupervised methods\nin patient-level identification and segment-level localization tasks. This\nframework establishes a new benchmark for incorporating uncertainty guidance\ninto UAD, highlighting its clinical utility in addressing domain shifts and\ntask conflicts in medical image anomaly detection. Our code is available:\nhttps:\/\/github.com\/zhibaishouheilab\/U2AD\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:33:32 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Zhang', 'Qi', ''], ['Chen', 'Xiuyuan', ''], ['He', 'Ziyi', ''], ['Wang', 'Kun', ''], ['Wu', 'Lianming', ''], ['Shen', 'Hongxing', ''], ['Sun', 'Jianqi', '']]","extracted_entities":"[{'text': 'Vision Transformer-based architecture', 'label': 'Transformer-based model'}]","assigned_concept":"Transformer-based model","matched_keyword":"Vision Transformer-based architecture","similarity_score":0.5505564213}
{"id":2306.14858,"submitter":"Dominik Peters","authors":"Nikhil Chandak, Shashwat Goel, Dominik Peters","title":"Proportional Aggregation of Preferences for Sequential Decision Making","comments":"Updated version with improved exposition. Axioms were renamed to\n  better fit the literature","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GT cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We study the problem of fair sequential decision making given voter\npreferences. In each round, a decision rule must choose a decision from a set\nof alternatives where each voter reports which of these alternatives they\napprove. Instead of going with the most popular choice in each round, we aim\nfor proportional representation across rounds, using axioms inspired by the\nmulti-winner voting literature. The axioms require that every group of\n$\\alpha\\%$ of the voters that agrees in every round (i.e., approves a common\nalternative), must approve at least $\\alpha\\%$ of the decisions. A stronger\nversion of the axioms requires that every group of $\\alpha\\%$ of the voters\nthat agrees in a $\\beta$ fraction of rounds must approve $\\beta\\cdot\\alpha\\%$\nof the decisions. We show that three attractive voting rules satisfy axioms of\nthis style. One of them (Sequential Phragm\\'en) makes its decisions online, and\nthe other two satisfy strengthened versions of the axioms but make decisions\nsemi-online (Method of Equal Shares) or fully offline (Proportional Approval\nVoting). We present empirical results for these rules based on synthetic data\nand U.S. political elections. We also run experiments using the moral machine\ndataset about ethical dilemmas: We train preference models on user responses\nfrom different countries and let the models cast votes. We find that\naggregating these votes using our rules leads to a more equal utility\ndistribution across demographics than making decisions using a single global\npreference model.\n","versions":"[{'version': 'v1', 'created': 'Mon, 26 Jun 2023 17:10:10 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 14:25:48 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Chandak', 'Nikhil', ''], ['Goel', 'Shashwat', ''], ['Peters', 'Dominik', '']]","extracted_entities":"[{'text': 'ethical dilemmas', 'label': 'AI Ethics'}]","assigned_concept":"AI Ethics","matched_keyword":"ethical dilemmas","similarity_score":0.5861333609}
{"id":2408.14329,"submitter":"Ghazal Alinezhad Noghre","authors":"Armin Danesh Pazho, Shanle Yao, Ghazal Alinezhad Noghre, Babak Rahimi\n  Ardabili, Vinit Katariya, Hamed Tabkhi","title":"Towards Adaptive Human-centric Video Anomaly Detection: A Comprehensive\n  Framework and A New Benchmark","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Human-centric Video Anomaly Detection (VAD) aims to identify human behaviors\nthat deviate from normal. At its core, human-centric VAD faces substantial\nchallenges, such as the complexity of diverse human behaviors, the rarity of\nanomalies, and ethical constraints. These challenges limit access to\nhigh-quality datasets and highlight the need for a dataset and framework\nsupporting continual learning. Moving towards adaptive human-centric VAD, we\nintroduce the HuVAD (Human-centric privacy-enhanced Video Anomaly Detection)\ndataset and a novel Unsupervised Continual Anomaly Learning (UCAL) framework.\nUCAL enables incremental learning, allowing models to adapt over time, bridging\ntraditional training and real-world deployment. HuVAD prioritizes privacy by\nproviding de-identified annotations and includes seven indoor\/outdoor scenes,\noffering over 5x more pose-annotated frames than previous datasets. Our\nstandard and continual benchmarks, utilize a comprehensive set of metrics,\ndemonstrating that UCAL-enhanced models achieve superior performance in 82.14%\nof cases, setting a new state-of-the-art (SOTA). The dataset can be accessed at\nhttps:\/\/github.com\/TeCSAR-UNCC\/HuVAD.\n","versions":"[{'version': 'v1', 'created': 'Mon, 26 Aug 2024 14:55:23 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 18:13:10 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Pazho', 'Armin Danesh', ''], ['Yao', 'Shanle', ''], ['Noghre', 'Ghazal Alinezhad', ''], ['Ardabili', 'Babak Rahimi', ''], ['Katariya', 'Vinit', ''], ['Tabkhi', 'Hamed', '']]","extracted_entities":"[{'text': 'ethical constraints', 'label': 'AI Ethics'}]","assigned_concept":"AI Ethics","matched_keyword":"ethical constraints","similarity_score":0.5736416578}
{"id":2410.23972,"submitter":"Shaukat Ali","authors":"Muneera Bano, Shaukat Ali, Didar Zowghi","title":"Envisioning Responsible Quantum Software Engineering and Quantum\n  Artificial Intelligence","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The convergence of Quantum Computing (QC), Quantum Software Engineering\n(QSE), and Artificial Intelligence (AI) presents transformative opportunities\nacross various domains. However, existing methodologies inadequately address\nthe ethical, security, and governance challenges arising from this\ntechnological shift. This paper highlights the urgent need for\ninterdisciplinary collaboration to embed ethical principles into the\ndevelopment of Quantum AI (QAI) and QSE, ensuring transparency, inclusivity,\nand equitable global access. Without proactive governance, there is a risk of\ndeepening digital inequalities and consolidating power among a select few. We\ncall on the software engineering community to actively shape a future where\nresponsible QSE and QAI are foundational for ethical, accountable, and socially\nbeneficial technological progress.\n","versions":"[{'version': 'v1', 'created': 'Thu, 31 Oct 2024 14:26:26 GMT'}, {'version': 'v2', 'created': 'Tue, 5 Nov 2024 09:17:50 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 13:03:12 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Bano', 'Muneera', ''], ['Ali', 'Shaukat', ''], ['Zowghi', 'Didar', '']]","extracted_entities":"[{'text': 'ethical principles', 'label': 'AI Ethics'}]","assigned_concept":"AI Ethics","matched_keyword":"ethical principles","similarity_score":0.5676258802}
{"id":2411.04905,"submitter":"Linzheng Chai","authors":"Siming Huang, Tianhao Cheng, J.K. Liu, Jiaran Hao, Liuyihan Song, Yang\n  Xu, J. Yang, Jiaheng Liu, Chenchen Zhang, Linzheng Chai, Ruifeng Yuan,\n  Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang, Zili Wang, Yuan Qi, Yinghui Xu,\n  Wei Chu","title":"OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.PL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large language models (LLMs) for code have become indispensable in various\ndomains, including code generation, reasoning tasks and agent systems. While\nopen-access code LLMs are increasingly approaching the performance levels of\nproprietary models, high-quality code LLMs suitable for rigorous scientific\ninvestigation, particularly those with reproducible data processing pipelines\nand transparent training protocols, remain limited. The scarcity is due to\nvarious challenges, including resource constraints, ethical considerations, and\nthe competitive advantages of keeping models advanced. To address the gap, we\nintroduce OpenCoder, a top-tier code LLM that not only achieves performance\ncomparable to leading models but also serves as an \"open cookbook\" for the\nresearch community. Unlike most prior efforts, we release not only model\nweights and inference code, but also the reproducible training data, complete\ndata processing pipeline, rigorous experimental ablation results, and detailed\ntraining protocols for open scientific research. Through this comprehensive\nrelease, we identify the key ingredients for building a top-tier code LLM: (1)\ncode optimized heuristic rules for data cleaning and methods for data\ndeduplication, (2) recall of text corpus related to code and (3) high-quality\nsynthetic data in both annealing and supervised fine-tuning stages. By offering\nthis level of openness, we aim to broaden access to all aspects of a top-tier\ncode LLM, with OpenCoder serving as both a powerful model and an open\nfoundation to accelerate research, and enable reproducible advancements in code\nAI.\n","versions":"[{'version': 'v1', 'created': 'Thu, 7 Nov 2024 17:47:25 GMT'}, {'version': 'v2', 'created': 'Sat, 9 Nov 2024 17:33:51 GMT'}, {'version': 'v3', 'created': 'Thu, 20 Mar 2025 03:28:56 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Huang', 'Siming', ''], ['Cheng', 'Tianhao', ''], ['Liu', 'J. K.', ''], ['Hao', 'Jiaran', ''], ['Song', 'Liuyihan', ''], ['Xu', 'Yang', ''], ['Yang', 'J.', ''], ['Liu', 'Jiaheng', ''], ['Zhang', 'Chenchen', ''], ['Chai', 'Linzheng', ''], ['Yuan', 'Ruifeng', ''], ['Zhang', 'Zhaoxiang', ''], ['Fu', 'Jie', ''], ['Liu', 'Qian', ''], ['Zhang', 'Ge', ''], ['Wang', 'Zili', ''], ['Qi', 'Yuan', ''], ['Xu', 'Yinghui', ''], ['Chu', 'Wei', '']]","extracted_entities":"[{'text': 'ethical considerations', 'label': 'AI Ethics'}, {'text': 'OpenCoder', 'label': 'Foundation Model'}, {'text': 'OpenCoder', 'label': 'Foundation Model'}]","assigned_concept":"AI Ethics","matched_keyword":"ethical considerations","similarity_score":0.5765206814}
{"id":2502.05206,"submitter":"Xingjun Ma","authors":"Xingjun Ma, Yifeng Gao, Yixu Wang, Ruofan Wang, Xin Wang, Ye Sun,\n  Yifan Ding, Hengyuan Xu, Yunhao Chen, Yunhan Zhao, Hanxun Huang, Yige Li,\n  Jiaming Zhang, Xiang Zheng, Yang Bai, Zuxuan Wu, Xipeng Qiu, Jingfeng Zhang,\n  Yiming Li, Xudong Han, Haonan Li, Jun Sun, Cong Wang, Jindong Gu, Baoyuan Wu,\n  Siheng Chen, Tianwei Zhang, Yang Liu, Mingming Gong, Tongliang Liu, Shirui\n  Pan, Cihang Xie, Tianyu Pang, Yinpeng Dong, Ruoxi Jia, Yang Zhang, Shiqing\n  Ma, Xiangyu Zhang, Neil Gong, Chaowei Xiao, Sarah Erfani, Tim Baldwin, Bo Li,\n  Masashi Sugiyama, Dacheng Tao, James Bailey, Yu-Gang Jiang","title":"Safety at Scale: A Comprehensive Survey of Large Model Safety","comments":"47 pages, 3 figures, 11 tables; GitHub:\n  https:\/\/github.com\/xingjunm\/Awesome-Large-Model-Safety","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI cs.CL cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models.\n","versions":"[{'version': 'v1', 'created': 'Sun, 2 Feb 2025 05:14:22 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Feb 2025 06:16:00 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 16:10:18 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Ma', 'Xingjun', ''], ['Gao', 'Yifeng', ''], ['Wang', 'Yixu', ''], ['Wang', 'Ruofan', ''], ['Wang', 'Xin', ''], ['Sun', 'Ye', ''], ['Ding', 'Yifan', ''], ['Xu', 'Hengyuan', ''], ['Chen', 'Yunhao', ''], ['Zhao', 'Yunhan', ''], ['Huang', 'Hanxun', ''], ['Li', 'Yige', ''], ['Zhang', 'Jiaming', ''], ['Zheng', 'Xiang', ''], ['Bai', 'Yang', ''], ['Wu', 'Zuxuan', ''], ['Qiu', 'Xipeng', ''], ['Zhang', 'Jingfeng', ''], ['Li', 'Yiming', ''], ['Han', 'Xudong', ''], ['Li', 'Haonan', ''], ['Sun', 'Jun', ''], ['Wang', 'Cong', ''], ['Gu', 'Jindong', ''], ['Wu', 'Baoyuan', ''], ['Chen', 'Siheng', ''], ['Zhang', 'Tianwei', ''], ['Liu', 'Yang', ''], ['Gong', 'Mingming', ''], ['Liu', 'Tongliang', ''], ['Pan', 'Shirui', ''], ['Xie', 'Cihang', ''], ['Pang', 'Tianyu', ''], ['Dong', 'Yinpeng', ''], ['Jia', 'Ruoxi', ''], ['Zhang', 'Yang', ''], ['Ma', 'Shiqing', ''], ['Zhang', 'Xiangyu', ''], ['Gong', 'Neil', ''], ['Xiao', 'Chaowei', ''], ['Erfani', 'Sarah', ''], ['Baldwin', 'Tim', ''], ['Li', 'Bo', ''], ['Sugiyama', 'Masashi', ''], ['Tao', 'Dacheng', ''], ['Bailey', 'James', ''], ['Jiang', 'Yu-Gang', '']]","extracted_entities":"[{'text': 'ethical implications', 'label': 'AI Ethics'}]","assigned_concept":"AI Ethics","matched_keyword":"ethical implications","similarity_score":0.5850832462}
{"id":2503.13754,"submitter":"Krti Tallam","authors":"Krti Tallam","title":"From Autonomous Agents to Integrated Systems, A New Paradigm:\n  Orchestrated Distributed Intelligence","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.AI cs.SY","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The rapid evolution of artificial intelligence (AI) has ushered in a new era\nof integrated systems that merge computational prowess with human\ndecision-making. In this paper, we introduce the concept of Orchestrated\nDistributed Intelligence (ODI), a novel paradigm that reconceptualizes AI not\nas isolated autonomous agents, but as cohesive, orchestrated networks that work\nin tandem with human expertise. ODI leverages advanced orchestration layers,\nmulti-loop feedback mechanisms, and a high cognitive density framework to\ntransform static, record-keeping systems into dynamic, action-oriented\nenvironments. Through a comprehensive review of multi-agent system literature,\nrecent technological advances, and practical insights from industry forums, we\nargue that the future of AI lies in integrating distributed intelligence within\nhuman-centric workflows. This approach not only enhances operational efficiency\nand strategic agility but also addresses challenges related to scalability,\ntransparency, and ethical decision-making. Our work outlines key theoretical\nimplications and presents a practical roadmap for future research and\nenterprise innovation, aiming to pave the way for responsible and adaptive AI\nsystems that drive sustainable innovation in human organizations.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 22:21:25 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 02:01:23 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Tallam', 'Krti', '']]","extracted_entities":"[{'text': 'scalability', 'label': 'AI Ethics'}, {'text': 'ethical decision-making', 'label': 'AI Ethics'}]","assigned_concept":"AI Ethics","matched_keyword":"ethical decision-making","similarity_score":0.5962504148}
{"id":2503.14563,"submitter":"Suzana Veljanovska","authors":"Suzana Veljanovska and Hans Dermot Doran","title":"Workflow for Safe-AI","comments":"Embedded World Conference, Nuremberg, 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  The development and deployment of safe and dependable AI models is crucial in\napplications where functional safety is a key concern. Given the rapid\nadvancement in AI research and the relative novelty of the safe-AI domain,\nthere is an increasing need for a workflow that balances stability with\nadaptability. This work proposes a transparent, complete, yet flexible and\nlightweight workflow that highlights both reliability and qualifiability. The\ncore idea is that the workflow must be qualifiable, which demands the use of\nqualified tools. Tool qualification is a resource-intensive process, both in\nterms of time and cost. We therefore place value on a lightweight workflow\nfeaturing a minimal number of tools with limited features. The workflow is\nbuilt upon an extended ONNX model description allowing for validation of AI\nalgorithms from their generation to runtime deployment. This validation is\nessential to ensure that models are validated before being reliably deployed\nacross different runtimes, particularly in mixed-criticality systems.\nKeywords-AI workflows, safe-AI, dependable-AI, functional safety, v-model\ndevelopment\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 07:45:18 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 07:32:39 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Veljanovska', 'Suzana', ''], ['Doran', 'Hans Dermot', '']]","extracted_entities":"[{'text': 'functional safety', 'label': 'AI Ethics'}, {'text': 'safe-AI', 'label': 'AI Ethics'}, {'text': 'functional safety', 'label': 'AI Ethics'}]","assigned_concept":"AI Ethics","matched_keyword":"safe-AI","similarity_score":0.5514620543}
{"id":2503.1537,"submitter":"Steve Benford","authors":"Steve Benford, Rachael Garrett, Christine Li, Paul Tennent, Claudia\n  N\\'u\\~nez-Pacheco, Ayse Kucukyilmaz, Vasiliki Tsaknaki, Kristina H\\\"o\\\"ok,\n  Praminda Caleb-Solly, Joe Marshall, Eike Schneiders, Kristina Popova, Jude\n  Afana","title":"Tangles: Unpacking Extended Collision Experiences with Soma Trajectories","comments":"32 pages, 13 figures","journal-ref":null,"doi":"10.1145\/3723875","report-no":null,"categories":"cs.RO cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We reappraise the idea of colliding with robots, moving from a position that\ntries to avoid or mitigate collisions to one that considers them an important\nfacet of human interaction. We report on a soma design workshop that explored\nhow our bodies could collide with telepresence robots, mobility aids, and a\nquadruped robot. Based on our findings, we employed soma trajectories to\nanalyse collisions as extended experiences that negotiate key transitions of\nconsent, preparation, launch, contact, ripple, sting, untangle, debris and\nreflect. We then employed these ideas to analyse two collision experiences, an\naccidental collision between a person and a drone, and the deliberate design of\na robot to play with cats, revealing how real-world collisions involve the\ncomplex and ongoing entanglement of soma trajectories. We discuss how viewing\ncollisions as entangled trajectories, or tangles, can be used analytically, as\na design approach, and as a lens to broach ethical complexity.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:09:52 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 06:50:52 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Benford', 'Steve', ''], ['Garrett', 'Rachael', ''], ['Li', 'Christine', ''], ['Tennent', 'Paul', ''], ['N\u00fa\u00f1ez-Pacheco', 'Claudia', ''], ['Kucukyilmaz', 'Ayse', ''], ['Tsaknaki', 'Vasiliki', ''], ['H\u00f6\u00f6k', 'Kristina', ''], ['Caleb-Solly', 'Praminda', ''], ['Marshall', 'Joe', ''], ['Schneiders', 'Eike', ''], ['Popova', 'Kristina', ''], ['Afana', 'Jude', '']]","extracted_entities":"[{'text': 'ethical complexity', 'label': 'AI Ethics'}]","assigned_concept":"AI Ethics","matched_keyword":"ethical complexity","similarity_score":0.6363573074}
{"id":2503.15682,"submitter":"Blair Attard-Frost","authors":"Blair Attard-Frost","title":"Transfeminist AI Governance","comments":"37 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  This article re-imagines the governance of artificial intelligence (AI)\nthrough a transfeminist lens, focusing on challenges of power, participation,\nand injustice, and on opportunities for advancing equity, community-based\nresistance, and transformative change. AI governance is a field of research and\npractice seeking to maximize benefits and minimize harms caused by AI systems.\nUnfortunately, AI governance practices are frequently ineffective at preventing\nAI systems from harming people and the environment, with historically\nmarginalized groups such as trans people being particularly vulnerable to harm.\nBuilding upon trans and feminist theories of ethics, I introduce an approach to\ntransfeminist AI governance. Applying a transfeminist lens in combination with\na critical self-reflexivity methodology, I retroactively reinterpret findings\nfrom three empirical studies of AI governance practices in Canada and globally.\nIn three reflections on my findings, I show that large-scale AI governance\nsystems structurally prioritize the needs of industry over marginalized\ncommunities. As a result, AI governance is limited by power imbalances and\nexclusionary norms. This research shows that re-grounding AI governance in\ntransfeminist ethical principles can support AI governance researchers,\npractitioners, and organizers in addressing those limitations.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 20:25:59 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Attard-Frost', 'Blair', '']]","extracted_entities":"[{'text': 'AI governance', 'label': 'AI Ethics'}, {'text': 'trans and feminist theories of ethics', 'label': 'AI Ethics'}, {'text': 'transfeminist AI governance', 'label': 'AI Ethics'}, {'text': 'AI governance', 'label': 'AI Ethics'}, {'text': 'AI governance', 'label': 'AI Ethics'}, {'text': 'transfeminist ethical principles', 'label': 'AI Ethics'}]","assigned_concept":"AI Ethics","matched_keyword":"AI governance","similarity_score":0.6930015683}
{"id":2503.16233,"submitter":"Dawood Wasif","authors":"Dawood Wasif, Dian Chen, Sindhuja Madabushi, Nithin Alluru, Terrence\n  J. Moore, Jin-Hee Cho","title":"Empirical Analysis of Privacy-Fairness-Accuracy Trade-offs in Federated\n  Learning: A Step Towards Responsible AI","comments":"Submitted to IJCAI 2025 (under review)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CR cs.DC cs.ET","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Federated Learning (FL) enables collaborative machine learning while\npreserving data privacy but struggles to balance privacy preservation (PP) and\nfairness. Techniques like Differential Privacy (DP), Homomorphic Encryption\n(HE), and Secure Multi-Party Computation (SMC) protect sensitive data but\nintroduce trade-offs. DP enhances privacy but can disproportionately impact\nunderrepresented groups, while HE and SMC mitigate fairness concerns at the\ncost of computational overhead. This work explores the privacy-fairness\ntrade-offs in FL under IID (Independent and Identically Distributed) and\nnon-IID data distributions, benchmarking q-FedAvg, q-MAML, and Ditto on diverse\ndatasets. Our findings highlight context-dependent trade-offs and offer\nguidelines for designing FL systems that uphold responsible AI principles,\nensuring fairness, privacy, and equitable real-world applications.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 15:31:01 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wasif', 'Dawood', ''], ['Chen', 'Dian', ''], ['Madabushi', 'Sindhuja', ''], ['Alluru', 'Nithin', ''], ['Moore', 'Terrence J.', ''], ['Cho', 'Jin-Hee', '']]","extracted_entities":"[{'text': 'Federated Learning', 'label': 'Zero-shot Learning'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'responsible AI principles', 'label': 'AI Ethics'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}]","assigned_concept":"AI Ethics","matched_keyword":"responsible AI principles","similarity_score":0.7095302939}
{"id":2403.20309,"submitter":"Zhiwen Fan","authors":"Zhiwen Fan, Kairun Wen, Wenyan Cong, Kevin Wang, Jian Zhang, Xinghao\n  Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, Zhangyang\n  Wang, Yue Wang","title":"InstantSplat: Sparse-view Gaussian Splatting in Seconds","comments":"Project Page: https:\/\/instantsplat.github.io\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  While neural 3D reconstruction has advanced substantially, its performance\nsignificantly degrades with sparse-view data, which limits its broader\napplicability, since SfM is often unreliable in sparse-view scenarios where\nfeature matches are scarce. In this paper, we introduce InstantSplat, a novel\napproach for addressing sparse-view 3D scene reconstruction at lightning-fast\nspeed. InstantSplat employs a self-supervised framework that optimizes 3D scene\nrepresentation and camera poses by unprojecting 2D pixels into 3D space and\naligning them using differentiable neural rendering. The optimization process\nis initialized with a large-scale trained geometric foundation model, which\nprovides dense priors that yield initial points through model inference, after\nwhich we further optimize all scene parameters using photometric errors. To\nmitigate redundancy introduced by the prior model, we propose a\nco-visibility-based geometry initialization, and a Gaussian-based bundle\nadjustment is employed to rapidly adapt both the scene representation and\ncamera parameters without relying on a complex adaptive density control\nprocess. Overall, InstantSplat is compatible with multiple point-based\nrepresentations for view synthesis and surface reconstruction. It achieves an\nacceleration of over 30x in reconstruction and improves visual quality (SSIM)\nfrom 0.3755 to 0.7624 compared to traditional SfM with 3D-GS.\n","versions":"[{'version': 'v1', 'created': 'Fri, 29 Mar 2024 17:29:58 GMT'}, {'version': 'v2', 'created': 'Sun, 30 Jun 2024 19:47:58 GMT'}, {'version': 'v3', 'created': 'Tue, 20 Aug 2024 20:57:47 GMT'}, {'version': 'v4', 'created': 'Tue, 17 Dec 2024 18:59:07 GMT'}, {'version': 'v5', 'created': 'Mon, 17 Mar 2025 22:39:06 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Fan', 'Zhiwen', ''], ['Wen', 'Kairun', ''], ['Cong', 'Wenyan', ''], ['Wang', 'Kevin', ''], ['Zhang', 'Jian', ''], ['Ding', 'Xinghao', ''], ['Xu', 'Danfei', ''], ['Ivanovic', 'Boris', ''], ['Pavone', 'Marco', ''], ['Pavlakos', 'Georgios', ''], ['Wang', 'Zhangyang', ''], ['Wang', 'Yue', '']]","extracted_entities":"[{'text': 'large-scale trained geometric foundation model', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"large-scale trained geometric foundation model","similarity_score":0.6818820834}
{"id":2404.15786,"submitter":"Christian Ledig","authors":"Sebastian Doerrich, Francesco Di Salvo, Julius Brockmann, Christian\n  Ledig","title":"Rethinking model prototyping through the MedMNIST+ dataset collection","comments":null,"journal-ref":"Scientific Reports 15, 7669 (2025)","doi":"10.1038\/s41598-025-92156-9","report-no":null,"categories":"eess.IV cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The integration of deep learning based systems in clinical practice is often\nimpeded by challenges rooted in limited and heterogeneous medical datasets. In\naddition, the field has increasingly prioritized marginal performance gains on\na few, narrowly scoped benchmarks over clinical applicability, slowing down\nmeaningful algorithmic progress. This trend often results in excessive\nfine-tuning of existing methods on selected datasets rather than fostering\nclinically relevant innovations. In response, this work introduces a\ncomprehensive benchmark for the MedMNIST+ dataset collection, designed to\ndiversify the evaluation landscape across several imaging modalities,\nanatomical regions, classification tasks and sample sizes. We systematically\nreassess commonly used Convolutional Neural Networks (CNNs) and Vision\nTransformer (ViT) architectures across distinct medical datasets, training\nmethodologies, and input resolutions to validate and refine existing\nassumptions about model effectiveness and development. Our findings suggest\nthat computationally efficient training schemes and modern foundation models\noffer viable alternatives to costly end-to-end training. Additionally, we\nobserve that higher image resolutions do not consistently improve performance\nbeyond a certain threshold. This highlights the potential benefits of using\nlower resolutions, particularly in prototyping stages, to reduce computational\ndemands without sacrificing accuracy. Notably, our analysis reaffirms the\ncompetitiveness of CNNs compared to ViTs, emphasizing the importance of\ncomprehending the intrinsic capabilities of different architectures. Finally,\nby establishing a standardized evaluation framework, we aim to enhance\ntransparency, reproducibility, and comparability within the MedMNIST+ dataset\ncollection. Code is available at\nhttps:\/\/github.com\/sdoerrich97\/rethinking-model-prototyping-MedMNISTPlus .\n","versions":"[{'version': 'v1', 'created': 'Wed, 24 Apr 2024 10:19:25 GMT'}, {'version': 'v2', 'created': 'Tue, 7 May 2024 20:49:46 GMT'}, {'version': 'v3', 'created': 'Mon, 17 Mar 2025 12:01:18 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Doerrich', 'Sebastian', ''], ['Di Salvo', 'Francesco', ''], ['Brockmann', 'Julius', ''], ['Ledig', 'Christian', '']]","extracted_entities":"[{'text': 'excessive\\nfine-tuning', 'label': 'Fine-tuning'}, {'text': 'modern foundation models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"modern foundation models","similarity_score":0.8904472589}
{"id":2406.05797,"submitter":"Qizhi Pei","authors":"Qizhi Pei, Rui Yan, Kaiyuan Gao, Jinhua Zhu, Lijun Wu","title":"3D-MolT5: Leveraging Discrete Structural Information for Molecule-Text\n  Modeling","comments":"Accepted by ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"q-bio.BM cs.AI cs.CE cs.CL cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The integration of molecular and natural language representations has emerged\nas a focal point in molecular science, with recent advancements in Language\nModels (LMs) demonstrating significant potential for comprehensive modeling of\nboth domains. However, existing approaches face notable limitations,\nparticularly in their neglect of three-dimensional (3D) information, which is\ncrucial for understanding molecular structures and functions. While some\nefforts have been made to incorporate 3D molecular information into LMs using\nexternal structure encoding modules, significant difficulties remain, such as\ninsufficient interaction across modalities in pre-training and challenges in\nmodality alignment. To address the limitations, we propose \\textbf{3D-MolT5}, a\nunified framework designed to model molecule in both sequence and 3D structure\nspaces. The key innovation of our approach lies in mapping fine-grained 3D\nsubstructure representations into a specialized 3D token vocabulary. This\nmethodology facilitates the seamless integration of sequence and structure\nrepresentations in a tokenized format, enabling 3D-MolT5 to encode molecular\nsequences, molecular structures, and text sequences within a unified\narchitecture. Leveraging this tokenized input strategy, we build a foundation\nmodel that unifies the sequence and structure data formats. We then conduct\njoint pre-training with multi-task objectives to enhance the model's\ncomprehension of these diverse modalities within a shared representation space.\nThus, our approach significantly improves cross-modal interaction and\nalignment, addressing key challenges in previous work. Further instruction\ntuning demonstrated that our 3D-MolT5 has strong generalization ability and\nsurpasses existing methods with superior performance in multiple downstream\ntasks. Our code is available at https:\/\/github.com\/QizhiPei\/3D-MolT5.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Jun 2024 14:20:55 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 08:03:45 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Pei', 'Qizhi', ''], ['Yan', 'Rui', ''], ['Gao', 'Kaiyuan', ''], ['Zhu', 'Jinhua', ''], ['Wu', 'Lijun', '']]","extracted_entities":"[{'text': '3D-MolT5', 'label': 'Foundation Model'}, {'text': 'foundation\\nmodel', 'label': 'Foundation Model'}, {'text': 'instruction\\ntuning', 'label': 'Fine-tuning'}, {'text': '3D-MolT5', 'label': 'Foundation Model'}, {'text': '3D-MolT5', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation\nmodel","similarity_score":1.0}
{"id":2409.16073,"submitter":"Sunoh Lee","authors":"Sunoh Lee, Minsik Jeon, Jihong Min, Junwon Seo","title":"OW-Rep: Open World Object Detection with Instance Representation\n  Learning","comments":"Our project website can be found at\n  https:\/\/sunohlee.github.io\/OW-Rep\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Open World Object Detection(OWOD) addresses realistic scenarios where unseen\nobject classes emerge, enabling detectors trained on known classes to detect\nunknown objects and incrementally incorporate the knowledge they provide. While\nexisting OWOD methods primarily focus on detecting unknown objects, they often\noverlook the rich semantic relationships between detected objects, which are\nessential for scene understanding and applications in open-world environments\n(e.g., open-world tracking and novel class discovery). In this paper, we extend\nthe OWOD framework to jointly detect unknown objects and learn semantically\nrich instance embeddings, enabling the detector to capture fine-grained\nsemantic relationships between instances. To this end, we propose two modules\nthat leverage the rich and generalizable knowledge of Vision Foundation\nModels(VFM). First, the Unknown Box Refine Module uses instance masks from the\nSegment Anything Model to accurately localize unknown objects. The Embedding\nTransfer Module then distills instance-wise semantic similarities from VFM\nfeatures to the detector's embeddings via a relaxed contrastive loss, enabling\nthe detector to learn a semantically meaningful and generalizable instance\nfeature. Extensive experiments show that our method significantly improves both\nunknown object detection and instance embedding quality, while also enhancing\nperformance in downstream tasks such as open-world tracking.\n","versions":"[{'version': 'v1', 'created': 'Tue, 24 Sep 2024 13:13:34 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 04:24:20 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Lee', 'Sunoh', ''], ['Jeon', 'Minsik', ''], ['Min', 'Jihong', ''], ['Seo', 'Junwon', '']]","extracted_entities":"[{'text': 'Vision Foundation\\nModels', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"Vision Foundation\nModels","similarity_score":0.6914944053}
{"id":2410.14633,"submitter":"Yuxiang Lu","authors":"Yuxiang Lu, Shengcao Cao, Yu-Xiong Wang","title":"Swiss Army Knife: Synergizing Biases in Knowledge from Vision Foundation\n  Models for Multi-Task Learning","comments":"Accepted by ICLR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Vision Foundation Models (VFMs) have demonstrated outstanding performance on\nnumerous downstream tasks. However, due to their inherent representation biases\noriginating from different training paradigms, VFMs exhibit advantages and\ndisadvantages across distinct vision tasks. Although amalgamating the strengths\nof multiple VFMs for downstream tasks is an intuitive strategy, effectively\nexploiting these biases remains a significant challenge. In this paper, we\npropose a novel and versatile \"Swiss Army Knife\" (SAK) solution, which\nadaptively distills knowledge from a committee of VFMs to enhance multi-task\nlearning. Unlike existing methods that use a single backbone for knowledge\ntransfer, our approach preserves the unique representation bias of each teacher\nby collaborating the lightweight Teacher-Specific Adapter Path modules with the\nTeacher-Agnostic Stem. Through dynamic selection and combination of\nrepresentations with Mixture-of-Representations Routers, our SAK is capable of\nsynergizing the complementary strengths of multiple VFMs. Extensive experiments\nshow that our SAK remarkably outperforms prior state of the arts in multi-task\nlearning by 10% on the NYUD-v2 benchmark, while also providing a flexible and\nrobust framework that can readily accommodate more advanced model designs.\nProject page: https:\/\/innovator-zero.github.io\/SAK\/ .\n","versions":"[{'version': 'v1', 'created': 'Fri, 18 Oct 2024 17:32:39 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 07:47:41 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Lu', 'Yuxiang', ''], ['Cao', 'Shengcao', ''], ['Wang', 'Yu-Xiong', '']]","extracted_entities":"[{'text': 'Vision Foundation Models', 'label': 'Foundation Model'}, {'text': 'VFMs', 'label': 'Foundation Model'}, {'text': 'VFMs', 'label': 'Foundation Model'}, {'text': 'VFMs', 'label': 'Foundation Model'}, {'text': 'VFMs', 'label': 'Foundation Model'}, {'text': 'multi-task\\nlearning', 'label': 'Few-shot Learning'}, {'text': 'Teacher-Agnostic Stem', 'label': 'Embedding'}, {'text': 'Mixture-of-Representations Routers', 'label': 'Transformers'}, {'text': 'multi-task\\nlearning', 'label': 'Few-shot Learning'}]","assigned_concept":"Foundation Model","matched_keyword":"Vision Foundation Models","similarity_score":0.6914944053}
{"id":2410.24119,"submitter":"Akash Dhruv","authors":"Akash Dhruv, Anshu Dubey","title":"Leveraging Large Language Models for Code Translation and Software\n  Development in Scientific Computing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The emergence of foundational models and generative artificial intelligence\n(GenAI) is poised to transform productivity in scientific computing, especially\nin code development, refactoring, and translating from one programming language\nto another. However, because the output of GenAI cannot be guaranteed to be\ncorrect, manual intervention remains necessary. Some of this intervention can\nbe automated through task-specific tools, alongside additional methodologies\nfor correctness verification and effective prompt development. We explored the\napplication of GenAI in assisting with code translation, language\ninteroperability, and codebase inspection within a legacy Fortran codebase used\nto simulate particle interactions at the Large Hadron Collider (LHC). In the\nprocess, we developed a tool, CodeScribe, which combines prompt engineering\nwith user supervision to establish an efficient process for code conversion. In\nthis paper, we demonstrate how CodeScribe assists in converting Fortran code to\nC++, generating Fortran-C APIs for integrating legacy systems with modern C++\nlibraries, and providing developer support for code organization and algorithm\nimplementation. We also address the challenges of AI-driven code translation\nand highlight its benefits for enhancing productivity in scientific computing\nworkflows.\n","versions":"[{'version': 'v1', 'created': 'Thu, 31 Oct 2024 16:48:41 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 02:38:43 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Dhruv', 'Akash', ''], ['Dubey', 'Anshu', '']]","extracted_entities":"[{'text': 'foundational models', 'label': 'Foundation Model'}, {'text': 'prompt development', 'label': 'Prompting'}, {'text': 'prompt engineering', 'label': 'Prompting'}]","assigned_concept":"Foundation Model","matched_keyword":"foundational models","similarity_score":0.8593510985}
{"id":2411.09361,"submitter":"Zepeng Frazier Huo","authors":"Zepeng Huo, Jason Alan Fries, Alejandro Lozano, Jeya Maria Jose\n  Valanarasu, Ethan Steinberg, Louis Blankemeier, Akshay S. Chaudhari, Curtis\n  Langlotz, and Nigam H. Shah","title":"Time-to-Event Pretraining for 3D Medical Imaging","comments":"34 pages, 19 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  With the rise of medical foundation models and the growing availability of\nimaging data, scalable pretraining techniques offer a promising way to identify\nimaging biomarkers predictive of future disease risk. While current\nself-supervised methods for 3D medical imaging models capture local structural\nfeatures like organ morphology, they fail to link pixel biomarkers with\nlong-term health outcomes due to a missing context problem. Current approaches\nlack the temporal context necessary to identify biomarkers correlated with\ndisease progression, as they rely on supervision derived only from images and\nconcurrent text descriptions. To address this, we introduce time-to-event\npretraining, a pretraining framework for 3D medical imaging models that\nleverages large-scale temporal supervision from paired, longitudinal electronic\nhealth records (EHRs). Using a dataset of 18,945 CT scans (4.2 million 2D\nimages) and time-to-event distributions across thousands of EHR-derived tasks,\nour method improves outcome prediction, achieving an average AUROC increase of\n23.7% and a 29.4% gain in Harrell's C-index across 8 benchmark tasks.\nImportantly, these gains are achieved without sacrificing diagnostic\nclassification performance. This study lays the foundation for integrating\nlongitudinal EHR and 3D imaging data to advance clinical risk prediction.\n","versions":"[{'version': 'v1', 'created': 'Thu, 14 Nov 2024 11:08:54 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 07:33:47 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Huo', 'Zepeng', ''], ['Fries', 'Jason Alan', ''], ['Lozano', 'Alejandro', ''], ['Valanarasu', 'Jeya Maria Jose', ''], ['Steinberg', 'Ethan', ''], ['Blankemeier', 'Louis', ''], ['Chaudhari', 'Akshay S.', ''], ['Langlotz', 'Curtis', ''], ['Shah', 'Nigam H.', '']]","extracted_entities":"[{'text': 'medical foundation models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"medical foundation models","similarity_score":0.7281274796}
{"id":2412.03142,"submitter":"Shijie Wu","authors":"Shijie Wu, Yihang Zhu, Yunao Huang, Kaizhen Zhu, Jiayuan Gu, Jingyi\n  Yu, Ye Shi, Jingya Wang","title":"AffordDP: Generalizable Diffusion Policy with Transferable Affordance","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Diffusion-based policies have shown impressive performance in robotic\nmanipulation tasks while struggling with out-of-domain distributions. Recent\nefforts attempted to enhance generalization by improving the visual feature\nencoding for diffusion policy. However, their generalization is typically\nlimited to the same category with similar appearances. Our key insight is that\nleveraging affordances--manipulation priors that define \"where\" and \"how\" an\nagent interacts with an object--can substantially enhance generalization to\nentirely unseen object instances and categories. We introduce the Diffusion\nPolicy with transferable Affordance (AffordDP), designed for generalizable\nmanipulation across novel categories. AffordDP models affordances through 3D\ncontact points and post-contact trajectories, capturing the essential static\nand dynamic information for complex tasks. The transferable affordance from\nin-domain data to unseen objects is achieved by estimating a 6D transformation\nmatrix using foundational vision models and point cloud registration\ntechniques. More importantly, we incorporate affordance guidance during\ndiffusion sampling that can refine action sequence generation. This guidance\ndirects the generated action to gradually move towards the desired manipulation\nfor unseen objects while keeping the generated action within the manifold of\naction space. Experimental results from both simulated and real-world\nenvironments demonstrate that AffordDP consistently outperforms previous\ndiffusion-based methods, successfully generalizing to unseen instances and\ncategories where others fail.\n","versions":"[{'version': 'v1', 'created': 'Wed, 4 Dec 2024 09:08:07 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 10:03:41 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wu', 'Shijie', ''], ['Zhu', 'Yihang', ''], ['Huang', 'Yunao', ''], ['Zhu', 'Kaizhen', ''], ['Gu', 'Jiayuan', ''], ['Yu', 'Jingyi', ''], ['Shi', 'Ye', ''], ['Wang', 'Jingya', '']]","extracted_entities":"[{'text': 'foundational vision models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundational vision models","similarity_score":0.6603088379}
{"id":2412.10831,"submitter":"Dengyang Jiang","authors":"Dengyang Jiang, Haoyu Wang, Lei Zhang, Wei Wei, Guang Dai, Mengmeng\n  Wang, Jingdong Wang, Yanning Zhang","title":"Low-Biased General Annotated Dataset Generation","comments":"CVPR2025 Accepted Paper","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Pre-training backbone networks on a general annotated dataset (e.g.,\nImageNet) that comprises numerous manually collected images with category\nannotations has proven to be indispensable for enhancing the generalization\ncapacity of downstream visual tasks. However, those manually collected images\noften exhibit bias, which is non-transferable across either categories or\ndomains, thus causing the model's generalization capacity degeneration. To\nmitigate this problem, we present a low-biased general annotated dataset\ngeneration framework (lbGen). Instead of expensive manual collection, we aim at\ndirectly generating low-biased images with category annotations. To achieve\nthis goal, we propose to leverage the advantage of a multimodal foundation\nmodel (e.g., CLIP), in terms of aligning images in a low-biased semantic space\ndefined by language. Specifically, we develop a bi-level semantic alignment\nloss, which not only forces all generated images to be consistent with the\nsemantic distribution of all categories belonging to the target dataset in an\nadversarial learning manner, but also requires each generated image to match\nthe semantic description of its category name. In addition, we further cast an\nexisting image quality scoring model into a quality assurance loss to preserve\nthe quality of the generated image. By leveraging these two loss functions, we\ncan obtain a low-biased image generation model by simply fine-tuning a\npre-trained diffusion model using only all category names in the target dataset\nas input. Experimental results confirm that, compared with the manually labeled\ndataset or other synthetic datasets, the utilization of our generated\nlow-biased dataset leads to stable generalization capacity enhancement of\ndifferent backbone networks across various tasks, especially in tasks where the\nmanually labeled samples are scarce.\n","versions":"[{'version': 'v1', 'created': 'Sat, 14 Dec 2024 13:28:40 GMT'}, {'version': 'v2', 'created': 'Mon, 3 Mar 2025 06:13:35 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 12:36:47 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Jiang', 'Dengyang', ''], ['Wang', 'Haoyu', ''], ['Zhang', 'Lei', ''], ['Wei', 'Wei', ''], ['Dai', 'Guang', ''], ['Wang', 'Mengmeng', ''], ['Wang', 'Jingdong', ''], ['Zhang', 'Yanning', '']]","extracted_entities":"[{'text': 'lbGen', 'label': 'Foundation Model'}, {'text': 'multimodal foundation\\nmodel', 'label': 'Foundation Model'}, {'text': 'CLIP', 'label': 'Foundation Model'}, {'text': 'adversarial learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Foundation Model","matched_keyword":"multimodal foundation\nmodel","similarity_score":0.7334585786}
{"id":2412.16178,"submitter":"Michael Wornow","authors":"Michael Wornow, Suhana Bedi, Miguel Angel Fuentes Hernandez, Ethan\n  Steinberg, Jason Alan Fries, Christopher Re, Sanmi Koyejo, Nigam H. Shah","title":"Context Clues: Evaluating Long Context Models for Clinical Prediction\n  Tasks on EHRs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Foundation Models (FMs) trained on Electronic Health Records (EHRs) have\nachieved state-of-the-art results on numerous clinical prediction tasks.\nHowever, most existing EHR FMs have context windows of <1k tokens. This\nprevents them from modeling full patient EHRs which can exceed 10k's of events.\nRecent advancements in subquadratic long-context architectures (e.g., Mamba)\noffer a promising solution. However, their application to EHR data has not been\nwell-studied. We address this gap by presenting the first systematic evaluation\nof the effect of context length on modeling EHR data. We find that longer\ncontext models improve predictive performance -- our Mamba-based model\nsurpasses the prior state-of-the-art on 9\/14 tasks on the EHRSHOT prediction\nbenchmark. For clinical applications, however, model performance alone is\ninsufficient -- robustness to the unique properties of EHR is crucial. Thus, we\nalso evaluate models across three previously underexplored properties of EHR\ndata: (1) the prevalence of \"copy-forwarded\" diagnoses which creates artificial\nrepetition of tokens within EHR sequences; (2) the irregular time intervals\nbetween EHR events which can lead to a wide range of timespans within a context\nwindow; and (3) the natural increase in disease complexity over time which\nmakes later tokens in the EHR harder to predict than earlier ones. Stratifying\nour EHRSHOT results, we find that higher levels of each property correlate\nnegatively with model performance, but that longer context models are more\nrobust to more extreme levels of these properties. Our work highlights the\npotential for using long-context architectures to model EHR data, and offers a\ncase study for identifying new challenges in modeling sequential data motivated\nby domains outside of natural language. We release our models and code at:\nhttps:\/\/github.com\/som-shahlab\/long_context_clues\n","versions":"[{'version': 'v1', 'created': 'Mon, 9 Dec 2024 21:58:27 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 18:04:32 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Wornow', 'Michael', ''], ['Bedi', 'Suhana', ''], ['Hernandez', 'Miguel Angel Fuentes', ''], ['Steinberg', 'Ethan', ''], ['Fries', 'Jason Alan', ''], ['Re', 'Christopher', ''], ['Koyejo', 'Sanmi', ''], ['Shah', 'Nigam H.', '']]","extracted_entities":"[{'text': 'Foundation Models', 'label': 'Foundation Model'}, {'text': 'Mamba', 'label': 'contextual Embedding'}]","assigned_concept":"Foundation Model","matched_keyword":"Foundation Models","similarity_score":0.9628887177}
{"id":2501.03575,"submitter":"Yogesh Balaji","authors":"NVIDIA: Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik\n  Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan\n  Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni,\n  Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth\n  Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin,\n  Seung Wook Kim, Gergely Kl\\'ar, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi\n  Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian\n  Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun\n  Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel,\n  Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik\n  Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne\n  Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang\n  Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu,\n  Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang,\n  Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, Artur Zolkowski","title":"Cosmos World Foundation Model Platform for Physical AI","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Physical AI needs to be trained digitally first. It needs a digital twin of\nitself, the policy model, and a digital twin of the world, the world model. In\nthis paper, we present the Cosmos World Foundation Model Platform to help\ndevelopers build customized world models for their Physical AI setups. We\nposition a world foundation model as a general-purpose world model that can be\nfine-tuned into customized world models for downstream applications. Our\nplatform covers a video curation pipeline, pre-trained world foundation models,\nexamples of post-training of pre-trained world foundation models, and video\ntokenizers. To help Physical AI builders solve the most critical problems of\nour society, we make Cosmos open-source and our models open-weight with\npermissive licenses available via\nhttps:\/\/github.com\/nvidia-cosmos\/cosmos-predict1.\n","versions":"[{'version': 'v1', 'created': 'Tue, 7 Jan 2025 06:55:50 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 16:59:07 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['NVIDIA', '', ''], [':', '', ''], ['Agarwal', 'Niket', ''], ['Ali', 'Arslan', ''], ['Bala', 'Maciej', ''], ['Balaji', 'Yogesh', ''], ['Barker', 'Erik', ''], ['Cai', 'Tiffany', ''], ['Chattopadhyay', 'Prithvijit', ''], ['Chen', 'Yongxin', ''], ['Cui', 'Yin', ''], ['Ding', 'Yifan', ''], ['Dworakowski', 'Daniel', ''], ['Fan', 'Jiaojiao', ''], ['Fenzi', 'Michele', ''], ['Ferroni', 'Francesco', ''], ['Fidler', 'Sanja', ''], ['Fox', 'Dieter', ''], ['Ge', 'Songwei', ''], ['Ge', 'Yunhao', ''], ['Gu', 'Jinwei', ''], ['Gururani', 'Siddharth', ''], ['He', 'Ethan', ''], ['Huang', 'Jiahui', ''], ['Huffman', 'Jacob', ''], ['Jannaty', 'Pooya', ''], ['Jin', 'Jingyi', ''], ['Kim', 'Seung Wook', ''], ['Kl\u00e1r', 'Gergely', ''], ['Lam', 'Grace', ''], ['Lan', 'Shiyi', ''], ['Leal-Taixe', 'Laura', ''], ['Li', 'Anqi', ''], ['Li', 'Zhaoshuo', ''], ['Lin', 'Chen-Hsuan', ''], ['Lin', 'Tsung-Yi', ''], ['Ling', 'Huan', ''], ['Liu', 'Ming-Yu', ''], ['Liu', 'Xian', ''], ['Luo', 'Alice', ''], ['Ma', 'Qianli', ''], ['Mao', 'Hanzi', ''], ['Mo', 'Kaichun', ''], ['Mousavian', 'Arsalan', ''], ['Nah', 'Seungjun', ''], ['Niverty', 'Sriharsha', ''], ['Page', 'David', ''], ['Paschalidou', 'Despoina', ''], ['Patel', 'Zeeshan', ''], ['Pavao', 'Lindsey', ''], ['Ramezanali', 'Morteza', ''], ['Reda', 'Fitsum', ''], ['Ren', 'Xiaowei', ''], ['Sabavat', 'Vasanth Rao Naik', ''], ['Schmerling', 'Ed', ''], ['Shi', 'Stella', ''], ['Stefaniak', 'Bartosz', ''], ['Tang', 'Shitao', ''], ['Tchapmi', 'Lyne', ''], ['Tredak', 'Przemek', ''], ['Tseng', 'Wei-Cheng', ''], ['Varghese', 'Jibin', ''], ['Wang', 'Hao', ''], ['Wang', 'Haoxiang', ''], ['Wang', 'Heng', ''], ['Wang', 'Ting-Chun', ''], ['Wei', 'Fangyin', ''], ['Wei', 'Xinyue', ''], ['Wu', 'Jay Zhangjie', ''], ['Xu', 'Jiashu', ''], ['Yang', 'Wei', ''], ['Yen-Chen', 'Lin', ''], ['Zeng', 'Xiaohui', ''], ['Zeng', 'Yu', ''], ['Zhang', 'Jing', ''], ['Zhang', 'Qinsheng', ''], ['Zhang', 'Yuxuan', ''], ['Zhao', 'Qingqing', ''], ['Zolkowski', 'Artur', '']]","extracted_entities":"[{'text': 'world foundation model', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"world foundation model","similarity_score":0.7631572485}
{"id":2501.11039,"submitter":"Yun Qu","authors":"Qi Cheems Wang, Zehao Xiao, Yixiu Mao, Yun Qu, Jiayi Shen, Yiqin Lv,\n  Xiangyang Ji","title":"Beyond Any-Shot Adaptation: Predicting Optimization Outcome for\n  Robustness Gains without Extra Pay","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Foundation models have revolutionized general-purpose problem-solving,\noffering rapid task adaptation through pretraining, meta-training, and\nfinetuning. Recent crucial advances in these paradigms reveal the importance of\nchallenging task prioritized sampling to enhance adaptation robustness under\ndistribution shifts. However, ranking task difficulties over iteration as a\npreliminary step typically requires exhaustive task evaluation, which is\npractically unaffordable in computation and data-annotation. This study\nprovides a novel perspective to illuminate the possibility of leveraging the\ndual importance of adaptation robustness and learning efficiency, particularly\nin scenarios where task evaluation is risky or costly, such as iterative\nagent-environment interactions for robotic policy evaluation or computationally\nintensive inference steps for finetuning foundation models. Firstly, we\nintroduce Model Predictive Task Sampling (MPTS), a framework that bridges the\ntask space and adaptation risk landscape, providing a theoretical foundation\nfor robust active task sampling. MPTS employs a generative model to\ncharacterize the episodic optimization process and predicts task-specific\nadaptation risk via posterior inference. The resulting risk learner amortizes\nthe costly evaluation of task adaptation performance and provably approximates\ntask difficulty rankings. MPTS seamlessly integrates into zero-shot, few-shot,\nand supervised finetuning settings. Empirically, we conduct extensive\nexperiments in pattern recognition using foundation models and sequential\ndecision-making. Our results demonstrate that MPTS significantly enhances\nadaptation robustness for tail or out-of-distribution (OOD) tasks and improves\nlearning efficiency compared to state-of-the-art (SOTA) methods. The code is\navailable at the project site https:\/\/github.com\/thu-rllab\/MPTS.\n","versions":"[{'version': 'v1', 'created': 'Sun, 19 Jan 2025 13:14:53 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Feb 2025 09:29:09 GMT'}, {'version': 'v3', 'created': 'Sun, 16 Feb 2025 08:38:16 GMT'}, {'version': 'v4', 'created': 'Mon, 17 Mar 2025 03:16:23 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wang', 'Qi Cheems', ''], ['Xiao', 'Zehao', ''], ['Mao', 'Yixiu', ''], ['Qu', 'Yun', ''], ['Shen', 'Jiayi', ''], ['Lv', 'Yiqin', ''], ['Ji', 'Xiangyang', '']]","extracted_entities":"[{'text': 'Foundation models', 'label': 'Foundation Model'}, {'text': 'finetuning', 'label': 'Fine-tuning'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'MPTS', 'label': 'Foundation Model'}, {'text': 'MPTS', 'label': 'Foundation Model'}, {'text': 'MPTS', 'label': 'Foundation Model'}, {'text': 'zero-shot', 'label': 'Zero-shot Learning'}, {'text': 'few-shot', 'label': 'Few-shot Learning'}, {'text': 'supervised finetuning', 'label': 'Fine-tuning'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'MPTS', 'label': 'Foundation Model'}, {'text': 'MPTS', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"Foundation models","similarity_score":0.9628887177}
{"id":2501.14216,"submitter":"Haowei Lin","authors":"Haowei Lin and Shanda Li and Haotian Ye and Yiming Yang and Stefano\n  Ermon and Yitao Liang and Jianzhu Ma","title":"TFG-Flow: Training-free Guidance in Multimodal Generative Flow","comments":null,"journal-ref":"ICLR 2025","doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Given an unconditional generative model and a predictor for a target property\n(e.g., a classifier), the goal of training-free guidance is to generate samples\nwith desirable target properties without additional training. As a highly\nefficient technique for steering generative models toward flexible outcomes,\ntraining-free guidance has gained increasing attention in diffusion models.\nHowever, existing methods only handle data in continuous spaces, while many\nscientific applications involve both continuous and discrete data (referred to\nas multimodality). Another emerging trend is the growing use of the simple and\ngeneral flow matching framework in building generative foundation models, where\nguided generation remains under-explored. To address this, we introduce\nTFG-Flow, a novel training-free guidance method for multimodal generative flow.\nTFG-Flow addresses the curse-of-dimensionality while maintaining the property\nof unbiased sampling in guiding discrete variables. We validate TFG-Flow on\nfour molecular design tasks and show that TFG-Flow has great potential in drug\ndesign by generating molecules with desired properties.\n","versions":"[{'version': 'v1', 'created': 'Fri, 24 Jan 2025 03:44:16 GMT'}, {'version': 'v2', 'created': 'Sun, 2 Mar 2025 03:00:53 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 07:30:25 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Lin', 'Haowei', ''], ['Li', 'Shanda', ''], ['Ye', 'Haotian', ''], ['Yang', 'Yiming', ''], ['Ermon', 'Stefano', ''], ['Liang', 'Yitao', ''], ['Ma', 'Jianzhu', '']]","extracted_entities":"[{'text': 'generative foundation models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"generative foundation models","similarity_score":0.6990206242}
{"id":2502.15013,"submitter":"Arun Sharma","authors":"Majid Farhadloo, Arun Sharma, Mingzhou Yang, Bharat Jayaprakash,\n  William Northrop, Shashi Shekhar","title":"Towards Physics-Guided Foundation Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Traditional foundation models are pre-trained on broad datasets to reduce the\ntraining resources (e.g., time, energy, labeled samples) needed for fine-tuning\na wide range of downstream tasks. However, traditional foundation models\nstruggle with out-of-distribution prediction and can produce outputs that are\nunrealistic and physically infeasible. We propose the notation of\nphysics-guided foundation models (PGFM), that is, foundation models integrated\nwith broad or general domain (e.g., scientific) physical knowledge applicable\nto a wide range of downstream tasks.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Feb 2025 20:10:22 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 20:51:46 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Farhadloo', 'Majid', ''], ['Sharma', 'Arun', ''], ['Yang', 'Mingzhou', ''], ['Jayaprakash', 'Bharat', ''], ['Northrop', 'William', ''], ['Shekhar', 'Shashi', '']]","extracted_entities":"[{'text': 'Traditional foundation models', 'label': 'Foundation Model'}, {'text': 'traditional foundation models', 'label': 'Foundation Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2503.06027,"submitter":"Xubin Wang","authors":"Xubin Wang, Zhiqing Tang, Jianxiong Guo, Tianhui Meng, Chenhao Wang,\n  Tian Wang, Weijia Jia","title":"Empowering Edge Intelligence: A Comprehensive Survey on On-Device AI\n  Models","comments":"This paper has been accepted by ACM Computing Surveys","journal-ref":null,"doi":"10.1145\/3724420","report-no":null,"categories":"cs.AI cs.LG cs.NI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The rapid advancement of artificial intelligence (AI) technologies has led to\nan increasing deployment of AI models on edge and terminal devices, driven by\nthe proliferation of the Internet of Things (IoT) and the need for real-time\ndata processing. This survey comprehensively explores the current state,\ntechnical challenges, and future trends of on-device AI models. We define\non-device AI models as those designed to perform local data processing and\ninference, emphasizing their characteristics such as real-time performance,\nresource constraints, and enhanced data privacy. The survey is structured\naround key themes, including the fundamental concepts of AI models, application\nscenarios across various domains, and the technical challenges faced in edge\nenvironments. We also discuss optimization and implementation strategies, such\nas data preprocessing, model compression, and hardware acceleration, which are\nessential for effective deployment. Furthermore, we examine the impact of\nemerging technologies, including edge computing and foundation models, on the\nevolution of on-device AI models. By providing a structured overview of the\nchallenges, solutions, and future directions, this survey aims to facilitate\nfurther research and application of on-device AI, ultimately contributing to\nthe advancement of intelligent systems in everyday life.\n","versions":"[{'version': 'v1', 'created': 'Sat, 8 Mar 2025 02:59:51 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 13:37:33 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wang', 'Xubin', ''], ['Tang', 'Zhiqing', ''], ['Guo', 'Jianxiong', ''], ['Meng', 'Tianhui', ''], ['Wang', 'Chenhao', ''], ['Wang', 'Tian', ''], ['Jia', 'Weijia', '']]","extracted_entities":"[{'text': 'AI models', 'label': 'AI model'}, {'text': 'on-device AI models', 'label': 'AI model'}, {'text': 'on-device AI models', 'label': 'AI model'}, {'text': 'AI models', 'label': 'AI model'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'on-device AI models', 'label': 'AI model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2503.08722,"submitter":"Yehonathan Refael","authors":"Aviad Barzilai, Yotam Gigi, Amr Helmy, Vered Silverman, Yehonathan\n  Refael, Bolous Jaber, Tomer Shekel, George Leifman, Genady Beryozkin","title":"A Recipe for Improving Remote Sensing VLM Zero Shot Generalization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Foundation models have had a significant impact across various AI\napplications, enabling use cases that were previously impossible. Contrastive\nVisual Language Models (VLMs), in particular, have outperformed other\ntechniques in many tasks. However, their prevalence in remote sensing (RS) is\nstill limited, due to the scarcity of diverse remote-sensing visual-language\ndatasets. In this work we introduce two novel image-caption datasets for\ntraining of remote sensing foundation models. The first dataset pairs aerial\nand satellite imagery with captions generated by Gemini using landmarks\nextracted from Google Maps. The second dataset utilizes public web images and\ntheir corresponding alt-text, filtered for the remote sensing domain, resulting\nin a diverse dataset with greater breadth in image styles and subject matter.\nThese datasets are used to pre-train the\nMaMMUT~\\citep{kuo2023mammutsimplearchitecturejoint} VLM architecture, resulting\nin state-of-the-art generalization performance in zero-shot cross-modal\nretrieval on well-known public benchmarks. Finally, we present our ongoing\nresearch to distill image-level knowledge gained in the VLM contrastive\ntraining procedure to enhance the model's localization ability. Specifically,\nwe iteratively generate pseudo-labels for image regions based on the model's\nattention maps and use these labels for further training. To mitigate noisy\nattention maps and create robust segmentation masks, we introduce a novel\nattention-pooling mechanism called the Smooth-Attention-Operation.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Mar 2025 21:09:02 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 13:49:27 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Barzilai', 'Aviad', ''], ['Gigi', 'Yotam', ''], ['Helmy', 'Amr', ''], ['Silverman', 'Vered', ''], ['Refael', 'Yehonathan', ''], ['Jaber', 'Bolous', ''], ['Shekel', 'Tomer', ''], ['Leifman', 'George', ''], ['Beryozkin', 'Genady', '']]","extracted_entities":"[{'text': 'Foundation models', 'label': 'Foundation Model'}, {'text': 'zero-shot cross-modal\\nretrieval', 'label': 'Few-shot Learning'}, {'text': 'Smooth-Attention-Operation', 'label': 'Attention mechanism'}]","assigned_concept":"Foundation Model","matched_keyword":"Foundation models","similarity_score":0.9628887177}
{"id":2503.09091,"submitter":"Chen Zhao","authors":"Dong Li, Guihong Wan, Xintao Wu, Xinyu Wu, Xiaohui Chen, Yi He,\n  Christine G. Lian, Peter K. Sorger, Yevgeniy R. Semenov, Chen Zhao","title":"Multi-Modal Foundation Models for Computational Pathology: A Survey","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Foundation models have emerged as a powerful paradigm in computational\npathology (CPath), enabling scalable and generalizable analysis of\nhistopathological images. While early developments centered on uni-modal models\ntrained solely on visual data, recent advances have highlighted the promise of\nmulti-modal foundation models that integrate heterogeneous data sources such as\ntextual reports, structured domain knowledge, and molecular profiles. In this\nsurvey, we provide a comprehensive and up-to-date review of multi-modal\nfoundation models in CPath, with a particular focus on models built upon\nhematoxylin and eosin (H&E) stained whole slide images (WSIs) and tile-level\nrepresentations. We categorize 32 state-of-the-art multi-modal foundation\nmodels into three major paradigms: vision-language, vision-knowledge graph, and\nvision-gene expression. We further divide vision-language models into\nnon-LLM-based and LLM-based approaches. Additionally, we analyze 28 available\nmulti-modal datasets tailored for pathology, grouped into image-text pairs,\ninstruction datasets, and image-other modality pairs. Our survey also presents\na taxonomy of downstream tasks, highlights training and evaluation strategies,\nand identifies key challenges and future directions. We aim for this survey to\nserve as a valuable resource for researchers and practitioners working at the\nintersection of pathology and AI.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 06:03:33 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 16:43:54 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Li', 'Dong', ''], ['Wan', 'Guihong', ''], ['Wu', 'Xintao', ''], ['Wu', 'Xinyu', ''], ['Chen', 'Xiaohui', ''], ['He', 'Yi', ''], ['Lian', 'Christine G.', ''], ['Sorger', 'Peter K.', ''], ['Semenov', 'Yevgeniy R.', ''], ['Zhao', 'Chen', '']]","extracted_entities":"[{'text': 'Foundation models', 'label': 'Foundation Model'}, {'text': 'multi-modal foundation models', 'label': 'Foundation Model'}, {'text': 'multi-modal\\nfoundation models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"Foundation models","similarity_score":0.9628887177}
{"id":2503.09487,"submitter":"Beier Zhu","authors":"Beier Zhu, Jiequan Cui, Hanwang Zhang, Chi Zhang","title":"Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness","comments":"Accepted by CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  While image-text foundation models have succeeded across diverse downstream\ntasks, they still face challenges in the presence of spurious correlations\nbetween the input and label. To address this issue, we propose a simple\nthree-step approach,Project-Probe-Aggregate (PPA), that enables\nparameter-efficient fine-tuning for foundation models without relying on group\nannotations. Building upon the failure-based debiasing scheme, our method, PPA,\nimproves its two key components: minority samples identification and the robust\ntraining algorithm. Specifically, we first train biased classifiers by\nprojecting image features onto the nullspace of class proxies from text\nencoders. Next, we infer group labels using the biased classifier and probe\ngroup targets with prior correction. Finally, we aggregate group weights of\neach class to produce the debiased classifier. Our theoretical analysis shows\nthat our PPA enhances minority group identification and is Bayes optimal for\nminimizing the balanced group error, mitigating spurious correlations.\nExtensive experimental results confirm the effectiveness of our PPA: it\noutperforms the state-of-the-art by an average worst-group accuracy while\nrequiring less than 0.01% tunable parameters without training group labels.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 15:46:12 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 14:58:40 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zhu', 'Beier', ''], ['Cui', 'Jiequan', ''], ['Zhang', 'Hanwang', ''], ['Zhang', 'Chi', '']]","extracted_entities":"[{'text': 'image-text foundation models', 'label': 'Foundation Model'}, {'text': 'parameter-efficient fine-tuning', 'label': 'Fine-tuning'}, {'text': 'foundation models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2503.10538,"submitter":"Teresa Head-Gordon","authors":"Eric C.-Y. Yuan, Yunsheng Liu, Junmin Chen, Peichen Zhong, Sanjeev\n  Raja, Tobias Kreiman, Santiago Vargas, Wenbin Xu, Martin Head-Gordon, Chao\n  Yang, Samuel M. Blau, Bingqing Cheng, Aditi Krishnapriyan, Teresa Head-Gordon","title":"Foundation Models for Atomistic Simulation of Chemistry and Materials","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.chem-ph","license":"http:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/","abstract":"  Given the power of large language and large vision models, it is of profound\nand fundamental interest to ask if a foundational model based on data and\nparameter scaling laws and pre-training strategies is possible for learned\nsimulations of chemistry and materials. The scaling of large and diverse\ndatasets and highly expressive architectures for chemical and materials\nsciences should result in a foundation model that is more efficient and broadly\ntransferable, robust to out-of-distribution challenges, and easily fine-tuned\nto a variety of downstream observables, when compared to specific training from\nscratch on targeted applications in atomistic simulation. In this Perspective\nwe aim to cover the rapidly advancing field of machine learned interatomic\npotentials (MLIP), and to illustrate a path to create chemistry and materials\nMLIP foundation models at larger scale.\n","versions":"[{'version': 'v1', 'created': 'Thu, 13 Mar 2025 16:52:12 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 07:16:25 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Yuan', 'Eric C. -Y.', ''], ['Liu', 'Yunsheng', ''], ['Chen', 'Junmin', ''], ['Zhong', 'Peichen', ''], ['Raja', 'Sanjeev', ''], ['Kreiman', 'Tobias', ''], ['Vargas', 'Santiago', ''], ['Xu', 'Wenbin', ''], ['Head-Gordon', 'Martin', ''], ['Yang', 'Chao', ''], ['Blau', 'Samuel M.', ''], ['Cheng', 'Bingqing', ''], ['Krishnapriyan', 'Aditi', ''], ['Head-Gordon', 'Teresa', '']]","extracted_entities":"[{'text': 'data and\\nparameter scaling laws', 'label': 'Scaling law'}, {'text': 'foundation model', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation model","similarity_score":1.0}
{"id":2503.11835,"submitter":"Haoxin Liu","authors":"Haoxin Liu, Harshavardhan Kamarthi, Zhiyuan Zhao, Shangqing Xu, Shiyu\n  Wang, Qingsong Wen, Tom Hartvigsen, Fei Wang, B. Aditya Prakash","title":"How Can Time Series Analysis Benefit From Multiple Modalities? A Survey\n  and Outlook","comments":"Github Repo: https:\/\/github.com\/AdityaLab\/MM4TSA","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Time series analysis (TSA) is a longstanding research topic in the data\nmining community and has wide real-world significance. Compared to \"richer\"\nmodalities such as language and vision, which have recently experienced\nexplosive development and are densely connected, the time-series modality\nremains relatively underexplored and isolated. We notice that many recent TSA\nworks have formed a new research field, i.e., Multiple Modalities for TSA\n(MM4TSA). In general, these MM4TSA works follow a common motivation: how TSA\ncan benefit from multiple modalities. This survey is the first to offer a\ncomprehensive review and a detailed outlook for this emerging field.\nSpecifically, we systematically discuss three benefits: (1) reusing foundation\nmodels of other modalities for efficient TSA, (2) multimodal extension for\nenhanced TSA, and (3) cross-modality interaction for advanced TSA. We further\ngroup the works by the introduced modality type, including text, images, audio,\ntables, and others, within each perspective. Finally, we identify the gaps with\nfuture opportunities, including the reused modalities selections, heterogeneous\nmodality combinations, and unseen tasks generalizations, corresponding to the\nthree benefits. We release an up-to-date GitHub repository that includes key\npapers and resources.\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 19:56:57 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 02:28:56 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Liu', 'Haoxin', ''], ['Kamarthi', 'Harshavardhan', ''], ['Zhao', 'Zhiyuan', ''], ['Xu', 'Shangqing', ''], ['Wang', 'Shiyu', ''], ['Wen', 'Qingsong', ''], ['Hartvigsen', 'Tom', ''], ['Wang', 'Fei', ''], ['Prakash', 'B. Aditya', '']]","extracted_entities":"[{'text': 'MM4TSA', 'label': 'Foundation Model'}, {'text': 'foundation\\nmodels', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation\nmodels","similarity_score":0.9628887177}
{"id":2503.12843,"submitter":"Haozhe Si","authors":"Haozhe Si, Yuxuan Wan, Minh Do, Deepak Vasisht, Han Zhao and Hendrik\n  F. Hamann","title":"Towards Scalable Foundation Model for Multi-modal and Hyperspectral\n  Geospatial Data","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Geospatial raster data, such as that collected by satellite-based imaging\nsystems at different times and spectral bands, hold immense potential for\nenabling a wide range of high-impact applications. This potential stems from\nthe rich information that is spatially and temporally contextualized across\nmultiple channels and sensing modalities. Recent work has adapted existing\nself-supervised learning approaches for such geospatial data. However, they\nfall short of scalable model architectures, leading to inflexibility and\ncomputational inefficiencies when faced with an increasing number of channels\nand modalities. To address these limitations, we introduce Low-rank Efficient\nSpatial-Spectral Vision Transformer with three key innovations: i) the LESS\nAttention Block that approximates high-dimensional spatial-spectral attention\nthrough Kronecker's product of the low-dimensional spatial and spectral\nattention components; ii) the Continuous Positional-Channel Embedding Layer\nthat preserves both the continuity and physical characteristics of each\nspatial-spectral patch; and iii) the Perception Field Mask that exploits local\nspatial dependencies by constraining attention to neighboring patches. To\nevaluate the proposed innovations, we construct GFM-Bench, which serves as a\ncomprehensive benchmark for such geospatial raster data. We pretrain LESS ViT\nusing a Hyperspectral Masked Autoencoder framework with integrated positional\nand channel masking strategies. Experimental results demonstrate that our\nproposed method achieves competitive performance against state-of-the-art\nmulti-modal geospatial foundation models while outperforming them on\ncross-satellite generalization tasks with higher computational efficiency. The\nflexibility and extensibility of our framework make it a promising direction\nfor future geospatial data analysis tasks that involve a wide range of\nmodalities and channels.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 05:42:19 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 02:13:50 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Si', 'Haozhe', ''], ['Wan', 'Yuxuan', ''], ['Do', 'Minh', ''], ['Vasisht', 'Deepak', ''], ['Zhao', 'Han', ''], ['Hamann', 'Hendrik F.', '']]","extracted_entities":"[{'text': 'Continuous Positional-Channel Embedding Layer', 'label': 'contextual Embedding'}, {'text': 'Perception Field Mask', 'label': 'Embedding'}, {'text': 'state-of-the-art\\nmulti-modal geospatial foundation models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"state-of-the-art\nmulti-modal geospatial foundation models","similarity_score":0.6358067989}
{"id":2503.12964,"submitter":"Zeeshan Patel","authors":"Zeeshan Patel, Ethan He, Parth Mannan, Xiaowei Ren, Ryan Wolf, Niket\n  Agarwal, Jacob Huffman, Zhuoyao Wang, Carl Wang, Jack Chang, Yan Bai, Tommy\n  Huang, Linnan Wang, Sahil Jain, Shanmugam Ramasamy, Joseph Jennings,\n  Ekaterina Sirazitdinova, Oleg Sudakov, Mingyuan Ma, Bobby Chen, Forrest Lin,\n  Hao Wang, Vasanth Rao Naik Sabavat, Sriharsha Niverty, Rong Ou, Pallab\n  Bhattacharya, David Page, Nima Tajbakhsh, Ashwath Aithal","title":"Training Video Foundation Models with NVIDIA NeMo","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Video Foundation Models (VFMs) have recently been used to simulate the real\nworld to train physical AI systems and develop creative visual experiences.\nHowever, there are significant challenges in training large-scale, high quality\nVFMs that can generate high-quality videos. We present a scalable, open-source\nVFM training pipeline with NVIDIA NeMo, providing accelerated video dataset\ncuration, multimodal data loading, and parallelized video diffusion model\ntraining and inference. We also provide a comprehensive performance analysis\nhighlighting best practices for efficient VFM training and inference.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 09:19:12 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Patel', 'Zeeshan', ''], ['He', 'Ethan', ''], ['Mannan', 'Parth', ''], ['Ren', 'Xiaowei', ''], ['Wolf', 'Ryan', ''], ['Agarwal', 'Niket', ''], ['Huffman', 'Jacob', ''], ['Wang', 'Zhuoyao', ''], ['Wang', 'Carl', ''], ['Chang', 'Jack', ''], ['Bai', 'Yan', ''], ['Huang', 'Tommy', ''], ['Wang', 'Linnan', ''], ['Jain', 'Sahil', ''], ['Ramasamy', 'Shanmugam', ''], ['Jennings', 'Joseph', ''], ['Sirazitdinova', 'Ekaterina', ''], ['Sudakov', 'Oleg', ''], ['Ma', 'Mingyuan', ''], ['Chen', 'Bobby', ''], ['Lin', 'Forrest', ''], ['Wang', 'Hao', ''], ['Sabavat', 'Vasanth Rao Naik', ''], ['Niverty', 'Sriharsha', ''], ['Ou', 'Rong', ''], ['Bhattacharya', 'Pallab', ''], ['Page', 'David', ''], ['Tajbakhsh', 'Nima', ''], ['Aithal', 'Ashwath', '']]","extracted_entities":"[{'text': 'Video Foundation Models', 'label': 'Foundation Model'}, {'text': 'VFMs', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"Video Foundation Models","similarity_score":0.707996726}
{"id":2503.13047,"submitter":"Ruiqi Song","authors":"Ruiqi Song, Xianda Guo, Hangbin Wu, Qinggong Wei, Long Chen","title":"InsightDrive: Insight Scene Representation for End-to-End Autonomous\n  Driving","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Directly generating planning results from raw sensors has become increasingly\nprevalent due to its adaptability and robustness in complex scenarios. Scene\nrepresentation, as a key module in the pipeline, has traditionally relied on\nconventional perception, which focus on the global scene. However, in driving\nscenarios, human drivers typically focus only on regions that directly impact\ndriving, which often coincide with those required for end-to-end autonomous\ndriving. In this paper, a novel end-to-end autonomous driving method called\nInsightDrive is proposed, which organizes perception by language-guided scene\nrepresentation. We introduce an instance-centric scene tokenizer that\ntransforms the surrounding environment into map- and object-aware instance\ntokens. Scene attention language descriptions, which highlight key regions and\nobstacles affecting the ego vehicle's movement, are generated by a\nvision-language model that leverages the cognitive reasoning capabilities of\nfoundation models. We then align scene descriptions with visual features using\nthe vision-language model, guiding visual attention through these descriptions\nto give effectively scene representation. Furthermore, we employ self-attention\nand cross-attention mechanisms to model the ego-agents and ego-map\nrelationships to comprehensively build the topological relationships of the\nscene. Finally, based on scene understanding, we jointly perform motion\nprediction and planning. Extensive experiments on the widely used nuScenes\nbenchmark demonstrate that the proposed InsightDrive achieves state-of-the-art\nperformance in end-to-end autonomous driving. The code is available at\nhttps:\/\/github.com\/songruiqi\/InsightDrive\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 10:52:32 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Song', 'Ruiqi', ''], ['Guo', 'Xianda', ''], ['Wu', 'Hangbin', ''], ['Wei', 'Qinggong', ''], ['Chen', 'Long', '']]","extracted_entities":"[{'text': 'vision-language model', 'label': 'Neural Language Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'vision-language model', 'label': 'Neural Language Model'}, {'text': 'visual attention', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'cross-attention mechanisms', 'label': 'Attention mechanism'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2503.13446,"submitter":"Zhenyu Wu","authors":"Zhenyu Wu, Yuheng Zhou, Xiuwei Xu, Ziwei Wang, Haibin Yan","title":"MoManipVLA: Transferring Vision-language-action Models for General\n  Mobile Manipulation","comments":"Accepted to CVPR 2025. Project Page:\n  https:\/\/gary3410.github.io\/momanipVLA\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Mobile manipulation is the fundamental challenge for robotics to assist\nhumans with diverse tasks and environments in everyday life. However,\nconventional mobile manipulation approaches often struggle to generalize across\ndifferent tasks and environments because of the lack of large-scale training.\nIn contrast, recent advances in vision-language-action (VLA) models have shown\nimpressive generalization capabilities, but these foundation models are\ndeveloped for fixed-base manipulation tasks. Therefore, we propose an efficient\npolicy adaptation framework named MoManipVLA to transfer pre-trained VLA models\nof fix-base manipulation to mobile manipulation, so that high generalization\nability across tasks and environments can be achieved in mobile manipulation\npolicy. Specifically, we utilize pre-trained VLA models to generate waypoints\nof the end-effector with high generalization ability. We design motion planning\nobjectives for the mobile base and the robot arm, which aim at maximizing the\nphysical feasibility of the trajectory. Finally, we present an efficient\nbi-level objective optimization framework for trajectory generation, where the\nupper-level optimization predicts waypoints for base movement to enhance the\nmanipulator policy space, and the lower-level optimization selects the optimal\nend-effector trajectory to complete the manipulation task. In this way,\nMoManipVLA can adjust the position of the robot base in a zero-shot manner,\nthus making the waypoints predicted from the fixed-base VLA models feasible.\nExtensive experimental results on OVMM and the real world demonstrate that\nMoManipVLA achieves a 4.2% higher success rate than the state-of-the-art mobile\nmanipulation, and only requires 50 training cost for real world deployment due\nto the strong generalization ability in the pre-trained VLA models.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:59:52 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wu', 'Zhenyu', ''], ['Zhou', 'Yuheng', ''], ['Xu', 'Xiuwei', ''], ['Wang', 'Ziwei', ''], ['Yan', 'Haibin', '']]","extracted_entities":"[{'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'MoManipVLA', 'label': 'Foundation Model'}, {'text': 'pre-trained VLA models', 'label': 'Foundation Model'}, {'text': 'pre-trained VLA models', 'label': 'Foundation Model'}, {'text': 'MoManipVLA', 'label': 'Foundation Model'}, {'text': 'zero-shot manner', 'label': 'Zero-shot Learning'}, {'text': 'MoManipVLA', 'label': 'Foundation Model'}, {'text': 'pre-trained VLA models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2503.14051,"submitter":"Tianshu Wu","authors":"Tianshu Wu, Jiyao Zhang, Shiqian Liang, Zhengxiao Han, Hao Dong","title":"Foundation Feature-Driven Online End-Effector Pose Estimation: A\n  Marker-Free and Learning-Free Approach","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Accurate transformation estimation between camera space and robot space is\nessential. Traditional methods using markers for hand-eye calibration require\noffline image collection, limiting their suitability for online\nself-calibration. Recent learning-based robot pose estimation methods, while\nadvancing online calibration, struggle with cross-robot generalization and\nrequire the robot to be fully visible. This work proposes a Foundation\nfeature-driven online End-Effector Pose Estimation (FEEPE) algorithm,\ncharacterized by its training-free and cross end-effector generalization\ncapabilities. Inspired by the zero-shot generalization capabilities of\nfoundation models, FEEPE leverages pre-trained visual features to estimate\n2D-3D correspondences derived from the CAD model and target image, enabling 6D\npose estimation via the PnP algorithm. To resolve ambiguities from partial\nobservations and symmetry, a multi-historical key frame enhanced pose\noptimization algorithm is introduced, utilizing temporal information for\nimproved accuracy. Compared to traditional hand-eye calibration, FEEPE enables\nmarker-free online calibration. Unlike robot pose estimation, it generalizes\nacross robots and end-effectors in a training-free manner. Extensive\nexperiments demonstrate its superior flexibility, generalization, and\nperformance.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 09:12:49 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wu', 'Tianshu', ''], ['Zhang', 'Jiyao', ''], ['Liang', 'Shiqian', ''], ['Han', 'Zhengxiao', ''], ['Dong', 'Hao', '']]","extracted_entities":"[{'text': 'zero-shot generalization', 'label': 'Zero-shot Learning'}, {'text': 'foundation models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2503.14129,"submitter":"Subhadeep Koley","authors":"Subhadeep Koley, Tapas Kumar Dutta, Aneeshan Sain, Pinaki Nath\n  Chowdhury, Ayan Kumar Bhunia, Yi-Zhe Song","title":"SketchFusion: Learning Universal Sketch Features through Fusing\n  Foundation Models","comments":"Accepted in CVPR 2025. Project page available at\n  https:\/\/subhadeepkoley.github.io\/SketchFusion\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  While foundation models have revolutionised computer vision, their\neffectiveness for sketch understanding remains limited by the unique challenges\nof abstract, sparse visual inputs. Through systematic analysis, we uncover two\nfundamental limitations: Stable Diffusion (SD) struggles to extract meaningful\nfeatures from abstract sketches (unlike its success with photos), and exhibits\na pronounced frequency-domain bias that suppresses essential low-frequency\ncomponents needed for sketch understanding. Rather than costly retraining, we\naddress these limitations by strategically combining SD with CLIP, whose strong\nsemantic understanding naturally compensates for SD's spatial-frequency biases.\nBy dynamically injecting CLIP features into SD's denoising process and\nadaptively aggregating features across semantic levels, our method achieves\nstate-of-the-art performance in sketch retrieval (+3.35%), recognition\n(+1.06%), segmentation (+29.42%), and correspondence learning (+21.22%),\ndemonstrating the first truly universal sketch feature representation in the\nera of foundation models.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 10:47:46 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Koley', 'Subhadeep', ''], ['Dutta', 'Tapas Kumar', ''], ['Sain', 'Aneeshan', ''], ['Chowdhury', 'Pinaki Nath', ''], ['Bhunia', 'Ayan Kumar', ''], ['Song', 'Yi-Zhe', '']]","extracted_entities":"[{'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'frequency-domain bias', 'label': 'Model Bias and Fairness'}, {'text': 'CLIP', 'label': 'contextual Embedding'}, {'text': 'CLIP', 'label': 'contextual Embedding'}, {'text': 'correspondence learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2503.14355,"submitter":"Runqi Meng","authors":"Runqi Meng, Sifan Song, Pengfei Jin, Yujin Oh, Lin Teng, Yulin Wang,\n  Yiqun Sun, Ling Chen, Xiang Li, Quanzheng Li, Ning Guo, Dinggang Shen","title":"MAST-Pro: Dynamic Mixture-of-Experts for Adaptive Segmentation of\n  Pan-Tumors with Knowledge-Driven Prompts","comments":"10 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Accurate tumor segmentation is crucial for cancer diagnosis and treatment.\nWhile foundation models have advanced general-purpose segmentation, existing\nmethods still struggle with: (1) limited incorporation of medical priors, (2)\nimbalance between generic and tumor-specific features, and (3) high\ncomputational costs for clinical adaptation. To address these challenges, we\npropose MAST-Pro (Mixture-of-experts for Adaptive Segmentation of pan-Tumors\nwith knowledge-driven Prompts), a novel framework that integrates dynamic\nMixture-of-Experts (D-MoE) and knowledge-driven prompts for pan-tumor\nsegmentation. Specifically, text and anatomical prompts provide domain-specific\npriors, guiding tumor representation learning, while D-MoE dynamically selects\nexperts to balance generic and tumor-specific feature learning, improving\nsegmentation accuracy across diverse tumor types. To enhance efficiency, we\nemploy Parameter-Efficient Fine-Tuning (PEFT), optimizing MAST-Pro with\nsignificantly reduced computational overhead. Experiments on multi-anatomical\ntumor datasets demonstrate that MAST-Pro outperforms state-of-the-art\napproaches, achieving up to a 5.20% improvement in average DSC while reducing\ntrainable parameters by 91.04%, without compromising accuracy.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 15:39:44 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Meng', 'Runqi', ''], ['Song', 'Sifan', ''], ['Jin', 'Pengfei', ''], ['Oh', 'Yujin', ''], ['Teng', 'Lin', ''], ['Wang', 'Yulin', ''], ['Sun', 'Yiqun', ''], ['Chen', 'Ling', ''], ['Li', 'Xiang', ''], ['Li', 'Quanzheng', ''], ['Guo', 'Ning', ''], ['Shen', 'Dinggang', '']]","extracted_entities":"[{'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'MAST-Pro', 'label': 'Foundation Model'}, {'text': 'knowledge-driven Prompts', 'label': 'Prompting'}, {'text': 'knowledge-driven prompts', 'label': 'Prompting'}, {'text': 'text and anatomical prompts', 'label': 'Prompting'}, {'text': 'tumor representation learning', 'label': 'Few-shot Learning'}, {'text': 'Parameter-Efficient Fine-Tuning', 'label': 'Fine-tuning'}, {'text': 'MAST-Pro', 'label': 'Foundation Model'}, {'text': 'MAST-Pro', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2503.14572,"submitter":"Justus Westerhoff","authors":"Justus Westerhoff, Golzar Atefi, Mario Koddenbrock, Alexei Figueroa,\n  Alexander L\\\"oser, Erik Rodner, Felix A. Gers","title":"Robust Weight Imprinting: Insights from Neural Collapse and Proxy-Based\n  Aggregation","comments":"Code: https:\/\/github.com\/DATEXIS\/multi-imprinting\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The capacity of a foundation model allows for adaptation to new downstream\ntasks. Weight imprinting is a universal and efficient method to fulfill this\npurpose. It has been reinvented several times, but it has not been\nsystematically studied. In this paper, we propose a framework for imprinting,\nidentifying three main components: generation, normalization, and aggregation.\nThis allows us to conduct an in-depth analysis of imprinting and a comparison\nof the existing work. We reveal the benefits of representing novel data with\nmultiple proxies in the generation step and show the importance of proper\nnormalization. We determine those proxies through clustering and propose a\nnovel variant of imprinting that outperforms previous work. We motivate this by\nthe neural collapse phenomenon -- an important connection that we can draw for\nthe first time. Our results show an increase of up to 4% in challenging\nscenarios with complex data distributions for new classes.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 14:27:45 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Westerhoff', 'Justus', ''], ['Atefi', 'Golzar', ''], ['Koddenbrock', 'Mario', ''], ['Figueroa', 'Alexei', ''], ['L\u00f6ser', 'Alexander', ''], ['Rodner', 'Erik', ''], ['Gers', 'Felix A.', '']]","extracted_entities":"[{'text': 'foundation model', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation model","similarity_score":1.0}
{"id":2503.14754,"submitter":"Matt Franchi","authors":"Matt Franchi, Nikhil Garg, Wendy Ju, and Emma Pierson","title":"Bayesian Modeling of Zero-Shot Classifications for Urban Flood Detection","comments":"In review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Street scene datasets, collected from Street View or dashboard cameras, offer\na promising means of detecting urban objects and incidents like street\nflooding. However, a major challenge in using these datasets is their lack of\nreliable labels: there are myriad types of incidents, many types occur rarely,\nand ground-truth measures of where incidents occur are lacking. Here, we\npropose BayFlood, a two-stage approach which circumvents this difficulty.\nFirst, we perform zero-shot classification of where incidents occur using a\npretrained vision-language model (VLM). Second, we fit a spatial Bayesian model\non the VLM classifications. The zero-shot approach avoids the need to annotate\nlarge training sets, and the Bayesian model provides frequent desiderata in\nurban settings - principled measures of uncertainty, smoothing across\nlocations, and incorporation of external data like stormwater accumulation\nzones. We comprehensively validate this two-stage approach, showing that VLMs\nprovide strong zero-shot signal for floods across multiple cities and time\nperiods, the Bayesian model improves out-of-sample prediction relative to\nbaseline methods, and our inferred flood risk correlates with known external\npredictors of risk. Having validated our approach, we show it can be used to\nimprove urban flood detection: our analysis reveals 113,738 people who are at\nhigh risk of flooding overlooked by current methods, identifies demographic\nbiases in existing methods, and suggests locations for new flood sensors. More\nbroadly, our results showcase how Bayesian modeling of zero-shot LM annotations\nrepresents a promising paradigm because it avoids the need to collect large\nlabeled datasets and leverages the power of foundation models while providing\nthe expressiveness and uncertainty quantification of Bayesian models.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 21:53:37 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Franchi', 'Matt', ''], ['Garg', 'Nikhil', ''], ['Ju', 'Wendy', ''], ['Pierson', 'Emma', '']]","extracted_entities":"[{'text': 'zero-shot approach', 'label': 'Zero-shot Learning'}, {'text': 'foundation models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2503.15092,"submitter":"Zonghao Ying","authors":"Zonghao Ying, Guangyi Zheng, Yongxin Huang, Deyue Zhang, Wenxin Zhang,\n  Quanchen Zou, Aishan Liu, Xianglong Liu, Dacheng Tao","title":"Towards Understanding the Safety Boundaries of DeepSeek Models:\n  Evaluation and Findings","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This study presents the first comprehensive safety evaluation of the DeepSeek\nmodels, focusing on evaluating the safety risks associated with their generated\ncontent. Our evaluation encompasses DeepSeek's latest generation of large\nlanguage models, multimodal large language models, and text-to-image models,\nsystematically examining their performance regarding unsafe content generation.\nNotably, we developed a bilingual (Chinese-English) safety evaluation dataset\ntailored to Chinese sociocultural contexts, enabling a more thorough evaluation\nof the safety capabilities of Chinese-developed models. Experimental results\nindicate that despite their strong general capabilities, DeepSeek models\nexhibit significant safety vulnerabilities across multiple risk dimensions,\nincluding algorithmic discrimination and sexual content. These findings provide\ncrucial insights for understanding and improving the safety of large foundation\nmodels. Our code is available at\nhttps:\/\/github.com\/NY1024\/DeepSeek-Safety-Eval.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 10:44:37 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Ying', 'Zonghao', ''], ['Zheng', 'Guangyi', ''], ['Huang', 'Yongxin', ''], ['Zhang', 'Deyue', ''], ['Zhang', 'Wenxin', ''], ['Zou', 'Quanchen', ''], ['Liu', 'Aishan', ''], ['Liu', 'Xianglong', ''], ['Tao', 'Dacheng', '']]","extracted_entities":"[{'text': 'multimodal large language models', 'label': 'Large Language Model'}, {'text': 'large foundation\\nmodels', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"large foundation\nmodels","similarity_score":0.8749241829}
{"id":2503.15672,"submitter":"William Ljungbergh","authors":"William Ljungbergh, Adam Lilja, Adam Tonderski. Arvid Laveno Ling,\n  Carl Lindstr\\\"om, Willem Verbeke, Junsheng Fu, Christoffer Petersson, Lars\n  Hammarstrand, Michael Felsberg","title":"GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for\n  Autonomous Driving","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.RO","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Self-supervised pre-training based on next-token prediction has enabled large\nlanguage models to capture the underlying structure of text, and has led to\nunprecedented performance on a large array of tasks when applied at scale.\nSimilarly, autonomous driving generates vast amounts of spatiotemporal data,\nalluding to the possibility of harnessing scale to learn the underlying\ngeometric and semantic structure of the environment and its evolution over\ntime. In this direction, we propose a geometric and semantic self-supervised\npre-training method, GASP, that learns a unified representation by predicting,\nat any queried future point in spacetime, (1) general occupancy, capturing the\nevolving structure of the 3D scene; (2) ego occupancy, modeling the ego vehicle\npath through the environment; and (3) distilled high-level features from a\nvision foundation model. By modeling geometric and semantic 4D occupancy fields\ninstead of raw sensor measurements, the model learns a structured,\ngeneralizable representation of the environment and its evolution through time.\nWe validate GASP on multiple autonomous driving benchmarks, demonstrating\nsignificant improvements in semantic occupancy forecasting, online mapping, and\nego trajectory prediction. Our results demonstrate that continuous 4D geometric\nand semantic occupancy prediction provides a scalable and effective\npre-training paradigm for autonomous driving. For code and additional\nvisualizations, see \\href{https:\/\/research.zenseact.com\/publications\/gasp\/.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 20:00:27 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Ljungbergh', 'William', ''], ['Lilja', 'Adam', ''], ['Ling', 'Adam Tonderski. Arvid Laveno', ''], ['Lindstr\u00f6m', 'Carl', ''], ['Verbeke', 'Willem', ''], ['Fu', 'Junsheng', ''], ['Petersson', 'Christoffer', ''], ['Hammarstrand', 'Lars', ''], ['Felsberg', 'Michael', '']]","extracted_entities":"[{'text': 'vision foundation model', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"vision foundation model","similarity_score":0.7219946384}
{"id":2503.15917,"submitter":"Beilei Cui","authors":"Beilei Cui, Long Bai, Mobarakol Islam, An Wang, Zhiqi Ma, Yiming\n  Huang, Feng Li, Zhen Chen, Zhongliang Jiang, Nassir Navab, Hongliang Ren","title":"Learning to Efficiently Adapt Foundation Models for Self-Supervised\n  Endoscopic 3D Scene Reconstruction from Any Cameras","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Accurate 3D scene reconstruction is essential for numerous medical tasks.\nGiven the challenges in obtaining ground truth data, there has been an\nincreasing focus on self-supervised learning (SSL) for endoscopic depth\nestimation as a basis for scene reconstruction. While foundation models have\nshown remarkable progress in visual tasks, their direct application to the\nmedical domain often leads to suboptimal results. However, the visual features\nfrom these models can still enhance endoscopic tasks, emphasizing the need for\nefficient adaptation strategies, which still lack exploration currently. In\nthis paper, we introduce Endo3DAC, a unified framework for endoscopic scene\nreconstruction that efficiently adapts foundation models. We design an\nintegrated network capable of simultaneously estimating depth maps, relative\nposes, and camera intrinsic parameters. By freezing the backbone foundation\nmodel and training only the specially designed Gated Dynamic Vector-Based\nLow-Rank Adaptation (GDV-LoRA) with separate decoder heads, Endo3DAC achieves\nsuperior depth and pose estimation while maintaining training efficiency.\nAdditionally, we propose a 3D scene reconstruction pipeline that optimizes\ndepth maps' scales, shifts, and a few parameters based on our integrated\nnetwork. Extensive experiments across four endoscopic datasets demonstrate that\nEndo3DAC significantly outperforms other state-of-the-art methods while\nrequiring fewer trainable parameters. To our knowledge, we are the first to\nutilize a single network that only requires surgical videos to perform both SSL\ndepth estimation and scene reconstruction tasks. The code will be released upon\nacceptance.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 07:49:04 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Cui', 'Beilei', ''], ['Bai', 'Long', ''], ['Islam', 'Mobarakol', ''], ['Wang', 'An', ''], ['Ma', 'Zhiqi', ''], ['Huang', 'Yiming', ''], ['Li', 'Feng', ''], ['Chen', 'Zhen', ''], ['Jiang', 'Zhongliang', ''], ['Navab', 'Nassir', ''], ['Ren', 'Hongliang', '']]","extracted_entities":"[{'text': 'self-supervised learning', 'label': 'Few-shot Learning'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'Endo3DAC', 'label': 'Foundation Model'}, {'text': 'foundation models', 'label': 'Foundation Model'}, {'text': 'Endo3DAC', 'label': 'Foundation Model'}, {'text': 'SSL', 'label': 'Few-shot Learning'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2503.16055,"submitter":"Abdelrahman Elsayed","authors":"Abdelrahman Elsayed, Sarim Hashmi, Mohammed Elseiagy, Hu Wang,\n  Mohammad Yaqub, Ibrahim Almakky","title":"SALT: Singular Value Adaptation with Low-Rank Transformation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  The complex nature of medical image segmentation calls for models that are\nspecifically designed to capture detailed, domain-specific features. Large\nfoundation models offer considerable flexibility, yet the cost of fine-tuning\nthese models remains a significant barrier. Parameter-Efficient Fine-Tuning\n(PEFT) methods, such as Low-Rank Adaptation (LoRA), efficiently update model\nweights with low-rank matrices but may suffer from underfitting when the chosen\nrank is insufficient to capture domain-specific nuances. Conversely, full-rank\nSingular Value Decomposition (SVD) based methods provide comprehensive updates\nby modifying all singular values, yet they often lack flexibility and exhibit\nvariable performance across datasets. We propose SALT (Singular Value\nAdaptation with Low-Rank Transformation), a method that selectively adapts the\nmost influential singular values using trainable scale and shift parameters\nwhile complementing this with a low-rank update for the remaining subspace.\nThis hybrid approach harnesses the advantages of both LoRA and SVD, enabling\neffective adaptation without relying on increasing model size or depth.\nEvaluated on 5 challenging medical datasets, ranging from as few as 20 samples\nto 1000, SALT outperforms state-of-the-art PEFT (LoRA and SVD) by 2% to 5% in\nDice with only 3.9% trainable parameters, demonstrating robust adaptation even\nin low-resource settings. The code for SALT is available at:\nhttps:\/\/github.com\/BioMedIA-MBZUAI\/SALT\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 11:42:41 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Elsayed', 'Abdelrahman', ''], ['Hashmi', 'Sarim', ''], ['Elseiagy', 'Mohammed', ''], ['Wang', 'Hu', ''], ['Yaqub', 'Mohammad', ''], ['Almakky', 'Ibrahim', '']]","extracted_entities":"[{'text': 'Large\\nfoundation models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"Large\nfoundation models","similarity_score":0.8749241829}
{"id":2503.1632,"submitter":"Noor Nashid","authors":"Noor Nashid, Islem Bouzenia, Michael Pradel, Ali Mesbah","title":"Issue2Test: Generating Reproducing Test Cases from Issue Reports","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Automated tools for solving GitHub issues are receiving significant attention\nby both researchers and practitioners, e.g., in the form of foundation models\nand LLM-based agents prompted with issues. A crucial step toward successfully\nsolving an issue is creating a test case that accurately reproduces the issue.\nSuch a test case can guide the search for an appropriate patch and help\nvalidate whether the patch matches the issue's intent. However, existing\ntechniques for issue reproduction show only moderate success. This paper\npresents Issue2Test, an LLM-based technique for automatically generating a\nreproducing test case for a given issue report. Unlike automated regression\ntest generators, which aim at creating passing tests, our approach aims at a\ntest that fails, and that fails specifically for the reason described in the\nissue. To this end, Issue2Test performs three steps: (1) understand the issue\nand gather context (e.g., related files and project-specific guidelines)\nrelevant for reproducing it; (2) generate a candidate test case; and (3)\niteratively refine the test case based on compilation and runtime feedback\nuntil it fails and the failure aligns with the problem described in the issue.\nWe evaluate Issue2Test on the SWT-bench-lite dataset, where it successfully\nreproduces 30.4 of the issues, achieving a 40.1% relative improvement over the\nbest existing technique. Our evaluation also shows that Issue2test reproduces\n28 issues that seven prior techniques fail to address, contributing a total of\n68.3% of all issues reproduced by any tool. We envision our approach to\ncontribute to enhancing the overall progress in the important task of\nautomatically solving GitHub issues.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 16:44:00 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Nashid', 'Noor', ''], ['Bouzenia', 'Islem', ''], ['Pradel', 'Michael', ''], ['Mesbah', 'Ali', '']]","extracted_entities":"[{'text': 'foundation models', 'label': 'Foundation Model'}]","assigned_concept":"Foundation Model","matched_keyword":"foundation models","similarity_score":0.9628887177}
{"id":2108.06062,"submitter":"Yike Xu","authors":"Yike Xu and Mark S. Andersland","title":"Worst-Case Services and State-Based Scheduling","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY math.OC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In this paper, we shed new light on a classical scheduling problem: given a\nslot-timed, constant-capacity server, what short-run scheduling decisions must\nbe made to provide long-run service guarantees to competing flows of unit-sized\ntasks? We model each flow's long-run guarantee as a worst-case service that\nmaps each queued arrival vector recording the flow's cumulative task arrivals,\nincluding those initially queued, to a worst-case acceptable departure vector\nlower-bounding its cumulative served tasks. We show that these maps are states\nthat can be updated as tasks arrive and are served, introduce state-based\nscheduling, find the schedulability condition necessary and sufficient to\nmaintain all flows' long-run guarantees, and use this condition to identify all\nshort-run scheduling decisions that preserve schedulability. Our framework is\ngeneral but computationally complex. To reduce complexity, we consider three\nspecializations. First, we show that when satisfactory short-run scheduling\ndecisions exist, at least one can be efficiently identified by maximizing the\nserver's capacity slack, a generalization of the earliest-deadline-first rule.\nSecond, we show that a special class of worst-case services, min-plus services,\ncan be efficiently specified and updated using properties of the min-plus\nalgebra. Finally, we show that efficiency can be further improved by\nrestricting attention to a min-plus service subclass, dual-curve services. This\nlast specialization turns out to be a dynamic extension of service curves that\nmaintains all essential features of our framework while approaching near\npractical viability.\n","versions":"[{'version': 'v1', 'created': 'Fri, 13 Aug 2021 05:00:10 GMT'}, {'version': 'v2', 'created': 'Tue, 22 Nov 2022 08:15:59 GMT'}, {'version': 'v3', 'created': 'Sat, 29 Apr 2023 14:42:13 GMT'}, {'version': 'v4', 'created': 'Thu, 20 Mar 2025 04:26:47 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Xu', 'Yike', ''], ['Andersland', 'Mark S.', '']]","extracted_entities":"[{'text': 'attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention","similarity_score":0.7383304834}
{"id":2209.12075,"submitter":"Jiamian Wang","authors":"Jiamian Wang, Kunpeng Li, Yulun Zhang, Xin Yuan, Zhiqiang Tao","title":"S^2-Transformer for Mask-Aware Hyperspectral Image Reconstruction","comments":"Accepted by TPAMI","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Snapshot compressive imaging (SCI) surges as a novel way of capturing\nhyperspectral images. It operates an optical encoder to compress the 3D data\ninto a 2D measurement and adopts a software decoder for the signal\nreconstruction. Recently, a representative SCI set-up of coded aperture\nsnapshot compressive imager (CASSI) with Transformer reconstruction backend\nremarks high-fidelity sensing performance. However, dominant spatial and\nspectral attention designs show limitations in hyperspectral modeling. The\nspatial attention values describe the inter-pixel correlation but overlook the\nacross-spectra variation within each pixel. The spectral attention size is\nunscalable to the token spatial size and thus bottlenecks information\nallocation. Besides, CASSI entangles the spatial and spectral information into\na 2D measurement, placing a barrier for information disentanglement and\nmodeling. In addition, CASSI blocks the light with a physical binary mask,\nyielding the masked data loss. To tackle above challenges, we propose a\nspatial-spectral (S2-) Transformer implemented by a paralleled attention design\nand a mask-aware learning strategy. Firstly, we systematically explore pros and\ncons of different spatial (-spectral) attention designs, based on which we find\nperforming both attentions in parallel well disentangles and models the blended\ninformation. Secondly, the masked pixels induce higher prediction difficulty\nand should be treated differently from unmasked ones. We adaptively prioritize\nthe loss penalty attributing to the mask structure by referring to the\nmask-encoded prediction as an uncertainty estimator. We theoretically discuss\nthe distinct convergence tendencies between masked\/unmasked regions of the\nproposed learning strategy. Extensive experiments demonstrate that on average,\nthe results of the proposed method are superior over the state-of-the-art\nmethod.\n","versions":"[{'version': 'v1', 'created': 'Sat, 24 Sep 2022 19:26:46 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Dec 2022 15:41:22 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 23:57:52 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Wang', 'Jiamian', ''], ['Li', 'Kunpeng', ''], ['Zhang', 'Yulun', ''], ['Yuan', 'Xin', ''], ['Tao', 'Zhiqiang', '']]","extracted_entities":"[{'text': 'spectral attention', 'label': 'Attention mechanism'}, {'text': 'mask-aware learning strategy', 'label': 'Few-shot Learning'}]","assigned_concept":"Attention mechanism","matched_keyword":"spectral attention","similarity_score":0.6150141954}
{"id":2312.01061,"submitter":"Huan Chen","authors":"Huan Chen, Wangcai Zhao, Tingfa Xu, Shiyun Zhou, Peifu Liu and Jianan\n  Li","title":"Spectral-wise Implicit Neural Representation for Hyperspectral Image\n  Reconstruction","comments":"Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology, has been published","journal-ref":"Volume: 34, Issue: 5, May 2024","doi":"10.1109\/TCSVT.2023.3318366","report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Coded Aperture Snapshot Spectral Imaging (CASSI) reconstruction aims to\nrecover the 3D spatial-spectral signal from 2D measurement. Existing methods\nfor reconstructing Hyperspectral Image (HSI) typically involve learning\nmappings from a 2D compressed image to a predetermined set of discrete spectral\nbands. However, this approach overlooks the inherent continuity of the spectral\ninformation. In this study, we propose an innovative method called\nSpectral-wise Implicit Neural Representation (SINR) as a pioneering step toward\naddressing this limitation. SINR introduces a continuous spectral amplification\nprocess for HSI reconstruction, enabling spectral super-resolution with\ncustomizable magnification factors. To achieve this, we leverage the concept of\nimplicit neural representation. Specifically, our approach introduces a\nspectral-wise attention mechanism that treats individual channels as distinct\ntokens, thereby capturing global spectral dependencies. Additionally, our\napproach incorporates two components, namely a Fourier coordinate encoder and a\nspectral scale factor module. The Fourier coordinate encoder enhances the\nSINR's ability to emphasize high-frequency components, while the spectral scale\nfactor module guides the SINR to adapt to the variable number of spectral\nchannels. Notably, the SINR framework enhances the flexibility of CASSI\nreconstruction by accommodating an unlimited number of spectral bands in the\ndesired output. Extensive experiments demonstrate that our SINR outperforms\nbaseline methods. By enabling continuous reconstruction within the CASSI\nframework, we take the initial stride toward integrating implicit neural\nrepresentation into the field.\n","versions":"[{'version': 'v1', 'created': 'Sat, 2 Dec 2023 08:06:07 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 02:47:41 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Chen', 'Huan', ''], ['Zhao', 'Wangcai', ''], ['Xu', 'Tingfa', ''], ['Zhou', 'Shiyun', ''], ['Liu', 'Peifu', ''], ['Li', 'Jianan', '']]","extracted_entities":"[{'text': 'spectral-wise attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"spectral-wise attention mechanism","similarity_score":0.8220318556}
{"id":2403.05906,"submitter":"Jingyun Xue","authors":"Jingyun Xue, Tao Wang, Pengwen Dai, Kaihao Zhang","title":"Segmentation Guided Sparse Transformer for Under-Display Camera Image\n  Restoration","comments":"13 pages, 10 figures, conference or other essential info","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Under-Display Camera (UDC) is an emerging technology that achieves\nfull-screen display via hiding the camera under the display panel. However, the\ncurrent implementation of UDC causes serious degradation. The incident light\nrequired for camera imaging undergoes attenuation and diffraction when passing\nthrough the display panel, leading to various artifacts in UDC imaging.\nPresently, the prevailing UDC image restoration methods predominantly utilize\nconvolutional neural network architectures, whereas Transformer-based methods\nhave exhibited superior performance in the majority of image restoration tasks.\nThis is attributed to the Transformer's capability to sample global features\nfor the local reconstruction of images, thereby achieving high-quality image\nrestoration. In this paper, we observe that when using the Vision Transformer\nfor UDC degraded image restoration, the global attention samples a large amount\nof redundant information and noise. Furthermore, compared to the ordinary\nTransformer employing dense attention, the Transformer utilizing sparse\nattention can alleviate the adverse impact of redundant information and noise.\nBuilding upon this discovery, we propose a Segmentation Guided Sparse\nTransformer method (SGSFormer) for the task of restoring high-quality images\nfrom UDC degraded images. Specifically, we utilize sparse self-attention to\nfilter out redundant information and noise, directing the model's attention to\nfocus on the features more relevant to the degraded regions in need of\nreconstruction. Moreover, we integrate the instance segmentation map as prior\ninformation to guide the sparse self-attention in filtering and focusing on the\ncorrect regions.\n","versions":"[{'version': 'v1', 'created': 'Sat, 9 Mar 2024 13:11:59 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 11:49:18 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Xue', 'Jingyun', ''], ['Wang', 'Tao', ''], ['Dai', 'Pengwen', ''], ['Zhang', 'Kaihao', '']]","extracted_entities":"[{'text': 'global attention', 'label': 'Attention mechanism'}, {'text': 'dense attention', 'label': 'Attention mechanism'}, {'text': 'sparse\\nattention', 'label': 'Attention mechanism'}, {'text': 'sparse self-attention', 'label': 'Attention mechanism'}, {'text': 'sparse self-attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"dense attention","similarity_score":0.7055617571}
{"id":2405.04476,"submitter":"Wang Lijun","authors":"Lijun Wang, Yixian Lu, Ziyan Gao, Kai Li, Jianqiang Huang, Yuntao\n  Kong, Shogo Okada","title":"BERP: A Blind Estimator of Room Parameters for Single-Channel Noisy\n  Speech Signals","comments":"16-page with supplementary materials, Submitted to IEEE\/ACM\n  Transaction on Audio Speech and Language Processing (TASLP)","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.AS cs.SD","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Room acoustical parameters (RAPs), room geometrical parameters (RGPs) and\ninstantaneous occupancy level are essential metrics for parameterizing the room\nacoustical characteristics (RACs) of a sound field around a listener's local\nenvironment, offering comprehensive indications for various applications.\nCurrent blind estimation methods either fail to cover a broad range of\nreal-world acoustic environments in the context of real background noise or\nestimate only a few RAPs and RGPs from noisy single-channel speech signals. In\naddition, they are limited in their ability to estimate the instantaneous\noccupancy level. In this paper, we propose a new universal blind estimation\nframework called the blind estimator of room parameters (BERP) to estimate\nRAPs, RGPs and occupancy level via a unified methodology. It consists of two\nmodules: a unified room feature encoder that combines attention mechanisms with\nconvolutional layers to learn common features across room parameters, and\nmultiple separate parametric predictors for continuous estimation of each\nparameter in parallel. The combination of attention and convolutions enables\nthe model to capture acoustic features locally and globally from speech,\nyielding more robust and multitask generalizable common features. Separate\npredictors allow the model to independently optimize for each room parameter to\nreduce task learning conflict and improve per-task performance. This estimation\nframework enables universal and efficient estimation of room parameters while\nmaintaining satisfactory performance. To evaluate the effectiveness of the\nproposed framework, we compile a task-specific dataset from several publicly\navailable datasets, including synthetic and real reverberant recordings. The\nresults reveal that BERP achieves state-of-the-art (SOTA) performance and\nexcellent adaptability to real-world scenarios. The code and weights are\navailable on GitHub.\n","versions":"[{'version': 'v1', 'created': 'Tue, 7 May 2024 16:41:41 GMT'}, {'version': 'v2', 'created': 'Thu, 16 May 2024 10:17:12 GMT'}, {'version': 'v3', 'created': 'Sat, 19 Oct 2024 12:44:24 GMT'}, {'version': 'v4', 'created': 'Wed, 23 Oct 2024 11:01:59 GMT'}, {'version': 'v5', 'created': 'Thu, 24 Oct 2024 01:59:56 GMT'}, {'version': 'v6', 'created': 'Tue, 18 Mar 2025 15:08:12 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wang', 'Lijun', ''], ['Lu', 'Yixian', ''], ['Gao', 'Ziyan', ''], ['Li', 'Kai', ''], ['Huang', 'Jianqiang', ''], ['Kong', 'Yuntao', ''], ['Okada', 'Shogo', '']]","extracted_entities":"[{'text': 'attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'GitHub', 'label': 'Open-source LLMs'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanisms","similarity_score":0.9558142424}
{"id":2406.06652,"submitter":"Yubin Xiao","authors":"Yubin Xiao, Di Wang, Xuan Wu, Yuesong Wu, Boyang Li, Wei Du, Liupu\n  Wang, You Zhou","title":"Improving Generalization of Neural Vehicle Routing Problem Solvers\n  Through the Lens of Model Architecture","comments":"This work has been accepted by Neural Networks","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Neural models produce promising results when solving Vehicle Routing Problems\n(VRPs), but often fall short in generalization. Recent attempts to enhance\nmodel generalization often incur unnecessarily large training cost or cannot be\ndirectly applied to other models solving different VRP variants. To address\nthese issues, we take a novel perspective on model architecture in this study.\nSpecifically, we propose a plug-and-play Entropy-based Scaling Factor (ESF) and\na Distribution-Specific (DS) decoder to enhance the size and distribution\ngeneralization, respectively. ESF adjusts the attention weight pattern of the\nmodel towards familiar ones discovered during training when solving VRPs of\nvarying sizes. The DS decoder explicitly models VRPs of multiple training\ndistribution patterns through multiple auxiliary light decoders, expanding the\nmodel representation space to encompass a broader range of distributional\nscenarios. We conduct extensive experiments on both synthetic and widely\nrecognized real-world benchmarking datasets and compare the performance with\nseven baseline models. The results demonstrate the effectiveness of using ESF\nand DS decoder to obtain a more generalizable model and showcase their\napplicability to solve different VRP variants, i.e., travelling salesman\nproblem and capacitated VRP. Notably, our proposed generic components require\nminimal computational resources, and can be effortlessly integrated into\nconventional generalization strategies to further elevate model generalization.\n","versions":"[{'version': 'v1', 'created': 'Mon, 10 Jun 2024 09:03:17 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Jun 2024 14:02:57 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 08:40:04 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Xiao', 'Yubin', ''], ['Wang', 'Di', ''], ['Wu', 'Xuan', ''], ['Wu', 'Yuesong', ''], ['Li', 'Boyang', ''], ['Du', 'Wei', ''], ['Wang', 'Liupu', ''], ['Zhou', 'You', '']]","extracted_entities":"[{'text': 'attention weight pattern', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention weight pattern","similarity_score":0.7244582176}
{"id":2406.11579,"submitter":"Yiming Zhang","authors":"Han-Hung Lee, Yiming Zhang, Angel X. Chang","title":"Duoduo CLIP: Efficient 3D Understanding with Multi-View Images","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce Duoduo CLIP, a model for 3D representation learning that learns\nshape encodings from multi-view images instead of point clouds. The choice of\nmulti-view images allows us to leverage 2D priors from off-the-shelf CLIP\nmodels to facilitate fine-tuning with 3D data. Our approach not only shows\nbetter generalization compared to existing point cloud methods, but also\nreduces GPU requirements and training time. In addition, the model is modified\nwith cross-view attention to leverage information across multiple frames of the\nobject which further boosts performance. Notably, our model is permutation\ninvariant to the order of multi-view images while being pose-free. Compared to\nthe current SOTA point cloud method that requires 480 A100 hours to train 1\nbillion model parameters we only require 57 A5000 hours and 87 million\nparameters. Multi-view images also provide more flexibility including being\nable to encode objects with a variable number of images, and performance scales\nwhen more views are used. In contrast, point cloud based methods require an\nentire scan or model of the object. We showcase this flexibility with\nbenchmarks from images of real-world objects. Our model also achieves better\nperformance in more fine-grained text to shape retrieval, demonstrating better\ntext-and-shape alignment than point cloud based models.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Jun 2024 14:16:12 GMT'}, {'version': 'v2', 'created': 'Thu, 17 Oct 2024 21:05:16 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 23:55:24 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Lee', 'Han-Hung', ''], ['Zhang', 'Yiming', ''], ['Chang', 'Angel X.', '']]","extracted_entities":"[{'text': 'cross-view attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross-view attention","similarity_score":0.5597537756}
{"id":2406.12179,"submitter":"Roman Beliy","authors":"Roman Beliy, Navve Wasserman, Amit Zalcher, Michal Irani","title":"The Wisdom of a Crowd of Brains: A Universal Brain Encoder","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Image-to-fMRI encoding is important for both neuroscience research and\npractical applications. However, such \"Brain-Encoders\" have been typically\ntrained per-subject and per fMRI-dataset, thus restricted to very limited\ntraining data. In this paper we propose a Universal Brain-Encoder, which can be\ntrained jointly on data from many different subjects\/datasets\/machines. What\nmakes this possible is our new voxel-centric Encoder architecture, which learns\na unique \"voxel-embedding\" per brain-voxel. Our Encoder trains to predict the\nresponse of each brain-voxel on every image, by directly computing the\ncross-attention between the brain-voxel embedding and multi-level deep image\nfeatures. This voxel-centric architecture allows the functional role of each\nbrain-voxel to naturally emerge from the voxel-image cross-attention. We show\nthe power of this approach to (i) combine data from multiple different subjects\n(a \"Crowd of Brains\") to improve each individual brain-encoding, (ii) quick &\neffective Transfer-Learning across subjects, datasets, and machines (e.g.,\n3-Tesla, 7-Tesla), with few training examples, and (iii) use the learned\nvoxel-embeddings as a powerful tool to explore brain functionality (e.g., what\nis encoded where in the brain).\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Jun 2024 01:17:07 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 23:24:48 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Beliy', 'Roman', ''], ['Wasserman', 'Navve', ''], ['Zalcher', 'Amit', ''], ['Irani', 'Michal', '']]","extracted_entities":"[{'text': 'Image-to-fMRI encoding', 'label': 'Embedding'}, {'text': 'voxel-embedding', 'label': 'Embedding'}, {'text': 'cross-attention', 'label': 'Attention mechanism'}, {'text': 'brain-voxel embedding', 'label': 'Embedding'}, {'text': 'voxel-image cross-attention', 'label': 'Attention mechanism'}, {'text': 'Transfer-Learning', 'label': 'Few-shot Learning'}, {'text': 'voxel-embeddings', 'label': 'Embedding'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross-attention","similarity_score":0.6773566008}
{"id":2407.1031,"submitter":"Dong Chen","authors":"Dong Chen, Arman Hosseini, Arik Smith, Zeyang Zheng, David Xiang,\n  Arsalan Heydarian, Omid Shoghli, Bradford Campbell","title":"Impact of Road Infrastructure and Traffic Scenarios on E-scooterists'\n  Riding and Gaze Behavior","comments":"12 pages, 10 figures","journal-ref":"International Conference on Transportation & Development (ICTD\n  2025)","doi":null,"report-no":null,"categories":"cs.CY cs.SY eess.SY","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The growing adoption of e-scooters has raised significant safety concerns,\nparticularly due to a surge in injuries and fatalities. This study explores the\nrelationship between road infrastructure, traffic scenarios, and e-scooterists'\nriding and gaze behaviors to improve road safety and user experience. A\nnaturalistic study was conducted using instrumented e-scooters, capturing gaze\npatterns, fixation metrics, and head movement data across various road layouts\nand traffic scenarios. Key findings reveal that bike lanes offer a stable\nenvironment with reduced horizontal head movement and focused attention on the\nroad, while shared roads and sidewalks lead to more dispersed gaze and\nincreased head movement, indicating higher uncertainty and complexity.\nInteractions with other road users, such as navigating intersections, passing\nbuses, riding near cars, and descending on downhill paths, demand greater\ncognitive load. Intersections require heightened visual focus and spatial\nawareness, reflected in increased horizontal eye and head movements.\nInteractions with vehicles prioritize visual scanning over head movement to\nmaintain stability and avoid collisions, while high-speed and downhill riding\ndemand focused attention on obstacles and the road surface. The results provide\ninsights into e-scooter riders' behavior and physiological response analysis,\npaving the way for safer riding experiences and improved understanding of their\nneeds.\n","versions":"[{'version': 'v1', 'created': 'Sun, 5 May 2024 19:55:46 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 03:00:26 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Chen', 'Dong', ''], ['Hosseini', 'Arman', ''], ['Smith', 'Arik', ''], ['Zheng', 'Zeyang', ''], ['Xiang', 'David', ''], ['Heydarian', 'Arsalan', ''], ['Shoghli', 'Omid', ''], ['Campbell', 'Bradford', '']]","extracted_entities":"[{'text': 'gaze\\npatterns', 'label': 'Attention mechanism'}, {'text': 'focused attention', 'label': 'Attention mechanism'}, {'text': 'focused attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"focused attention","similarity_score":0.7952965498}
{"id":2407.12331,"submitter":"Junseo Park","authors":"Junseo Park and Hyeryung Jang","title":"I2AM: Interpreting Image-to-Image Latent Diffusion Models via\n  Bi-Attribution Maps","comments":"23 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Large-scale diffusion models have made significant advances in image\ngeneration, particularly through cross-attention mechanisms. While\ncross-attention has been well-studied in text-to-image tasks, their\ninterpretability in image-to-image (I2I) diffusion models remains\nunderexplored. This paper introduces Image-to-Image Attribution Maps (I2AM), a\nmethod that enhances the interpretability of I2I models by visualizing\nbidirectional attribution maps, from the reference image to the generated image\nand vice versa. I2AM aggregates cross-attention scores across time steps,\nattention heads, and layers, offering insights into how critical features are\ntransferred between images. We demonstrate the effectiveness of I2AM across\nobject detection, inpainting, and super-resolution tasks. Our results\ndemonstrate that I2AM successfully identifies key regions responsible for\ngenerating the output, even in complex scenes. Additionally, we introduce the\nInpainting Mask Attention Consistency Score (IMACS) as a novel evaluation\nmetric to assess the alignment between attribution maps and inpainting masks,\nwhich correlates strongly with existing performance metrics. Through extensive\nexperiments, we show that I2AM enables model debugging and refinement,\nproviding practical tools for improving I2I model's performance and\ninterpretability.\n","versions":"[{'version': 'v1', 'created': 'Wed, 17 Jul 2024 06:15:05 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 08:27:10 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Park', 'Junseo', ''], ['Jang', 'Hyeryung', '']]","extracted_entities":"[{'text': 'cross-attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'cross-attention', 'label': 'Attention mechanism'}, {'text': 'cross-attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross-attention mechanisms","similarity_score":0.8177332282}
{"id":2407.15176,"submitter":"Xiaoran Liu","authors":"Xiaoran Liu, Ruixiao Li, Qipeng Guo, Zhigeng Liu, Yuerong Song, Kai\n  Lv, Hang Yan, Linlin Li, Qun Liu, Xipeng Qiu","title":"ReAttention: Training-Free Infinite Context with Finite Attention Scope","comments":"21 pages, 11 figures, Accepted by ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The long-context capability of the Large Language Models (LLM) has made\nsignificant breakthroughs, but the maximum supported context length in length\nextrapolation remains a critical bottleneck limiting their practical\napplications. The constraint of context length in LLMs arises from the\nself-attention mechanism, which cannot effectively and efficiently capture the\nsemantic relationships within infinitely long contexts via the limited\npre-trained positional information and attention scope. In this work, we\npropose ReAttention, a training-free approach enabling LLM based on the\nself-attention mechanism to support an infinite context with a finite attention\nscope under sufficient memory resources. ReAttention performs the\nposition-agnostic top-$k$ attention before the ordinary position-aware\nself-attention, freeing LLMs from the length extrapolation issue. We validate\nthe performance of ReAttention on the LongBench, L-Eval, and InfiniteBench and\ndemonstrate that it is on par with traditional methods. Furthermore, we also\napply ReAttention on mainstream LLMs, including LLaMA3.1-8B and\nMistral-v0.3-7B, enabling them to support context lengths of at least 1M and\neven expanding the context length of LLaMA3.2-3B-chat by 128$\\times$ to 4M\nwithout any further training in Needle-In-A-Haystack tests. We also improve the\nefficiency of ReAttention with Triton and achieve an efficient extrapolation\nwithout additional overhead. The code is available at\nhttps:\/\/github.com\/OpenMOSS\/ReAttention.\n","versions":"[{'version': 'v1', 'created': 'Sun, 21 Jul 2024 14:23:37 GMT'}, {'version': 'v2', 'created': 'Sat, 5 Oct 2024 02:09:26 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 12:15:10 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Liu', 'Xiaoran', ''], ['Li', 'Ruixiao', ''], ['Guo', 'Qipeng', ''], ['Liu', 'Zhigeng', ''], ['Song', 'Yuerong', ''], ['Lv', 'Kai', ''], ['Yan', 'Hang', ''], ['Li', 'Linlin', ''], ['Liu', 'Qun', ''], ['Qiu', 'Xipeng', '']]","extracted_entities":"[{'text': 'self-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'ReAttention', 'label': 'contextual Embedding'}, {'text': 'self-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'ReAttention', 'label': 'contextual Embedding'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'ReAttention', 'label': 'contextual Embedding'}, {'text': 'ReAttention', 'label': 'contextual Embedding'}, {'text': 'ReAttention', 'label': 'contextual Embedding'}, {'text': 'Triton', 'label': 'Mistral'}, {'text': 'ReAttention', 'label': 'contextual Embedding'}]","assigned_concept":"Attention mechanism","matched_keyword":"self-attention mechanism","similarity_score":0.8757837415}
{"id":2408.12252,"submitter":"Haotian Zhang","authors":"Haotian Zhang, Shijian Gao, Xiang Cheng, Liuqing Yang","title":"Synesthesia of Machines (SoM)-Enhanced Wideband Multi-User CSI Learning\n  With LiDAR Sensing","comments":"6 pages, 4 figures, 1 table","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Light detection and ranging (LiDAR) has been utilized for optimizing wireless\ncommunications due to its ability to detect the environment. This paper\nexplores the use of LiDAR in channel estimation for wideband multi-user\nmultiple-input-multiple-output orthogonal frequency division multiplexing\nsystems and introduces a LiDAR-enhanced Channel State Information (CSI)\nlearning network (LE-CLN). By utilizing user positioning information, LE-CLN\nfirst calculates user-localized over-complete angular measurements. It then\ninvestigates the correlation between LiDAR and CSI, transforming raw LiDAR data\ninto a low-complexity format embedded with signal propagation characteristics.\nLE-CLN also adapts the use of LiDAR based on channel conditions through\nattention mechanisms. Thanks to the unique wireless features offered by LiDAR,\nLE-CLN achieves higher estimation accuracy and spectrum efficiency compared to\nbenchmarks, particularly in latency-sensitive applications where pilot\ntransmissions are expected to be reduced.\n","versions":"[{'version': 'v1', 'created': 'Thu, 22 Aug 2024 09:44:15 GMT'}, {'version': 'v2', 'created': 'Fri, 6 Dec 2024 02:41:47 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 05:46:23 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Zhang', 'Haotian', ''], ['Gao', 'Shijian', ''], ['Cheng', 'Xiang', ''], ['Yang', 'Liuqing', '']]","extracted_entities":"[{'text': 'attention mechanisms', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanisms","similarity_score":0.9558142424}
{"id":2408.15185,"submitter":"Ghazal Alinezhad Noghre","authors":"Ghazal Alinezhad Noghre, Armin Danesh Pazho, Hamed Tabkhi","title":"Human-Centric Video Anomaly Detection Through Spatio-Temporal Pose\n  Tokenization and Transformer","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Video Anomaly Detection (VAD) presents a significant challenge in computer\nvision, particularly due to the unpredictable and infrequent nature of\nanomalous events, coupled with the diverse and dynamic environments in which\nthey occur. Human-centric VAD, a specialized area within this domain, faces\nadditional complexities, including variations in human behavior, potential\nbiases in data, and substantial privacy concerns related to human subjects.\nThese issues complicate the development of models that are both robust and\ngeneralizable. To address these challenges, recent advancements have focused on\npose-based VAD, which leverages human pose as a high-level feature to mitigate\nprivacy concerns, reduce appearance biases, and minimize background\ninterference. In this paper, we introduce SPARTA, a novel transformer-based\narchitecture designed specifically for human-centric pose-based VAD. SPARTA\nintroduces an innovative Spatio-Temporal Pose and Relative Pose (ST-PRP)\ntokenization method that produces an enriched representation of human motion\nover time. This approach ensures that the transformer's attention mechanism\ncaptures both spatial and temporal patterns simultaneously, rather than\nfocusing on only one aspect. The addition of the relative pose further\nemphasizes subtle deviations from normal human movements. The architecture's\ncore, a novel Unified Encoder Twin Decoders (UETD) transformer, significantly\nimproves the detection of anomalous behaviors in video data. Extensive\nevaluations across multiple benchmark datasets demonstrate that SPARTA\nconsistently outperforms existing methods, establishing a new state-of-the-art\nin pose-based VAD.\n","versions":"[{'version': 'v1', 'created': 'Tue, 27 Aug 2024 16:40:14 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 14:05:49 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Noghre', 'Ghazal Alinezhad', ''], ['Pazho', 'Armin Danesh', ''], ['Tabkhi', 'Hamed', '']]","extracted_entities":"[{'text': 'privacy concerns', 'label': 'AI Ethics'}, {'text': 'privacy concerns', 'label': 'AI Ethics'}, {'text': 'attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanism","similarity_score":1.0}
{"id":2409.17273,"submitter":"Shravan Venkatraman","authors":"Shravan Venkatraman, Pandiyaraju V, Abeshek A, Aravintakshan S A,\n  Pavan Kumar S, Kannan A, Madhan S","title":"Targeted Neural Architectures in Multi-Objective Frameworks for Complete\n  Glioma Characterization from Multimodal MRI","comments":"29 pages, 25 figures, 6 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Brain tumors result from abnormal cell growth in brain tissue. If\nundiagnosed, they cause neurological deficits, including cognitive impairment,\nmotor dysfunction, and sensory loss. As tumors grow, intracranial pressure\nincreases, potentially leading to fatal complications such as brain herniation.\nEarly diagnosis and treatment are crucial to controlling these effects and\nslowing tumor progression. Deep learning (DL) and artificial intelligence (AI)\nare increasingly used to assist doctors in early diagnosis through magnetic\nresonance imaging (MRI) scans. Our research proposes targeted neural\narchitectures within multi-objective frameworks that can localize, segment, and\nclassify the grade of these gliomas from multimodal MRI images to solve this\ncritical issue. Our localization framework utilizes a targeted architecture\nthat enhances the LinkNet framework with an encoder inspired by VGG19 for\nbetter multimodal feature extraction from the tumor along with spatial and\ngraph attention mechanisms that sharpen feature focus and inter-feature\nrelationships. For the segmentation objective, we deployed a specialized\nframework using the SeResNet101 CNN model as the encoder backbone integrated\ninto the LinkNet architecture, achieving an IoU Score of 96%. The\nclassification objective is addressed through a distinct framework implemented\nby combining the SeResNet152 feature extractor with Adaptive Boosting\nclassifier, reaching an accuracy of 98.53%. Our multi-objective approach with\ntargeted neural architectures demonstrated promising results for complete\nglioma characterization, with the potential to advance medical AI by enabling\nearly diagnosis and providing more accurate treatment options for patients.\n","versions":"[{'version': 'v1', 'created': 'Wed, 25 Sep 2024 18:38:57 GMT'}, {'version': 'v2', 'created': 'Sat, 23 Nov 2024 07:55:26 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Mar 2025 15:56:39 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Venkatraman', 'Shravan', ''], ['V', 'Pandiyaraju', ''], ['A', 'Abeshek', ''], ['A', 'Aravintakshan S', ''], ['S', 'Pavan Kumar', ''], ['A', 'Kannan', ''], ['S', 'Madhan', '']]","extracted_entities":"[{'text': 'spatial and\\ngraph attention mechanisms', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"spatial and\ngraph attention mechanisms","similarity_score":0.7406826615}
{"id":2410.21967,"submitter":"Chengkai Huang","authors":"Hongtao Huang, Chengkai Huang, Tong Yu, Xiaojun Chang, Wen Hu, Julian\n  McAuley, Lina Yao","title":"Dual Conditional Diffusion Models for Sequential Recommendation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Recent advancements in diffusion models have shown promising results in\nsequential recommendation (SR). Existing approaches predominantly rely on\nimplicit conditional diffusion models, which compress user behaviors into a\nsingle representation during the forward diffusion process. While effective to\nsome extent, this oversimplification often leads to the loss of sequential and\ncontextual information, which is critical for understanding user behavior.\nMoreover, explicit information, such as user-item interactions or sequential\npatterns, remains underutilized, despite its potential to directly guide the\nrecommendation process and improve precision. However, combining implicit and\nexplicit information is non-trivial, as it requires dynamically integrating\nthese complementary signals while avoiding noise and irrelevant patterns within\nuser behaviors. To address these challenges, we propose Dual Conditional\nDiffusion Models for Sequential Recommendation (DCRec), which effectively\nintegrates implicit and explicit information by embedding dual conditions into\nboth the forward and reverse diffusion processes. This allows the model to\nretain valuable sequential and contextual information while leveraging explicit\nuser-item interactions to guide the recommendation process. Specifically, we\nintroduce the Dual Conditional Diffusion Transformer (DCDT), which employs a\ncross-attention mechanism to dynamically integrate explicit signals throughout\nthe diffusion stages, ensuring contextual understanding and minimizing the\ninfluence of irrelevant patterns. This design enables precise and contextually\nrelevant recommendations. Extensive experiments on public benchmark datasets\ndemonstrate that DCRec significantly outperforms state-of-the-art methods in\nboth accuracy and computational efficiency.\n","versions":"[{'version': 'v1', 'created': 'Tue, 29 Oct 2024 11:51:06 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 04:42:54 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Huang', 'Hongtao', ''], ['Huang', 'Chengkai', ''], ['Yu', 'Tong', ''], ['Chang', 'Xiaojun', ''], ['Hu', 'Wen', ''], ['McAuley', 'Julian', ''], ['Yao', 'Lina', '']]","extracted_entities":"[{'text': 'cross-attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross-attention mechanism","similarity_score":0.8302809}
{"id":2411.02767,"submitter":"Uday Kiran Reddy Tadipatri","authors":"Uday Kiran Reddy Tadipatri, Benjamin D. Haeffele, Joshua Agterberg,\n  Ren\\'e Vidal","title":"A Convex Relaxation Approach to Generalization Analysis for Parallel\n  Positively Homogeneous Networks","comments":"Accepted at AISTATS 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG eess.SP stat.ML","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We propose a general framework for deriving generalization bounds for\nparallel positively homogeneous neural networks--a class of neural networks\nwhose input-output map decomposes as the sum of positively homogeneous maps.\nExamples of such networks include matrix factorization and sensing,\nsingle-layer multi-head attention mechanisms, tensor factorization, deep linear\nand ReLU networks, and more. Our general framework is based on linking the\nnon-convex empirical risk minimization (ERM) problem to a closely related\nconvex optimization problem over prediction functions, which provides a global,\nachievable lower-bound to the ERM problem. We exploit this convex lower-bound\nto perform generalization analysis in the convex space while controlling the\ndiscrepancy between the convex model and its non-convex counterpart. We apply\nour general framework to a wide variety of models ranging from low-rank matrix\nsensing, to structured matrix sensing, two-layer linear networks, two-layer\nReLU networks, and single-layer multi-head attention mechanisms, achieving\ngeneralization bounds with a sample complexity that scales almost linearly with\nthe network width.\n","versions":"[{'version': 'v1', 'created': 'Tue, 5 Nov 2024 03:24:34 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 21:12:45 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Tadipatri', 'Uday Kiran Reddy', ''], ['Haeffele', 'Benjamin D.', ''], ['Agterberg', 'Joshua', ''], ['Vidal', 'Ren\u00e9', '']]","extracted_entities":"[{'text': 'single-layer multi-head attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'single-layer multi-head attention mechanisms', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"single-layer multi-head attention mechanisms","similarity_score":0.7406455874}
{"id":2411.08205,"submitter":"Ricardo Felipe Ferreira","authors":"Ricardo F. Ferreira, Matheus E. Pacola, Vitor G. Schiavone and Rodrigo\n  F. O. Pena","title":"Consistent model selection for estimating functional interactions among\n  stochastic neurons with variable-length memory","comments":"51 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.AP","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We address the problem of identifying functional interactions among\nstochastic neurons with variable-length memory from their spiking activity. The\nneuronal network is modeled by a stochastic system of interacting point\nprocesses with variable-length memory. Each chain describes the activity of a\nsingle neuron, indicating whether it spikes at a given time. One neuron's\ninfluence on another can be either excitatory or inhibitory. To identify the\nexistence and nature of an interaction between a neuron and its postsynaptic\ncounterpart, we propose a model selection procedure based on the observation of\nthe spike activity of a finite set of neurons over a finite time. The proposed\nprocedure is also based on the maximum likelihood estimator for the synaptic\nweight matrix of the network neuronal model. In this sense, we prove the\nconsistency of the maximum likelihood estimator {followed} by a proof of the\nconsistency of the neighborhood interaction estimation procedure. The\neffectiveness of the proposed model selection procedure is demonstrated using\nsimulated data, which validates the underlying theory. The method is also\napplied to analyze spike train data recorded from hippocampal neurons in rats\nduring a visual attention task, where a computational model reconstructs the\nspiking activity and the results reveal interesting and biologically relevant\ninformation.\n","versions":"[{'version': 'v1', 'created': 'Tue, 12 Nov 2024 21:52:51 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 23:26:46 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Ferreira', 'Ricardo F.', ''], ['Pacola', 'Matheus E.', ''], ['Schiavone', 'Vitor G.', ''], ['Pena', 'Rodrigo F. O.', '']]","extracted_entities":"[{'text': 'visual attention task', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"visual attention task","similarity_score":0.7174323797}
{"id":2411.10411,"submitter":"Onay Urfalioglu","authors":"Markus Karmann, Onay Urfalioglu","title":"Repurposing Stable Diffusion Attention for Training-Free Unsupervised\n  Interactive Segmentation","comments":"Accepted by CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent progress in interactive point prompt based Image Segmentation allows\nto significantly reduce the manual effort to obtain high quality semantic\nlabels. State-of-the-art unsupervised methods use self-supervised pre-trained\nmodels to obtain pseudo-labels which are used in training a prompt-based\nsegmentation model. In this paper, we propose a novel unsupervised and\ntraining-free approach based solely on the self-attention of Stable Diffusion.\nWe interpret the self-attention tensor as a Markov transition operator, which\nenables us to iteratively construct a Markov chain. Pixel-wise counting of the\nrequired number of iterations along the Markov chain to reach a relative\nprobability threshold yields a Markov-iteration-map, which we simply call a\nMarkov-map. Compared to the raw attention maps, we show that our proposed\nMarkov-map has less noise, sharper semantic boundaries and more uniform values\nwithin semantically similar regions. We integrate the Markov-map in a simple\nyet effective truncated nearest neighbor framework to obtain interactive point\nprompt based segmentation. Despite being training-free, we experimentally show\nthat our approach yields excellent results in terms of Number of Clicks (NoC),\neven outperforming state-of-the-art training based unsupervised methods in most\nof the datasets. Code is available at https:\/\/github.com\/mkarmann\/m2n2.\n","versions":"[{'version': 'v1', 'created': 'Fri, 15 Nov 2024 18:29:59 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 16:15:14 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Karmann', 'Markus', ''], ['Urfalioglu', 'Onay', '']]","extracted_entities":"[{'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'Markov transition operator', 'label': 'Attention mechanism'}, {'text': 'Markov chain', 'label': 'Chain of thought'}, {'text': 'Markov chain', 'label': 'Attention mechanism'}, {'text': 'interactive point\\nprompt based segmentation', 'label': 'Prompting'}]","assigned_concept":"Attention mechanism","matched_keyword":"self-attention","similarity_score":0.731767118}
{"id":2411.15205,"submitter":"Jignyu Zhuang","authors":"Jingyu Zhuang, Di Kang, Linchao Bao, Liang Lin, Guanbin Li","title":"DAGSM: Disentangled Avatar Generation with GS-enhanced Mesh","comments":"Accepted by CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.GR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Text-driven avatar generation has gained significant attention owing to its\nconvenience. However, existing methods typically model the human body with all\ngarments as a single 3D model, limiting its usability, such as clothing\nreplacement, and reducing user control over the generation process. To overcome\nthe limitations above, we propose DAGSM, a novel pipeline that generates\ndisentangled human bodies and garments from the given text prompts.\nSpecifically, we model each part (e.g., body, upper\/lower clothes) of the\nclothed human as one GS-enhanced mesh (GSM), which is a traditional mesh\nattached with 2D Gaussians to better handle complicated textures (e.g., woolen,\ntranslucent clothes) and produce realistic cloth animations. During the\ngeneration, we first create the unclothed body, followed by a sequence of\nindividual cloth generation based on the body, where we introduce a\nsemantic-based algorithm to achieve better human-cloth and garment-garment\nseparation. To improve texture quality, we propose a view-consistent texture\nrefinement module, including a cross-view attention mechanism for texture style\nconsistency and an incident-angle-weighted denoising (IAW-DE) strategy to\nupdate the appearance. Extensive experiments have demonstrated that DAGSM\ngenerates high-quality disentangled avatars, supports clothing replacement and\nrealistic animation, and outperforms the baselines in visual quality.\n","versions":"[{'version': 'v1', 'created': 'Wed, 20 Nov 2024 07:00:48 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 03:14:15 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Zhuang', 'Jingyu', ''], ['Kang', 'Di', ''], ['Bao', 'Linchao', ''], ['Lin', 'Liang', ''], ['Li', 'Guanbin', '']]","extracted_entities":"[{'text': 'cross-view attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross-view attention mechanism","similarity_score":0.7163535357}
{"id":2411.17214,"submitter":"Xiaoming Zhang","authors":"Chengxing Xie, Xiaoming Zhang, Linze Li, Yuqian Fu, Biao Gong, Tianrui\n  Li, Kai Zhang","title":"MAT: Multi-Range Attention Transformer for Efficient Image\n  Super-Resolution","comments":"IEEE CSVT","journal-ref":null,"doi":"10.1109\/TCSVT.2025.3553135","report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Image super-resolution (SR) has significantly advanced through the adoption\nof Transformer architectures. However, conventional techniques aimed at\nenlarging the self-attention window to capture broader contexts come with\ninherent drawbacks, especially the significantly increased computational\ndemands. Moreover, the feature perception within a fixed-size window of\nexisting models restricts the effective receptive field (ERF) and the\nintermediate feature diversity. We demonstrate that a flexible integration of\nattention across diverse spatial extents can yield significant performance\nenhancements. In line with this insight, we introduce Multi-Range Attention\nTransformer (MAT) for SR tasks. MAT leverages the computational advantages\ninherent in dilation operation, in conjunction with self-attention mechanism,\nto facilitate both multi-range attention (MA) and sparse multi-range attention\n(SMA), enabling efficient capture of both regional and sparse global features.\nCombined with local feature extraction, MAT adeptly capture dependencies across\nvarious spatial ranges, improving the diversity and efficacy of its feature\nrepresentations. We also introduce the MSConvStar module, which augments the\nmodel's ability for multi-range representation learning. Comprehensive\nexperiments show that our MAT exhibits superior performance to existing\nstate-of-the-art SR models with remarkable efficiency (~3.3 faster than\nSRFormer-light).\n","versions":"[{'version': 'v1', 'created': 'Tue, 26 Nov 2024 08:30:31 GMT'}, {'version': 'v2', 'created': 'Tue, 17 Dec 2024 03:01:53 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Mar 2025 03:09:55 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Xie', 'Chengxing', ''], ['Zhang', 'Xiaoming', ''], ['Li', 'Linze', ''], ['Fu', 'Yuqian', ''], ['Gong', 'Biao', ''], ['Li', 'Tianrui', ''], ['Zhang', 'Kai', '']]","extracted_entities":"[{'text': 'Transformer architectures', 'label': 'Transformers'}, {'text': 'self-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'multi-range attention', 'label': 'Attention mechanism'}, {'text': 'sparse multi-range attention', 'label': 'Attention mechanism'}, {'text': 'multi-range representation learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Attention mechanism","matched_keyword":"self-attention mechanism","similarity_score":0.8757837415}
{"id":2412.01537,"submitter":"Xingyu Chen","authors":"Xingyu Chen, Zhuheng Song, Xiaoke Jiang, Yaoqing Hu, Junzhi Yu, Lei\n  Zhang","title":"HandOS: 3D Hand Reconstruction in One Stage","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.GR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Existing approaches of hand reconstruction predominantly adhere to a\nmulti-stage framework, encompassing detection, left-right classification, and\npose estimation. This paradigm induces redundant computation and cumulative\nerrors. In this work, we propose HandOS, an end-to-end framework for 3D hand\nreconstruction. Our central motivation lies in leveraging a frozen detector as\nthe foundation while incorporating auxiliary modules for 2D and 3D keypoint\nestimation. In this manner, we integrate the pose estimation capacity into the\ndetection framework, while at the same time obviating the necessity of using\nthe left-right category as a prerequisite. Specifically, we propose an\ninteractive 2D-3D decoder, where 2D joint semantics is derived from detection\ncues while 3D representation is lifted from those of 2D joints. Furthermore,\nhierarchical attention is designed to enable the concurrent modeling of 2D\njoints, 3D vertices, and camera translation. Consequently, we achieve an\nend-to-end integration of hand detection, 2D pose estimation, and 3D mesh\nreconstruction within a one-stage framework, so that the above multi-stage\ndrawbacks are overcome. Meanwhile, the HandOS reaches state-of-the-art\nperformances on public benchmarks, e.g., 5.0 PA-MPJPE on FreiHand and 64.6\\%\nPCK@0.05 on HInt-Ego4D. Project page: idea-research.github.io\/HandOSweb.\n","versions":"[{'version': 'v1', 'created': 'Mon, 2 Dec 2024 14:28:29 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 02:43:35 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Chen', 'Xingyu', ''], ['Song', 'Zhuheng', ''], ['Jiang', 'Xiaoke', ''], ['Hu', 'Yaoqing', ''], ['Yu', 'Junzhi', ''], ['Zhang', 'Lei', '']]","extracted_entities":"[{'text': 'hierarchical attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"hierarchical attention","similarity_score":0.6840362549}
{"id":2412.0315,"submitter":"Siyoon Jin","authors":"Siyoon Jin, Jisu Nam, Jiyoung Kim, Dahyun Chung, Yeong-Seok Kim,\n  Joonhyung Park, Heonjeong Chu, Seungryong Kim","title":"Appearance Matching Adapter for Exemplar-based Semantic Image Synthesis\n  in-the-Wild","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Exemplar-based semantic image synthesis generates images aligned with\nsemantic content while preserving the appearance of an exemplar. Conventional\nstructure-guidance models like ControlNet, are limited as they rely solely on\ntext prompts to control appearance and cannot utilize exemplar images as input.\nRecent tuning-free approaches address this by transferring local appearance via\nimplicit cross-image matching in the augmented self-attention mechanism of\npre-trained diffusion models. However, prior works are often restricted to\nsingle-object cases or foreground object appearance transfer, struggling with\ncomplex scenes involving multiple objects. To overcome this, we propose\nAM-Adapter (Appearance Matching Adapter) to address exemplar-based semantic\nimage synthesis in-the-wild, enabling multi-object appearance transfer from a\nsingle scene-level image. AM-Adapter automatically transfers local appearances\nfrom the scene-level input. AM-Adapter alternatively provides controllability\nto map user-defined object details to specific locations in the synthesized\nimages. Our learnable framework enhances cross-image matching within augmented\nself-attention by integrating semantic information from segmentation maps. To\ndisentangle generation and matching, we adopt stage-wise training. We first\ntrain the structure-guidance and generation networks, followed by training the\nmatching adapter while keeping the others frozen. During inference, we\nintroduce an automated exemplar retrieval method for selecting exemplar\nimage-segmentation pairs efficiently. Despite utilizing minimal learnable\nparameters, AM-Adapter achieves state-of-the-art performance, excelling in both\nsemantic alignment and local appearance fidelity. Extensive ablations validate\nour design choices. Code and weights will be released.:\nhttps:\/\/cvlab-kaist.github.io\/AM-Adapter\/\n","versions":"[{'version': 'v1', 'created': 'Wed, 4 Dec 2024 09:17:47 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 07:31:49 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Jin', 'Siyoon', ''], ['Nam', 'Jisu', ''], ['Kim', 'Jiyoung', ''], ['Chung', 'Dahyun', ''], ['Kim', 'Yeong-Seok', ''], ['Park', 'Joonhyung', ''], ['Chu', 'Heonjeong', ''], ['Kim', 'Seungryong', '']]","extracted_entities":"[{'text': 'text prompts', 'label': 'Prompting'}, {'text': 'augmented self-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'AM-Adapter', 'label': 'LLM-based'}, {'text': 'AM-Adapter', 'label': 'LLM-based'}, {'text': 'AM-Adapter', 'label': 'LLM-based'}, {'text': 'AM-Adapter', 'label': 'LLM-based'}]","assigned_concept":"Attention mechanism","matched_keyword":"augmented self-attention mechanism","similarity_score":0.7923624516}
{"id":2412.0846,"submitter":"Fermin Orozco","authors":"Fermin Orozco, Pedro Porto Buarque de Gusm\\~ao, Hongkai Wen, Johan\n  Wahlstr\\\"om, Man Luo","title":"Federated Learning for Traffic Flow Prediction with Synthetic Data\n  Augmentation","comments":"11 pages, 7 figures, 6 tables, ACM format","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.DC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Deep-learning based traffic prediction models require vast amounts of data to\nlearn embedded spatial and temporal dependencies. The inherent privacy and\ncommercial sensitivity of such data has encouraged a shift towards\ndecentralised data-driven methods, such as Federated Learning (FL). Under a\ntraditional Machine Learning paradigm, traffic flow prediction models can\ncapture spatial and temporal relationships within centralised data. In reality,\ntraffic data is likely distributed across separate data silos owned by multiple\nstakeholders. In this work, a cross-silo FL setting is motivated to facilitate\nstakeholder collaboration for optimal traffic flow prediction applications.\nThis work introduces an FL framework, referred to as FedTPS, to generate\nsynthetic data to augment each client's local dataset by training a\ndiffusion-based trajectory generation model through FL. The proposed framework\nis evaluated on a large-scale real world ride-sharing dataset using various FL\nmethods and Traffic Flow Prediction models, including a novel prediction model\nwe introduce, which leverages Temporal and Graph Attention mechanisms to learn\nthe Spatio-Temporal dependencies embedded within regional traffic flow data.\nExperimental results show that FedTPS outperforms multiple other FL baselines\nwith respect to global model performance.\n","versions":"[{'version': 'v1', 'created': 'Wed, 11 Dec 2024 15:25:38 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 13:29:36 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Orozco', 'Fermin', ''], ['de Gusm\u00e3o', 'Pedro Porto Buarque', ''], ['Wen', 'Hongkai', ''], ['Wahlstr\u00f6m', 'Johan', ''], ['Luo', 'Man', '']]","extracted_entities":"[{'text': 'Temporal and Graph Attention mechanisms', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Temporal and Graph Attention mechanisms","similarity_score":0.7344149351}
{"id":2501.01023,"submitter":"Ziyang Chen","authors":"Ziyang Chen, Yongjun Zhang, Wenting Li, Bingshu Wang, Yabo Wu, Yong\n  Zhao, C.L. Philip Chen","title":"Hadamard Attention Recurrent Transformer: A Strong Baseline for Stereo\n  Matching Transformer","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  In light of the advancements in transformer technology, extant research\nposits the construction of stereo transformers as a potential solution to the\nbinocular stereo matching challenge. However, constrained by the low-rank\nbottleneck and quadratic complexity of attention mechanisms, stereo\ntransformers still fail to demonstrate sufficient nonlinear expressiveness\nwithin a reasonable inference time. The lack of focus on key homonymous points\nrenders the representations of such methods vulnerable to challenging\nconditions, including reflections and weak textures. Furthermore, a slow\ncomputing speed is not conducive to the application. To overcome these\ndifficulties, we present the Hadamard Attention Recurrent Stereo Transformer\n(HART) that incorporates the following components: 1) For faster inference, we\npresent a Hadamard product paradigm for the attention mechanism, achieving\nlinear computational complexity. 2) We designed a Dense Attention Kernel (DAK)\nto amplify the differences between relevant and irrelevant feature responses.\nThis allows HART to focus on important details. DAK also converts zero elements\nto non-zero elements to mitigate the reduced expressiveness caused by the\nlow-rank bottleneck. 3) To compensate for the spatial and channel interaction\nmissing in the Hadamard product, we propose MKOI to capture both global and\nlocal information through the interleaving of large and small kernel\nconvolutions. Experimental results demonstrate the effectiveness of our HART.\nIn reflective area, HART ranked 1st on the KITTI 2012 benchmark among all\npublished methods at the time of submission. Code is available at\nhttps:\/\/github.com\/ZYangChen\/HART.\n","versions":"[{'version': 'v1', 'created': 'Thu, 2 Jan 2025 02:51:16 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 15:30:22 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Chen', 'Ziyang', ''], ['Zhang', 'Yongjun', ''], ['Li', 'Wenting', ''], ['Wang', 'Bingshu', ''], ['Wu', 'Yabo', ''], ['Zhao', 'Yong', ''], ['Chen', 'C. L. Philip', '']]","extracted_entities":"[{'text': 'stereo transformers', 'label': 'Transformers'}, {'text': 'attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'stereo\\ntransformers', 'label': 'Transformers'}, {'text': 'attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanism","similarity_score":1.0}
{"id":2501.06187,"submitter":"Tsai-Shien Chen","authors":"Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot\n  Sin Lee, Ivan Skorokhodov, Kfir Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, Sergey\n  Tulyakov","title":"Multi-subject Open-set Personalization in Video Generation","comments":"CVPR 2025. Project page:\n  https:\/\/snap-research.github.io\/open-set-video-personalization\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Video personalization methods allow us to synthesize videos with specific\nconcepts such as people, pets, and places. However, existing methods often\nfocus on limited domains, require time-consuming optimization per subject, or\nsupport only a single subject. We present Video Alchemist $-$ a video model\nwith built-in multi-subject, open-set personalization capabilities for both\nforeground objects and background, eliminating the need for time-consuming\ntest-time optimization. Our model is built on a new Diffusion Transformer\nmodule that fuses each conditional reference image and its corresponding\nsubject-level text prompt with cross-attention layers. Developing such a large\nmodel presents two main challenges: dataset and evaluation. First, as paired\ndatasets of reference images and videos are extremely hard to collect, we\nsample selected video frames as reference images and synthesize a clip of the\ntarget video. However, while models can easily denoise training videos given\nreference frames, they fail to generalize to new contexts. To mitigate this\nissue, we design a new automatic data construction pipeline with extensive\nimage augmentations. Second, evaluating open-set video personalization is a\nchallenge in itself. To address this, we introduce a personalization benchmark\nthat focuses on accurate subject fidelity and supports diverse personalization\nscenarios. Finally, our extensive experiments show that our method\nsignificantly outperforms existing personalization methods in both quantitative\nand qualitative evaluations.\n","versions":"[{'version': 'v1', 'created': 'Fri, 10 Jan 2025 18:59:54 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 17:59:56 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chen', 'Tsai-Shien', ''], ['Siarohin', 'Aliaksandr', ''], ['Menapace', 'Willi', ''], ['Fang', 'Yuwei', ''], ['Lee', 'Kwot Sin', ''], ['Skorokhodov', 'Ivan', ''], ['Aberman', 'Kfir', ''], ['Zhu', 'Jun-Yan', ''], ['Yang', 'Ming-Hsuan', ''], ['Tulyakov', 'Sergey', '']]","extracted_entities":"[{'text': 'subject-level text prompt', 'label': 'Prompting'}, {'text': 'cross-attention layers', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross-attention layers","similarity_score":0.5637996197}
{"id":2502.1554,"submitter":"Milad Sefidgaran","authors":"Milad Sefidgaran and Abdellatif Zaidi and Piotr Krasnowski","title":"Generalization Guarantees for Representation Learning via Data-Dependent\n  Gaussian Mixture Priors","comments":"Accepted as a Spotlight Paper at ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.IT cs.LG math.IT","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We establish in-expectation and tail bounds on the generalization error of\nrepresentation learning type algorithms. The bounds are in terms of the\nrelative entropy between the distribution of the representations extracted from\nthe training and \"test'' datasets and a data-dependent symmetric prior, i.e.,\nthe Minimum Description Length (MDL) of the latent variables for the training\nand test datasets. Our bounds are shown to reflect the \"structure\" and\n\"simplicity'' of the encoder and significantly improve upon the few existing\nones for the studied model. We then use our in-expectation bound to devise a\nsuitable data-dependent regularizer; and we investigate thoroughly the\nimportant question of the selection of the prior. We propose a systematic\napproach to simultaneously learning a data-dependent Gaussian mixture prior and\nusing it as a regularizer. Interestingly, we show that a weighted attention\nmechanism emerges naturally in this procedure. Our experiments show that our\napproach outperforms the now popular Variational Information Bottleneck (VIB)\nmethod as well as the recent Category-Dependent VIB (CDVIB).\n","versions":"[{'version': 'v1', 'created': 'Fri, 21 Feb 2025 15:43:31 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 22:37:44 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Sefidgaran', 'Milad', ''], ['Zaidi', 'Abdellatif', ''], ['Krasnowski', 'Piotr', '']]","extracted_entities":"[{'text': 'weighted attention\\nmechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"weighted attention\nmechanism","similarity_score":0.861823082}
{"id":2503.02009,"submitter":"Mohamed Sayed","authors":"Jamie Wynn, Zawar Qureshi, Jakub Powierza, Jamie Watson, Mohamed Sayed","title":"Morpheus: Text-Driven 3D Gaussian Splat Shape and Color Stylization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Exploring real-world spaces using novel-view synthesis is fun, and\nreimagining those worlds in a different style adds another layer of excitement.\nStylized worlds can also be used for downstream tasks where there is limited\ntraining data and a need to expand a model's training distribution. Most\ncurrent novel-view synthesis stylization techniques lack the ability to\nconvincingly change geometry. This is because any geometry change requires\nincreased style strength which is often capped for stylization stability and\nconsistency. In this work, we propose a new autoregressive 3D Gaussian\nSplatting stylization method. As part of this method, we contribute a new RGBD\ndiffusion model that allows for strength control over appearance and shape\nstylization. To ensure consistency across stylized frames, we use a combination\nof novel depth-guided cross attention, feature injection, and a Warp ControlNet\nconditioned on composite frames for guiding the stylization of new frames. We\nvalidate our method via extensive qualitative results, quantitative\nexperiments, and a user study. Code online.\n","versions":"[{'version': 'v1', 'created': 'Mon, 3 Mar 2025 19:33:22 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 14:11:26 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wynn', 'Jamie', ''], ['Qureshi', 'Zawar', ''], ['Powierza', 'Jakub', ''], ['Watson', 'Jamie', ''], ['Sayed', 'Mohamed', '']]","extracted_entities":"[{'text': 'novel depth-guided cross attention', 'label': 'Attention mechanism'}, {'text': 'feature injection', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"novel depth-guided cross attention","similarity_score":0.5329167843}
{"id":2503.05966,"submitter":"Md Talha Mohsin","authors":"Md Talha Mohsin and Nabid Bin Nasim","title":"Explaining the Unexplainable: A Systematic Review of Explainable AI in\n  Finance","comments":"2 tables, 11 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"q-fin.GN cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Practitioners and researchers trying to strike a balance between accuracy and\ntransparency center Explainable Artificial Intelligence (XAI) at the junction\nof finance. This paper offers a thorough overview of the changing scene of XAI\napplications in finance together with domain-specific implementations,\nmethodological developments, and trend mapping of research. Using bibliometric\nand content analysis, we find topic clusters, significant research, and most\noften used explainability strategies used in financial industries. Our results\nshow a substantial dependence on post-hoc interpretability techniques;\nattention mechanisms, feature importance analysis and SHAP are the most often\nused techniques among them. This review stresses the need of multidisciplinary\napproaches combining financial knowledge with improved explainability paradigms\nand exposes important shortcomings in present XAI systems.\n","versions":"[{'version': 'v1', 'created': 'Fri, 7 Mar 2025 22:36:44 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 15:37:42 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Mohsin', 'Md Talha', ''], ['Nasim', 'Nabid Bin', '']]","extracted_entities":"[{'text': 'attention mechanisms', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanisms","similarity_score":0.9558142424}
{"id":2503.06052,"submitter":"Xuexin Chen","authors":"Xuexin Chen, Ruichu Cai, Zhengting Huang, Zijian Li, Jie Zheng, Min Wu","title":"Interpretable High-order Knowledge Graph Neural Network for Predicting\n  Synthetic Lethality in Human Cancers","comments":"15 pages. Accepted by Briefings in Bioinformatics","journal-ref":"Briefings in Bioinformatics 2025","doi":null,"report-no":null,"categories":"cs.LG q-bio.QM","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Synthetic lethality (SL) is a promising gene interaction for cancer therapy.\nRecent SL prediction methods integrate knowledge graphs (KGs) into graph neural\nnetworks (GNNs) and employ attention mechanisms to extract local subgraphs as\nexplanations for target gene pairs. However, attention mechanisms often lack\nfidelity, typically generate a single explanation per gene pair, and fail to\nensure trustworthy high-order structures in their explanations. To overcome\nthese limitations, we propose Diverse Graph Information Bottleneck for\nSynthetic Lethality (DGIB4SL), a KG-based GNN that generates multiple faithful\nexplanations for the same gene pair and effectively encodes high-order\nstructures. Specifically, we introduce a novel DGIB objective, integrating a\nDeterminant Point Process (DPP) constraint into the standard IB objective, and\nemploy 13 motif-based adjacency matrices to capture high-order structures in\ngene representations. Experimental results show that DGIB4SL outperforms\nstate-of-the-art baselines and provides multiple explanations for SL\nprediction, revealing diverse biological mechanisms underlying SL inference.\n","versions":"[{'version': 'v1', 'created': 'Sat, 8 Mar 2025 04:37:28 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 04:23:11 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Chen', 'Xuexin', ''], ['Cai', 'Ruichu', ''], ['Huang', 'Zhengting', ''], ['Li', 'Zijian', ''], ['Zheng', 'Jie', ''], ['Wu', 'Min', '']]","extracted_entities":"[{'text': 'attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'attention mechanisms', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanisms","similarity_score":0.9558142424}
{"id":2503.06473,"submitter":"Hanze Li","authors":"Hanze Li, Xiande Huang","title":"Enhancing Layer Attention Efficiency through Pruning Redundant\n  Retrievals","comments":"11 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Growing evidence suggests that layer attention mechanisms, which enhance\ninteraction among layers in deep neural networks, have significantly advanced\nnetwork architectures. However, existing layer attention methods suffer from\nredundancy, as attention weights learned by adjacent layers often become highly\nsimilar. This redundancy causes multiple layers to extract nearly identical\nfeatures, reducing the model's representational capacity and increasing\ntraining time. To address this issue, we propose a novel approach to quantify\nredundancy by leveraging the Kullback-Leibler (KL) divergence between adjacent\nlayers. Additionally, we introduce an Enhanced Beta Quantile Mapping (EBQM)\nmethod that accurately identifies and skips redundant layers, thereby\nmaintaining model stability. Our proposed Efficient Layer Attention (ELA)\narchitecture, improves both training efficiency and overall performance,\nachieving a 30\\% reduction in training time while enhancing performance in\ntasks such as image classification and object detection.\n","versions":"[{'version': 'v1', 'created': 'Sun, 9 Mar 2025 06:20:11 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 06:21:10 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Li', 'Hanze', ''], ['Huang', 'Xiande', '']]","extracted_entities":"[{'text': 'layer attention mechanisms', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"layer attention mechanisms","similarity_score":0.8598178029}
{"id":2503.08121,"submitter":"Thanh Nhat Huy Nguyen","authors":"Huy Nguyen, Kien Nguyen, Akila Pemasiri, Feng Liu, Sridha Sridharan,\n  Clinton Fookes","title":"AG-VPReID: A Challenging Large-Scale Benchmark for Aerial-Ground\n  Video-based Person Re-Identification","comments":"Accepted at Computer Vision and Pattern Recognition Conference (CVPR)\n  2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We introduce AG-VPReID, a new large-scale dataset for aerial-ground\nvideo-based person re-identification (ReID) that comprises 6,632 subjects,\n32,321 tracklets and over 9.6 million frames captured by drones (altitudes\nranging from 15-120m), CCTV, and wearable cameras. This dataset offers a\nreal-world benchmark for evaluating the robustness to significant viewpoint\nchanges, scale variations, and resolution differences in cross-platform\naerial-ground settings. In addition, to address these challenges, we propose\nAG-VPReID-Net, an end-to-end framework composed of three complementary streams:\n(1) an Adapted Temporal-Spatial Stream addressing motion pattern\ninconsistencies and facilitating temporal feature learning, (2) a Normalized\nAppearance Stream leveraging physics-informed techniques to tackle resolution\nand appearance changes, and (3) a Multi-Scale Attention Stream handling scale\nvariations across drone altitudes. We integrate visual-semantic cues from all\nstreams to form a robust, viewpoint-invariant whole-body representation.\nExtensive experiments demonstrate that AG-VPReID-Net outperforms\nstate-of-the-art approaches on both our new dataset and existing video-based\nReID benchmarks, showcasing its effectiveness and generalizability.\nNevertheless, the performance gap observed on AG-VPReID across all methods\nunderscores the dataset's challenging nature. The dataset, code and trained\nmodels are available at https:\/\/github.com\/agvpreid25\/AG-VPReID-Net.\n","versions":"[{'version': 'v1', 'created': 'Tue, 11 Mar 2025 07:38:01 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 01:07:25 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Nguyen', 'Huy', ''], ['Nguyen', 'Kien', ''], ['Pemasiri', 'Akila', ''], ['Liu', 'Feng', ''], ['Sridharan', 'Sridha', ''], ['Fookes', 'Clinton', '']]","extracted_entities":"[{'text': 'temporal feature learning', 'label': 'Few-shot Learning'}, {'text': 'Multi-Scale Attention Stream', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Multi-Scale Attention Stream","similarity_score":0.5518479347}
{"id":2503.08992,"submitter":"Xuzhong Hu","authors":"Xuzhong Hu, Zaipeng Duan, Pei An, Jun zhang, and Jie Ma","title":"Dual-Domain Homogeneous Fusion with Cross-Modal Mamba and Progressive\n  Decoder for 3D Object Detection","comments":"13 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Fusing LiDAR and image features in a homogeneous BEV domain has become\npopular for 3D object detection in autonomous driving. However, this paradigm\nis constrained by the excessive feature compression. While some works explore\ndense voxel fusion to enable better feature interaction, they face high\ncomputational costs and challenges in query generation. Additionally, feature\nmisalignment in both domains results in suboptimal detection accuracy. To\naddress these limitations, we propose a Dual-Domain Homogeneous Fusion network\n(DDHFusion), which leverages the complementarily of both BEV and voxel domains\nwhile mitigating their drawbacks. Specifically, we first transform image\nfeatures into BEV and sparse voxel representations using lift-splat-shot and\nour proposed Semantic-Aware Feature Sampling (SAFS) module. The latter\nsignificantly reduces computational overhead by discarding unimportant voxels.\nNext, we introduce Homogeneous Voxel and BEV Fusion (HVF and HBF) networks for\nmulti-modal fusion within respective domains. They are equipped with novel\ncross-modal Mamba blocks to resolve feature misalignment and enable\ncomprehensive scene perception. The output voxel features are injected into the\nBEV space to compensate for the information loss brought by direct height\ncompression. During query selection, the Progressive Query Generation (PQG)\nmechanism is implemented in the BEV domain to reduce false negatives caused by\nfeature compression. Furthermore, we propose a Progressive Decoder (QD) that\nsequentially aggregates not only context-rich BEV features but also\ngeometry-aware voxel features with deformable attention and the Multi-Modal\nVoxel Feature Mixing (MMVFM) block for precise classification and box\nregression.\n","versions":"[{'version': 'v1', 'created': 'Wed, 12 Mar 2025 01:55:02 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Mar 2025 15:33:08 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Hu', 'Xuzhong', ''], ['Duan', 'Zaipeng', ''], ['An', 'Pei', ''], ['zhang', 'Jun', ''], ['Ma', 'Jie', '']]","extracted_entities":"[{'text': 'deformable attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"deformable attention","similarity_score":0.7499341369}
{"id":2503.11737,"submitter":"Jiseong Park","authors":"Jiseong Park, Hanjin Kim, Seojin Kim, Jueun Choi","title":"Multi-View Node Pruning for Accurate Graph Representation","comments":"Jiseong Park and Hanjin Kim are co-first author for this work","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Graph pooling, which compresses a whole graph into a smaller coarsened graph,\nis an essential component of graph representation learning. To efficiently\ncompress a given graph, graph pooling methods often drop their nodes with\nattention-based scoring with the task loss. However, this often results in\nsimply removing nodes with lower degrees without consideration of their\nfeature-level relevance to the given task. To fix this problem, we propose a\nMulti-View Pruning(MVP), a graph pruning method based on a multi-view framework\nand reconstruction loss. Given a graph, MVP first constructs multiple graphs\nfor different views either by utilizing the predefined modalities or by\nrandomly partitioning the input features, to consider the importance of each\nnode in diverse perspectives. Then, it learns the score for each node by\nconsidering both the reconstruction and the task loss. MVP can be incorporated\nwith any hierarchical pooling framework to score the nodes. We validate MVP on\nmultiple benchmark datasets by coupling it with two graph pooling methods, and\nshow that it significantly improves the performance of the base graph pooling\nmethod, outperforming all baselines. Further analysis shows that both the\nencoding of multiple views and the consideration of reconstruction loss are the\nkey to the success of MVP, and that it indeed identifies nodes that are less\nimportant according to domain knowledge.\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 14:44:54 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 14:34:49 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Park', 'Jiseong', ''], ['Kim', 'Hanjin', ''], ['Kim', 'Seojin', ''], ['Choi', 'Jueun', '']]","extracted_entities":"[{'text': 'attention-based scoring', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention-based scoring","similarity_score":0.5608227253}
{"id":2503.11851,"submitter":"Jutika Borah M.Sc.","authors":"Jutika Borah and Hidam Kumarjit Singh","title":"DCAT: Dual Cross-Attention Fusion for Disease Classification in\n  Radiological Images with Uncertainty Estimation","comments":"18 pages, 8 figures, 5 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.AI cs.CV cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Accurate and reliable image classification is crucial in radiology, where\ndiagnostic decisions significantly impact patient outcomes. Conventional deep\nlearning models tend to produce overconfident predictions despite underlying\nuncertainties, potentially leading to misdiagnoses. Attention mechanisms have\nemerged as powerful tools in deep learning, enabling models to focus on\nrelevant parts of the input data. Combined with feature fusion, they can be\neffective in addressing uncertainty challenges. Cross-attention has become\nincreasingly important in medical image analysis for capturing dependencies\nacross features and modalities. This paper proposes a novel dual\ncross-attention fusion model for medical image analysis by addressing key\nchallenges in feature integration and interpretability. Our approach introduces\na bidirectional cross-attention mechanism with refined channel and spatial\nattention that dynamically fuses feature maps from EfficientNetB4 and ResNet34\nleveraging multi-network contextual dependencies. The refined features through\nchannel and spatial attention highlights discriminative patterns crucial for\naccurate classification. The proposed model achieved AUC of 99.75%, 100%,\n99.93% and 98.69% and AUPR of 99.81%, 100%, 99.97%, and 96.36% on Covid-19,\nTuberculosis, Pneumonia Chest X-ray images and Retinal OCT images respectively.\nThe entropy values and several high uncertain samples give an interpretable\nvisualization from the model enhancing transparency. By combining multi-scale\nfeature extraction, bidirectional attention and uncertainty estimation, our\nproposed model strongly impacts medical image analysis.\n","versions":"[{'version': 'v1', 'created': 'Fri, 14 Mar 2025 20:28:20 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 12:18:48 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Borah', 'Jutika', ''], ['Singh', 'Hidam Kumarjit', '']]","extracted_entities":"[{'text': 'Attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'Cross-attention', 'label': 'Attention mechanism'}, {'text': 'bidirectional cross-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'spatial\\nattention', 'label': 'Attention mechanism'}, {'text': 'multi-network contextual dependencies', 'label': 'contextual Embedding'}, {'text': 'channel and spatial attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Attention mechanisms","similarity_score":0.9558142424}
{"id":2503.11935,"submitter":"Yang Zheng","authors":"Jun Yu and Yang Zheng and Lei Wang and Yongqi Wang and Shengfan Xu","title":"Design of an Expression Recognition Solution Employing the Global\n  Channel-Spatial Attention Mechanism","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Facial expression recognition is a challenging classification task with broad\napplication prospects in the field of human - computer interaction. This paper\naims to introduce the methods of our upcoming 8th Affective Behavior Analysis\nin the Wild (ABAW) competition to be held at CVPR2025. To address issues such\nas low recognition accuracy caused by subtle expression changes and multi -\nscales in facial expression recognition in videos, we propose global channel -\nspatial attention and median - enhanced spatial - channel attention to\nstrengthen feature processing for speech and images respectively. Secondly, to\nfully utilize the complementarity between the speech and facial expression\nmodalities, a speech - and - facial - expression key - frame alignment\ntechnique is adopted to calculate the weights of speech and facial expressions.\nThese weights are input into the feature fusion layer for multi - scale dilated\nfusion, which effectively improves the recognition rate of facial expression\nrecognition. In the facial expression recognition task of the 6th ABAW\ncompetition, our method achieved excellent results on the official validation\nset, which fully demonstrates the effectiveness and competitiveness of the\nproposed method.\n","versions":"[{'version': 'v1', 'created': 'Sat, 15 Mar 2025 00:59:34 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Mar 2025 05:50:24 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Yu', 'Jun', ''], ['Zheng', 'Yang', ''], ['Wang', 'Lei', ''], ['Wang', 'Yongqi', ''], ['Xu', 'Shengfan', '']]","extracted_entities":"[{'text': 'global channel -\\nspatial attention', 'label': 'Attention mechanism'}, {'text': 'median - enhanced spatial - channel attention', 'label': 'Attention mechanism'}, {'text': 'feature fusion layer', 'label': 'Embedding'}, {'text': 'multi - scale dilated\\nfusion', 'label': 'Embedding'}]","assigned_concept":"Attention mechanism","matched_keyword":"global channel -\nspatial attention","similarity_score":0.6017642021}
{"id":2503.12461,"submitter":"Fanhu Zeng","authors":"Fanhu Zeng, Hao Tang, Yihua Shao, Siyu Chen, Ling Shao, Yan Wang","title":"MambaIC: State Space Models for High-Performance Learned Image\n  Compression","comments":"Accepted to CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV eess.IV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  A high-performance image compression algorithm is crucial for real-time\ninformation transmission across numerous fields. Despite rapid progress in\nimage compression, computational inefficiency and poor redundancy modeling\nstill pose significant bottlenecks, limiting practical applications. Inspired\nby the effectiveness of state space models (SSMs) in capturing long-range\ndependencies, we leverage SSMs to address computational inefficiency in\nexisting methods and improve image compression from multiple perspectives. In\nthis paper, we integrate the advantages of SSMs for better\nefficiency-performance trade-off and propose an enhanced image compression\napproach through refined context modeling, which we term MambaIC. Specifically,\nwe explore context modeling to adaptively refine the representation of hidden\nstates. Additionally, we introduce window-based local attention into\nchannel-spatial entropy modeling to reduce potential spatial redundancy during\ncompression, thereby increasing efficiency. Comprehensive qualitative and\nquantitative results validate the effectiveness and efficiency of our approach,\nparticularly for high-resolution image compression. Code is released at\nhttps:\/\/github.com\/AuroraZengfh\/MambaIC.\n","versions":"[{'version': 'v1', 'created': 'Sun, 16 Mar 2025 11:32:34 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 02:27:03 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zeng', 'Fanhu', ''], ['Tang', 'Hao', ''], ['Shao', 'Yihua', ''], ['Chen', 'Siyu', ''], ['Shao', 'Ling', ''], ['Wang', 'Yan', '']]","extracted_entities":"[{'text': 'context modeling', 'label': 'contextual Embedding'}, {'text': 'context modeling', 'label': 'contextual Embedding'}, {'text': 'window-based local attention', 'label': 'Attention mechanism'}, {'text': 'channel-spatial entropy modeling', 'label': 'contextual Embedding'}]","assigned_concept":"Attention mechanism","matched_keyword":"window-based local attention","similarity_score":0.6087658405}
{"id":2503.12734,"submitter":"Jianliang He","authors":"Jianliang He, Xintian Pan, Siyu Chen, Zhuoran Yang","title":"In-Context Linear Regression Demystified: Training Dynamics and\n  Mechanistic Interpretability of Multi-Head Softmax Attention","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ML","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We study how multi-head softmax attention models are trained to perform\nin-context learning on linear data. Through extensive empirical experiments and\nrigorous theoretical analysis, we demystify the emergence of elegant attention\npatterns: a diagonal and homogeneous pattern in the key-query (KQ) weights, and\na last-entry-only and zero-sum pattern in the output-value (OV) weights.\nRemarkably, these patterns consistently appear from gradient-based training\nstarting from random initialization. Our analysis reveals that such emergent\nstructures enable multi-head attention to approximately implement a debiased\ngradient descent predictor -- one that outperforms single-head attention and\nnearly achieves Bayesian optimality up to proportional factor. Furthermore,\ncompared to linear transformers, the softmax attention readily generalizes to\nsequences longer than those seen during training. We also extend our study to\nscenarios with non-isotropic covariates and multi-task linear regression. In\nthe former, multi-head attention learns to implement a form of pre-conditioned\ngradient descent. In the latter, we uncover an intriguing regime where the\ninterplay between head number and task number triggers a superposition\nphenomenon that efficiently resolves multi-task in-context learning. Our\nresults reveal that in-context learning ability emerges from the trained\ntransformer as an aggregated effect of its architecture and the underlying data\ndistribution, paving the way for deeper understanding and broader applications\nof in-context learning.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 02:00:49 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['He', 'Jianliang', ''], ['Pan', 'Xintian', ''], ['Chen', 'Siyu', ''], ['Yang', 'Zhuoran', '']]","extracted_entities":"[{'text': 'softmax attention', 'label': 'Attention mechanism'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}, {'text': 'multi-head attention', 'label': 'Attention mechanism'}, {'text': 'single-head attention', 'label': 'Attention mechanism'}, {'text': 'linear transformers', 'label': 'Transformers'}, {'text': 'softmax attention', 'label': 'Attention mechanism'}, {'text': 'multi-head attention', 'label': 'Attention mechanism'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}, {'text': 'in-context learning', 'label': 'contextual Embedding'}]","assigned_concept":"Attention mechanism","matched_keyword":"single-head attention","similarity_score":0.71798563}
{"id":2503.12838,"submitter":"Guanbin Li","authors":"Junjia Huang, Pengxiang Yan, Jinhang Cai, Jiyang Liu, Zhao Wang,\n  Yitong Wang, Xinglong Wu, Guanbin Li","title":"DreamLayer: Simultaneous Multi-Layer Generation via Diffusion Mode","comments":"Under submission","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Text-driven image generation using diffusion models has recently gained\nsignificant attention. To enable more flexible image manipulation and editing,\nrecent research has expanded from single image generation to transparent layer\ngeneration and multi-layer compositions. However, existing approaches often\nfail to provide a thorough exploration of multi-layer structures, leading to\ninconsistent inter-layer interactions, such as occlusion relationships, spatial\nlayout, and shadowing. In this paper, we introduce DreamLayer, a novel\nframework that enables coherent text-driven generation of multiple image\nlayers, by explicitly modeling the relationship between transparent foreground\nand background layers. DreamLayer incorporates three key components, i.e.,\nContext-Aware Cross-Attention (CACA) for global-local information exchange,\nLayer-Shared Self-Attention (LSSA) for establishing robust inter-layer\nconnections, and Information Retained Harmonization (IRH) for refining fusion\ndetails at the latent level. By leveraging a coherent full-image context,\nDreamLayer builds inter-layer connections through attention mechanisms and\napplies a harmonization step to achieve seamless layer fusion. To facilitate\nresearch in multi-layer generation, we construct a high-quality, diverse\nmulti-layer dataset including 400k samples. Extensive experiments and user\nstudies demonstrate that DreamLayer generates more coherent and well-aligned\nlayers, with broad applicability, including latent-space image editing and\nimage-to-layer decomposition.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 05:34:11 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Huang', 'Junjia', ''], ['Yan', 'Pengxiang', ''], ['Cai', 'Jinhang', ''], ['Liu', 'Jiyang', ''], ['Wang', 'Zhao', ''], ['Wang', 'Yitong', ''], ['Wu', 'Xinglong', ''], ['Li', 'Guanbin', '']]","extracted_entities":"[{'text': 'Context-Aware Cross-Attention (CACA)', 'label': 'Attention mechanism'}, {'text': 'Layer-Shared Self-Attention (LSSA)', 'label': 'Attention mechanism'}, {'text': 'attention mechanisms', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanisms","similarity_score":0.9558142424}
{"id":2503.12847,"submitter":"Chen Liu","authors":"Chen Liu, Peike Li, Liying Yang, Dadong Wang, Lincheng Li, Xin Yu","title":"Robust Audio-Visual Segmentation via Audio-Guided Visual Convergent\n  Alignment","comments":"Accepted by CVPR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Accurately localizing audible objects based on audio-visual cues is the core\nobjective of audio-visual segmentation. Most previous methods emphasize spatial\nor temporal multi-modal modeling, yet overlook challenges from ambiguous\naudio-visual correspondences such as nearby visually similar but acoustically\ndifferent objects and frequent shifts in objects' sounding status.\nConsequently, they may struggle to reliably correlate audio and visual cues,\nleading to over- or under-segmentation. To address these limitations, we\npropose a novel framework with two primary components: an audio-guided modality\nalignment (AMA) module and an uncertainty estimation (UE) module. Instead of\nindiscriminately correlating audio-visual cues through a global attention\nmechanism, AMA performs audio-visual interactions within multiple groups and\nconsolidates group features into compact representations based on their\nresponsiveness to audio cues, effectively directing the model's attention to\naudio-relevant areas. Leveraging contrastive learning, AMA further\ndistinguishes sounding regions from silent areas by treating features with\nstrong audio responses as positive samples and weaker responses as negatives.\nAdditionally, UE integrates spatial and temporal information to identify\nhigh-uncertainty regions caused by frequent changes in sound state, reducing\nprediction errors by lowering confidence in these areas. Experimental results\ndemonstrate that our approach achieves superior accuracy compared to existing\nstate-of-the-art methods, particularly in challenging scenarios where\ntraditional approaches struggle to maintain reliable segmentation.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 05:48:22 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Liu', 'Chen', ''], ['Li', 'Peike', ''], ['Yang', 'Liying', ''], ['Wang', 'Dadong', ''], ['Li', 'Lincheng', ''], ['Yu', 'Xin', '']]","extracted_entities":"[{'text': 'global attention\\nmechanism', 'label': 'Attention mechanism'}, {'text': 'contrastive learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Attention mechanism","matched_keyword":"global attention\nmechanism","similarity_score":0.8699287176}
{"id":2503.12853,"submitter":"Jiacheng Hu","authors":"Yanlin Xiang, Qingyuan He, Ting Xu, Ran Hao, Jiacheng Hu, Hanchao\n  Zhang","title":"Adaptive Transformer Attention and Multi-Scale Fusion for Spine 3D\n  Segmentation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This study proposes a 3D semantic segmentation method for the spine based on\nthe improved SwinUNETR to improve segmentation accuracy and robustness. Aiming\nat the complex anatomical structure of spinal images, this paper introduces a\nmulti-scale fusion mechanism to enhance the feature extraction capability by\nusing information of different scales, thereby improving the recognition\naccuracy of the model for the target area. In addition, the introduction of the\nadaptive attention mechanism enables the model to dynamically adjust the\nattention to the key area, thereby optimizing the boundary segmentation effect.\nThe experimental results show that compared with 3D CNN, 3D U-Net, and 3D U-Net\n+ Transformer, the model of this study has achieved significant improvements in\nmIoU, mDice, and mAcc indicators, and has better segmentation performance. The\nablation experiment further verifies the effectiveness of the proposed improved\nmethod, proving that multi-scale fusion and adaptive attention mechanism have a\npositive effect on the segmentation task. Through the visualization analysis of\nthe inference results, the model can better restore the real anatomical\nstructure of the spinal image. Future research can further optimize the\nTransformer structure and expand the data scale to improve the generalization\nability of the model. This study provides an efficient solution for the task of\nmedical image segmentation, which is of great significance to intelligent\nmedical image analysis.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 06:27:43 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Xiang', 'Yanlin', ''], ['He', 'Qingyuan', ''], ['Xu', 'Ting', ''], ['Hao', 'Ran', ''], ['Hu', 'Jiacheng', ''], ['Zhang', 'Hanchao', '']]","extracted_entities":"[{'text': 'multi-scale fusion mechanism', 'label': 'Attention mechanism'}, {'text': 'adaptive attention mechanism', 'label': 'Attention mechanism'}, {'text': 'adaptive attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"adaptive attention mechanism","similarity_score":0.8649787903}
{"id":2503.12885,"submitter":"Dewei Zhou","authors":"Dewei Zhou, Mingwei Li, Zongxin Yang, Yi Yang","title":"DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale\n  Text-to-Image Models","comments":"11 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Image-conditioned generation methods, such as depth- and canny-conditioned\napproaches, have demonstrated remarkable abilities for precise image synthesis.\nHowever, existing models still struggle to accurately control the content of\nmultiple instances (or regions). Even state-of-the-art models like FLUX and\n3DIS face challenges, such as attribute leakage between instances, which limits\nuser control. To address these issues, we introduce DreamRenderer, a\ntraining-free approach built upon the FLUX model. DreamRenderer enables users\nto control the content of each instance via bounding boxes or masks, while\nensuring overall visual harmony. We propose two key innovations: 1) Bridge\nImage Tokens for Hard Text Attribute Binding, which uses replicated image\ntokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely\non text data, bind the correct visual attributes for each instance during Joint\nAttention; 2) Hard Image Attribute Binding applied only to vital layers.\nThrough our analysis of FLUX, we identify the critical layers responsible for\ninstance attribute rendering and apply Hard Image Attribute Binding only in\nthese layers, using soft binding in the others. This approach ensures precise\ncontrol while preserving image quality. Evaluations on the COCO-POS and\nCOCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success\nRatio by 17.7% over FLUX and enhances the performance of layout-to-image models\nlike GLIGEN and 3DIS by up to 26.8%. Project Page:\nhttps:\/\/limuloo.github.io\/DreamRenderer\/.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 07:30:16 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Zhou', 'Dewei', ''], ['Li', 'Mingwei', ''], ['Yang', 'Zongxin', ''], ['Yang', 'Yi', '']]","extracted_entities":"[{'text': 'T5 text embeddings', 'label': 'Embedding'}, {'text': 'Joint\\nAttention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Joint\nAttention","similarity_score":0.6880277395}
{"id":2503.12919,"submitter":"Aref Einizade","authors":"Aref Einizade, Dorina Thanou, Fragkiskos D. Malliaros, Jhony H.\n  Giraldo","title":"COSMOS: Continuous Simplicial Neural Networks","comments":"17 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Simplicial complexes provide a powerful framework for modeling high-order\ninteractions in structured data, making them particularly suitable for\napplications such as trajectory prediction and mesh processing. However,\nexisting simplicial neural networks (SNNs), whether convolutional or\nattention-based, rely primarily on discrete filtering techniques, which can be\nrestrictive. In contrast, partial differential equations (PDEs) on simplicial\ncomplexes offer a principled approach to capture continuous dynamics in such\nstructures. In this work, we introduce COntinuous SiMplicial neural netwOrkS\n(COSMOS), a novel SNN architecture derived from PDEs on simplicial complexes.\nWe provide theoretical and experimental justifications of COSMOS's stability\nunder simplicial perturbations. Furthermore, we investigate the over-smoothing\nphenomenon, a common issue in geometric deep learning, demonstrating that\nCOSMOS offers better control over this effect than discrete SNNs. Our\nexperiments on real-world datasets of ocean trajectory prediction and\nregression on partial deformable shapes demonstrate that COSMOS achieves\ncompetitive performance compared to state-of-the-art SNNs in complex and noisy\nenvironments.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 08:31:25 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Einizade', 'Aref', ''], ['Thanou', 'Dorina', ''], ['Malliaros', 'Fragkiskos D.', ''], ['Giraldo', 'Jhony H.', '']]","extracted_entities":"[{'text': 'attention-based', 'label': 'Attention mechanism'}, {'text': 'SNNs', 'label': 'Neural Language Model'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention-based","similarity_score":0.7038730383}
{"id":2503.12963,"submitter":"Chaolong Yang","authors":"Chaolong Yang, Kai Yao, Yuyao Yan, Chenru Jiang, Weiguang Zhao, Jie\n  Sun, Guangliang Cheng, Yifei Zhang, Bin Dong, Kaizhu Huang","title":"Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based\n  Spatiotemporal Diffusion for Audio-driven Talking Portrait","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Audio-driven single-image talking portrait generation plays a crucial role in\nvirtual reality, digital human creation, and filmmaking. Existing approaches\nare generally categorized into keypoint-based and image-based methods.\nKeypoint-based methods effectively preserve character identity but struggle to\ncapture fine facial details due to the fixed points limitation of the 3D\nMorphable Model. Moreover, traditional generative networks face challenges in\nestablishing causality between audio and keypoints on limited datasets,\nresulting in low pose diversity. In contrast, image-based approaches produce\nhigh-quality portraits with diverse details using the diffusion network but\nincur identity distortion and expensive computational costs. In this work, we\npropose KDTalker, the first framework to combine unsupervised implicit 3D\nkeypoint with a spatiotemporal diffusion model. Leveraging unsupervised\nimplicit 3D keypoints, KDTalker adapts facial information densities, allowing\nthe diffusion process to model diverse head poses and capture fine facial\ndetails flexibly. The custom-designed spatiotemporal attention mechanism\nensures accurate lip synchronization, producing temporally consistent,\nhigh-quality animations while enhancing computational efficiency. Experimental\nresults demonstrate that KDTalker achieves state-of-the-art performance\nregarding lip synchronization accuracy, head pose diversity, and execution\nefficiency.Our codes are available at https:\/\/github.com\/chaolongy\/KDTalker.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 09:18:31 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Yang', 'Chaolong', ''], ['Yao', 'Kai', ''], ['Yan', 'Yuyao', ''], ['Jiang', 'Chenru', ''], ['Zhao', 'Weiguang', ''], ['Sun', 'Jie', ''], ['Cheng', 'Guangliang', ''], ['Zhang', 'Yifei', ''], ['Dong', 'Bin', ''], ['Huang', 'Kaizhu', '']]","extracted_entities":"[{'text': 'spatiotemporal attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"spatiotemporal attention mechanism","similarity_score":0.8278745413}
{"id":2503.12992,"submitter":"Michael Pichat","authors":"Michael Pichat, William Pogrund, Paloma Pichat, Armanouche Gasparian,\n  Samuel Demarchi, Corbet Alois Georgeon, Michael Veillet-Guillem","title":"Intra-neuronal attention within language models Relationships between\n  activation and semantics","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL q-bio.NC","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  This study investigates the ability of perceptron-type neurons in language\nmodels to perform intra-neuronal attention; that is, to identify different\nhomogeneous categorical segments within the synthetic thought category they\nencode, based on a segmentation of specific activation zones for the tokens to\nwhich they are particularly responsive. The objective of this work is therefore\nto determine to what extent formal neurons can establish a homomorphic\nrelationship between activation-based and categorical segmentations. The\nresults suggest the existence of such a relationship, albeit tenuous, only at\nthe level of tokens with very high activation levels. This intra-neuronal\nattention subsequently enables categorical restructuring processes at the level\nof neurons in the following layer, thereby contributing to the progressive\nformation of high-level categorical abstractions.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 09:47:11 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Pichat', 'Michael', ''], ['Pogrund', 'William', ''], ['Pichat', 'Paloma', ''], ['Gasparian', 'Armanouche', ''], ['Demarchi', 'Samuel', ''], ['Georgeon', 'Corbet Alois', ''], ['Veillet-Guillem', 'Michael', '']]","extracted_entities":"[{'text': 'intra-neuronal attention', 'label': 'Attention mechanism'}, {'text': 'intra-neuronal\\nattention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"intra-neuronal attention","similarity_score":0.7111603618}
{"id":2503.13119,"submitter":"Paul Wawerek-L\\'opez","authors":"Paul Wawerek-L\\'opez, Navid Mahmoudian Bidgoli, Pascal Frossard,\n  Andr\\'e Kaup, Thomas Maugey","title":"OSLO-IC: On-the-Sphere Learned Omnidirectional Image Compression with\n  Attention Modules and Spatial Context","comments":"5 pages, 5 figures, accepted for IEEE International Conference on\n  Acoustics, Speech and Signal Processing 2025 (IEEE ICASSP 2025)","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Developing effective 360-degree (spherical) image compression techniques is\ncrucial for technologies like virtual reality and automated driving. This paper\nadvances the state-of-the-art in on-the-sphere learning (OSLO) for\nomnidirectional image compression framework by proposing spherical attention\nmodules, residual blocks, and a spatial autoregressive context model. These\nimprovements achieve a 23.1% bit rate reduction in terms of WS-PSNR BD rate.\nAdditionally, we introduce a spherical transposed convolution operator for\nupsampling, which reduces trainable parameters by a factor of four compared to\nthe pixel shuffling used in the OSLO framework, while maintaining similar\ncompression performance. Therefore, in total, our proposed method offers\nsignificant rate savings with a smaller architecture and can be applied to any\nspherical convolutional application.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 12:46:14 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wawerek-L\u00f3pez', 'Paul', ''], ['Bidgoli', 'Navid Mahmoudian', ''], ['Frossard', 'Pascal', ''], ['Kaup', 'Andr\u00e9', ''], ['Maugey', 'Thomas', '']]","extracted_entities":"[{'text': 'on-the-sphere learning', 'label': 'Few-shot Learning'}, {'text': 'spherical attention\\nmodules', 'label': 'Attention mechanism'}, {'text': 'residual blocks', 'label': 'contextual Embedding'}]","assigned_concept":"Attention mechanism","matched_keyword":"spherical attention\nmodules","similarity_score":0.5317863226}
{"id":2503.13179,"submitter":"Yi Zhang","authors":"Yi Zhang, Wenye Zhou, Ruonan Lin","title":"A super-resolution reconstruction method for lightweight building images\n  based on an expanding feature modulation network","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This study proposes a lightweight method for building image super-resolution\nusing a Dilated Contextual Feature Modulation Network (DCFMN). The process\nincludes obtaining high-resolution images, down-sampling them to\nlow-resolution, enhancing the low-resolution images, constructing and training\na lightweight network model, and generating super-resolution outputs. To\naddress challenges such as regular textures and long-range dependencies in\nbuilding images, the DCFMN integrates an expansion separable modulation unit\nand a local feature enhancement module. The former employs multiple expansion\nconvolutions equivalent to a large kernel to efficiently aggregate multi-scale\nfeatures while leveraging a simple attention mechanism for adaptivity. The\nlatter encodes local features, mixes channel information, and ensures no\nadditional computational burden during inference through reparameterization.\nThis approach effectively resolves the limitations of existing lightweight\nsuper-resolution networks in modeling long-range dependencies, achieving\naccurate and efficient global feature modeling without increasing computational\ncosts, and significantly improving both reconstruction quality and lightweight\nefficiency for building image super-resolution models.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 13:54:26 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Zhang', 'Yi', ''], ['Zhou', 'Wenye', ''], ['Lin', 'Ruonan', '']]","extracted_entities":"[{'text': 'simple attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"simple attention mechanism","similarity_score":0.9481173158}
{"id":2503.13222,"submitter":"Chi Han","authors":"Chi Han","title":"Can Language Models Follow Multiple Turns of Entangled Instructions?","comments":"8 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Despite significant achievements in improving the instruction-following\ncapabilities of large language models (LLMs), the ability to process multiple\npotentially entangled or conflicting instructions remains a considerable\nchallenge. Real-world scenarios often require consistency across multiple\ninstructions over time, such as secret privacy, personal preferences, and\nprioritization, which demand sophisticated abilities to integrate multiple\nturns and carefully balance competing objectives when instructions intersect or\nconflict. This work presents a systematic investigation of LLMs' capabilities\nin handling multiple turns of instructions, covering three levels of\ndifficulty: (1) retrieving information from instructions, (2) tracking and\nreasoning across turns, and (3) resolving conflicts among instructions. We\nconstruct MultiTurnInstruct with around 1.1K high-quality multi-turn\nconversations through the human-in-the-loop approach and result in nine\ncapability categories, including statics and dynamics, reasoning, and\nmultitasking. Our finding reveals an intriguing trade-off between different\ncapabilities. While GPT models demonstrate superior memorization, they show\nreduced effectiveness in privacy-protection tasks requiring selective\ninformation withholding. Larger models exhibit stronger reasoning capabilities\nbut still struggle with resolving conflicting instructions. Importantly, these\nperformance gaps cannot be attributed solely to information loss, as models\ndemonstrate strong BLEU scores on memorization tasks but their attention\nmechanisms fail to integrate multiple related instructions effectively. These\nfindings highlight critical areas for improvement in complex real-world tasks\ninvolving multi-turn instructions.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 14:31:37 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Han', 'Chi', '']]","extracted_entities":"[{'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'attention\\nmechanisms', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention\nmechanisms","similarity_score":0.9558142424}
{"id":2503.13268,"submitter":"Jian Xiao","authors":"Jian Xiao, Ji Wang, and Yuanwei Liu","title":"Channel Estimation for Pinching-Antenna Systems (PASS)","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IT eess.SP math.IT","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Pinching Antennas (PAs) represent a revolutionary flexible antenna technology\nthat leverages dielectric waveguides and electromagnetic coupling to mitigate\nlarge-scale path loss. This letter is the first to explore channel estimation\nfor Pinching-Antenna SyStems (PASS), addressing their uniquely ill-conditioned\nand underdetermined channel characteristics. In particular, two efficient deep\nlearning-based channel estimators are proposed. 1) PAMoE: This estimator\nincorporates dynamic padding, feature embedding, fusion, and mixture of experts\n(MoE) modules, which effectively leverage the positional information of PAs and\nexploit expert diversity. 2) PAformer: This Transformer-style estimator employs\nthe self-attention mechanism to predict channel coefficients in a per-antenna\nmanner, which offers more flexibility to adaptively deal with dynamic numbers\nof PAs in practical deployment. Numerical results demonstrate that 1) the\nproposed deep learning-based channel estimators outperform conventional methods\nand exhibit excellent zero-shot learning capabilities, and 2) PAMoE delivers\nhigher channel estimation accuracy via MoE specialization, while PAformer\nnatively handles an arbitrary number of PAs, trading self-attention complexity\nfor superior scalability.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:20:49 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Xiao', 'Jian', ''], ['Wang', 'Ji', ''], ['Liu', 'Yuanwei', '']]","extracted_entities":"[{'text': 'PAMoE', 'label': 'Transformer-based model'}, {'text': 'dynamic padding', 'label': 'Embedding'}, {'text': 'feature embedding', 'label': 'Embedding'}, {'text': 'fusion', 'label': 'Embedding'}, {'text': 'PAformer', 'label': 'Transformer-based model'}, {'text': 'self-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'zero-shot learning', 'label': 'Few-shot Learning'}, {'text': 'PAMoE', 'label': 'Transformer-based model'}, {'text': 'PAformer', 'label': 'Transformer-based model'}]","assigned_concept":"Attention mechanism","matched_keyword":"self-attention mechanism","similarity_score":0.8757837415}
{"id":2503.13304,"submitter":"Witold Wydma\\'nski","authors":"Witold Wydma\\'nski, Marek \\'Smieja","title":"GFSNetwork: Differentiable Feature Selection via Gumbel-Sigmoid\n  Relaxation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Feature selection in deep learning remains a critical challenge, particularly\nfor high-dimensional tabular data where interpretability and computational\nefficiency are paramount. We present GFSNetwork, a novel neural architecture\nthat performs differentiable feature selection through temperature-controlled\nGumbel-Sigmoid sampling. Unlike traditional methods, where the user has to\ndefine the requested number of features, GFSNetwork selects it automatically\nduring an end-to-end process. Moreover, GFSNetwork maintains constant\ncomputational overhead regardless of the number of input features. We evaluate\nGFSNetwork on a series of classification and regression benchmarks, where it\nconsistently outperforms recent methods including DeepLasso, attention maps, as\nwell as traditional feature selectors, while using significantly fewer\nfeatures. Furthermore, we validate our approach on real-world metagenomic\ndatasets, demonstrating its effectiveness in high-dimensional biological data.\nConcluding, our method provides a scalable solution that bridges the gap\nbetween neural network flexibility and traditional feature selection\ninterpretability. We share our python implementation of GFSNetwork at\nhttps:\/\/github.com\/wwydmanski\/GFSNetwork, as well as a PyPi package\n(gfs_network).\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 15:47:26 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wydma\u0144ski', 'Witold', ''], ['\u015amieja', 'Marek', '']]","extracted_entities":"[{'text': 'GFSNetwork', 'label': 'Neural Language Model'}, {'text': 'attention maps', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention maps","similarity_score":0.672863245}
{"id":2503.13439,"submitter":"Tianhao Wu","authors":"Tianhao Wu, Chuanxia Zheng, Frank Guan, Andrea Vedaldi, Tat-Jen Cham","title":"Amodal3R: Amodal 3D Reconstruction from Occluded 2D Images","comments":"Project Page: https:\/\/sm0kywu.github.io\/Amodal3R\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Most image-based 3D object reconstructors assume that objects are fully\nvisible, ignoring occlusions that commonly occur in real-world scenarios. In\nthis paper, we introduce Amodal3R, a conditional 3D generative model designed\nto reconstruct 3D objects from partial observations. We start from a\n\"foundation\" 3D generative model and extend it to recover plausible 3D geometry\nand appearance from occluded objects. We introduce a mask-weighted multi-head\ncross-attention mechanism followed by an occlusion-aware attention layer that\nexplicitly leverages occlusion priors to guide the reconstruction process. We\ndemonstrate that, by training solely on synthetic data, Amodal3R learns to\nrecover full 3D objects even in the presence of occlusions in real scenes. It\nsubstantially outperforms existing methods that independently perform 2D amodal\ncompletion followed by 3D reconstruction, thereby establishing a new benchmark\nfor occlusion-aware 3D reconstruction.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 17:59:01 GMT'}]","update_date":"2025-03-18","authors_parsed":"[['Wu', 'Tianhao', ''], ['Zheng', 'Chuanxia', ''], ['Guan', 'Frank', ''], ['Vedaldi', 'Andrea', ''], ['Cham', 'Tat-Jen', '']]","extracted_entities":"[{'text': 'mask-weighted multi-head\\ncross-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'occlusion-aware attention layer', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"mask-weighted multi-head\ncross-attention mechanism","similarity_score":0.6544810534}
{"id":2503.13617,"submitter":"Hao Li","authors":"Hao Li, Yubin Xiao, Ke Liang, Mengzhu Wang, Long Lan, Kenli Li and\n  Xinwang Liu","title":"Let Synthetic Data Shine: Domain Reassembly and Soft-Fusion for Single\n  Domain Generalization","comments":"26 pages, 10 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Single Domain Generalization (SDG) aims to train models with consistent\nperformance across diverse scenarios using data from a single source. While\nusing latent diffusion models (LDMs) show promise in augmenting limited source\ndata, we demonstrate that directly using synthetic data can be detrimental due\nto significant feature distribution discrepancies between synthetic and real\ntarget domains, leading to performance degradation. To address this issue, we\npropose Discriminative Domain Reassembly and Soft-Fusion (DRSF), a training\nframework leveraging synthetic data to improve model generalization. We employ\nLDMs to produce diverse pseudo-target domain samples and introduce two key\nmodules to handle distribution bias. First, Discriminative Feature Decoupling\nand Reassembly (DFDR) module uses entropy-guided attention to recalibrate\nchannel-level features, suppressing synthetic noise while preserving semantic\nconsistency. Second, Multi-pseudo-domain Soft Fusion (MDSF) module uses\nadversarial training with latent-space feature interpolation, creating\ncontinuous feature transitions between domains. Extensive SDG experiments on\nobject detection and semantic segmentation tasks demonstrate that DRSF achieves\nsubstantial performance gains with only marginal computational overhead.\nNotably, DRSF's plug-and-play architecture enables seamless integration with\nunsupervised domain adaptation paradigms, underscoring its broad applicability\nin addressing diverse and real-world domain challenges.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 18:08:03 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Li', 'Hao', ''], ['Xiao', 'Yubin', ''], ['Liang', 'Ke', ''], ['Wang', 'Mengzhu', ''], ['Lan', 'Long', ''], ['Li', 'Kenli', ''], ['Liu', 'Xinwang', '']]","extracted_entities":"[{'text': 'entropy-guided attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"entropy-guided attention","similarity_score":0.5984995365}
{"id":2503.13724,"submitter":"Shristi Das Biswas","authors":"Shristi Das Biswas, Efstathia Soufleri, Arani Roy, Kaushik Roy","title":"Towards Scalable Modeling of Compressed Videos for Efficient Action\n  Recognition","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Training robust deep video representations has proven to be computationally\nchallenging due to substantial decoding overheads, the enormous size of raw\nvideo streams, and their inherent high temporal redundancy. Different from\nexisting schemes, operating exclusively in the compressed video domain and\nexploiting all freely available modalities, i.e., I-frames, and P-frames\n(motion vectors and residuals) offers a compute-efficient alternative. Existing\nmethods approach this task as a naive multi-modality problem, ignoring the\ntemporal correlation and implicit sparsity across P-frames for modeling\nstronger shared representations for videos of the same action, making training\nand generalization easier. By revisiting the high-level design of dominant\nvideo understanding backbones, we increase inference speed by a factor of $56$\nwhile retaining similar performance. For this, we propose a hybrid end-to-end\nframework that factorizes learning across three key concepts to reduce\ninference cost by $330\\times$ versus prior art: First, a specially designed\ndual-encoder scheme with efficient Spiking Temporal Modulators to minimize\nlatency while retaining cross-domain feature aggregation. Second, a unified\ntransformer model to capture inter-modal dependencies using global\nself-attention to enhance I-frame -- P-frame contextual interactions. Third, a\nMulti-Modal Mixer Block to model rich representations from the joint\nspatiotemporal token embeddings. Experiments show that our method results in a\nlightweight architecture achieving state-of-the-art video recognition\nperformance on UCF-101, HMDB-51, K-400, K-600 and SS-v2 datasets with favorable\ncosts ($0.73$J\/V) and fast inference ($16$V\/s). Our observations bring new\ninsights into practical design choices for efficient next-generation\nspatiotemporal learners. Code is available.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 21:13:48 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Biswas', 'Shristi Das', ''], ['Soufleri', 'Efstathia', ''], ['Roy', 'Arani', ''], ['Roy', 'Kaushik', '']]","extracted_entities":"[{'text': 'global\\nself-attention', 'label': 'Attention mechanism'}, {'text': 'joint\\nspatiotemporal token embeddings', 'label': 'contextual Embedding'}]","assigned_concept":"Attention mechanism","matched_keyword":"global\nself-attention","similarity_score":0.6738587618}
{"id":2503.1374,"submitter":"Yuxuan Jiang","authors":"Yuxuan Jiang, Chengxi Zeng, Siyue Teng, Fan Zhang, Xiaoqing Zhu, Joel\n  Sole and David Bull","title":"C2D-ISR: Optimizing Attention-based Image Super-resolution from\n  Continuous to Discrete Scales","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  In recent years, attention mechanisms have been exploited in single image\nsuper-resolution (SISR), achieving impressive reconstruction results. However,\nthese advancements are still limited by the reliance on simple training\nstrategies and network architectures designed for discrete up-sampling scales,\nwhich hinder the model's ability to effectively capture information across\nmultiple scales. To address these limitations, we propose a novel framework,\n\\textbf{C2D-ISR}, for optimizing attention-based image super-resolution models\nfrom both performance and complexity perspectives. Our approach is based on a\ntwo-stage training methodology and a hierarchical encoding mechanism. The new\ntraining methodology involves continuous-scale training for discrete scale\nmodels, enabling the learning of inter-scale correlations and multi-scale\nfeature representation. In addition, we generalize the hierarchical encoding\nmechanism with existing attention-based network structures, which can achieve\nimproved spatial feature fusion, cross-scale information aggregation, and more\nimportantly, much faster inference. We have evaluated the C2D-ISR framework\nbased on three efficient attention-based backbones, SwinIR-L, SRFormer-L and\nMambaIRv2-L, and demonstrated significant improvements over the other existing\noptimization framework, HiT, in terms of super-resolution performance (up to\n0.2dB) and computational complexity reduction (up to 11%). The source code will\nbe made publicly available at www.github.com.\n","versions":"[{'version': 'v1', 'created': 'Mon, 17 Mar 2025 21:52:18 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Jiang', 'Yuxuan', ''], ['Zeng', 'Chengxi', ''], ['Teng', 'Siyue', ''], ['Zhang', 'Fan', ''], ['Zhu', 'Xiaoqing', ''], ['Sole', 'Joel', ''], ['Bull', 'David', '']]","extracted_entities":"[{'text': 'attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'hierarchical encoding mechanism', 'label': 'Attention mechanism'}, {'text': 'hierarchical encoding\\nmechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanisms","similarity_score":0.9558142424}
{"id":2503.13798,"submitter":"Amirhossein Khakpour","authors":"Amirhossein Khakpour, Lucia Florescu, Richard Tilley, Haibo Jiang, K.\n  Swaminathan Iyer, Gustavo Carneiro","title":"AI-Powered Prediction of Nanoparticle Pharmacokinetics: A Multi-View\n  Learning Approach","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  The clinical translation of nanoparticle-based treatments remains limited due\nto the unpredictability of (nanoparticle) NP\npharmacokinetics$\\unicode{x2014}$how they distribute, accumulate, and clear\nfrom the body. Predicting these behaviours is challenging due to complex\nbiological interactions and the difficulty of obtaining high-quality\nexperimental datasets. Existing AI-driven approaches rely heavily on\ndata-driven learning but fail to integrate crucial knowledge about NP\nproperties and biodistribution mechanisms. We introduce a multi-view deep\nlearning framework that enhances pharmacokinetic predictions by incorporating\nprior knowledge of key NP properties such as size and charge into a\ncross-attention mechanism, enabling context-aware feature selection and\nimproving generalization despite small datasets. To further enhance prediction\nrobustness, we employ an ensemble learning approach, combining deep learning\nwith XGBoost (XGB) and Random Forest (RF), which significantly outperforms\nexisting AI models. Our interpretability analysis reveals key physicochemical\nproperties driving NP biodistribution, providing biologically meaningful\ninsights into possible mechanisms governing NP behaviour in vivo rather than a\nblack-box model. Furthermore, by bridging machine learning with physiologically\nbased pharmacokinetic (PBPK) modelling, this work lays the foundation for\ndata-efficient AI-driven drug discovery and precision nanomedicine.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 01:09:32 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Khakpour', 'Amirhossein', ''], ['Florescu', 'Lucia', ''], ['Tilley', 'Richard', ''], ['Jiang', 'Haibo', ''], ['Iyer', 'K. Swaminathan', ''], ['Carneiro', 'Gustavo', '']]","extracted_entities":"[{'text': 'cross-attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross-attention mechanism","similarity_score":0.8302809}
{"id":2503.13799,"submitter":"Liangrui Pan","authors":"Liangrui Pan, Xiaoyu Li, Yutao Dou, Qiya Song, Jiadi Luo, Qingchun\n  Liang, Shaoliang Peng","title":"SMILE: a Scale-aware Multiple Instance Learning Method for Multicenter\n  STAS Lung Cancer Histopathology Diagnosis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Spread through air spaces (STAS) represents a newly identified aggressive\npattern in lung cancer, which is known to be associated with adverse prognostic\nfactors and complex pathological features. Pathologists currently rely on time\nconsuming manual assessments, which are highly subjective and prone to\nvariation. This highlights the urgent need for automated and precise diag\nnostic solutions. 2,970 lung cancer tissue slides are comprised from multiple\ncenters, re-diagnosed them, and constructed and publicly released three lung\ncancer STAS datasets: STAS CSU (hospital), STAS TCGA, and STAS CPTAC. All STAS\ndatasets provide corresponding pathological feature diagnoses and related\nclinical data. To address the bias, sparse and heterogeneous nature of STAS, we\npropose an scale-aware multiple instance learning(SMILE) method for STAS\ndiagnosis of lung cancer. By introducing a scale-adaptive attention mechanism,\nthe SMILE can adaptively adjust high attention instances, reducing\nover-reliance on local regions and promoting consistent detection of STAS\nlesions. Extensive experiments show that SMILE achieved competitive diagnostic\nresults on STAS CSU, diagnosing 251 and 319 STAS samples in CPTAC\nandTCGA,respectively, surpassing clinical average AUC. The 11 open baseline\nresults are the first to be established for STAS research, laying the\nfoundation for the future expansion, interpretability, and clinical integration\nof computational pathology technologies. The datasets and code are available at\nhttps:\/\/anonymous.4open.science\/r\/IJCAI25-1DA1.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 01:09:52 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Pan', 'Liangrui', ''], ['Li', 'Xiaoyu', ''], ['Dou', 'Yutao', ''], ['Song', 'Qiya', ''], ['Luo', 'Jiadi', ''], ['Liang', 'Qingchun', ''], ['Peng', 'Shaoliang', '']]","extracted_entities":"[{'text': 'scale-adaptive attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"scale-adaptive attention mechanism","similarity_score":0.7705892324}
{"id":2503.13806,"submitter":"Jiancheng Ye","authors":"Wenjie Zhang, Ziyang Zhang, Mengnan He, Jiancheng Ye","title":"Organ-aware Multi-scale Medical Image Segmentation Using Text Prompt\n  Engineering","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Accurate segmentation is essential for effective treatment planning and\ndisease monitoring. Existing medical image segmentation methods predominantly\nrely on uni-modal visual inputs, such as images or videos, requiring\nlabor-intensive manual annotations. Additionally, medical imaging techniques\ncapture multiple intertwined organs within a single scan, further complicating\nsegmentation accuracy. To address these challenges, MedSAM, a large-scale\nmedical segmentation model based on the Segment Anything Model (SAM), was\ndeveloped to enhance segmentation accuracy by integrating image features with\nuser-provided prompts. While MedSAM has demonstrated strong performance across\nvarious medical segmentation tasks, it primarily relies on geometric prompts\n(e.g., points and bounding boxes) and lacks support for text-based prompts,\nwhich could help specify subtle or ambiguous anatomical structures. To overcome\nthese limitations, we propose the Organ-aware Multi-scale Text-guided Medical\nImage Segmentation Model (OMT-SAM) for multi-organ segmentation. Our approach\nintroduces CLIP encoders as a novel image-text prompt encoder, operating with\nthe geometric prompt encoder to provide informative contextual guidance. We\npair descriptive textual prompts with corresponding images, processing them\nthrough pre-trained CLIP encoders and a cross-attention mechanism to generate\nfused image-text embeddings. Additionally, we extract multi-scale visual\nfeatures from MedSAM, capturing fine-grained anatomical details at different\nlevels of granularity. We evaluate OMT-SAM on the FLARE 2021 dataset,\nbenchmarking its performance against existing segmentation methods. Empirical\nresults demonstrate that OMT-SAM achieves a mean Dice Similarity Coefficient of\n0.937, outperforming MedSAM (0.893) and other segmentation models, highlighting\nits superior capability in handling complex medical image segmentation tasks.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 01:35:34 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Zhang', 'Wenjie', ''], ['Zhang', 'Ziyang', ''], ['He', 'Mengnan', ''], ['Ye', 'Jiancheng', '']]","extracted_entities":"[{'text': 'MedSAM', 'label': 'Large Language Model'}, {'text': 'user-provided prompts', 'label': 'Prompting'}, {'text': 'MedSAM', 'label': 'Large Language Model'}, {'text': 'geometric prompts', 'label': 'Prompting'}, {'text': 'text-based prompts', 'label': 'Prompting'}, {'text': 'cross-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'fused image-text embeddings', 'label': 'contextual Embedding'}, {'text': 'MedSAM', 'label': 'Large Language Model'}, {'text': 'MedSAM', 'label': 'Large Language Model'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross-attention mechanism","similarity_score":0.8302809}
{"id":2503.13858,"submitter":"Hongyu Ke","authors":"Hongyu Ke, Jack Morris, Kentaro Oguchi, Xiaofei Cao, Yongkang Liu,\n  Haoxin Wang, Yi Ding","title":"MamBEV: Enabling State Space Models to Learn Birds-Eye-View\n  Representations","comments":null,"journal-ref":"ICLR 2025","doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  3D visual perception tasks, such as 3D detection from multi-camera images,\nare essential components of autonomous driving and assistance systems. However,\ndesigning computationally efficient methods remains a significant challenge. In\nthis paper, we propose a Mamba-based framework called MamBEV, which learns\nunified Bird's Eye View (BEV) representations using linear spatio-temporal\nSSM-based attention. This approach supports multiple 3D perception tasks with\nsignificantly improved computational and memory efficiency. Furthermore, we\nintroduce SSM based cross-attention, analogous to standard cross attention,\nwhere BEV query representations can interact with relevant image features.\nExtensive experiments demonstrate MamBEV's promising performance across diverse\nvisual perception metrics, highlighting its advantages in input scaling\nefficiency compared to existing benchmark models.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 03:18:45 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Ke', 'Hongyu', ''], ['Morris', 'Jack', ''], ['Oguchi', 'Kentaro', ''], ['Cao', 'Xiaofei', ''], ['Liu', 'Yongkang', ''], ['Wang', 'Haoxin', ''], ['Ding', 'Yi', '']]","extracted_entities":"[{'text': 'linear spatio-temporal\\nSSM-based attention', 'label': 'Attention mechanism'}, {'text': 'standard cross attention', 'label': 'Attention mechanism'}, {'text': 'BEV', 'label': 'BERT'}, {'text': 'input scaling\\nefficiency', 'label': 'Scaling law'}]","assigned_concept":"Attention mechanism","matched_keyword":"standard cross attention","similarity_score":0.6138131618}
{"id":2503.13883,"submitter":"Ziyu Lin","authors":"Ziyu Lin, Yunfan Wu, Yuhang Ma, Junzhou Chen, Ronghui Zhang, Jiaming\n  Wu, Guodong Yin, and Liang Lin","title":"YOLO-LLTS: Real-Time Low-Light Traffic Sign Detection via Prior-Guided\n  Enhancement and Multi-Branch Feature Interaction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Detecting traffic signs effectively under low-light conditions remains a\nsignificant challenge. To address this issue, we propose YOLO-LLTS, an\nend-to-end real-time traffic sign detection algorithm specifically designed for\nlow-light environments. Firstly, we introduce the High-Resolution Feature Map\nfor Small Object Detection (HRFM-TOD) module to address indistinct small-object\nfeatures in low-light scenarios. By leveraging high-resolution feature maps,\nHRFM-TOD effectively mitigates the feature dilution problem encountered in\nconventional PANet frameworks, thereby enhancing both detection accuracy and\ninference speed. Secondly, we develop the Multi-branch Feature Interaction\nAttention (MFIA) module, which facilitates deep feature interaction across\nmultiple receptive fields in both channel and spatial dimensions, significantly\nimproving the model's information extraction capabilities. Finally, we propose\nthe Prior-Guided Enhancement Module (PGFE) to tackle common image quality\nchallenges in low-light environments, such as noise, low contrast, and\nblurriness. This module employs prior knowledge to enrich image details and\nenhance visibility, substantially boosting detection performance. To support\nthis research, we construct a novel dataset, the Chinese Nighttime Traffic Sign\nSample Set (CNTSSS), covering diverse nighttime scenarios, including urban,\nhighway, and rural environments under varying weather conditions. Experimental\nevaluations demonstrate that YOLO-LLTS achieves state-of-the-art performance,\noutperforming the previous best methods by 2.7% mAP50 and 1.6% mAP50:95 on\nTT100K-night, 1.3% mAP50 and 1.9% mAP50:95 on CNTSSS, and achieving superior\nresults on the CCTSDB2021 dataset. Moreover, deployment experiments on edge\ndevices confirm the real-time applicability and effectiveness of our proposed\napproach.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 04:28:05 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Lin', 'Ziyu', ''], ['Wu', 'Yunfan', ''], ['Ma', 'Yuhang', ''], ['Chen', 'Junzhou', ''], ['Zhang', 'Ronghui', ''], ['Wu', 'Jiaming', ''], ['Yin', 'Guodong', ''], ['Lin', 'Liang', '']]","extracted_entities":"[{'text': 'Multi-branch Feature Interaction\\nAttention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Multi-branch Feature Interaction\nAttention","similarity_score":0.535713017}
{"id":2503.13926,"submitter":"Huan Ren","authors":"Huan Ren, Wenfei Yang, Xiang Liu, Shifeng Zhang, Tianzhu Zhang","title":"Learning Shape-Independent Transformation via Spherical Representations\n  for Category-Level Object Pose Estimation","comments":"Accepted by ICLR 2025. Project page is available at\n  https:\/\/renhuan1999.github.io\/SpherePose","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Category-level object pose estimation aims to determine the pose and size of\nnovel objects in specific categories. Existing correspondence-based approaches\ntypically adopt point-based representations to establish the correspondences\nbetween primitive observed points and normalized object coordinates. However,\ndue to the inherent shape-dependence of canonical coordinates, these methods\nsuffer from semantic incoherence across diverse object shapes. To resolve this\nissue, we innovatively leverage the sphere as a shared proxy shape of objects\nto learn shape-independent transformation via spherical representations. Based\non this insight, we introduce a novel architecture called SpherePose, which\nyields precise correspondence prediction through three core designs. Firstly,\nWe endow the point-wise feature extraction with SO(3)-invariance, which\nfacilitates robust mapping between camera coordinate space and object\ncoordinate space regardless of rotation transformation. Secondly, the spherical\nattention mechanism is designed to propagate and integrate features among\nspherical anchors from a comprehensive perspective, thus mitigating the\ninterference of noise and incomplete point cloud. Lastly, a hyperbolic\ncorrespondence loss function is designed to distinguish subtle distinctions,\nwhich can promote the precision of correspondence prediction. Experimental\nresults on CAMERA25, REAL275 and HouseCat6D benchmarks demonstrate the superior\nperformance of our method, verifying the effectiveness of spherical\nrepresentations and architectural innovations.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 05:43:42 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 11:29:13 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Ren', 'Huan', ''], ['Yang', 'Wenfei', ''], ['Liu', 'Xiang', ''], ['Zhang', 'Shifeng', ''], ['Zhang', 'Tianzhu', '']]","extracted_entities":"[{'text': 'spherical\\nattention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"spherical\nattention mechanism","similarity_score":0.7816957831}
{"id":2503.13982,"submitter":"Huy Hoang Bui Mr","authors":"Huy-Hoang Bui, Bach-Thuan Bui, Quang-Vinh Tran, Yasuyuki Fujii, Joo-Ho\n  Lee","title":"A-SCoRe: Attention-based Scene Coordinate Regression for wide-ranging\n  scenarios","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/","abstract":"  Visual localization is considered to be one of the crucial parts in many\nrobotic and vision systems. While state-of-the art methods that relies on\nfeature matching have proven to be accurate for visual localization, its\nrequirements for storage and compute are burdens. Scene coordinate regression\n(SCR) is an alternative approach that remove the barrier for storage by\nlearning to map 2D pixels to 3D scene coordinates. Most popular SCR use\nConvolutional Neural Network (CNN) to extract 2D descriptor, which we would\nargue that it miss the spatial relationship between pixels. Inspired by the\nsuccess of vision transformer architecture, we present a new SCR architecture,\ncalled A-ScoRe, an Attention-based model which leverage attention on descriptor\nmap level to produce meaningful and high-semantic 2D descriptors. Since the\noperation is performed on descriptor map, our model can work with multiple data\nmodality whether it is a dense or sparse from depth-map, SLAM to\nStructure-from-Motion (SfM). This versatility allows A-SCoRe to operate in\ndifferent kind of environments, conditions and achieve the level of flexibility\nthat is important for mobile robots. Results show our methods achieve\ncomparable performance with State-of-the-art methods on multiple benchmark\nwhile being light-weighted and much more flexible. Code and pre-trained models\nare public in our repository: https:\/\/github.com\/ais-lab\/A-SCoRe.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 07:39:50 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Bui', 'Huy-Hoang', ''], ['Bui', 'Bach-Thuan', ''], ['Tran', 'Quang-Vinh', ''], ['Fujii', 'Yasuyuki', ''], ['Lee', 'Joo-Ho', '']]","extracted_entities":"[{'text': 'attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention","similarity_score":0.7383304834}
{"id":2503.13985,"submitter":"Jaewoo Song","authors":"Jaewoo Song, Daemin Park, Kanghyun Baek, Sangyub Lee, Jooyoung Choi,\n  Eunji Kim, Sungroh Yoon","title":"DefectFill: Realistic Defect Generation with Inpainting Diffusion Model\n  for Visual Inspection","comments":"Accepted by CVPR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Developing effective visual inspection models remains challenging due to the\nscarcity of defect data. While image generation models have been used to\nsynthesize defect images, producing highly realistic defects remains difficult.\nWe propose DefectFill, a novel method for realistic defect generation that\nrequires only a few reference defect images. It leverages a fine-tuned\ninpainting diffusion model, optimized with our custom loss functions\nincorporating defect, object, and attention terms. It enables precise capture\nof detailed, localized defect features and their seamless integration into\ndefect-free objects. Additionally, our Low-Fidelity Selection method further\nenhances the defect sample quality. Experiments show that DefectFill generates\nhigh-quality defect images, enabling visual inspection models to achieve\nstate-of-the-art performance on the MVTec AD dataset.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 07:42:11 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Song', 'Jaewoo', ''], ['Park', 'Daemin', ''], ['Baek', 'Kanghyun', ''], ['Lee', 'Sangyub', ''], ['Choi', 'Jooyoung', ''], ['Kim', 'Eunji', ''], ['Yoon', 'Sungroh', '']]","extracted_entities":"[{'text': 'attention terms', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention terms","similarity_score":0.7109348774}
{"id":2503.14035,"submitter":"Joopyo Hong","authors":"Seung Woo Ko, Joopyo Hong, Suyoung Kim, Seungjai Bang, Sungzoon Cho,\n  Nojun Kwak, Hyung-Sin Kim, Joonseok Lee","title":"A Revisit to the Decoder for Camouflaged Object Detection","comments":"Published in BMVC 2024, 13 pages, 7 figures (Appendix: 5 pages, 2\n  figures)","journal-ref":"British Machine Vision Conference (BMVC) 2024","doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Camouflaged object detection (COD) aims to generate a fine-grained\nsegmentation map of camouflaged objects hidden in their background. Due to the\nhidden nature of camouflaged objects, it is essential for the decoder to be\ntailored to effectively extract proper features of camouflaged objects and\nextra-carefully generate their complex boundaries. In this paper, we propose a\nnovel architecture that augments the prevalent decoding strategy in COD with\nEnrich Decoder and Retouch Decoder, which help to generate a fine-grained\nsegmentation map. Specifically, the Enrich Decoder amplifies the channels of\nfeatures that are important for COD using channel-wise attention. Retouch\nDecoder further refines the segmentation maps by spatially attending to\nimportant pixels, such as the boundary regions. With extensive experiments, we\ndemonstrate that ENTO shows superior performance using various encoders, with\nthe two novel components playing their unique roles that are mutually\ncomplementary.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 08:51:50 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Ko', 'Seung Woo', ''], ['Hong', 'Joopyo', ''], ['Kim', 'Suyoung', ''], ['Bang', 'Seungjai', ''], ['Cho', 'Sungzoon', ''], ['Kwak', 'Nojun', ''], ['Kim', 'Hyung-Sin', ''], ['Lee', 'Joonseok', '']]","extracted_entities":"[{'text': 'channel-wise attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"channel-wise attention","similarity_score":0.7209811807}
{"id":2503.14037,"submitter":"Liyan Wang","authors":"Cong Wang, Jinshan Pan, Liyan Wang, and Wei Wang","title":"Intra and Inter Parser-Prompted Transformers for Effective Image\n  Restoration","comments":"This version is accepted by the Association for the Advancement of\n  Artificial Intelligence (AAAI-25)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We propose Intra and Inter Parser-Prompted Transformers (PPTformer) that\nexplore useful features from visual foundation models for image restoration.\nSpecifically, PPTformer contains two parts: an Image Restoration Network\n(IRNet) for restoring images from degraded observations and a Parser-Prompted\nFeature Generation Network (PPFGNet) for providing IRNet with reliable parser\ninformation to boost restoration. To enhance the integration of the parser\nwithin IRNet, we propose Intra Parser-Prompted Attention (IntraPPA) and Inter\nParser-Prompted Attention (InterPPA) to implicitly and explicitly learn useful\nparser features to facilitate restoration. The IntraPPA re-considers cross\nattention between parser and restoration features, enabling implicit perception\nof the parser from a long-range and intra-layer perspective. Conversely, the\nInterPPA initially fuses restoration features with those of the parser,\nfollowed by formulating these fused features within an attention mechanism to\nexplicitly perceive parser information. Further, we propose a parser-prompted\nfeed-forward network to guide restoration within pixel-wise gating modulation.\nExperimental results show that PPTformer achieves state-of-the-art performance\non image deraining, defocus deblurring, desnowing, and low-light enhancement.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 08:56:02 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Wang', 'Cong', ''], ['Pan', 'Jinshan', ''], ['Wang', 'Liyan', ''], ['Wang', 'Wei', '']]","extracted_entities":"[{'text': 'visual foundation models', 'label': 'Foundation Model'}, {'text': 'IntraPPA', 'label': 'Transformers'}, {'text': 'InterPPA', 'label': 'Transformers'}, {'text': 'attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanism","similarity_score":1.0}
{"id":2503.1407,"submitter":"Junliang Guo","authors":"Yang Ye, Junliang Guo, Haoyu Wu, Tianyu He, Tim Pearce, Tabish Rashid,\n  Katja Hofmann, Jiang Bian","title":"Fast Autoregressive Video Generation with Diagonal Decoding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Autoregressive Transformer models have demonstrated impressive performance in\nvideo generation, but their sequential token-by-token decoding process poses a\nmajor bottleneck, particularly for long videos represented by tens of thousands\nof tokens. In this paper, we propose Diagonal Decoding (DiagD), a training-free\ninference acceleration algorithm for autoregressively pre-trained models that\nexploits spatial and temporal correlations in videos. Our method generates\ntokens along diagonal paths in the spatial-temporal token grid, enabling\nparallel decoding within each frame as well as partially overlapping across\nconsecutive frames. The proposed algorithm is versatile and adaptive to various\ngenerative models and tasks, while providing flexible control over the\ntrade-off between inference speed and visual quality. Furthermore, we propose a\ncost-effective finetuning strategy that aligns the attention patterns of the\nmodel with our decoding order, further mitigating the training-inference gap on\nsmall-scale models. Experiments on multiple autoregressive video generation\nmodels and datasets demonstrate that DiagD achieves up to $10\\times$ speedup\ncompared to naive sequential decoding, while maintaining comparable visual\nfidelity.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 09:42:55 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Ye', 'Yang', ''], ['Guo', 'Junliang', ''], ['Wu', 'Haoyu', ''], ['He', 'Tianyu', ''], ['Pearce', 'Tim', ''], ['Rashid', 'Tabish', ''], ['Hofmann', 'Katja', ''], ['Bian', 'Jiang', '']]","extracted_entities":"[{'text': 'cost-effective finetuning strategy', 'label': 'Fine-tuning'}, {'text': 'attention patterns', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention patterns","similarity_score":0.8255490065}
{"id":2503.1413,"submitter":"Paul Darm","authors":"Paul Darm, James Xie, Annalisa Riccardi","title":"Inference-Time Intervention in Large Language Models for Reliable\n  Requirement Verification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.SE","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Steering the behavior of Large Language Models (LLMs) remains a challenge,\nparticularly in engineering applications where precision and reliability are\ncritical. While fine-tuning and prompting methods can modify model behavior,\nthey lack the dynamic and exact control necessary for engineering applications.\nInference-time intervention techniques provide a promising alternative,\nallowing targeted adjustments to LLM outputs. In this work, we demonstrate how\ninterventions enable fine-grained control for automating the usually\ntime-intensive requirement verification process in Model-Based Systems\nEngineering (MBSE). Using two early-stage Capella SysML models of space\nmissions with associated requirements, we apply the intervened LLMs to reason\nover a graph representation of the model to determine whether a requirement is\nfulfilled. Our method achieves robust and reliable outputs, significantly\nimproving over both a baseline model and a fine-tuning approach. By identifying\nand modifying as few as one to three specialised attention heads, we can\nsignificantly change the model's behavior. When combined with self-consistency,\nthis allows us to achieve perfect precision on our holdout test set.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 10:49:36 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Darm', 'Paul', ''], ['Xie', 'James', ''], ['Riccardi', 'Annalisa', '']]","extracted_entities":"[{'text': 'specialised attention heads', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"specialised attention heads","similarity_score":0.6921795607}
{"id":2503.1415,"submitter":"Yihang Zhou","authors":"Yihang Zhou, Ruige Kong, Zhengsen Xu, Linlin Xu, Sibo Cheng","title":"Comparative and Interpretative Analysis of CNN and Transformer Models in\n  Predicting Wildfire Spread Using Remote Sensing Data","comments":null,"journal-ref":null,"doi":"10.1029\/2024JH000409","report-no":null,"categories":"cs.CV eess.IV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Facing the escalating threat of global wildfires, numerous computer vision\ntechniques using remote sensing data have been applied in this area. However,\nthe selection of deep learning methods for wildfire prediction remains\nuncertain due to the lack of comparative analysis in a quantitative and\nexplainable manner, crucial for improving prevention measures and refining\nmodels. This study aims to thoroughly compare the performance, efficiency, and\nexplainability of four prevalent deep learning architectures: Autoencoder,\nResNet, UNet, and Transformer-based Swin-UNet. Employing a real-world dataset\nthat includes nearly a decade of remote sensing data from California, U.S.,\nthese models predict the spread of wildfires for the following day. Through\ndetailed quantitative comparison analysis, we discovered that Transformer-based\nSwin-UNet and UNet generally outperform Autoencoder and ResNet, particularly\ndue to the advanced attention mechanisms in Transformer-based Swin-UNet and the\nefficient use of skip connections in both UNet and Transformer-based Swin-UNet,\nwhich contribute to superior predictive accuracy and model interpretability.\nThen we applied XAI techniques on all four models, this not only enhances the\nclarity and trustworthiness of models but also promotes focused improvements in\nwildfire prediction capabilities. The XAI analysis reveals that UNet and\nTransformer-based Swin-UNet are able to focus on critical features such as\n'Previous Fire Mask', 'Drought', and 'Vegetation' more effectively than the\nother two models, while also maintaining balanced attention to the remaining\nfeatures, leading to their superior performance. The insights from our thorough\ncomparative analysis offer substantial implications for future model design and\nalso provide guidance for model selection in different scenarios.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:16:48 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Zhou', 'Yihang', ''], ['Kong', 'Ruige', ''], ['Xu', 'Zhengsen', ''], ['Xu', 'Linlin', ''], ['Cheng', 'Sibo', '']]","extracted_entities":"[{'text': 'Transformer-based Swin-UNet', 'label': 'Transformer-based model'}, {'text': 'Transformer-based\\nSwin-UNet', 'label': 'Transformer-based model'}, {'text': 'advanced attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'Transformer-based Swin-UNet', 'label': 'Transformer-based model'}, {'text': 'Transformer-based Swin-UNet', 'label': 'Transformer-based model'}, {'text': 'Transformer-based Swin-UNet', 'label': 'Transformer-based model'}]","assigned_concept":"Attention mechanism","matched_keyword":"advanced attention mechanisms","similarity_score":0.8760451674}
{"id":2503.14151,"submitter":"Yong Zhong","authors":"Yong Zhong, Zhuoyi Yang, Jiayan Teng, Xiaotao Gu, Chongxuan Li","title":"Concat-ID: Towards Universal Identity-Preserving Video Synthesis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present Concat-ID, a unified framework for identity-preserving video\ngeneration. Concat-ID employs Variational Autoencoders to extract image\nfeatures, which are concatenated with video latents along the sequence\ndimension, leveraging solely 3D self-attention mechanisms without the need for\nadditional modules. A novel cross-video pairing strategy and a multi-stage\ntraining regimen are introduced to balance identity consistency and facial\neditability while enhancing video naturalness. Extensive experiments\ndemonstrate Concat-ID's superiority over existing methods in both single and\nmulti-identity generation, as well as its seamless scalability to multi-subject\nscenarios, including virtual try-on and background-controllable generation.\nConcat-ID establishes a new benchmark for identity-preserving video synthesis,\nproviding a versatile and scalable solution for a wide range of applications.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 11:17:32 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Zhong', 'Yong', ''], ['Yang', 'Zhuoyi', ''], ['Teng', 'Jiayan', ''], ['Gu', 'Xiaotao', ''], ['Li', 'Chongxuan', '']]","extracted_entities":"[{'text': '3D self-attention mechanisms', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"3D self-attention mechanisms","similarity_score":0.7052303553}
{"id":2503.14231,"submitter":"Giovanni Delnevo","authors":"Ziyao Ling, Giovanni Delnevo, Paola Salomoni, Silvia Mirri","title":"Multi-task Learning for Identification of Porcelain in Song and Yuan\n  Dynasties","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Chinese porcelain holds immense historical and cultural value, making its\naccurate classification essential for archaeological research and cultural\nheritage preservation. Traditional classification methods rely heavily on\nexpert analysis, which is time-consuming, subjective, and difficult to scale.\nThis paper explores the application of DL and transfer learning techniques to\nautomate the classification of porcelain artifacts across four key attributes:\ndynasty, glaze, ware, and type. We evaluate four Convolutional Neural Networks\n(CNNs) - ResNet50, MobileNetV2, VGG16, and InceptionV3 - comparing their\nperformance with and without pre-trained weights. Our results demonstrate that\ntransfer learning significantly enhances classification accuracy, particularly\nfor complex tasks like type classification, where models trained from scratch\nexhibit lower performance. MobileNetV2 and ResNet50 consistently achieve high\naccuracy and robustness across all tasks, while VGG16 struggles with more\ndiverse classifications. We further discuss the impact of dataset limitations\nand propose future directions, including domain-specific pre-training,\nintegration of attention mechanisms, explainable AI methods, and generalization\nto other cultural artifacts.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 13:09:00 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Ling', 'Ziyao', ''], ['Delnevo', 'Giovanni', ''], ['Salomoni', 'Paola', ''], ['Mirri', 'Silvia', '']]","extracted_entities":"[{'text': 'transfer learning', 'label': 'Few-shot Learning'}, {'text': 'ResNet50', 'label': 'GPT-2'}, {'text': 'MobileNetV2', 'label': 'GPT-2'}, {'text': 'VGG16', 'label': 'GPT-2'}, {'text': 'InceptionV3', 'label': 'GPT-2'}, {'text': 'transfer learning', 'label': 'Few-shot Learning'}, {'text': 'MobileNetV2', 'label': 'GPT-2'}, {'text': 'attention mechanisms', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanisms","similarity_score":0.9558142424}
{"id":2503.1424,"submitter":"Viet Nguyen","authors":"Viet The Nguyen, Duy Anh Pham, An Thai Le, Jans Peter, Gunther Gust","title":"Persistent Homology-induced Graph Ensembles for Time Series Regressions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The effectiveness of Spatio-temporal Graph Neural Networks (STGNNs) in\ntime-series applications is often limited by their dependence on fixed,\nhand-crafted input graph structures. Motivated by insights from the Topological\nData Analysis (TDA) paradigm, of which real-world data exhibits multi-scale\npatterns, we construct several graphs using Persistent Homology Filtration -- a\nmathematical framework describing the multiscale structural properties of data\npoints. Then, we use the constructed graphs as an input to create an ensemble\nof Graph Neural Networks. The ensemble aggregates the signals from the\nindividual learners via an attention-based routing mechanism, thus\nsystematically encoding the inherent multiscale structures of data. Four\ndifferent real-world experiments on seismic activity prediction and traffic\nforecasting (PEMS-BAY, METR-LA) demonstrate that our approach consistently\noutperforms single-graph baselines while providing interpretable insights.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 13:22:52 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 10:33:40 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Nguyen', 'Viet The', ''], ['Pham', 'Duy Anh', ''], ['Le', 'An Thai', ''], ['Peter', 'Jans', ''], ['Gust', 'Gunther', '']]","extracted_entities":"[{'text': 'attention-based routing mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention-based routing mechanism","similarity_score":0.6317738295}
{"id":2503.14428,"submitter":"Yufan Deng","authors":"Hongyu Zhang, Yufan Deng, Shenghai Yuan, Peng Jin, Zesen Cheng, Yian\n  Zhao, Chang Liu, Jie Chen","title":"MagicComp: Training-free Dual-Phase Refinement for Compositional Video\n  Generation","comments":"Project webpage: https:\/\/hong-yu-zhang.github.io\/MagicComp-Page\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Text-to-video (T2V) generation has made significant strides with diffusion\nmodels. However, existing methods still struggle with accurately binding\nattributes, determining spatial relationships, and capturing complex action\ninteractions between multiple subjects. To address these limitations, we\npropose MagicComp, a training-free method that enhances compositional T2V\ngeneration through dual-phase refinement. Specifically, (1) During the\nConditioning Stage: We introduce the Semantic Anchor Disambiguation to\nreinforces subject-specific semantics and resolve inter-subject ambiguity by\nprogressively injecting the directional vectors of semantic anchors into\noriginal text embedding; (2) During the Denoising Stage: We propose Dynamic\nLayout Fusion Attention, which integrates grounding priors and model-adaptive\nspatial perception to flexibly bind subjects to their spatiotemporal regions\nthrough masked attention modulation. Furthermore, MagicComp is a model-agnostic\nand versatile approach, which can be seamlessly integrated into existing T2V\narchitectures. Extensive experiments on T2V-CompBench and VBench demonstrate\nthat MagicComp outperforms state-of-the-art methods, highlighting its potential\nfor applications such as complex prompt-based and trajectory-controllable video\ngeneration. Project page: https:\/\/hong-yu-zhang.github.io\/MagicComp-Page\/.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:02:14 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Zhang', 'Hongyu', ''], ['Deng', 'Yufan', ''], ['Yuan', 'Shenghai', ''], ['Jin', 'Peng', ''], ['Cheng', 'Zesen', ''], ['Zhao', 'Yian', ''], ['Liu', 'Chang', ''], ['Chen', 'Jie', '']]","extracted_entities":"[{'text': 'original text embedding', 'label': 'Embedding'}, {'text': 'Dynamic\\nLayout Fusion Attention', 'label': 'Attention mechanism'}, {'text': 'masked attention modulation', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"masked attention modulation","similarity_score":0.700588882}
{"id":2503.14439,"submitter":"Kyriakos Stylianopoulos","authors":"Kyriakos Stylianopoulos, Panagiotis Gavriilidis, Gabriele Gradoni,\n  George C. Alexandropoulos","title":"Graph-CNNs for RF Imaging: Learning the Electric Field Integral\n  Equations","comments":"Submitted to EUSIPCO 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG eess.SP","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Radio-Frequency (RF) imaging concerns the digital recreation of the surfaces\nof scene objects based on the scattered field at distributed receivers. To\nsolve this difficult inverse scattering problems, data-driven methods are often\nemployed that extract patterns from similar training examples, while offering\nminimal latency. In this paper, we first provide an approximate yet fast\nelectromagnetic model, which is based on the electric field integral equations,\nfor data generation, and subsequently propose a Deep Neural Network (DNN)\narchitecture to learn the corresponding inverse model. A graph-attention\nbackbone allows for the system geometry to be passed to the DNN, where residual\nconvolutional layers extract features about the objects, while a UNet head\nperforms the final image reconstruction. Our quantitative and qualitative\nevaluations on two synthetic data sets of different characteristics showcase\nthe performance gains of thee proposed advanced architecture and its relative\nresilience to signal noise levels and various reception configurations.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:16:40 GMT'}]","update_date":"2025-03-19","authors_parsed":"[['Stylianopoulos', 'Kyriakos', ''], ['Gavriilidis', 'Panagiotis', ''], ['Gradoni', 'Gabriele', ''], ['Alexandropoulos', 'George C.', '']]","extracted_entities":"[{'text': 'graph-attention\\nbackbone', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"graph-attention\nbackbone","similarity_score":0.5559301376}
{"id":2503.14493,"submitter":"Chuxin Wang","authors":"Chuxin Wang, Wenfei Yang, Xiang Liu, Tianzhu Zhang","title":"State Space Model Meets Transformer: A New Paradigm for 3D Object\n  Detection","comments":"Accepted by ICLR 2025. Project url:\n  https:\/\/chuxwa.github.io\/project_DEST\/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  DETR-based methods, which use multi-layer transformer decoders to refine\nobject queries iteratively, have shown promising performance in 3D indoor\nobject detection. However, the scene point features in the transformer decoder\nremain fixed, leading to minimal contributions from later decoder layers,\nthereby limiting performance improvement. Recently, State Space Models (SSM)\nhave shown efficient context modeling ability with linear complexity through\niterative interactions between system states and inputs. Inspired by SSMs, we\npropose a new 3D object DEtection paradigm with an interactive STate space\nmodel (DEST). In the interactive SSM, we design a novel state-dependent SSM\nparameterization method that enables system states to effectively serve as\nqueries in 3D indoor detection tasks. In addition, we introduce four key\ndesigns tailored to the characteristics of point cloud and SSM: The\nserialization and bidirectional scanning strategies enable bidirectional\nfeature interaction among scene points within the SSM. The inter-state\nattention mechanism models the relationships between state points, while the\ngated feed-forward network enhances inter-channel correlations. To the best of\nour knowledge, this is the first method to model queries as system states and\nscene points as system inputs, which can simultaneously update scene point\nfeatures and query features with linear complexity. Extensive experiments on\ntwo challenging datasets demonstrate the effectiveness of our DEST-based\nmethod. Our method improves the GroupFree baseline in terms of AP50 on ScanNet\nV2 (+5.3) and SUN RGB-D (+3.2) datasets. Based on the VDETR baseline, Our\nmethod sets a new SOTA on the ScanNetV2 and SUN RGB-D datasets.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 17:58:03 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Mar 2025 14:10:18 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Wang', 'Chuxin', ''], ['Yang', 'Wenfei', ''], ['Liu', 'Xiang', ''], ['Zhang', 'Tianzhu', '']]","extracted_entities":"[{'text': 'inter-state\\nattention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"inter-state\nattention mechanism","similarity_score":0.8573140502}
{"id":2503.1464,"submitter":"Yi Liao","authors":"Yi Liao, Yongsheng Gao, and Weichuan Zhang","title":"Dynamic Accumulated Attention Map for Interpreting Evolution of\n  Decision-Making in Vision Transformer","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Various Vision Transformer (ViT) models have been widely used for image\nrecognition tasks. However, existing visual explanation methods can not display\nthe attention flow hidden inside the inner structure of ViT models, which\nexplains how the final attention regions are formed inside a ViT for its\ndecision-making. In this paper, a novel visual explanation approach, Dynamic\nAccumulated Attention Map (DAAM), is proposed to provide a tool that can\nvisualize, for the first time, the attention flow from the top to the bottom\nthrough ViT networks. To this end, a novel decomposition module is proposed to\nconstruct and store the spatial feature information by unlocking the [class]\ntoken generated by the self-attention module of each ViT block. The module can\nalso obtain the channel importance coefficients by decomposing the\nclassification score for supervised ViT models. Because of the lack of\nclassification score in self-supervised ViT models, we propose dimension-wise\nimportance weights to compute the channel importance coefficients. Such spatial\nfeatures are linearly combined with the corresponding channel importance\ncoefficients, forming the attention map for each block. The dynamic attention\nflow is revealed by block-wisely accumulating each attention map. The\ncontribution of this work focuses on visualizing the evolution dynamic of the\ndecision-making attention for any intermediate block inside a ViT model by\nproposing a novel decomposition module and dimension-wise importance weights.\nThe quantitative and qualitative analysis consistently validate the\neffectiveness and superior capacity of the proposed DAAM for not only\ninterpreting ViT models with the fully-connected layers as the classifier but\nalso self-supervised ViT models. The code is available at\nhttps:\/\/github.com\/ly9802\/DynamicAccumulatedAttentionMap.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 18:41:01 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Liao', 'Yi', ''], ['Gao', 'Yongsheng', ''], ['Zhang', 'Weichuan', '']]","extracted_entities":"[{'text': 'attention flow', 'label': 'Attention mechanism'}, {'text': 'attention flow', 'label': 'Attention mechanism'}, {'text': 'attention\\nflow', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention flow","similarity_score":0.6710098982}
{"id":2503.14751,"submitter":"Nicola Franco","authors":"Rohan Menon, Nicola Franco, Stephan G\\\"unnemann","title":"LipShiFT: A Certifiably Robust Shift-based Vision Transformer","comments":"ICLR 2025 Workshop: VerifAI: AI Verification in the Wild","journal-ref":"ICLR 2025 Workshop: VerifAI: AI Verification in the Wild","doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Deriving tight Lipschitz bounds for transformer-based architectures presents\na significant challenge. The large input sizes and high-dimensional attention\nmodules typically prove to be crucial bottlenecks during the training process\nand leads to sub-optimal results. Our research highlights practical constraints\nof these methods in vision tasks. We find that Lipschitz-based margin training\nacts as a strong regularizer while restricting weights in successive layers of\nthe model. Focusing on a Lipschitz continuous variant of the ShiftViT model, we\naddress significant training challenges for transformer-based architectures\nunder norm-constrained input setting. We provide an upper bound estimate for\nthe Lipschitz constants of this model using the $l_2$ norm on common image\nclassification datasets. Ultimately, we demonstrate that our method scales to\nlarger models and advances the state-of-the-art in certified robustness for\ntransformer-based architectures.\n","versions":"[{'version': 'v1', 'created': 'Tue, 18 Mar 2025 21:38:18 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Menon', 'Rohan', ''], ['Franco', 'Nicola', ''], ['G\u00fcnnemann', 'Stephan', '']]","extracted_entities":"[{'text': 'high-dimensional attention\\nmodules', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"high-dimensional attention\nmodules","similarity_score":0.5948148966}
{"id":2503.14936,"submitter":"Yifan Zhang","authors":"Yifan Zhang, Chen Huang, Zachary Karas, Dung Thuy Nguyen, Kevin Leach,\n  Yu Huang","title":"Enhancing Code LLM Training with Programmer Attention","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.HC cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Human attention provides valuable yet underexploited signals for code LLM\ntraining, offering a perspective beyond purely machine-driven attention.\nDespite the complexity and cost of collecting eye-tracking data, there has also\nbeen limited progress in systematically using these signals for code LLM\ntraining. To address both issues, we propose a cohesive pipeline spanning\naugmentation and reward-based fine-tuning. Specifically, we introduce (1) an\neye-tracking path augmentation method to expand programmer attention datasets,\n(2) a pattern abstraction step that refines raw fixations into learnable\nattention motifs, and (3) a reward-guided strategy for integrating these\ninsights directly into a CodeT5 supervised fine-tuning process. Our experiments\nyield +7.16 in CodeBLEU on the CodeXGlue benchmark for code summarization,\nunderscoring how uniting human and machine attention can boost code\nintelligence. We hope this work encourages broader exploration of human-centric\nmethods in next-generation AI4SE.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 06:44:29 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Zhang', 'Yifan', ''], ['Huang', 'Chen', ''], ['Karas', 'Zachary', ''], ['Nguyen', 'Dung Thuy', ''], ['Leach', 'Kevin', ''], ['Huang', 'Yu', '']]","extracted_entities":"[{'text': 'Human attention', 'label': 'Attention mechanism'}, {'text': 'code LLM', 'label': 'LLM'}, {'text': 'reward-based fine-tuning', 'label': 'Fine-tuning'}, {'text': 'programmer attention', 'label': 'Attention mechanism'}, {'text': 'code summarization', 'label': 'Knowledge distillation'}, {'text': 'human and machine attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Human attention","similarity_score":0.7963528633}
{"id":2503.14938,"submitter":"Ci Liu","authors":"Zhong Ji, Ci Liu, Jingren Liu, Chen Tang, Yanwei Pang, Xuelong Li","title":"Optimal Transport Adapter Tuning for Bridging Modality Gaps in Few-Shot\n  Remote Sensing Scene Classification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Few-Shot Remote Sensing Scene Classification (FS-RSSC) presents the challenge\nof classifying remote sensing images with limited labeled samples. Existing\nmethods typically emphasize single-modal feature learning, neglecting the\npotential benefits of optimizing multi-modal representations. To address this\nlimitation, we propose a novel Optimal Transport Adapter Tuning (OTAT)\nframework aimed at constructing an ideal Platonic representational space\nthrough optimal transport (OT) theory. This framework seeks to harmonize rich\nvisual information with less dense textual cues, enabling effective cross-modal\ninformation transfer and complementarity. Central to this approach is the\nOptimal Transport Adapter (OTA), which employs a cross-modal attention\nmechanism to enrich textual representations and facilitate subsequent better\ninformation interaction. By transforming the network optimization into an OT\noptimization problem, OTA establishes efficient pathways for balanced\ninformation exchange between modalities. Moreover, we introduce a sample-level\nEntropy-Aware Weighted (EAW) loss, which combines difficulty-weighted\nsimilarity scores with entropy-based regularization. This loss function\nprovides finer control over the OT optimization process, enhancing its\nsolvability and stability. Our framework offers a scalable and efficient\nsolution for advancing multimodal learning in remote sensing applications.\nExtensive experiments on benchmark datasets demonstrate that OTAT achieves\nstate-of-the-art performance in FS-RSSC, significantly improving the model\nperformance and generalization.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 07:04:24 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Ji', 'Zhong', ''], ['Liu', 'Ci', ''], ['Liu', 'Jingren', ''], ['Tang', 'Chen', ''], ['Pang', 'Yanwei', ''], ['Li', 'Xuelong', '']]","extracted_entities":"[{'text': 'single-modal feature learning', 'label': 'Zero-shot Learning'}, {'text': 'cross-modal attention\\nmechanism', 'label': 'Attention mechanism'}, {'text': 'FS-RSSC', 'label': 'Few-shot Learning'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross-modal attention\nmechanism","similarity_score":0.7663174868}
{"id":2503.14944,"submitter":"Zihan Cao","authors":"Zihan Cao, Yu Zhong, Ziqi Wang, Liang-Jian Deng","title":"MMAIF: Multi-task and Multi-degradation All-in-One for Image Fusion with\n  Language Guidance","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Image fusion, a fundamental low-level vision task, aims to integrate multiple\nimage sequences into a single output while preserving as much information as\npossible from the input. However, existing methods face several significant\nlimitations: 1) requiring task- or dataset-specific models; 2) neglecting\nreal-world image degradations (\\textit{e.g.}, noise), which causes failure when\nprocessing degraded inputs; 3) operating in pixel space, where attention\nmechanisms are computationally expensive; and 4) lacking user interaction\ncapabilities. To address these challenges, we propose a unified framework for\nmulti-task, multi-degradation, and language-guided image fusion. Our framework\nincludes two key components: 1) a practical degradation pipeline that simulates\nreal-world image degradations and generates interactive prompts to guide the\nmodel; 2) an all-in-one Diffusion Transformer (DiT) operating in latent space,\nwhich fuses a clean image conditioned on both the degraded inputs and the\ngenerated prompts. Furthermore, we introduce principled modifications to the\noriginal DiT architecture to better suit the fusion task. Based on this\nframework, we develop two versions of the model: Regression-based and Flow\nMatching-based variants. Extensive qualitative and quantitative experiments\ndemonstrate that our approach effectively addresses the aforementioned\nlimitations and outperforms previous restoration+fusion and all-in-one\npipelines. Codes are available at https:\/\/github.com\/294coder\/MMAIF.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 07:20:02 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Cao', 'Zihan', ''], ['Zhong', 'Yu', ''], ['Wang', 'Ziqi', ''], ['Deng', 'Liang-Jian', '']]","extracted_entities":"[{'text': 'attention\\nmechanisms', 'label': 'Attention mechanism'}, {'text': 'interactive prompts', 'label': 'Prompting'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention\nmechanisms","similarity_score":0.9558142424}
{"id":2503.1496,"submitter":"Seungyeon Cho","authors":"Seungyeon Cho, Tae-Kyun Kim","title":"Body-Hand Modality Expertized Networks with Cross-attention for\n  Fine-grained Skeleton Action Recognition","comments":"7 figures, 8 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Skeleton-based Human Action Recognition (HAR) is a vital technology in\nrobotics and human-robot interaction. However, most existing methods\nconcentrate primarily on full-body movements and often overlook subtle hand\nmotions that are critical for distinguishing fine-grained actions. Recent work\nleverages a unified graph representation that combines body, hand, and foot\nkeypoints to capture detailed body dynamics. Yet, these models often blur fine\nhand details due to the disparity between body and hand action characteristics\nand the loss of subtle features during the spatial-pooling. In this paper, we\npropose BHaRNet (Body-Hand action Recognition Network), a novel framework that\naugments a typical body-expert model with a hand-expert model. Our model\njointly trains both streams with an ensemble loss that fosters cooperative\nspecialization, functioning in a manner reminiscent of a Mixture-of-Experts\n(MoE). Moreover, cross-attention is employed via an expertized branch method\nand a pooling-attention module to enable feature-level interactions and\nselectively fuse complementary information. Inspired by MMNet, we also\ndemonstrate the applicability of our approach to multi-modal tasks by\nleveraging RGB information, where body features guide RGB learning to capture\nricher contextual cues. Experiments on large-scale benchmarks (NTU RGB+D 60,\nNTU RGB+D 120, PKU-MMD, and Northwestern-UCLA) demonstrate that BHaRNet\nachieves SOTA accuracies -- improving from 86.4\\% to 93.0\\% in hand-intensive\nactions -- while maintaining fewer GFLOPs and parameters than the relevant\nunified methods.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 07:54:52 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Cho', 'Seungyeon', ''], ['Kim', 'Tae-Kyun', '']]","extracted_entities":"[{'text': 'cross-attention', 'label': 'Attention mechanism'}, {'text': 'RGB learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross-attention","similarity_score":0.6773566008}
{"id":2503.15008,"submitter":"Saddam Hussain Khan","authors":"Aamir Mehmood, Yue Hu, Saddam Hussain Khan (Artificial Intelligence\n  Lab, Department of Computer Systems Engineering, University of Engineering\n  and Applied Sciences (UEAS), Swat, Pakistan)","title":"A Novel Channel Boosted Residual CNN-Transformer with Regional-Boundary\n  Learning for Breast Cancer Detection","comments":"12 pages, 10 Figures, 2 Tables. arXiv admin note: substantial text\n  overlap with arXiv:2405.12986","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.AI cs.CV cs.LG","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Recent advancements in detecting tumors using deep learning on breast\nultrasound images (BUSI) have demonstrated significant success. Deep CNNs and\nvision-transformers (ViTs) have demonstrated individually promising initial\nperformance. However, challenges related to model complexity and contrast,\ntexture, and tumor morphology variations introduce uncertainties that hinder\nthe effectiveness of current methods. This study introduces a novel hybrid\nframework, CB-Res-RBCMT, combining customized residual CNNs and new ViT\ncomponents for detailed BUSI cancer analysis. The proposed RBCMT uses stem\nconvolution blocks with CNN Meet Transformer (CMT) blocks, followed by new\nRegional and boundary (RB) feature extraction operations for capturing contrast\nand morphological variations. Moreover, the CMT block incorporates global\ncontextual interactions through multi-head attention, enhancing computational\nefficiency with a lightweight design. Additionally, the customized inverse\nresidual and stem CNNs within the CMT effectively extract local texture\ninformation and handle vanishing gradients. Finally, the new channel-boosted\n(CB) strategy enriches the feature diversity of the limited dataset by\ncombining the original RBCMT channels with transfer learning-based residual\nCNN-generated maps. These diverse channels are processed through a spatial\nattention block for optimal pixel selection, reducing redundancy and improving\nthe discrimination of minor contrast and texture variations. The proposed\nCB-Res-RBCMT achieves an F1-score of 95.57%, accuracy of 95.63%, sensitivity of\n96.42%, and precision of 94.79% on the standard harmonized stringent BUSI\ndataset, outperforming existing ViT and CNN methods. These results demonstrate\nthe versatility of our integrated CNN-Transformer framework in capturing\ndiverse features and delivering superior performance in BUSI cancer diagnosis.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 08:59:02 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Mehmood', 'Aamir', '', 'Artificial Intelligence\\n  Lab, Department of Computer Systems Engineering, University of Engineering\\n  and Applied Sciences'], ['Hu', 'Yue', '', 'Artificial Intelligence\\n  Lab, Department of Computer Systems Engineering, University of Engineering\\n  and Applied Sciences'], ['Khan', 'Saddam Hussain', '', 'Artificial Intelligence\\n  Lab, Department of Computer Systems Engineering, University of Engineering\\n  and Applied Sciences']]","extracted_entities":"[{'text': 'multi-head attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"multi-head attention","similarity_score":0.69643116}
{"id":2503.15141,"submitter":"Nikola {\\DJ}uki\\'c","authors":"Nikola {\\DJ}uki\\'c, Tim Lebailly, Tinne Tuytelaars","title":"Object-Centric Pretraining via Target Encoder Bootstrapping","comments":"ICLR 2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Object-centric representation learning has recently been successfully applied\nto real-world datasets. This success can be attributed to pretrained\nnon-object-centric foundation models, whose features serve as reconstruction\ntargets for slot attention. However, targets must remain frozen throughout the\ntraining, which sets an upper bound on the performance object-centric models\ncan attain. Attempts to update the target encoder by bootstrapping result in\nlarge performance drops, which can be attributed to its lack of object-centric\ninductive biases, causing the object-centric model's encoder to drift away from\nrepresentations useful as reconstruction targets. To address these limitations,\nwe propose Object-CEntric Pretraining by Target Encoder BOotstrapping, a\nself-distillation setup for training object-centric models from scratch, on\nreal-world data, for the first time ever. In OCEBO, the target encoder is\nupdated as an exponential moving average of the object-centric model, thus\nexplicitly being enriched with object-centric inductive biases introduced by\nslot attention while removing the upper bound on performance present in other\nmodels. We mitigate the slot collapse caused by random initialization of the\ntarget encoder by introducing a novel cross-view patch filtering approach that\nlimits the supervision to sufficiently informative patches. When pretrained on\n241k images from COCO, OCEBO achieves unsupervised object discovery performance\ncomparable to that of object-centric models with frozen non-object-centric\ntarget encoders pretrained on hundreds of millions of images. The code and\npretrained models are publicly available at https:\/\/github.com\/djukicn\/ocebo.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 12:06:50 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['\u0110uki\u0107', 'Nikola', ''], ['Lebailly', 'Tim', ''], ['Tuytelaars', 'Tinne', '']]","extracted_entities":"[{'text': 'Object-centric representation learning', 'label': 'Few-shot Learning'}, {'text': 'slot attention', 'label': 'Attention mechanism'}, {'text': 'object-centric models', 'label': 'Foundation Model'}, {'text': 'object-centric models', 'label': 'Foundation Model'}, {'text': 'slot attention', 'label': 'Attention mechanism'}, {'text': 'object-centric models', 'label': 'Foundation Model'}]","assigned_concept":"Attention mechanism","matched_keyword":"slot attention","similarity_score":0.6686759591}
{"id":2503.15404,"submitter":"Yuchen Ren","authors":"Yuchen Ren, Zhengyu Zhao, Chenhao Lin, Bo Yang, Lu Zhou, Zhe Liu, Chao\n  Shen","title":"Improving Adversarial Transferability on Vision Transformers via Forward\n  Propagation Refinement","comments":"CVPR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Vision Transformers (ViTs) have been widely applied in various computer\nvision and vision-language tasks. To gain insights into their robustness in\npractical scenarios, transferable adversarial examples on ViTs have been\nextensively studied. A typical approach to improving adversarial\ntransferability is by refining the surrogate model. However, existing work on\nViTs has restricted their surrogate refinement to backward propagation. In this\nwork, we instead focus on Forward Propagation Refinement (FPR) and specifically\nrefine two key modules of ViTs: attention maps and token embeddings. For\nattention maps, we propose Attention Map Diversification (AMD), which\ndiversifies certain attention maps and also implicitly imposes beneficial\ngradient vanishing during backward propagation. For token embeddings, we\npropose Momentum Token Embedding (MTE), which accumulates historical token\nembeddings to stabilize the forward updates in both the Attention and MLP\nblocks. We conduct extensive experiments with adversarial examples transferred\nfrom ViTs to various CNNs and ViTs, demonstrating that our FPR outperforms the\ncurrent best (backward) surrogate refinement by up to 7.0\\% on average. We also\nvalidate its superiority against popular defenses and its compatibility with\nother transfer methods. Codes and appendix are available at\nhttps:\/\/github.com\/RYC-98\/FPR.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 16:44:23 GMT'}]","update_date":"2025-03-20","authors_parsed":"[['Ren', 'Yuchen', ''], ['Zhao', 'Zhengyu', ''], ['Lin', 'Chenhao', ''], ['Yang', 'Bo', ''], ['Zhou', 'Lu', ''], ['Liu', 'Zhe', ''], ['Shen', 'Chao', '']]","extracted_entities":"[{'text': 'Vision Transformers', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'attention maps', 'label': 'Attention mechanism'}, {'text': 'token embeddings', 'label': 'Embedding'}, {'text': 'attention maps', 'label': 'Attention mechanism'}, {'text': 'Attention', 'label': 'Attention mechanism'}, {'text': 'attention maps', 'label': 'Attention mechanism'}, {'text': 'Momentum Token Embedding', 'label': 'Embedding'}, {'text': 'token\\nembeddings', 'label': 'Embedding'}, {'text': 'ViTs', 'label': 'Transformers'}, {'text': 'ViTs', 'label': 'Transformers'}]","assigned_concept":"Attention mechanism","matched_keyword":"Attention","similarity_score":0.7383304834}
{"id":2503.15469,"submitter":"ZhengLin Lai","authors":"ZhengLin Lai, MengYao Liao, Dong Xu","title":"Dynamic Bi-Elman Attention Networks (DBEAN): Dual-Directional\n  Context-Aware Representation Learning for Enhanced Text Classification","comments":"11 pages,1 figure","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Text classification, a fundamental task in natural language processing (NLP),\naims to categorize textual data into predefined labels. Traditional methods\nstruggled with complex linguistic structures and semantic dependencies. The\nadvent of deep learning, particularly recurrent neural networks (RNNs) and\nTransformer-based models, has significantly advanced the field by enabling\nnuanced feature extraction and context-aware predictions. Despite improvements,\nexisting models exhibit limitations in balancing interpretability,\ncomputational efficiency, and long-range contextual understanding. This paper\nproposes the Dynamic Bidirectional Elman with Attention Network (DBEAN), which\nintegrates bidirectional temporal modelling with self-attention mechanisms.\nDBEAN dynamically assigns weights to critical segments of input, improving\ncontextual representation while maintaining computational efficiency.\n","versions":"[{'version': 'v1', 'created': 'Wed, 19 Mar 2025 17:45:13 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Mar 2025 10:09:43 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Lai', 'ZhengLin', ''], ['Liao', 'MengYao', ''], ['Xu', 'Dong', '']]","extracted_entities":"[{'text': 'self-attention mechanisms', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"self-attention mechanisms","similarity_score":0.8465653658}
{"id":2503.15758,"submitter":"Venmugil Elango","authors":"Venmugil Elango","title":"ATTENTION2D: Communication Efficient Distributed Self-Attention\n  Mechanism","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.DC","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Transformer-based models have emerged as a leading architecture for natural\nlanguage processing, natural language generation, and image generation tasks. A\nfundamental element of the transformer architecture is self-attention, which\nallows the model to capture intricate dependencies within the data. However,\nthe self-attention mechanism also incurs significant computational and memory\ncosts, particularly for long sequences.\n  In this paper, we introduce ATTENTION2D, a novel approach that exploits\nparallelism along two dimensions - query and key\/value - of the self-attention\noperation. This method enables efficient distribution and parallelization of\ncomputations across multiple devices. Our approach facilitates asymptotically\nfaster training and inference phases compared to previous methods, without\nrelying on approximations or incurring additional computational or memory\noverheads. Furthermore, unlike existing techniques that struggle to scale with\nan increasing number of processing units, our approach effectively scales with\nadditional processing units.\n  Our experimental results confirm the effectiveness of our method in improving\ncommunication efficiency and scalability. Compared to Ring Attention, our\napproach demonstrated up to a 5x performance boost on a GPT-3-like model using\n64 NVIDIA A100 GPUs across 16 nodes, and up to a 9.4x performance boost on 64\nNVIDIA H100 GPUs across 64 nodes.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 00:25:44 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Elango', 'Venmugil', '']]","extracted_entities":"[{'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"self-attention","similarity_score":0.731767118}
{"id":2503.15815,"submitter":"Vishnu Dasu","authors":"Vishnu Asutosh Dasu, Md Rafi ur Rashid, Vipul Gupta, Saeid\n  Tizpaz-Niari, Gang Tan","title":"Attention Pruning: Automated Fairness Repair of Language Models via\n  Surrogate Simulated Annealing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  This paper explores pruning attention heads as a post-processing bias\nmitigation method for large language models (LLMs). Modern AI systems such as\nLLMs are expanding into sensitive social contexts where fairness concerns\nbecome especially crucial. Since LLMs develop decision-making patterns by\ntraining on massive datasets of human-generated content, they naturally encode\nand perpetuate societal biases. While modifying training datasets and\nalgorithms is expensive and requires significant resources; post-processing\ntechniques-such as selectively deactivating neurons and attention heads in\npre-trained LLMs-can provide feasible and effective approaches to improve\nfairness. However, identifying the optimal subset of parameters to prune\npresents a combinatorial challenge within LLMs' immense parameter space,\nrequiring solutions that efficiently balance competing objectives across the\nfrontiers of model fairness and utility.\n  To address the computational challenges, we explore a search-based program\nrepair approach via randomized simulated annealing. Given the prohibitive\nevaluation costs in billion-parameter LLMs, we develop surrogate deep neural\nnetworks that efficiently model the relationship between attention head states\n(active\/inactive) and their corresponding fairness\/utility metrics. This allows\nus to perform optimization over the surrogate models and efficiently identify\noptimal subsets of attention heads for selective pruning rather than directly\nsearching through the LLM parameter space. This paper introduces Attention\nPruning, a fairness-aware surrogate simulated annealing approach to prune\nattention heads in LLMs that disproportionately contribute to bias while\nminimally impacting overall model utility. Our experiments show that Attention\nPruning achieves up to $40\\%$ reduction in gender bias and outperforms the\nstate-of-the-art bias mitigation strategies.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 03:02:32 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Dasu', 'Vishnu Asutosh', ''], ['Rashid', 'Md Rafi ur', ''], ['Gupta', 'Vipul', ''], ['Tizpaz-Niari', 'Saeid', ''], ['Tan', 'Gang', '']]","extracted_entities":"[{'text': 'attention heads', 'label': 'Attention mechanism'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'attention heads', 'label': 'Attention mechanism'}, {'text': 'LLMs-can', 'label': 'Large Language Model'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'fairness', 'label': 'Model Bias and Fairness'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'attention heads', 'label': 'Attention mechanism'}, {'text': 'attention heads', 'label': 'Attention mechanism'}, {'text': 'LLMs', 'label': 'Large Language Model'}, {'text': 'gender bias', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention heads","similarity_score":0.7165269852}
{"id":2503.15828,"submitter":"Houqi Su","authors":"Xuhui Peng, Houqi Su","title":"Ergodicity of the viscous scalar conservation laws with a degenerate\n  noise","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  This paper establishes the ergodicity in $H^\\mathfrak n,\\mathfrak\nn=\\lfloor\\frac{d}{2}+1\\rfloor$ of the viscous scalar conservation laws on torus\n$\\mathbb T^d$ with general polynomial flux and a degenerate noise. The noise\ncould appear in as few as several directions. We introduce a localized\nframework that restricts attention to trajectories with controlled energy\ngrowth, circumventing the limitations of traditional contraction-based\napproaches. This localized method allows for a demonstration of e-property and\nconsequently proves the uniqueness of invariant measure under a\nH{\\\"o}rmander-type condition. Furthermore, we characterize the absolute\ncontinuity of the invariant measure's projections onto any finite-dimensional\nsubspaces under requirement on an algebraic nondegenerate condition for the\nflux.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 03:50:58 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Peng', 'Xuhui', ''], ['Su', 'Houqi', '']]","extracted_entities":"[{'text': 'viscous scalar conservation laws', 'label': 'Scaling law'}, {'text': 'torus', 'label': 'Mistral'}, {'text': 'general polynomial flux', 'label': 'Scaling law'}, {'text': 'attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention","similarity_score":0.7383304834}
{"id":2503.15831,"submitter":"Zihao Zhang","authors":"Zihao Zhang, Haoran Chen, Haoyu Zhao, Guansong Lu, Yanwei Fu, Hang Xu,\n  Zuxuan Wu","title":"EDEN: Enhanced Diffusion for High-quality Large-motion Video Frame\n  Interpolation","comments":"CVPR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Handling complex or nonlinear motion patterns has long posed challenges for\nvideo frame interpolation. Although recent advances in diffusion-based methods\noffer improvements over traditional optical flow-based approaches, they still\nstruggle to generate sharp, temporally consistent frames in scenarios with\nlarge motion. To address this limitation, we introduce EDEN, an Enhanced\nDiffusion for high-quality large-motion vidEo frame iNterpolation. Our approach\nfirst utilizes a transformer-based tokenizer to produce refined latent\nrepresentations of the intermediate frames for diffusion models. We then\nenhance the diffusion transformer with temporal attention across the process\nand incorporate a start-end frame difference embedding to guide the generation\nof dynamic motion. Extensive experiments demonstrate that EDEN achieves\nstate-of-the-art results across popular benchmarks, including nearly a 10%\nLPIPS reduction on DAVIS and SNU-FILM, and an 8% improvement on DAIN-HD.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 03:54:52 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zhang', 'Zihao', ''], ['Chen', 'Haoran', ''], ['Zhao', 'Haoyu', ''], ['Lu', 'Guansong', ''], ['Fu', 'Yanwei', ''], ['Xu', 'Hang', ''], ['Wu', 'Zuxuan', '']]","extracted_entities":"[{'text': 'temporal attention', 'label': 'Attention mechanism'}, {'text': 'start-end frame difference embedding', 'label': 'Embedding'}]","assigned_concept":"Attention mechanism","matched_keyword":"temporal attention","similarity_score":0.703645587}
{"id":2503.15926,"submitter":"Paolo Burelli","authors":"Meisam J. Seikavandi, Maria J. Barrett and Paolo Burelli","title":"Modeling Face Emotion Perception from Naturalistic Face Viewing:\n  Insights from Fixational Events and Gaze Strategies","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Face Emotion Recognition (FER) is essential for social interactions and\nunderstanding others' mental states. Utilizing eye tracking to investigate FER\nhas yielded insights into cognitive processes. In this study, we utilized an\ninstructionless paradigm to collect eye movement data from 21 participants,\nexamining two FER processes: free viewing and grounded FER. We analyzed\nfixational, pupillary, and microsaccadic events from eye movements,\nestablishing their correlation with emotion perception and performance in the\ngrounded task. By identifying regions of interest on the face, we explored the\nimpact of eye-gaze strategies on face processing, their connection to emotions,\nand performance in emotion perception. During free viewing, participants\ndisplayed specific attention patterns for various emotions. In grounded tasks,\nwhere emotions were interpreted based on words, we assessed performance and\ncontextual understanding. Notably, gaze patterns during free viewing predicted\nsuccess in grounded FER tasks, underscoring the significance of initial gaze\nbehavior. We also employed features from pre-trained deep-learning models for\nface recognition to enhance the scalability and comparability of attention\nanalysis during free viewing across different datasets and populations. This\nmethod facilitated the prediction and modeling of individual emotion perception\nperformance from minimal observations. Our findings advance the understanding\nof the link between eye movements and emotion perception, with implications for\npsychology, human-computer interaction, and affective computing, and pave the\nway for developing precise emotion recognition systems.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 08:01:59 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Seikavandi', 'Meisam J.', ''], ['Barrett', 'Maria J.', ''], ['Burelli', 'Paolo', '']]","extracted_entities":"[{'text': 'free viewing', 'label': 'Attention mechanism'}, {'text': 'attention patterns', 'label': 'Attention mechanism'}, {'text': 'free viewing', 'label': 'Attention mechanism'}, {'text': 'free viewing', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention patterns","similarity_score":0.8255490065}
{"id":2503.15973,"submitter":"Zichen Liu","authors":"Zichen Liu, Kunlun Xu, Bing Su, Xu Zou, Yuxin Peng, Jiahuan Zhou","title":"STOP: Integrated Spatial-Temporal Dynamic Prompting for Video\n  Understanding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Pre-trained on tremendous image-text pairs, vision-language models like CLIP\nhave demonstrated promising zero-shot generalization across numerous\nimage-based tasks. However, extending these capabilities to video tasks remains\nchallenging due to limited labeled video data and high training costs. Recent\nvideo prompting methods attempt to adapt CLIP for video tasks by introducing\nlearnable prompts, but they typically rely on a single static prompt for all\nvideo sequences, overlooking the diverse temporal dynamics and spatial\nvariations that exist across frames. This limitation significantly hinders the\nmodel's ability to capture essential temporal information for effective video\nunderstanding. To address this, we propose an integrated Spatial-TempOral\ndynamic Prompting (STOP) model which consists of two complementary modules, the\nintra-frame spatial prompting and inter-frame temporal prompting. Our\nintra-frame spatial prompts are designed to adaptively highlight discriminative\nregions within each frame by leveraging intra-frame attention and temporal\nvariation, allowing the model to focus on areas with substantial temporal\ndynamics and capture fine-grained spatial details. Additionally, to highlight\nthe varying importance of frames for video understanding, we further introduce\ninter-frame temporal prompts, dynamically inserting prompts between frames with\nhigh temporal variance as measured by frame similarity. This enables the model\nto prioritize key frames and enhances its capacity to understand temporal\ndependencies across sequences. Extensive experiments on various video\nbenchmarks demonstrate that STOP consistently achieves superior performance\nagainst state-of-the-art methods. The code is available at\nhttps:\/\/github.com\/zhoujiahuan1991\/CVPR2025-STOP.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 09:16:20 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Liu', 'Zichen', ''], ['Xu', 'Kunlun', ''], ['Su', 'Bing', ''], ['Zou', 'Xu', ''], ['Peng', 'Yuxin', ''], ['Zhou', 'Jiahuan', '']]","extracted_entities":"[{'text': 'intra-frame spatial prompting', 'label': 'Prompting'}, {'text': 'inter-frame temporal prompting', 'label': 'Prompting'}, {'text': 'intra-frame spatial prompts', 'label': 'Prompting'}, {'text': 'intra-frame attention', 'label': 'Attention mechanism'}, {'text': 'inter-frame temporal prompts', 'label': 'Prompting'}]","assigned_concept":"Attention mechanism","matched_keyword":"intra-frame attention","similarity_score":0.6550148129}
{"id":2503.16036,"submitter":"Zhihang Liu","authors":"Zhihang Liu and Chen-Wei Xie and Pandeng Li and Liming Zhao and\n  Longxiang Tang and Yun Zheng and Chuanbin Liu and Hongtao Xie","title":"Hybrid-Level Instruction Injection for Video Token Compression in\n  Multi-modal Large Language Models","comments":"Accepted to CVPR2025","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Recent Multi-modal Large Language Models (MLLMs) have been challenged by the\ncomputational overhead resulting from massive video frames, often alleviated\nthrough compression strategies. However, the visual content is not equally\ncontributed to user instructions, existing strategies (\\eg, average pool)\ninevitably lead to the loss of potentially useful information. To tackle this,\nwe propose the Hybrid-level Instruction Injection Strategy for Conditional\nToken Compression in MLLMs (HICom), utilizing the instruction as a condition to\nguide the compression from both local and global levels. This encourages the\ncompression to retain the maximum amount of user-focused information while\nreducing visual tokens to minimize computational burden. Specifically, the\ninstruction condition is injected into the grouped visual tokens at the local\nlevel and the learnable tokens at the global level, and we conduct the\nattention mechanism to complete the conditional compression. From the\nhybrid-level compression, the instruction-relevant visual parts are highlighted\nwhile the temporal-spatial structure is also preserved for easier understanding\nof LLMs. To further unleash the potential of HICom, we introduce a new\nconditional pre-training stage with our proposed dataset HICom-248K.\nExperiments show that our HICom can obtain distinguished video understanding\nability with fewer tokens, increasing the performance by 2.43\\% average on\nthree multiple-choice QA benchmarks and saving 78.8\\% tokens compared with the\nSOTA method. The code is available at https:\/\/github.com\/lntzm\/HICom.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 11:09:18 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Liu', 'Zhihang', ''], ['Xie', 'Chen-Wei', ''], ['Li', 'Pandeng', ''], ['Zhao', 'Liming', ''], ['Tang', 'Longxiang', ''], ['Zheng', 'Yun', ''], ['Liu', 'Chuanbin', ''], ['Xie', 'Hongtao', '']]","extracted_entities":"[{'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'MLLMs', 'label': 'Large Language Model'}, {'text': 'attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanism","similarity_score":1.0}
{"id":2503.16047,"submitter":"Akinyemi Sadeeq Akintola","authors":"Bisola Faith Kayode, Akinyemi Sadeeq Akintola, Oluwole Fagbohun,\n  Egonna Anaesiuba-Bristol, Onyekachukwu Ojumah, Oluwagbade Odimayo, Toyese\n  Oloyede, Aniema Inyang, Teslim Kazeem, Habeeb Alli, Udodirim Ibem Offia,\n  Prisca Chinazor Amajuoyi","title":"Temporal-Spatial Attention Network (TSAN) for DoS Attack Detection in\n  Network Traffic","comments":"19 Pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Denial-of-Service (DoS) attacks remain a critical threat to network security,\ndisrupting services and causing significant economic losses. Traditional\ndetection methods, including statistical and rule-based models, struggle to\nadapt to evolving attack patterns. To address this challenge, we propose a\nnovel Temporal-Spatial Attention Network (TSAN) architecture for detecting\nDenial of Service (DoS) attacks in network traffic. By leveraging both temporal\nand spatial features of network traffic, our approach captures complex traffic\npatterns and anomalies that traditional methods might miss. The TSAN model\nincorporates transformer-based temporal encoding, convolutional spatial\nencoding, and a cross-attention mechanism to fuse these complementary feature\nspaces. Additionally, we employ multi-task learning with auxiliary tasks to\nenhance the model's robustness. Experimental results on the NSL-KDD dataset\ndemonstrate that TSAN outperforms state-of-the-art models, achieving superior\naccuracy, precision, recall, and F1-score while maintaining computational\nefficiency for real-time deployment. The proposed architecture offers an\noptimal balance between detection accuracy and computational overhead, making\nit highly suitable for real-world network security applications.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 11:31:45 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Kayode', 'Bisola Faith', ''], ['Akintola', 'Akinyemi Sadeeq', ''], ['Fagbohun', 'Oluwole', ''], ['Anaesiuba-Bristol', 'Egonna', ''], ['Ojumah', 'Onyekachukwu', ''], ['Odimayo', 'Oluwagbade', ''], ['Oloyede', 'Toyese', ''], ['Inyang', 'Aniema', ''], ['Kazeem', 'Teslim', ''], ['Alli', 'Habeeb', ''], ['Offia', 'Udodirim Ibem', ''], ['Amajuoyi', 'Prisca Chinazor', '']]","extracted_entities":"[{'text': 'convolutional spatial\\nencoding', 'label': 'Attention mechanism'}, {'text': 'cross-attention mechanism', 'label': 'Attention mechanism'}, {'text': 'multi-task learning', 'label': 'Few-shot Learning'}]","assigned_concept":"Attention mechanism","matched_keyword":"cross-attention mechanism","similarity_score":0.8302809}
{"id":2503.16048,"submitter":"Michael Goodale","authors":"Michael Goodale, Salvador Mascarenhas and Yair Lakretz","title":"Meta-Learning Neural Mechanisms rather than Bayesian Priors","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Children acquire language despite being exposed to several orders of\nmagnitude less data than large language models require. Meta-learning has been\nproposed as a way to integrate human-like learning biases into neural-network\narchitectures, combining both the structured generalizations of symbolic models\nwith the scalability of neural-network models. But what does meta-learning\nexactly imbue the model with? We investigate the meta-learning of formal\nlanguages and find that, contrary to previous claims, meta-trained models are\nnot learning simplicity-based priors when meta-trained on datasets organised\naround simplicity. Rather, we find evidence that meta-training imprints neural\nmechanisms (such as counters) into the model, which function like cognitive\nprimitives for the network on downstream tasks. Most surprisingly, we find that\nmeta-training on a single formal language can provide as much improvement to a\nmodel as meta-training on 5000 different formal languages, provided that the\nformal language incentivizes the learning of useful neural mechanisms. Taken\ntogether, our findings provide practical implications for efficient\nmeta-learning paradigms and new theoretical insights into linking symbolic\ntheories and neural mechanisms.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 11:33:59 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Goodale', 'Michael', ''], ['Mascarenhas', 'Salvador', ''], ['Lakretz', 'Yair', '']]","extracted_entities":"[{'text': 'Meta-learning', 'label': 'Few-shot Learning'}, {'text': 'meta-learning', 'label': 'Few-shot Learning'}, {'text': 'neural\\nmechanisms', 'label': 'Attention mechanism'}, {'text': 'meta-training', 'label': 'Few-shot Learning'}, {'text': 'meta-training', 'label': 'Few-shot Learning'}, {'text': 'neural mechanisms', 'label': 'Attention mechanism'}, {'text': 'neural mechanisms', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"neural\nmechanisms","similarity_score":0.5939817429}
{"id":2503.16056,"submitter":"Yunzhe Zhang","authors":"Wanshu Fan, Yue Wang, Cong Wang, Yunzhe Zhang, Wei Wang and Dongsheng\n  Zhou","title":"Semantic-Guided Global-Local Collaborative Networks for Lightweight\n  Image Super-Resolution","comments":"14 pages,13 figures, 9 tables","journal-ref":"Ieee Transactions on Instrument and Measurement 2025","doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  Single-Image Super-Resolution (SISR) plays a pivotal role in enhancing the\naccuracy and reliability of measurement systems, which are integral to various\nvision-based instrumentation and measurement applications. These systems often\nrequire clear and detailed images for precise object detection and recognition.\nHowever, images captured by visual measurement tools frequently suffer from\ndegradation, including blurring and loss of detail, which can impede\nmeasurement accuracy.As a potential remedy, we in this paper propose a\nSemantic-Guided Global-Local Collaborative Network (SGGLC-Net) for lightweight\nSISR. Our SGGLC-Net leverages semantic priors extracted from a pre-trained\nmodel to guide the super-resolution process, enhancing image detail quality\neffectively. Specifically,we propose a Semantic Guidance Module that seamlessly\nintegrates the semantic priors into the super-resolution network, enabling the\nnetwork to more adeptly capture and utilize semantic priors, thereby enhancing\nimage details. To further explore both local and non-local interactions for\nimproved detail rendition,we propose a Global-Local Collaborative Module, which\nfeatures three Global and Local Detail Enhancement Modules, as well as a Hybrid\nAttention Mechanism to work together to efficiently learn more useful features.\nOur extensive experiments show that SGGLC-Net achieves competitive PSNR and\nSSIM values across multiple benchmark datasets, demonstrating higher\nperformance with the multi-adds reduction of 12.81G compared to\nstate-of-the-art lightweight super-resolution approaches. These improvements\nunderscore the potential of our approach to enhance the precision and\neffectiveness of visual measurement systems. Codes are at\nhttps:\/\/github.com\/fanamber831\/SGGLC-Net.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 11:43:55 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Fan', 'Wanshu', ''], ['Wang', 'Yue', ''], ['Wang', 'Cong', ''], ['Zhang', 'Yunzhe', ''], ['Wang', 'Wei', ''], ['Zhou', 'Dongsheng', '']]","extracted_entities":"[{'text': 'Hybrid\\nAttention Mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Hybrid\nAttention Mechanism","similarity_score":0.8765320778}
{"id":2503.16065,"submitter":"Yingmao Miao","authors":"Yingmao Miao, Zhanpeng Huang, Rui Han, Zibin Wang, Chenhao Lin, Chao\n  Shen","title":"Shining Yourself: High-Fidelity Ornaments Virtual Try-on with Diffusion\n  Model","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  While virtual try-on for clothes and shoes with diffusion models has gained\nattraction, virtual try-on for ornaments, such as bracelets, rings, earrings,\nand necklaces, remains largely unexplored. Due to the intricate tiny patterns\nand repeated geometric sub-structures in most ornaments, it is much more\ndifficult to guarantee identity and appearance consistency under large pose and\nscale variances between ornaments and models. This paper proposes the task of\nvirtual try-on for ornaments and presents a method to improve the geometric and\nappearance preservation of ornament virtual try-ons. Specifically, we estimate\nan accurate wearing mask to improve the alignments between ornaments and models\nin an iterative scheme alongside the denoising process. To preserve structure\ndetails, we further regularize attention layers to map the reference ornament\nmask to the wearing mask in an implicit way. Experimental results demonstrate\nthat our method successfully wears ornaments from reference images onto target\nmodels, handling substantial differences in scale and pose while preserving\nidentity and achieving realistic visual effects.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 11:57:32 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Miao', 'Yingmao', ''], ['Huang', 'Zhanpeng', ''], ['Han', 'Rui', ''], ['Wang', 'Zibin', ''], ['Lin', 'Chenhao', ''], ['Shen', 'Chao', '']]","extracted_entities":"[{'text': 'attention layers', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention layers","similarity_score":0.7064620256}
{"id":2503.16067,"submitter":"Tim Seizinger","authors":"Tim Seizinger, Florin-Alexandru Vasluianu, Marcos V. Conde, Radu\n  Timofte","title":"Bokehlicious: Photorealistic Bokeh Rendering with Controllable Apertures","comments":"Technical Report","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/","abstract":"  Bokeh rendering methods play a key role in creating the visually appealing,\nsoftly blurred backgrounds seen in professional photography. While recent\nlearning-based approaches show promising results, generating realistic Bokeh\nwith variable strength remains challenging. Existing methods require additional\ninputs and suffer from unrealistic Bokeh reproduction due to reliance on\nsynthetic data. In this work, we propose Bokehlicious, a highly efficient\nnetwork that provides intuitive control over Bokeh strength through an\nAperture-Aware Attention mechanism, mimicking the physical lens aperture. To\nfurther address the lack of high-quality real-world data, we present RealBokeh,\na novel dataset featuring 23,000 high-resolution (24-MP) images captured by\nprofessional photographers, covering diverse scenes with varied aperture and\nfocal length settings. Evaluations on both our new RealBokeh and established\nBokeh rendering benchmarks show that Bokehlicious consistently outperforms SOTA\nmethods while significantly reducing computational cost and exhibiting strong\nzero-shot generalization. Our method and dataset further extend to defocus\ndeblurring, achieving competitive results on the RealDOF benchmark. Our code\nand data can be found at https:\/\/github.com\/TimSeizinger\/Bokehlicious\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 12:00:45 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Seizinger', 'Tim', ''], ['Vasluianu', 'Florin-Alexandru', ''], ['Conde', 'Marcos V.', ''], ['Timofte', 'Radu', '']]","extracted_entities":"[{'text': 'Aperture-Aware Attention mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"Aperture-Aware Attention mechanism","similarity_score":0.7342512608}
{"id":2503.16069,"submitter":"Aniek Eijpe","authors":"Aniek Eijpe, Soufyan Lakbir, Melis Erdal Cesur, Sara P. Oliveira,\n  Sanne Abeln and Wilson Silva","title":"Disentangled and Interpretable Multimodal Attention Fusion for Cancer\n  Survival Prediction","comments":"11 pages, 1 figure, 3 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  To improve the prediction of cancer survival using whole-slide images and\ntranscriptomics data, it is crucial to capture both modality-shared and\nmodality-specific information. However, multimodal frameworks often entangle\nthese representations, limiting interpretability and potentially suppressing\ndiscriminative features. To address this, we propose Disentangled and\nInterpretable Multimodal Attention Fusion (DIMAF), a multimodal framework that\nseparates the intra- and inter-modal interactions within an attention-based\nfusion mechanism to learn distinct modality-specific and modality-shared\nrepresentations. We introduce a loss based on Distance Correlation to promote\ndisentanglement between these representations and integrate Shapley additive\nexplanations to assess their relative contributions to survival prediction. We\nevaluate DIMAF on four public cancer survival datasets, achieving a relative\naverage improvement of 1.85% in performance and 23.7% in disentanglement\ncompared to current state-of-the-art multimodal models. Beyond improved\nperformance, our interpretable framework enables a deeper exploration of the\nunderlying interactions between and within modalities in cancer biology.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 12:02:10 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Eijpe', 'Aniek', ''], ['Lakbir', 'Soufyan', ''], ['Cesur', 'Melis Erdal', ''], ['Oliveira', 'Sara P.', ''], ['Abeln', 'Sanne', ''], ['Silva', 'Wilson', '']]","extracted_entities":"[{'text': 'attention-based\\nfusion mechanism', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention-based\nfusion mechanism","similarity_score":0.7242620587}
{"id":2503.16149,"submitter":"Dong Chen","authors":"Dong Chen, Boyue Zhao, Yi Zhang, Meng Zhao","title":"Selective Complementary Feature Fusion and Modal Feature Compression\n  Interaction for Brain Tumor Segmentation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Efficient modal feature fusion strategy is the key to achieve accurate\nsegmentation of brain glioma. However, due to the specificity of different MRI\nmodes, it is difficult to carry out cross-modal fusion with large differences\nin modal features, resulting in the model ignoring rich feature information. On\nthe other hand, the problem of multi-modal feature redundancy interaction\noccurs in parallel networks due to the proliferation of feature dimensions,\nfurther increase the difficulty of multi-modal feature fusion at the bottom\nend. In order to solve the above problems, we propose a noval complementary\nfeature compression interaction network (CFCI-Net), which realizes the\ncomplementary fusion and compression interaction of multi-modal feature\ninformation with an efficient mode fusion strategy. Firstly, we propose a\nselective complementary feature fusion (SCFF) module, which adaptively fuses\nrich cross-modal feature information by complementary soft selection weights.\nSecondly, a modal feature compression interaction (MFCI) transformer is\nproposed to deal with the multi-mode fusion redundancy problem when the feature\ndimension surges. The MFCI transformer is composed of modal feature compression\n(MFC) and modal feature interaction (MFI) to realize redundancy feature\ncompression and multi-mode feature interactive learning. %In MFI, we propose a\nhierarchical interactive attention mechanism based on multi-head attention.\nEvaluations on the BraTS2019 and BraTS2020 datasets demonstrate that CFCI-Net\nachieves superior results compared to state-of-the-art models. Code:\nhttps:\/\/github.com\/CDmm0\/CFCI-Net\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 13:52:51 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chen', 'Dong', ''], ['Zhao', 'Boyue', ''], ['Zhang', 'Yi', ''], ['Zhao', 'Meng', '']]","extracted_entities":"[{'text': 'multi-mode feature interactive learning', 'label': 'Few-shot Learning'}, {'text': 'MFI', 'label': 'Attention mechanism'}, {'text': 'hierarchical interactive attention mechanism', 'label': 'Attention mechanism'}, {'text': 'multi-head attention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"hierarchical interactive attention mechanism","similarity_score":0.814907074}
{"id":2503.16284,"submitter":"Sharon Peled","authors":"Sharon Peled, Yosef E. Maruvka, Moti Freiman","title":"PSA-MIL: A Probabilistic Spatial Attention-Based Multiple Instance\n  Learning for Whole Slide Image Classification","comments":"8 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/","abstract":"  Whole Slide Images (WSIs) are high-resolution digital scans widely used in\nmedical diagnostics. WSI classification is typically approached using Multiple\nInstance Learning (MIL), where the slide is partitioned into tiles treated as\ninterconnected instances. While attention-based MIL methods aim to identify the\nmost informative tiles, they often fail to fully exploit the spatial\nrelationships among them, potentially overlooking intricate tissue structures\ncrucial for accurate diagnosis. To address this limitation, we propose\nProbabilistic Spatial Attention MIL (PSA-MIL), a novel attention-based MIL\nframework that integrates spatial context into the attention mechanism through\nlearnable distance-decayed priors, formulated within a probabilistic\ninterpretation of self-attention as a posterior distribution. This formulation\nenables a dynamic inference of spatial relationships during training,\neliminating the need for predefined assumptions often imposed by previous\napproaches. Additionally, we suggest a spatial pruning strategy for the\nposterior, effectively reducing self-attention's quadratic complexity. To\nfurther enhance spatial modeling, we introduce a diversity loss that encourages\nvariation among attention heads, ensuring each captures distinct spatial\nrepresentations. Together, PSA-MIL enables a more data-driven and adaptive\nintegration of spatial context, moving beyond predefined constraints. We\nachieve state-of-the-art performance across both contextual and non-contextual\nbaselines, while significantly reducing computational costs.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 16:12:42 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Peled', 'Sharon', ''], ['Maruvka', 'Yosef E.', ''], ['Freiman', 'Moti', '']]","extracted_entities":"[{'text': 'Multiple\\nInstance Learning (MIL)', 'label': 'Few-shot Learning'}, {'text': 'attention mechanism', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'self-attention', 'label': 'Attention mechanism'}, {'text': 'spatial context', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanism","similarity_score":1.0}
{"id":2503.16389,"submitter":"Kristin Qi","authors":"Kristin Qi, Xinhan Di","title":"Attentional Triple-Encoder Network in Spatiospectral Domains for Medical\n  Image Segmentation","comments":"IEEE Conference on Artificial Intelligence (IEEE CAI)","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.AI cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  Retinal Optical Coherence Tomography (OCT) segmentation is essential for\ndiagnosing pathology. Traditional methods focus on either spatial or spectral\ndomains, overlooking their combined dependencies. We propose a triple-encoder\nnetwork that integrates CNNs for spatial features, Fast Fourier Convolution\n(FFC) for spectral features, and attention mechanisms to capture global\nrelationships across both domains. Attention fusion modules integrate\nconvolution and cross-attention to further enhance features. Our method\nachieves an average Dice score improvement from 0.855 to 0.864, outperforming\nprior work.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:49:01 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Qi', 'Kristin', ''], ['Di', 'Xinhan', '']]","extracted_entities":"[{'text': 'attention mechanisms', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"attention mechanisms","similarity_score":0.9558142424}
{"id":2503.16396,"submitter":"Chun-Han Yao","authors":"Chun-Han Yao, Yiming Xie, Vikram Voleti, Huaizu Jiang, Varun Jampani","title":"SV4D 2.0: Enhancing Spatio-Temporal Consistency in Multi-View Video\n  Diffusion for High-Quality 4D Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/creativecommons.org\/licenses\/by\/4.0\/","abstract":"  We present Stable Video 4D 2.0 (SV4D 2.0), a multi-view video diffusion model\nfor dynamic 3D asset generation. Compared to its predecessor SV4D, SV4D 2.0 is\nmore robust to occlusions and large motion, generalizes better to real-world\nvideos, and produces higher-quality outputs in terms of detail sharpness and\nspatio-temporal consistency. We achieve this by introducing key improvements in\nmultiple aspects: 1) network architecture: eliminating the dependency of\nreference multi-views and designing blending mechanism for 3D and frame\nattention, 2) data: enhancing quality and quantity of training data, 3)\ntraining strategy: adopting progressive 3D-4D training for better\ngeneralization, and 4) 4D optimization: handling 3D inconsistency and large\nmotion via 2-stage refinement and progressive frame sampling. Extensive\nexperiments demonstrate significant performance gain by SV4D 2.0 both visually\nand quantitatively, achieving better detail (-14\\% LPIPS) and 4D consistency\n(-44\\% FV4D) in novel-view video synthesis and 4D optimization (-12\\% LPIPS and\n-24\\% FV4D) compared to SV4D. Project page: https:\/\/sv4d2.0.github.io.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:53:38 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Yao', 'Chun-Han', ''], ['Xie', 'Yiming', ''], ['Voleti', 'Vikram', ''], ['Jiang', 'Huaizu', ''], ['Jampani', 'Varun', '']]","extracted_entities":"[{'text': 'blending mechanism', 'label': 'Attention mechanism'}, {'text': '3D and frame\\nattention', 'label': 'Attention mechanism'}]","assigned_concept":"Attention mechanism","matched_keyword":"3D and frame\nattention","similarity_score":0.5626410246}
{"id":2503.16413,"submitter":"Xueyan Zou","authors":"Xueyan Zou, Yuchen Song, Ri-Zhao Qiu, Xuanbin Peng, Jianglong Ye,\n  Sifei Liu, Xiaolong Wang","title":"M3: 3D-Spatial MultiModal Memory","comments":"ICLR2025 homepage: https:\/\/m3-spatial-memory.github.io code:\n  https:\/\/github.com\/MaureenZOU\/m3-spatial","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.RO","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  We present 3D Spatial MultiModal Memory (M3), a multimodal memory system\ndesigned to retain information about medium-sized static scenes through video\nsources for visual perception. By integrating 3D Gaussian Splatting techniques\nwith foundation models, M3 builds a multimodal memory capable of rendering\nfeature representations across granularities, encompassing a wide range of\nknowledge. In our exploration, we identify two key challenges in previous works\non feature splatting: (1) computational constraints in storing high-dimensional\nfeatures for each Gaussian primitive, and (2) misalignment or information loss\nbetween distilled features and foundation model features. To address these\nchallenges, we propose M3 with key components of principal scene components and\nGaussian memory attention, enabling efficient training and inference. To\nvalidate M3, we conduct comprehensive quantitative evaluations of feature\nsimilarity and downstream tasks, as well as qualitative visualizations to\nhighlight the pixel trace of Gaussian memory attention. Our approach\nencompasses a diverse range of foundation models, including vision-language\nmodels (VLMs), perception models, and large multimodal and language models\n(LMMs\/LLMs). Furthermore, to demonstrate real-world applicability, we deploy\nM3's feature field in indoor scenes on a quadruped robot. Notably, we claim\nthat M3 is the first work to address the core compression challenges in 3D\nfeature distillation.\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:59:12 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Zou', 'Xueyan', ''], ['Song', 'Yuchen', ''], ['Qiu', 'Ri-Zhao', ''], ['Peng', 'Xuanbin', ''], ['Ye', 'Jianglong', ''], ['Liu', 'Sifei', ''], ['Wang', 'Xiaolong', '']]","extracted_entities":"[{'text': 'M3', 'label': 'Foundation Model'}, {'text': 'Gaussian memory attention', 'label': 'Attention mechanism'}, {'text': 'Gaussian memory attention', 'label': 'Attention mechanism'}, {'text': 'perception models', 'label': 'Foundation Model'}, {'text': 'M3', 'label': 'Large Language Model'}, {'text': '3D\\nfeature distillation', 'label': 'Knowledge distillation'}]","assigned_concept":"Attention mechanism","matched_keyword":"Gaussian memory attention","similarity_score":0.6678389311}
{"id":2503.16426,"submitter":"Keyan Chen","authors":"Keyan Chen, Chenyang Liu, Bowen Chen, Wenyuan Li, Zhengxia Zou,\n  Zhenwei Shi","title":"DynamicVis: An Efficient and General Visual Foundation Model for Remote\n  Sensing Image Understanding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http:\/\/arxiv.org\/licenses\/nonexclusive-distrib\/1.0\/","abstract":"  The advancement of remote sensing technology has improved the spatial\nresolution of satellite imagery, facilitating more detailed visual\nrepresentations for diverse interpretations. However, existing methods exhibit\nlimited generalization capabilities across varied applications. While some\ncontemporary foundation models demonstrate potential, they are hindered by\ninsufficient cross-task adaptability and primarily process low-resolution\nimagery of restricted sizes, thus failing to fully exploit high-resolution data\nor leverage comprehensive large-scene semantics. Crucially, remote sensing\nimagery differs fundamentally from natural images, as key foreground targets\n(eg., maritime objects, artificial structures) often occupy minimal spatial\nproportions (~1%) and exhibit sparse distributions. Efficiently modeling\ncross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a\nsignificant challenge yet remains critical for remote sensing image\nunderstanding. Motivated by the selective attention mechanisms inherent to the\nhuman visual system, we propose DynamicVis, a dynamic visual perception\nfoundation model for remote sensing imagery. The framework integrates a novel\ndynamic region perception backbone based on the selective state space model,\nwhich strategically balances localized detail extraction with global contextual\nintegration, enabling computationally efficient encoding of large-scale data\nwhile maintaining architectural scalability. To enhance cross-task knowledge\ntransferring, we introduce a multi-instance learning paradigm utilizing\nmeta-embedding representations, trained on million-scale region-level\nannotations. Evaluations across nine downstream tasks demonstrate the model's\nversatility. DynamicVis achieves multi-level feature modeling with exceptional\nefficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and\n833 MB GPU memory (3% of ViT's).\n","versions":"[{'version': 'v1', 'created': 'Thu, 20 Mar 2025 17:59:54 GMT'}]","update_date":"2025-03-21","authors_parsed":"[['Chen', 'Keyan', ''], ['Liu', 'Chenyang', ''], ['Chen', 'Bowen', ''], ['Li', 'Wenyuan', ''], ['Zou', 'Zhengxia', ''], ['Shi', 'Zhenwei', '']]","extracted_entities":"[{'text': 'selective attention mechanisms', 'label': 'Attention mechanism'}, {'text': 'DynamicVis', 'label': 'Foundation Model'}, {'text': 'global contextual\\nintegration', 'label': 'contextual Embedding'}, {'text': 'architectural scalability', 'label': 'Scaling law'}, {'text': 'meta-embedding representations', 'label': 'contextual Embedding'}, {'text': 'DynamicVis', 'label': 'Foundation Model'}]","assigned_concept":"Attention mechanism","matched_keyword":"selective attention mechanisms","similarity_score":0.915332675}
